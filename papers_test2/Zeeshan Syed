Contributions of PDM Systems in Organizational Technical Data Management

  Product Data Management (PDM) claims of producing desktop and web basedsystems to maintain the organizational data to increase the quality of productsby improving the process of development, business process flows, changemanagement, product structure management, project tracking and resourceplanning. Moreover PDM helps in reducing the cost and effort required inengineering. This paper discusses PDM desktop and web based system, neededinformation and important guidelines for PDM system development, functionalrequirements, basic components in detail and some already implemented PDMSys-tems. In the end paper investigates and briefly concludes major currentlyfaced challenges to Product Data Management (PDM) community.

An accelerated CLPSO algorithm

  The particle swarm approach provides a low complexity solution to theoptimization problem among various existing heuristic algorithms. Recentadvances in the algorithm resulted in improved performance at the cost ofincreased computational complexity, which is undesirable. Literature shows thatthe particle swarm optimization algorithm based on comprehensive learningprovides the best complexity-performance trade-off. We show how to reduce thecomplexity of this algorithm further, with a slight but acceptable performanceloss. This enhancement allows the application of the algorithm in time criticalapplications, such as, real-time tracking, equalization etc.

Association of Networked Flying Platforms with Small Cells for Network  Centric 5G+ C-RAN

  5G+ systems expect enhancement in data rate and coverage area under limitedpower constraint. Such requirements can be fulfilled by the densification ofsmall cells (SCs). However, a major challenge is the management of fronthaullinks connected with an ultra dense network of SCs. A cost effective andscalable idea of using network flying platforms (NFPs) is employed here, wherethe NFPs are used as fronthaul hubs that connect the SCs to the core network.The association problem of NFPs and SCs is formulated considering a number ofpractical constraints such as backhaul data rate limit, maximum supported linksand bandwidth by NFPs and quality of service requirement of the system. Thenetwork centric case of the system is considered that aims to maximize thenumber of associated SCs without any biasing, i.e., no preference for highpriority SCs. Then, two new efficient greedy algorithms are designed to solvethe presented association problem. Numerical results show a favorableperformance of our proposed methods in comparison to exhaustive search.

A Distributed Approach for Networked Flying Platform Association with  Small Cells in 5G+ Networks

  The densification of small-cell base stations in a 5G architecture is apromising approach to enhance the coverage area and facilitate the everincreasing capacity demand of end users. However, the bottleneck is anintelligent management of a backhaul/fronthaul network for these small-cellbase stations. This involves efficient association and placement of thebackhaul hubs that connects these small-cells with the core network.Terrestrial hubs suffer from an inefficient non line of sight link limitationsand unavailability of a proper infrastructure in an urban area. Seeing thepopularity of flying platforms, we employ here an idea of using networkedflying platform (NFP) such as unmanned aerial vehicles (UAVs), drones, unmannedballoons flying at different altitudes, as aerial backhaul hubs. Theassociation problem of these NFP-hubs and small-cell base stations isformulated considering backhaul link and NFP related limitations such asmaximum number of supported links and bandwidth. Then, this paper presents anefficient and distributed solution of the designed problem, which performs agreedy search in order to maximize the sum rate of the overall network. Afavorable performance is observed via a numerical comparison of our proposedmethod with optimal exhaustive search algorithm in terms of sum rate andrun-time speed.

Small Cell Association with Networked Flying Platforms: Novel Algorithms  and Performance Bounds

  Fifth generation (5G) and beyond-5G (B5G) systems expect coverage andcapacity enhancements along with the consideration of limited power, cost andspectrum. Densification of small cells (SCs) is a promising approach to caterthese demands of 5G and B5G systems. However, such an ultra dense network ofSCs requires provision of smart backhaul and fronthaul networks. In this paper,we employ a scalable idea of using networked flying platforms (NFPs) as aerialhubs to provide fronthaul connectivity to the SCs. We consider the associationproblem of SCs and NFPs in a SC network and study the effect of practicalconstraints related to the system and NFPs. Mainly, we show that theassociation problem is related to the generalized assignment problem (GAP).Using this relation with the GAP, we show the NP-hard complexity of theassociation problem and further derive an upper bound for the maximumachievable sum data rate. Linear Programming relaxation of the problem is alsostudied to compare the results with the derived bounds. Finally, two efficient(less complex) greedy solutions of the association problem are presented, whereone of them is a distributed solution and the other one is its centralizedversion. Numerical results show a favorable performance of the presentedalgorithms with respect to the exhaustive search and derived bounds. Thecomputational complexity comparison of the algorithms with the exhaustivesearch is also presented to show that the presented algorithms can bepractically implemented.

Low-Complexity Particle Swarm Optimization for Time-Critical  Applications

  Particle swam optimization (PSO) is a popular stochastic optimization methodthat has found wide applications in diverse fields. However, PSO suffers fromhigh computational complexity and slow convergence speed. High computationalcomplexity hinders its use in applications that have limited power resourceswhile slow convergence speed makes it unsuitable for time criticalapplications. In this paper, we propose two techniques to overcome theselimitations. The first technique reduces the computational complexity of PSOwhile the second technique speeds up its convergence. These techniques can beapplied, either separately or in conjunction, to any existing PSO variant. Theproposed techniques are robust to the number of dimensions of the optimizationproblem. Simulation results are presented for the proposed techniques appliedto the standard PSO as well as to several PSO variants. The results show thatthe use of both these techniques in conjunction results in a reduction in thenumber of computations required as well as faster convergence speed whilemaintaining an acceptable error performance for time-critical applications.

Uncovering Voice Misuse Using Symbolic Mismatch

  Voice disorders affect an estimated 14 million working-aged Americans, andmany more worldwide. We present the first large scale study of vocal misusebased on long-term ambulatory data collected by an accelerometer placed on theneck. We investigate an unsupervised data mining approach to uncovering latentinformation about voice misuse.  We segment signals from over 253 days of data from 22 subjects into over ahundred million single glottal pulses (closures of the vocal folds), clustersegments into symbols, and use symbolic mismatch to uncover differences betweenpatients and matched controls, and between patients pre- and post-treatment.Our results show significant behavioral differences between patients andcontrols, as well as between some pre- and post-treatment patients. Ourproposed approach provides an objective basis for helping diagnose behavioralvoice disorders, and is a first step towards a more data-driven understandingof the impact of voice therapy.

EEG Spatial Decoding and Classification with Logit Shrinkage Regularized  Directed Information Assessment (L-SODA)

  There is an increasing interest in studying the neural interaction mechanismsbehind patterns of cognitive brain activity. This paper proposes a new approachto infer such interaction mechanisms from electroencephalographic (EEG) datausing a new estimator of directed information (DI) called logit shrinkageoptimized directed information assessment (L-SODA). Unlike previous directedinformation measures applied to neural decoding, L-SODA uses shrinkageregularization on multinomial logistic regression to deal with the highdimensionality of multi-channel EEG signals and the small sizes of manyreal-world datasets. It is designed to make few a priori assumptions and canhandle both non-linear and non-Gaussian flows among electrodes. Our L-SODAestimator of the DI is accompanied by robust statistical confidence intervalson the true DI that make it especially suitable for hypothesis testing on theinformation flow patterns. We evaluate our work in the context of two differentproblems where interaction localization is used to determine highly interactiveareas for EEG signals spatially and temporally. First, by mapping the areasthat have high DI into Brodmann area, we identify that the areas with high DIare associated with motor-related functions. We demonstrate that L-SODAprovides better accuracy for neural decoding of EEG signals as compared toseveral state-of-the-art approaches on the Brain Computer Interface (BCI) EEGmotor activity dataset. Second, the proposed L-SODA estimator is evaluated onthe CHB-MIT Scalp EEG database. We demonstrate that compared to thestate-of-the-art approaches, the proposed method provides better performance indetecting the epileptic seizure.

Design and Implementation of iMacros-based Data Crawler for Behavioral  Analysis of Facebook Users

  Obtaining the desired dataset is still a prime challenge faced by researcherswhile analysing Online Social Network (OSN) sites. Application ProgrammingInterfaces (APIs) provided by OSN service providers for retrieving data imposeseveral unavoidable restrictions which make it difficult to get a desirabledataset. In this paper, we present an iMacros technology-based data crawlercalled IMcrawler,capable of collecting every piece of information which isaccessible through a browser from the Facebook website within the legalframework reauthorized by Facebook.The proposed crawler addresses most of thechallenges allied with web data extraction approaches and most of the APIsprovided by OSN service providers. Two broad sections have been extracted fromFacebook user profiles, namely, Personal Information and Wall Activities. Thecollected data is pre-processed into two datasets and each data set isstatistically analysed to draw semantic knowledge and understand the severalbehavioral aspects of Facebook users such as kind of information mostlydisclosed by users, gender differences in the pattern of revealed information,highly posted content on the network, highly performed activities on thenetwork, the relationships among personal and post attributes, etc. To the bestof our knowledge, the present work is the first attempt towards providing thedetailed description of crawler design and gender-based information revealingbehaviour of Facebook users.

Clinically Meaningful Comparisons Over Time: An Approach to Measuring  Patient Similarity based on Subsequence Alignment

  Longitudinal patient data has the potential to improve clinical riskstratification models for disease. However, chronic diseases that progressslowly over time are often heterogeneous in their clinical presentation.Patients may progress through disease stages at varying rates. This leads topathophysiological misalignment over time, making it difficult to consistentlycompare patients in a clinically meaningful way. Furthermore, patients presentclinically for the first time at different stages of disease. This eliminatesthe possibility of simply aligning patients based on their initialpresentation. Finally, patient data may be sampled at different rates due todifferences in schedules or missed visits. To address these challenges, wepropose a robust measure of patient similarity based on subsequence alignment.Compared to global alignment techniques that do not account forpathophysiological misalignment, focusing on the most relevant subsequencesallows for an accurate measure of similarity between patients. We demonstratethe utility of our approach in settings where longitudinal data, while useful,are limited and lack a clear temporal alignment for comparison. Applied to thetask of stratifying patients for risk of progression to probable Alzheimer'sDisease, our approach outperforms models that use only snapshot data (AUROC of0.839 vs. 0.812) and models that use global alignment techniques (AUROC of0.822). Our results support the hypothesis that patients' trajectories areuseful for quantifying inter-patient similarities and that using subsequencematching and can help account for heterogeneity and misalignment inlongitudinal data.

A closer look at Intrusion Detection System for web applications

  Intrusion Detection System (IDS) is one of the security measures being usedas an additional defence mechanism to prevent the security breaches on web. Ithas been well known methodology for detecting network-based attacks but stillimmature in the domain of securing web application. The objective of the paperis to thoroughly understand the design methodology of the detection system inrespect to web applications. In this paper, we discuss several specific aspectsof a web application in detail that makes challenging for a developer to buildan efficient web IDS. The paper also provides a comprehensive overview of theexisting detection systems exclusively designed to observe web traffic.Furthermore, we identify various dimensions for comparing the IDS fromdifferent perspectives based on their design and functionalities. We alsoprovide a conceptual framework of an IDS with prevention mechanism to offer asystematic guidance for the implementation of the system specific to the webapplications. We compare its features with five existing detection systems,namely AppSensor, PHPIDS, ModSecurity, Shadow Daemon and AQTRONIX WebKnight.The paper will highly facilitate the interest groups with the cutting edgeinformation to understand the stronger and weaker sections of the web IDS andprovide a firm foundation for developing an intelligent and efficient system.

Identification of Flaws in the Design of Signatures for Intrusion  Detection Systems

  Signature-based Intrusion Detection System (SIDS) provides a promisingsolution to the problem of web application security. However, the performanceof the system highly relies on the quality of the signatures designed to detectattacks. A weak signature set may considerably cause an increase in false alarmrate, making impractical to deploy the system. The objective of the paper is toidentify the flaws in the signature structure which are responsible to reducethe efficiency of the detection system. The paper targets SQL injectionsignatures particularly. Initially, some essential concepts of the domain ofthe attack that should be focused by the developer in prior to designing thesignatures have been discussed. Afterwards, we conducted a case study on thewell known PHPIDS tool for analyzing the quality of its SQL signatures. Basedon the analysis, we identify various flaws in the designing practice that yieldinefficient signatures. We divide the weak signatures into six categories,namely incomplete, irrelevant, semi-relevant, susceptible, redundant andinconsistent signatures. Moreover, we quantify these weaknesses and define themmathematically in terms of set theory. To the best of our knowledge, we haveidentified some novel signature design issues. The paper will basically assistthe signature developer to know what level of expertise is required fordevising a quality signature set and how a little ignorance may lead todeterioration in the performance of the SIDS. Furthermore, a security expertmay evaluate the detector against the identified flaws by conducting structuralanalysis on its signature set.

