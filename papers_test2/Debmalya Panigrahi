A Linear-time Algorithm for Sparsification of Unweighted Graphs

  Given an undirected graph $G$ and an error parameter $\epsilon > 0$, the {\emgraph sparsification} problem requires sampling edges in $G$ and giving thesampled edges appropriate weights to obtain a sparse graph $G_{\epsilon}$ withthe following property: the weight of every cut in $G_{\epsilon}$ is within afactor of $(1\pm \epsilon)$ of the weight of the corresponding cut in $G$. If$G$ is unweighted, an $O(m\log n)$-time algorithm for constructing$G_{\epsilon}$ with $O(n\log n/\epsilon^2)$ edges in expectation, and an$O(m)$-time algorithm for constructing $G_{\epsilon}$ with $O(n\log^2n/\epsilon^2)$ edges in expectation have recently been developed(Hariharan-Panigrahi, 2010). In this paper, we improve these results by givingan $O(m)$-time algorithm for constructing $G_{\epsilon}$ with $O(n\logn/\epsilon^2)$ edges in expectation, for unweighted graphs. Our algorithm isoptimal in terms of its time complexity; further, no efficient algorithm isknown for constructing a sparser $G_{\epsilon}$. Our algorithm is Monte-Carlo,i.e. it produces the correct output with high probability, as are all efficientgraph sparsification algorithms.

Online Service with Delay

  In this paper, we introduce the online service with delay problem. In thisproblem, there are $n$ points in a metric space that issue service requestsover time, and a server that serves these requests. The goal is to minimize thesum of distance traveled by the server and the total delay in serving therequests. This problem models the fundamental tradeoff between batchingrequests to improve locality and reducing delay to improve response time, thathas many applications in operations management, operating systems, logistics,supply chain management, and scheduling.  Our main result is to show a poly-logarithmic competitive ratio for theonline service with delay problem. This result is obtained by an algorithm thatwe call the preemptive service algorithm. The salient feature of this algorithmis a process called preemptive service, which uses a novel combination of(recursive) time forwarding and spatial exploration on a metric space. We hopethis technique will be useful for related problems such as reordering buffermanagement, online TSP, vehicle routing, etc. We also generalize our results to$k > 1$ servers.

A General Framework for Graph Sparsification

  Given a weighted graph $G$ and an error parameter $\epsilon > 0$, the {\emgraph sparsification} problem requires sampling edges in $G$ and giving thesampled edges appropriate weights to obtain a sparse graph $G_{\epsilon}$(containing O(n\log n) edges in expectation) with the following property: theweight of every cut in $G_{\epsilon}$ is within a factor of $(1\pm \epsilon)$of the weight of the corresponding cut in $G$. We provide a generic frameworkthat sets out sufficient conditions for any particular sampling scheme toresult in good sparsifiers, and obtain a set of results by simpleinstantiations of this framework. The results we obtain include the following:(1) We improve the time complexity of graph sparsification from O(m\log^3 n) toO(m + n\log^4 n) for graphs with polynomial edge weights. (2) We improve thetime complexity of graph sparsification from O(m\log^3 n) to O(m\log^2 n) forgraphs with arbitrary edge weights. (3) If the size of the sparsifier isallowed to be O(n\log^2 n/\epsilon^2) instead of O(n\log n/\epsilon^2), weimprove the time complexity of sparsification to O(m) for graphs withpolynomial edge weights. (4) We show that sampling using standardconnectivities results in good sparsifiers, thus resolving an open question ofBenczur and Karger. As a corollary, we give a simple proof of (a slightlyweaker version of) a result due to Spielman and Srivastava showing thatsampling using effective resistances produces good sparsifiers. (5) We give asimple proof showing that sampling using strong connectivities results in goodsparsifiers, a result obtained previously using a more involved proof byBenczur and Karger. A key ingredient of our proofs is a generalization ofbounds on the number of small cuts in an undirected graph due to Karger; thisgeneralization might be of independent interest.

Online Load Balancing on Unrelated Machines with Startup Costs

  Motivated by applications in energy-efficient scheduling in data centers,Khuller, Li, and Saha introduced the {\em machine activation} problem as ageneralization of the classical optimization problems of set cover and loadbalancing on unrelated machines. In this problem, a set of $n$ jobs have to bedistributed among a set of $m$ (unrelated) machines, given the processing timeof each job on each machine, where each machine has a startup cost. The goal isto produce a schedule of minimum total startup cost subject to a constraint$\bf L$ on its makespan. While Khuller {\em et al} considered the offlineversion of this problem, a typical scenario in scheduling is one where jobsarrive online and have to be assigned to a machine immediately on arrival. Wegive an $(O(\log (mn)\log m), O(\log m))$-competitive randomized onlinealgorithm for this problem, i.e. the schedule produced by our algorithm has amakespan of $O({\bf L} \log m)$ with high probability, and a total expectedstartup cost of $O(\log (mn)\log m)$ times that of an optimal offline schedulewith makespan $\bf L$. The competitive ratios of our algorithm are (almost)optimal.  Our algorithms use the online primal dual framework introduced by Alon {\emet al} for the online set cover problem, and subsequently developed further byBuchbinder, Naor, and co-authors. To the best of our knowledge, all previousapplications of this framework have been to linear programs (LPs) with eitherpacking or covering constraints. One novelty of our application is that we usethis framework for a mixed LP that has both covering and packing constraints.We hope that the algorithmic techniques developed in this paper tosimultaneously handle packing and covering constraints will be useful forsolving other online optimization problems as well.

Provenance Views for Module Privacy

  Scientific workflow systems increasingly store provenance information aboutthe module executions used to produce a data item, as well as the parametersettings and intermediate data items passed between module executions. However,authors/owners of workflows may wish to keep some of this informationconfidential. In particular, a module may be proprietary, and users should notbe able to infer its behavior by seeing mappings between all data inputs andoutputs. The problem we address in this paper is the following: Given aworkflow, abstractly modeled by a relation R, a privacy requirement \Gamma andcosts associated with data. The owner of the workflow decides which data(attributes) to hide, and provides the user with a view R' which is theprojection of R over attributes which have not been hidden. The goal is tominimize the cost of hidden data while guaranteeing that individual modules are\Gamma -private. We call this the "secureview" problem. We formally define theproblem, study its complexity, and offer algorithmic solutions.

Precedence-constrained Scheduling of Malleable Jobs with Preemption

  Scheduling jobs with precedence constraints on a set of identical machines tominimize the total processing time (makespan) is a fundamental problem incombinatorial optimization. In practical settings such as cloud computing, jobsare often malleable, i.e., can be processed on multiple machinessimultaneously. The instantaneous processing rate of a job is a non-decreasingfunction of the number of machines assigned to it (we call it the processingfunction). Previous research has focused on practically relevant concaveprocessing functions, which obey the law of diminishing utility and generalizethe classical (non-malleable) problem. Our main result is a$(2+\epsilon)$-approximation algorithm for concave processing functions (forany $\epsilon > 0$), which is the best possible under complexity theoreticassumptions. The approximation ratio improves to $(1 + \epsilon)$ for theinteresting and practically relevant special case of power functions, i.e.,$p_j(z) = c_j \cdot z^{\gamma}$.

Online Buy-at-Bulk Network Design

  We present the first non-trivial online algorithms for the non-uniform,multicommodity buy-at-bulk (MC-BB) network design problem in undirected anddirected graphs. Our competitive ratios qualitatively match the best knownapproximation factors for the corresponding offline problems. The main enginefor our results is an online reduction theorem of MC-BB problems to theirsingle-sink (SS-BB) counterparts. We use the concept of junction-tree solutions(Chekuri et al., FOCS 2006) that play an important role in solving the offlineversions of the problem via a greedy subroutine -- an inherently offlineprocedure. Our main technical contribution is in designing an online algorithmusing only the existence of good junction-trees to reduce an MC-BB instance tomultiple SS-BB sub-instances. Along the way, we also give the first non-trivialonline node-weighted/directed single-sink buy-at-bulk algorithms. In additionto the new results, our generic reduction also yields new proofs of recentresults for the online node-weighted Steiner forest and online group Steinerforest problems.

Minimizing Latency in Online Ride and Delivery Services

  Motivated by the popularity of online ride and delivery services, we studynatural variants of classical multi-vehicle minimum latency problems where theobjective is to route a set of vehicles located at depots to serve requestlocated on a metric space so as to minimize the total latency. In this paper,we consider point-to-point requests that come with source-destination pairs andrelease-time constraints that restrict when each request can be served. Thepoint-to-point requests and release-time constraints model taxi rides anddeliveries. For all the variants considered, we show constant-factorapproximation algorithms based on a linear programming framework. To the bestof our knowledge, these are the first set of results for the aforementionedvariants of the minimum latency problems. Furthermore, we provide an empiricalstudy of heuristics based on our theoretical algorithms on a real data set oftaxi rides.

Online Algorithms for Machine Minimization

  In this paper, we consider the online version of the machine minimizationproblem (introduced by Chuzhoy et al., FOCS 2004), where the goal is toschedule a set of jobs with release times, deadlines, and processing lengths ona minimum number of identical machines. Since the online problem has stronglower bounds if all the job parameters are arbitrary, we focus on jobs withuniform length. Our main result is a complete resolution of the deterministiccomplexity of this problem by showing that a competitive ratio of $e$ isachievable and optimal, thereby improving upon existing lower and upper boundsof 2.09 and 5.2 respectively. We also give a constant-competitive onlinealgorithm for the case of uniform deadlines (but arbitrary job lengths); to thebest of our knowledge, no such algorithm was known previously. Finally, weconsider the complimentary problem of throughput maximization where the goal isto maximize the sum of weights of scheduled jobs on a fixed set of identicalmachines (introduced by Bar-Noy et al. STOC 1999). We give a randomized onlinealgorithm for this problem with a competitive ratio of e/e-1; previous resultsachieved this bound only for the case of a single machine or in the limit of aninfinite number of machines.

Tight Bounds for Online Vector Scheduling

  Modern data centers face a key challenge of effectively serving user requeststhat arrive online. Such requests are inherently multi-dimensional andcharacterized by demand vectors over multiple resources such as processorcycles, storage space, and network bandwidth. Typically, different resourcesrequire different objectives to be optimized, and $L_r$ norms of loads areamong the most popular objectives considered. To address these problems, weconsider the online vector scheduling problem in this paper. Introduced byChekuri and Khanna (SIAM J of Comp. 2006), vector scheduling is ageneralization of classical load balancing, where every job has a vector loadinstead of a scalar load. In this paper, we resolve the online complexity ofthe vector scheduling problem and its important generalizations. Our mainresults are:  -For identical machines, we show that the optimal competitive ratio is$\Theta(\log d / \log \log d)$ by giving an online lower bound and an algorithmwith an asymptotically matching competitive ratio. The lower bound istechnically challenging, and is obtained via an online lower bound for theminimum mono-chromatic clique problem using a novel online coloring game andrandomized coding scheme.  -For unrelated machines, we show that the optimal competitive ratio is$\Theta(\log m + \log d)$ by giving an online lower bound that matches apreviously known upper bound. Unlike identical machines, however, extendingthese results, particularly the upper bound, to general $L_r$ norms requiresnew ideas. In particular, we use a carefully constructed potential functionthat balances the individual $L_r$ objectives with the overall (convexified)min-max objective to guide the online algorithm and track the changes inpotential to bound the competitive ratio.

Online Budgeted Allocation with General Budgets

  We study the online budgeted allocation (also called ADWORDS) problem, wherea set of impressions arriving online are allocated to a set ofbudget-constrained advertisers to maximize revenue. Motivated by connections toInternet advertising, several variants of this problem have been studied sincethe seminal work of Mehta, Saberi, Vazirani, and Vazirani (FOCS 2005). However,this entire body of work focuses on a single budget for every advertisingcampaign, whereas in order to fully represent the actual agenda of anadvertiser, an advertising budget should be expressible over multiple tiers ofuser-attribute granularity. A simple example is an advertising campaign that isconstrained by an overall budget but is also accompanied by a set ofsub-budgets for each target demographic. In such a contract scheme, anadvertiser can specify their true user-targeting goals, allowing the publisherto fulfill them through relevant allocations.  In this paper, we give a complete characterization of the ADWORDS problem forgeneral advertising budgets. In the most general setting, we show that, unlikein the single-budget ADWORDS problem, obtaining a constant competitive ratio isimpossible and give asymptotically tight upper and lower bounds. However forour main result, we observe that in many real-world scenarios (as in the aboveexample), multi-tier budgets have a laminar structure, since most relevantconsumer or product classifications are hierarchical. For laminar budgets, weobtain a competitive ratio of e/(e-1) in the small bids case, which matches thebest known ADWORDS result for single budgets. Our algorithm has a primal-dualstructure and generalizes the primal-dual analysis for single- budget ADWORDSfirst given by Buchbinder, Jain, and Naor (ESA 2007).

On the Price of Stability of Undirected Multicast Games

  In multicast network design games, a set of agents choose paths from theirsource locations to a common sink with the goal of minimizing their individualcosts, where the cost of an edge is divided equally among the agents using it.Since the work of Anshelevich et al. (FOCS 2004) that introduced network designgames, the main open problem in this field has been the price of stability(PoS) of multicast games. For the special case of broadcast games (every vertexis a terminal, i.e., has an agent), a series of works has culminated in aconstant upper bound on the PoS (Bilo` et al., FOCS 2013). However, nosignificantly sub-logarithmic bound is known for multicast games. In thispaper, we make progress toward resolving this question by showing a constantupper bound on the PoS of multicast games for quasi-bipartite graphs. These aregraphs where all edges are between two terminals (as in broadcast games) orbetween a terminal and a nonterminal, but there is no edge betweennonterminals. This represents a natural class of intermediate generalitybetween broadcast and multicast games. In addition to the result itself, ourtechniques overcome some of the fundamental difficulties of analyzing the PoSof general multicast games, and are a promising step toward resolving thismajor open problem.

Online and Dynamic Algorithms for Set Cover

  In this paper, we study the set cover problem in the fully dynamic model. Inthis model, the set of active elements, i.e., those that must be covered at anygiven time, can change due to element arrivals and departures. The goal is tomaintain an algorithmic solution that is competitive with respect to thecurrent optimal solution. This model is popular in both the dynamic algorithmsand online algorithms communities. The difference is in the restriction placedon the algorithm: in dynamic algorithms, the running time of the algorithmmaking updates (called update time) is bounded, while in online algorithms, thenumber of updates made to the solution (called recourse) is limited.  In this paper we show the following results: In the update time setting, weobtain O(log n)-competitiveness with O(f log n) amortized update time, andO(f^3)-competitiveness with O(f^2) update time. The O(log n)-competitivealgorithm is the first one to achieve a competitive ratio independent of f inthis setting. In the recourse setting, we show a competitive ratio of O(min{logn,f}) with constant amortized recourse. Note that this matches the best offlinebounds with just constant recourse, something that is impossible in theclassical online model.  Our results are based on two algorithmic frameworks in the fully-dynamicmodel that are inspired by the classic greedy and primal-dual algorithms foroffline set cover. We show that both frameworks can be used for obtaining bothrecourse and update time bounds, thereby demonstrating algorithmic techniquescommon to these strands of research.

Online Load Balancing for Related Machines

  In the load balancing problem, introduced by Graham in the 1960s (SIAM J. ofAppl. Math. 1966, 1969), jobs arriving online have to be assigned to machinesso to minimize an objective defined on machine loads. A long line of work hasaddressed this problem for both the makespan norm and arbitrary $\ell_q$-normsof machine loads. Recent literature (e.g., Azar et al., STOC 2013; Im et al.,FOCS 2015) has further expanded the scope of this problem to vector loads, tocapture jobs with multi-dimensional resource requirements in applications suchas data centers. In this paper, we completely resolve the job schedulingproblem for both scalar and vector jobs on related machines, i.e., where eachmachine has a given speed and the time taken to process a job is inverselyproportional to the speed of the machine it is assigned on.  We show the following results. For scalar scheduling, we give a constantcompetitive algorithm for optimizing any $\ell_q$-norm for related machines.The only previously known result was for the makespan norm. For vectorscheduling, there are two natural variants for vector scheduling, depending onwhether the speed of a machine is dimension-dependent or not. We show a sharpcontrast between these two variants, proving that they are respectivelyequivalent to unrelated machines and identical machines for the makespan norm.We also extend these results to arbitrary $\ell_q$-norms of the machine loads.No previous results were known for vector scheduling on related machines.

Pacing Equilibrium in First-Price Auction Markets

  In ad auctions--the prevalent monetization mechanism of Internetcompanies--advertisers compete for online impressions in a sequential auctionmarket. Since advertisers are typically budget-constrained, a common toolemployed to improve their ROI is that of pacing, i.e., uniform scaling of theirbids to preserve their budget for a longer duration. If the advertisers areexcessively paced, they end up not spending their budget, while if they are notsufficiently paced, they use up their budget too soon. Therefore, it isimportant that they are paced at just the right amount, a solution concept thatwe call a pacing equilibrium. In this paper, we study pacing equilibria in thecontext of first-price auctions, which are popular in the theory of admechanisms. We show existence, uniqueness, and efficient computability offirst-price pacing equilibria (FPPE), while also establishing several othersalient features of this solution concept. In the process, we uncover a sharpcontrast between these solutions and second price pacing equilibria (SPPE), thelatter being known to produce non-unique, fragile solutions that are alsocomputationally hard to obtain. Simulations show that FPPE have better revenueproperties than SPPE, that bidders have lower ex-post regret, and thatincentives to misreport budgets for thick markets are smaller.

Faster Algorithms for the Geometric Transportation Problem

  Let $R$ and $B$ be two point sets in $\mathbb{R}^d$, with $|R|+ |B| = n$ andwhere $d$ is a constant. Next, let $\lambda : R \cup B \to \mathbb{N}$ suchthat $\sum_{r \in R } \lambda(r) = \sum_{b \in B} \lambda(b)$ be demandfunctions over $R$ and $B$. Let $\|\cdot\|$ be a suitable distance functionsuch as the $L_p$ distance. The transportation problem asks to find a map $\tau: R \times B \to \mathbb{N}$ such that $\sum_{b \in B}\tau(r,b) = \lambda(r)$,$\sum_{r \in R}\tau(r,b) = \lambda(b)$, and $\sum_{r \in R, b \in B} \tau(r,b)\|r-b\|$ is minimized. We present three new results for the transportationproblem when $\|r-b\|$ is any $L_p$ metric:  - For any constant $\varepsilon > 0$, an $O(n^{1+\varepsilon})$ expected timerandomized algorithm that returns a transportation map with expected cost$O(\log^2(1/\varepsilon))$ times the optimal cost.  - For any $\varepsilon > 0$, a $(1+\varepsilon)$-approximation in$O(n^{3/2}\varepsilon^{-d} \operatorname{polylog}(U)\operatorname{polylog}(n))$ time, where $U = \max_{p\in R\cup B} \lambda(p)$.  - An exact strongly polynomial $O(n^2 \operatorname{polylog}n)$ timealgorithm, for $d = 2$.

Online Covering with Convex Objectives and Applications

  We give an algorithmic framework for minimizing general convex objectives(that are differentiable and monotone non-decreasing) over a set of coveringconstraints that arrive online. This substantially extends previous work ononline covering for linear objectives (Alon {\em et al.}, STOC 2003) and onlinecovering with offline packing constraints (Azar {\em et al.}, SODA 2013). Tothe best of our knowledge, this is the first result in online optimization forgeneric non-linear objectives; special cases of such objectives have previouslybeen considered, particularly for energy minimization.  As a specific problem in this genre, we consider the unrelated machinescheduling problem with startup costs and arbitrary $\ell_p$ norms on machineloads (including the surprisingly non-trivial $\ell_1$ norm representing totalmachine load). This problem was studied earlier for the makespan norm in boththe offline (Khuller~{\em et al.}, SODA 2010; Li and Khuller, SODA 2011) andonline settings (Azar {\em et al.}, SODA 2013). We adapt the two-phase approachof obtaining a fractional solution and then rounding it online (usedsuccessfully to many linear objectives) to the non-linear objective. Thefractional algorithm uses ideas from our general framework that we describedabove (but does not fit the framework exactly because of non-positive entriesin the constraint matrix). The rounding algorithm uses ideas from offlinerounding of LPs with non-linear objectives (Azar and Epstein, STOC 2005; Kumar{\em et al.}, FOCS 2005). Our competitive ratio is tight up to a logarithmicfactor. Finally, for the important special case of total load ($\ell_1$ norm),we give a different rounding algorithm that obtains a better competitive ratiothan the generic rounding algorithm for $\ell_p$ norms. We show that thiscompetitive ratio is asymptotically tight.

Timing Matters: Online Dynamics in Broadcast Games

  A central question in algorithmic game theory is to measure the inefficiency(ratio of costs) of Nash equilibria (NE) with respect to socially optimalsolutions. The two established metrics used for this purpose are price ofanarchy (POA) and price of stability (POS), which respectively provide upperand lower bounds on this ratio. A deficiency of these metrics, however, is thatthey are purely existential and shed no light on which of the equilibriumstates are reachable in an actual game, i.e., via natural game dynamics. Thisis particularly striking if these metrics differ significantly, such as innetwork design games where the exponential gap between the best and worst NEstates originally prompted the notion of POS in game theory (Anshelevich etal., FOCS 2002). In this paper, we make progress toward bridging this gap bystudying network design games under natural game dynamics.  First we show that in a completely decentralized setting, where agentsarrive, depart, and make improving moves in an arbitrary order, theinefficiency of NE attained can be polynomially large. This implies that thegame designer must have some control over the interleaving of these events inorder to force the game to attain efficient NE. We complement our negativeresult by showing that if the game designer is allowed to execute a sequence ofimproving moves to create an equilibrium state after every batch of agentarrivals or departures, then the resulting equilibrium states attained by thegame are exponentially more efficient, i.e., the ratio of costs compared to theoptimum is only logarithmic. Overall, our two results establish that in networkgames, the efficiency of equilibrium states is dictated by whether agents areallowed to join or leave the game in arbitrary states, an observation thatmight be useful in analyzing the dynamics of other classes of games withdivergent POS and POA bounds.

Dynamic Set Cover: Improved Algorithms & Lower Bounds

  Set cover is a classic problem in combinatorial optimization where a set of$n$ elements have to be covered by a minimum number of subsets from a givencollection of size $m$. The two traditional lines of inquiry for this problemare via greedy and primal dual algorithms, and respectively yield (tight)approximation factors of $\ln n$, where $n$ is the total number of elements,and $f$, where every element belongs to at most $f$ sets. Recent research hasfocused on the dynamic setting, where the set of elements changes over time.Using the same lines of inquiry, this has led to the following results: (a) an$O(\log n)$-approximation in $O(f \log n)$ (amortized) update time (Gupta {\emet al.}, STOC 2017), and (b) an $O(f^2)$-approximation in $O(f \log (m+n))$(amortized) update time (Bhattacharya {\em et al.}, ICALP 2015). While theformer result matches the offline approximation within a constant factor, thelatter does not; indeed, the only $O(f)$-approximation known in the dynamicsetting is by re-solving the problem after every update.  In this paper, we show that it is possible to maintain efficiently a solution(almost) as good as the primal-dual offline one: we give a $(1+\epsilon)f$-approximation for set cover in $O(f^2\log n/\epsilon)$ (amortized) updatetime. If we are in a decremental setting, i.e., there are element deletions butno insertions, the update time can be improved to $O(f^2/\epsilon)$, whilestill obtaining an $(1+\epsilon) f$-approximation. Finally, we study thedependence of the update time on $f$. Using the recent distributedPCP-framework, we show that any dynamic algorithm for set cover that has anamortized update time of $O(f^{1-\epsilon})$ must have an approximation factorthat is $\Omega(n^\delta)$ for some $\delta>0$ under the Strong ExponentialTime Hypothesis.

