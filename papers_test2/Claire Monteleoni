Convergence rate of stochastic k-means

  We analyze online and mini-batch k-means variants. Both scale up the widelyused Lloyd 's algorithm via stochastic approximation, and have become popularfor large-scale clustering and unsupervised feature learning. We show, for thefirst time, that they have global convergence towards local optima at$O(\frac{1}{t})$ rate under general conditions. In addition, we show if thedataset is clusterable, with suitable initialization, mini-batch k-meansconverges to an optimal k-means solution with $O(\frac{1}{t})$ convergence ratewith high probability. The k-means objective is non-convex andnon-differentiable: we exploit ideas from non-convex gradient-basedoptimization by providing a novel characterization of the trajectory of k-meansalgorithm on its solution space, and circumvent its non-differentiability viageometric insights about k-means update.

Convergence rate of stochastic k-means

  We analyze online \cite{BottouBengio} and mini-batch \cite{Sculley} $k$-meansvariants. Both scale up the widely used $k$-means algorithm via stochasticapproximation, and have become popular for large-scale clustering andunsupervised feature learning. We show, for the first time, that starting withany initial solution, they converge to a "local optimum" at rate$O(\frac{1}{t})$ (in terms of the $k$-means objective) under generalconditions. In addition, we show if the dataset is clusterable, wheninitialized with a simple and scalable seeding algorithm, mini-batch $k$-meansconverges to an optimal $k$-means solution at rate $O(\frac{1}{t})$ with highprobability. The $k$-means objective is non-convex and non-differentiable: weexploit ideas from recent work on stochastic gradient descent for non-convexproblems \cite{ge:sgd_tensor, balsubramani13} by providing a novelcharacterization of the trajectory of $k$-means algorithm on its solutionspace, and circumvent the non-differentiability problem via geometric insightsabout $k$-means update.

Cooperative Online Learning: Keeping your Neighbors Updated

  We study an asynchronous online learning setting with a network of agents. Ateach time step, some of the agents are activated, requested to make aprediction, and pay the corresponding loss. The loss function is then revealedto these agents and also to their neighbors in the network. When activationsare stochastic, we show that the regret achieved by $N$ agents running thestandard online Mirror Descent is $O(\sqrt{\alpha T})$, where $T$ is thehorizon and $\alpha \le N$ is the independence number of the network. This isin contrast to the regret $\Omega(\sqrt{N T})$ which $N$ agents incur in thesame setting when feedback is not shared. We also show a matching lower boundof order $\sqrt{\alpha T}$ that holds for any given network. When the patternof agent activations is arbitrary, the problem changes significantly: we provea $\Omega(T)$ lower bound on the regret that holds for any online algorithmoblivious to the feedback source.

Differentially Private Empirical Risk Minimization

  Privacy-preserving machine learning algorithms are crucial for theincreasingly common setting in which personal data, such as medical orfinancial records, are analyzed. We provide general techniques to produceprivacy-preserving approximations of classifiers learned via (regularized)empirical risk minimization (ERM). These algorithms are private under the$\epsilon$-differential privacy definition due to Dwork et al. (2006). First weapply the output perturbation ideas of Dwork et al. (2006), to ERMclassification. Then we propose a new method, objective perturbation, forprivacy-preserving machine learning algorithm design. This method entailsperturbing the objective function before optimizing over classifiers. If theloss and regularizer satisfy certain convexity and differentiability criteria,we prove theoretical results showing that our algorithms preserve privacy, andprovide generalization bounds for linear and nonlinear kernels. We furtherpresent a privacy-preserving technique for tuning the parameters in generalmachine learning algorithms, thereby providing end-to-end privacy guaranteesfor the training process. We apply these results to produce privacy-preservinganalogues of regularized logistic regression and support vector machines. Weobtain encouraging results from evaluating their performance on realdemographic and benchmark data sets. Our results show that both theoreticallyand empirically, objective perturbation is superior to the previousstate-of-the-art, output perturbation, in managing the inherent tradeoffbetween privacy and learning performance.

