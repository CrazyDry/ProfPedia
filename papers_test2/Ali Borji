Vanishing point detection with convolutional neural networks

  Inspired by the finding that vanishing point (road tangent) guides driver'sgaze, in our previous work we showed that vanishing point attracts gaze duringfree viewing of natural scenes as well as in visual search (Borji et al.,Journal of Vision 2016). We have also introduced improved saliency models usingvanishing point detectors (Feng et al., WACV 2016). Here, we aim to predictvanishing points in naturalistic environments by training convolutional neuralnetworks in an end-to-end manner over a large set of road images downloadedfrom Youtube with vanishing points annotated. Results demonstrate effectivenessof our approach compared to classic approaches of vanishing point detection inthe literature.

Vanishing Point Attracts Eye Movements in Scene Free-viewing

  Eye movements are crucial in understanding complex scenes. By predictingwhere humans look in natural scenes, we can understand how they percieve scenesand priotriaze information for further high-level processing. Here, we studythe effect of a particular type of scene structural information known asvanishing point and show that human gaze is attracted to vanishing pointregions. We then build a combined model of traditional saliency and vanishingpoint channel that outperforms state of the art saliency models.

Computational models of attention

  This chapter reviews recent computational models of visual attention. Webegin with models for the bottom-up or stimulus-driven guidance of attention tosalient visual items, which we examine in seven different broad categories. Wethen examine more complex models which address the top-down or goal-orientedguidance of attention towards items that are more relevant to the task at hand.

Bottom-up Attention, Models of

  In this review, we examine the recent progress in saliency prediction andproposed several avenues for future research. In spite of tremendous effortsand huge progress, there is still room for improvement in terms finer-grainedanalysis of deep saliency models, evaluation measures, datasets, annotationmethods, cognitive studies, and new applications. This chapter will appear inEncyclopedia of Computational Neuroscience.

Salient Object Detection: A Survey

  Detecting and segmenting salient objects in natural scenes, often referred toas salient object detection, has attracted a lot of interest in computervision. While many models have been proposed and several applications haveemerged, yet a deep understanding of achievements and issues is lacking. We aimto provide a comprehensive review of the recent progress in salient objectdetection and situate this field among other closely related areas such asgeneric scene segmentation, object proposal generation, and saliency forfixation prediction. Covering 228 publications, we survey i) roots, keyconcepts, and tasks, ii) core techniques and main modeling trends, and iii)datasets and evaluation metrics in salient object detection. We also discussopen problems such as evaluation metrics and dataset bias in model performanceand suggest future research directions.

Reconciling saliency and object center-bias hypotheses in explaining  free-viewing fixations

  Predicting where people look in natural scenes has attracted a lot ofinterest in computer vision and computational neuroscience over the past twodecades. Two seemingly contrasting categories of cues have been proposed toinfluence where people look: \textit{low-level image saliency} and\textit{high-level semantic information}. Our first contribution is to take adetailed look at these cues to confirm the hypothesis proposed byHenderson~\cite{henderson1993eye} and Nuthmann \&Henderson~\cite{nuthmann2010object} that observers tend to look at the centerof objects. We analyzed fixation data for scene free-viewing over 17 observerson 60 fully annotated images with various types of objects. Images containeddifferent types of scenes, such as natural scenes, line drawings, and 3Drendered scenes. Our second contribution is to propose a simple combined modelof low-level saliency and object center-bias that outperforms each individualcomponent significantly over our data, as well as on the OSIE dataset by Xu etal.~\cite{xu2014predicting}. The results reconcile saliency with objectcenter-bias hypotheses and highlight that both types of cues are important inguiding fixations. Our work opens new directions to understand strategies thathumans use in observing scenes and objects, and demonstrates the constructionof combined models of low-level saliency and high-level object-basedinformation.

CAT2000: A Large Scale Fixation Dataset for Boosting Saliency Research

  Saliency modeling has been an active research area in computer vision forabout two decades. Existing state of the art models perform very well inpredicting where people look in natural scenes. There is, however, the riskthat these models may have been overfitting themselves to available small scalebiased datasets, thus trapping the progress in a local minimum. To gain adeeper insight regarding current issues in saliency modeling and to bettergauge progress, we recorded eye movements of 120 observers while they freelyviewed a large number of naturalistic and artificial images. Our stimuliincludes 4000 images; 200 from each of 20 categories covering different typesof scenes such as Cartoons, Art, Objects, Low resolution images, Indoor,Outdoor, Jumbled, Random, and Line drawings. We analyze some basic propertiesof this dataset and compare some successful models. We believe that our datasetopens new challenges for the next generation of saliency models and helpsconduct behavioral studies on bottom-up visual attention.

Computational models: Bottom-up and top-down aspects

  Computational models of visual attention have become popular over the pastdecade, we believe primarily for two reasons: First, models make testablepredictions that can be explored by experimentalists as well as theoreticians,second, models have practical and technological applications of interest to theapplied science and engineering communities. In this chapter, we take acritical look at recent attention modeling efforts. We focus on {\emcomputational models of attention} as defined by Tsotsos \& Rothenstein\shortcite{Tsotsos_Rothenstein11}: Models which can process any visual stimulus(typically, an image or video clip), which can possibly also be given some taskdefinition, and which make predictions that can be compared to human or animalbehavioral or physiological responses elicited by the same stimulus and task.Thus, we here place less emphasis on abstract models, phenomenological models,purely data-driven fitting or extrapolation models, or models specificallydesigned for a single task or for a restricted class of stimuli. Fortheoretical models, we refer the reader to a number of previous reviews thataddress attention theories and models more generally\cite{Itti_Koch01nrn,Paletta_etal05,Frintrop_etal10,Rothenstein_Tsotsos08,Gottlieb_Balan10,Toet11,Borji_Itti12pami}.

Fixation prediction with a combined model of bottom-up saliency and  vanishing point

  By predicting where humans look in natural scenes, we can understand how theyperceive complex natural scenes and prioritize information for furtherhigh-level visual processing. Several models have been proposed for thispurpose, yet there is a gap between best existing saliency models and humanperformance. While many researchers have developed purely computational modelsfor fixation prediction, less attempts have been made to discover cognitivefactors that guide gaze. Here, we study the effect of a particular type ofscene structural information, known as the vanishing point, and show that humangaze is attracted to the vanishing point regions. We record eye movements of 10observers over 532 images, out of which 319 have vanishing points. We thenconstruct a combined model of traditional saliency and a vanishing pointchannel and show that our model outperforms state of the art saliency modelsusing three scores on our dataset.

Negative Results in Computer Vision: A Perspective

  A negative result is when the outcome of an experiment or a model is not whatis expected or when a hypothesis does not hold. Despite being often overlookedin the scientific community, negative results are results and they carry value.While this topic has been extensively discussed in other fields such as socialsciences and biosciences, less attention has been paid to it in the computervision community. The unique characteristics of computer vision, particularlyits experimental aspect, call for a special treatment of this matter. In thispaper, I will address what makes negative results important, how they should bedisseminated and incentivized, and what lessons can be learned from cognitivevision research in this regard. Further, I will discuss issues such as computervision and human vision interaction, experimental design and statisticalhypothesis testing, explanatory versus predictive modeling, performanceevaluation, model comparison, as well as computer vision research culture.

Pros and Cons of GAN Evaluation Measures

  Generative models, in particular generative adversarial networks (GANs), havereceived significant attention recently. A number of GAN variants have beenproposed and have been utilized in many applications. Despite large strides interms of theoretical progress, evaluating and comparing GANs remains a dauntingtask. While several measures have been introduced, as of yet, there is noconsensus as to which measure best captures strengths and limitations of modelsand should be used for fair model comparison. As in other areas of computervision and machine learning, it is critical to settle on one or few goodmeasures to steer the progress in this field. In this paper, I review andcritically discuss more than 24 quantitative and 5 qualitative measures forevaluating generative models with a particular emphasis on GAN-derived models.I also provide a set of 7 desiderata followed by an evaluation of whether agiven measure or a family of measures is compatible with them.

Saliency Prediction in the Deep Learning Era: An Empirical Investigation

  Visual saliency models have enjoyed a big leap in performance in recentyears, thanks to advances in deep learning and large scale annotated data.Despite enormous effort and huge breakthroughs, however, models still fallshort in reaching human-level accuracy. In this work, I explore the landscapeof the field emphasizing on new deep saliency models, benchmarks, and datasets.A large number of image and video saliency models are reviewed and comparedover two image benchmarks and two large scale video datasets. Further, Iidentify factors that contribute to the gap between models and humans anddiscuss remaining issues that need to be addressed to build the next generationof more powerful saliency models. Some specific questions that are addressedinclude: in what ways current models fail, how to remedy them, what can belearned from cognitive studies of attention, how explicit saliency judgmentsrelate to fixations, how to conduct fair model comparison, and what are theemerging applications of saliency models.

What are the Receptive, Effective Receptive, and Projective Fields of  Neurons in Convolutional Neural Networks?

  In this work, we explain in detail how receptive fields, effective receptivefields, and projective fields of neurons in different layers, convolution orpooling, of a Convolutional Neural Network (CNN) are calculated. While ourfocus here is on CNNs, the same operations, but in the reverse order, can beused to calculate these quantities for deconvolutional neural networks. Theseare important concepts, not only for better understanding and analyzingconvolutional and deconvolutional networks, but also for optimizing theirperformance in real-world applications.

Vanishing point attracts gaze in free-viewing and visual search tasks

  To investigate whether the vanishing point (VP) plays a significant role ingaze guidance, we ran two experiments. In the first one, we recorded fixationsof 10 observers (4 female; mean age 22; SD=0.84) freely viewing 532 images, outof which 319 had VP (shuffled presentation; each image for 4 secs). We foundthat the average number of fixations at a local region (80x80 pixels) centeredat the VP is significantly higher than the average fixations at randomlocations (t-test; n=319; p=1.8e-35). To address the confounding factor ofsaliency, we learned a combined model of bottom-up saliency and VP. AUC scoreof our model (0.85; SD=0.01) is significantly higher than the original saliencymodel (e.g., 0.8 using AIM model by Bruce & Tsotsos (2009), t-test; p=3.14e-16) and the VP-only model (0.64, t-test; p= 4.02e-22). In the secondexperiment, we asked 14 subjects (4 female, mean age 23.07, SD=1.26) to searchfor a target character (T or L) placed randomly on a 3x3 imaginary gridoverlaid on top of an image. Subjects reported their answers by pressing one oftwo keys. Stimuli consisted of 270 color images (180 with a single VP, 90without). The target happened with equal probability inside each cell (15 timesL, 15 times T). We found that subjects were significantly faster (and moreaccurate) when target happened inside the cell containing the VP compared tocells without VP (median across 14 subjects 1.34 sec vs. 1.96; Wilcoxonrank-sum test; p = 0.0014). Response time at VP cells were also significantlylower than response time on images without VP (median 2.37; p= 4.77e-05). Thesefindings support the hypothesis that vanishing point, similar to face and text(Cerf et al., 2009) as well as gaze direction (Borji et al., 2014) attractsattention in free-viewing and visual search.

What can we learn about CNNs from a large scale controlled object  dataset?

  Tolerance to image variations (e.g. translation, scale, pose, illumination)is an important desired property of any object recognition system, be it humanor machine. Moving towards increasingly bigger datasets has been trending incomputer vision specially with the emergence of highly popular deep learningmodels. While being very useful for learning invariance to object inter- andintra-class shape variability, these large-scale wild datasets are not veryuseful for learning invariance to other parameters forcing researchers toresort to other tricks for training a model. In this work, we introduce alarge-scale synthetic dataset, which is freely and publicly available, and useit to answer several fundamental questions regarding invariance and selectivityproperties of convolutional neural networks. Our dataset contains two parts: a)objects shot on a turntable: 16 categories, 8 rotation angles, 11 cameras on asemicircular arch, 5 lighting conditions, 3 focus levels, variety ofbackgrounds (23.4 per instance) generating 1320 images per instance (over 20million images in total), and b) scenes: in which a robot arm takes pictures ofobjects on a 1:160 scale scene. We study: 1) invariance and selectivity ofdifferent CNN layers, 2) knowledge transfer from one object category toanother, 3) systematic or random sampling of images to build a train set, 4)domain adaptation from synthetic to natural scenes, and 5) order of knowledgedelivery to CNNs. We also explore how our analyses can lead the field todevelop more efficient CNNs.

Human-like Clustering with Deep Convolutional Neural Networks

  Classification and clustering have been studied separately in machinelearning and computer vision. Inspired by the recent success of deep learningmodels in solving various vision problems (e.g., object recognition, semanticsegmentation) and the fact that humans serve as the gold standard in assessingclustering algorithms, here, we advocate for a unified treatment of the twoproblems and suggest that hierarchical frameworks that progressively buildcomplex patterns on top of the simpler ones (e.g., convolutional neuralnetworks) offer a promising solution. We do not dwell much on the learningmechanisms in these frameworks as they are still a matter of debate, withrespect to biological constraints. Instead, we emphasize on thecompositionality of the real world structures and objects. In particular, weshow that CNNs, trained end to end using back propagation with noisy labels,are able to cluster data points belonging to several overlapping shapes, and doso much better than the state of the art algorithms. The main takeaway lessonfrom our study is that mechanisms of human vision, particularly the hierarchalorganization of the visual ventral stream should be taken into account inclustering algorithms (e.g., for learning representations in an unsupervisedmanner or with minimum supervision) to reach human level clusteringperformance. This, by no means, suggests that other methods do not hold merits.For example, methods relying on pairwise affinities (e.g., spectral clustering)have been very successful in many scenarios but still fail in some cases (e.g.,overlapping clusters).

Deeply supervised salient object detection with short connections

  Recent progress on saliency detection is substantial, benefiting mostly fromthe explosive development of Convolutional Neural Networks (CNNs). Semanticsegmentation and saliency detection algorithms developed lately have beenmostly based on Fully Convolutional Neural Networks (FCNs). There is still alarge room for improvement over the generic FCN models that do not explicitlydeal with the scale-space problem. Holistically-Nested Edge Detector (HED)provides a skip-layer structure with deep supervision for edge and boundarydetection, but the performance gain of HED on salience detection is notobvious. In this paper, we propose a new method for saliency detection byintroducing short connections to the skip-layer structures within the HEDarchitecture. Our framework provides rich multi-scale feature maps at eachlayer, a property that is critically needed to perform segment detection. Ourmethod produces state-of-the-art results on 5 widely tested salient objectdetection benchmarks, with advantages in terms of efficiency (0.15 seconds perimage), effectiveness, and simplicity over the existing algorithms.

Protecting entanglement by adjusting the velocities of moving qubits  inside non-Markovian environments

  Efficient entanglement preservation in open quantum systems is a crucialscope towards a reliable exploitation of quantum resources. We address thisissue by studying how two-qubit entanglement dynamically behaves when two atomqubits move inside two separated identical cavities. The moving qubitsindependently interact with their respective cavity. As a main general result,we find that under resonant qubit-cavity interaction the initial entanglementbetween two moving qubits remains closer to its initial value as time passescompared to the case of stationary qubits. In particular, we show that theinitial entanglement can be strongly protected from decay by suitably adjustingthe velocities of the qubits according to the non-Markovian features of thecavities. Our results supply a further way of preserving quantum correlationsagainst noise with a natural implementation in cavity-QED scenarios and arestraightforwardly extendable to many qubits for scalability.

What is a salient object? A dataset and a baseline model for salient  object detection

  Salient object detection or salient region detection models, diverging fromfixation prediction models, have traditionally been dealing with locating andsegmenting the most salient object or region in a scene. While the notion ofmost salient object is sensible when multiple objects exist in a scene, currentdatasets for evaluation of saliency detection approaches often have scenes withonly one single object. We introduce three main contributions in this paper:First, we take an indepth look at the problem of salient object detection bystudying the relationship between where people look in scenes and what theychoose as the most salient object when they are explicitly asked. Based on theagreement between fixations and saliency judgments, we then suggest that themost salient object is the one that attracts the highest fraction of fixations.Second, we provide two new less biased benchmark datasets containing sceneswith multiple objects that challenge existing saliency models. Indeed, weobserved a severe drop in performance of 8 state-of-the-art models on ourdatasets (40% to 70%). Third, we propose a very simple yet powerful model basedon superpixels to be used as a baseline for model evaluation and comparison.While on par with the best models on MSRA-5K dataset, our model wins over othermodels on our data highlighting a serious drawback of existing models, which isconvoluting the processes of locating the most salient object and itssegmentation. We also provide a review and statistical analysis of some labeledscene datasets that can be used for evaluating salient object detection models.We believe that our work can greatly help remedy the over-fitting of models toexisting biased datasets and opens new venues for future research in thisfast-evolving field.

Exploiting inter-image similarity and ensemble of extreme learners for  fixation prediction using deep features

  This paper presents a novel fixation prediction and saliency modelingframework based on inter-image similarities and ensemble of Extreme LearningMachines (ELM). The proposed framework is inspired by two observations, 1) thecontextual information of a scene along with low-level visual cues modulatesattention, 2) the influence of scene memorability on eye movement patternscaused by the resemblance of a scene to a former visual experience. Motivatedby such observations, we develop a framework that estimates the saliency of agiven image using an ensemble of extreme learners, each trained on an imagesimilar to the input image. That is, after retrieving a set of similar imagesfor a given image, a saliency predictor is learnt from each of the images inthe retrieved image set using an ELM, resulting in an ensemble. The saliency ofthe given image is then measured in terms of the mean of predicted saliencyvalue by the ensemble's members.

Learning to predict where to look in interactive environments using deep  recurrent q-learning

  Bottom-Up (BU) saliency models do not perform well in complex interactiveenvironments where humans are actively engaged in tasks (e.g., sandwich makingand playing the video games). In this paper, we leverage Reinforcement Learning(RL) to highlight task-relevant locations of input frames. We propose a softattention mechanism combined with the Deep Q-Network (DQN) model to teach an RLagent how to play a game and where to look by focusing on the most pertinentparts of its visual input. Our evaluations on several Atari 2600 games showthat the soft attention based model could predict fixation locationssignificantly better than bottom-up models such as Itti-Kochs saliency andGraph-Based Visual Saliency (GBVS) models.

Paying Attention to Descriptions Generated by Image Captioning Models

  To bridge the gap between humans and machines in image understanding anddescribing, we need further insight into how people describe a perceived scene.In this paper, we study the agreement between bottom-up saliency-based visualattention and object referrals in scene description constructs. We investigatethe properties of human-written descriptions and machine-generated ones. Wethen propose a saliency-boosted image captioning model in order to investigatebenefits from low-level cues in language models. We learn that (1) humansmention more salient objects earlier than less salient ones in theirdescriptions, (2) the better a captioning model performs, the better attentionagreement it has with human descriptions, (3) the proposed saliency-boostedmodel, compared to its baseline form, does not improve significantly on the MSCOCO database, indicating explicit bottom-up boosting does not help when thetask is well learnt and tuned on a data, (4) a better generalization is,however, observed for the saliency-boosted model on unseen data.

Saliency Revisited: Analysis of Mouse Movements versus Fixations

  This paper revisits visual saliency prediction by evaluating the recentadvancements in this field such as crowd-sourced mouse tracking-based databasesand contextual annotations. We pursue a critical and quantitative approachtowards some of the new challenges including the quality of mouse trackingversus eye tracking for model training and evaluation. We extend quantitativeevaluation of models in order to incorporate contextual information byproposing an evaluation methodology that allows accounting for contextualfactors such as text, faces, and object attributes. The proposed contextualevaluation scheme facilitates detailed analysis of models and helps identifytheir pros and cons. Through several experiments, we find that (1) mousetracking data has lower inter-participant visual congruency and higherdispersion, compared to the eye tracking data, (2) mouse tracking data does nottotally agree with eye tracking in general and in terms of different contextualregions in specific, and (3) mouse tracking data leads to acceptable results intraining current existing models, and (4) mouse tracking data is less reliablefor model selection and evaluation. The contextual evaluation also revealsthat, among the studied models, there is no single model that performs best onall the tested annotations.

Structure-measure: A New Way to Evaluate Foreground Maps

  Foreground map evaluation is crucial for gauging the progress of objectsegmentation algorithms, in particular in the filed of salient object detectionwhere the purpose is to accurately detect and segment the most salient objectin a scene. Several widely-used measures such as Area Under the Curve (AUC),Average Precision (AP) and the recently proposed Fbw have been utilized toevaluate the similarity between a non-binary saliency map (SM) and aground-truth (GT) map. These measures are based on pixel-wise errors and oftenignore the structural similarities. Behavioral vision studies, however, haveshown that the human visual system is highly sensitive to structures in scenes.Here, we propose a novel, efficient, and easy to calculate measure known anstructural similarity measure (Structure-measure) to evaluate non-binaryforeground maps. Our new measure simultaneously evaluates region-aware andobject-aware structural similarity between a SM and a GT map. We demonstratesuperiority of our measure over existing ones using 5 meta-measures on 5benchmark datasets.

An Unsupervised Game-Theoretic Approach to Saliency Detection

  We propose a novel unsupervised game-theoretic salient object detectionalgorithm that does not require labeled training data. First, saliencydetection problem is formulated as a non-cooperative game, hereinafter referredto as Saliency Game, in which image regions are players who choose to be"background" or "foreground" as their pure strategies. A payoff function isconstructed by exploiting multiple cues and combining complementary features.Saliency maps are generated according to each region's strategy in the Nashequilibrium of the proposed Saliency Game. Second, we explore the complementaryrelationship between color and deep features and propose an Iterative RandomWalk algorithm to combine saliency maps produced by the Saliency Game usingdifferent features. Iterative random walk allows sharing information acrossfeature spaces, and detecting objects that are otherwise very hard to detect.Extensive experiments over 6 challenging datasets demonstrate the superiorityof our proposed unsupervised algorithm compared to several state of the artsupervised algorithms.

Segmenting Sky Pixels in Images

  Outdoor scene parsing models are often trained on ideal datasets and producequality results. However, this leads to a discrepancy when applied to the realworld. The quality of scene parsing, particularly sky classification, decreasesin night time images, images involving varying weather conditions, and scenechanges due to seasonal weather. This project focuses on approaching thesechallenges by using a state-of-the-art model in conjunction with a non-idealdataset: SkyFinder and a subset from SUN database with Sky object. We focusspecifically on sky segmentation, the task of determining sky and not-skypixels, and improving upon an existing state-of-the-art model: RefineNet. As aresult of our efforts, we have seen an improvement of 10-15% in the average MCRcompared to the prior methods on SkyFinder dataset. We have also improved froman off-the shelf-model in terms of average mIOU by nearly 35%. Further, weanalyze our trained models on images w.r.t two aspects: times of day andweather, and find that, in spite of facing same challenges as prior methods,our trained models significantly outperform them.

Visual Weather Temperature Prediction

  In this paper, we attempt to employ convolutional recurrent neural networksfor weather temperature estimation using only image data. We study ambienttemperature estimation based on deep neural networks in two scenarios a)estimating temperature of a single outdoor image, and b) predicting temperatureof the last image in an image sequence. In the first scenario, visual featuresare extracted by a convolutional neural network trained on a large-scale imagedataset. We demonstrate that promising performance can be obtained, and analyzehow volume of training data influences performance. In the second scenario, weconsider the temporal evolution of visual appearance, and construct a recurrentneural network to predict the temperature of the last image in a given imagesequence. We obtain better prediction accuracy compared to the state-of-the-artmodels. Further, we investigate how performance varies when information isextracted from different scene regions, and when images are captured indifferent daytime hours. Our approach further reinforces the idea of using onlyvisual information for cost efficient weather prediction in the future.

What Catches the Eye? Visualizing and Understanding Deep Saliency Models

  Deep convolutional neural networks have demonstrated high performances forfixation prediction in recent years. How they achieve this, however, is lessexplored and they remain to be black box models. Here, we attempt to shed lighton the internal structure of deep saliency models and study what features theyextract for fixation prediction. Specifically, we use a simple yet powerfularchitecture, consisting of only one CNN and a single resolution input,combined with a new loss function for pixel-wise fixation prediction duringfree viewing of natural scenes. We show that our simple method is on par orbetter than state-of-the-art complicated saliency models. Furthermore, wepropose a method, related to saliency model evaluation metrics, to visualizedeep models for fixation prediction. Our method reveals the innerrepresentations of deep models for fixation prediction and provides evidencethat saliency, as experienced by humans, is likely to involve high-levelsemantic knowledge in addition to low-level perceptual cues. Our results can beuseful to measure the gap between current saliency models and the humaninter-observer model and to build new models to close this gap.

Salient Objects in Clutter: Bringing Salient Object Detection to the  Foreground

  We provide a comprehensive evaluation of salient object detection (SOD)models. Our analysis identifies a serious design bias of existing SOD datasetswhich assumes that each image contains at least one clearly outstanding salientobject in low clutter. The design bias has led to a saturated high performancefor state-of-the-art SOD models when evaluated on existing datasets. Themodels, however, still perform far from being satisfactory when applied toreal-world daily scenes. Based on our analyses, we first identify 7 crucialaspects that a comprehensive and balanced dataset should fulfill. Then, wepropose a new high quality dataset and update the previous saliency benchmark.Specifically, our SOC (Salient Objects in Clutter) dataset, includes imageswith salient and non-salient objects from daily object categories. Beyondobject category annotations, each salient image is accompanied by attributesthat reflect common challenges in real-world scenes. Finally, we reportattribute-based performance assessment on our dataset.

Three Birds One Stone: A General Architecture for Salient Object  Segmentation, Edge Detection and Skeleton Extraction

  In this paper, we aim at solving pixel-wise binary problems, includingsalient object segmentation, skeleton extraction, and edge detection, byintroducing a unified architecture. Previous works have proposed tailoredmethods for solving each of the three tasks independently. Here, we show thatthese tasks share some similarities that can be exploited for developing aunified framework. In particular, we introduce a horizontal cascade, eachcomponent of which is densely connected to the outputs of previous component.Stringing these components together allows us to effectively exploit featuresacross different levels hierarchically to effectively address the multiplepixel-wise binary regression tasks. To assess the performance of our proposednetwork on these tasks, we carry out exhaustive evaluations on multiplerepresentative datasets. Although these tasks are inherently very different, weshow that our unified approach performs very well on all of them and works farbetter than current single-purpose state-of-the-art methods. All the code inthis paper will be publicly available.

Enhanced-alignment Measure for Binary Foreground Map Evaluation

  The existing binary foreground map (FM) measures to address various types oferrors in either pixel-wise or structural ways. These measures considerpixel-level match or image-level information independently, while cognitivevision studies have shown that human vision is highly sensitive to both globalinformation and local details in scenes. In this paper, we take a detailed lookat current binary FM evaluation measures and propose a novel and effectiveE-measure (Enhanced-alignment measure). Our measure combines local pixel valueswith the image-level mean value in one term, jointly capturing image-levelstatistics and local pixel matching information. We demonstrate the superiorityof our measure over the available measures on 4 popular datasets via 5meta-measures, including ranking models for applications, demoting generic,random Gaussian noise maps, ground-truth switch, as well as human judgments. Wefind large improvements in almost all the meta-measures. For instance, in termsof application ranking, we observe improvementrangingfrom9.08% to 19.65%compared with other popular measures.

Improving Sequential Determinantal Point Processes for Supervised Video  Summarization

  It is now much easier than ever before to produce videos. While theubiquitous video data is a great source for information discovery andextraction, the computational challenges are unparalleled. Automaticallysummarizing the videos has become a substantial need for browsing, searching,and indexing visual content. This paper is in the vein of supervised videosummarization using sequential determinantal point process (SeqDPP), whichmodels diversity by a probabilistic distribution. We improve this model in twofolds. In terms of learning, we propose a large-margin algorithm to address theexposure bias problem in SeqDPP. In terms of modeling, we design a newprobabilistic distribution such that, when it is integrated into SeqDPP, theresulting model accepts user input about the expected length of the summary.Moreover, we also significantly extend a popular video summarization dataset by1) more egocentric videos, 2) dense user annotations, and 3) a refinedevaluation scheme. We conduct extensive experiments on this dataset (about 60hours of videos in total) and compare our approach to several competitivebaselines.

Cross-view image synthesis using geometry-guided conditional GANs

  We address the problem of generating images across two drastically differentviews, namely ground (street) and aerial (overhead) views. Image synthesis byitself is a very challenging computer vision task and is even more so whengeneration is conditioned on an image in another view. Due the difference inviewpoints, there is small overlapping field of view and little common contentbetween these two views. Here, we try to preserve the pixel information betweenthe views so that the generated image is a realistic representation of crossview input image. For this, we propose to use homography as a guide to map theimages between the views based on the common field of view to preserve thedetails in the input image. We then use generative adversarial networks toinpaint the missing regions in the transformed image and add realism to it. Ourexhaustive evaluation and model comparison demonstrate that utilizing geometryconstraints adds fine details to the generated images and can be a betterapproach for cross view image synthesis than purely pixel based synthesismethods.

Invariance Analysis of Saliency Models versus Human Gaze During Scene  Free Viewing

  Most of current studies on human gaze and saliency modeling have usedhigh-quality stimuli. In real world, however, captured images undergo varioustypes of distortions during the whole acquisition, transmission, and displayingchain. Some distortion types include motion blur, lighting variations androtation. Despite few efforts, influences of ubiquitous distortions on visualattention and saliency models have not been systematically investigated. Inthis paper, we first create a large-scale database including eye movements of10 observers over 1900 images degraded by 19 types of distortions. Second, byanalyzing eye movements and saliency models, we find that: a) observers look atdifferent locations over distorted versus original images, and b) performancesof saliency models are drastically hindered over distorted images, with themaximum performance drop belonging to Rotation and Shearing distortions.Finally, we investigate the effectiveness of different distortions when servingas data augmentation transformations. Experimental results verify that someuseful data augmentation transformations which preserve human gaze of referenceimages can improve deep saliency models against distortions, while some invalidtransformations which severely change human gaze will degrade the performance.

Adversarial Attacks against Deep Saliency Models

  Currently, a plethora of saliency models based on deep neural networks haveled great breakthroughs in many complex high-level vision tasks (e.g. scenedescription, object detection). The robustness of these models, however, hasnot yet been studied. In this paper, we propose a sparse feature-spaceadversarial attack method against deep saliency models for the first time. Theproposed attack only requires a part of the model information, and is able togenerate a sparser and more insidious adversarial perturbation, compared totraditional image-space attacks. These adversarial perturbations are so subtlethat a human observer cannot notice their presences, but the model outputs willbe revolutionized. This phenomenon raises security threats to deep saliencymodels in practical applications. We also explore some intriguing properties ofthe feature-space attack, e.g. 1) the hidden layers with bigger receptivefields generate sparser perturbations, 2) the deeper hidden layers achievehigher attack success rates, and 3) different loss functions and differentattacked layers will result in diverse perturbations. Experiments indicate thatthe proposed method is able to successfully attack different modelarchitectures across various image scenes.

Non-Markovianity and coherence of a moving qubit inside a leaky cavity

  Non-Markovian features of a system evolution, stemming from memory effects,may be utilized to transfer, storage, and revive basic quantum properties ofthe system states. It is well known that an atom qubit undergoes non-Markoviandynamics in high quality cavities. We here consider the qubit-cavityinteraction in the case when the qubit is in motion inside a leaky cavity. Weshow that, owing to the inhibition of the decay rate, the coherence of thetraveling qubit remains closer to its initial value as time goes by compared tothat of a qubit at rest. We also demonstrate that quantum coherence ispreserved more efficiently for larger qubit velocities. This is trueindependently of the evolution being Markovian or non-Markovian, albeit thelatter condition is more effective at a given value of velocity. We howeverfind that the degree of non-Markovianity is eventually weakened as the qubitvelocity increases, despite a better coherence maintenance.

Salient Object Detection: A Benchmark

  We extensively compare, qualitatively and quantitatively, 40 state-of-the-artmodels (28 salient object detection, 10 fixation prediction, 1 objectness, and1 baseline) over 6 challenging datasets for the purpose of benchmarking salientobject detection and segmentation methods. From the results obtained so far,our evaluation shows a consistent rapid progress over the last few years interms of both accuracy and running time. The top contenders in this benchmarksignificantly outperform the models identified as the best in the previousbenchmark conducted just two years ago. We find that the models designedspecifically for salient object detection generally work better than models inclosely related areas, which in turn provides a precise definition and suggestsan appropriate treatment of this problem that distinguishes it from otherproblems. In particular, we analyze the influences of center bias and scenecomplexity in model performance, which, along with the hard cases forstate-of-the-art models, provide useful hints towards constructing morechallenging large scale datasets and better saliency models. Finally, wepropose probable solutions for tackling several open problems such asevaluation scores and dataset bias, which also suggest future researchdirections in the rapidly-growing field of salient object detection.

A Review of Co-saliency Detection Technique: Fundamentals, Applications,  and Challenges

  Co-saliency detection is a newly emerging and rapidly growing research areain computer vision community. As a novel branch of visual saliency, co-saliencydetection refers to the discovery of common and salient foregrounds from two ormore relevant images, and can be widely used in many computer vision tasks. Theexisting co-saliency detection algorithms mainly consist of three components:extracting effective features to represent the image regions, exploring theinformative cues or factors to characterize co-saliency, and designingeffective computational frameworks to formulate co-saliency. Although numerousmethods have been developed, the literature is still lacking a deep review andevaluation of co-saliency detection techniques. In this paper, we aim atproviding a comprehensive review of the fundamentals, challenges, andapplications of co-saliency detection. Specifically, we provide an overview ofsome related computer vision works, review the history of co-saliencydetection, summarize and categorize the major algorithms in this research area,discuss some open issues in this area, present the potential applications ofco-saliency detection, and finally point out some unsolved challenges andpromising future works. We expect this review to be beneficial to both freshand senior researchers in this field, and give insights to researchers in otherrelated areas regarding the utility of co-saliency detection algorithms.

Ego2Top: Matching Viewers in Egocentric and Top-view Videos

  Egocentric cameras are becoming increasingly popular and provide us withlarge amounts of videos, captured from the first person perspective. At thesame time, surveillance cameras and drones offer an abundance of visualinformation, often captured from top-view. Although these two sources ofinformation have been separately studied in the past, they have not beencollectively studied and related. Having a set of egocentric cameras and atop-view camera capturing the same area, we propose a framework to identify theegocentric viewers in the top-view video. We utilize two types of features forour assignment procedure. Unary features encode what a viewer (seen fromtop-view or recording an egocentric video) visually experiences over time.Pairwise features encode the relationship between the visual content of a pairof viewers. Modeling each view (egocentric or top) by a graph, the assignmentprocess is formulated as spectral graph matching. Evaluating our method over adataset of 50 top-view and 188 egocentric videos taken in different scenariosdemonstrates the efficiency of the proposed approach in assigning egocentricviewers to identities present in top-view camera. We also study the effect ofdifferent parameters such as the number of egocentric viewers and visualfeatures.

Egocentric Meets Top-view

  Thanks to the availability and increasing popularity of Egocentric camerassuch as GoPro cameras, glasses, and etc. we have been provided with a plethoraof videos captured from the first person perspective. Surveillance cameras andUnmanned Aerial Vehicles(also known as drones) also offer tremendous amount ofvideos, mostly with top-down or oblique view-point. Egocentric vision andtop-view surveillance videos have been studied extensively in the past in thecomputer vision community. However, the relationship between the two has yet tobe explored thoroughly. In this effort, we attempt to explore this relationshipby approaching two questions. First, having a set of egocentric videos and atop-view video, can we verify if the top-view video contains all, or some ofthe egocentric viewers present in the egocentric set? And second, can weidentify the egocentric viewers in the content of the top-view video? In otherwords, can we find the cameramen in the surveillance videos? These problems canbecome more challenging when the videos are not time-synchronous. Thus weformalize the problem in a way which handles and also estimates the unknownrelative time-delays between the egocentric videos and the top-view video. Weformulate the problem as a spectral graph matching instance, and jointly seekthe optimal assignments and relative time-delays of the videos. As a result, wespatiotemporally localize the egocentric observers in the top-view video. Wemodel each view (egocentric or top) using a graph, and compute the assignmentand time-delays in an iterative-alternative fashion.

Egocentric Height Estimation

  Egocentric, or first-person vision which became popular in recent years withan emerge in wearable technology, is different than exocentric (third-person)vision in some distinguishable ways, one of which being that the camera weareris generally not visible in the video frames. Recent work has been done onaction and object recognition in egocentric videos, as well as work onbiometric extraction from first-person videos. Height estimation can be auseful feature for both soft-biometrics and object tracking. Here, we propose amethod of estimating the height of an egocentric camera without any calibrationor reference points. We used both traditional computer vision approaches anddeep learning in order to determine the visual cues that results in best heightestimation. Here, we introduce a framework inspired by two stream networkscomprising of two Convolutional Neural Networks, one based on spatialinformation, and one based on information given by optical flow in a frame.Given an egocentric video as an input to the framework, our model yields aheight estimate as an output. We also incorporate late fusion to learn acombination of temporal and spatial cues. Comparing our model with othermethods we used as baselines, we achieve height estimates for videos with aMean Average Error of 14.04 cm over a range of 103 cm of data, andclassification accuracy for relative height (tall, medium or short) up to93.75% where chance level is 33%.

EgoTransfer: Transferring Motion Across Egocentric and Exocentric  Domains using Deep Neural Networks

  Mirror neurons have been observed in the primary motor cortex of primatespecies, in particular in humans and monkeys. A mirror neuron fires when aperson performs a certain action, and also when he observes the same actionbeing performed by another person. A crucial step towards building fullyautonomous intelligent systems with human-like learning abilities is thecapability in modeling the mirror neuron. On one hand, the abundance ofegocentric cameras in the past few years has offered the opportunity to study alot of vision problems from the first-person perspective. A great deal ofinteresting research has been done during the past few years, trying to explorevarious computer vision tasks from the perspective of the self. On the otherhand, videos recorded by traditional static cameras, capture humans performingdifferent actions from an exocentric third-person perspective. In this work, wetake the first step towards relating motion information across these twoperspectives. We train models that predict motion in an egocentric view, byobserving it from an exocentric view, and vice versa. This allows models topredict how an egocentric motion would look like from outside. To do so, wetrain linear and nonlinear models and evaluate their performance in terms ofretrieving the egocentric (exocentric) motion features, while having access toan exocentric (egocentric) motion feature. Our experimental results demonstratethat motion information can be successfully transferred across the two views.

Revisiting Video Saliency: A Large-scale Benchmark and a New Model

  In this work, we contribute to video saliency research in two ways. First, weintroduce a new benchmark for predicting human eye movements during dynamicscene free-viewing, which is long-time urged in this field. Our dataset, namedDHF1K (Dynamic Human Fixation), consists of 1K high-quality, elaboratelyselected video sequences spanning a large range of scenes, motions, objecttypes and background complexity. Existing video saliency datasets lack varietyand generality of common dynamic scenes and fall short in covering challengingsituations in unconstrained environments. In contrast, DHF1K makes asignificant leap in terms of scalability, diversity and difficulty, and isexpected to boost video saliency modeling. Second, we propose a novel videosaliency model that augments the CNN-LSTM network architecture with anattention mechanism to enable fast, end-to-end saliency learning. The attentionmechanism explicitly encodes static saliency information, thus allowing LSTM tofocus on learning more flexible temporal saliency representation acrosssuccessive frames. Such a design fully leverages existing large-scale staticfixation datasets, avoids overfitting, and significantly improves trainingefficiency and testing performance. We thoroughly examine the performance ofour model, with respect to state-of-the-art saliency models, on threelarge-scale datasets (i.e., DHF1K, Hollywood2, UCF sports). Experimentalresults over more than 1.2K testing videos containing 400K frames demonstratethat our model outperforms other competitors.

Analysis of Hand Segmentation in the Wild

  A large number of works in egocentric vision have concentrated on action andobject recognition. Detection and segmentation of hands in first-person videos,however, has less been explored. For many applications in this domain, it isnecessary to accurately segment not only hands of the camera wearer but alsothe hands of others with whom he is interacting. Here, we take an in-depth lookat the hand segmentation problem. In the quest for robust hand segmentationmethods, we evaluated the performance of the state of the art semanticsegmentation methods, off the shelf and fine-tuned, on existing datasets. Wefine-tune RefineNet, a leading semantic segmentation method, for handsegmentation and find that it does much better than the best contenders.Existing hand segmentation datasets are collected in the laboratory settings.To overcome this limitation, we contribute by collecting two new datasets: a)EgoYouTubeHands including egocentric videos containing hands in the wild, andb) HandOverFace to analyze the performance of our models in presence of similarappearance occlusions. We further explore whether conditional random fields canhelp refine generated hand segmentations. To demonstrate the benefit ofaccurate hand maps, we train a CNN for hand-based activity recognition andachieve higher accuracy when a CNN was trained using hand maps produced by thefine-tuned RefineNet. Finally, we annotate a subset of the EgoHands dataset forfine-grained action recognition and show that an accuracy of 58.6% can beachieved by just looking at a single hand pose which is much better than thechance level (12.5%).

Cross-View Image Synthesis using Conditional GANs

  Learning to generate natural scenes has always been a challenging task incomputer vision. It is even more painstaking when the generation is conditionedon images with drastically different views. This is mainly becauseunderstanding, corresponding, and transforming appearance and semanticinformation across the views is not trivial. In this paper, we attempt to solvethe novel problem of cross-view image synthesis, aerial to street-view and viceversa, using conditional generative adversarial networks (cGAN). Two newarchitectures called Crossview Fork (X-Fork) and Crossview Sequential (X-Seq)are proposed to generate scenes with resolutions of 64x64 and 256x256 pixels.X-Fork architecture has a single discriminator and a single generator. Thegenerator hallucinates both the image and its semantic segmentation in thetarget view. X-Seq architecture utilizes two cGANs. The first one generates thetarget image which is subsequently fed to the second cGAN for generating itscorresponding semantic segmentation map. The feedback from the second cGANhelps the first cGAN generate sharper images. Both of our proposedarchitectures learn to generate natural images as well as their semanticsegmentation maps. The proposed methods show that they are able to capture andmaintain the true semantics of objects in source and target views better thanthe traditional image-to-image translation method which considers only thevisual appearance of the scene. Extensive qualitative and quantitativeevaluations support the effectiveness of our frameworks, compared to two stateof the art methods, for natural scene generation across drastically differentviews.

Learning a Saliency Evaluation Metric Using Crowdsourced Perceptual  Judgments

  In the area of human fixation prediction, dozens of computational saliencymodels are proposed to reveal certain saliency characteristics under differentassumptions and definitions. As a result, saliency model benchmarking oftenrequires several evaluation metrics to simultaneously assess saliency modelsfrom multiple perspectives. However, most computational metrics are notdesigned to directly measure the perceptual similarity of saliency maps so thatthe evaluation results may be sometimes inconsistent with the subjectiveimpression. To address this problem, this paper first conducts extensivesubjective tests to find out how the visual similarities between saliency mapsare perceived by humans. Based on the crowdsourced data collected in thesetests, we conclude several key factors in assessing saliency maps and quantizethe performance of existing metrics. Inspired by these factors, we propose tolearn a saliency evaluation metric based on a two-stream convolutional neuralnetwork using crowdsourced perceptual judgements. Specifically, the relativesaliency score of each pair from the crowdsourced data is utilized toregularize the network during the training process. By capturing the keyfactors shared by various subjects in comparing saliency maps, the learnedmetric better aligns with human perception of saliency maps, making it a goodcomplement to the existing metrics. Experimental results validate that thelearned metric can be generalized to the comparisons of saliency maps from newimages, new datasets, new models and synthetic data. Due to the effectivenessof the learned metric, it also can be used to facilitate the development of newmodels for fixation prediction.

From Third Person to First Person: Dataset and Baselines for Synthesis  and Retrieval

  First-person (egocentric) and third person (exocentric) videos aredrastically different in nature. The relationship between these two views havebeen studied in recent years, however, it has yet to be fully explored. In thiswork, we introduce two datasets (synthetic and natural/real) containingsimultaneously recorded egocentric and exocentric videos. We also explorerelating the two domains (egocentric and exocentric) in two aspects. First, wesynthesize images in the egocentric domain from the exocentric domain using aconditional generative adversarial network (cGAN). We show that with enoughtraining data, our network is capable of hallucinating how the world would looklike from an egocentric perspective, given an exocentric video. Second, weaddress the cross-view retrieval problem across the two views. Given anegocentric query frame (or its momentary optical flow), we retrieve itscorresponding exocentric frame (or optical flow) from a gallery set. We showthat using synthetic data could be beneficial in retrieving real data. We showthat performing domain adaptation from the synthetic domain to the natural/realdomain, is helpful in tasks such as retrieval. We believe that the presenteddatasets and the proposed baselines offer new opportunities for furtherresearch in this direction. The code and dataset are publicly available.

Multi-View Egocentric Video Summarization

  With vast amounts of video content being uploaded to the Internet everyminute, video summarization becomes critical for efficient browsing, searching,and indexing of visual content. Nonetheless, the spread of social andegocentric cameras tends to create an abundance of sparse scenarios captured byseveral devices, and ultimately required to be jointly summarized. In thispaper, we propose the problem of summarizing videos recorded simultaneously byseveral egocentric cameras that intermittently share the field of view. Wepresent a supervised-learning framework that (a) identifies a diverse set ofimportant events among dynamically moving cameras that often are not capturingthe same scene, and (b) selects the most representative view(s) at each eventto be included in the universal summary. A key contribution of our work iscollecting a new multi-view egocentric dataset, Multi-Ego, due to the lack ofan applicable and relevant alternative. Our dataset consists of 41 sequences,each recorded simultaneously by 3 cameras and covering a wide variety ofreal-life scenarios. The footage is annotated comprehensively by multipleindividuals under various summarization settings: (a) single view, (b) twoview, and (c) three view, with a consensus analysis ensuring a reliable groundtruth. We conduct extensive experiments on the compiled dataset to show theeffectiveness of our approach over several state-of-the-art baselines. We alsoshow that it can learn from data of varied number-of-views, deeming it ascalable and a generic summarization approach. Our dataset and materials arepublicly available.

Video Summarization via Actionness Ranking

  To automatically produce a brief yet expressive summary of a long video, anautomatic algorithm should start by resembling the human process of summarygeneration. Prior work proposed supervised and unsupervised algorithms to trainmodels for learning the underlying behavior of humans by increasing modelingcomplexity or craft-designing better heuristics to simulate human summarygeneration process. In this work, we take a different approach by analyzing amajor cue that humans exploit for the summary generation; the nature andintensity of actions.  We empirically observed that a frame is more likely to be included inhuman-generated summaries if it contains a substantial amount of deliberatemotion performed by an agent, which is referred to as actionness. Therefore, wehypothesize that learning to automatically generate summaries involves animplicit knowledge of actionness estimation and ranking. We validate ourhypothesis by running a user study that explores the correlation betweenhuman-generated summaries and actionness ranks. We also run a consensus andbehavioral analysis between human subjects to ensure reliable and consistentresults. The analysis exhibits a considerable degree of agreement amongsubjects within obtained data and verifying our initial hypothesis.  Based on the study findings, we develop a method to incorporate actionnessdata to explicitly regulate a learning algorithm that is trained for summarygeneration. We assess the performance of our approach to four summarizationbenchmark datasets and demonstrate an evident advantage compared tostate-of-the-art summarization methods.

A Synchronized Multi-Modal Attention-Caption Dataset and Analysis

  In this work, we present a novel multi-modal dataset consisting of eyemovements and verbal descriptions recorded synchronously over images. Usingthis data, we study the differences between human attention in free-viewing andimage captioning tasks. We look into the relationship between human attentionand language constructs during perception and sentence articulation. We alsocompare human and machine attention, in particular the top-down soft attentionapproach that is argued to mimick human attention, in captioning tasks. Ourstudy reveals that, (1) human attention behaviour in free-viewing is differentthan image description as humans tend to fixate on a greater variety of regionsunder the latter task; (2) there is a strong relationship between the describedobjects and the objects attended by subjects ($97\%$ of described objects arebeing attended); (3) a convolutional neural network as feature encoder capturesregions that human attend under image captioning to a great extent (around$78\%$); (4) the soft-attention as the top-down mechanism does not agree withhuman attention behaviour neither spatially nor temporally; and (5)soft-attention does not add strong beneficial human-like attention behaviourfor the task of captioning as it has low correlation between caption scores andattention consistency scores, indicating a large gap between human and machinein regard to top-down attention.

