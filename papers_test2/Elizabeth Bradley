Determinism, Complexity, and Predictability in Computer Performance

  Computers are deterministic dynamical systems (CHAOS 19:033124, 2009). Amongother things, that implies that one should be able to use deterministicforecast rules to predict their behavior. That statement is sometimes-but notalways-true. The memory and processor loads of some simple programs are easy topredict, for example, but those of more-complex programs like compilers arenot. The goal of this paper is to determine why that is the case. We conjecturethat, in practice, complexity can effectively overwhelm the predictive power ofdeterministic forecast models. To explore that, we build models of a number ofperformance traces from different programs running on different Intel-basedcomputers. We then calculate the permutation entropy-a temporal entropy metricthat uses ordinal analysis-of those traces and correlate those values againstthe prediction success

Nonlinear dynamics of running: Speed, stability, symmetry and the  effects of leg amputations

  In this paper, we study dynamic stability during running, focusing on theeffects of speed and the use of a leg prosthesis. We compute and compare themaximal Lyapunov exponents of kinematic time-series data from subjects with andwithout unilateral transtibial amputations running at a wide range of speeds.We find that the dynamics of the affected leg with the running-specificprosthesis are less stable than the dynamics of the unaffected leg, and alsoless stable than the biological legs of the non-amputee runners. Surprisingly,we find that the center-of-mass dynamics of runners with two intact biologicallegs are slightly less stable than those of runners with amputations. Ourresults suggest that while leg asymmetries may be associated with instability,runners may compensate for this effect by increased control of theircenter-of-mass dynamics.

Nonlinear time-series analysis revisited

  In 1980 and 1981, two pioneering papers laid the foundation for what becameknown as nonlinear time-series analysis: the analysis of observeddata---typically univariate---via dynamical systems theory. Based on theconcept of state-space reconstruction, this set of methods allows us to computecharacteristic quantities such as Lyapunov exponents and fractal dimensions, topredict the future course of the time series, and even to reconstruct theequations of motion in some cases. In practice, however, there are a number ofissues that restrict the power of this approach: whether the signal accuratelyand thoroughly samples the dynamics, for instance, and whether it containsnoise. Moreover, the numerical algorithms that we use to instantiate theseideas are not perfect; they involve approximations, scale parameters, andfinite-precision arithmetic, among other things. Even so, nonlinear time-seriesanalysis has been used to great advantage on thousands of real and syntheticdata sets from a wide variety of systems ranging from roulette wheels to lasersto the human heart. Even in cases where the data do not meet the mathematicalor algorithmic requirements to assure full topological conjugacy, the resultsof nonlinear time-series analysis can be helpful in understanding,characterizing, and predicting dynamical systems.

Unix Memory Allocations are Not Poisson

  In multitasking operating systems, requests for free memory are traditionallymodeled as a stochastic counting process with independent,exponentially-distributed interarrival times because of the analytic simplicitysuch Poisson models afford. We analyze the distribution of several million unixpage commits to show that although this approach could be valid over relativelylong timespans, the behavior of the arrival process over shorter periods isdecidedly not Poisson. We find that this result holds regardless of theoriginator of the request: unlike network packets, there is little differencebetween system- and user-level page-request distributions. We believe this tobe due to the bursty nature of page allocations, which tend to occur in eithersmall or extremely large increments. Burstiness and persistent variance haverecently been found in self-similar processes in computer networks, but we showthat although page commits are both bursty and possess high variance over longtimescales, they are probably not self-similar. These results suggest thataltogether different models are needed for fine-grained analysis of memorysystems, an important consideration not only for understanding behavior butalso for the design of online control systems.

Advanced Cyberinfrastructure for Science, Engineering, and Public Policy

  Progress in many domains increasingly benefits from our ability to view thesystems through a computational lens, i.e., using computational abstractions ofthe domains; and our ability to acquire, share, integrate, and analyzedisparate types of data. These advances would not be possible without theadvanced data and computational cyberinfrastructure and tools for data capture,integration, analysis, modeling, and simulation. However, despite, and perhapsbecause of, advances in "big data" technologies for data acquisition,management and analytics, the other largely manual, and labor-intensive aspectsof the decision making process, e.g., formulating questions, designing studies,organizing, curating, connecting, correlating and integrating crossdomain data,drawing inferences and interpreting results, have become the rate-limitingsteps to progress. Advancing the capability and capacity for evidence-basedimprovements in science, engineering, and public policy requires support for(1) computational abstractions of the relevant domains coupled withcomputational methods and tools for their analysis, synthesis, simulation,visualization, sharing, and integration; (2) cognitive tools that leverage andextend the reach of human intellect, and partner with humans on all aspects ofthe activity; (3) nimble and trustworthy data cyber-infrastructures thatconnect, manage a variety of instruments, multiple interrelated data types andassociated metadata, data representations, processes, protocols and workflows;and enforce applicable security and data access and use policies; and (4)organizational and social structures and processes for collaborative andcoordinated activity across disciplinary and institutional boundaries.

Iterated Function System Models in Data Analysis: Detection and  Separation

  We investigate the use of iterated function system (IFS) models for dataanalysis. An IFS is a discrete dynamical system in which each time stepcorresponds to the application of one of a finite collection of maps. The maps,which represent distinct dynamical regimes, may act in some pre-determinedsequence or may be applied in random order. An algorithm is developed to detectthe sequence of regime switches under the assumption of continuity. This methodis tested on a simple IFS and applied to an experimental computer performancedata set. This methodology has a wide range of potential uses: fromchange-point detection in time-series data to the field of digitalcommunications.

On the importance of nonlinear modeling in computer performance  prediction

  Computers are nonlinear dynamical systems that exhibit complex and sometimeseven chaotic behavior. The models used in the computer systems community,however, are linear. This paper is an exploration of that disconnect: whenlinear models are adequate for predicting computer performance and when theyare not. Specifically, we build linear and nonlinear models of the processorload of an Intel i7-based computer as it executes a range of differentprograms. We then use those models to predict the processor loads forward intime and compare those forecasts to the true continuations of the time series

Prediction in Projection

  Prediction models that capture and use the structure of state-space dynamicscan be very effective. In practice, however, one rarely has access to fullinformation about that structure, and accurate reconstruction of the dynamicsfrom scalar time-series data---e.g., via delay-coordinate embedding---can be areal challenge. In this paper, we show that forecast models that employincomplete embeddings of the dynamics can produce surprisingly accuratepredictions of the state of a dynamical system. In particular, we demonstratethe effectiveness of a simple near-neighbor forecast technique that works witha two-dimensional embedding. Even though correctness of the topology is notguaranteed for incomplete reconstructions like this, the dynamical structurethat they capture allows for accurate predictions---in many cases, even moreaccurate than predictions generated using a full embedding. This could be veryuseful in the context of real-time forecasting, where the human effort requiredto produce a correct delay-coordinate embedding is prohibitive.

Big Data, Data Science, and Civil Rights

  Advances in data analytics bring with them civil rights implications.Data-driven and algorithmic decision making increasingly determine howbusinesses target advertisements to consumers, how police departments monitorindividuals or groups, how banks decide who gets a loan and who does not, howemployers hire, how colleges and universities make admissions and financial aiddecisions, and much more. As data-driven decisions increasingly affect everycorner of our lives, there is an urgent need to ensure they do not becomeinstruments of discrimination, barriers to equality, threats to social justice,and sources of unfairness. In this paper, we argue for a concrete researchagenda aimed at addressing these concerns, comprising five areas of emphasis:(i) Determining if models and modeling procedures exhibit objectionable bias;(ii) Building awareness of fairness into machine learning methods; (iii)Improving the transparency and control of data- and model-driven decisionmaking; (iv) Looking beyond the algorithm(s) for sources of bias andunfairness-in the myriad human decisions made during the problem formulationand modeling process; and (v) Supporting the cross-disciplinary scholarshipnecessary to do all of that well.

Computational Topology Techniques for Characterizing Time-Series Data

  Topological data analysis (TDA), while abstract, allows a characterization oftime-series data obtained from nonlinear and complex dynamical systems. Thoughit is surprising that such an abstract measure of structure - counting piecesand holes - could be useful for real-world data, TDA lets us compare differentsystems, and even do membership testing or change-point detection. However, TDAis computationally expensive and involves a number of free parameters. Thiscomplexity can be obviated by coarse-graining, using a construct called thewitness complex. The parametric dependence gives rise to the concept ofpersistent homology: how shape changes with scale. Its results allow us todistinguish time-series data from different systems - e.g., the same noteplayed on different musical instruments.

The Resolved Narrow Line Region in NGC4151

  We present slitless spectra of the Narrow Line Region (NLR) in NGC4151 fromthe Space Telescope Imaging Spectrograph (STIS) on HST, and investigate thekinematics and physical conditions of the emission line clouds in this region.Using medium resolution (~0.5 Angstrom) slitless spectra at two roll angles andnarrow band undispersed images, we have mapped the NLR velocity field from 1.2kpc to within 13 pc (H_o=75 km/s/Mpc) of the nucleus. The inner biconical clouddistribution exhibits recessional velocities relative to the nucleus to the NEand approaching velocities to the SW of the nucleus. We find evidence for atleast two kinematic components in the NLR. One kinematic component ischaracterized by Low Velocities and Low Velocity Dispersions (LVLVD clouds: |v|< 400 km/s, and Delta_v < 130 km/s). This population extends through the NLRand their observed kinematics may be gravitationally associated with the hostgalaxy. Another component is characterized by High Velocities and High VelocityDispersions (HVHVD clouds: 400 < |v| < ~1700 km/s, Delta_v > 130 km/s). Thisset of clouds is located within 1.1 arcsec (~70pc) of the nucleus and hasradial velocities which are too high to be gravitational in origin, but show nostrong correlation between velocity or velocity dispersion and the position ofthe radio knots. Outflow scenarios will be discussed as the driving mechanismfor these HVHVD clouds.

Optical Variability of the Three Brightest Nearby Quasars

  We report on the relative optical variability of the three brightest nearbyquasars, 3C 273, PDS 456, and PHL 1811. All three have comparable absolutemagnitudes, but PDS 456 and PHL 1811 are radio quiet. PDS 456 is a broad-lineobject, but PHL 1811 could be classified as a high-luminosity Narrow-LineSeyfert 1 (NLS1). Both of the radio-quiet quasars show significant variabilityon a timescale of a few days. The seasonal rms V-band variability amplitudes of3C 273 and PDS 456 are indistinguishable, and the seasonal rms variabilityamplitude of PHL 1811 was only exceeded by 3C 273 once in 30 years ofmonitoring. We find no evidence that the optical variability of 3C 273 isgreater than or more rapid than the variability of the comparably-bright,radio-quiet quasars. This suggests that not only do radio-loud and radio-quietAGNs have similar spectral energy distributions, but that the variabilitymechanisms are also similar. The optical variability of 3C 273 is not dominatedby a "blazer" component.

Strange Beta: An Assistance System for Indoor Rock Climbing Route  Setting Using Chaotic Variations and Machine Learning

  This paper applies machine learning and the mathematics of chaos to the taskof designing indoor rock-climbing routes. Chaotic variation has been used togreat advantage on music and dance, but the challenges here are quitedifferent, beginning with the representation. We present a formalized systemfor transcribing rock climbing problems, then describe a variation generatorthat is designed to support human route-setters in designing new andinteresting climbing problems. This variation generator, termed Strange Beta,combines chaos and machine learning, using the former to introduce novelty andthe latter to smooth transitions in a manner that is consistent with the styleof the climbs This entails parsing the domain-specific natural language thatrock climbers use to describe routes and movement and then learning thepatterns in the results. We validated this approach with a pilot study in asmall university rock climbing gym, followed by a large blinded study in acommercial climbing gym, in cooperation with experienced climbers and expertroute setters. The results show that {\sc Strange Beta} can help a human setterproduce routes that are at least as good as, and in some cases better than,those produced in the traditional manner.

Model-free quantification of time-series predictability

  This paper provides insight into when, why, and how forecast strategies failwhen they are applied to complicated time series. We conjecture that theinherent complexity of real-world time-series data---which results from thedimension, nonlinearity, and non-stationarity of the generating process, aswell as from measurement issues like noise, aggregation, and finite datalength---is both empirically quantifiable and directly correlated withpredictability. In particular, we argue that redundancy is an effective way tomeasure complexity and predictive structure in an experimental time series andthat weighted permutation entropy is an effective way to estimate thatredundancy. To validate these conjectures, we study 120 different time-seriesdata sets. For each time series, we construct predictions using a wide varietyof forecast models, then compare the accuracy of the predictions with thepermutation entropy of that time series. We use the results to develop amodel-free heuristic that can help practitioners recognize when a particularprediction method is not well matched to the task at hand: that is, when thetime series has more predictive structure than that method can capture andexploit.

Simplicial Multivalued Maps and the Witness Complex for Dynamical  Analysis of Time Series

  Topology based analysis of time-series data from dynamical systems ispowerful: it potentially allows for computer-based proofs of the existence ofvarious classes of regular and chaotic invariant sets for high-dimensionaldynamics. Standard methods are based on a cubical discretization of thedynamics and use the time series to construct an outer approximation of theunderlying dynamical system. The resulting multivalued map can be used tocompute the Conley index of isolated invariant sets of cubes. In this paper weintroduce a discretization that uses instead a simplicial complex constructedfrom a witness-landmark relationship. The goal is to obtain a naturaldiscretization that is more tightly connected with the invariant density of thetime series itself. The time-ordering of the data also directly leads to a mapon this simplicial complex that we call the witness map. We obtain conditionsunder which this witness map gives an outer approximation of the dynamics, andthus can be used to compute the Conley index of isolated invariant sets. Themethod is illustrated by a simple example using data from the classical H\'enonmap.

Not In Our Backyard: Spectroscopic Support for the CLASH z=11 Candidate  MACS0647-JD

  We report on our first set of spectroscopic Hubble Space Telescopeobservations of the z~11 candidate galaxy strongly lensed by theMACSJ0647.7+7015 galaxy cluster. The three lensed images are faint and we showthat these early slitless grism observations are of sufficient depth toinvestigate whether this high-redshift candidate, identified by its strongphotometric break at ~1.5 micron, could possibly be an emission line galaxy ata much lower redshift. While such an interloper would imply the existence of arather peculiar object, we show here that such strong emission lines wouldclearly have been detected. Comparing realistic, two-dimensional simulations tothese new observations we would expect the necessary emission lines to bedetected at >5 sigma while we see no evidence for such lines in the disperseddata of any of the three lensed images. We therefore exclude that this objectcould be a low redshift emission line interloper, which significantly increasesthe likelihood of this candidate being a bona fide z~11 galaxy.

Exploring the Topology of Dynamical Reconstructions

  Computing the state-space topology of a dynamical system from scalar datarequires accurate reconstruction of those dynamics and construction of anappropriate simplicial complex from the results. The reconstruction processinvolves a number of free parameters and the computation of homology for alarge number of simplices can be expensive. This paper is a study of how toavoid a full (diffeomorphic) reconstruction and how to decrease thecomputational burden. Using trajectories from the classic Lorenz system, wereconstruct the dynamics using the method of delays, then build a parsimonioussimplicial complex---the "witness complex"---to compute its homology.Surprisingly, we find that the witness complex correctly resolves the homologyof the underlying invariant set from noisy samples of that set even if thereconstruction dimension is well below the thresholds specified in theembedding theorems for assuring topological conjugacy between the true andreconstructed dynamics. We conjecture that this is because the requirements forreconstructing homology, are less stringent than those in the embeddingtheorems. In particular, to faithfully reconstruct the homology, ahomeomorphism is sufficient---as opposed to a diffeomorphism, as is necessaryfor the full dynamics. We provide preliminary evidence that a homeomorphism, inthe form of a delay-coordinate reconstruction map, may manifest at a lowerdimension than that required to achieve an embedding.

A new method for choosing parameters in delay reconstruction-based  forecast strategies

  Delay-coordinate reconstruction is a proven modeling strategy for buildingeffective forecasts of nonlinear time series. The first step in this process isthe estimation of good values for two parameters, the time delay and theembedding dimension. Many heuristics and strategies have been proposed in theliterature for estimating these values. Few, if any, of these methods weredeveloped with forecasting in mind, however, and their results are not optimalfor that purpose. Even so, these heuristics---intended for otherapplications---are routinely used when building delay coordinatereconstruction-based forecast models. In this paper, we propose a new strategyfor choosing optimal parameter values for forecast methods that are based ondelay-coordinate reconstructions. The basic calculation involves maximizing theshared information between each delay vector and the future state of thesystem. We illustrate the effectiveness of this method on several synthetic andexperimental systems, showing that this metric can be calculated quickly andreliably from a relatively short time series, and that it provides a directindication of how well a near-neighbor based forecasting method will work on agiven delay reconstruction of that time series. This allows a practitioner tochoose reconstruction parameters that avoid any pathologies, regardless of theunderlying mechanism, and maximize the predictive information contained in thereconstruction.

Prediction in Projection: A new paradigm in delay-coordinate  reconstruction

  Delay-coordinate embedding is a powerful, time-tested mathematical frameworkfor reconstructing the dynamics of a system from a series of scalarobservations. Most of the associated theory and heuristics are overly stringentfor real-world data, however, and real-time use is out of the question due tothe expert human intuition needed to use these heuristics correctly. Theapproach outlined in this thesis represents a paradigm shift away from thattraditional approach. I argue that perfect reconstructions are not onlyunnecessary for the purposes of delay-coordinate based forecasting, but thatthey can often be less effective than reduced-order versions of those samemodels. I demonstrate this using a range of low- and high-dimensional dynamicalsystems, showing that forecast models that employ imperfect reconstructions ofthe dynamics---i.e., models that are not necessarily true embeddings---canproduce surprisingly accurate predictions of the future state of these systems.I develop a theoretical framework for understanding why this is so. Thisframework, which combines information theory and computational topology, alsoallows one to quantify the amount of predictive structure in a given timeseries, and even to choose which forecast method will be the most effective forthose data.

Climate entropy production recorded in a deep Antarctic ice core

  Paleoclimate records are extremely rich sources of information about the pasthistory of the Earth system. Information theory, the branch of mathematicscapable of quantifying the degree to which the present is informed by the past,provides a new means for studying these records. Here, we demonstrate thatestimates of the Shannon entropy rate of the water-isotope data from the WestAntarctica Ice Sheet (WAIS) Divide ice core, calculated using weightedpermutation entropy (WPE), can bring out valuable new information from thisrecord. We find that WPE correlates with accumulation, reveals possiblesignatures of geothermal heating at the base of the core, and clearly bringsout laboratory and data-processing effects that are difficult to see in the rawdata. For example, the signatures of Dansgaard-Oeschger events in theinformation record are small, suggesting that these abrupt warming events maynot represent significant changes in the climate system dynamics. While thepotential power of information theory in paleoclimatology problems issignificant, the associated methods require careful handling and well-dated,high-resolution data. The WAIS Divide ice core is the first such record thatcan support this kind of analysis. As more high-resolution records becomeavailable, information theory will likely become a common forensic tool inclimate science.

Anomaly Detection in Paleoclimate Records using Permutation Entropy

  Permutation entropy techniques can be useful in identifying anomalies inpaleoclimate data records, including noise, outliers, and post-processingissues. We demonstrate this using weighted and unweighted permutation entropyof water-isotope records in a deep polar ice core. In one region of theseisotope records, our previous calculations revealed an abrupt change in thecomplexity of the traces: specifically, in the amount of new information thatappeared at every time step. We conjectured that this effect was due to noiseintroduced by an older laboratory instrument. In this paper, we validate thatconjecture by re-analyzing a section of the ice core using a more-advancedversion of the laboratory instrument. The anomalous noise levels are absentfrom the permutation entropy traces of the new data. In other sections of thecore, we show that permutation entropy techniques can be used to identifyanomalies in the raw data that are not associated with climatic orglaciological processes, but rather effects occurring during field work,laboratory analysis, or data post-processing. These examples make it clear thatpermutation entropy is a useful forensic tool for identifying sections of datathat require targeted re-analysis---and can even be useful in guiding thatanalysis.

The Ionized Gas and Nuclear Environment in NGC 3783 II. Averaged  HST/STIS and FUSE Spectra

  We present observations of the intrinsic absorption in the Seyfert 1 galaxyNGC 3783 obtained with the STIS/HST and FUSE. We have coadded multiple STIS andFUSE observations to obtain a high S/N averaged spectrum spanning 905-1730 A.The averaged spectrum reveals absorption in O VI, N V, C IV, N III, C III andthe Lyman lines up to LyE in the three blueshifted kinematic componentspreviously detected in the STIS spectrum (at radial velocities of -1320, -724,and -548 km/s). The highest velocity component exhibits absorption in Si IV. Wealso detect metastable C III* in this component, indicating a high density inthis absorber. We separate the individual covering factors of the continuum andemission-line sources as a function of velocity in each kinematic componentusing the LyA and LyB lines. Additionally, we find that the continuum coveringfactor varies with velocity within the individual kinematic components,decreasing smoothly in the wings of the absorption by at least 60%. Thecovering factor of Si IV is found to be less than half that of H I and N V inthe high velocity component. Additionally, the FWHM of N III and Si IV arenarrower than the higher ionization lines in this component. These resultsindicate there is substructure within this absorber. We derive a lower limit onthe total column (N_H>=10^{19}cm^{-2}) and ionization parameter (U>=0.005) inthe low ionization subcomponent of this absorber. The metastable-to-total C IIIcolumn density ratio implies n_e~10^9 cm^{-3} and an upper limit on thedistance of the absorber from the ionizing continuum of R<=8x10^{17} cm.

Further Definition of the Mass-Metallicity Relation in Globular Cluster  Systems Around Brightest Cluster Galaxies

  We combine the globular cluster data for fifteen Brightest Cluster Galaxiesand use this material to trace the mass-metallicity relations (MMR) in theirglobular cluster systems (GCSs). This work extends previous studies whichcorrelate the properties of the MMR with those of the host galaxy. Our combineddata sets show a mean trend for the metal-poor (MP) subpopulation whichcorresponds to a scaling of heavy-element abundance with cluster mass Z ~M^(0.30+/-0.05). No trend is seen for the metal-rich (MR) subpopulation whichhas a scaling relation that is consistent with zero. We also find that thescaling exponent is independent of the GCS specific frequency and host galaxyluminosity, except perhaps for dwarf galaxies.  We present new photometry in (g',i') obtained with Gemini/GMOS for theglobular cluster populations around the southern giant ellipticals NGC 5193 andIC 4329. Both galaxies have rich cluster populations which show up as normal,bimodal sequences in the colour-magnitude diagram.  We test the observed MMRs and argue that they are statistically real, and notan artifact caused by the method we used. We also argue against asymmetriccontamination causing the observed MMR as our mean results are no differentfrom other contamination-free studies. Finally, we compare our method to thestandard bimodal fitting method (KMM or RMIX) and find our results areconsistent.  Interpretation of these results is consistent with recent models for globularcluster formation in which the MMR is determined by GC self-enrichment duringtheir brief formation period.

Disk-Jet Connection in the Radio Galaxy 3C 120

  We present the results of extensive multi-frequency monitoring of the radiogalaxy 3C 120 between 2002 and 2007 at X-ray, optical, and radio wave bands, aswell as imaging with the Very Long Baseline Array (VLBA). Over the 5 yr ofobservation, significant dips in the X-ray light curve are followed byejections of bright superluminal knots in the VLBA images. Consistent withthis, the X-ray flux and 37 GHz flux are anti-correlated with X-ray leading theradio variations. This implies that, in this radio galaxy, the radiative stateof accretion disk plus corona system, where the X-rays are produced, has adirect effect on the events in the jet, where the radio emission originates.The X-ray power spectral density of 3C 120 shows a break, with steeper slope atshorter timescale and the break timescale is commensurate with the mass of thecentral black hole based on observations of Seyfert galaxies and black holeX-ray binaries. These findings provide support for the paradigm that black holeX-ray binaries and active galactic nuclei are fundamentally similar systems,with characteristic time and size scales linearly proportional to the mass ofthe central black hole. The X-ray and optical variations are stronglycorrelated in 3C 120, which implies that the optical emission in this objectarises from the same general region as the X-rays, i.e., in the accretiondisk-corona system. We numerically model multi-wavelength light curves of 3C120 from such a system with the optical-UV emission produced in the disk andthe X-rays generated by scattering of thermal photons by hot electrons in thecorona. From the comparison of the temporal properties of the model lightcurves to that of the observed variability, we constrain the physical size ofthe corona and the distances of the emitting regions from the central BH.

The First Post-Kepler Brightness Dips of KIC 8462852

  We present a photometric detection of the first brightness dips of the uniquevariable star KIC 8462852 since the end of the Kepler space mission in 2013May. Our regular photometric surveillance started in October 2015, and asequence of dipping began in 2017 May continuing on through the end of 2017,when the star was no longer visible from Earth. We distinguish four main 1-2.5%dips, named "Elsie," "Celeste," "Skara Brae," and "Angkor", which persist ontimescales from several days to weeks. Our main results so far are: (i) thereare no apparent changes of the stellar spectrum or polarization during thedips; (ii) the multiband photometry of the dips shows differential reddeningfavoring non-grey extinction. Therefore, our data are inconsistent with dipmodels that invoke optically thick material, but rather they are in-line withpredictions for an occulter consisting primarily of ordinary dust, where muchof the material must be optically thin with a size scale <<1um, and may also beconsistent with models invoking variations intrinsic to the stellarphotosphere. Notably, our data do not place constraints on the color of thelonger-term "secular" dimming, which may be caused by independent processes, orprobe different regimes of a single process.

