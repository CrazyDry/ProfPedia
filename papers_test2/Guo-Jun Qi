Learning from Collective Intelligence in Groups

  Collective intelligence, which aggregates the shared information from largecrowds, is often negatively impacted by unreliable information sources with thelow quality data. This becomes a barrier to the effective use of collectiveintelligence in a variety of applications. In order to address this issue, wepropose a probabilistic model to jointly assess the reliability of sources andfind the true data. We observe that different sources are often not independentof each other. Instead, sources are prone to be mutually influenced, whichmakes them dependent when sharing information with each other. High dependencybetween sources makes collective intelligence vulnerable to the overuse ofredundant (and possibly incorrect) information from the dependent sources.Thus, we reveal the latent group structure among dependent sources, andaggregate the information at the group level rather than from individualsources directly. This can prevent the collective intelligence from beinginappropriately dominated by dependent sources. We will also explicitly revealthe reliability of groups, and minimize the negative impacts of unreliablegroups. Experimental results on real-world data sets show the effectiveness ofthe proposed approach with respect to existing algorithms.

Differential Recurrent Neural Networks for Action Recognition

  The long short-term memory (LSTM) neural network is capable of processingcomplex sequential information since it utilizes special gating schemes forlearning representations from long input sequences. It has the potential tomodel any sequential time-series data, where the current hidden state has to beconsidered in the context of the past hidden states. This property makes LSTMan ideal choice to learn the complex dynamics of various actions.Unfortunately, the conventional LSTMs do not consider the impact ofspatio-temporal dynamics corresponding to the given salient motion patterns,when they gate the information that ought to be memorized through time. Toaddress this problem, we propose a differential gating scheme for the LSTMneural network, which emphasizes on the change in information gain caused bythe salient motions between the successive frames. This change in informationgain is quantified by Derivative of States (DoS), and thus the proposed LSTMmodel is termed as differential Recurrent Neural Network (dRNN). We demonstratethe effectiveness of the proposed model by automatically recognizing actionsfrom the real-world 2D and 3D human action datasets. Our study is one of thefirst works towards demonstrating the potential of learning complex time-seriesrepresentations via high-order derivatives of states.

Task-Agnostic Meta-Learning for Few-shot Learning

  Meta-learning approaches have been proposed to tackle the few-shot learningproblem.Typically, a meta-learner is trained on a variety of tasks in the hopesof being generalizable to new tasks. However, the generalizability on new tasksof a meta-learner could be fragile when it is over-trained on existing tasksduring meta-training phase. In other words, the initial model of a meta-learnercould be too biased towards existing tasks to adapt to new tasks, especiallywhen only very few examples are available to update the model. To avoid abiased meta-learner and improve its generalizability, we propose a novelparadigm of Task-Agnostic Meta-Learning (TAML) algorithms. Specifically, wepresent an entropy-based approach that meta-learns an unbiased initial modelwith the largest uncertainty over the output labels by preventing it fromover-performing in classification tasks. Alternatively, a more generalinequality-minimization TAML is presented for more ubiquitous scenarios bydirectly minimizing the inequality of initial losses beyond the classificationtasks wherever a suitable loss can be defined.Experiments on benchmarkeddatasets demonstrate that the proposed approaches outperform comparedmeta-learning algorithms in both few-shot classification and reinforcementlearning tasks.

AVT: Unsupervised Learning of Transformation Equivariant Representations  by Autoencoding Variational Transformations

  The learning of Transformation-Equivariant Representations (TERs), which isintroduced by Hinton et al. \cite{hinton2011transforming}, has been consideredas a principle to reveal visual structures under various transformations. Itcontains the celebrated Convolutional Neural Networks (CNNs) as a special casethat only equivary to the translations. In contrast, we seek to train TERs fora generic class of transformations and train them in an {\em unsupervised}fashion. To this end, we present a novel principled method by AutoencodingVariational Transformations (AVT), compared with the conventional approach toautoencoding data. Formally, given transformed images, the AVT seeks to trainthe networks by maximizing the mutual information between the transformationsand representations. This ensures the resultant TERs of individual imagescontain the {\em intrinsic} information about their visual structures thatwould equivary {\em extricably} under various transformations. Technically, weshow that the resultant optimization problem can be efficiently solved bymaximizing a variational lower-bound of the mutual information. Thisvariational approach introduces a transformation decoder to approximate theintractable posterior of transformations, resulting in an autoencodingarchitecture with a pair of the representation encoder and the transformationdecoder. Experiments demonstrate the proposed AVT model sets a new record forthe performances on unsupervised tasks, greatly closing the performance gap tothe supervised models.

Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities

  In this paper, we present the Lipschitz regularization theory and algorithmsfor a novel Loss-Sensitive Generative Adversarial Network (LS-GAN).Specifically, it trains a loss function to distinguish between real and fakesamples by designated margins, while learning a generator alternately toproduce realistic samples by minimizing their losses. The LS-GAN furtherregularizes its loss function with a Lipschitz regularity condition on thedensity of real data, yielding a regularized model that can better generalizeto produce new data from a reasonable number of training examples than theclassic GAN. We will further present a Generalized LS-GAN (GLS-GAN) and show itcontains a large family of regularized GAN models, including both LS-GAN andWasserstein GAN, as its special cases. Compared with the other GAN models, wewill conduct experiments to show both LS-GAN and GLS-GAN exhibit competitiveability in generating new images in terms of the Minimum Reconstruction Error(MRE) assessed on a separate test set. We further extend the LS-GAN to aconditional form for supervised and semi-supervised learning problems, anddemonstrate its outstanding performance on image classification tasks.

Joint Intermodal and Intramodal Label Transfers for Extremely Rare or  Unseen Classes

  In this paper, we present a label transfer model from texts to images forimage classification tasks. The problem of image classification is often muchmore challenging than text classification. On one hand, labeled text data ismore widely available than the labeled images for classification tasks. On theother hand, text data tends to have natural semantic interpretability, and theyare often more directly related to class labels. On the contrary, the imagefeatures are not directly related to concepts inherent in class labels. One ofour goals in this paper is to develop a model for revealing the functionalrelationships between text and image features as to directly transferintermodal and intramodal labels to annotate the images. This is implemented bylearning a transfer function as a bridge to propagate the labels between twomultimodal spaces. However, the intermodal label transfers could be underminedby blindly transferring the labels of noisy texts to annotate images. Tomitigate this problem, we present an intramodal label transfer process, whichcomplements the intermodal label transfer by transferring the image labelsinstead when relevant text is absent from the source corpus. In addition, wegeneralize the inter-modal label transfer to zero-shot learning scenario wherethere are only text examples available to label unseen classes of imageswithout any positive image examples. We evaluate our algorithm on an imageclassification task and show the effectiveness with respect to the othercompared algorithms.

Global versus Localized Generative Adversarial Nets

  In this paper, we present a novel localized Generative Adversarial Net (GAN)to learn on the manifold of real data. Compared with the classic GAN that {\emglobally} parameterizes a manifold, the Localized GAN (LGAN) uses localcoordinate charts to parameterize distinct local geometry of how data pointscan transform at different locations on the manifold. Specifically, around eachpoint there exists a {\em local} generator that can produce data followingdiverse patterns of transformations on the manifold. The locality nature ofLGAN enables local generators to adapt to and directly access the localgeometry without need to invert the generator in a global GAN. Furthermore, itcan prevent the manifold from being locally collapsed to a dimensionallydeficient tangent subspace by imposing an orthonormality prior betweentangents. This provides a geometric approach to alleviating mode collapse atleast locally on the manifold by imposing independence between datatransformations in different tangent directions. We will also demonstrate theLGAN can be applied to train a robust classifier that prefers locallyconsistent classification decisions on the manifold, and the resultantregularizer is closely related with the Laplace-Beltrami operator. Ourexperiments show that the proposed LGANs can not only produce diverse imagetransformations, but also deliver superior classification performances.

AET vs. AED: Unsupervised Representation Learning by Auto-Encoding  Transformations rather than Data

  The success of deep neural networks often relies on a large amount of labeledexamples, which can be difficult to obtain in many real scenarios. To addressthis challenge, unsupervised methods are strongly preferred for training neuralnetworks without using any labeled data. In this paper, we present a novelparadigm of unsupervised representation learning by Auto-EncodingTransformation (AET) in contrast to the conventional Auto-Encoding Data (AED)approach. Given a randomly sampled transformation, AET seeks to predict itmerely from the encoded features as accurately as possible at the output end.The idea is the following: as long as the unsupervised features successfullyencode the essential information about the visual structures of original andtransformed images, the transformation can be well predicted. We will show thatthis AET paradigm allows us to instantiate a large variety of transformations,from parameterized, to non-parameterized and GAN-induced ones. Our experimentsshow that AET greatly improves over existing unsupervised approaches, settingnew state-of-the-art performances being greatly closer to the upper bounds bytheir fully supervised counterparts on CIFAR-10, ImageNet and Places datasets.

Small Data Challenges in Big Data Era: A Survey of Recent Progress on  Unsupervised and Semi-Supervised Methods

  Small data challenges have emerged in many learning problems, since thesuccess of deep neural networks often relies on the availability of a hugeamount of labeled data that is expensive to collect. To address it, manyefforts have been made on training complex models with small data in anunsupervised and semi-supervised fashion. In this paper, we will review therecent progresses on these two major categories of methods. A wide spectrum ofsmall data models will be categorized in a big picture, where we will show howthey interplay with each other to motivate explorations of new ideas. We willreview the criteria of learning the transformation equivariant, disentangled,self-supervised and semi-supervised representations, which underpin thefoundations of recent developments. Many instantiations of unsupervised andsemi-supervised generative models have been developed on the basis of thesecriteria, greatly expanding the territory of existing autoencoders, generativeadversarial nets (GANs) and other deep networks by exploring the distributionof unlabeled data for more powerful representations. While we focus on theunsupervised and semi-supervised methods, we will also provide a broader reviewof other emerging topics, from unsupervised and semi-supervised domainadaptation to the fundamental roles of transformation equivariance andinvariance in training a wide spectrum of deep networks. It is impossible forus to write an exclusive encyclopedia to include all related works. Instead, weaim at exploring the main ideas, principles and methods in this area to revealwhere we are heading on the journey towards addressing the small datachallenges in this big data era.

CapProNet: Deep Feature Learning via Orthogonal Projections onto Capsule  Subspaces

  In this paper, we formalize the idea behind capsule nets of using a capsulevector rather than a neuron activation to predict the label of samples. To thisend, we propose to learn a group of capsule subspaces onto which an inputfeature vector is projected. Then the lengths of resultant capsules are used toscore the probability of belonging to different classes. We train such aCapsule Projection Network (CapProNet) by learning an orthogonal projectionmatrix for each capsule subspace, and show that each capsule subspace isupdated until it contains input feature vectors corresponding to the associatedclass. We will also show that the capsule projection can be viewed asnormalizing the multiple columns of the weight matrix simultaneously to form anorthogonal basis, which makes it more effective in incorporating novelcomponents of input features to update capsule representations. In other words,the capsule projection can be viewed as a multi-dimensional weightnormalization in capsule subspaces, where the conventional weight normalizationis simply a special case of the capsule projection onto 1D lines. Only a smallnegligible computing overhead is incurred to train the network inlow-dimensional capsule subspaces or through an alternative hyper-poweriteration to estimate the normalization matrix. Experiment results on imagedatasets show the presented model can greatly improve the performance of thestate-of-the-art ResNet backbones by $10-20\%$ and that of the Densenet by$5-7\%$ respectively at the same level of computing and memory expenses. TheCapProNet establishes the competitive state-of-the-art performance for thefamily of capsule nets by significantly reducing test errors on the benchmarkdatasets.

