The HRC Model Set for Human-Robot Collaboration Research

  In this paper, we present a model set for designing human-robot collaboration(HRC) experiments. It targets a common scenario in HRC, which is thecollaborative assembly of furniture, and it consists of a combination ofstandard components and custom designs. With this work, we aim at reducing theamount of work required to set up and reproduce HRC experiments, and we providea unified framework to facilitate the comparison and integration ofcontributions to the field. The model set is designed to be modular,extendable, and easy to distribute. Importantly, it covers the majority ofrelevant research in HRC, and it allows tuning of a number of experimentalvariables that are particularly valuable to the field. Additionally, we providea set of software libraries for perception, control and interaction, with thegoal of encouraging other researchers to proactively contribute to our work.

How to be Helpful? Implementing Supportive Behaviors for Human-Robot  Collaboration

  The field of Human-Robot Collaboration (HRC) has seen a considerable amountof progress in the recent years. Although genuinely collaborative platforms arefar from being deployed in real-world scenarios, advances in control andperception algorithms have progressively popularized robots in manufacturingsettings, where they work side by side with human peers to achieve sharedtasks. Unfortunately, little progress has been made toward the development ofsystems that are proactive in their collaboration, and autonomously take careof some of the chores that compose most of the collaboration tasks. In thiswork, we present a collaborative system capable of assisting the human partnerwith a variety of supportive behaviors in spite of its limited perceptual andmanipulation capabilities and incomplete model of the task. Our frameworkleverages information from a high-level, hierarchical model of the task. Themodel, that is shared between the human and robot, enables transparentsynchronization between the peers and understanding of each other's plan. Moreprecisely, we derive a partially observable Markov model from the high-leveltask representation. We then use an online solver to compute a robot policy,that is robust to unexpected observations such as inaccuracies of perception,failures in object manipulations, as well as discovers hidden user preferences.We demonstrate that the system is capable of robustly providing support to thehuman in a furniture construction task.

That's Mine! Learning Ownership Relations and Norms for Robots

  The ability for autonomous agents to learn and conform to human norms iscrucial for their safety and effectiveness in social environments. While recentwork has led to frameworks for the representation and inference of simplesocial rules, research into norm learning remains at an exploratory stage.Here, we present a robotic system capable of representing, learning, andinferring ownership relations and norms. Ownership is represented as a graph ofprobabilistic relations between objects and their owners, along with a databaseof predicate-based norms that constrain the actions permissible on ownedobjects. To learn these norms and relations, our system integrates (i) a novelincremental norm learning algorithm capable of both one-shot learning andinduction from specific examples, (ii) Bayesian inference of ownershiprelations in response to apparent rule violations, and (iii) percept-basedprediction of an object's likely owners. Through a series of simulated andreal-world experiments, we demonstrate the competence and flexibility of thesystem in performing object manipulation tasks that require a variety of normsto be followed, laying the groundwork for future research into the acquisitionand application of social norms.

