GMine: A System for Scalable, Interactive Graph Visualization and Mining

  Several graph visualization tools exist. However, they are not able to handlelarge graphs, and/or they do not allow interaction. We are interested on largegraphs, with hundreds of thousands of nodes. Such graphs bring two challenges:the first one is that any straightforward interactive manipulation will beprohibitively slow. The second one is sensory overload: even if we could plotand replot the graph quickly, the user would be overwhelmed with the vastvolume of information because the screen would be too cluttered as nodes andedges overlap each other. GMine system addresses both these issues, by usingsummarization and multi-resolution. GMine offers multi-resolution graphexploration by partitioning a given graph into a hierarchy ofcom-munities-within-communities and storing it into a novel R-tree-likestructure which we name G-Tree. GMine offers summarization by implementing aninnovative subgraph extraction algorithm and then visualizing its output.

MaTrust: An Effective Multi-Aspect Trust Inference Model

  Trust is a fundamental concept in many real-world applications such ase-commerce and peer-to-peer networks. In these applications, users can generatelocal opinions about the counterparts based on direct experiences, and theseopinions can then be aggregated to build trust among unknown users. Themechanism to build new trust relationships based on existing ones is referredto as trust inference. State-of-the-art trust inference approaches employ thetransitivity property of trust by propagating trust along connected users. Inthis paper, we propose a novel trust inference model (MaTrust) by exploring anequally important property of trust, i.e., the multi-aspect property. MaTrustdirectly characterizes multiple latent factors for each trustor and trusteefrom the locally-generated trust relationships. Furthermore, it can naturallyincorporate prior knowledge as specified factors. These factors in turn serveas the basis to infer the unseen trustworthiness scores. Experimentalevaluations on real data sets show that the proposed MaTrust significantlyoutperforms several benchmark trust inference models in both effectiveness andefficiency.

Want a Good Answer? Ask a Good Question First!

  Community Question Answering (CQA) websites have become valuable repositorieswhich host a massive volume of human knowledge. To maximize the utility of suchknowledge, it is essential to evaluate the quality of an existing question oranswer, especially soon after it is posted on the CQA website.  In this paper, we study the problem of inferring the quality of questions andanswers through a case study of a software CQA (Stack Overflow). Our keyfinding is that the quality of an answer is strongly positively correlated withthat of its question. Armed with this observation, we propose a family ofalgorithms to jointly predict the quality of questions and answers, for bothquantifying numerical quality scores and differentiating the high-qualityquestions/answers from those of low quality. We conduct extensive experimentalevaluations to demonstrate the effectiveness and efficiency of our methods.

The Links Have It: Infobox Generation by Summarization over Linked  Entities

  Online encyclopedia such as Wikipedia has become one of the best sources ofknowledge. Much effort has been devoted to expanding and enriching thestructured data by automatic information extraction from unstructured text inWikipedia. Although remarkable progresses have been made, their effectivenessand efficiency is still limited as they try to tackle an extremely difficultnatural language understanding problems and heavily relies on supervisedlearning approaches which require large amount effort to label the trainingdata. In this paper, instead of performing information extraction overunstructured natural language text directly, we focus on a rich set ofsemi-structured data in Wikipedia articles: linked entities. The idea of thispaper is the following: If we can summarize the relationship between the entityand its linked entities, we immediately harvest some of the most importantinformation about the entity. To this end, we propose a novel rank aggregationapproach to remove noise, an effective clustering and labeling algorithm toextract knowledge.

Replacing the Irreplaceable: Fast Algorithms for Team Member  Recommendation

  In this paper, we study the problem of Team Member Replacement: given a teamof people embedded in a social network working on the same task, find a goodcandidate who can fit in the team after one team member becomes unavailable. Weconjecture that a good team member replacement should have good skill matchingas well as good structure matching. We formulate this problem using the conceptof graph kernel. To tackle the computational challenges, we propose a family offast algorithms by (a) designing effective pruning strategies, and (b)exploring the smoothness between the existing and the new team structures. Weconduct extensive experimental evaluations on real world datasets todemonstrate the effectiveness and efficiency. Our algorithms (a) performsignificantly better than the alternative choices in terms of both precisionand recall; and (b) scale sub-linearly.

The Child is Father of the Man: Foresee the Success at the Early Stage

  Understanding the dynamic mechanisms that drive the high-impact scientificwork (e.g., research papers, patents) is a long-debated research topic and hasmany important implications, ranging from personal career development andrecruitment search, to the jurisdiction of research resources. Recent advancesin characterizing and modeling scientific success have made it possible toforecast the long-term impact of scientific work, where data mining techniques,supervised learning in particular, play an essential role. Despite muchprogress, several key algorithmic challenges in relation to predictinglong-term scientific impact have largely remained open. In this paper, wepropose a joint predictive model to forecast the long-term scientific impact atthe early stage, which simultaneously addresses a number of these openchallenges, including the scholarly feature design, the non-linearity, thedomain-heterogeneity and dynamics. In particular, we formulate it as aregularized optimization problem and propose effective and scalable algorithmsto solve it. We perform extensive empirical evaluations on large, realscholarly data sets to validate the effectiveness and the efficiency of ourmethod.

Panther: Fast Top-k Similarity Search in Large Networks

  Estimating similarity between vertices is a fundamental issue in networkanalysis across various domains, such as social networks and biologicalnetworks. Methods based on common neighbors and structural contexts havereceived much attention. However, both categories of methods are difficult toscale up to handle large networks (with billions of nodes). In this paper, wepropose a sampling method that provably and accurately estimates the similaritybetween vertices. The algorithm is based on a novel idea of random path, and anextended method is also presented, to enhance the structural similarity whentwo vertices are completely disconnected. We provide theoretical proofs for theerror-bound and confidence of the proposed algorithm. We perform extensiveempirical study and show that our algorithm can obtain top-k similar verticesfor any vertex in a network approximately 300x faster than state-of-the-artmethods. We also use identity resolution and structural hole spanner finding,two important applications in social networks, to evaluate the accuracy of theestimated similarities. Our experimental results demonstrate that the proposedalgorithm achieves clearly better performance than several alternative methods.

Local Partition in Rich Graphs

  Local graph partitioning is a key graph mining tool that allows researchersto identify small groups of interrelated nodes (e.g. people) and theirconnective edges (e.g. interactions). Because local graph partitioning isprimarily focused on the network structure of the graph (vertices and edges),it often fails to consider the additional information contained in theattributes. In this paper we propose---(i) a scalable algorithm to improvelocal graph partitioning by taking into account both the network structure ofthe graph and the attribute data and (ii) an application of the proposed localgraph partitioning algorithm (AttriPart) to predict the evolution of localcommunities (LocalForecasting). Experimental results show that our proposedAttriPart algorithm finds up to 1.6$\times$ denser local partitions, whilerunning approximately 43$\times$ faster than traditional local partitioningtechniques (PageRank-Nibble). In addition, our LocalForecasting algorithm showsa significant improvement in the number of nodes and edges correctly predictedover baseline methods.

EXTRA: Explaining Team Recommendation in Networks

  State-of-the-art in network science of teams offers effective recommendationmethods to answer questions like who is the best replacement, what is the bestteam expansion strategy, but lacks intuitive ways to explain why theoptimization algorithm gives the specific recommendation for a given teamoptimization scenario. To tackle this problem, we develop an interactiveprototype system, EXTRA, as the first step towards addressing such asense-making challenge, through the lens of the underlying network where teamsembed, to explain the team recommendation results. The main advantages are (1)Algorithm efficacy: we propose an effective and fast algorithm to explainrandom walk graph kernel, the central technique for networked teamrecommendation; (2) Intuitive visual explanation: we present intuitive visualanalysis of the recommendation results, which can help users better understandthe rationality of the underlying team recommendation algorithm.

Graph-based Anomaly Detection and Description: A Survey

  Detecting anomalies in data is a vital task, with numerous high-impactapplications in areas such as security, finance, health care, and lawenforcement. While numerous techniques have been developed in past years forspotting outliers and anomalies in unstructured collections ofmulti-dimensional points, with graph data becoming ubiquitous, techniques forstructured {\em graph} data have been of focus recently. As objects in graphshave long-range correlations, a suite of novel technology has been developedfor anomaly detection in graph data.  This survey aims to provide a general, comprehensive, and structured overviewof the state-of-the-art methods for anomaly detection in data represented asgraphs. As a key contribution, we provide a comprehensive exploration of bothdata mining and machine learning algorithms for these {\em detection} tasks. wegive a general framework for the algorithms categorized under various settings:unsupervised vs. (semi-)supervised approaches, for static vs. dynamic graphs,for attributed vs. plain graphs. We highlight the effectiveness, scalability,generality, and robustness aspects of the methods. What is more, we stress theimportance of anomaly {\em attribution} and highlight the major techniques thatfacilitate digging out the root cause, or the `why', of the detected anomaliesfor further analysis and sense-making. Finally, we present several real-worldapplications of graph-based anomaly detection in diverse domains, includingfinancial, auction, computer traffic, and social networks. We conclude oursurvey with a discussion on open theoretical and practical challenges in thefield.

Flow-based Influence Graph Visual Summarization

  Visually mining a large influence graph is appealing yet challenging. Peopleare amazed by pictures of newscasting graph on Twitter, engaged by hiddencitation networks in academics, nevertheless often troubled by the unpleasantreadability of the underlying visualization. Existing summarization methodsenhance the graph visualization with blocked views, but have adverse effect onthe latent influence structure. How can we visually summarize a large graph tomaximize influence flows? In particular, how can we illustrate the impact of anindividual node through the summarization? Can we maintain the appealing graphmetaphor while preserving both the overall influence pattern and finereadability?  To answer these questions, we first formally define the influence graphsummarization problem. Second, we propose an end-to-end framework to solve thenew problem. Our method can not only highlight the flow-based influencepatterns in the visual summarization, but also inherently support rich graphattributes. Last, we present a theoretic analysis and report our experimentresults. Both evidences demonstrate that our framework can effectivelyapproximate the proposed influence graph summarization objective whileoutperforming previous methods in a typical scenario of visually miningacademic citation networks.

Structured Low-Rank Matrix Factorization with Missing and Grossly  Corrupted Observations

  Recovering low-rank and sparse matrices from incomplete or corruptedobservations is an important problem in machine learning, statistics,bioinformatics, computer vision, as well as signal and image processing. Intheory, this problem can be solved by the natural convex joint/mixedrelaxations (i.e., l_{1}-norm and trace norm) under certain conditions.However, all current provable algorithms suffer from superlinear per-iterationcost, which severely limits their applicability to large-scale problems. Inthis paper, we propose a scalable, provable structured low-rank matrixfactorization method to recover low-rank and sparse matrices from missing andgrossly corrupted data, i.e., robust matrix completion (RMC) problems, orincomplete and grossly corrupted measurements, i.e., compressive principalcomponent pursuit (CPCP) problems. Specifically, we first present twosmall-scale matrix trace norm regularized bilinear structured factorizationmodels for RMC and CPCP problems, in which repetitively calculating SVD of alarge-scale matrix is replaced by updating two much smaller factor matrices.Then, we apply the alternating direction method of multipliers (ADMM) toefficiently solve the RMC problems. Finally, we provide the convergenceanalysis of our algorithm, and extend it to address general CPCP problems.Experimental results verified both the efficiency and effectiveness of ourmethod compared with the state-of-the-art methods.

Large Graph Analysis in the GMine System

  Current applications have produced graphs on the order of hundreds ofthousands of nodes and millions of edges. To take advantage of such graphs, onemust be able to find patterns, outliers and communities. These tasks are betterperformed in an interactive environment, where human expertise can guide theprocess. For large graphs, though, there are some challenges: the excessiveprocessing requirements are prohibitive, and drawing hundred-thousand nodesresults in cluttered images hard to comprehend. To cope with these problems, wepropose an innovative framework suited for any kind of tree-like graph visualdesign. GMine integrates (a) a representation for graphs organized ashierarchies of partitions - the concepts of SuperGraph and Graph-Tree; and (b)a graph summarization methodology - CEPS. Our graph representation deals withthe problem of tracing the connection aspects of a graph hierarchy with sublinear complexity, allowing one to grasp the neighborhood of a single node orof a group of nodes in a single click. As a proof of concept, the visualenvironment of GMine is instantiated as a system in which large graphs can beinvestigated globally and locally.

AURORA: Auditing PageRank on Large Graphs

  Ranking on large-scale graphs plays a fundamental role in many high-impactapplication domains, ranging from information retrieval, recommender systems,sports team management, biology to neuroscience and many more. PageRank,together with many of its random walk based variants, has become one of themost well-known and widely used algorithms, due to its mathematical eleganceand the superior performance across a variety of application domains. Importantas it might be, state-of-the-art lacks an intuitive way to explain the rankingresults by PageRank (or its variants), e.g., why it thinks the returned top-kwebpages are most important ones in the entire graph; why it gives a higherrank to actor John than actor Smith in terms of their relevance w.r.t. aparticular movie? In order to answer these questions, this paper proposes aparadigm shift for PageRank, from identifying which nodes are most important tounderstanding why the ranking algorithm gives a particular ranking result. Weformally define the PageRank auditing problem, whose central idea is toidentify a set of key graph elements (e.g., edges, nodes, subgraphs) with thehighest influence on the ranking results. We formulate it as an optimizationproblem and propose a family of effective and scalable algorithms (AURORA) tosolve it. Our algorithms measure the influence of graph elements andincrementally select influential elements w.r.t. their gradients over theranking results. We perform extensive empirical evaluations on real-worlddatasets, which demonstrate that the proposed methods (AURORA) provideintuitive explanations with a linear scalability.

