Information-Theoretical Learning of Discriminative Clusters for  Unsupervised Domain Adaptation

  We study the problem of unsupervised domain adaptation, which aims to adaptclassifiers trained on a labeled source domain to an unlabeled target domain.Many existing approaches first learn domain-invariant features and thenconstruct classifiers with them. We propose a novel approach that jointly learnthe both. Specifically, while the method identifies a feature space where datain the source and the target domains are similarly distributed, it also learnsthe feature space discriminatively, optimizing an information-theoretic metricas an proxy to the expected misclassification error on the target domain. Weshow how this optimization can be effectively carried out with simplegradient-based methods and how hyperparameters can be cross-validated withoutdemanding any labeled data from the target domain. Empirical studies onbenchmark tasks of object recognition and sentiment analysis validated ourmodeling assumptions and demonstrated significant improvement of our methodover competing ones in classification accuracies.

Large-Margin Determinantal Point Processes

  Determinantal point processes (DPPs) offer a powerful approach to modelingdiversity in many applications where the goal is to select a diverse subset. Westudy the problem of learning the parameters (the kernel matrix) of a DPP fromlabeled training data. We make two contributions. First, we show how toreparameterize a DPP's kernel matrix with multiple kernel functions, thusenhancing modeling flexibility. Second, we propose a novel parameter estimationtechnique based on the principle of large margin separation. In contrast to thestate-of-the-art method of maximum likelihood estimation, our large-margin lossfunction explicitly models errors in selecting the target subsets, and it canbe customized to trade off different types of errors (precision vs. recall).Extensive empirical studies validate our contributions, including applicationson challenging document and video summarization, where flexibility in modelingthe kernel matrix and balancing different errors is indispensable.

Aligning where to see and what to tell: image caption with region-based  attention and scene factorization

  Recent progress on automatic generation of image captions has shown that itis possible to describe the most salient information conveyed by images withaccurate and meaningful sentences. In this paper, we propose an image captionsystem that exploits the parallel structures between images and sentences. Inour model, the process of generating the next word, given the previouslygenerated ones, is aligned with the visual perception experience where theattention shifting among the visual regions imposes a thread of visualordering. This alignment characterizes the flow of "abstract meaning", encodingwhat is semantically shared by both the visual scene and the text description.Our system also makes another novel modeling contribution by introducingscene-specific contexts that capture higher-level semantic information encodedin an image. The contexts adapt language models for word generation to specificscene types. We benchmark our system and contrast to published results onseveral popular datasets. We show that using either region-based attention orscene-specific contexts improves systems without those components. Furthermore,combining these two modeling ingredients attains the state-of-the-artperformance.

Aiming to Know You Better Perhaps Makes Me a More Engaging Dialogue  Partner

  There have been several attempts to define a plausible motivation for achit-chat dialogue agent that can lead to engaging conversations. In this work,we explore a new direction where the agent specifically focuses on discoveringinformation about its interlocutor. We formalize this approach by defining aquantitative metric. We propose an algorithm for the agent to maximize it. Wevalidate the idea with human evaluation where our system outperforms variousbaselines. We demonstrate that the metric indeed correlates with the humanjudgments of engagingness.

Hyper-parameter Tuning under a Budget Constraint

  We study a budgeted hyper-parameter tuning problem, where we optimize thetuning result under a hard resource constraint. We propose to solve it as asequential decision making problem, such that we can use the partial trainingprogress of configurations to dynamically allocate the remaining budget. Ouralgorithm combines a Bayesian belief model which estimates the futureperformance of configurations, with an action-value function which balancesexploration-exploitation tradeoff, to optimize the final output. Itautomatically adapts the tuning behaviors to different constraints, which isuseful in practice. Experiment results demonstrate superior performance overexisting algorithms, including the-state-of-the-art one, on real-world tuningtasks across a range of different budgets.

How to Scale Up Kernel Methods to Be As Good As Deep Neural Nets

  The computational complexity of kernel methods has often been a major barrierfor applying them to large-scale learning problems. We argue that this barriercan be effectively overcome. In particular, we develop methods to scale upkernel models to successfully tackle large-scale learning problems that are sofar only approachable by deep learning architectures. Based on the seminal workby Rahimi and Recht on approximating kernel functions with features derivedfrom random projections, we advance the state-of-the-art by proposing methodsthat can efficiently train models with hundreds of millions of parameters, andlearn optimal representations from multiple kernels. We conduct extensiveempirical studies on problems from image recognition and automatic speechrecognition, and show that the performance of our kernel models matches that ofwell-engineered deep neural nets (DNNs). To the best of our knowledge, this isthe first time that a direct comparison between these two methods onlarge-scale problems is reported. Our kernel methods have several appealingproperties: training with convex optimization, cost for training a single modelcomparable to DNNs, and significantly reduced total cost due to fewerhyperparameters to tune for model selection. Our contrastive study betweenthese two very different but equally competitive models sheds light onfundamental questions such as how to learn good representations.

Rapid Feature Learning with Stacked Linear Denoisers

  We investigate unsupervised pre-training of deep architectures as featuregenerators for "shallow" classifiers. Stacked Denoising Autoencoders (SdA),when used as feature pre-processing tools for SVM classification, can lead tosignificant improvements in accuracy - however, at the price of a substantialincrease in computational cost. In this paper we create a simple algorithmwhich mimics the layer by layer training of SdAs. However, in contrast to SdAs,our algorithm requires no training through gradient descent as the parameterscan be computed in closed-form. It can be implemented in less than 20 lines ofMATLABTMand reduces the computation time from several hours to mere seconds. Weshow that our feature transformation reliably improves the results of SVMclassification significantly on all our data sets - often outperforming SdAsand even deep neural networks in three out of four deep learning benchmarks.

Learning Discriminative Metrics via Generative Models and Kernel  Learning

  Metrics specifying distances between data points can be learned in adiscriminative manner or from generative models. In this paper, we show how tounify generative and discriminative learning of metrics via a kernel learningframework. Specifically, we learn local metrics optimized from parametricgenerative models. These are then used as base kernels to construct a globalkernel that minimizes a discriminative training criterion. We consider bothlinear and nonlinear combinations of local metric kernels. Our empiricalresults show that these combinations significantly improve performance onclassification tasks. The proposed learning algorithm is also very efficient,achieving order of magnitude speedup in training time compared to previousdiscriminative baseline methods.

Marginalized Denoising Autoencoders for Domain Adaptation

  Stacked denoising autoencoders (SDAs) have been successfully used to learnnew representations for domain adaptation. Recently, they have attained recordaccuracy on standard benchmark tasks of sentiment analysis across differenttext domains. SDAs learn robust data representations by reconstruction,recovering original features from data that are artificially corrupted withnoise. In this paper, we propose marginalized SDA (mSDA) that addresses twocrucial limitations of SDAs: high computational cost and lack of scalability tohigh-dimensional features. In contrast to SDAs, our approach of mSDAmarginalizes noise and thus does not require stochastic gradient descent orother optimization algorithms to learn parameters ? in fact, they are computedin closed-form. Consequently, mSDA, which can be implemented in only 20 linesof MATLAB^{TM}, significantly speeds up SDAs by two orders of magnitude.Furthermore, the representations learnt by mSDA are as effective as thetraditional SDAs, attaining almost identical accuracies in benchmark tasks.

An alternative text representation to TF-IDF and Bag-of-Words

  In text mining, information retrieval, and machine learning, text documentsare commonly represented through variants of sparse Bag of Words (sBoW) vectors(e.g. TF-IDF). Although simple and intuitive, sBoW style representations sufferfrom their inherent over-sparsity and fail to capture word-level synonymy andpolysemy. Especially when labeled data is limited (e.g. in documentclassification), or the text documents are short (e.g. emails or abstracts),many features are rarely observed within the training corpus. This leads tooverfitting and reduced generalization accuracy. In this paper we propose DenseCohort of Terms (dCoT), an unsupervised algorithm to learn improved sBoWdocument features. dCoT explicitly models absent words by removing andreconstructing random sub-sets of words in the unlabeled corpus. With thisapproach, dCoT learns to reconstruct frequent words from co-occurringinfrequent words and maps the high dimensional sparse sBoW vectors into alow-dimensional dense representation. We show that the feature removal can bemarginalized out and that the reconstruction can be solved for in closed-form.We demonstrate empirically, on several benchmark datasets, that dCoT featuressignificantly improve the classification accuracy across several documentclassification tasks.

Demystifying Information-Theoretic Clustering

  We propose a novel method for clustering data which is grounded ininformation-theoretic principles and requires no parametric assumptions.Previous attempts to use information theory to define clusters in anassumption-free way are based on maximizing mutual information between data andcluster labels. We demonstrate that this intuition suffers from a fundamentalconceptual flaw that causes clustering performance to deteriorate as the amountof data increases. Instead, we return to the axiomatic foundations ofinformation theory to define a meaningful clustering measure based on thenotion of consistency under coarse-graining for finite data.

A Distributed Frank-Wolfe Algorithm for Communication-Efficient Sparse  Learning

  Learning sparse combinations is a frequent theme in machine learning. In thispaper, we study its associated optimization problem in the distributed settingwhere the elements to be combined are not centrally located but spread over anetwork. We address the key challenges of balancing communication costs andoptimization errors. To this end, we propose a distributed Frank-Wolfe (dFW)algorithm. We obtain theoretical guarantees on the optimization error$\epsilon$ and communication cost that do not depend on the total number ofcombining elements. We further show that the communication cost of dFW isoptimal by deriving a lower-bound on the communication cost required toconstruct an $\epsilon$-approximate solution. We validate our theoreticalanalysis with empirical studies on synthetic and real-world data, whichdemonstrate that dFW outperforms both baselines and competing methods. We alsostudy the performance of dFW when the conditions of our analysis are relaxed,and show that dFW is fairly robust.

Sparse Compositional Metric Learning

  We propose a new approach for metric learning by framing it as learning asparse combination of locally discriminative metrics that are inexpensive togenerate from the training data. This flexible framework allows us to naturallyderive formulations for global, multi-task and local metric learning. Theresulting algorithms have several advantages over existing methods in theliterature: a much smaller number of parameters to be estimated and aprincipled way to generalize learned metrics to new testing data points. Toanalyze the approach theoretically, we derive a generalization bound thatjustifies the sparse combination. Empirically, we evaluate our algorithms onseveral datasets against state-of-the-art metric learning methods. The resultsare consistent with our theoretical findings and demonstrate the superiority ofour approach in terms of classification performance and scalability.

Two-Stage Metric Learning

  In this paper, we present a novel two-stage metric learning algorithm. Wefirst map each learning instance to a probability distribution by computing itssimilarities to a set of fixed anchor points. Then, we define the distance inthe input data space as the Fisher information distance on the associatedstatistical manifold. This induces in the input data space a new family ofdistance metric with unique properties. Unlike kernelized metric learning, wedo not require the similarity measure to be positive semi-definite. Moreover,it can also be interpreted as a local metric learning algorithm with welldefined distance approximation. We evaluate its performance on a number ofdatasets. It outperforms significantly other metric learning methods and SVM.

Similarity Learning for High-Dimensional Sparse Data

  A good measure of similarity between data points is crucial to many tasks inmachine learning. Similarity and metric learning methods learn such measuresautomatically from data, but they do not scale well respect to thedimensionality of the data. In this paper, we propose a method that can learnefficiently similarity measure from high-dimensional sparse data. The core ideais to parameterize the similarity measure as a convex combination of rank-onematrices with specific sparsity structures. The parameters are then optimizedwith an approximate Frank-Wolfe procedure to maximally satisfy relativesimilarity constraints on the training data. Our algorithm greedilyincorporates one pair of features at a time into the similarity measure,providing an efficient way to control the number of active features and thusreduce overfitting. It enjoys very appealing convergence guarantees and itstime and memory complexity depends on the sparsity of the data instead of thedimension of the feature space. Our experiments on real-world high-dimensionaldatasets demonstrate its potential for classification, dimensionality reductionand data exploration.

Construction of Bound Entangled States Based on Permutation Operators

  We present a construction of new bound entangled states from given boundentangled states for arbitrary dimensional bipartite systems. One way toconstruct bound entangled states is to show that these states are PPT (positivepartial transpose) and violate the range criterion at the same time. Byapplying certain operators to given bound entangled states or to one of thesubsystems of the given bound entangled states, we obtain a set of new stateswhich are both PPT and violate the range criterion. We show that the derivedbound entangled states are not local unitary equivalent to the original boundentangled states by detail examples.

Synthesized Classifiers for Zero-Shot Learning

  Given semantic descriptions of object classes, zero-shot learning aims toaccurately recognize objects of the unseen classes, from which no examples areavailable at the training stage, by associating them to the seen classes, fromwhich labeled examples are provided. We propose to tackle this problem from theperspective of manifold learning. Our main idea is to align the semantic spacethat is derived from external information to the model space that concernsitself with recognizing visual features. To this end, we introduce a set of"phantom" object classes whose coordinates live in both the semantic space andthe model space. Serving as bases in a dictionary, they can be optimized fromlabeled data such that the synthesized real object classifiers achieve optimaldiscriminative performance. We demonstrate superior accuracy of our approachover the state of the art on four benchmark datasets for zero-shot learning,including the full ImageNet Fall 2011 dataset with more than 20,000 unseenclasses.

Summary Transfer: Exemplar-based Subset Selection for Video  Summarization

  Video summarization has unprecedented importance to help us digest, browse,and search today's ever-growing video collections. We propose a novel subsetselection technique that leverages supervision in the form of human-createdsummaries to perform automatic keyframe-based video summarization. The mainidea is to nonparametrically transfer summary structures from annotated videosto unseen test videos. We show how to extend our method to exploit semanticside information about the video's category/genre to guide the transfer processby those training videos semantically consistent with the test input. We alsoshow how to generalize our method to subshot-based summarization, which notonly reduces computational costs but also provides more flexible ways ofdefining visual similarity across subshots spanning several frames. We conductextensive evaluation on several benchmarks and demonstrate promising results,outperforming existing methods in several settings.

A Comparison between Deep Neural Nets and Kernel Acoustic Models for  Speech Recognition

  We study large-scale kernel methods for acoustic modeling and compare to DNNson performance metrics related to both acoustic modeling and recognition.Measuring perplexity and frame-level classification accuracy, kernel-basedacoustic models are as effective as their DNN counterparts. However, ontoken-error-rates DNN models can be significantly better. We have discoveredthat this might be attributed to DNN's unique strength in reducing both theperplexity and the entropy of the predicted posterior probabilities. Motivatedby our findings, we propose a new technique, entropy regularized perplexity,for model selection. This technique can noticeably improve the recognitionperformance of both types of models, and reduces the gap between them. Whileeffective on Broadcast News, this technique could be also applicable to othertasks.

An Empirical Study and Analysis of Generalized Zero-Shot Learning for  Object Recognition in the Wild

  Zero-shot learning (ZSL) methods have been studied in the unrealistic settingwhere test data are assumed to come from unseen classes only. In this paper, weadvocate studying the problem of generalized zero-shot learning (GZSL) wherethe test data's class memberships are unconstrained. We show empirically thatnaively using the classifiers constructed by ZSL approaches does not performwell in the generalized setting. Motivated by this, we propose a simple buteffective calibration method that can be used to balance two conflictingforces: recognizing data from seen classes versus those from unseen ones. Wedevelop a performance metric to characterize such a trade-off and examine theutility of this metric in evaluating various ZSL approaches. Our analysisfurther shows that there is a large gap between the performance of existingapproaches and an upper bound established via idealized semantic embeddings,suggesting that improving class semantic embeddings is vital to GZSL.

Video Summarization with Long Short-term Memory

  We propose a novel supervised learning technique for summarizing videos byautomatically selecting keyframes or key subshots. Casting the problem as astructured prediction problem on sequential data, our main idea is to use LongShort-Term Memory (LSTM), a special type of recurrent neural networks to modelthe variable-range dependencies entailed in the task of video summarization.Our learning models attain the state-of-the-art results on two benchmark videodatasets. Detailed analysis justifies the design of the models. In particular,we show that it is crucial to take into consideration the sequential structuresin videos and model them. Besides advances in modeling techniques, we introducetechniques to address the need of a large number of annotated data for trainingcomplex learning models. There, our main idea is to exploit the existence ofauxiliary annotated video datasets, albeit heterogeneous in visual styles andcontents. Specifically, we show domain adaptation techniques can improvesummarization by reducing the discrepancies in statistical properties acrossthose datasets.

Predicting Visual Exemplars of Unseen Classes for Zero-Shot Learning

  Leveraging class semantic descriptions and examples of known objects,zero-shot learning makes it possible to train a recognition model for an objectclass whose examples are not available. In this paper, we propose a novelzero-shot learning model that takes advantage of clustering structures in thesemantic embedding space. The key idea is to impose the structural constraintthat semantic representations must be predictive of the locations of theircorresponding visual exemplars. To this end, this reduces to training multiplekernel-based regressors from semantic representation-exemplar pairs fromlabeled data of the seen object categories. Despite its simplicity, ourapproach significantly outperforms existing zero-shot learning methods onstandard benchmark datasets, including the ImageNet dataset with more than20,000 unseen categories.

Attention Correctness in Neural Image Captioning

  Attention mechanisms have recently been introduced in deep learning forvarious tasks in natural language processing and computer vision. But despitetheir popularity, the "correctness" of the implicitly-learned attention mapshas only been assessed qualitatively by visualization of several examples. Inthis paper we focus on evaluating and improving the correctness of attention inneural image captioning models. Specifically, we propose a quantitativeevaluation metric for the consistency between the generated attention maps andhuman annotations, using recently released datasets with alignment betweenregions in images and entities in captions. We then propose novel models withdifferent levels of explicit supervision for learning attention maps duringtraining. The supervision can be strong when alignment between regions andcaption entities are available, or weak when only object segments andcategories are provided. We show on the popular Flickr30k and COCO datasetsthat introducing supervision of attention maps during training solidly improvesboth attention correctness and caption quality, showing the promise of makingmachine perception more human-like.

Recalling Holistic Information for Semantic Segmentation

  Semantic segmentation requires a detailed labeling of image pixels by objectcategory. Information derived from local image patches is necessary to describethe detailed shape of individual objects. However, this information isambiguous and can result in noisy labels. Global inference of image content caninstead capture the general semantic concepts present. We advocate thathigh-recall holistic inference of image concepts provides valuable informationfor detailed pixel labeling. We build a two-stream neural network architecturethat facilitates information flow from holistic information to local pixels,while keeping common image features shared among the low-level layers of boththe holistic analysis and segmentation branches. We empirically evaluate ournetwork on four standard semantic segmentation datasets. Our network obtainsstate-of-the-art performance on PASCAL-Context and NYUDv2, and ablation studiesverify its effectiveness on ADE20K and SIFT-Flow.

FastMask: Segment Multi-scale Object Candidates in One Shot

  Objects appear to scale differently in natural images. This fact requiresmethods dealing with object-centric tasks (e.g. object proposal) to have robustperformance over variances in object scales. In the paper, we present a novelsegment proposal framework, namely FastMask, which takes advantage ofhierarchical features in deep convolutional neural networks to segmentmulti-scale objects in one shot. Innovatively, we adapt segment proposalnetwork into three different functional components (body, neck and head). Wefurther propose a weight-shared residual neck module as well as ascale-tolerant attentional head module for efficient one-shot inference. On MSCOCO benchmark, the proposed FastMask outperforms all state-of-the-art segmentproposal methods in average recall being 2~5 times faster. Moreover, with aslight trade-off in accuracy, FastMask can segment objects in near real time(~13 fps) with 800*600 resolution images, demonstrating its potential inpractical applications. Our implementation is available onhttps://github.com/voidrank/FastMask.

LabelBank: Revisiting Global Perspectives for Semantic Segmentation

  Semantic segmentation requires a detailed labeling of image pixels by objectcategory. Information derived from local image patches is necessary to describethe detailed shape of individual objects. However, this information isambiguous and can result in noisy labels. Global inference of image content caninstead capture the general semantic concepts present. We advocate thatholistic inference of image concepts provides valuable information for detailedpixel labeling. We propose a generic framework to leverage holistic informationin the form of a LabelBank for pixel-level segmentation.  We show the ability of our framework to improve semantic segmentationperformance in a variety of settings. We learn models for extracting a holisticLabelBank from visual cues, attributes, and/or textual descriptions. Wedemonstrate improvements in semantic segmentation accuracy on standard datasetsacross a range of state-of-the-art segmentation architectures and holisticinference approaches.

Cross-Dataset Adaptation for Visual Question Answering

  We investigate the problem of cross-dataset adaptation for visual questionanswering (Visual QA). Our goal is to train a Visual QA model on a sourcedataset but apply it to another target one. Analogous to domain adaptation forvisual recognition, this setting is appealing when the target dataset does nothave a sufficient amount of labeled data to learn an "in-domain" model. The keychallenge is that the two datasets are constructed differently, resulting inthe cross-dataset mismatch on images, questions, or answers.  We overcome this difficulty by proposing a novel domain adaptation algorithm.Our method reduces the difference in statistical distributions by transformingthe feature representation of the data in the target dataset. Moreover, itmaximizes the likelihood of answering questions (in the target dataset)correctly using the Visual QA model trained on the source dataset. Weempirically studied the effectiveness of the proposed approach on adaptingamong several popular Visual QA datasets. We show that the proposed methodimproves over baselines where there is no adaptation and several otheradaptation methods. We both quantitatively and qualitatively analyze when theadaptation can be mostly effective.

Multi-Task Learning for Sequence Tagging: An Empirical Study

  We study three general multi-task learning (MTL) approaches on 11 sequencetagging tasks. Our extensive empirical results show that in about 50% of thecases, jointly learning all 11 tasks improves upon either independent orpairwise learning of the tasks. We also show that pairwise MTL can inform uswhat tasks can benefit others or what tasks can be benefited if they arelearned jointly. In particular, we identify tasks that can always benefitothers as well as tasks that can always be harmed by others. Interestingly, oneof our MTL approaches yields embeddings of the tasks that reveal the naturalclustering of semantic and syntactic tasks. Our inquiries have opened the doorsto further utilization of MTL in NLP.

Actor-Attention-Critic for Multi-Agent Reinforcement Learning

  Reinforcement learning in multi-agent scenarios is important for real-worldapplications but presents challenges beyond those seen in single-agentsettings. We present an actor-critic algorithm that trains decentralizedpolicies in multi-agent settings, using centrally computed critics that sharean attention mechanism which selects relevant information for each agent atevery timestep. This attention mechanism enables more effective and scalablelearning in complex multi-agent environments, when compared to recentapproaches. Our approach is applicable not only to cooperative settings withshared rewards, but also individualized reward settings, including adversarialsettings, and it makes no assumptions about the action spaces of the agents. Assuch, it is flexible enough to be applied to most multi-agent learningproblems.

Cross-Modal and Hierarchical Modeling of Video and Text

  Visual data and text data are composed of information at multiplegranularities. A video can describe a complex scene that is composed ofmultiple clips or shots, where each depicts a semantically coherent event oraction. Similarly, a paragraph may contain sentences with different topics,which collectively conveys a coherent message or story. In this paper, weinvestigate the modeling techniques for such hierarchical sequential data wherethere are correspondences across multiple modalities. Specifically, weintroduce hierarchical sequence embedding (HSE), a generic model for embeddingsequential data of different modalities into hierarchically semantic spaces,with either explicit or implicit correspondence information. We performempirical studies on large-scale video and paragraph retrieval datasets anddemonstrated superior performance by the proposed methods. Furthermore, weexamine the effectiveness of our learned embeddings when applied to downstreamtasks. We show its utility in zero-shot action recognition and videocaptioning.

Classifier and Exemplar Synthesis for Zero-Shot Learning

  Zero-shot learning (ZSL) enables solving a task without the need to see itsexamples. In this paper, we propose two ZSL frameworks that learn to synthesizeparameters for novel unseen classes. First, we propose to cast the problem ofZSL as learning manifold embeddings from graphs composed of object classes,leading to a flexible approach that synthesizes "classifiers" for the unseenclasses. Then, we define an auxiliary task of synthesizing "exemplars" for theunseen classes to be used as an automatic denoising mechanism for any existingZSL approaches or as an effective ZSL model by itself. On five visualrecognition benchmark datasets, we demonstrate the superior performances of ourproposed frameworks in various scenarios of both conventional and generalizedZSL. Finally, we provide valuable insights through a series of empiricalanalyses, among which are a comparison of semantic representations on the fullImageNet benchmark as well as a comparison of metrics used in generalized ZSL.Our code and data are publicly available athttps://github.com/pujols/Zero-shot-learning-journal

Synthesized Policies for Transfer and Adaptation across Tasks and  Environments

  The ability to transfer in reinforcement learning is key towards building anagent of general artificial intelligence. In this paper, we consider theproblem of learning to simultaneously transfer across both environments (ENV)and tasks (TASK), probably more importantly, by learning from only sparse (ENV,TASK) pairs out of all the possible combinations. We propose a novelcompositional neural network architecture which depicts a meta rule forcomposing policies from the environment and task embeddings. Notably, one ofthe main challenges is to learn the embeddings jointly with the meta rule. Wefurther propose new training methods to disentangle the embeddings, making themboth distinctive signatures of the environments and tasks and effectivebuilding blocks for composing the policies. Experiments on GridWorld and Thor,of which the agent takes as input an egocentric view, show that our approachgives rise to high success rates on all the (ENV, TASK) pairs after learningfrom only 40\% of them.

Understanding Image and Text Simultaneously: a Dual Vision-Language  Machine Comprehension Task

  We introduce a new multi-modal task for computer systems, posed as a combinedvision-language comprehension challenge: identifying the most suitable textdescribing a scene, given several similar options. Accomplishing the taskentails demonstrating comprehension beyond just recognizing "keywords" (orkey-phrases) and their corresponding visual concepts. Instead, it requires analignment between the representations of the two modalities that achieves avisually-grounded "understanding" of various linguistic elements and theirdependencies. This new task also admits an easy-to-compute and well-studiedmetric: the accuracy in detecting the true target among the decoys.  The paper makes several contributions: an effective and extensible mechanismfor generating decoys from (human-created) image captions; an instance ofapplying this mechanism, yielding a large-scale machine comprehension dataset(based on the COCO images and captions) that we make publicly available; humanevaluation results on this dataset, informing a performance upper-bound; andseveral baseline and competitive learning approaches that illustrate theutility of the proposed task and dataset in advancing both image and languagecomprehension. We also show that, in a multi-task learning setting, theperformance on the proposed task is positively correlated with the end-to-endtask of image captioning.

Being Negative but Constructively: Lessons Learnt from Creating Better  Visual Question Answering Datasets

  Visual question answering (Visual QA) has attracted a lot of attentionlately, seen essentially as a form of (visual) Turing test that artificialintelligence should strive to achieve. In this paper, we study a crucialcomponent of this task: how can we design good datasets for the task? We focuson the design of multiple-choice based datasets where the learner has to selectthe right answer from a set of candidate ones including the target (\ie thecorrect one) and the decoys (\ie the incorrect ones). Through careful analysisof the results attained by state-of-the-art learning models and humanannotators on existing datasets, we show that the design of the decoy answershas a significant impact on how and what the learning models learn from thedatasets. In particular, the resulting learner can ignore the visualinformation, the question, or both while still doing well on the task. Inspiredby this, we propose automatic procedures to remedy such design deficiencies. Weapply the procedures to re-construct decoy answers for two popular Visual QAdatasets as well as to create a new Visual QA dataset from the Visual Genomeproject, resulting in the largest dataset for this task. Extensive empiricalstudies show that the design deficiencies have been alleviated in the remedieddatasets and the performance on them is likely a more faithful indicator of thedifference among learning models. The datasets are released and publiclyavailable via http://www.teds.usc.edu/website_vqa/.

Learning Answer Embeddings for Visual Question Answering

  We propose a novel probabilistic model for visual question answering (VisualQA). The key idea is to infer two sets of embeddings: one for the image and thequestion jointly and the other for the answers. The learning objective is tolearn the best parameterization of those embeddings such that the correctanswer has higher likelihood among all possible answers. In contrast to severalexisting approaches of treating Visual QA as multi-way classification, theproposed approach takes the semantic relationships (as characterized by theembeddings) among answers into consideration, instead of viewing them asindependent ordinal numbers. Thus, the learned embedded function can be used toembed unseen answers (in the training dataset). These properties make theapproach particularly appealing for transfer learning for open-ended Visual QA,where the source dataset on which the model is learned has limited overlappingwith the target dataset in the space of answers. We have also developedlarge-scale optimization techniques for applying the model to datasets with alarge number of answers, where the challenge is to properly normalize theproposed probabilistic models. We validate our approach on several Visual QAdatasets and investigate its utility for transferring models across datasets.The empirical results have shown that the approach performs well not only onin-domain learning but also on transfer learning.

Learning Embedding Adaptation for Few-Shot Learning

  Learning with limited data is a key challenge for visual recognition.Few-shot learning methods address this challenge by learning an instanceembedding function from seen classes and apply the function to instances fromunseen classes with limited labels. This style of transfer learning istask-agnostic: the embedding function is not learned optimally discriminativewith respect to the unseen classes, where discerning among them is the targettask. In this paper, we propose a novel approach to adapt the embedding modelto the target classification task, yielding embeddings that are task-specificand are discriminative. To this end, we employ a type of self-attentionmechanism called Transformer to transform the embeddings from task-agnostic totask-specific by focusing on relating instances from the test instances to thetraining instances in both seen and unseen classes. We verify the effectivenessof our model on both the standard few-shot classification benchmark and fourextended few-shot learning settings with essential use cases (i.e.cross-domain, transductive, generalized few-shot learning, and large scalelow-shot learning). Our approach archived consistent improvements over baselinemodels and previous state-of-the-art methods.

Kernel Approximation Methods for Speech Recognition

  We study large-scale kernel methods for acoustic modeling in speechrecognition and compare their performance to deep neural networks (DNNs). Weperform experiments on four speech recognition datasets, including the TIMITand Broadcast News benchmark tasks, and compare these two types of models onframe-level performance metrics (accuracy, cross-entropy), as well as onrecognition metrics (word/character error rate). In order to scale kernelmethods to these large datasets, we use the random Fourier feature method ofRahimi and Recht (2007). We propose two novel techniques for improving theperformance of kernel acoustic models. First, in order to reduce the number ofrandom features required by kernel models, we propose a simple but effectivemethod for feature selection. The method is able to explore a large number ofnon-linear features while maintaining a compact model more efficiently thanexisting approaches. Second, we present a number of frame-level metrics whichcorrelate very strongly with recognition performance when computed on theheldout set; we take advantage of these correlations by monitoring thesemetrics during training in order to decide when to stop learning. Thistechnique can noticeably improve the recognition performance of both DNN andkernel models, while narrowing the gap between them. Additionally, we show thatthe linear bottleneck method of Sainath et al. (2013) improves the performanceof our kernel models significantly, in addition to speeding up training andmaking the models more compact. Together, these three methods dramaticallyimprove the performance of kernel acoustic models, making their performancecomparable to DNNs on the tasks we explored.

