Adaptation-Based Programming in Haskell

  We present an embedded DSL to support adaptation-based programming (ABP) inHaskell. ABP is an abstract model for defining adaptive values, calledadaptives, which adapt in response to some associated feedback. We show how ourdesign choices in Haskell motivate higher-level combinators and constructs andhelp us derive more complicated compositional adaptives.  We also show an important specialization of ABP is in support ofreinforcement learning constructs, which optimize adaptive values based on aprogrammer-specified objective function. This permits ABP users to easilydefine adaptive values that express uncertainty anywhere in their programs.Over repeated executions, these adaptive values adjust to more efficient onesand enable the user's programs to self optimize.  The design of our DSL depends significantly on the use of type classes. Wewill illustrate, along with presenting our DSL, how the use of type classes cansupport the gradual evolution of DSLs.

Output Space Search for Structured Prediction

  We consider a framework for structured prediction based on search in thespace of complete structured outputs. Given a structured input, an output isproduced by running a time-bounded search procedure guided by a learned costfunction, and then returning the least cost output uncovered during the search.This framework can be instantiated for a wide range of search spaces and searchprocedures, and easily incorporates arbitrary structured-prediction lossfunctions. In this paper, we make two main technical contributions. First, wedefine the limited-discrepancy search space over structured outputs, which isable to leverage powerful classification learning algorithms to improve thesearch space quality. Second, we give a generic cost function learningapproach, where the key idea is to learn a cost function that attempts to mimicthe behavior of conducting searches guided by the true loss function. Ourexperiments on six benchmark domains demonstrate that using our framework withonly a small amount of search is sufficient for significantly improving onstate-of-the-art structured-prediction performance.

Active Imitation Learning via Reduction to I.I.D. Active Learning

  In standard passive imitation learning, the goal is to learn a target policyby passively observing full execution trajectories of it. Unfortunately,generating such trajectories can require substantial expert effort and beimpractical in some cases. In this paper, we consider active imitation learningwith the goal of reducing this effort by querying the expert about the desiredaction at individual states, which are selected based on answers to pastqueries and the learner's interactions with an environment simulator. Weintroduce a new approach based on reducing active imitation learning to i.i.d.active learning, which can leverage progress in the i.i.d. setting. Our firstcontribution, is to analyze reductions for both non-stationary and stationarypolicies, showing that the label complexity (number of queries) of activeimitation learning can be substantially less than passive learning. Our secondcontribution, is to introduce a practical algorithm inspired by the reductions,which is shown to be highly effective in four test domains compared to a numberof alternatives.

Inferring Strategies from Limited Reconnaissance in Real-time Strategy  Games

  In typical real-time strategy (RTS) games, enemy units are visible only whenthey are within sight range of a friendly unit. Knowledge of an opponent'sdisposition is limited to what can be observed through scouting. Information iscostly, since units dedicated to scouting are unavailable for other purposes,and the enemy will resist scouting attempts. It is important to infer as muchas possible about the opponent's current and future strategy from the availableobservations. We present a dynamic Bayes net model of strategies in the RTSgame Starcraft that combines a generative model of how strategies relate toobservable quantities with a principled framework for incorporating evidencegained via scouting. We demonstrate the model's ability to infer unobservedaspects of the game from realistic observations.

Inductive Policy Selection for First-Order MDPs

  We select policies for large Markov Decision Processes (MDPs) with compactfirst-order representations. We find policies that generalize well as thenumber of objects in the domain grows, potentially without bound. Existingdynamic-programming approaches based on flat, propositional, or first-orderrepresentations either are impractical here or do not naturally scale as thenumber of objects grows without bound. We implement and evaluate an alternativeapproach that induces first-order policies using training data constructed bysolving small problem instances using PGraphplan (Blum & Langford, 1999). Ourpolicies are represented as ensembles of decision lists, using a taxonomicconcept language. This approach extends the work of Martin and Geffner (2000)to stochastic domains, ensemble learning, and a wider variety of problems.Empirically, we find "good" policies for several stochastic first-order MDPsthat are beyond the scope of previous approaches. We also discuss theapplication of this work to the relational reinforcement-learning problem.

Coactive Learning for Locally Optimal Problem Solving

  Coactive learning is an online problem solving setting where the solutionsprovided by a solver are interactively improved by a domain expert, which inturn drives learning. In this paper we extend the study of coactive learning toproblems where obtaining a globally optimal or near-optimal solution may beintractable or where an expert can only be expected to make small, localimprovements to a candidate solution. The goal of learning in this new settingis to minimize the cost as measured by the expert effort over time. We firstestablish theoretical bounds on the average cost of the existing coactivePerceptron algorithm. In addition, we consider new online algorithms that usecost-sensitive and Passive-Aggressive (PA) updates, showing similar or improvedtheoretical bounds. We provide an empirical evaluation of the learners invarious domains, which show that the Perceptron based algorithms are quiteeffective and that unlike the case for online classification, the PA algorithmsdo not yield significant performance gains.

AutoMode: Relational Learning With Less Black Magic

  Relational databases are valuable resources for learning novel andinteresting relations and concepts. Relational learning algorithms learn theDatalog definition of new relations in terms of the existing relations in thedatabase. In order to constraint the search through the large space ofcandidate definitions, users must tune the algorithm by specifying a languagebias. Unfortunately, specifying the language bias is done via trial and errorand is guided by the expert's intuitions. Hence, it normally takes a great dealof time and effort to effectively use these algorithms. In particular, it ishard to find a user that knows computer science concepts, such as databaseschema, and has a reasonable intuition about the target relation in specialdomains, such as biology. We propose AutoMode, a system that leveragesinformation in the schema and content of the database to automatically inducethe language bias used by popular relational learning systems. We show thatAutoMode delivers the same accuracy as using manually-written language bias byimposing only a slight overhead on the running time of the learning algorithm.

Visualizing and Understanding Atari Agents

  While deep reinforcement learning (deep RL) agents are effective atmaximizing rewards, it is often unclear what strategies they use to do so. Inthis paper, we take a step toward explaining deep RL agents through a casestudy using Atari 2600 environments. In particular, we focus on using saliencymaps to understand how an agent learns and executes a policy. We introduce amethod for generating useful saliency maps and use it to show 1) what strongagents attend to, 2) whether agents are making decisions for the right or wrongreasons, and 3) how agents evolve during learning. We also test our method onnon-expert human subjects and find that it improves their ability to reasonabout these agents. Overall, our results show that saliency information canprovide significant insight into an RL agent's decisions and learning behavior.

Learning Finite State Representations of Recurrent Policy Networks

  Recurrent neural networks (RNNs) are an effective representation of controlpolicies for a wide range of reinforcement and imitation learning problems. RNNpolicies, however, are particularly difficult to explain, understand, andanalyze due to their use of continuous-valued memory vectors and observationfeatures. In this paper, we introduce a new technique, Quantized BottleneckInsertion, to learn finite representations of these vectors and features. Theresult is a quantized representation of the RNN that can be analyzed to improveour understanding of memory use and general behavior. We present results ofthis approach on synthetic environments and six Atari games. The resultingfinite representations are surprisingly small in some cases, using as few as 3discrete memory states and 10 observations for a perfect Pong policy. We alsoshow that these finite policy representations lead to improvedinterpretability.

Explaining Reinforcement Learning to Mere Mortals: An Empirical Study

  We present a user study to investigate the impact of explanations onnon-experts' understanding of reinforcement learning (RL) agents. Weinvestigate both a common RL visualization, saliency maps (the focus ofattention), and a more recent explanation type, reward-decomposition bars(predictions of future types of rewards). We designed a 124 participant,four-treatment experiment to compare participants' mental models of an RL agentin a simple Real-Time Strategy (RTS) game. Our results show that thecombination of both saliency and reward bars were needed to achieve astatistically significant improvement in mental model score over the control.In addition, our qualitative analysis of the data reveals a number of effectsfor further study.

Magnetohydrodynamics dynamical relaxation of coronal magnetic fields. I.  Parallel untwisted magnetic fields in 2D

  Context. For the last thirty years, most of the studies on the relaxation ofstressed magnetic fields in the solar environment have onlyconsidered theLorentz force, neglecting plasma contributions, and therefore, limiting everyequilibrium to that of a force-free field. Aims. Here we begin a study of thenon-resistive evolution of finite beta plasmas and their relaxation tomagnetohydrostatic states, where magnetic forces are balanced byplasma-pressure gradients, by using a simple 2D scenario involving ahydromagnetic disturbance to a uniform magnetic field. The final equilibriumstate is predicted as a function of the initial disturbances, with aims todemonstrate what happens to the plasma during the relaxation process and to seewhat effects it has on the final equilibrium state. Methods. A set of numericalexperiments are run using a full MHD code, with the relaxation driven bymagnetoacoustic waves damped by viscous effects. The numerical results arecompared with analytical calculations made within the linear regime, in whichthe whole process must remain adiabatic. Particular attention is paid to thethermodynamic behaviour of the plasma during the relaxation. Results. Theanalytical predictions for the final non force-free equilibrium depend only onthe initial perturbations and the total pressure of the system. It is foundthat these predictions hold surprisingly well even for amplitudes of theperturbation far outside the linear regime. Conclusions. Including the effectsof a finite plasma beta in relaxation experiments leads to significantdifferences from the force-free case.

Batch Active Learning via Coordinated Matching

  Most prior work on active learning of classifiers has focused on sequentiallyselecting one unlabeled example at a time to be labeled in order to reduce theoverall labeling effort. In many scenarios, however, it is desirable to labelan entire batch of examples at once, for example, when labels can be acquiredin parallel. This motivates us to study batch active learning, whichiteratively selects batches of $k>1$ examples to be labeled. We propose a novelbatch active learning method that leverages the availability of high-qualityand efficient sequential active-learning policies by attempting to approximatetheir behavior when applied for $k$ steps. Specifically, our algorithm firstuses Monte-Carlo simulation to estimate the distribution of unlabeled examplesselected by a sequential policy over $k$ step executions. The algorithm thenattempts to select a set of $k$ examples that best matches this distribution,leading to a combinatorial optimization problem that we term "boundedcoordinated matching". While we show this problem is NP-hard in general, wegive an efficient greedy solution, which inherits approximation bounds fromsupermodular minimization theory. Our experimental results on eight benchmarkdatasets show that the proposed approach is highly effective

A Policy Switching Approach to Consolidating Load Shedding and Islanding  Protection Schemes

  In recent years there have been many improvements in the reliability ofcritical infrastructure systems. Despite these improvements, the power systemsindustry has seen relatively small advances in this regard. For instance, powerquality deficiencies, a high number of localized contingencies, and largecascading outages are still too widespread. Though progress has been made inimproving generation, transmission, and distribution infrastructure, remedialaction schemes (RAS) remain non-standardized and are often not uniformlyimplemented across different utilities, ISOs, and RTOs. Traditionally, loadshedding and islanding have been successful protection measures in restrainingpropagation of contingencies and large cascading outages. This paper proposes anovel, algorithmic approach to selecting RAS policies to optimize the operationof the power network during and after a contingency. Specifically, we usepolicy-switching to consolidate traditional load shedding and islandingschemes. In order to model and simulate the functionality of the proposed powersystems protection algorithm, we conduct Monte-Carlo, time-domain simulationsusing Siemens PSS/E. The algorithm is tested via experiments on the IEEE-39topology to demonstrate that the proposed approach achieves optimal powersystem performance during emergency situations, given a specific set of RASpolicies.

Representation Independent Analytics Over Structured Data

  Database analytics algorithms leverage quantifiable structural properties ofthe data to predict interesting concepts and relationships. The sameinformation, however, can be represented using many different structures andthe structural properties observed over particular representations do notnecessarily hold for alternative structures. Thus, there is no guarantee thatcurrent database analytics algorithms will still provide the correct insights,no matter what structures are chosen to organize the database. Because thesealgorithms tend to be highly effective over some choices of structure, such asthat of the databases used to validate them, but not so effective with others,database analytics has largely remained the province of experts who can findthe desired forms for these algorithms. We argue that in order to make databaseanalytics usable, we should use or develop algorithms that are effective over awide range of choices of structural organizations. We introduce the notion ofrepresentation independence, study its fundamental properties for a wide rangeof data analytics algorithms, and empirically analyze the amount ofrepresentation independence of some popular database analytics algorithms. Ourresults indicate that most algorithms are not generally representationindependent and find the characteristics of more representation independentheuristics under certain representational shifts.

Sequential Feature Explanations for Anomaly Detection

  In many applications, an anomaly detection system presents the most anomalousdata instance to a human analyst, who then must determine whether the instanceis truly of interest (e.g. a threat in a security setting). Unfortunately, mostanomaly detectors provide no explanation about why an instance was consideredanomalous, leaving the analyst with no guidance about where to begin theinvestigation. To address this issue, we study the problems of computing andevaluating sequential feature explanations (SFEs) for anomaly detectors. An SFEof an anomaly is a sequence of features, which are presented to the analyst oneat a time (in order) until the information contained in the highlightedfeatures is enough for the analyst to make a confident judgement about theanomaly. Since analyst effort is related to the amount of information that theyconsider in an investigation, an explanation's quality is related to the numberof features that must be revealed to attain confidence. One of our maincontributions is to present a novel framework for large scale quantitativeevaluations of SFEs, where the quality measure is based on analyst effort. Todo this we construct anomaly detection benchmarks from real data sets alongwith artificial experts that can be simulated for evaluation. Our secondcontribution is to evaluate several novel explanation approaches within theframework and on traditional anomaly detection benchmarks, offering severalinsights into the approaches.

A Meta-Analysis of the Anomaly Detection Problem

  This article provides a thorough meta-analysis of the anomaly detectionproblem. To accomplish this we first identify approaches to benchmarkinganomaly detection algorithms across the literature and produce a large corpusof anomaly detection benchmarks that vary in their construction across severaldimensions we deem important to real-world applications: (a) point difficulty,(b) relative frequency of anomalies, (c) clusteredness of anomalies, and (d)relevance of features. We apply a representative set of anomaly detectionalgorithms to this corpus, yielding a very large collection of experimentalresults. We analyze these results to understand many phenomena observed inprevious work. First we observe the effects of experimental design onexperimental results. Second, results are evaluated with two metrics, ROC AreaUnder the Curve and Average Precision. We employ statistical hypothesis testingto demonstrate the value (or lack thereof) of our benchmarks. We then offerseveral approaches to summarizing our experimental results, drawing severalconclusions about the impact of our methodology as well as the strengths andweaknesses of some algorithms. Last, we compare results against a trivialsolution as an alternate means of normalizing the reported performance ofalgorithms. The intended contributions of this article are many; in addition toproviding a large publicly-available corpus of anomaly detection benchmarks, weprovide an ontology for describing anomaly detection contexts, a methodologyfor controlling various aspects of benchmark creation, guidelines for futureexperimental design and a discussion of the many potential pitfalls of tryingto measure success in this field.

Schema Independent Relational Learning

  Learning novel concepts and relations from relational databases is animportant problem with many applications in database systems and machinelearning. Relational learning algorithms learn the definition of a new relationin terms of existing relations in the database. Nevertheless, the same data setmay be represented under different schemas for various reasons, such asefficiency, data quality, and usability. Unfortunately, the output of currentrelational learning algorithms tends to vary quite substantially over thechoice of schema, both in terms of learning accuracy and efficiency. Thisvariation complicates their off-the-shelf application. In this paper, weintroduce and formalize the property of schema independence of relationallearning algorithms, and study both the theoretical and empirical dependence ofexisting algorithms on the common class of (de) composition schematransformations. We study both sample-based learning algorithms, which learnfrom sets of labeled examples, and query-based algorithms, which learn byasking queries to an oracle. We prove that current relational learningalgorithms are generally not schema independent. For query-based learningalgorithms we show that the (de) composition transformations influence theirquery complexity. We propose Castor, a sample-based relational learningalgorithm that achieves schema independence by leveraging data dependencies. Wesupport the theoretical results with an empirical study that demonstrates theschema dependence/independence of several algorithms on existing benchmark andreal-world datasets under (de) compositions.

Approximate Policy Iteration for Budgeted Semantic Video Segmentation

  This paper formulates and presents a solution to the new problem of budgetedsemantic video segmentation. Given a video, the goal is to accurately assign asemantic class label to every pixel in the video within a specified timebudget. Typical approaches to such labeling problems, such as ConditionalRandom Fields (CRFs), focus on maximizing accuracy but do not provide aprincipled method for satisfying a time budget. For video data, the timerequired by CRF and related methods is often dominated by the time to computelow-level descriptors of supervoxels across the video. Our key contribution isthe new budgeted inference framework for CRF models that intelligently selectsthe most useful subsets of descriptors to run on subsets of supervoxels withinthe time budget. The objective is to maintain an accuracy as close as possibleto the CRF model with no time bound, while remaining within the time budget.Our second contribution is the algorithm for learning a policy for the sparseselection of supervoxels and their descriptors for budgeted CRF inference. Thislearning algorithm is derived by casting our problem in the framework of MarkovDecision Processes, and then instantiating a state-of-the-art policy learningalgorithm known as Classification-Based Approximate Policy Iteration. Ourexperiments on multiple video datasets show that our learning approach andframework is able to significantly reduce computation time, and maintaincompetitive accuracy under varying budgets.

Incorporating Feedback into Tree-based Anomaly Detection

  Anomaly detectors are often used to produce a ranked list of statisticalanomalies, which are examined by human analysts in order to extract the actualanomalies of interest. Unfortunately, in realworld applications, this processcan be exceedingly difficult for the analyst since a large fraction ofhigh-ranking anomalies are false positives and not interesting from theapplication perspective. In this paper, we aim to make the analyst's job easierby allowing for analyst feedback during the investigation process. Ideally, thefeedback influences the ranking of the anomaly detector in a way that reducesthe number of false positives that must be examined before discovering theanomalies of interest. In particular, we introduce a novel technique forincorporating simple binary feedback into tree-based anomaly detectors. Wefocus on the Isolation Forest algorithm as a representative tree-based anomalydetector, and show that we can significantly improve its performance byincorporating feedback, when compared with the baseline algorithm that does notincorporate feedback. Our technique is simple and scales well as the size ofthe data increases, which makes it suitable for interactive discovery ofanomalies in large datasets.

Open Category Detection with PAC Guarantees

  Open category detection is the problem of detecting "alien" test instancesthat belong to categories or classes that were not present in the trainingdata. In many applications, reliably detecting such aliens is central toensuring the safety and accuracy of test set predictions. Unfortunately, thereare no algorithms that provide theoretical guarantees on their ability todetect aliens under general assumptions. Further, while there are algorithmsfor open category detection, there are few empirical results that directlyreport alien detection rates. Thus, there are significant theoretical andempirical gaps in our understanding of open category detection. In this paper,we take a step toward addressing this gap by studying a simple, butpractically-relevant variant of open category detection. In our setting, we areprovided with a "clean" training set that contains only the target categoriesof interest and an unlabeled "contaminated" training set that contains afraction $\alpha$ of alien examples. Under the assumption that we know an upperbound on $\alpha$, we develop an algorithm with PAC-style guarantees on thealien detection rate, while aiming to minimize false alarms. Empirical resultson synthetic and standard benchmark datasets demonstrate the regimes in whichthe algorithm can be effective and provide a baseline for further advancements.

Interactive Naming for Explaining Deep Neural Networks: A Formative  Study

  We consider the problem of explaining the decisions of deep neural networksfor image recognition in terms of human-recognizable visual concepts. Inparticular, given a test set of images, we aim to explain each classificationin terms of a small number of image regions, or activation maps, which havebeen associated with semantic concepts by a human annotator. This allows forgenerating summary views of the typical reasons for classifications, which canhelp build trust in a classifier and/or identify example types for which theclassifier may not be trusted. For this purpose, we developed a user interfacefor "interactive naming," which allows a human annotator to manually clustersignificant activation maps in a test set into meaningful groups called "visualconcepts". The main contribution of this paper is a systematic study of thevisual concepts produced by five human annotators using the interactive naminginterface. In particular, we consider the adequacy of the concepts forexplaining the classification of test-set images, correspondence of theconcepts to activations of individual neurons, and the inter-annotatoragreement of visual concepts. We find that a large fraction of the activationmaps have recognizable visual concepts, and that there is significant agreementbetween the different annotators about their denotations. Our work is anexploratory study of the interplay between machine learning and humanrecognition mediated by visualizations of the results of learning.

Magnetohydrodynamics dynamical relaxation of coronal magnetic fields.  II. 2D magnetic X-points

  We provide a valid magnetohydrostatic equilibrium from the collapse of a 2DX-point in the presence of a finite plasma pressure, in which the currentdensity is not simply concentrated in an infinitesimally thin, one-dimensionalcurrent sheet, as found in force-free solutions. In particular, we wish todetermine if a finite pressure current sheet will still involve a singularcurrent, and if so, what is the nature of the singularity. We use a full MHDcode, with the resistivity set to zero, so that reconnection is not allowed, torun a series of experiments in which an X-point is perturbed and then isallowed to relax towards an equilibrium, via real, viscous damping forces.Changes to the magnitude of the perturbation and the initial plasma pressureare investigated systematically. The final state found in our experiments is a"quasi-static" equilibrium where the viscous relaxation has completely ended,but the peak current density at the null increases very slowly following anasymptotic regime towards an infinite time singularity. Using a high gridresolution allows us to resolve the current structures in this state both inwidth and length. In comparison with the well known pressureless studies, thesystem does not evolve towards a thin current sheet, but concentrates thecurrent at the null and the separatrices. The growth rate of the singularity isfound to be tD, with 0 < D < 1. This rate depends directly on the initialplasma pressure, and decreases as the pressure is increased. At the end of ourstudy, we present an analytical description of the system in a quasi-staticnon-singular equilibrium at a given time, in which a finite thick current layerhas formed at the null.

Consequences of spontaneous reconnection at a two-dimensional  non-force-free current layer

  Magnetic neutral points, where the magnitude of the magnetic field vanisheslocally, are potential locations for energy conversion in the solar corona. Thefact that the magnetic field is identically zero at these points suggests thatfor the study of current sheet formation and of any subsequent resistivedissipation phase, a finite beta plasma should be considered, rather thanneglecting the plasma pressure as has often been the case in the past. Therapid dissipation of a finite current layer in non-force-free equilibrium isinvestigated numerically, after the sudden onset of an anomalous resistivity.The aim of this study is to determine how the energy is redistributed duringthe initial diffusion phase, and what is the nature of the outward transmissionof information and energy. The resistivity rapidly diffuses the current at thenull point. The presence of a plasma pressure allows the vast majority of thefree energy to be transferred into internal energy. Most of the convertedenergy is used in direct heating of the surrounding plasma, and only about 3%is converted into kinetic energy, causing a perturbation in the magnetic fieldand the plasma which propagates away from the null at the local fastmagnetoacoustic speed. The propagating pulses show a complex structure due tothe highly non-uniform initial state. It is shown that this perturbationcarries no net current as it propagates away from the null. The fact that,under the assumptions taken in this paper, most of the magnetic energy releasedin the reconnection converts internal energy of the plasma, may be highlyimportant for the chromospheric and coronal heating problem.

