Direct Learning to Rank and Rerank

  Learning-to-rank techniques have proven to be extremely useful forprioritization problems, where we rank items in order of their estimatedprobabilities, and dedicate our limited resources to the top-ranked items. Thiswork exposes a serious problem with the state of learning-to-rank algorithms,which is that they are based on convex proxies that lead to poorapproximations. We then discuss the possibility of "exact" reranking algorithmsbased on mathematical programming. We prove that a relaxed version of the"exact" problem has the same optimal solution, and provide an empiricalanalysis.

Please Stop Explaining Black Box Models for High Stakes Decisions

  Black box machine learning models are currently being used for high stakesdecision-making throughout society, causing problems throughout healthcare,criminal justice, and in other domains. People have hoped that creating methodsfor explaining these black box models will alleviate some of these problems,but trying to explain black box models, rather than creating models that areinterpretable in the first place, is likely to perpetuate bad practices and canpotentially cause catastrophic harm to society. There is a way forward - it isto design models that are inherently interpretable.

Supersparse Linear Integer Models for Predictive Scoring Systems

  We introduce Supersparse Linear Integer Models (SLIM) as a tool to createscoring systems for binary classification. We derive theoretical bounds on thetrue risk of SLIM scoring systems, and present experimental results to showthat SLIM scoring systems are accurate, sparse, and interpretableclassification models.

Proceedings of the 2018 ICML Workshop on Human Interpretability in  Machine Learning (WHI 2018)

  This is the Proceedings of the 2018 ICML Workshop on Human Interpretabilityin Machine Learning (WHI 2018), which was held in Stockholm, Sweden, July 14,2018. Invited speakers were Barbara Engelhardt, Cynthia Rudin, FernandaVi\'egas, and Martin Wattenberg.

Shall I Compare Thee to a Machine-Written Sonnet? An Approach to  Algorithmic Sonnet Generation

  We provide code that produces beautiful poetry. Our sonnet-generationalgorithm includes several novel elements that improve over thestate-of-the-art, leading to rhythmic and inspiring poems. The work discussedhere is the winner of the 2018 PoetiX Literary Turing Test Award forcomputer-generated poetry.

Stability Analysis for Regularized Least Squares Regression

  We discuss stability for a class of learning algorithms with respect to noisylabels. The algorithms we consider are for regression, and they involve theminimization of regularized risk functionals, such as L(f) := 1/N sum_i(f(x_i)-y_i)^2+ lambda ||f||_H^2. We shall call the algorithm `stable' if, wheny_i is a noisy version of f*(x_i) for some function f* in H, the output of thealgorithm converges to f* as the regularization term and noise simultaneouslyvanish. We consider two flavors of this problem, one where a data set of Npoints remains fixed, and the other where N -> infinity. For the case where N-> infinity, we give conditions for convergence to f_E (the function which isthe expectation of y(x) for each x), as lambda -> 0. For the fixed N case, wedescribe the limiting 'non-noisy', 'non-regularized' function f*, and giveconditions for convergence. In the process, we develop a set of tools fordealing with functionals such as L(f), which are applicable to many otherproblems in learning theory.

Analysis of boosting algorithms using the smooth margin function

  We introduce a useful tool for analyzing boosting algorithms called the``smooth margin function,'' a differentiable approximation of the usual marginfor boosting algorithms. We present two boosting algorithms based on thissmooth margin, ``coordinate ascent boosting'' and ``approximate coordinateascent boosting,'' which are similar to Freund and Schapire's AdaBoostalgorithm and Breiman's arc-gv algorithm. We give convergence rates to themaximum margin solution for both of our algorithms and for arc-gv. We thenstudy AdaBoost's convergence properties using the smooth margin function. Weprecisely bound the margin attained by AdaBoost when the edges of the weakclassifiers fall within a specified range. This shows that a previous boundproved by R\"{a}tsch and Warmuth is exactly tight. Furthermore, we use thesmooth margin to capture explicit properties of AdaBoost in cases where cyclicbehavior occurs.

Online Coordinate Boosting

  We present a new online boosting algorithm for adapting the weights of aboosted classifier, which yields a closer approximation to Freund andSchapire's AdaBoost algorithm than previous online boosting algorithms. We alsocontribute a new way of deriving the online algorithm that ties togetherprevious online boosting work. We assume that the weak hypotheses were selectedbeforehand, and only their weights are updated during online boosting. Theupdate rule is derived by minimizing AdaBoost's loss when viewed in anincremental form. The equations show that optimization is computationallyexpensive. However, a fast online approximation is possible. We compareapproximation error to batch AdaBoost on synthetic datasets and generalizationerror on face datasets and the MNIST dataset.

Learning to Predict the Wisdom of Crowds

  The problem of "approximating the crowd" is that of estimating the crowd'smajority opinion by querying only a subset of it. Algorithms that approximatethe crowd can intelligently stretch a limited budget for a crowdsourcing task.We present an algorithm, "CrowdSense," that works in an online fashion todynamically sample subsets of labelers based on an exploration/exploitationcriterion. The algorithm produces a weighted combination of a subset of thelabelers' votes that approximates the crowd's opinion.

Falling Rule Lists

  Falling rule lists are classification models consisting of an ordered list ofif-then rules, where (i) the order of rules determines which example should beclassified by each rule, and (ii) the estimated probability of successdecreases monotonically down the list. These kinds of rule lists are inspiredby healthcare applications where patients would be stratified into risk setsand the highest at-risk patients should be considered first. We provide aBayesian framework for learning falling rule lists that does not rely ontraditional greedy decision tree learning methods.

CRAFT: ClusteR-specific Assorted Feature selecTion

  We present a framework for clustering with cluster-specific featureselection. The framework, CRAFT, is derived from asymptotic log posteriorformulations of nonparametric MAP-based clustering models. CRAFT handlesassorted data, i.e., both numeric and categorical data, and the underlyingobjective functions are intuitively appealing. The resulting algorithm issimple to implement and scales nicely, requires minimal parameter tuning,obviates the need to specify the number of clusters a priori, and comparesfavorably with other methods on real datasets.

The age of secrecy and unfairness in recidivism prediction

  In our current society, secret algorithms make important decisions aboutindividuals. There has been substantial discussion about whether thesealgorithms are unfair to groups of individuals. While noble, this pursuit iscomplex and ultimately stagnating because there is no clear definition offairness and competing definitions are largely incompatible. We argue that thefocus on the question of fairness is misplaced, as these algorithms fail tomeet a more important and yet readily obtainable goal: transparency. As aresult, creators of secret algorithms can provide incomplete or misleadingdescriptions about how their models work, and various other kinds of errors caneasily go unnoticed. By partially reverse engineering the COMPAS algorithm -- arecidivism-risk scoring algorithm used throughout the criminal justice system-- we show that it does not seem to depend linearly on the defendant's age,despite statements to the contrary by the algorithm's creator. Furthermore, bysubtracting from COMPAS its (hypothesized) nonlinear age component, we showthat COMPAS does not necessarily depend on race, contradicting ProPublica'sanalysis, which assumed linearity in age. In other words, faulty assumptionsabout a proprietary algorithm lead to faulty conclusions that go uncheckedwithout careful reverse engineering. Were the algorithm transparent in thefirst place, this would likely not have occurred. The most important result inthis work is that we find that there are many defendants with low risk scorebut long criminal histories, suggesting that data inconsistencies occurfrequently in criminal justice databases. We argue that transparency satisfiesa different notion of procedural fairness by providing both the defendants andthe public with the opportunity to scrutinize the methodology and calculationsbehind risk scores for recidivism.

On Combining Machine Learning with Decision Making

  We present a new application and covering number bound for the framework of"Machine Learning with Operational Costs (MLOC)," which is an exploratory formof decision theory. The MLOC framework incorporates knowledge about how apredictive model will be used for a subsequent task, thus combining machinelearning with the decision that is made afterwards. In this work, we use theMLOC framework to study a problem that has implications for power gridreliability and maintenance, called the Machine Learning and TravelingRepairman Problem ML&TRP. The goal of the ML&TRP is to determine a route for a"repair crew," which repairs nodes on a graph. The repair crew aims to minimizethe cost of failures at the nodes, but as in many real situations, the failureprobabilities are not known and must be estimated. The MLOC framework allows usto understand how this uncertainty influences the repair route. We also presentnew covering number generalization bounds for the MLOC framework.

Bayesian hierarchical rule modeling for predicting medical conditions

  We propose a statistical modeling technique, called the HierarchicalAssociation Rule Model (HARM), that predicts a patient's possible futuremedical conditions given the patient's current and past history of reportedconditions. The core of our technique is a Bayesian hierarchical model forselecting predictive association rules (such as "condition 1 and condition 2$\rightarrow$ condition 3") from a large set of candidate rules. Because thismethod "borrows strength" using the conditions of many similar patients, it isable to provide predictions specialized to any given patient, even when littleinformation about the patient's history of conditions is available.

Learning About Meetings

  Most people participate in meetings almost every day, multiple times a day.The study of meetings is important, but also challenging, as it requires anunderstanding of social signals and complex interpersonal dynamics. Our aimthis work is to use a data-driven approach to the science of meetings. Weprovide tentative evidence that: i) it is possible to automatically detect whenduring the meeting a key decision is taking place, from analyzing only thelocal dialogue acts, ii) there are common patterns in the way social dialogueacts are interspersed throughout a meeting, iii) at the time key decisions aremade, the amount of time left in the meeting can be predicted from the amountof time that has passed, iv) it is often possible to predict whether a proposalduring a meeting will be accepted or rejected based entirely on the language(the set of persuasive words) used by the speaker.

Supersparse Linear Integer Models for Interpretable Classification

  Scoring systems are classification models that only require users to add,subtract and multiply a few meaningful numbers to make a prediction. Thesemodels are often used because they are practical and interpretable. In thispaper, we introduce an off-the-shelf tool to create scoring systems that bothaccurate and interpretable, known as a Supersparse Linear Integer Model (SLIM).SLIM is a discrete optimization problem that minimizes the 0-1 loss toencourage a high level of accuracy, regularizes the L0-norm to encourage a highlevel of sparsity, and constrains coefficients to a set of interpretablevalues. We illustrate the practical and interpretable nature of SLIM scoringsystems through applications in medicine and criminology, and show that theyare are accurate and sparse in comparison to state-of-the-art classificationmodels using numerical experiments.

A Statistical Learning Theory Framework for Supervised Pattern Discovery

  This paper formalizes a latent variable inference problem we call {\emsupervised pattern discovery}, the goal of which is to find sets ofobservations that belong to a single ``pattern.'' We discuss two versions ofthe problem and prove uniform risk bounds for both. In the first version,collections of patterns can be generated in an arbitrary manner and the dataconsist of multiple labeled collections. In the second version, the patternsare assumed to be generated independently by identically distributed processes.These processes are allowed to take an arbitrary form, so observations within apattern are not in general independent of each other. The bounds for the secondversion of the problem are stated in terms of a new complexity measure, thequasi-Rademacher complexity.

Box Drawings for Learning with Imbalanced Data

  The vast majority of real world classification problems are imbalanced,meaning there are far fewer data from the class of interest (the positiveclass) than from other classes. We propose two machine learning algorithms tohandle highly imbalanced classification problems. The classifiers constructedby both methods are created as unions of parallel axis rectangles around thepositive examples, and thus have the benefit of being interpretable. The firstalgorithm uses mixed integer programming to optimize a weighted balance betweenpositive and negative class accuracies. Regularization is introduced to improvegeneralization performance. The second method uses an approximation in order toassist with scalability. Specifically, it follows a \textit{characterize thendiscriminate} approach, where the positive class is characterized first byboxes, and then each box boundary becomes a separate discriminative classifier.This method has the computational advantages that it can be easilyparallelized, and considers only the relevant regions of feature space.

Methods and Models for Interpretable Linear Classification

  We present an integer programming framework to build accurate andinterpretable discrete linear classification models. Unlike existingapproaches, our framework is designed to provide practitioners with the controland flexibility they need to tailor accurate and interpretable models for adomain of choice. To this end, our framework can produce models that are fullyoptimized for accuracy, by minimizing the 0--1 classification loss, and thataddress multiple aspects of interpretability, by incorporating a range ofdiscrete constraints and penalty functions. We use our framework to producemodels that are difficult to create with existing methods, such as scoringsystems and M-of-N rule tables. In addition, we propose specially designedoptimization methods to improve the scalability of our framework throughdecomposition and data reduction. We show that discrete linear classifiers canattain the training accuracy of any other linear classifier, and provide anOccam's Razor type argument as to why the use of small discrete coefficientscan provide better generalization. We demonstrate the performance andflexibility of our framework through numerical experiments and a case study inwhich we construct a highly tailored clinical tool for sleep apnea diagnosis.

Generalization Bounds for Learning with Linear, Polygonal, Quadratic and  Conic Side Knowledge

  In this paper, we consider a supervised learning setting where side knowledgeis provided about the labels of unlabeled examples. The side knowledge has theeffect of reducing the hypothesis space, leading to tighter generalizationbounds, and thus possibly better generalization. We consider several types ofside knowledge, the first leading to linear and polygonal constraints on thehypothesis space, the second leading to quadratic constraints, and the lastleading to conic constraints. We show how different types of domain knowledgecan lead directly to these kinds of side knowledge. We prove bounds oncomplexity measures of the hypothesis space for quadratic and conic sideknowledge, and show that these bounds are tight in a specific sense for thequadratic case.

Robust Optimization using Machine Learning for Uncertainty Sets

  Our goal is to build robust optimization problems for making decisions basedon complex data from the past. In robust optimization (RO) generally, the goalis to create a policy for decision-making that is robust to our uncertaintyabout the future. In particular, we want our policy to best handle the theworst possible situation that could arise, out of an uncertainty set ofpossible situations. Classically, the uncertainty set is simply chosen by theuser, or it might be estimated in overly simplistic ways with strongassumptions; whereas in this work, we learn the uncertainty set from datacollected in the past. The past data are drawn randomly from an (unknown)possibly complicated high-dimensional distribution. We propose a newuncertainty set design and show how tools from statistical learning theory canbe employed to provide probabilistic guarantees on the robustness of thepolicy.

Bayesian Inference of Arrival Rate and Substitution Behavior from Sales  Transaction Data with Stockouts

  When an item goes out of stock, sales transaction data no longer reflect theoriginal customer demand, since some customers leave with no purchase whileothers substitute alternative products for the one that was out of stock. Herewe develop a Bayesian hierarchical model for inferring the underlying customerarrival rate and choice model from sales transaction data and the correspondingstock levels. The model uses a nonhomogeneous Poisson process to allow thearrival rate to vary throughout the day, and allows for a variety of choicemodels. Model parameters are inferred using a stochastic gradient MCMCalgorithm that can scale to large transaction databases. We fit the model todata from a local bakery and show that it is able to make accurateout-of-sample predictions, and to provide actionable insight into lost cookiesales.

The Bayesian Case Model: A Generative Approach for Case-Based Reasoning  and Prototype Classification

  We present the Bayesian Case Model (BCM), a general framework for Bayesiancase-based reasoning (CBR) and prototype classification and clustering. BCMbrings the intuitive power of CBR to a Bayesian generative framework. The BCMlearns prototypes, the "quintessential" observations that best representclusters in a dataset, by performing joint inference on cluster labels,prototypes and important features. Simultaneously, BCM pursues sparsity bylearning subspaces, the sets of features that play important roles in thecharacterization of the prototypes. The prototype and subspace representationprovides quantitative benefits in interpretability while preservingclassification accuracy. Human subject experiments verify statisticallysignificant improvements to participants' understanding when using explanationsproduced by BCM, compared to those given by prior art.

Modeling Recovery Curves With Application to Prostatectomy

  We propose a Bayesian model that predicts recovery curves based oninformation available before the disruptive event. A recovery curve of interestis the quantified sexual function of prostate cancer patients afterprostatectomy surgery. We illustrate the utility of our model as apre-treatment medical decision aid, producing personalized predictions that areboth interpretable and accurate. We uncover covariate relationships that agreewith and supplement that in existing medical literature.

Causal Falling Rule Lists

  A causal falling rule list (CFRL) is a sequence of if-then rules thatspecifies heterogeneous treatment effects, where (i) the order of rulesdetermines the treatment effect subgroup a subject belongs to, and (ii) thetreatment effect decreases monotonically down the list. A given CFRLparameterizes a hierarchical bayesian regression model in which the treatmenteffects are incorporated as parameters, and assumed constant withinmodel-specific subgroups. We formulate the search for the CFRL best supportedby the data as a Bayesian model selection problem, where we perform a searchover the space of CFRL models, and approximate the evidence for a given CFRLmodel using standard variational techniques. We apply CFRL to a census wagedataset to identify subgroups of differing wage inequalities between men andwomen.

Cascaded High Dimensional Histograms: A Generative Approach to Density  Estimation

  We present tree- and list- structured density estimation methods for highdimensional binary/categorical data. Our density estimation models are highdimensional analogies to variable bin width histograms. In each leaf of thetree (or list), the density is constant, similar to the flat density within thebin of a histogram. Histograms, however, cannot easily be visualized in higherdimensions, whereas our models can. The accuracy of histograms fades asdimensions increase, whereas our models have priors that help withgeneralization. Our models are sparse, unlike high-dimensional histograms. Wepresent three generative models, where the first one allows the user to specifythe number of desired leaves in the tree within a Bayesian prior. The secondmodel allows the user to specify the desired number of branches within theprior. The third model returns lists (rather than trees) and allows the user tospecify the desired number of rules and the length of rules within the prior.Our results indicate that the new approaches yield a better balance betweensparsity and accuracy of density estimates than other methods for this task.

Learning Optimized Or's of And's

  Or's of And's (OA) models are comprised of a small number of disjunctions ofconjunctions, also called disjunctive normal form. An example of an OA model isas follows: If ($x_1 = $ `blue' AND $x_2=$ `middle') OR ($x_1 = $ `yellow'),then predict $Y=1$, else predict $Y=0$. Or's of And's models have the advantageof being interpretable to human experts, since they are a set of conditionsthat concisely capture the characteristics of a specific subset of data. Wepresent two optimization-based machine learning frameworks for constructing OAmodels, Optimized OA (OOA) and its faster version, Optimized OA withApproximations (OOAx). We prove theoretical bounds on the properties ofpatterns in an OA model. We build OA models as a diagnostic screening tool forobstructive sleep apnea, that achieves high accuracy with a substantial gain ininterpretability over other methods.

The latent state hazard model, with application to wind turbine  reliability

  We present a new model for reliability analysis that is able to distinguishthe latent internal vulnerability state of the equipment from the vulnerabilitycaused by temporary external sources. Consider a wind farm where each turbineis running under the external effects of temperature, wind speed and direction,etc. The turbine might fail because of the external effects of a spike intemperature. If it does not fail during the temperature spike, it could stillfail due to internal degradation, and the spike could cause (or be anindication of) this degradation. The ability to identify the underlying latentstate can help better understand the effects of external sources and thus leadto more robust decision-making. We present an experimental study using SCADAsensor measurements from wind turbines in Italy.

Scalable Bayesian Rule Lists

  We present an algorithm for building probabilistic rule lists that is twoorders of magnitude faster than previous work. Rule list algorithms arecompetitors for decision tree algorithms. They are associative classifiers, inthat they are built from pre-mined association rules. They have a logicalstructure that is a sequence of IF-THEN rules, identical to a decision list orone-sided decision tree. Instead of using greedy splitting and pruning likedecision tree algorithms, we fully optimize over rule lists, striking apractical balance between accuracy, interpretability, and computational speed.The algorithm presented here uses a mixture of theoretical bounds (tight enoughto have practical implications as a screening or bounding procedure),computational reuse, and highly tuned language libraries to achievecomputational efficiency. Currently, for many practical problems, this methodachieves better accuracy and sparsity than decision trees; further, in manycases, the computational time is practical and often less than that of decisiontrees. The result is a probabilistic classifier (which estimates P(y = 1|x) foreach x) that optimizes the posterior of a Bayesian hierarchical model over rulelists.

Interpretable Machine Learning Models for the Digital Clock Drawing Test

  The Clock Drawing Test (CDT) is a rapid, inexpensive, and popularneuropsychological screening tool for cognitive conditions. The Digital ClockDrawing Test (dCDT) uses novel software to analyze data from a digitizingballpoint pen that reports its position with considerable spatial and temporalprecision, making possible the analysis of both the drawing process and finalproduct. We developed methodology to analyze pen stroke data from thesedrawings, and computed a large collection of features which were then analyzedwith a variety of machine learning techniques. The resulting scoring systemswere designed to be more accurate than the systems currently used byclinicians, but just as interpretable and easy to use. The systems also allowus to quantify the tradeoff between accuracy and interpretability. We createdautomated versions of the CDT scoring systems currently used by clinicians,allowing us to benchmark our models, which indicated that our machine learningmodels substantially outperformed the existing scoring systems.

Learning Cost-Effective Treatment Regimes using Markov Decision  Processes

  Decision makers, such as doctors and judges, make crucial decisions such asrecommending treatments to patients, and granting bails to defendants on adaily basis. Such decisions typically involve weighting the potential benefitsof taking an action against the costs involved. In this work, we aim toautomate this task of learning \emph{cost-effective, interpretable andactionable treatment regimes}. We formulate this as a problem of learning adecision list -- a sequence of if-then-else rules -- which maps characteristicsof subjects (eg., diagnostic test results of patients) to treatments. Wepropose a novel objective to construct a decision list which maximizes outcomesfor the population, and minimizes overall costs. We model the problem oflearning such a list as a Markov Decision Process (MDP) and employ a variant ofthe Upper Confidence Bound for Trees (UCT) strategy which leverages customizedchecks for pruning the search space effectively. Experimental results on realworld observational data capturing judicial bail decisions and treatmentrecommendations for asthma patients demonstrate the effectiveness of ourapproach.

Learning Cost-Effective and Interpretable Regimes for Treatment  Recommendation

  Decision makers, such as doctors and judges, make crucial decisions such asrecommending treatments to patients, and granting bails to defendants on adaily basis. Such decisions typically involve weighting the potential benefitsof taking an action against the costs involved. In this work, we aim toautomate this task of learning {cost-effective, interpretable and actionabletreatment regimes. We formulate this as a problem of learning a decision list-- a sequence of if-then-else rules -- which maps characteristics of subjects(eg., diagnostic test results of patients) to treatments. We propose a novelobjective to construct a decision list which maximizes outcomes for thepopulation, and minimizes overall costs. We model the problem of learning sucha list as a Markov Decision Process (MDP) and employ a variant of the UpperConfidence Bound for Trees (UCT) strategy which leverages customized checks forpruning the search space effectively. Experimental results on real worldobservational data capturing treatment recommendations for asthma patientsdemonstrate the effectiveness of our approach.

Learning Certifiably Optimal Rule Lists for Categorical Data

  We present the design and implementation of a custom discrete optimizationtechnique for building rule lists over a categorical feature space. Ouralgorithm produces rule lists with optimal training performance, according tothe regularized empirical risk, with a certificate of optimality. By leveragingalgorithmic bounds, efficient data structures, and computational reuse, weachieve several orders of magnitude speedup in time and a massive reduction ofmemory consumption. We demonstrate that our approach produces optimal rulelists on practical problems in seconds. Our results indicate that it ispossible to construct optimal sparse rule lists that are approximately asaccurate as the COMPAS proprietary risk prediction tool on data from BrowardCounty, Florida, but that are completely interpretable. This framework is anovel alternative to CART and other decision tree methods for interpretablemodeling.

FLAME: A Fast Large-scale Almost Matching Exactly Approach to Causal  Inference

  A classical problem in causal inference is that of matching, where treatmentunits need to be matched to control units. Some of the main challenges indeveloping matching methods arise from the tension among (i) inclusion of asmany covariates as possible in defining the matched groups, (ii) having matchedgroups with enough treated and control units for a valid estimate of AverageTreatment Effect (ATE) in each group, and (iii) computing the matched pairsefficiently for large datasets. In this paper we propose a fast method forapproximate and exact matching in causal analysis called FLAME (FastLarge-scale Almost Matching Exactly). We define an optimization objective formatch quality, which gives preferences to matching on covariates that can beuseful for predicting the outcome while encouraging as many matches aspossible. FLAME aims to optimize our match quality measure, leveragingtechniques that are natural for query processing in the area of databasemanagement. We provide two implementations of FLAME using SQL queries andbit-vector techniques.

An Optimization Approach to Learning Falling Rule Lists

  A falling rule list is a probabilistic decision list for binaryclassification, consisting of a series of if-then rules with antecedents in theif clauses and probabilities of the desired outcome ("1") in the then clauses.Just as in a regular decision list, the order of rules in a falling rule listis important -- each example is classified by the first rule whose antecedentit satisfies. Unlike a regular decision list, a falling rule list requires theprobabilities of the desired outcome ("1") to be monotonically decreasing downthe list. We propose an optimization approach to learning falling rule listsand "softly" falling rule lists, along with Monte-Carlo search algorithms thatuse bounds on the optimal solution to prune the search space.

Causal Rule Sets for Identifying Subgroups with Enhanced Treatment  Effect

  We introduce a novel generative model for interpretable subgroup analysis forcausal inference applications, Causal Rule Sets (CRS). A CRS model uses a smallset of short rules to capture a subgroup where the average treatment effect iselevated compared to the entire population. We present a Bayesian framework forlearning a causal rule set. The Bayesian framework consists of a prior thatfavors simpler models and a Bayesian logistic regression that characterizes therelation between outcomes, attributes and subgroup membership. We find maximuma posteriori models using discrete Monte Carlo steps in the joint solutionspace of rules sets and parameters. We provide theoretically groundedheuristics and bounding strategies to improve search efficiency. Experimentsshow that the search algorithm can efficiently recover a true underlyingsubgroup and CRS shows consistently competitive performance compared to otherstate-of-the-art baseline methods.

Extreme Dimension Reduction for Handling Covariate Shift

  In the covariate shift learning scenario, the training and test covariatedistributions differ, so that a predictor's average loss over the training andtest distributions also differ. In this work, we explore the potential ofextreme dimension reduction, i.e. to very low dimensions, in improving theperformance of importance weighting methods for handling covariate shift, whichfail in high dimensions due to potentially high train/test covariate divergenceand the inability to accurately estimate the requisite density ratios. We firstformulate and solve a problem optimizing over linear subspaces a combination oftheir predictive utility and train/test divergence within. Applying it tosimulated and real data, we show extreme dimension reduction helps sometimesbut not always, due to a bias introduced by dimension reduction.

A Minimax Surrogate Loss Approach to Conditional Difference Estimation

  We present a new machine learning approach to estimate personalized treatmenteffects in the classical potential outcomes framework with binary outcomes. Toovercome the problem that both treatment and control outcomes for the same unitare required for supervised learning, we propose surrogate loss functions thatincorporate both treatment and control data. The new surrogates yield tighterbounds than the sum of losses for treatment and control groups. A specificchoice of loss function, namely a type of hinge loss, yields a minimax supportvector machine formulation. The resulting optimization problem requires thesolution to only a single convex optimization problem, incorporating bothtreatment and control units, and it enables the kernel trick to be used tohandle nonlinear (also non-parametric) estimation. Statistical learning boundsare also presented for the framework, and experimental results.

New Techniques for Preserving Global Structure and Denoising with Low  Information Loss in Single-Image Super-Resolution

  This work identifies and addresses two important technical challenges insingle-image super-resolution: (1) how to upsample an image without magnifyingnoise and (2) how to preserve large scale structure when upsampling. Wesummarize the techniques we developed for our second place entry in Track 1(Bicubic Downsampling), seventh place entry in Track 2 (Realistic AdverseConditions), and seventh place entry in Track 3 (Realistic difficult) in the2018 NTIRE Super-Resolution Challenge. Furthermore, we present new neuralnetwork architectures that specifically address the two challenges listedabove: denoising and preservation of large-scale structure.

Bayesian Patchworks: An Approach to Case-Based Reasoning

  Doctors often rely on their past experience in order to diagnose patients.For a doctor with enough experience, almost every patient would havesimilarities to key cases seen in the past, and each new patient could beviewed as a mixture of these key past cases. Because doctors often tend toreason this way, an efficient computationally aided diagnostic tool that thinksin the same way might be helpful in locating key past cases of interest thatcould assist with diagnosis. This article develops a novel mathematical modelto mimic the type of logical thinking that physicians use when considering pastcases. The proposed model can also provide physicians with explanations thatwould be similar to the way they would naturally reason about cases. Theproposed method is designed to yield predictive accuracy, computationalefficiency, and insight into medical data; the key element is the insight intomedical data, in some sense we are automating a complicated process thatphysicians might perform manually. We finally implemented the result of thiswork on two publicly available healthcare datasets, for heart diseaseprediction and breast cancer prediction.

MALTS: Matching After Learning to Stretch

  We introduce a flexible framework for matching in causal inference thatproduces high quality almost-exact matches. Most prior work in matching uses adhoc distance metrics, often leading to poor quality matches, particularly whenthere are irrelevant covariates that degrade the distance metric. In this work,we learn an interpretable distance metric used for matching, which leads tosubstantially higher quality matches. The distance metric can stretchcontinuous covariates and matches exactly on categorical covariates. Theframework is flexible in that the user can choose the form of distance metric,the type of optimization algorithm, and the type of relaxation for matching.Our ability to learn flexible distance metrics leads to matches that areinterpretable and useful for estimation of conditional average treatmenteffects.

An Interpretable Model with Globally Consistent Explanations for Credit  Risk

  We propose a possible solution to a public challenge posed by the Fair IsaacCorporation (FICO), which is to provide an explainable model for credit riskassessment. Rather than present a black box model and explain it afterwards, weprovide a globally interpretable model that is as accurate as other neuralnetworks. Our "two-layer additive risk model" is decomposable into subscales,where each node in the second layer represents a meaningful subscale, and allof the nonlinearities are transparent. We provide three types of explanationsthat are simpler than, but consistent with, the global model. One of theseexplanation methods involves solving a minimum set cover problem to findhigh-support globally-consistent explanations. We present a new onlinevisualization tool to allow users to explore the global model and itsexplanations.

Variable Importance Clouds: A Way to Explore Variable Importance for the  Set of Good Models

  Variable importance is central to scientific studies, including the socialsciences and causal inference, healthcare, and in other domains. However,current notions of variable importance are often tied to a specific predictivemodel. This is problematic: what if there were multiple well-performingpredictive models, and a specific variable is important to some of them and notto others? In that case, we may not be able to tell from a singlewell-performing model whether a variable is always important in predicting theoutcome. Rather than depending on variable importance for a single predictivemodel, we would like to explore variable importance for allapproximately-equally-accurate predictive models. This work introduces theconcept of a variable importance cloud, which maps every variable to itsimportance for every good predictive model. We show properties of the variableimportance cloud and draw connections other areas of statistics. We introducevariable importance diagrams as a projection of the variable importance cloudinto two dimensions for visualization purposes. Experiments with criminaljustice and marketing data illustrate how variables can change dramatically inimportance for approximately-equally-accurate predictive models.

A Practical Bandit Method with Advantages in Neural Network Tuning

  Stochastic bandit algorithms can be used for challenging non-convexoptimization problems. Hyperparameter tuning of neural networks is particularlychallenging, necessitating new approaches. To this end, we present a methodthat adaptively partitions the combined space of hyperparameters, context, andtraining resources (e.g., total number of training iterations). By adaptivelypartitioning the space, the algorithm is able to focus on the portions of thehyperparameter search space that are most relevant in a practical way. Byincluding the resources in the combined space, the method tends to use fewertraining resources overall. Our experiments show that this method can surpassstate-of-the-art methods in tuning neural networks on benchmark datasets. Insome cases, our implementations can achieve the same levels of accuracy onbenchmark datasets as existing state-of-the-art approaches while saving over50% of our computational resources (e.g. time, training iterations).

The Rate of Convergence of AdaBoost

  The AdaBoost algorithm was designed to combine many "weak" hypotheses thatperform slightly better than random guessing into a "strong" hypothesis thathas very low error. We study the rate at which AdaBoost iteratively convergesto the minimum of the "exponential loss." Unlike previous work, our proofs donot require a weak-learning assumption, nor do they require that minimizers ofthe exponential loss are finite. Our first result shows that at iteration $t$,the exponential loss of AdaBoost's computed parameter vector will be at most$\epsilon$ more than that of any parameter vector of $\ell_1$-norm bounded by$B$ in a number of rounds that is at most a polynomial in $B$ and $1/\epsilon$.We also provide lower bounds showing that a polynomial dependence on theseparameters is necessary. Our second result is that within $C/\epsilon$iterations, AdaBoost achieves a value of the exponential loss that is at most$\epsilon$ more than the best possible value, where $C$ depends on the dataset.We show that this dependence of the rate on $\epsilon$ is optimal up toconstant factors, i.e., at least $\Omega(1/\epsilon)$ rounds are necessary toachieve within $\epsilon$ of the optimal exponential loss.

Machine Learning with Operational Costs

  This work proposes a way to align statistical modeling with decision making.We provide a method that propagates the uncertainty in predictive modeling tothe uncertainty in operational cost, where operational cost is the amount spentby the practitioner in solving the problem. The method allows us to explore therange of operational costs associated with the set of reasonable statisticalmodels, so as to provide a useful way for practitioners to understanduncertainty. To do this, the operational cost is cast as a regularization termin a learning algorithm's objective function, allowing either an optimistic orpessimistic view of possible costs, depending on the regularization parameter.From another perspective, if we have prior knowledge about the operationalcost, for instance that it should be low, this knowledge can help to restrictthe hypothesis space, and can help with generalization. We provide atheoretical generalization bound for this scenario. We also show that learningwith operational costs is related to robust optimization.

Supersparse Linear Integer Models for Optimized Medical Scoring Systems

  Scoring systems are linear classification models that only require users toadd, subtract and multiply a few small numbers in order to make a prediction.These models are in widespread use by the medical community, but are difficultto learn from data because they need to be accurate and sparse, have coprimeinteger coefficients, and satisfy multiple operational constraints. We presenta new method for creating data-driven scoring systems called a SupersparseLinear Integer Model (SLIM). SLIM scoring systems are built by solving aninteger program that directly encodes measures of accuracy (the 0-1 loss) andsparsity (the $\ell_0$-seminorm) while restricting coefficients to coprimeintegers. SLIM can seamlessly incorporate a wide range of operationalconstraints related to accuracy and sparsity, and can produce highly tailoredmodels without parameter tuning. We provide bounds on the testing and trainingaccuracy of SLIM scoring systems, and present a new data reduction techniquethat can improve scalability by eliminating a portion of the training databeforehand. Our paper includes results from a collaboration with theMassachusetts General Hospital Sleep Laboratory, where SLIM was used to createa highly tailored scoring system for sleep apnea screening

Interpretable Classification Models for Recidivism Prediction

  We investigate a long-debated question, which is how to create predictivemodels of recidivism that are sufficiently accurate, transparent, andinterpretable to use for decision-making. This question is complicated as thesemodels are used to support different decisions, from sentencing, to determiningrelease on probation, to allocating preventative social services. Each use casemight have an objective other than classification accuracy, such as a desiredtrue positive rate (TPR) or false positive rate (FPR). Each (TPR, FPR) pair isa point on the receiver operator characteristic (ROC) curve. We use popularmachine learning methods to create models along the full ROC curve on a widerange of recidivism prediction problems. We show that many methods (SVM, RidgeRegression) produce equally accurate models along the full ROC curve. However,methods that designed for interpretability (CART, C5.0) cannot be tuned toproduce models that are accurate and/or interpretable. To handle thisshortcoming, we use a new method known as SLIM (Supersparse Linear IntegerModels) to produce accurate, transparent, and interpretable models along thefull ROC curve. These models can be used for decision-making for many differentuse cases, since they are just as accurate as the most powerful black-boxmachine learning models, but completely transparent, and highly interpretable.

Or's of And's for Interpretable Classification, with Application to  Context-Aware Recommender Systems

  We present a machine learning algorithm for building classifiers that arecomprised of a small number of disjunctions of conjunctions (or's of and's). Anexample of a classifier of this form is as follows: If X satisfies (x1 = 'blue'AND x3 = 'middle') OR (x1 = 'blue' AND x2 = '<15') OR (x1 = 'yellow'), then wepredict that Y=1, ELSE predict Y=0. An attribute-value pair is called a literaland a conjunction of literals is called a pattern. Models of this form have theadvantage of being interpretable to human experts, since they produce a set ofconditions that concisely describe a specific class. We present twoprobabilistic models for forming a pattern set, one with a Beta-Binomial prior,and the other with Poisson priors. In both cases, there are prior parametersthat the user can set to encourage the model to have a desired size and shape,to conform with a domain-specific definition of interpretability. We providetwo scalable MAP inference approaches: a pattern level search, which involvesassociation rule mining, and a literal level search. We show stronger priorsreduce computation. We apply the Bayesian Or's of And's (BOA) model to predictuser behavior with respect to in-vehicle context-aware personalized recommendersystems.

Regulating Greed Over Time

  In retail, there are predictable yet dramatic time-dependent patterns incustomer behavior, such as periodic changes in the number of visitors, orincreases in visitors just before major holidays. The current paradigm ofmulti-armed bandit analysis does not take these known patterns into account.This means that for applications in retail, where prices are fixed for periodsof time, current bandit algorithms will not suffice. This work provides aremedy that takes the time-dependent patterns into account, and we show howthis remedy is implemented in the UCB and {\epsilon}-greedy methods and weintroduce a new policy called the variable arm pool method. In the correctedmethods, exploitation (greed) is regulated over time, so that more exploitationoccurs during higher reward periods, and more exploration occurs in periods oflow reward. In order to understand why regret is reduced with the correctedmethods, we present a set of bounds that provide insight into why we would wantto exploit during periods of high reward, and discuss the impact on regret. Ourproposed methods perform well in experiments, and were inspired by ahigh-scoring entry in the Exploration and Exploitation 3 contest using datafrom Yahoo! Front Page. That entry heavily used time-series methods to regulategreed over time, which was substantially more effective than other contextualbandit methods.

