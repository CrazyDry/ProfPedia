Use of Weighted Finite State Transducers in Part of Speech Tagging

  This paper addresses issues in part of speech disambiguation usingfinite-state transducers and presents two main contributions to the field. Oneof them is the use of finite-state machines for part of speech tagging.Linguistic and statistical information is represented in terms of weights ontransitions in weighted finite-state transducers. Another contribution is thesuccessful combination of techniques -- linguistic and statistical -- for worddisambiguation, compounded with the notion of word classes.

Centroid-based summarization of multiple documents: sentence extraction,  utility-based evaluation, and user studies

  We present a multi-document summarizer, called MEAD, which generatessummaries using cluster centroids produced by a topic detection and trackingsystem. We also describe two new techniques, based on sentence utility andsubsumption, which we have applied to the evaluation of both single andmultiple document summaries. Finally, we describe two user studies that testour models of multi-document summarization.

Ranking suspected answers to natural language questions using predictive  annotation

  In this paper, we describe a system to rank suspected answers to naturallanguage questions. We process both corpus and query using a new technique,predictive annotation, which augments phrases in texts with labels anticipatingtheir being targets of certain kinds of questions. Given a natural languagequestion, an IR system returns a set of matching passages, which are thenanalyzed and ranked according to various criteria described in this paper. Weprovide an evaluation of the techniques based on results from the TREC Q&Aevaluation in which our system participated.

Building a Generation Knowledge Source using Internet-Accessible  Newswire

  In this paper, we describe a method for automatic creation of a knowledgesource for text generation using information extraction over the Internet. Wepresent a prototype system called PROFILE which uses a client-serverarchitecture to extract noun-phrase descriptions of entities such as people,places, and organizations. The system serves two purposes: as an informationextraction tool, it allows users to search for textual descriptions ofentities; as a utility to generate functional descriptions (FD), it is used ina functional-unification based generation system. We present an evaluation ofthe approach and its applications to natural language generation andsummarization.

Tagging French Without Lexical Probabilities -- Combining Linguistic  Knowledge And Statistical Learning

  This paper explores morpho-syntactic ambiguities for French to develop astrategy for part-of-speech disambiguation that a) reflects the complexity ofFrench as an inflected language, b) optimizes the estimation of probabilities,c) allows the user flexibility in choosing a tagset. The problem in extractinglexical probabilities from a limited training corpus is that the statisticalmodel may not necessarily represent the use of a particular word in aparticular context. In a highly morphologically inflected language, thisargument is particularly serious since a word can be tagged with a large numberof parts of speech. Due to the lack of sufficient training data, we argueagainst estimating lexical probabilities to disambiguate parts of speech inunrestricted texts. Instead, we use the strength of contextual probabilitiesalong with a feature we call ``genotype'', a set of tags associated with aword. Using this knowledge, we have built a part-of-speech tagger that combineslinguistic and statistical approaches: contextual information is disambiguatedby linguistic rules and n-gram probabilities on parts of speech only areestimated in order to disambiguate the remaining ambiguous tags.

Learning Correlations between Linguistic Indicators and Semantic  Constraints: Reuse of Context-Dependent Descriptions of Entities

  This paper presents the results of a study on the semantic constraintsimposed on lexical choice by certain contextual indicators. We show how suchindicators are computed and how correlations between them and the choice of anoun phrase description of a named entity can be automatically establishedusing supervised learning. Based on this correlation, we have developed atechnique for automatic lexical choice of descriptions of entities in textgeneration. We discuss the underlying relationship between the pragmatics ofchoosing an appropriate description that serves a specific purpose in theautomatically generated text and the semantics of the description itself. Wepresent our work in the framework of the more general concept of reuse oflinguistic structures that are automatically extracted from large corpora. Wepresent a formal evaluation of our approach and we conclude with some thoughtson potential applications of our method.

Scientific Paper Summarization Using Citation Summary Networks

  Quickly moving to a new area of research is painful for researchers due tothe vast amount of scientific literature in each field of study. One possibleway to overcome this problem is to summarize a scientific topic. In this paper,we propose a model of summarizing a single article, which can be further usedto summarize an entire topic. Our model is based on analyzing others' viewpointof the target article's contributions and the study of its citation summarynetwork using a clustering approach.

What Should I Learn First: Introducing LectureBank for NLP Education and  Prerequisite Chain Learning

  Recent years have witnessed the rising popularity of Natural LanguageProcessing (NLP) and related fields such as Artificial Intelligence (AI) andMachine Learning (ML). Many online courses and resources are available even forthose without a strong background in the field. Often the student is curiousabout a specific topic but does not quite know where to begin studying. Toanswer the question of "what should one learn first," we apply anembedding-based method to learn prerequisite relations for course concepts inthe domain of NLP. We introduce LectureBank, a dataset containing 1,352 Englishlecture files collected from university courses which are each classifiedaccording to an existing taxonomy as well as 208 manually-labeled prerequisiterelation topics, which is publicly available. The dataset will be useful foreducational purposes such as lecture preparation and organization as well asapplications such as reading list generation. Additionally, we experiment withneural graph-based networks and non-neural classifiers to learn theseprerequisite relations from our dataset.

A Computational Analysis of Collective Discourse

  This paper is focused on the computational analysis of collective discourse,a collective behavior seen in non-expert content contributions in online socialmedia. We collect and analyze a wide range of real-world collective discoursedatasets from movie user reviews to microblogs and news headlines to scientificcitations. We show that all these datasets exhibit diversity of perspective, aproperty seen in other collective systems and a criterion in wise crowds. Ourexperiments also confirm that the network of different perspectiveco-occurrences exhibits the small-world property with high clustering ofdifferent perspectives. Finally, we show that non-expert contributions incollective discourse can be used to answer simple questions that are otherwisehard to answer.

Generating Extractive Summaries of Scientific Paradigms

  Researchers and scientists increasingly find themselves in the position ofhaving to quickly understand large amounts of technical material. Our goal isto effectively serve this need by using bibliometric text mining andsummarization techniques to generate summaries of scientific literature. Weshow how we can use citations to produce automatically generated, readilyconsumable, technical extractive summaries. We first propose C-LexRank, a modelfor summarizing single scientific articles based on citations, which employscommunity detection and extracts salient information-rich sentences. Next, wefurther extend our experiments to summarize a set of papers, which cover thesame scientific topic. We generate extractive summaries of a set of QuestionAnswering (QA) and Dependency Parsing (DP) papers, their abstracts, and theircitation sentences and show that citations have unique information amenable tocreating a summary.

Zero-shot Transfer Learning for Semantic Parsing

  While neural networks have shown impressive performance on large datasets,applying these models to tasks where little data is available remains achallenging problem.  In this paper we propose to use feature transfer in a zero-shot experimentalsetting on the task of semantic parsing.  We first introduce a new method for learning the shared space betweenmultiple domains based on the prediction of the domain label for each example.  Our experiments support the superiority of this method in a zero-shotexperimental setting in terms of accuracy metrics compared to state-of-the-arttechniques.  In the second part of this paper we study the impact of individual domainsand examples on semantic parsing performance.  We use influence functions to this aim and investigate the sensitivity ofdomain-label classification loss on each example.  Our findings reveal that cross-domain adversarial attacks identify usefulexamples for training even from the domains the least similar to the targetdomain. Augmenting our training data with these influential examples furtherboosts our accuracy at both the token and the sequence level.

TutorialBank: A Manually-Collected Corpus for Prerequisite Chains,  Survey Extraction and Resource Recommendation

  The field of Natural Language Processing (NLP) is growing rapidly, with newresearch published daily along with an abundance of tutorials, codebases andother online resources. In order to learn this dynamic field or stay up-to-dateon the latest research, students as well as educators and researchers mustconstantly sift through multiple sources to find valuable, relevantinformation. To address this situation, we introduce TutorialBank, a new,publicly available dataset which aims to facilitate NLP education and research.We have manually collected and categorized over 6,300 resources on NLP as wellas the related fields of Artificial Intelligence (AI), Machine Learning (ML)and Information Retrieval (IR). Our dataset is notably the largestmanually-picked corpus of resources intended for NLP education which does notinclude only academic papers. Additionally, we have created both a searchengine and a command-line tool for the resources and have annotated the corpusto include lists of research topics, relevant resources for each topic,prerequisite relations among topics, relevant sub-parts of individualresources, among other annotations. We are releasing the dataset and presentseveral avenues for further research.

LexRank: Graph-based Lexical Centrality as Salience in Text  Summarization

  We introduce a stochastic graph-based method for computing relativeimportance of textual units for Natural Language Processing. We test thetechnique on the problem of Text Summarization (TS). Extractive TS relies onthe concept of sentence salience to identify the most important sentences in adocument or set of documents. Salience is typically defined in terms of thepresence of particular important words or in terms of similarity to a centroidpseudo-sentence. We consider a new approach, LexRank, for computing sentenceimportance based on the concept of eigenvector centrality in a graphrepresentation of sentences. In this model, a connectivity matrix based onintra-sentence cosine similarity is used as the adjacency matrix of the graphrepresentation of sentences. Our system, based on LexRank ranked in first placein more than one task in the recent DUC 2004 evaluation. In this paper wepresent a detailed analysis of our approach and apply it to a larger data setincluding data from earlier DUC evaluations. We discuss several methods tocompute centrality using the similarity graph. The results show thatdegree-based methods (including LexRank) outperform both centroid-based methodsand other systems participating in DUC in most of the cases. Furthermore, theLexRank with threshold method outperforms the other degree-based techniquesincluding continuous LexRank. We also show that our approach is quiteinsensitive to the noise in the data that may result from an imperfect topicalclustering of documents.

