Time-varying Learning and Content Analytics via Sparse Factor Analysis

  We propose SPARFA-Trace, a new machine learning-based framework fortime-varying learning and content analytics for education applications. Wedevelop a novel message passing-based, blind, approximate Kalman filter forsparse factor analysis (SPARFA), that jointly (i) traces learner conceptknowledge over time, (ii) analyzes learner concept knowledge state transitions(induced by interacting with learning resources, such as textbook sections,lecture videos, etc, or the forgetting effect), and (iii) estimates the contentorganization and intrinsic difficulty of the assessment questions. Thesequantities are estimated solely from binary-valued (correct/incorrect) gradedlearner response data and a summary of the specific actions each learnerperforms (e.g., answering a question or studying a learning resource) at eachtime instance. Experimental results on two online course datasets demonstratethat SPARFA-Trace is capable of tracing each learner's concept knowledgeevolution over time, as well as analyzing the quality and content organizationof learning resources, the question-concept associations, and the questionintrinsic difficulties. Moreover, we show that SPARFA-Trace achieves comparableor better performance in predicting unobserved learner responses than existingcollaborative filtering and knowledge tracing approaches for personalizededucation.

Quantized Matrix Completion for Personalized Learning

  The recently proposed SPARse Factor Analysis (SPARFA) framework forpersonalized learning performs factor analysis on ordinal or binary-valued(e.g., correct/incorrect) graded learner responses to questions. The underlyingfactors are termed "concepts" (or knowledge components) and are used forlearning analytics (LA), the estimation of learner concept-knowledge profiles,and for content analytics (CA), the estimation of question-concept associationsand question difficulties. While SPARFA is a powerful tool for LA and CA, itrequires a number of algorithm parameters (including the number of concepts),which are difficult to determine in practice. In this paper, we proposeSPARFA-Lite, a convex optimization-based method for LA that builds on matrixcompletion, which only requires a single algorithm parameter and enables us toautomatically identify the required number of concepts. Using a variety ofeducational datasets, we demonstrate that SPARFALite (i) achieves comparableperformance in predicting unobserved learner responses to existing methods,including item response theory (IRT) and SPARFA, and (ii) is computationallymore efficient.

Linearized Binary Regression

  Probit regression was first proposed by Bliss in 1934 to study mortalityrates of insects. Since then, an extensive body of work has analyzed and usedprobit or related binary regression methods (such as logistic regression) innumerous applications and fields. This paper provides a fresh angle to suchwell-established binary regression methods. Concretely, we demonstrate thatlinearizing the probit model in combination with linear estimators performs onpar with state-of-the-art nonlinear regression methods, such as posterior meanor maximum aposteriori estimation, for a broad range of real-world regressionproblems. We derive exact, closed-form, and nonasymptotic expressions for themean-squared error of our linearized estimators, which clearly separates themfrom nonlinear regression methods that are typically difficult to analyze. Weshowcase the efficacy of our methods and results for a number of synthetic andreal-world datasets, which demonstrates that linearized binary regression findspotential use in a variety of inference, estimation, signal processing, andmachine learning applications that deal with binary-valued observations ormeasurements.

Joint Topic Modeling and Factor Analysis of Textual Information and  Graded Response Data

  Modern machine learning methods are critical to the development oflarge-scale personalized learning systems that cater directly to the needs ofindividual learners. The recently developed SPARse Factor Analysis (SPARFA)framework provides a new statistical model and algorithms for machinelearning-based learning analytics, which estimate a learner's knowledge of thelatent concepts underlying a domain, and content analytics, which estimate therelationships among a collection of questions and the latent concepts. SPARFAestimates these quantities given only the binary-valued graded responses to acollection of questions. In order to better interpret the estimated latentconcepts, SPARFA relies on a post-processing step that utilizes user-definedtags (e.g., topics or keywords) available for each question. In this paper, werelax the need for user-defined tags by extending SPARFA to jointly processboth graded learner responses and the text of each question and its associatedanswer(s) or other feedback. Our purely data-driven approach (i) enhances theinterpretability of the estimated latent concepts without the need ofexplicitly generating a set of tags or performing a post-processing step, (ii)improves the prediction performance of SPARFA, and (iii) scales to largetest/assessments where human annotation would prove burdensome. We demonstratethe efficacy of the proposed approach on two real educational datasets.

Sparse Factor Analysis for Learning and Content Analytics

  We develop a new model and algorithms for machine learning-based learninganalytics, which estimate a learner's knowledge of the concepts underlying adomain, and content analytics, which estimate the relationships among acollection of questions and those concepts. Our model represents theprobability that a learner provides the correct response to a question in termsof three factors: their understanding of a set of underlying concepts, theconcepts involved in each question, and each question's intrinsic difficulty.We estimate these factors given the graded responses to a collection ofquestions. The underlying estimation problem is ill-posed in general,especially when only a subset of the questions are answered. The keyobservation that enables a well-posed solution is the fact that typicaleducational domains of interest involve only a small number of key concepts.Leveraging this observation, we develop both a bi-convex maximum-likelihood anda Bayesian solution to the resulting SPARse Factor Analysis (SPARFA) problem.We also incorporate user-defined tags on questions to facilitate theinterpretability of the estimated factors. Experiments with synthetic andreal-world data demonstrate the efficacy of our approach. Finally, we make aconnection between SPARFA and noisy, binary-valued (1-bit) dictionary learningthat is of independent interest.

Tag-Aware Ordinal Sparse Factor Analysis for Learning and Content  Analytics

  Machine learning offers novel ways and means to design personalized learningsystems wherein each student's educational experience is customized in realtime depending on their background, learning goals, and performance to date.SPARse Factor Analysis (SPARFA) is a novel framework for machine learning-basedlearning analytics, which estimates a learner's knowledge of the conceptsunderlying a domain, and content analytics, which estimates the relationshipsamong a collection of questions and those concepts. SPARFA jointly learns theassociations among the questions and the concepts, learner concept knowledgeprofiles, and the underlying question difficulties, solely based on thecorrect/incorrect graded responses of a population of learners to a collectionof questions. In this paper, we extend the SPARFA framework significantly toenable: (i) the analysis of graded responses on an ordinal scale (partialcredit) rather than a binary scale (correct/incorrect); (ii) the exploitationof tags/labels for questions that partially describe the question{conceptassociations. The resulting Ordinal SPARFA-Tag framework greatly enhances theinterpretability of the estimated concepts. We demonstrate using realeducational data that Ordinal SPARFA-Tag outperforms both SPARFA and existingcollaborative filtering techniques in predicting missing learner responses.

Personalized Thread Recommendation for MOOC Discussion Forums

  Social learning, i.e., students learning from each other through socialinteractions, has the potential to significantly scale up instruction in onlineeducation. In many cases, such as in massive open online courses (MOOCs),social learning is facilitated through discussion forums hosted by courseproviders. In this paper, we propose a probabilistic model for the process oflearners posting on such forums, using point processes. Different from existingworks, our method integrates topic modeling of the post text, timescalemodeling of the decay in post activity over time, and learner topic interestmodeling into a single model, and infers this information from user data. Ourmethod also varies the excitation levels induced by posts according to thethread structure, to reflect typical notification settings in discussionforums. We experimentally validate the proposed model on three real-world MOOCdatasets, with the largest one containing up to 6,000 learners making 40,000posts in 5,000 threads. Results show that our model excels at threadrecommendation, achieving significant improvement over a number of baselines,thus showing promise of being able to direct learners to threads that they areinterested in more efficiently. Moreover, we demonstrate analytics that ourmodel parameters can provide, such as the timescales of different topiccategories in a course.

Insense: Incoherent Sensor Selection for Sparse Signals

  Sensor selection refers to the problem of intelligently selecting a smallsubset of a collection of available sensors to reduce the sensing cost whilepreserving signal acquisition performance. The majority of sensor selectionalgorithms find the subset of sensors that best recovers an arbitrary signalfrom a number of linear measurements that is larger than the dimension of thesignal. In this paper, we develop a new sensor selection algorithm for sparse(or near sparse) signals that finds a subset of sensors that best recovers suchsignals from a number of measurements that is much smaller than the dimensionof the signal. Existing sensor selection algorithms cannot be applied in suchsituations. Our proposed Incoherent Sensor Selection (Insense) algorithmminimizes a coherence-based cost function that is adapted from recent resultsin sparse recovery theory. Using six datasets, including two real-worlddatasets on microbial diagnostics and structural health monitoring, wedemonstrate the superior performance of Insense for sparse-signal sensorselection.

Data-Mining Textual Responses to Uncover Misconception Patterns

  An important, yet largely unstudied, problem in student data analysis is todetect misconceptions from students' responses to open-response questions.Misconception detection enables instructors to deliver more targeted feedbackon the misconceptions exhibited by many students in their class, thus improvingthe quality of instruction. In this paper, we propose a new natural languageprocessing-based framework to detect the common misconceptions among students'textual responses to short-answer questions. We propose a probabilistic modelfor students' textual responses involving misconceptions and experimentallyvalidate it on a real-world student-response dataset. Experimental results showthat our proposed framework excels at classifying whether a response exhibitsone or more misconceptions. More importantly, it can also automatically detectthe common misconceptions exhibited across responses from multiple students tomultiple questions; this property is especially important at large scale, sinceinstructors will no longer need to manually specify all possible misconceptionsthat students might exhibit.

PhaseLin: Linear Phase Retrieval

  Phase retrieval deals with the recovery of complex- or real-valued signalsfrom magnitude measurements. As shown recently, the method PhaseMax enablesphase retrieval via convex optimization and without lifting the problem to ahigher dimension. To succeed, PhaseMax requires an initial guess of thesolution, which can be calculated via spectral initializers. In this paper, weshow that with the availability of an initial guess, phase retrieval can becarried out with an ever simpler, linear procedure. Our algorithm, calledPhaseLin, is the linear estimator that minimizes the mean squared error (MSE)when applied to the magnitude measurements. The linear nature of PhaseLinenables an exact and nonasymptotic MSE analysis for arbitrary measurementmatrices. We furthermore demonstrate that by iteratively using PhaseLin, onearrives at an efficient phase retrieval algorithm that performs on par withexisting convex and nonconvex methods on synthetic and real-world data.

Linear Spectral Estimators and an Application to Phase Retrieval

  Phase retrieval refers to the problem of recovering real- or complex-valuedvectors from magnitude measurements. The best-known algorithms for this problemare iterative in nature and rely on so-called spectral initializers thatprovide accurate initialization vectors. We propose a novel class of estimatorssuitable for general nonlinear measurement systems, called linear spectralestimators (LSPEs), which can be used to compute accurate initializationvectors for phase retrieval problems. The proposed LSPEs not only provideaccurate initialization vectors for noisy phase retrieval systems withstructured or random measurement matrices, but also enable the derivation ofsharp and nonasymptotic mean-squared error bounds. We demonstrate the efficacyof LSPEs on synthetic and real-world phase retrieval problems, and show thatour estimators significantly outperform existing methods for structuredmeasurement systems that arise in practice.

An Estimation and Analysis Framework for the Rasch Model

  The Rasch model is widely used for item response analysis in applicationsranging from recommender systems to psychology, education, and finance. While anumber of estimators have been proposed for the Rasch model over the lastdecades, the available analytical performance guarantees are mostly asymptotic.This paper provides a framework that relies on a novel linear minimummean-squared error (L-MMSE) estimator which enables an exact, nonasymptotic,and closed-form analysis of the parameter estimation error under the Raschmodel. The proposed framework provides guidelines on the number of items andresponses required to attain low estimation errors in tests or surveys. Wefurthermore demonstrate its efficacy on a number of real-world collaborativefiltering datasets, which reveals that the proposed L-MMSE estimator performson par with state-of-the-art nonlinear estimators in terms of predictiveperformance.

The galaxy population in cold and warm dark matter cosmologies

  We use a pair of high resolution N-body simulations implementing two darkmatter models, namely the standard cold dark matter (CDM) cosmogony and a warmdark matter (WDM) alternative where the dark matter particle is a 1.5keVthermal relic. We combine these simulations with the GALFORM semi-analyticalgalaxy formation model in order to explore differences between the resultinggalaxy populations. We use GALFORM model variants for CDM and WDM that resultin the same z=0 galaxy stellar mass function by construction. We find that mostof the studied galaxy properties have the same values in these two models,indicating that both dark matter scenarios match current observational dataequally well. Even in under-dense regions, where discrepancies in structureformation between CDM and WDM are expected to be most pronounced, the galaxyproperties are only slightly different. The only significant difference in thelocal universe we find is in the galaxy populations of "Local Volumes", regionsof radius 1 to 8Mpc around simulated Milky Way analogues. In such regions ourWDM model provides a better match to observed local galaxy number counts and isfive times more likely than the CDM model to predict sub-regions within themthat are as empty as the observed Local Void. Thus, a highly complete census ofthe Local Volume and future surveys of void regions could provide constraintson the nature of dark matter.

Unified storage systems for distributed Tier-2 centres

  The start of data taking at the Large Hadron Collider will herald a new erain data volumes and distributed processing in particle physics. Data volumes ofhundreds of Terabytes will be shipped to Tier-2 centres for analysis by the LHCexperiments using the Worldwide LHC Computing Grid (WLCG).  In many countries Tier-2 centres are distributed between a number ofinstitutes, e.g., the geographically spread Tier-2s of GridPP in the UK. Thispresents a number of challenges for experiments to utilise these centresefficaciously, as CPU and storage resources may be sub-divided and exposed insmaller units than the experiment would ideally want to work with. In addition,unhelpful mismatches between storage and CPU at the individual centres may beseen, which make efficient exploitation of a Tier-2's resources difficult.  One method of addressing this is to unify the storage across a distributedTier-2, presenting the centres' aggregated storage as a single system. Thisgreatly simplifies data management for the VO, which then can access a greateramount of data across the Tier-2. However, such an approach will lead toscenarios where analysis jobs on one site's batch system must access datahosted on another site.  We investigate this situation using the Glasgow and Edinburgh clusters, whichare part of the ScotGrid distributed Tier-2. In particular we look at how tomitigate the problems associated with ``distant'' data access and discuss thesecurity implications of having LAN access protocols traverse the WAN betweencentres.

Towards the solution of the many-electron problem in real materials:  equation of state of the hydrogen chain with state-of-the-art many-body  methods

  We present numerical results for the equation of state of an infinite chainof hydrogen atoms. A variety of modern many-body methods are employed, withexhaustive cross-checks and validation. Approaches for reaching the continuousspace limit and the thermodynamic limit are investigated, proposed, and tested.The detailed comparisons provide a benchmark for assessing the current state ofthe art in many-body computation, and for the development of new methods. Theground-state energy per atom in the linear chain is accurately determinedversus bondlength, with a confidence bound given on all uncertainties.

Mathematical Language Processing: Automatic Grading and Feedback for  Open Response Mathematical Questions

  While computer and communication technologies have provided effective meansto scale up many aspects of education, the submission and grading ofassessments such as homework assignments and tests remains a weak link. In thispaper, we study the problem of automatically grading the kinds of open responsemathematical questions that figure prominently in STEM (science, technology,engineering, and mathematics) courses. Our data-driven framework formathematical language processing (MLP) leverages solution data from a largenumber of learners to evaluate the correctness of their solutions, assignpartial-credit scores, and provide feedback to each learner on the likelylocations of any errors. MLP takes inspiration from the success of naturallanguage processing for text data and comprises three main steps. First, weconvert each solution to an open response mathematical question into a seriesof numerical features. Second, we cluster the features from several solutionsto uncover the structures of correct, partially correct, and incorrectsolutions. We develop two different clustering approaches, one that leveragesgeneric clustering algorithms and one based on Bayesian nonparametrics. Third,we automatically grade the remaining (potentially large number of) solutionsbased on their assigned cluster and one instructor-provided grade per cluster.As a bonus, we can track the cluster assignment of each step of a multistepsolution and determine when it departs from a cluster of correct solutions,which enables us to indicate the likely locations of errors to learners. Wetest and validate MLP on real-world MOOC data to demonstrate how it cansubstantially reduce the human effort required in large-scale educationalplatforms.

Understanding the circumgalactic medium is critical for understanding  galaxy evolution

  Galaxies evolve under the influence of gas flows between their interstellarmedium and their surrounding gaseous halos known as the circumgalactic medium(CGM). The CGM is a major reservoir of galactic baryons and metals, and plays akey role in the long cycles of accretion, feedback, and recycling of gas thatdrive star formation. In order to fully understand the physical processes atwork within galaxies, it is therefore essential to have a firm understanding ofthe composition, structure, kinematics, thermodynamics, and evolution of theCGM. In this white paper we outline connections between the CGM and galacticstar formation histories, internal kinematics, chemical evolution, quenching,satellite evolution, dark matter halo occupation, and the reionization of thelarger-scale intergalactic medium in light of the advances that will be made onthese topics in the 2020s. We argue that, in the next decade, fundamentalprogress on all of these major issues depends critically on improved empiricalcharacterization and theoretical understanding of the CGM. In particular, wediscuss how future advances in spatially-resolved CGM observations at highspectral resolution, broader characterization of the CGM across galaxy mass andredshift, and expected breakthroughs in cosmological hydrodynamic simulationswill help resolve these major problems in galaxy evolution.

The Fifteenth Data Release of the Sloan Digital Sky Surveys: First  Release of MaNGA Derived Quantities, Data Visualization Tools and Stellar  Library

  Twenty years have passed since first light for the Sloan Digital Sky Survey(SDSS). Here, we release data taken by the fourth phase of SDSS (SDSS-IV)across its first three years of operation (July 2014-July 2017). This is thethird data release for SDSS-IV, and the fifteenth from SDSS (Data ReleaseFifteen; DR15). New data come from MaNGA - we release 4824 datacubes, as wellas the first stellar spectra in the MaNGA Stellar Library (MaStar), the firstset of survey-supported analysis products (e.g. stellar and gas kinematics,emission line, and other maps) from the MaNGA Data Analysis Pipeline (DAP), anda new data visualisation and access tool we call "Marvin". The next datarelease, DR16, will include new data from both APOGEE-2 and eBOSS; thosesurveys release no new data here, but we document updates and corrections totheir data processing pipelines. The release is cumulative; it also includesthe most recent reductions and calibrations of all data taken by SDSS sincefirst light. In this paper we describe the location and format of the data andtools and cite technical references describing how it was obtained andprocessed. The SDSS website (www.sdss.org) has also been updated, providinglinks to data downloads, tutorials and examples of data use. While SDSS-IV willcontinue to collect astronomical data until 2020, and will be followed bySDSS-V (2020-2025), we end this paper by describing plans to ensure thesustainability of the SDSS data archive for many years beyond the collection ofdata.

The Tenth Data Release of the Sloan Digital Sky Survey: First  Spectroscopic Data from the SDSS-III Apache Point Observatory Galactic  Evolution Experiment

  The Sloan Digital Sky Survey (SDSS) has been in operation since 2000 April.This paper presents the tenth public data release (DR10) from its currentincarnation, SDSS-III. This data release includes the first spectroscopic datafrom the Apache Point Observatory Galaxy Evolution Experiment (APOGEE), alongwith spectroscopic data from the Baryon Oscillation Spectroscopic Survey (BOSS)taken through 2012 July. The APOGEE instrument is a near-infrared R~22,500300-fiber spectrograph covering 1.514--1.696 microns. The APOGEE survey isstudying the chemical abundances and radial velocities of roughly 100,000 redgiant star candidates in the bulge, bar, disk, and halo of the Milky Way. DR10includes 178,397 spectra of 57,454 stars, each typically observed three or moretimes, from APOGEE. Derived quantities from these spectra (radial velocities,effective temperatures, surface gravities, and metallicities) are alsoincluded.DR10 also roughly doubles the number of BOSS spectra over thoseincluded in the ninth data release. DR10 includes a total of 1,507,954 BOSSspectra, comprising 927,844 galaxy spectra; 182,009 quasar spectra; and 159,327stellar spectra, selected over 6373.2 square degrees.

The Eleventh and Twelfth Data Releases of the Sloan Digital Sky Survey:  Final Data from SDSS-III

  The third generation of the Sloan Digital Sky Survey (SDSS-III) took datafrom 2008 to 2014 using the original SDSS wide-field imager, the original andan upgraded multi-object fiber-fed optical spectrograph, a new near-infraredhigh-resolution spectrograph, and a novel optical interferometer. All the datafrom SDSS-III are now made public. In particular, this paper describes DataRelease 11 (DR11) including all data acquired through 2013 July, and DataRelease 12 (DR12) adding data acquired through 2014 July (including all dataincluded in previous data releases), marking the end of SDSS-III observing.Relative to our previous public release (DR10), DR12 adds one million newspectra of galaxies and quasars from the Baryon Oscillation SpectroscopicSurvey (BOSS) over an additional 3000 sq. deg of sky, more than triples thenumber of H-band spectra of stars as part of the Apache Point Observatory (APO)Galactic Evolution Experiment (APOGEE), and includes repeated accurate radialvelocity measurements of 5500 stars from the Multi-Object APO Radial VelocityExoplanet Large-area Survey (MARVELS). The APOGEE outputs now include measuredabundances of 15 different elements for each star. In total, SDSS-III added2350 sq. deg of ugriz imaging; 155,520 spectra of 138,099 stars as part of theSloan Exploration of Galactic Understanding and Evolution 2 (SEGUE-2) survey;2,497,484 BOSS spectra of 1,372,737 galaxies, 294,512 quasars, and 247,216stars over 9376 sq. deg; 618,080 APOGEE spectra of 156,593 stars; and 197,040MARVELS spectra of 5,513 stars. Since its first light in 1998, SDSS has imagedover 1/3 of the Celestial sphere in five bands and obtained over five millionastronomical spectra.

