Decision Lists for Lexical Ambiguity Resolution: Application to Accent  Restoration in Spanish and French

  This paper presents a statistical decision procedure for lexical ambiguityresolution. The algorithm exploits both local syntactic patterns and moredistant collocational evidence, generating an efficient, effective, and highlyperspicuous recipe for resolving a given ambiguity. By identifying andutilizing only the single best disambiguating evidence in a target context, thealgorithm avoids the problematic complex modeling of statistical dependencies.Although directly applicable to a wide class of ambiguities, the algorithm isdescribed and evaluated in a realistic case study, the problem of restoringmissing accents in Spanish and French text.

Dynamic Nonlocal Language Modeling via Hierarchical Topic-Based  Adaptation

  This paper presents a novel method of generating and applying hierarchical,dynamic topic-based language models. It proposes and evaluates new clustergeneration, hierarchical smoothing and adaptive topic-probability estimationtechniques. These combined models help capture long-distance lexicaldependencies. Experiments on the Broadcast News corpus show significantimprovement in perplexity (10.5% overall and 33.5% on target vocabulary).

Rule Writing or Annotation: Cost-efficient Resource Usage for Base Noun  Phrase Chunking

  This paper presents a comprehensive empirical comparison between twoapproaches for developing a base noun phrase chunker: human rule writing andactive learning using interactive real-time human annotation. Several novelvariations on active learning are investigated, and underlying cost models forcross-modal machine learning comparison are presented and explored. Resultsshow that it is more efficient and more successful by several measures to traina system using active learning annotation rather than hand-crafted rule writingat a comparable level of human labor investment.

Paradigm Completion for Derivational Morphology

  The generation of complex derived word forms has been an overlooked problemin NLP; we fill this gap by applying neural sequence-to-sequence models to thetask. We overview the theoretical motivation for a paradigmatic treatment ofderivational morphology, and introduce the task of derivational paradigmcompletion as a parallel to inflectional paradigm completion. State-of-the-artneural models, adapted from the inflection task, are able to learn a range ofderivation patterns, and outperform a non-neural baseline by 16.4%. However,due to semantic, historical, and lexical considerations involved inderivational morphology, future work will be needed to achieve performanceparity with inflection-generating systems.

Massively Multilingual Adversarial Speech Recognition

  We report on adaptation of multilingual end-to-end speech recognition modelstrained on as many as 100 languages. Our findings shed light on the relativeimportance of similarity between the target and pretraining languages along thedimensions of phonetics, phonology, language family, geographical location, andorthography. In this context, experiments demonstrate the effectiveness of twoadditional pretraining objectives in encouraging language-independent encoderrepresentations: a context-independent phoneme objective paired with alanguage-adversarial classification objective.

Decision Lists for English and Basque

  In this paper we describe the systems we developed for the English (lexicaland all-words) and Basque tasks. They were all supervised systems based onYarowsky's Decision Lists. We used Semcor for training in the English all-wordstask. We defined different feature sets for each language. For Basque, in orderto extract all the information from the text, we defined features that have notbeen used before in the literature, using a morphological analyzer. We alsoimplemented systems that selected automatically good features and were able toobtain a prefixed precision (85%) at the cost of coverage. The systems thatused all the features were identified as BCU-ehu-dlist-all and the systems thatselected some features as BCU-ehu-dlist-best.

Marrying Universal Dependencies and Universal Morphology

  The Universal Dependencies (UD) and Universal Morphology (UniMorph) projectseach present schemata for annotating the morphosyntactic details of language.Each project also provides corpora of annotated text in many languages - UD atthe token level and UniMorph at the type level. As each corpus is built bydifferent annotators, language-specific decisions hinder the goal of universalschemata. With compatibility of tags, each project's annotations could be usedto validate the other's. Additionally, the availability of both type- andtoken-level resources would be a boon to tasks such as parsing and homographdisambiguation. To ease this interoperability, we present a deterministicmapping from Universal Dependencies v2 features into the UniMorph schema. Wevalidate our approach by lookup in the UniMorph corpora and find amacro-average of 64.13% recall. We also note incompatibilities due to paucityof data on either side. Finally, we present a critical evaluation of thefoundations, strengths, and weaknesses of the two annotation projects.

UniMorph 2.0: Universal Morphology

  The Universal Morphology UniMorph project is a collaborative effort toimprove how NLP handles complex morphology across the world's languages. Theproject releases annotated morphological data using a universal tagset, theUniMorph schema. Each inflected form is associated with a lemma, whichtypically carries its underlying lexical meaning, and a bundle of morphologicalfeatures from our schema. Additional supporting data and tools are alsoreleased on a per-language basis when available. UniMorph is based at theCenter for Language and Speech Processing (CLSP) at Johns Hopkins University inBaltimore, Maryland and is sponsored by the DARPA LORELEI program. This paperdetails advances made to the collection, annotation, and dissemination ofproject resources since the initial UniMorph release described at LREC 2016.lexical resources} }

CoNLL-SIGMORPHON 2017 Shared Task: Universal Morphological Reinflection  in 52 Languages

  The CoNLL-SIGMORPHON 2017 shared task on supervised morphological generationrequired systems to be trained and tested in each of 52 typologically diverselanguages. In sub-task 1, submitted systems were asked to predict a specificinflected form of a given lemma. In sub-task 2, systems were given a lemma andsome of its specific inflected forms, and asked to complete the inflectionalparadigm by predicting all of the remaining inflected forms. Both sub-tasksincluded high, medium, and low-resource conditions. Sub-task 1 received 24system submissions, while sub-task 2 received 3 system submissions. Followingthe success of neural sequence-to-sequence models in the SIGMORPHON 2016 sharedtask, all but one of the submissions included a neural component. The resultsshow that high performance can be achieved with small training datasets, solong as models have appropriate inductive bias or make use of additionalunlabeled data or synthetic data. However, different biasing and dataaugmentation resulted in disjoint sets of inflected forms being predictedcorrectly, suggesting that there is room for future improvement.

The CoNLL--SIGMORPHON 2018 Shared Task: Universal Morphological  Reinflection

  The CoNLL--SIGMORPHON 2018 shared task on supervised learning ofmorphological generation featured data sets from 103 typologically diverselanguages. Apart from extending the number of languages involved in earliersupervised tasks of generating inflected forms, this year the shared task alsofeatured a new second task which asked participants to inflect words insentential context, similar to a cloze task. This second task featured sevenlanguages. Task 1 received 27 submissions and task 2 received 6 submissions.Both tasks featured a low, medium, and high data condition. Nearly allsubmissions featured a neural component and built on highly-ranked systems fromthe earlier 2017 shared task. In the inflection task (task 1), 41 of the 52languages present in last year's inflection task showed improvement by the bestsystems in the low-resource setting. The cloze task (task 2) proved to bedifficult, and few submissions managed to consistently improve upon both asimple neural baseline system and a lemma-repeating baseline.

