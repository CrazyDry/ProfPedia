Constrained Submodular Maximization: Beyond 1/e

  In this work, we present a new algorithm for maximizing a non-monotonesubmodular function subject to a general constraint. Our algorithm finds anapproximate fractional solution for maximizing the multilinear extension of thefunction over a down-closed polytope. The approximation guarantee is 0.372 andit is the first improvement over the 1/e approximation achieved by the unifiedContinuous Greedy algorithm [Feldman et al., FOCS 2011].

A note on Cunningham's algorithm for matroid intersection

  In the matroid intersection problem, we are given two matroids of rank $r$ ona common ground set $E$ of $n$ elements and the goal is to find the maximum setthat is independent in both matroids. In this note, we show that Cunningham'salgorithm for matroid intersection can be implemented to use $O(nr\log^2(r))$independent oracle calls.

Approximate Nearest Neighbor Search in $\ell_p$

  We present a new locality sensitive hashing (LSH) algorithm for$c$-approximate nearest neighbor search in $\ell_p$ with $1<p<2$. For adatabase of $n$ points in $\ell_p$, we achieve $O(dn^{\rho})$ query time and$O(dn+n^{1+\rho})$ space, where $\rho \le O((\ln c)^2/c^p)$. This improves uponthe previous best upper bound $\rho\le 1/c$ by Datar et al. (SOCG 2004), and isclose to the lower bound $\rho \ge 1/c^p$ by O'Donnell, Wu and Zhou (ITCS2011). The proof is a simple generalization of the LSH scheme for $\ell_2$ byAndoni and Indyk (FOCS 2006).

Random Coordinate Descent Methods for Minimizing Decomposable Submodular  Functions

  Submodular function minimization is a fundamental optimization problem thatarises in several applications in machine learning and computer vision. Theproblem is known to be solvable in polynomial time, but general purposealgorithms have high running times and are unsuitable for large-scale problems.Recent work have used convex optimization techniques to obtain very practicalalgorithms for minimizing functions that are sums of ``simple" functions. Inthis paper, we use random coordinate descent methods to obtain algorithms withfaster linear convergence rates and cheaper iteration costs. Compared toalternating projection methods, our algorithms do not rely on full-dimensionalvector operations and they converge in significantly fewer iterations.

A Parallel Double Greedy Algorithm for Submodular Maximization

  We study parallel algorithms for the problem of maximizing a non-negativesubmodular function. Our main result is an algorithm that achieves anearly-optimal $1/2 -\epsilon$ approximation using $O(\log(1/\epsilon) /\epsilon)$ parallel rounds of function evaluations. Our algorithm is based on acontinuous variant of the double greedy algorithm of Buchbinder et al. thatachieves the optimal $1/2$ approximation in the sequential setting. Ouralgorithm applies more generally to the problem of maximizing a continuousdiminishing-returns (DR) function.

Sublinear Time Algorithms for Earth Mover's Distance

  We study the problem of estimating the Earth Mover's Distance (EMD) betweenprobability distributions when given access only to samples. We give closenesstesters and additive-error estimators over domains in $[0, \Delta]^d$, withsample complexities independent of domain size - permitting the testabilityeven of continuous distributions over infinite domains. Instead, our algorithmsdepend on other parameters, such as the diameter of the domain space, which maybe significantly smaller. We also prove lower bounds showing the dependencieson these parameters to be essentially optimal. Additionally, we considerwhether natural classes of distributions exist for which there are algorithmswith better dependence on the dimension, and show that for highly clusterabledata, this is indeed the case. Lastly, we consider a variant of the EMD,defined over tree metrics instead of the usual L1 metric, and give optimalalgorithms.

Decomposable Submodular Function Minimization: Discrete and Continuous

  This paper investigates connections between discrete and continuousapproaches for decomposable submodular function minimization. We provideimproved running time estimates for the state-of-the-art continuous algorithmsfor the problem using combinatorial arguments. We also provide a systematicexperimental comparison of the two types of methods, based on a cleardistinction between level-0 and level-1 algorithms.

Efficient Private Algorithms for Learning Halfspaces

  We present new differentially private algorithms for learning a large-marginhalfspace. In contrast to previous algorithms, which are based on eitherdifferentially private simulations of the statistical query model or on privateconvex optimization, the sample complexity of our algorithms depends only onthe margin of the data, and not on the dimension.

The Power of Randomization: Distributed Submodular Maximization on  Massive Datasets

  A wide variety of problems in machine learning, including exemplarclustering, document summarization, and sensor placement, can be cast asconstrained submodular maximization problems. Unfortunately, the resultingsubmodular optimization problems are often too large to be solved on a singlemachine. We develop a simple distributed algorithm that is embarrassinglyparallel and it achieves provable, constant factor, worst-case approximationguarantees. In our experiments, we demonstrate its efficiency in large problemswith different kinds of constraints with objective values always close to whatis achievable in the centralized setting.

A New Framework for Distributed Submodular Maximization

  A wide variety of problems in machine learning, including exemplarclustering, document summarization, and sensor placement, can be cast asconstrained submodular maximization problems. A lot of recent effort has beendevoted to developing distributed algorithms for these problems. However, theseresults suffer from high number of rounds, suboptimal approximation ratios, orboth. We develop a framework for bringing existing algorithms in the sequentialsetting to the distributed setting, achieving near optimal approximation ratiosfor many settings in only a constant number of MapReduce rounds. Our techniquesalso give a fast sequential algorithm for non-monotone maximization subject toa matroid constraint.

A Reduction for Optimizing Lattice Submodular Functions with Diminishing  Returns

  A function $f: \mathbb{Z}_+^E \rightarrow \mathbb{R}_+$ is DR-submodular ifit satisfies $f({\bf x} + \chi_i) -f ({\bf x}) \ge f({\bf y} + \chi_i) - f({\bfy})$ for all ${\bf x}\le {\bf y}, i\in E$. Recently, the problem of maximizinga DR-submodular function $f: \mathbb{Z}_+^E \rightarrow \mathbb{R}_+$ subjectto a budget constraint $\|{\bf x}\|_1 \leq B$ as well as additional constraintshas received significant attention \cite{SKIK14,SY15,MYK15,SY16}.  In this note, we give a generic reduction from the DR-submodular setting tothe submodular setting. The running time of the reduction and the size of theresulting submodular instance depends only \emph{logarithmically} on $B$. Usingthis reduction, one can translate the results for unconstrained and constrainedsubmodular maximization to the DR-submodular setting for many types ofconstraints in a unified manner.

Frequency Bin Entangled Photons

  A monochromatic laser pumping a parametric down conversion crystal generatesfrequency entangled photon pairs. We study this experimentally by addressingsuch frequency entangled photons at telecommunication wavelengths (around 1550nm) with fiber optics components such as electro-optic phase modulators andnarrow band frequency filters. The theory underlying our approach is developedby introducing the notion of frequency bin entanglement. Our results show thatthe phase modulators address coherently up to eleven frequency bins, leading toan interference pattern which can violate a Bell inequality adapted to oursetup by more than five standard deviations.

Lower bounds for oblivious subspace embeddings

  An oblivious subspace embedding (OSE) for some eps, delta in (0,1/3) and d <=m <= n is a distribution D over R^{m x n} such that for any linear subspace Wof R^n of dimension d,  Pr_{Pi ~ D}(for all x in W, (1-eps) |x|_2 <= |Pi x|_2 <= (1+eps)|x|_2) >= 1 -delta.  We prove that any OSE with delta < 1/3 must have m = Omega((d +log(1/delta))/eps^2), which is optimal. Furthermore, if every Pi in the supportof D is sparse, having at most s non-zero entries per column, then we showtradeoff lower bounds between m and s.

A Two Dimensional Backward Heat Problem With Statistical Discrete Data

  In this paper, we focus on the backward heat problem of finding the function$\theta(x,y)=u(x,y,0)$ such that \[ {l l l} u_t - a(t)(u_{xx} + u_{yy}) & =f(x,y,t), & \qquad (x,y,t) \in \Omega\times (0,T), u(x,y,T) & = h(x,y), &\qquad (x,y) \in\bar{\Omega}. \] where $\Omega = (0,\pi) \times (0,\pi)$ andthe heat transfer coefficient $a(t)$ is known. In our problem, the source $f =f(x,y,t)$ and the final data $h(x,y)$ are unknown. We only know random noisedata $g_{ij}(t)$ and $d_{ij}$ satisfying the regression models g_{ij}(t) &=&f(x_i,y_j,t) + \vartheta\xi_{ij}(t), d_{ij} &=& h(x_i,y_j) +\sigma_{ij}\epsilon_{ij}, where $\xi_{ij}(t)$ are Brownian motions,$\epsilon_{ij}\sim \mathcal{N}(0,1)$, $(x_i,y_j)$ are grid points of $\Omega$and $\sigma_{ij}, \vartheta$ are unknown positive constants. The noises$\xi_{ij}(t), \epsilon_{ij}$ are mutually independent. From the known data$g_{ij}(t)$ and $d_{ij}$, we can recovery the initial temperature$\theta(x,y)$. However, the result thus obtained is not stable and the problemis severely ill--posed. To regularize the instable solution, we use thetrigonometric method in nonparametric regression associated with the truncatedexpansion method. In addition, convergence rate is also investigatednumerically.

Online Bipartite Matching with Decomposable Weights

  We study a weighted online bipartite matching problem: $G(V_1, V_2, E)$ is aweighted bipartite graph where $V_1$ is known beforehand and the vertices of$V_2$ arrive online. The goal is to match vertices of $V_2$ as they arrive tovertices in $V_1$, so as to maximize the sum of weights of edges in thematching. If assignments to $V_1$ cannot be changed, no bounded competitiveratio is achievable. We study the weighted online matching problem with {\emfree disposal}, where vertices in $V_1$ can be assigned multiple times, butonly get credit for the maximum weight edge assigned to them over the course ofthe algorithm. For this problem, the greedy algorithm is $0.5$-competitive anddetermining whether a better competitive ratio is achievable is a well knownopen problem.  We identify an interesting special case where the edge weights aredecomposable as the product of two factors, one corresponding to each end pointof the edge. This is analogous to the well studied related machines model inthe scheduling literature, although the objective functions are different. Forthis case of decomposable edge weights, we design a 0.5664 competitiverandomized algorithm in complete bipartite graphs. We show that such instanceswith decomposable weights are non-trivial by establishing upper bounds of 0.618for deterministic and $0.8$ for randomized algorithms.  A tight competitive ratio of $1-1/e \approx 0.632$ was known previously forboth the 0-1 case as well as the case where edge weights depend on the offlinevertices only, but for these cases, reassignments cannot change the quality ofthe solution. Beating 0.5 for weighted matching where reassignments arenecessary has been a significant challenge. We thus give the first onlinealgorithm with competitive ratio strictly better than 0.5 for a non-trivialcase of weighted matching with free disposal.

Beyond Locality-Sensitive Hashing

  We present a new data structure for the c-approximate near neighbor problem(ANN) in the Euclidean space. For n points in R^d, our algorithm achievesO(n^{\rho} + d log n) query time and O(n^{1 + \rho} + d log n) space, where\rho <= 7/(8c^2) + O(1 / c^3) + o(1). This is the first improvement over theresult by Andoni and Indyk (FOCS 2006) and the first data structure thatbypasses a locality-sensitive hashing lower bound proved by O'Donnell, Wu andZhou (ICS 2011). By a standard reduction we obtain a data structure for theHamming space and \ell_1 norm with \rho <= 7/(8c) + O(1/c^{3/2}) + o(1), whichis the first improvement over the result of Indyk and Motwani (STOC 1998).

Tight Lower Bound for Linear Sketches of Moments

  The problem of estimating frequency moments of a data stream has attracted alot of attention since the onset of streaming algorithms [AMS99]. While thespace complexity for approximately computing the $p^{\rm th}$ moment, for$p\in(0,2]$ has been settled [KNW10], for $p>2$ the exact complexity remainsopen. For $p>2$ the current best algorithm uses $O(n^{1-2/p}\log n)$ words ofspace [AKO11,BO10], whereas the lower bound is of $\Omega(n^{1-2/p})$ [BJKS04].  In this paper, we show a tight lower bound of $\Omega(n^{1-2/p}\log n)$ wordsfor the class of algorithms based on linear sketches, which store only a sketch$Ax$ of input vector $x$ and some (possibly randomized) matrix $A$. We notethat all known algorithms for this problem are linear sketches.

Time lower bounds for nonadaptive turnstile streaming algorithms

  We say a turnstile streaming algorithm is "non-adaptive" if, during updates,the memory cells written and read depend only on the index being updated andrandom coins tossed at the beginning of the stream (and not on the memorycontents of the algorithm). Memory cells read during queries may be decidedupon adaptively. All known turnstile streaming algorithms in the literature arenon-adaptive.  We prove the first non-trivial update time lower bounds for both randomizedand deterministic turnstile streaming algorithms, which hold when thealgorithms are non-adaptive. While there has been abundant success in provingspace lower bounds, there have been no non-trivial update time lower bounds inthe turnstile model. Our lower bounds hold against classically studied problemssuch as heavy hitters, point query, entropy estimation, and moment estimation.In some cases of deterministic algorithms, our lower bounds nearly match knownupper bounds.

On the Convergence of the Hegselmann-Krause System

  We study convergence of the following discrete-time non-linear dynamicalsystem: n agents are located in R^d and at every time step, each movessynchronously to the average location of all agents within a unit distance ofit. This popularly studied system was introduced by Krause to model thedynamics of opinion formation and is often referred to as the Hegselmann-Krausemodel. We prove the first polynomial time bound for the convergence of thissystem in arbitrary dimensions. This improves on the bound of n^{O(n)}resulting from a more general theorem of Chazelle. Also, we show a quadraticlower bound and improve the upper bound for one-dimensional systems to O(n^3).

Submodular Maximization over Sliding Windows

  In this paper we study the extraction of representative elements in the datastream model in the form of submodular maximization. Different from theprevious work on streaming submodular maximization, we are interested only inthe recent data, and study the maximization problem over sliding windows. Weprovide a general reduction from the sliding window model to the standardstreaming model, and thus our approach works for general constraints as long asthere is a corresponding streaming algorithm in the standard streaming model.As a consequence, we obtain the first algorithms in the sliding window modelfor maximizing a monotone/non-monotone submodular function under cardinalityand matroid constraints. We also propose several heuristics and show theirefficiency in real-world datasets.

Approximate Near Neighbors for General Symmetric Norms

  We show that every symmetric normed space admits an efficient nearestneighbor search data structure with doubly-logarithmic approximation.Specifically, for every $n$, $d = n^{o(1)}$, and every $d$-dimensionalsymmetric norm $\|\cdot\|$, there exists a data structure for$\mathrm{poly}(\log \log n)$-approximate nearest neighbor search over$\|\cdot\|$ for $n$-point datasets achieving $n^{o(1)}$ query time and$n^{1+o(1)}$ space. The main technical ingredient of the algorithm is alow-distortion embedding of a symmetric norm into a low-dimensional iteratedproduct of top-$k$ norms.  We also show that our techniques cannot be extended to general norms.

A Nearly-linear Time Algorithm for Submodular Maximization with a  Knapsack Constraint

  We consider the problem of maximizing a monotone submodular function subjectto a knapsack constraint. Our main contribution is an algorithm that achieves anearly-optimal, $1 - 1/e - \epsilon$ approximation, using$(1/\epsilon)^{O(1/\epsilon^4)} n \log^2{n}$ function evaluations andarithmetic operations. Our algorithm is impractical but theoreticallyinteresting, since it overcomes a fundamental running time bottleneck of themultilinear extension relaxation framework. This is the main approach forobtaining nearly-optimal approximation guarantees for important classes ofconstraints but it leads to $\Omega(n^2)$ running times, since evaluating themultilinear extension is expensive. Our algorithm maintains a fractionalsolution with only a constant number of entries that are strictly fractional,which allows us to overcome this obstacle.

Submodular Maximization with Nearly-optimal Approximation and Adaptivity  in Nearly-linear Time

  In this paper, we study the tradeoff between the approximation guarantee andadaptivity for the problem of maximizing a monotone submodular function subjectto a cardinality constraint. The adaptivity of an algorithm is the number ofsequential rounds of queries it makes to the evaluation oracle of the function,where in every round the algorithm is allowed to make polynomially-manyparallel queries. Adaptivity is an important consideration in settings wherethe objective function is estimated using samples and in applications whereadaptivity is the main running time bottleneck. Previous algorithms achieving anearly-optimal $1 - 1/e - \epsilon$ approximation require $\Omega(n)$ rounds ofadaptivity. In this work, we give the first algorithm that achieves a $1 - 1/e- \epsilon$ approximation using $O(\ln{n} / \epsilon^2)$ rounds of adaptivity.The number of function evaluations and additional running time of the algorithmare $O(n \mathrm{poly}(\log{n}, 1/\epsilon))$.

Improved Algorithms for Collaborative PAC Learning

  We study a recent model of collaborative PAC learning where $k$ players with$k$ different tasks collaborate to learn a single classifier that works for alltasks. Previous work showed that when there is a classifier that has very smallerror on all tasks, there is a collaborative algorithm that finds a singleclassifier for all tasks and has $O((\ln (k))^2)$ times the worst-case samplecomplexity for learning a single task. In this work, we design new algorithmsfor both the realizable and the non-realizable setting, having samplecomplexity only $O(\ln (k))$ times the worst-case sample complexity forlearning a single task. The sample complexity upper bounds of our algorithmsmatch previous lower bounds and in some range of parameters are even betterthan previous algorithms that are allowed to output different classifiers fordifferent tasks.

Towards Nearly-linear Time Algorithms for Submodular Maximization with a  Matroid Constraint

  We consider fast algorithms for monotone submodular maximization subject to amatroid constraint. We assume that the matroid is given as input in an explicitform, and the goal is to obtain the best possible running times for importantmatroids. We develop a new algorithm for a \emph{general matroid constraint}with a $1 - 1/e - \epsilon$ approximation that achieves a fast running timeprovided we have a fast data structure for maintaining a maximum weight base inthe matroid through a sequence of decrease weight operations. We construct suchdata structures for graphic matroids and partition matroids, and we obtain the\emph{first algorithms} for these classes of matroids that achieve anearly-optimal, $1 - 1/e - \epsilon$ approximation, using a nearly-linearnumber of function evaluations and arithmetic operations.

BICEP3 focal plane design and detector performance

  BICEP3, the latest telescope in the BICEP/Keck program, started scienceobservations in March 2016. It is a 550mm aperture refractive telescopeobserving the polarization of the cosmic microwave background at 95 GHz. Weshow the focal plane design and detector performance, including spectralresponse, optical efficiency and preliminary sensitivity of the upgradedBICEP3. We demonstrate 9.72$\mu$K$\sqrt{\textrm{s}}$ noise performance of theBICEP3 receiver.

Sparsity Lower Bounds for Dimensionality Reducing Maps

  We give near-tight lower bounds for the sparsity required in severaldimensionality reducing linear maps. First, consider the JL lemma which statesthat for any set of n vectors in R there is a matrix A in R^{m x d} with m =O(eps^{-2}log n) such that mapping by A preserves pairwise Euclidean distancesof these n vectors up to a 1 +/- eps factor. We show that there exists a set ofn vectors such that any such matrix A with at most s non-zero entries percolumn must have s = Omega(eps^{-1}log n/log(1/eps)) as long as m <O(n/log(1/eps)). This bound improves the lower bound of Omega(min{eps^{-2},eps^{-1}sqrt{log_m d}}) by [Dasgupta-Kumar-Sarlos, STOC 2010], which only heldagainst the stronger property of distributional JL, and only against a certainrestricted class of distributions. Meanwhile our lower bound is against the JLlemma itself, with no restrictions. Our lower bound matches the sparseJohnson-Lindenstrauss upper bound of [Kane-Nelson, SODA 2012] up to anO(log(1/eps)) factor.  Next, we show that any m x n matrix with the k-restricted isometry property(RIP) with constant distortion must have at least Omega(klog(n/k)) non-zeroesper column if the number of the rows is the optimal value m = O(klog (n/k)),and if k < n/polylog n. This improves the previous lower bound of Omega(min{k,n/m}) by [Chandar, 2010] and shows that for virtually all k it is impossible tohave a sparse RIP matrix with an optimal number of rows.  Lastly, we show that any oblivious distribution over subspace embeddingmatrices with 1 non-zero per column and preserving all distances in a ddimensional-subspace up to a constant factor with constant probability musthave at least Omega(d^2) rows. This matches one of the upper bounds in[Nelson-Nguyen, 2012] and shows the impossibility of obtaining the best of bothof constructions in that work, namely 1 non-zero per column and \~O(d) rows.

OSNAP: Faster numerical linear algebra algorithms via sparser subspace  embeddings

  An "oblivious subspace embedding (OSE)" given some parameters eps,d is adistribution D over matrices B in R^{m x n} such that for any linear subspace Win R^n with dim(W) = d it holds that Pr_{B ~ D}(forall x in W ||B x||_2 in (1+/- eps)||x||_2) > 2/3 We show an OSE exists with m = O(d^2/eps^2) and whereevery B in the support of D has exactly s=1 non-zero entries per column. Thisimproves previously best known bound in [Clarkson-Woodruff, arXiv:1207.6365].Our quadratic dependence on d is optimal for any OSE with s=1 [Nelson-Nguyen,2012]. We also give two OSE's, which we call Oblivious SparseNorm-Approximating Projections (OSNAPs), that both allow the parameter settingsm = \~O(d/eps^2) and s = polylog(d)/eps, or m = O(d^{1+gamma}/eps^2) ands=O(1/eps) for any constant gamma>0. This m is nearly optimal since m >= d isrequired simply to no non-zero vector of W lands in the kernel of B. These arethe first constructions with m=o(d^2) to have s=o(d). In fact, our OSNAPs arenothing more than the sparse Johnson-Lindenstrauss matrices of [Kane-Nelson,SODA 2012]. Our analyses all yield OSE's that are sampled using eitherO(1)-wise or O(log d)-wise independent hash functions, which provides someefficiency advantages over previous work for turnstile streaming applications.Our main result is essentially a Bai-Yin type theorem in random matrix theoryand is likely to be of independent interest: i.e. we show that for any U inR^{n x d} with orthonormal columns and random sparse B, all singular values ofBU lie in [1-eps, 1+eps] with good probability.  Plugging OSNAPs into known algorithms for numerical linear algebra problemssuch as approximate least squares regression, low rank approximation, andapproximating leverage scores implies faster algorithms for all these problems.

Approximate k-flat Nearest Neighbor Search

  Let $k$ be a nonnegative integer. In the approximate $k$-flat nearestneighbor ($k$-ANN) problem, we are given a set $P \subset \mathbb{R}^d$ of $n$points in $d$-dimensional space and a fixed approximation factor $c > 1$. Ourgoal is to preprocess $P$ so that we can efficiently answer approximate$k$-flat nearest neighbor queries: given a $k$-flat $F$, find a point in $P$whose distance to $F$ is within a factor $c$ of the distance between $F$ andthe closest point in $P$. The case $k = 0$ corresponds to the well-studiedapproximate nearest neighbor problem, for which a plethora of results areknown, both in low and high dimensions. The case $k = 1$ is called approximateline nearest neighbor. In this case, we are aware of only one provablyefficient data structure, due to Andoni, Indyk, Krauthgamer, and Nguyen. For $k\geq 2$, we know of no previous results.  We present the first efficient data structure that can handle approximatenearest neighbor queries for arbitrary $k$. We use a data structure for$0$-ANN-queries as a black box, and the performance depends on the parametersof the $0$-ANN solution: suppose we have an $0$-ANN structure with query time$O(n^{\rho})$ and space requirement $O(n^{1+\sigma})$, for $\rho, \sigma > 0$.Then we can answer $k$-ANN queries in time $O(n^{k/(k + 1 - \rho) + t})$ andspace $O(n^{1+\sigma k/(k + 1 - \rho)} + n\log^{O(1/t)} n)$. Here, $t > 0$ isan arbitrary constant and the $O$-notation hides exponential factors in $k$,$1/t$, and $c$ and polynomials in $d$. Our new data structures also give animprovement in the space requirement over the previous result for $1$-ANN: wecan achieve near-linear space and sublinear query time, a further step towardspractical applications where space constitutes the bottleneck.

Cutting corners cheaply, or how to remove Steiner points

  Our main result is that the Steiner Point Removal (SPR) problem can always besolved with polylogarithmic distortion, which answers in the affirmative aquestion posed by Chan, Xia, Konjevod, and Richa (2006). Specifically, we provethat for every edge-weighted graph $G = (V,E,w)$ and a subset of terminals $T\subseteq V$, there is a graph $G'=(T,E',w')$ that is isomorphic to a minor of$G$, such that for every two terminals $u,v\in T$, the shortest-path distancesbetween them in $G$ and in $G'$ satisfy $d_{G,w}(u,v) \le d_{G',w'}(u,v) \leO(\log^5|T|) \cdot d_{G,w}(u,v)$. Our existence proof actually gives arandomized polynomial-time algorithm. Our proof features a new variant ofmetric decomposition. It is well-known that every $n$-point metric space$(X,d)$ admits a $\beta$-separating decomposition for $\beta=O(\log n)$, whichroughly means for every desired diameter bound $\Delta>0$ there is a randomizedpartitioning of $X$, which satisfies the following separation requirement: forevery $x,y \in X$, the probability they lie in different clusters of thepartition is at most $\beta\,d(x,y)/\Delta$. We introduce an additionalrequirement, which is the following tail bound: for every shortest-path $P$ oflength $d(P) \leq \Delta/\beta$, the number of clusters of the partition thatmeet the path $P$, denoted $Z_P$, satisfies $\Pr[Z_P > t] \le 2e^{-\Omega(t)}$for all $t>0$.

On Communication Cost of Distributed Statistical Estimation and  Dimensionality

  We explore the connection between dimensionality and communication cost indistributed learning problems. Specifically we study the problem of estimatingthe mean $\vec{\theta}$ of an unknown $d$ dimensional gaussian distribution inthe distributed setting. In this problem, the samples from the unknowndistribution are distributed among $m$ different machines. The goal is toestimate the mean $\vec{\theta}$ at the optimal minimax rate whilecommunicating as few bits as possible. We show that in this setting, thecommunication cost scales linearly in the number of dimensions i.e. one needsto deal with different dimensions individually. Applying this result toprevious lower bounds for one dimension in the interactive setting\cite{ZDJW13} and to our improved bounds for the simultaneous setting, we provenew lower bounds of $\Omega(md/\log(m))$ and $\Omega(md)$ for the bits ofcommunication needed to achieve the minimax squared loss, in the interactiveand simultaneous settings respectively. To complement, we also demonstrate aninteractive protocol achieving the minimax squared loss with $O(md)$ bits ofcommunication, which improves upon the simple simultaneous protocol by alogarithmic factor. Given the strong lower bounds in the general setting, weinitiate the study of the distributed parameter estimation problems withstructured parameters. Specifically, when the parameter is promised to be$s$-sparse, we show a simple thresholding based protocol that achieves the samesquared loss while saving a $d/s$ factor of communication. We conjecture thatthe tradeoff between communication and squared loss demonstrated by thisprotocol is essentially optimal up to logarithmic factor.

Communication Lower Bounds for Statistical Estimation Problems via a  Distributed Data Processing Inequality

  We study the tradeoff between the statistical error and communication cost ofdistributed statistical estimation problems in high dimensions. In thedistributed sparse Gaussian mean estimation problem, each of the $m$ machinesreceives $n$ data points from a $d$-dimensional Gaussian distribution withunknown mean $\theta$ which is promised to be $k$-sparse. The machinescommunicate by message passing and aim to estimate the mean $\theta$. Weprovide a tight (up to logarithmic factors) tradeoff between the estimationerror and the number of bits communicated between the machines. This directlyleads to a lower bound for the distributed \textit{sparse linear regression}problem: to achieve the statistical minimax error, the total communication isat least $\Omega(\min\{n,d\}m)$, where $n$ is the number of observations thateach machine receives and $d$ is the ambient dimension. These lower resultsimprove upon [Sha14,SD'14] by allowing multi-round iterative communicationmodel. We also give the first optimal simultaneous protocol in the dense casefor mean estimation.  As our main technique, we prove a \textit{distributed data processinginequality}, as a generalization of usual data processing inequalities, whichmight be of independent interest and useful for other problems.

Submodular Maximization with Matroid and Packing Constraints in Parallel

  We consider the problem of maximizing the multilinear extension of asubmodular function subject a single matroid constraint or multiple packingconstraints with a small number of adaptive rounds of evaluation queries.  We obtain the first algorithms with low adaptivity for submodularmaximization with a matroid constraint. Our algorithms achieve a$1-1/e-\epsilon$ approximation for monotone functions and a $1/e-\epsilon$approximation for non-monotone functions, which nearly matches the bestguarantees known in the fully adaptive setting. The number of rounds ofadaptivity is $O(\log^2{n}/\epsilon^3)$, which is an exponential speedup overthe existing algorithms.  We obtain the first parallel algorithm for non-monotone submodularmaximization subject to packing constraints. Our algorithm achieves a$1/e-\epsilon$ approximation using $O(\log(n/\epsilon) \log(1/\epsilon)\log(n+m)/ \epsilon^2)$ parallel rounds, which is again an exponential speedupin parallel time over the existing algorithms. For monotone functions, weobtain a $1-1/e-\epsilon$ approximation in$O(\log(n/\epsilon)\log(m)/\epsilon^2)$ parallel rounds. The number of parallelrounds of our algorithm matches that of the state of the art algorithm forsolving packing LPs with a linear objective.  Our results apply more generally to the problem of maximizing a diminishingreturns submodular (DR-submodular) function.

Antenna-coupled TES bolometers for the Keck Array, Spider, and Polar-1

  Between the BICEP2 and Keck Array experiments, we have deployed over 1500dual polarized antenna coupled bolometers to map the Cosmic MicrowaveBackground's polarization. We have been able to rapidly deploy these detectorsbecause they are completely planar with an integrated phased-array antenna.Through our experience in these experiments, we have learned of severalchallenges with this technology- specifically the beam synthesis in theantenna- and in this paper we report on how we have modified our designs tomitigate these challenges. In particular, we discus differential steeringerrors between the polarization pairs' beam centroids due to microstrip crosstalk and gradients of penetration depth in the niobium thin films of ourmillimeter wave circuits. We also discuss how we have suppressed side loberesponse with a Gaussian taper of our antenna illumination pattern. Theseimprovements will be used in Spider, Polar-1, and this season's retrofit ofKeck Array.

BICEP2 and Keck Array operational overview and status of observations

  The BICEP2 and Keck Array experiments are designed to measure thepolarization of the cosmic microwave background (CMB) on angular scales of 2-4degrees (l=50-100). This is the region in which the B-mode signal, a signatureprediction of cosmic inflation, is expected to peak. BICEP2 was deployed to theSouth Pole at the end of 2009 and is in the middle of its third year ofobserving with 500 polarization-sensitive detectors at 150 GHz. The Keck Arraywas deployed to the South Pole at the end of 2010, initially with threereceivers--each similar to BICEP2. An additional two receivers have been addedduring the 2011-12 summer. We give an overview of the two experiments, reporton substantial gains in the sensitivity of the two experiments afterpost-deployment optimization, and show preliminary maps of CMB polarizationfrom BICEP2.

BICEP Array: a multi-frequency degree-scale CMB polarimeter

  BICEP Array is the newest multi-frequency instrument in the BICEP/Keck Arrayprogram. It is comprised of four 550 mm aperture refractive telescopesobserving the polarization of the cosmic microwave background (CMB) at 30/40,95, 150 and 220/270 GHz with over 30,000 detectors. We present an overview ofthe receiver, detailing the optics, thermal, mechanical, and magnetic shieldingdesign. BICEP Array follows BICEP3's modular focal plane concept, and upgradesto 6" wafer to reduce fabrication with higher detector count per module. Thefirst receiver at 30/40 GHz is expected to start observing at the South Poleduring the 2019-20 season. By the end of the planned BICEP Array program, weproject $\sigma(r) \sim 0.003$, assuming current modeling of polarized Galacticforeground and depending on the level of delensing that can be achieved withhigher resolution maps from the South Pole Telescope.

Heavy hitters via cluster-preserving clustering

  In turnstile $\ell_p$ $\varepsilon$-heavy hitters, one maintains ahigh-dimensional $x\in\mathbb{R}^n$ subject to $\texttt{update}(i,\Delta)$causing $x_i\leftarrow x_i + \Delta$, where $i\in[n]$, $\Delta\in\mathbb{R}$.Upon receiving a query, the goal is to report a small list $L\subset[n]$, $|L|= O(1/\varepsilon^p)$, containing every "heavy hitter" $i\in[n]$ with $|x_i|\ge \varepsilon \|x_{\overline{1/\varepsilon^p}}\|_p$, where $x_{\overline{k}}$denotes the vector obtained by zeroing out the largest $k$ entries of $x$ inmagnitude.  For any $p\in(0,2]$ the CountSketch solves $\ell_p$ heavy hitters using$O(\varepsilon^{-p}\log n)$ words of space with $O(\log n)$ update time,$O(n\log n)$ query time to output $L$, and whose output after any query iscorrect with high probability (whp) $1 - 1/poly(n)$. Unfortunately the querytime is very slow. To remedy this, the work [CM05] proposed for $p=1$ in thestrict turnstile model, a whp correct algorithm achieving suboptimal space$O(\varepsilon^{-1}\log^2 n)$, worse update time $O(\log^2 n)$, but much betterquery time $O(\varepsilon^{-1}poly(\log n))$.  We show this tradeoff between space and update time versus query time isunnecessary. We provide a new algorithm, ExpanderSketch, which in the mostgeneral turnstile model achieves optimal $O(\varepsilon^{-p}\log n)$ space,$O(\log n)$ update time, and fast $O(\varepsilon^{-p}poly(\log n))$ query time,and whp correctness. Our main innovation is an efficient reduction from theheavy hitters to a clustering problem in which each heavy hitter is encoded assome form of noisy spectral cluster in a much bigger graph, and the goal is toidentify every cluster. Since every heavy hitter must be found, correctnessrequires that every cluster be found. We then develop a "cluster-preservingclustering" algorithm, partitioning the graph into clusters without destroyingany original cluster.

BICEP3: a 95 GHz refracting telescope for degree-scale CMB polarization

  BICEP3 is a 550 mm-aperture refracting telescope for polarimetry of radiationin the cosmic microwave background at 95 GHz. It adopts the methodology ofBICEP1, BICEP2 and the Keck Array experiments - it possesses sufficientresolution to search for signatures of the inflation-induced cosmicgravitational-wave background while utilizing a compact design for ease ofconstruction and to facilitate the characterization and mitigation ofsystematics. However, BICEP3 represents a significant breakthrough inper-receiver sensitivity, with a focal plane area 5$\times$ larger than aBICEP2/Keck Array receiver and faster optics ($f/1.6$ vs. $f/2.4$).Large-aperture infrared-reflective metal-mesh filters and infrared-absorptivecold alumina filters and lenses were developed and implemented for its optics.The camera consists of 1280 dual-polarization pixels; each is a pair oforthogonal antenna arrays coupled to transition-edge sensor bolometers and readout by multiplexed SQUIDs. Upon deployment at the South Pole during the 2014-15season, BICEP3 will have survey speed comparable to Keck Array 150 GHz (2013),and will significantly enhance spectral separation of primordial B-mode powerfrom that of possible galactic dust contamination in the BICEP2 observationpatch.

Initial Performance of BICEP3: A Degree Angular Scale 95 GHz Band  Polarimeter

  BICEP3 is a $550~mm$ aperture telescope with cold, on-axis, refractive opticsdesigned to observe at the $95~GHz$ band from the South Pole. It is the newestmember of the BICEP/Keck family of inflationary probes specifically designed tomeasure the polarization of the cosmic microwave background (CMB) atdegree-angular scales. BICEP3 is designed to house 1280 dual-polarizationpixels, which, when fully-populated, totals to $\sim$9$\times$ the number ofpixels in a single Keck $95~GHz$ receiver, thus further advancing theBICEP/Keck program's $95~GHz$ mapping speed. BICEP3 was deployed during theaustral summer of 2014-2015 with 9 detector tiles, to be increased to its fullcapacity of 20 in the second season. After instrument characterizationmeasurements were taken, CMB observation commenced in April 2015. Together withmulti-frequency observation data from Planck, BICEP2, and the Keck Array,BICEP3 is projected to set upper limits on the tensor-to-scalar ratio to $r$$\lesssim 0.03$ at $95\%$ C.L..

BICEP2 / Keck Array IX: New Bounds on Anisotropies of CMB Polarization  Rotation and Implications for Axion-Like Particles and Primordial Magnetic  Fields

  We present the strongest constraints to date on anisotropies of CMBpolarization rotation derived from $150$ GHz data taken by the BICEP2 & KeckArray CMB experiments up to and including the 2014 observing season (BK14). Thedefinition of polarization angle in BK14 maps has gone through self-calibrationin which the overall angle is adjusted to minimize the observed $TB$ and $EB$power spectra. After this procedure, the $QU$ maps lose sensitivity to auniform polarization rotation but are still sensitive to anisotropies ofpolarization rotation. This analysis places constraints on the anisotropies ofpolarization rotation, which could be generated by CMB photons interacting withaxion-like pseudoscalar fields or Faraday rotation induced by primordialmagnetic fields. The sensitivity of BK14 maps ($\sim 3\mu$K-arcmin) makes itpossible to reconstruct anisotropies of polarization rotation angle and measuretheir angular power spectrum much more precisely than previous attempts. Ourdata are found to be consistent with no polarization rotation anisotropies,improving the upper bound on the amplitude of the rotation angle spectrum byroughly an order of magnitude compared to the previous best constraints. Ourresults lead to an order of magnitude better constraint on the couplingconstant of the Chern-Simons electromagnetic term $f_a \geq 1.7\times10^2\times (H_I/2\pi)$ ($2\sigma$) than the constraint derived from uniformrotation, where $H_I$ is the inflationary Hubble scale. The upper bound on theamplitude of the primordial magnetic fields is 30nG ($2\sigma$) from thepolarization rotation anisotropies.

Measurements of Degree-Scale B-mode Polarization with the BICEP/Keck  Experiments at South Pole

  The BICEP and Keck Array experiments are a suite of small-aperture refractingtelescopes observing the microwave sky from the South Pole. They target thedegree-scale B-mode polarization signal imprinted in the Cosmic MicrowaveBackground (CMB) by primordial gravitational waves. Such a measurement wouldshed light on the physics of the very early universe. While BICEP2 observed forthe first time a B-mode signal at 150 GHz, higher frequencies from the Plancksatellite showed that it could be entirely due to the polarized emission fromGalactic dust, though uncertainty remained high. Keck Array has been observingthe same region of the sky for several years, with an increased detector count,producing the deepest polarized CMB maps to date. New detectors at 95 GHz wereinstalled in 2014, and at 220 GHz in 2015. These observations enable a betterconstraint of galactic foreground emissions, as presented here. In 2015, BICEP2was replaced by BICEP3, a 10 times higher throughput telescope observing at 95GHz, while Keck Array is now focusing on higher frequencies. In the nearfuture, BICEP Array will replace Keck Array, and will allow unprecedentedsensitivity to the gravitational wave signal. High resolution observations fromthe South Pole Telescope (SPT) will also be used to remove the lensingcontribution to B-modes.

2017 upgrade and performance of BICEP3: a 95GHz refracting telescope for  degree-scale CMB polarization

  BICEP3 is a 520mm aperture on-axis refracting telescope observing thepolarization of the cosmic microwave background (CMB) at 95GHz in search of theB-mode signal originating from inflationary gravitational waves. BICEP3's focalplane is populated with modularized tiles of antenna-coupled transition edgesensor (TES) bolometers. BICEP3 was deployed to the South Pole during 2014-15austral summer and has been operational since. During the 2016-17 australsummer, we implemented changes to optical elements that lead to better noiseperformance. We discuss this upgrade and show the performance of BICEP3 at itsfull mapping speed from the 2017 and 2018 observing seasons. BICEP3 achieves anorder-of-magnitude improvement in mapping speed compared to a Keck 95GHzreceiver. We demonstrate $6.6\mu K\sqrt{s}$ noise performance of the BICEP3receiver.

BICEP Array cryostat and mount design

  Bicep Array is a cosmic microwave background (CMB) polarization experimentthat will begin observing at the South Pole in early 2019. This experimentreplaces the five Bicep2 style receivers that compose the Keck Array with fourlarger Bicep3 style receivers observing at six frequencies from 30 to 270GHz.The 95GHz and 150GHz receivers will continue to push the already deepBicep/Keck CMB maps while the 30/40GHz and 220/270GHz receivers will constrainthe synchrotron and galactic dust foregrounds respectively. Here we report onthe design and performance of the Bicep Array instruments focusing on the mountand cryostat systems.

Ultra-Thin Large-Aperture Vacuum Windows for Millimeter Wavelengths  Receivers

  Targeting faint polarization patterns arising from Primordial GravitationalWaves in the Cosmic Microwave Background requires excellent observationalsensitivity. Optical elements in small aperture experiments such as Bicep3 andKeck Array are designed to optimize throughput and minimize losses fromtransmission, reflection and scattering at millimeter wavelengths. As aperturesize increases, cryostat vacuum windows must withstand larger forces fromatmospheric pressure and the solution has often led to a thicker window at theexpense of larger transmission loss. We have identified a new candidatematerial for the fabrication of vacuum windows: with a tensile strength twoorders of magnitude larger than previously used materials, woven high-moduluspolyethylene could allow for dramatically thinner windows, and thereforesignificantly reduced losses and higher sensitivity. In these proceedings weinvestigate the suitability of high-modulus polyethylene windows forground-based CMB experiments, such as current and future receivers in theBicep/Keck Array program. This includes characterizing their opticaltransmission as well as their mechanical behavior under atmospheric pressure.We find that such ultra-thin materials are promising candidates to improve theperformance of large-aperture instruments at millimeter wavelengths, andoutline a plan for further tests ahead of a possible upcoming field deploymentof such a science-grade window.

Design and performance of wide-band corrugated walls for the BICEP Array  detector modules at 30/40 GHz

  BICEP Array is a degree-scale Cosmic Microwave Background (CMB) experimentthat will search for primordial B-mode polarization while constraining Galacticforegrounds. BICEP Array will be comprised of four receivers to cover a broadfrequency range with channels at 30/40, 95, 150 and 220/270 GHz. The firstlow-frequency receiver will map synchrotron emission at 30 and 40 GHz and willdeploy to the South Pole at the end of 2019. In this paper, we give an overviewof the BICEP Array science and instrument, with a focus on the detector module.We designed corrugations in the metal frame of the module to suppress unwantedinteractions with the antenna-coupled detectors that would otherwise deform thebeams of edge pixels. This design reduces the residual beam systematics andtemperature-to-polarization leakage due to beam steering and shape mismatchbetween polarized beam pairs. We report on the simulated performance of single-and wide-band corrugations designed to minimize these effects. Our optimizeddesign alleviates beam differential ellipticity caused by the metal frame toabout 7% over 57% bandwidth (25 to 45 GHz), which is close to the level due thebare antenna itself without a metal frame. Initial laboratory measurements arealso presented.

BICEP2 / Keck Array VII: Matrix based E/B Separation applied to BICEP2  and the Keck Array

  A linear polarization field on the sphere can be uniquely decomposed into anE-mode and a B-mode component. These two components are analytically defined interms of spin-2 spherical harmonics. Maps that contain filtered modes on apartial sky can also be decomposed into E-mode and B-mode components. However,the lack of full sky information prevents orthogonally separating thesecomponents using spherical harmonics. In this paper, we present a technique fordecomposing an incomplete map into E and B-mode components using E and Beigenmodes of the pixel covariance in the observed map. This method is found toorthogonally define E and B in the presence of both partial sky coverage andspatial filtering. This method has been applied to the BICEP2 and the KeckArray maps and results in reducing E to B leakage from LCDM E-modes to a levelcorresponding to a tensor-to-scalar ratio of $r<1\times10^{-4}$.

BICEP2 / Keck Array VIII: Measurement of gravitational lensing from  large-scale B-mode polarization

  We present measurements of polarization lensing using the 150 GHz maps whichinclude all data taken by the BICEP2 & Keck Array CMB polarization experimentsup to and including the 2014 observing season (BK14). Despite their modestangular resolution ($\sim 0.5^\circ$), the excellent sensitivity ($\sim3\mu$K-arcmin) of these maps makes it possible to directly reconstruct thelensing potential using only information at larger angular scales ($\ell\leq700$). From the auto-spectrum of the reconstructed potential we measure anamplitude of the spectrum to be $A^{\phi\phi}_{\rm L}=1.15\pm 0.36$ (Planck$\Lambda$CDM prediction corresponds to $A^{\phi\phi}_{\rm L}=1$), and rejectthe no-lensing hypothesis at 5.8$\sigma$, which is the highest significanceachieved to date using an EB lensing estimator. Taking the cross-spectrum ofthe reconstructed potential with the Planck 2015 lensing map yields$A^{\phi\phi}_{\rm L}=1.13\pm 0.20$. These direct measurements of$A^{\phi\phi}_{\rm L}$ are consistent with the $\Lambda$CDM cosmology, and withthat derived from the previously reported BK14 B-mode auto-spectrum ($A^{\rmBB}_{\rm L}=1.20\pm 0.17$). We perform a series of null tests and consistencychecks to show that these results are robust against systematics and areinsensitive to analysis choices. These results unambiguously demonstrate thatthe B-modes previously reported by BICEP / Keck at intermediate angular scales($150\lesssim\ell\lesssim 350$) are dominated by gravitational lensing. Thegood agreement between the lensing amplitudes obtained from the lensingreconstruction and B-mode spectrum starts to place constraints on anyalternative cosmological sources of B-modes at these angular scales.

Antenna-coupled TES bolometers used in BICEP2, Keck array, and SPIDER

  We have developed antenna-coupled transition-edge sensor (TES) bolometers fora wide range of cosmic microwave background (CMB) polarimetry experiments,including BICEP2, Keck Array, and the balloon borne SPIDER. These detectorshave reached maturity and this paper reports on their design principles,overall performance, and key challenges associated with design and production.Our detector arrays repeatedly produce spectral bands with 20%-30% bandwidth at95, 150, or 220~GHz. The integrated antenna arrays synthesize symmetricco-aligned beams with controlled side-lobe levels. Cross-polarized response onboresight is typically ~0.5%, consistent with cross-talk in our multiplexedreadout system. End-to-end optical efficiencies in our cameras are routinely35% or higher, with per detector sensitivities of NET~300 uKrts. Thanks to thescalability of this design, we have deployed 2560 detectors as 1280 matchedpairs in Keck Array with a combined instantaneous sensitivity of ~9 uKrts, asmeasured directly from CMB maps in the 2013 season. Similar arrays haverecently flown in the SPIDER instrument, and development of this technology isongoing.

BICEP2 / Keck Array V: Measurements of B-mode Polarization at Degree  Angular Scales and 150 GHz by the Keck Array

  The Keck Array is a system of cosmic microwave background (CMB) polarimeters,each similar to the BICEP2 experiment. In this paper we report results from the2012 and 2013 observing seasons, during which the Keck Array consisted of fivereceivers all operating in the same (150 GHz) frequency band and observingfield as BICEP2. We again find an excess of B-mode power over thelensed-$\Lambda$CDM expectation of $> 5 \sigma$ in the range $30 < \ell < 150$and confirm that this is not due to systematics using jackknife tests andsimulations based on detailed calibration measurements. In map difference andspectral difference tests these new data are shown to be consistent withBICEP2. Finally, we combine the maps from the two experiments to produce finalQ and U maps which have a depth of 57 nK deg (3.4 $\mu$K arcmin) over aneffective area of 400 deg$^2$ for an equivalent survey weight of 250,000$\mu$K$^{-2}$. The final BB band powers have noise uncertainty a factor of 2.3times better than the previous results, and a significance of detection ofexcess power of $> 6\sigma$.

Optical Characterization of the BICEP3 CMB Polarimeter at the South Pole

  BICEP3 is a small-aperture refracting cosmic microwave background (CMB)telescope designed to make sensitive polarization maps in pursuit of apotential B-mode signal from inflationary gravitational waves. It is the latestin the BICEP/Keck Array series of CMB experiments at the South Pole, which hasprovided the most stringent constraints on inflation to date. For the 2016observing season, BICEP3 was outfitted with a full suite of 2400 opticallycoupled detectors operating at 95 GHz. In these proceedings we report on thefar field beam performance using calibration data taken during the 2015-2016summer deployment season in situ with a thermal chopped source. We generatehigh-fidelity per-detector beam maps, show the array-averaged beam profile, andcharacterize the differential beam response between co-located, orthogonallypolarized detectors which contributes to the leading instrumental systematic inpair differencing experiments. We find that the levels of differentialpointing, beamwidth, and ellipticity are similar to or lower than thosemeasured for BICEP2 and Keck Array. The magnitude and distribution of BICEP3'sdifferential beam mismatch - and the level to which temperature-to-polarizationleakage may be marginalized over or subtracted in analysis - will inform thedesign of next-generation CMB experiments with many thousands of detectors.

