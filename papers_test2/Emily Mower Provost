Progressive Neural Networks for Transfer Learning in Emotion Recognition

  Many paralinguistic tasks are closely related and thus representationslearned in one domain can be leveraged for another. In this paper, weinvestigate how knowledge can be transferred between three paralinguistictasks: speaker, emotion, and gender recognition. Further, we extend thisproblem to cross-dataset tasks, asking how knowledge captured in one emotiondataset can be transferred to another. We focus on progressive neural networksand compare these networks to the conventional deep learning method ofpre-training and fine-tuning. Progressive neural networks provide a way totransfer knowledge and avoid the forgetting effect present when pre-trainingneural networks on different tasks. Our experiments demonstrate that: (1)emotion recognition can benefit from using representations originally learnedfor different paralinguistic tasks and (2) transfer learning can effectivelyleverage additional datasets to improve the performance of emotion recognitionsystems.

Capturing Long-term Temporal Dependencies with Convolutional Networks  for Continuous Emotion Recognition

  The goal of continuous emotion recognition is to assign an emotion value toevery frame in a sequence of acoustic features. We show that incorporatinglong-term temporal dependencies is critical for continuous emotion recognitiontasks. To this end, we first investigate architectures that use dilatedconvolutions. We show that even though such architectures outperform previouslyreported systems, the output signals produced from such architectures undergoerratic changes between consecutive time steps. This is inconsistent with theslow moving ground-truth emotion labels that are obtained from humanannotators. To deal with this problem, we model a downsampled version of theinput signal and then generate the output signal through upsampling. Not onlydoes the resulting downsampling/upsampling network achieve good performance, italso generates smooth output trajectories. Our method yields the best knownaudio-only performance on the RECOLA dataset.

Improving End-of-turn Detection in Spoken Dialogues by Detecting Speaker  Intentions as a Secondary Task

  This work focuses on the use of acoustic cues for modeling turn-taking indyadic spoken dialogues. Previous work has shown that speaker intentions (e.g.,asking a question, uttering a backchannel, etc.) can influence turn-takingbehavior and are good predictors of turn-transitions in spoken dialogues.However, speaker intentions are not readily available for use by automatedsystems at run-time; making it difficult to use this information to anticipatea turn-transition. To this end, we propose a multi-task neural approach forpredicting turn- transitions and speaker intentions simultaneously. Our resultsshow that adding the auxiliary task of speaker intention prediction improvesthe performance of turn-transition prediction in spoken dialogues, withoutrelying on additional input features during run-time.

The PRIORI Emotion Dataset: Linking Mood to Emotion Detected In-the-Wild

  Bipolar Disorder is a chronic psychiatric illness characterized bypathological mood swings associated with severe disruptions in emotionregulation. Clinical monitoring of mood is key to the care of these dynamic andincapacitating mood states. Frequent and detailed monitoring improves clinicalsensitivity to detect mood state changes, but typically requires costly andlimited resources. Speech characteristics change during both depressed andmanic states, suggesting automatic methods applied to the speech signal can beeffectively used to monitor mood state changes. However, speech is modulated bymany factors, which renders mood state prediction challenging. We hypothesizethat emotion can be used as an intermediary step to improve mood stateprediction. This paper presents critical steps in developing this pipeline,including (1) a new in the wild emotion dataset, the PRIORI Emotion Dataset,collected from everyday smartphone conversational speech recordings, (2)activation/valence emotion recognition baselines on this dataset (PCC of 0.71and 0.41, respectively), and (3) significant correlation between predictedemotion and mood state for individuals with bipolar disorder. This providesevidence and a working baseline for the use of emotion as a meta-feature formood state monitoring.

Trainable Time Warping: Aligning Time-Series in the Continuous-Time  Domain

  DTW calculates the similarity or alignment between two signals, subject totemporal warping. However, its computational complexity grows exponentiallywith the number of time-series. Although there have been algorithms developedthat are linear in the number of time-series, they are generally quadratic intime-series length. The exception is generalized time warping (GTW), which haslinear computational cost. Yet, it can only identify simple time warpingfunctions. There is a need for a new fast, high-quality multisequence alignmentalgorithm. We introduce trainable time warping (TTW), whose complexity islinear in both the number and the length of time-series. TTW performs alignmentin the continuous-time domain using a sinc convolutional kernel and agradient-based optimization technique. We compare TTW and GTW on 85 UCRdatasets in time-series averaging and classification. TTW outperforms GTW on67.1% of the datasets for the averaging tasks, and 61.2% of the datasets forthe classification tasks.

MuSE-ing on the Impact of Utterance Ordering On Crowdsourced Emotion  Annotations

  Emotion recognition algorithms rely on data annotated with high qualitylabels. However, emotion expression and perception are inherently subjective.There is generally not a single annotation that can be unambiguously declared"correct". As a result, annotations are colored by the manner in which theywere collected. In this paper, we conduct crowdsourcing experiments toinvestigate this impact on both the annotations themselves and on theperformance of these algorithms. We focus on one critical question: the effectof context. We present a new emotion dataset, Multimodal Stressed Emotion(MuSE), and annotate the dataset using two conditions: randomized, in whichannotators are presented with clips in random order, and contextualized, inwhich annotators are presented with clips in order. We find that contextuallabeling schemes result in annotations that are more similar to a speaker's ownself-reported labels and that labels generated from randomized schemes are mosteasily predictable by automated systems.

Barking up the Right Tree: Improving Cross-Corpus Speech Emotion  Recognition with Adversarial Discriminative Domain Generalization (ADDoG)

  Automatic speech emotion recognition provides computers with critical contextto enable user understanding. While methods trained and tested within the samedataset have been shown successful, they often fail when applied to unseendatasets. To address this, recent work has focused on adversarial methods tofind more generalized representations of emotional speech. However, many ofthese methods have issues converging, and only involve datasets collected inlaboratory conditions. In this paper, we introduce Adversarial DiscriminativeDomain Generalization (ADDoG), which follows an easier to train "meet in themiddle" approach. The model iteratively moves representations learned for eachdataset closer to one another, improving cross-dataset generalization. We alsointroduce Multiclass ADDoG, or MADDoG, which is able to extend the proposedmethod to more than two datasets, simultaneously. Our results show consistentconvergence for the introduced methods, with significantly improved resultswhen not using labels from the target dataset. We also show how, in most cases,ADDoG and MADDoG can be used to improve upon baseline state-of-the-art methodswhen target dataset labels are added and in-the-wild data are considered. Eventhough our experiments focus on cross-corpus speech emotion, these methodscould be used to remove unwanted factors of variation in other settings.

