A Maximum Likelihood Approach For Selecting Sets of Alternatives

  We consider the problem of selecting a subset of alternatives given noisyevaluations of the relative strength of different alternatives. We wish toselect a k-subset (for a given k) that provides a maximum likelihood estimatefor one of several objectives, e.g., containing the strongest alternative.Although this problem is NP-hard, we show that when the noise level issufficiently high, intuitive methods provide the optimal solution. We thusgeneralize classical results about singling out one alternative and identifyingthe hidden ranking of alternatives by strength. Extensive experiments show thatour methods perform well in practical settings.

A Voting-Based System for Ethical Decision Making

  We present a general approach to automating ethical decisions, drawing onmachine learning and computational social choice. In a nutshell, we propose tolearn a model of societal preferences, and, when faced with a specific ethicaldilemma at runtime, efficiently aggregate those preferences to identify adesirable choice. We provide a concrete algorithm that instantiates ourapproach; some of its crucial steps are informed by a new theory ofswap-dominance efficient voting rules. Finally, we implement and evaluate asystem for ethical decision making in the autonomous vehicle domain, usingpreference data collected from 1.3 million people through the Moral Machinewebsite.

Verifiably Truthful Mechanisms

  It is typically expected that if a mechanism is truthful, then the agentswould, indeed, truthfully report their private information. But why would anagent believe that the mechanism is truthful? We wish to design truthfulmechanisms, whose truthfulness can be verified efficiently (in thecomputational sense). Our approach involves three steps: (i) specifying thestructure of mechanisms, (ii) constructing a verification algorithm, and (iii)measuring the quality of verifiably truthful mechanisms. We demonstrate thisapproach using a case study: approximate mechanism design without money forfacility location.

Influence in Classification via Cooperative Game Theory

  A dataset has been classified by some unknown classifier into two types ofpoints. What were the most important factors in determining the classificationoutcome? In this work, we employ an axiomatic approach in order to uniquelycharacterize an influence measure: a function that, given a set of classifiedpoints, outputs a value for each feature corresponding to its influence indetermining the classification outcome. We show that our influence measuretakes on an intuitive form when the unknown classifier is linear. Finally, weemploy our influence measure in order to analyze the effects of user profilingon Google's online display advertising.

Learning Cooperative Games

  This paper explores a PAC (probably approximately correct) learning model incooperative games. Specifically, we are given $m$ random samples of coalitionsand their values, taken from some unknown cooperative game; can we predict thevalues of unseen coalitions? We study the PAC learnability of severalwell-known classes of cooperative games, such as network flow games, thresholdtask games, and induced subgraph games. We also establish a novel connectionbetween PAC learnability and core stability: for games that are efficientlylearnable, it is possible to find payoff divisions that are likely to be stableusing a polynomial number of samples.

The Provable Virtue of Laziness in Motion Planning

  The Lazy Shortest Path (LazySP) class consists of motion-planning algorithmsthat only evaluate edges along shortest paths between the source and target.These algorithms were designed to minimize the number of edge evaluations insettings where edge evaluation dominates the running time of the algorithm; buthow close to optimal are LazySP algorithms in terms of this objective? Our mainresult is an analytical upper bound, in a probabilistic model, on the number ofedge evaluations required by LazySP algorithms; a matching lower bound showsthat these algorithms are asymptotically optimal in the worst case.

A partisan districting protocol with provably nonpartisan outcomes

  We design and analyze a protocol for dividing a state into districts, whereparties take turns proposing a division, and freezing a district from the otherparty's proposed division. We show that our protocol has predictable andprovable guarantees for both the number of districts in which each party has amajority of supporters, and the extent to which either party has the power topack a specific population into a single district.

Opting Into Optimal Matchings

  We revisit the problem of designing optimal, individually rational matchingmechanisms (in a general sense, allowing for cycles in directed graphs), whereeach player --- who is associated with a subset of vertices --- matches as manyof his own vertices when he opts into the matching mechanism as when he optsout. We offer a new perspective on this problem by considering an arbitrarygraph, but assuming that vertices are associated with players at random. Ourmain result asserts that, under certain conditions, any fixed optimal matchingis likely to be individually rational up to lower-order terms. We also showthat a simple and practical mechanism is (fully) individually rational, andlikely to be optimal up to lower-order terms. We discuss the implications ofour results for market design in general, and kidney exchange in particular.

Strategyproof Approximation Mechanisms for Location on Networks

  We consider the problem of locating a facility on a network, represented by agraph. A set of strategic agents have different ideal locations for thefacility; the cost of an agent is the distance between its ideal location andthe facility. A mechanism maps the locations reported by the agents to thelocation of the facility. Specifically, we are interested in social choicemechanisms that do not utilize payments. We wish to design mechanisms that arestrategyproof, in the sense that agents can never benefit by lying, or, evenbetter, group strategyproof, in the sense that a coalition of agents cannot allbenefit by lying. At the same time, our mechanisms must provide a smallapproximation ratio with respect to one of two optimization targets: the socialcost or the maximum cost.  We give an almost complete characterization of the feasible truthfulapproximation ratio under both target functions, deterministic and randomizedmechanisms, and with respect to different network topologies. Our main resultsare: We show that a simple randomized mechanism is group strategyproof andgives a (2-2/n)-approximation for the social cost, where n is the number ofagents, when the network is a circle (known as a ring in the case of computernetworks); we design a novel "hybrid" strategyproof randomized mechanism thatprovides a tight approximation ratio of 3/2 for the maximum cost when thenetwork is a circle; and we show that no randomized SP mechanism can provide anapproximation ratio better than 2-o(1) to the maximum cost even when thenetwork is a tree, thereby matching a trivial upper bound of two.

Sum of Us: Strategyproof Selection from the Selectors

  We consider directed graphs over a set of n agents, where an edge (i,j) istaken to mean that agent i supports or trusts agent j. Given such a graph andan integer k\leq n, we wish to select a subset of k agents that maximizes thesum of indegrees, i.e., a subset of k most popular or most trusted agents. Atthe same time we assume that each individual agent is only interested in beingselected, and may misreport its outgoing edges to this end. This problemformulation captures realistic scenarios where agents choose among themselves,which can be found in the context of Internet search, social networks likeTwitter, or reputation systems like Epinions.  Our goal is to design mechanisms without payments that map each graph to ak-subset of agents to be selected and satisfy the following two constraints:strategyproofness, i.e., agents cannot benefit from misreporting their outgoingedges, and approximate optimality, i.e., the sum of indegrees of the selectedsubset of agents is always close to optimal. Our first main result is asurprising impossibility: for k \in {1,...,n-1}, no deterministic strategyproofmechanism can provide a finite approximation ratio. Our second main result is arandomized strategyproof mechanism with an approximation ratio that is boundedfrom above by four for any value of k, and approaches one as k grows.

A Smooth Transition from Powerlessness to Absolute Power

  We study the phase transition of the coalitional manipulation problem forgeneralized scoring rules. Previously it has been shown that, under someconditions on the distribution of votes, if the number of manipulators is$o(\sqrt{n})$, where $n$ is the number of voters, then the probability that arandom profile is manipulable by the coalition goes to zero as the number ofvoters goes to infinity, whereas if the number of manipulators is$\omega(\sqrt{n})$, then the probability that a random profile is manipulablegoes to one. Here we consider the critical window, where a coalition has size$c\sqrt{n}$, and we show that as $c$ goes from zero to infinity, the limitingprobability that a random profile is manipulable goes from zero to one in asmooth fashion, i.e., there is a smooth phase transition between the tworegimes. This result analytically validates recent empirical results, andsuggests that deciding the coalitional manipulation problem may be of limitedcomputational hardness in practice.

Bayesian Vote Manipulation: Optimal Strategies and Impact on Welfare

  Most analyses of manipulation of voting schemes have adopted two assumptionsthat greatly diminish their practical import. First, it is usually assumed thatthe manipulators have full knowledge of the votes of the nonmanipulatingagents. Second, analysis tends to focus on the probability of manipulationrather than its impact on the social choice objective (e.g., social welfare).We relax both of these assumptions by analyzing optimal Bayesian manipulationstrategies when the manipulators have only partial probabilistic informationabout nonmanipulator votes, and assessing the expected loss in social welfare(in the broad sense of the term). We present a general optimization frameworkfor the derivation of optimal manipulation strategies given arbitrary votingrules and distributions over preferences. We theoretically and empiricallyanalyze the optimal manipulability of some popular voting rules usingdistributions and real data sets that go well beyond the common, butunrealistic, impartial culture assumption. We also shed light on the starkdifference between the loss in social welfare and the probability ofmanipulation by showing that even when manipulation is likely, impact to socialwelfare is slight (and often negligible).

Audit Games

  Effective enforcement of laws and policies requires expending resources toprevent and detect offenders, as well as appropriate punishment schemes todeter violators. In particular, enforcement of privacy laws and policies inmodern organizations that hold large volumes of personal information (e.g.,hospitals, banks, and Web services providers) relies heavily on internal auditmechanisms. We study economic considerations in the design of these mechanisms,focusing in particular on effective resource allocation and appropriatepunishment schemes. We present an audit game model that is a naturalgeneralization of a standard security game model for resource allocation withan additional punishment parameter. Computing the Stackelberg equilibrium forthis game is challenging because it involves solving an optimization problemwith non-convex quadratic constraints. We present an additive FPTAS thatefficiently computes a solution that is arbitrarily close to the optimalsolution.

An Algorithmic Framework for Strategic Fair Division

  We study the paradigmatic fair division problem of allocating a divisiblegood among agents with heterogeneous preferences, commonly known as cakecutting. Classical cake cutting protocols are susceptible to manipulation. Dotheir strategic outcomes still guarantee fairness?  To address this question we adopt a novel algorithmic approach, by designinga concrete computational framework for fair division---the class of GeneralizedCut and Choose (GCC) protocols}---and reasoning about the game-theoreticproperties of algorithms that operate in this model. The class of GCC protocolsincludes the most important discrete cake cutting protocols, and turns out tobe compatible with the study of fair division among strategic agents. Inparticular, GCC protocols are guaranteed to have approximate subgame perfectNash equilibria, or even exact equilibria if the protocol's tie-breaking ruleis flexible. We further observe that the (approximate) equilibria ofproportional GCC protocols---which guarantee each of the $n$ agents a$1/n$-fraction of the cake---must be (approximately) proportional. Finally, wedesign a protocol in this framework with the property that its Nash equilibriumallocations coincide with the set of (contiguous) envy-free allocations.

Small Representations of Big Kidney Exchange Graphs

  Kidney exchanges are organized markets where patients swap willing butincompatible donors. In the last decade, kidney exchanges grew from small andregional to large and national---and soon, international. This growth resultsin more lives saved, but exacerbates the empirical hardness of the$\mathcal{NP}$-complete problem of optimally matching patients to donors.State-of-the-art matching engines use integer programming techniques to clearfielded kidney exchanges, but these methods must be tailored to specific modelsand objective functions, and may fail to scale to larger exchanges. In thispaper, we observe that if the kidney exchange compatibility graph can beencoded by a constant number of patient and donor attributes, the clearingproblem is solvable in polynomial time. We give necessary and sufficientconditions for losslessly shrinking the representation of an arbitrarycompatibility graph. Then, using real compatibility graphs from the UNOSnationwide kidney exchange, we show how many attributes are needed to encodereal compatibility graphs. The experiments show that, indeed, small numbers ofattributes suffice.

Game-Theoretic Modeling of Human Adaptation in Human-Robot Collaboration

  In human-robot teams, humans often start with an inaccurate model of therobot capabilities. As they interact with the robot, they infer the robot'scapabilities and partially adapt to the robot, i.e., they might change theiractions based on the observed outcomes and the robot's actions, withoutreplicating the robot's policy. We present a game-theoretic model of humanpartial adaptation to the robot, where the human responds to the robot'sactions by maximizing a reward function that changes stochastically over time,capturing the evolution of their expectations of the robot's capabilities. Therobot can then use this model to decide optimally between taking actions thatreveal its capabilities to the human and taking the best action given theinformation that the human currently has. We prove that under certainobservability assumptions, the optimal policy can be computed efficiently. Wedemonstrate through a human subject experiment that the proposed modelsignificantly improves human-robot team performance, compared to policies thatassume complete adaptation of the human to the robot.

Weighted Voting Via No-Regret Learning

  Voting systems typically treat all voters equally. We argue that perhaps theyshould not: Voters who have supported good choices in the past should be givenhigher weight than voters who have supported bad ones. To develop a formalframework for desirable weighting schemes, we draw on no-regret learning.Specifically, given a voting rule, we wish to design a weighting scheme suchthat applying the voting rule, with voters weighted by the scheme, leads tochoices that are almost as good as those endorsed by the best voter inhindsight. We derive possibility and impossibility results for the existence ofsuch weighting schemes, depending on whether the voting rule and the weightingscheme are deterministic or randomized, as well as on the social choice axiomssatisfied by the voting rule.

Why You Should Charge Your Friends for Borrowing Your Stuff

  We consider goods that can be shared with k-hop neighbors (i.e., the set ofnodes within k hops from an owner) on a social network. We examine incentivesto buy such a good by devising game-theoretic models where each node decideswhether to buy the good or free ride. First, we find that social inefficiency,specifically excessive purchase of the good, occurs in Nash equilibria. Second,the social inefficiency decreases as k increases and thus a good can be sharedwith more nodes. Third, and most importantly, the social inefficiency can alsobe significantly reduced by charging free riders an access cost and paying itto owners, leading to the conclusion that organizations and system designersshould impose such a cost. These findings are supported by our theoreticalanalysis in terms of the price of anarchy and the price of stability; and bysimulations based on synthetic and real social networks.

Strategyproof Linear Regression in High Dimensions

  This paper is part of an emerging line of work at the intersection of machinelearning and mechanism design, which aims to avoid noise in training data bycorrectly aligning the incentives of data sources. Specifically, we focus onthe ubiquitous problem of linear regression, where strategyproof mechanismshave previously been identified in two dimensions. In our setting, agents havesingle-peaked preferences and can manipulate only their response variables. Ourmain contribution is the discovery of a family of group strategyproof linearregression mechanisms in any number of dimensions, which we call generalizedresistant hyperplane mechanisms. The game-theoretic properties of thesemechanisms -- and, in fact, their very existence -- are established through aconnection to a discrete version of the Ham Sandwich Theorem.

Fairly Allocating Many Goods with Few Queries

  We investigate the query complexity of the fair allocation of indivisiblegoods. For two agents with arbitrary monotonic valuations, we design analgorithm that computes an allocation satisfying envy-freeness up to one good(EF1), a relaxation of envy-freeness, using a logarithmic number of queries. Weshow that the logarithmic query complexity bound also holds for three agentswith additive valuations. These results suggest that it is possible to fairlyallocate goods in practice even when the number of goods is extremely large. Bycontrast, we prove that computing an allocation satisfying envy-freeness andanother of its relaxations, envy-freeness up to any good (EFX), requires alinear number of queries even when there are only two agents with identicaladditive valuations.

The Fluid Mechanics of Liquid Democracy

  Liquid democracy is the principle of making collective decisions by lettingagents transitively delegate their votes. Despite its significant appeal, ithas become apparent that a weakness of liquid democracy is that a small subsetof agents may gain massive influence. To address this, we propose to change thecurrent practice by allowing agents to specify multiple delegation optionsinstead of just one. Much like in nature, where --- fluid mechanics teaches us--- liquid maintains an equal level in connected vessels, so do we seek tocontrol the flow of votes in a way that balances influence as much as possible.Specifically, we analyze the problem of choosing delegations to approximatelyminimize the maximum number of votes entrusted to any agent, by drawingconnections to the literature on confluent flow. We also introduce a randomgraph model for liquid democracy, and use it to demonstrate the benefits of ourapproach both theoretically and empirically.

Choosing How to Choose Papers

  It is common to see a handful of reviewers reject a highly novel paper,because they view, say, extensive experiments as far more important thannovelty, whereas the community as a whole would have embraced the paper. Moregenerally, the disparate mapping of criteria scores to final recommendations bydifferent reviewers is a major source of inconsistency in peer review. In thispaper we present a framework --- based on $L(p,q)$-norm empirical riskminimization --- for learning the community's aggregate mapping. We draw oncomputational social choice to identify desirable values of $p$ and $q$;specifically, we characterize $p=q=1$ as the only choice that satisfies threenatural axiomatic properties. Finally, we implement and apply our approach toreviews from IJCAI 2017.

Migration as Submodular Optimization

  Migration presents sweeping societal challenges that have recently attractedsignificant attention from the scientific community. One of the prominentapproaches that have been suggested employs optimization and machine learningto match migrants to localities in a way that maximizes the expected number ofmigrants who find employment. However, it relies on a strong additivityassumption that, we argue, does not hold in practice, due to competitioneffects; we propose to enhance the data-driven approach by explicitlyoptimizing for these effects. Specifically, we cast our problem as themaximization of an approximately submodular function subject to matroidconstraints, and prove that the worst-case guarantees given by the classicgreedy algorithm extend to this setting. We then present three different modelsfor competition effects, and show that they all give rise to submodularobjectives. Finally, we demonstrate via simulations that our approach leads tosignificant gains across the board.

Envy-Free Classification

  In classic fair division problems such as cake cutting and rent division,envy-freeness requires that each individual (weakly) prefer his allocation toanyone else's. On a conceptual level, we argue that envy-freeness also providesa compelling notion of fairness for classification tasks. Our technical focusis the generalizability of envy-free classification, i.e., understandingwhether a classifier that is envy free on a sample would be almost envy freewith respect to the underlying distribution with high probability. Our mainresult establishes that a small sample is sufficient to achieve suchguarantees, when the classifier in question is a mixture of deterministicclassifiers that belong to a family of low Natarajan dimension.

A Computational Approach to Organizational Structure

  An organizational structure defines how an organization arranges and managesits individuals. Because individuals in an organization take time to bothperform tasks and communicate, organizational structure must take bothcomputation and communication into account. Sociologists have studiedorganizational structure from an empirical perspective for decades, but theirwork has been limited by small sample sizes. By contrast, we studyorganizational structure from a computational and theoretical perspective.  Specifically, we introduce a model of organizations that involves bothcomputation and communication, and captures the spirit of past sociologyexperiments. In our model, each node in a graph starts with a token, and at anytime can either perform computation to merge two tokens in $t_c$ time, orperform communication by sending a token to a neighbor in $t_m$ time. We studyhow to schedule computations and communications so as to merge all tokens asquickly as possible.  As our first result, we give a polynomial-time algorithm that optimallysolves this problem on a complete graph. This result characterizes the optimalgraph structure---the edges used for communication in the optimalschedule---and therefore informs the optimal design of large-scaleorganizations. Moreover, since pre-existing organizations may want to optimizetheir workflow, we also study this problem on arbitrary graphs. We demonstratethat our problem on arbitrary graphs is not only NP-hard but also hard toapproximate within a multiplicative $1.5$ factor. Finally, we give an $O(\log n\cdot \log \frac{\text{OPT}}{t_m})$-approximation algorithm for our problem onarbitrary graphs.

Mix and Match

  Consider a matching problem on a graph where disjoint sets of vertices areprivately owned by self-interested agents. An edge between a pair of verticesindicates compatibility and allows the vertices to match. We seek a mechanismto maximize the number of matches despite self-interest, with agents that eachwant to maximize the number of their own vertices that match. Each agent canchoose to hide some of its vertices, and then privately match the hiddenvertices with any of its own vertices that go unmatched by the mechanism. Aprominent application of this model is to kidney exchange, where agentscorrespond to hospitals and vertices to donor-patient pairs. Here hospitals maygame an exchange by holding back pairs and harm social welfare. In this paperwe seek to design mechanisms that are strategyproof, in the sense that agentscannot benefit from hiding vertices, and approximately maximize efficiency,i.e., produce a matching that is close in cardinality to the maximumcardinality matching. Our main result is the design and analysis of theeponymous Mix-and-Match mechanism; we show that this randomized mechanism isstrategyproof and provides a 2-approximation. Lower bounds establish that themechanism is near optimal.

Ignorance is Almost Bliss: Near-Optimal Stochastic Matching With Few  Queries

  The stochastic matching problem deals with finding a maximum matching in agraph whose edges are unknown but can be accessed via queries. This is aspecial case of stochastic $k$-set packing, where the problem is to find amaximum packing of sets, each of which exists with some probability. In thispaper, we provide edge and set query algorithms for these two problems,respectively, that provably achieve some fraction of the omniscient optimalsolution.  Our main theoretical result for the stochastic matching (i.e., $2$-setpacking) problem is the design of an \emph{adaptive} algorithm that queriesonly a constant number of edges per vertex and achieves a $(1-\epsilon)$fraction of the omniscient optimal solution, for an arbitrarily small$\epsilon>0$. Moreover, this adaptive algorithm performs the queries in only aconstant number of rounds. We complement this result with a \emph{non-adaptive}(i.e., one round of queries) algorithm that achieves a $(0.5 - \epsilon)$fraction of the omniscient optimum. We also extend both our results tostochastic $k$-set packing by designing an adaptive algorithm that achieves a$(\frac{2}{k} - \epsilon)$ fraction of the omniscient optimal solution, againwith only $O(1)$ queries per element. This guarantee is close to the best knownpolynomial-time approximation ratio of $\frac{3}{k+1} -\epsilon$ for the\emph{deterministic} $k$-set packing problem [Furer and Yu, 2013]  We empirically explore the application of (adaptations of) these algorithmsto the kidney exchange problem, where patients with end-stage renal failureswap willing but incompatible donors. We show on both generated data and onreal data from the first 169 match runs of the UNOS nationwide kidney exchangethat even a very small number of non-adaptive edge queries per vertex resultsin large gains in expected successful matches.

