Parametric Fokker-Planck equation

  We derive the Fokker-Planck equation on the parametric space. It is theWasserstein gradient flow of relative entropy on the statistical manifold. Wepull back the PDE to a finite dimensional ODE on parameter space. Someanalytical example and numerical examples are presented.

Principal Manifolds and Nonlinear Dimension Reduction via Local Tangent  Space Alignment

  Nonlinear manifold learning from unorganized data points is a verychallenging unsupervised learning and data visualization problem with a greatvariety of applications. In this paper we present a new algorithm for manifoldlearning and nonlinear dimension reduction. Based on a set of unorganized datapoints sampled with noise from the manifold, we represent the local geometry ofthe manifold using tangent spaces learned by fitting an affine subspace in aneighborhood of each data point. Those tangent spaces are aligned to give theinternal global coordinates of the data points with respect to the underlyingmanifold by way of a partial eigendecomposition of the neighborhood connectionmatrix. We present a careful error analysis of our algorithm and show that thereconstruction errors are of second-order accuracy. We illustrate our algorithmusing curves and surfaces both in  2D/3D and higher dimensional Euclidean spaces, and 64-by-64 pixel face imageswith various pose and lighting conditions. We also address several theoreticaland algorithmic issues for further research and improvements.

Bipartite graph partitioning and data clustering

  Many data types arising from data mining applications can be modeled asbipartite graphs, examples include terms and documents in a text corpus,customers and purchasing items in market basket analysis and reviewers andmovies in a movie recommender system. In this paper, we propose a new dataclustering method based on partitioning the underlying bipartite graph. Thepartition is constructed by minimizing a normalized sum of edge weights betweenunmatched pairs of vertices of the bipartite graph. We show that an approximatesolution to the minimization problem can be obtained by computing a partialsingular value decomposition (SVD) of the associated edge weight matrix of thebipartite graph. We point out the connection of our clustering algorithm tocorrespondence analysis used in multivariate analysis. We also briefly discussthe issue of assigning data objects to multiple clusters. In the experimentalresults, we apply our clustering algorithm to the problem of documentclustering to illustrate its effectiveness and efficiency.

Contour regression: A general approach to dimension reduction

  We propose a novel approach to sufficient dimension reduction in regression,based on estimating contour directions of small variation in the response.These directions span the orthogonal complement of the minimal space relevantfor the regression and can be extracted according to two measures of variationin the response, leading to simple and general contour regression (SCR and GCR)methodology. In comparison with existing sufficient dimension reductiontechniques, this contour-based methodology guarantees exhaustive estimation ofthe central subspace under ellipticity of the predictor distribution and mildadditional assumptions, while maintaining \sqrtn-consistency and computationalease. Moreover, it proves robust to departures from ellipticity. We establishpopulation properties for both SCR and GCR, and asymptotic properties for SCR.Simulations to compare performance with that of standard techniques such asordinary least squares, sliced inverse regression, principal Hessian directionsand sliced average variance estimation confirm the advantages anticipated bythe theoretical analyses. We demonstrate the use of contour-based methods on adata set concerning soil evaporation.

Consistent Computation of First- and Second-Order Differential  Quantities for Surface Meshes

  Differential quantities, including normals, curvatures, principal directions,and associated matrices, play a fundamental role in geometric processing andphysics-based modeling. Computing these differential quantities consistently onsurface meshes is important and challenging, and some existing methods oftenproduce inconsistent results and require ad hoc fixes. In this paper, we showthat the computation of the gradient and Hessian of a height function providesthe foundation for consistently computing the differential quantities. Wederive simple, explicit formulas for the transformations between the first- andsecond-order differential quantities (i.e., normal vector and principalcurvature tensor) of a smooth surface and the first- and second-orderderivatives (i.e., gradient and Hessian) of its corresponding height function.We then investigate a general, flexible numerical framework to estimate thederivatives of the height function based on local polynomial fittingsformulated as weighted least squares approximations. We also propose aniterative fitting scheme to improve accuracy. This framework generalizespolynomial fitting and addresses some of its accuracy and stability issues, asdemonstrated by our theoretical analysis as well as experimental results.

Hybrid Generative/Discriminative Learning for Automatic Image Annotation

  Automatic image annotation (AIA) raises tremendous challenges to machinelearning as it requires modeling of data that are both ambiguous in input andoutput, e.g., images containing multiple objects and labeled with multiplesemantic tags. Even more challenging is that the number of candidate tags isusually huge (as large as the vocabulary size) yet each image is only relatedto a few of them. This paper presents a hybrid generative-discriminativeclassifier to simultaneously address the extreme data-ambiguity andoverfitting-vulnerability issues in tasks such as AIA. Particularly: (1) anExponential-Multinomial Mixture (EMM) model is established to capture both theinput and output ambiguity and in the meanwhile to encourage predictionsparsity; and (2) the prediction ability of the EMM model is explicitlymaximized through discriminative learning that integrates variational inferenceof graphical models and the pairwise formulation of ordinal regression.Experiments show that our approach achieves both superior annotationperformance and better tag scalability.

Supervised Laplacian Eigenmaps with Applications in Clinical Diagnostics  for Pediatric Cardiology

  Electronic health records contain rich textual data which possess criticalpredictive information for machine-learning based diagnostic aids. However manytraditional machine learning methods fail to simultaneously integrate bothvector space data and text. We present a supervised method using Laplacianeigenmaps to augment existing machine-learning methods with low-dimensionalrepresentations of textual predictors which preserve the local similarities.The proposed implementation performs alternating optimization using gradientdescent. For the evaluation we applied our method to over 2,000 patient recordsfrom a large single-center pediatric cardiology practice to predict if patientswere diagnosed with cardiac disease. Our method was compared with latentsemantic indexing, latent Dirichlet allocation, and local Fisher discriminantanalysis. The results were assessed using AUC, MCC, specificity, andsensitivity. Results indicate supervised Laplacian eigenmaps was the highestperforming method in our study, achieving 0.782 and 0.374 for AUC and MCCrespectively. SLE showed an increase in 8.16% in AUC and 20.6% in MCC over thebaseline which excluded textual data and a 2.69% and 5.35% increase in AUC andMCC respectively over unsupervised Laplacian eigenmaps. This method allows manyexisting machine learning predictors to effectively and efficiently utilize thepotential of textual predictors.

Learning the Gain Values and Discount Factors of DCG

  Evaluation metrics are an essential part of a ranking system, and in the pastmany evaluation metrics have been proposed in information retrieval and Websearch. Discounted Cumulated Gains (DCG) has emerged as one of the evaluationmetrics widely adopted for evaluating the performance of ranking functions usedin Web search. However, the two sets of parameters, gain values and discountfactors, used in DCG are determined in a rather ad-hoc way. In this paper wefirst show that DCG is generally not coherent, meaning that comparing theperformance of ranking functions using DCG very much depends on the particulargain values and discount factors used. We then propose a novel methodology thatcan learn the gain values and discount factors from user preferences overrankings. Numerical simulations illustrate the effectiveness of our proposedmethods. Please contact the authors for the full version of this work.

Shaping Social Activity by Incentivizing Users

  Events in an online social network can be categorized roughly into endogenousevents, where users just respond to the actions of their neighbors within thenetwork, or exogenous events, where users take actions due to drives externalto the network. How much external drive should be provided to each user, suchthat the network activity can be steered towards a target state? In this paper,we model social events using multivariate Hawkes processes, which can captureboth endogenous and exogenous event intensities, and derive a time dependentlinear relation between the intensity of exogenous events and the overallnetwork activity. Exploiting this connection, we develop a convex optimizationframework for determining the required level of external drive in order for thenetwork to reach a desired activity level. We experimented with event datagathered from Twitter, and show that our method can steer the activity of thenetwork more accurately than alternatives.

Linear Contour Learning: A Method for Supervised Dimension Reduction

  We propose a novel approach to sufficient dimension reduction in regression,based on estimating contour directions of negligible variation for the responsesurface. These directions span the orthogonal complement of the minimal spacerelevant for the regression, and can be extracted according to a measure of thevariation in the response, leading to General Contour Regression(GCR). Incomparison to exiisting sufficient dimension reduction techniques, thissontour-based mothology guarantees exhaustive estimation of the central spaceunder ellipticity of the predictoor distribution and very mild additionalassumptions, while maintaining vn-consisytency and somputational ease.Moreover, it proves to be robust to departures from ellipticity. We alsoestablish some useful population properties for GCR. Simulations to compareperformance with that of standard techniques such as ordinary least squares,sliced inverse regression, principal hessian directions, and sliced averagevariance estimation confirm the advntages anticipated by theoretical analyses.We also demonstrate the use of contour-based methods on a data set concerninggrades of students from Massachusetts colleges.

Back to the Past: Source Identification in Diffusion Networks from  Partially Observed Cascades

  When a piece of malicious information becomes rampant in an informationdiffusion network, can we identify the source node that originally introducedthe piece into the network and infer the time when it initiated this? Beingable to do so is critical for curtailing the spread of malicious information,and reducing the potential losses incurred. This is a very challenging problemsince typically only incomplete traces are observed and we need to unroll theincomplete traces into the past in order to pinpoint the source. In this paper,we tackle this problem by developing a two-stage framework, which first learnsa continuous-time diffusion network model based on historical diffusion tracesand then identifies the source of an incomplete diffusion trace by maximizingthe likelihood of the trace under the learned model. Experiments on both largesynthetic and real-world data show that our framework can effectively go backto the past, and pinpoint the source node and its initiation time significantlymore accurately than previous state-of-the-arts.

Correlated Cascades: Compete or Cooperate

  In real world social networks, there are multiple cascades which are rarelyindependent. They usually compete or cooperate with each other. Motivated bythe reinforcement theory in sociology we leverage the fact that adoption of auser to any behavior is modeled by the aggregation of behaviors of itsneighbors. We use a multidimensional marked Hawkes process to model usersproduct adoption and consequently spread of cascades in social networks. Theresulting inference problem is proved to be convex and is solved in parallel byusing the barrier method. The advantage of the proposed model is twofold; itmodels correlated cascades and also learns the latent diffusion network.Experimental results on synthetic and two real datasets gathered from Twitter,URL shortening and music streaming services, illustrate the superiorperformance of the proposed model over the alternatives.

Influence Prediction for Continuous-Time Information Propagation on  Networks

  We consider the problem of predicting the time evolution of influence, theexpected number of activated nodes, given a set of initially active nodes on apropagation network. To address the significant computational challenges ofthis problem on large-scale heterogeneous networks, we establish a system ofdifferential equations governing the dynamics of probability mass functions onthe state graph where the nodes each lumps a number of activation states of thenetwork, which can be considered as an analogue to the Fokker-Planck equationin continuous space. We provides several methods to estimate the systemparameters which depend on the identities of the initially active nodes,network topology, and activation rates etc. The influence is then estimated bythe solution of such a system of differential equations. This approach givesrise to a class of novel and scalable algorithms that work effectively forlarge-scale and dense networks. Numerical results are provided to show the verypromising performance in terms of prediction accuracy and computationalefficiency of this approach.

Learning Granger Causality for Hawkes Processes

  Learning Granger causality for general point processes is a very challengingtask. In this paper, we propose an effective method, learning Grangercausality, for a special but significant type of point processes --- Hawkesprocess. We reveal the relationship between Hawkes process's impact functionand its Granger causality graph. Specifically, our model represents impactfunctions using a series of basis functions and recovers the Granger causalitygraph via group sparsity of the impact functions' coefficients. We propose aneffective learning algorithm combining a maximum likelihood estimator (MLE)with a sparse-group-lasso (SGL) regularizer. Additionally, the flexibility ofour model allows to incorporate the clustering structure event types intolearning framework. We analyze our learning algorithm and propose an adaptiveprocedure to select basis functions. Experiments on both synthetic andreal-world data show that our method can learn the Granger causality graph andthe triggering patterns of the Hawkes processes simultaneously.

A Self-Paced Regularization Framework for Multi-Label Learning

  In this paper, we propose a novel multi-label learning framework, calledMulti-Label Self-Paced Learning (MLSPL), in an attempt to incorporate theself-paced learning strategy into multi-label learning regime. In light of thebenefits of adopting the easy-to-hard strategy proposed by self-paced learning,the devised MLSPL aims to learn multiple labels jointly by gradually includinglabel learning tasks and instances into model training from the easy to thehard. We first introduce a self-paced function as a regularizer in themulti-label learning formulation, so as to simultaneously rank priorities ofthe label learning tasks and the instances in each learning iteration.Considering that different multi-label learning scenarios often need differentself-paced schemes during optimization, we thus propose a general way to findthe desired self-paced functions. Experimental results on three benchmarkdatasets suggest the state-of-the-art performance of our approach.

Self-Paced Multi-Task Learning

  In this paper, we propose a novel multi-task learning (MTL) framework, calledSelf-Paced Multi-Task Learning (SPMTL). Different from previous works treatingall tasks and instances equally when training, SPMTL attempts to jointly learnthe tasks by taking into consideration the complexities of both tasks andinstances. This is inspired by the cognitive process of human brain that oftenlearns from the easy to the hard. We construct a compact SPMTL formulation byproposing a new task-oriented regularizer that can jointly prioritize the tasksand the instances. Thus it can be interpreted as a self-paced learner for MTL.A simple yet effective algorithm is designed for optimizing the proposedobjective function. An error bound for a simplified formulation is alsoanalyzed theoretically. Experimental results on toy and real-world datasetsdemonstrate the effectiveness of the proposed approach, compared to thestate-of-the-art methods.

Multistage Campaigning in Social Networks

  We consider the problem of how to optimize multi-stage campaigning oversocial networks. The dynamic programming framework is employed to balance thehigh present reward and large penalty on low future outcome in the presence ofextensive uncertainties. In particular, we establish theoretical foundations ofoptimal campaigning over social networks where the user activities are modeledas a multivariate Hawkes process, and we derive a time dependent linearrelation between the intensity of exogenous events and several commonly usedobjective functions of campaigning. We further develop a convex dynamicprogramming framework for determining the optimal intervention policy thatprescribes the required level of external drive at each stage for the desiredcampaigning result. Experiments on both synthetic data and the real-worldMemeTracker dataset show that our algorithm can steer the user activities foroptimal campaigning much more accurately than baselines.

A hybrid approach for risk assessment of loan guarantee network

  Groups of Small and Medium Enterprises (SME) back each other and formguarantee network to obtain loan from banks. The risk over the networkedenterprises may cause significant contagious damage. To dissolve such risks, wepropose a hybrid feature representation, which is feeded into a gradientboosting model for credit risk assessment of guarantee network. Empirical studyis performed on a ten-year guarantee loan record from commercial banks. We findthat often hundreds or thousands of enterprises back each other and constitutea sparse complex network. We study the risk of various structures of loanguarantee network, and observe the high correlation between defaults withcentrality, and with the communities of the network. In particular, ourquantitative risk evaluation model shows promising prediction performance onreal-world data, which can be useful to both regulators and stakeholders.

Learning Hawkes Processes from Short Doubly-Censored Event Sequences

  Many real-world applications require robust algorithms to learn pointprocesses based on a type of incomplete data --- the so-called shortdoubly-censored (SDC) event sequences. We study this critical problem ofquantitative asynchronous event sequence analysis under the framework of Hawkesprocesses by leveraging the idea of data synthesis. Given SDC event sequencesobserved in a variety of time intervals, we propose a sampling-stitching datasynthesis method --- sampling predecessors and successors for each SDC eventsequence from potential candidates and stitching them together to synthesizelong training sequences. The rationality and the feasibility of our method arediscussed in terms of arguments based on likelihood. Experiments on bothsynthetic and real-world data demonstrate that the proposed data synthesismethod improves learning results indeed for both time-invariant andtime-varying Hawkes processes.

Fake News Mitigation via Point Process Based Intervention

  We propose the first multistage intervention framework that tackles fake newsin social networks by combining reinforcement learning with a point processnetwork activity model. The spread of fake news and mitigation events withinthe network is modeled by a multivariate Hawkes process with additionalexogenous control terms. By choosing a feature representation of states,defining mitigation actions and constructing reward functions to measure theeffectiveness of mitigation activities, we map the problem of fake newsmitigation into the reinforcement learning framework. We develop a policyiteration method unique to the multivariate networked point process, with thegoal of optimizing the actions for maximal total reward under budgetconstraints. Our method shows promising performance in real-time interventionexperiments on a Twitter network to mitigate a surrogate fake news campaign,and outperforms alternatives on synthetic datasets.

Deep Extreme Multi-label Learning

  Extreme multi-label learning (XML) or classification has been a practical andimportant problem since the boom of big data. The main challenge lies in theexponential label space which involves $2^L$ possible label sets especiallywhen the label dimension $L$ is huge, e.g., in millions for Wikipedia labels.This paper is motivated to better explore the label space by originallyestablishing an explicit label graph. In the meanwhile, deep learning has beenwidely studied and used in various classification problems includingmulti-label classification, however it has not been properly introduced to XML,where the label space can be as large as in millions. In this paper, we proposea practical deep embedding method for extreme multi-label classification, whichharvests the ideas of non-linear embedding and graph priors-based label spacemodeling simultaneously. Extensive experiments on public datasets for XML showthat our method performs competitive against state-of-the-art result.

Wasserstein Learning of Deep Generative Point Process Models

  Point processes are becoming very popular in modeling asynchronous sequentialdata due to their sound mathematical foundation and strength in modeling avariety of real-world phenomena. Currently, they are often characterized viaintensity function which limits model's expressiveness due to unrealisticassumptions on its parametric form used in practice. Furthermore, they arelearned via maximum likelihood approach which is prone to failure inmulti-modal distributions of sequences. In this paper, we propose anintensity-free approach for point processes modeling that transforms nuisanceprocesses to a target one. Furthermore, we train the model using alikelihood-free leveraging Wasserstein distance between point processes.Experiments on various synthetic and real-world data substantiate thesuperiority of the proposed point process model over conventional ones.

THAP: A Matlab Toolkit for Learning with Hawkes Processes

  As a powerful tool of asynchronous event sequence analysis, point processeshave been studied for a long time and achieved numerous successes in differentfields. Among various point process models, Hawkes process and its variantsattract many researchers in statistics and computer science these years becausethey capture the self- and mutually-triggering patterns between differentevents in complicated sequences explicitly and quantitatively and are broadlyapplicable to many practical problems. In this paper, we describe anopen-source toolkit implementing many learning algorithms and analysis toolsfor Hawkes process model and its variants. Our toolkit systematicallysummarizes recent state-of-the-art algorithms as well as most classicalgorithms of Hawkes processes, which is beneficial for both academicaleducation and research. Source code can be downloaded fromhttps://github.com/HongtengXu/Hawkes-Process-Toolkit.

Learning Registered Point Processes from Idiosyncratic Observations

  A parametric point process model is developed, with modeling based on theassumption that sequential observations often share latent phenomena, whilealso possessing idiosyncratic effects. An alternating optimization method isproposed to learn a "registered" point process that accounts for sharedstructure, as well as "warping" functions that characterize idiosyncraticaspects of each observed sequence. Under reasonable constraints, in eachiteration we update the sample-specific warping functions by solving a set ofconstrained nonlinear programming problems in parallel, and update the model bymaximum likelihood estimation. The justifiability, complexity and robustness ofthe proposed method are investigated in detail, and the influence of sequencestitching on the learning results is examined empirically. Experiments on bothsynthetic and real-world data demonstrate that the method yields explainablepoint process models, achieving encouraging results compared tostate-of-the-art methods.

A unified framework for manifold landmarking

  The success of semi-supervised manifold learning is highly dependent on thequality of the labeled samples. Active manifold learning aims to select andlabel representative landmarks on a manifold from a given set of samples toimprove semi-supervised manifold learning. In this paper, we propose a novelactive manifold learning method based on a unified framework of manifoldlandmarking. In particular, our method combines geometric manifold landmarkingmethods with algebraic ones. We achieve this by using the Gershgorin circletheorem to construct an upper bound on the learning error that depends on thelandmarks and the manifold's alignment matrix in a way that captures both thegeometric and algebraic criteria. We then attempt to select landmarks so as tominimize this bound by iteratively deleting the Gershgorin circlescorresponding to the selected landmarks. We also analyze the complexity,scalability, and robustness of our method through simulations, and demonstrateits superiority compared to existing methods. Experiments in regression andclassification further verify that our method performs better than itscompetitors.

Learning Deep Mean Field Games for Modeling Large Population Behavior

  We consider the problem of representing collective behavior of largepopulations and predicting the evolution of a population distribution over adiscrete state space. A discrete time mean field game (MFG) is motivated as aninterpretable model founded on game theory for understanding the aggregateeffect of individual actions and predicting the temporal evolution ofpopulation distributions. We achieve a synthesis of MFG and Markov decisionprocesses (MDP) by showing that a special MFG is reducible to an MDP. Thisenables us to broaden the scope of mean field game theory and infer MFG modelsof large real-world systems via deep inverse reinforcement learning. Our methodlearns both the reward function and forward dynamics of an MFG from real data,and we report the first empirical test of a mean field game model of areal-world social media population.

tau-FPL: Tolerance-Constrained Learning in Linear Time

  Learning a classifier with control on the false-positive rate plays acritical role in many machine learning applications. Existing approaches eitherintroduce prior knowledge dependent label cost or tune parameters based ontraditional classifiers, which lack consistency in methodology because they donot strictly adhere to the false-positive rate constraint. In this paper, wepropose a novel scoring-thresholding approach, tau-False Positive Learning(tau-FPL) to address this problem. We show the scoring problem which takes thefalse-positive rate tolerance into accounts can be efficiently solved in lineartime, also an out-of-bootstrap thresholding method can transform the learnedranking function into a low false-positive classifier. Both theoreticalanalysis and experimental results show superior performance of the proposedtau-FPL over existing approaches.

Decoupled Learning for Factorial Marked Temporal Point Processes

  This paper introduces the factorial marked temporal point process model andpresents efficient learning methods. In conventional (multi-dimensional) markedtemporal point process models, event is often encoded by a single discretevariable i.e. a marker. In this paper, we describe the factorial marked pointprocesses whereby time-stamped event is factored into multiple markers.Accordingly the size of the infectivity matrix modeling the effect betweenpairwise markers is in power order w.r.t. the number of the discrete markerspace. We propose a decoupled learning method with two learning procedures: i)directly solving the model based on two techniques: Alternating DirectionMethod of Multipliers and Fast Iterative Shrinkage-Thresholding Algorithm; ii)involving a reformulation that transforms the original problem into a LogisticRegression model for more efficient learning. Moreover, a sparse groupregularizer is added to identify the key profile features and event labels.Empirical results on real world datasets demonstrate the efficiency of ourdecoupled and reformulated method. The source code is available online.

Learning to Match via Inverse Optimal Transport

  We propose a unified data-driven framework based on inverse optimal transportthat can learn adaptive, nonlinear interaction cost function from noisy andincomplete empirical matching matrix and predict new matching in variousmatching contexts. We emphasize that the discrete optimal transport plays therole of a variational principle which gives rise to an optimization-basedframework for modeling the observed empirical matching data. Our formulationleads to a non-convex optimization problem which can be solved efficiently byan alternating optimization method. A key novel aspect of our formulation isthe incorporation of marginal relaxation via regularized Wasserstein distance,significantly improving the robustness of the method in the face of noisy ormissing empirical matching data. Our model falls into the category ofprescriptive models, which not only predict potential future matching, but isalso able to explain what leads to empirical matching and quantifies the impactof changes in matching factors. The proposed approach has wide applicabilityincluding predicting matching in online dating, labor market, collegeapplication and crowdsourcing. We back up our claims with numerical experimentson both synthetic data and real world data sets.

A Fast Proximal Point Method for Computing Wasserstein Distance

  Wasserstein distance plays increasingly important roles in machine learning,stochastic programming and image processing. Major efforts have been under wayto address its high computational complexity, some leading to approximate orregularized variations such as Sinkhorn distance. However, as we willdemonstrate, regularized variations with large regularization parameter willdegradate the performance in several important machine learning applications,and small regularization parameter will fail due to numerical stability issueswith existing algorithms. We address this challenge by developing an InexactProximal point method for Optimal Transport (IPOT) with the proximal operatorapproximately evaluated at each iteration using projections to the probabilitysimplex. We prove the algorithm has linear convergence rate. We also apply IPOTto learning generative models, and generalize the idea of IPOT to a new methodfor computing Wasserstein barycenter.

Representation Learning over Dynamic Graphs

  How can we effectively encode evolving information over dynamic graphs intolow-dimensional representations? In this paper, we propose DyRep, an inductivedeep representation learning framework that learns a set of functions toefficiently produce low-dimensional node embeddings that evolves over time. Thelearned embeddings drive the dynamics of two key processes namely,communication and association between nodes in dynamic graphs. These processesexhibit complex nonlinear dynamics that evolve at different time scales andsubsequently contribute to the update of node embeddings. We employ atime-scale dependent multivariate point process model to capture thesedynamics. We devise an efficient unsupervised learning procedure anddemonstrate that our approach significantly outperforms representativebaselines on two real-world datasets for the problem of dynamic link predictionand event time prediction.

Learning to Optimize via Wasserstein Deep Inverse Optimal Control

  We study the inverse optimal control problem in social sciences: we aim atlearning a user's true cost function from the observed temporal behavior. Incontrast to traditional phenomenological works that aim to learn a generativemodel to fit the behavioral data, we propose a novel variational principle andtreat user as a reinforcement learning algorithm, which acts by optimizing hiscost function. We first propose a unified KL framework that generalizesexisting maximum entropy inverse optimal control methods. We further propose atwo-step Wasserstein inverse optimal control framework. In the first step, wecompute the optimal measure with a novel mass transport equation. In the secondstep, we formulate the learning problem as a generative adversarial network. Intwo real world experiments - recommender systems and social networks, we showthat our framework obtains significant performance gains over both existinginverse optimal control methods and point process based generative models.

LinkNBed: Multi-Graph Representation Learning with Entity Linkage

  Knowledge graphs have emerged as an important model for studying complexmulti-relational data. This has given rise to the construction of numerouslarge scale but incomplete knowledge graphs encoding information extracted fromvarious resources. An effective and scalable approach to jointly learn overmultiple graphs and eventually construct a unified graph is a crucial next stepfor the success of knowledge-based inference for many downstream applications.To this end, we propose LinkNBed, a deep relational learning framework thatlearns entity and relationship representations across multiple graphs. Weidentify entity linkage across graphs as a vital component to achieve our goal.We design a novel objective that leverage entity linkage and build an efficientmulti-task training procedure. Experiments on link prediction and entitylinkage demonstrate substantial improvements over the state-of-the-artrelational learning approaches.

CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement  Learning

  We propose CM3, a new deep reinforcement learning method for cooperativemulti-agent problems where agents must coordinate for joint success inachieving different individual goals. We restructure multi-agent learning intoa two-stage curriculum, consisting of a single-agent stage for learning toaccomplish individual tasks, followed by a multi-agent stage for learning tocooperate in the presence of other agents. These two stages are bridged bymodular augmentation of neural network policy and value functions. We furtheradapt the actor-critic framework to this curriculum by formulating local andglobal views of the policy gradient and learning via a double critic,consisting of a decentralized value function and a centralized action-valuefunction. We evaluated CM3 on a new high-dimensional multi-agent environmentwith sparse rewards: negotiating lane changes among multiple autonomousvehicles in the Simulation of Urban Mobility (SUMO) traffic simulator. Detailedablation experiments show the positive contribution of each component in CM3,and the overall synthesis converges significantly faster to higher performancepolicies than existing cooperative multi-agent methods.

A Jump Stochastic Differential Equation Approach for Influence  Prediction on Information Propagation Networks

  We propose a novel problem formulation of continuous-time informationpropagation on heterogenous networks based on jump stochastic differentialequations (SDE). The structure of the network and activation rates betweennodes are naturally taken into account in the SDE system. This new formulationallows for efficient and stable algorithm for many challenging informationpropagation problems, including estimations of individual activationprobability and influence level, by solving the SDE numerically. To this end,we develop an efficient numerical algorithm incorporating variance reduction;furthermore, we provide theoretical bounds for its sample complexity. Moreover,we show that the proposed jump SDE approach can be applied to a much largerclass of critical information propagation problems with more complicatedsettings. Numerical experiments on a variety of synthetic and real-worldpropagation networks show that the proposed method is more accurate andefficient compared with the state-of-the-art methods.

Gromov-Wasserstein Learning for Graph Matching and Node Embedding

  A novel Gromov-Wasserstein learning framework is proposed to jointly match(align) graphs and learn embedding vectors for the associated graph nodes.Using Gromov-Wasserstein discrepancy, we measure the dissimilarity between twographs and find their correspondence, according to the learned optimaltransport. The node embeddings associated with the two graphs are learned underthe guidance of the optimal transport, the distance of which not only reflectsthe topological structure of each graph but also yields the correspondenceacross the graphs. These two learning steps are mutually-beneficial, and areunified here by minimizing the Gromov-Wasserstein discrepancy with structuralregularizers. This framework leads to an optimization problem that is solved bya proximal point method. We apply the proposed method to matching problems inreal-world networks, and demonstrate its superior performance compared toalternative approaches.

Scalable Influence Estimation in Continuous-Time Diffusion Networks

  If a piece of information is released from a media site, can it spread, in 1month, to a million web pages? This influence estimation problem is verychallenging since both the time-sensitive nature of the problem and the issueof scalability need to be addressed simultaneously. In this paper, we propose arandomized algorithm for influence estimation in continuous-time diffusionnetworks. Our algorithm can estimate the influence of every node in a networkwith |V| nodes and |E| edges to an accuracy of $\varepsilon$ using$n=O(1/\varepsilon^2)$ randomizations and up to logarithmic factorsO(n|E|+n|V|) computations. When used as a subroutine in a greedy influencemaximization algorithm, our proposed method is guaranteed to find a set ofnodes with an influence of at least (1-1/e)OPT-2$\varepsilon$, where OPT is theoptimal value. Experiments on both synthetic and real-world data show that theproposed method can easily scale up to networks of millions of nodes whilesignificantly improves over previous state-of-the-arts in terms of the accuracyof the estimated influence and the quality of the selected nodes in maximizingthe influence.

A General Multi-Graph Matching Approach via Graduated  Consistency-regularized Boosting

  This paper addresses the problem of matching $N$ weighted graphs referring toan identical object or category. More specifically, matching the common nodecorrespondences among graphs. This multi-graph matching problem involves twoingredients affecting the overall accuracy: i) the local pairwise matchingaffinity score among graphs; ii) the global matching consistency that measuresthe uniqueness of the pairwise matching results by different chaining orders.Previous studies typically either enforce the matching consistency constraintsin the beginning of iterative optimization, which may propagate matching errorboth over iterations and across graph pairs; or separate affinity optimizingand consistency regularization in two steps. This paper is motivated by theobservation that matching consistency can serve as a regularizer in theaffinity objective function when the function is biased due to noises orinappropriate modeling. We propose multi-graph matching methods to incorporatethe two aspects by boosting the affinity score, meanwhile gradually infusingthe consistency as a regularizer. Furthermore, we propose a node-wiseconsistency/affinity-driven mechanism to elicit the common inlier nodes out ofthe irrelevant outliers. Extensive results on both synthetic and public imagedatasets demonstrate the competency of the proposed algorithms.

Joint Active Learning with Feature Selection via CUR Matrix  Decomposition

  This paper presents an unsupervised learning approach for simultaneous sampleand feature selection, which is in contrast to existing works which mainlytackle these two problems separately. In fact the two tasks are ofteninterleaved with each other: noisy and high-dimensional features will bringadverse effect on sample selection, while informative or representative sampleswill be beneficial to feature selection. Specifically, we propose a frameworkto jointly conduct active learning and feature selection based on the CURmatrix decomposition. From the data reconstruction perspective, both theselected samples and features can best approximate the original datasetrespectively, such that the selected samples characterized by the features arehighly representative. In particular, our method runs in one-shot without theprocedure of iterative sample selection for progressive labeling. Thus, ourmodel is especially suitable when there are few labeled samples or even in theabsence of supervision, which is a particular challenge for existing methods.As the joint learning problem is NP-hard, the proposed formulation involves aconvex but non-smooth optimization problem. We solve it efficiently by aniterative algorithm, and prove its global convergence. Experimental results onpublicly available datasets corroborate the efficacy of our method comparedwith the state-of-the-art.

COEVOLVE: A Joint Point Process Model for Information Diffusion and  Network Co-evolution

  Information diffusion in online social networks is affected by the underlyingnetwork topology, but it also has the power to change it. Online users areconstantly creating new links when exposed to new information sources, and inturn these links are alternating the way information spreads. However, thesetwo highly intertwined stochastic processes, information diffusion and networkevolution, have been predominantly studied separately, ignoring theirco-evolutionary dynamics.  We propose a temporal point process model, COEVOLVE, for such joint dynamics,allowing the intensity of one process to be modulated by that of the other.This model allows us to efficiently simulate interleaved diffusion and networkevents, and generate traces obeying common diffusion and network patternsobserved in real-world networks. Furthermore, we also develop a convexoptimization framework to learn the parameters of the model from historicaldiffusion and network evolution traces. We experimented with both syntheticdata and data gathered from Twitter, and show that our model provides a goodfit to the data as well as more accurate predictions than alternatives.

A Continuous-time Mutually-Exciting Point Process Framework for  Prioritizing Events in Social Media

  The overwhelming amount and rate of information update in online social mediais making it increasingly difficult for users to allocate their attention totheir topics of interest, thus there is a strong need for prioritizing newsfeeds. The attractiveness of a post to a user depends on many complexcontextual and temporal features of the post. For instance, the contents of thepost, the responsiveness of a third user, and the age of the post may all haveimpact. So far, these static and dynamic features has not been incorporated ina unified framework to tackle the post prioritization problem. In this paper,we propose a novel approach for prioritizing posts based on a feature modulatedmulti-dimensional point process. Our model is able to simultaneously capturetextual and sentiment features, and temporal features such as self-excitation,mutual-excitation and bursty nature of social interaction. As an evaluation, wealso curated a real-world conversational benchmark dataset crawled fromFacebook. In our experiments, we demonstrate that our algorithm is able toachieve the-state-of-the-art performance in terms of analyzing, predicting, andprioritizing events. In terms of interpretability of our method, we observethat features indicating individual user profile and linguistic characteristicsof the events work best for prediction and prioritization of new events.

Fractal Dimension Invariant Filtering and Its CNN-based Implementation

  Fractal analysis has been widely used in computer vision, especially intexture image processing and texture analysis. The key concept of fractal-basedimage model is the fractal dimension, which is invariant to bi-Lipschitztransformation of image, and thus capable of representing intrinsic structuralinformation of image robustly. However, the invariance of fractal dimensiongenerally does not hold after filtering, which limits the application offractal-based image model. In this paper, we propose a novel fractal dimensioninvariant filtering (FDIF) method, extending the invariance of fractaldimension to filtering operations. Utilizing the notion of localself-similarity, we first develop a local fractal model for images. By adding anonlinear post-processing step behind anisotropic filter banks, we demonstratethat the proposed filtering method is capable of preserving the localinvariance of the fractal dimension of image. Meanwhile, we show that the FDIFmethod can be re-instantiated approximately via a CNN-based architecture, wherethe convolution layer extracts anisotropic structure of image and the nonlinearlayer enhances the structure via preserving local fractal dimension of image.The proposed filtering method provides us with a novel geometric interpretationof CNN-based image model. Focusing on a challenging image processing task ---detecting complicated curves from the texture-like images, the proposed methodobtains superior results to the state-of-art approaches.

A constrained clustering based approach for matching a collection of  feature sets

  In this paper, we consider the problem of finding the feature correspondencesamong a collection of feature sets, by using their point-wise unary features.This is a fundamental problem in computer vision and pattern recognition, whichalso closely relates to other areas such as operational research. Differentfrom two-set matching which can be transformed to a quadratic assignmentprogramming task that is known NP-hard, inclusion of merely unary attributesleads to a linear assignment problem for matching two feature sets. Thisproblem has been well studied and there are effective polynomial global optimumsolvers such as the Hungarian method. However, it becomes ill-posed when theunary attributes are (heavily) corrupted. The global optimal correspondenceconcerning the best score defined by the attribute affinity/cost between thetwo sets can be distinct to the ground truth correspondence since the scorefunction is biased by noises. To combat this issue, we devise a method formatching a collection of feature sets by synergetically exploring theinformation across the sets. In general, our method can be perceived from a(constrained) clustering perspective: in each iteration, it assigns thefeatures of one set to the clusters formed by the rest of feature sets, andupdates the cluster centers in turn. Results on both synthetic data and realimages suggest the efficacy of our method against state-of-the-arts.

Scalable Influence Maximization for Multiple Products in Continuous-Time  Diffusion Networks

  A typical viral marketing model identifies influential users in a socialnetwork to maximize a single product adoption assuming unlimited userattention, campaign budgets, and time. In reality, multiple products needcampaigns, users have limited attention, convincing users incurs costs, andadvertisers have limited budgets and expect the adoptions to be maximized soon.Facing these user, monetary, and timing constraints, we formulate the problemas a submodular maximization task in a continuous-time diffusion model underthe intersection of a matroid and multiple knapsack constraints. We propose arandomized algorithm estimating the user influence in a network($|\mathcal{V}|$ nodes, $|\mathcal{E}|$ edges) to an accuracy of $\epsilon$with $n=\mathcal{O}(1/\epsilon^2)$ randomizations and$\tilde{\mathcal{O}}(n|\mathcal{E}|+n|\mathcal{V}|)$ computations. Byexploiting the influence estimation algorithm as a subroutine, we develop anadaptive threshold greedy algorithm achieving an approximation factor $k_a/(2+2k)$ of the optimal when $k_a$ out of the $k$ knapsack constraints are active.Extensive experiments on networks of millions of nodes demonstrate that theproposed algorithms achieve the state-of-the-art in terms of effectiveness andscalability.

A Dirichlet Mixture Model of Hawkes Processes for Event Sequence  Clustering

  We propose an effective method to solve the event sequence clusteringproblems based on a novel Dirichlet mixture model of a special but significanttype of point processes --- Hawkes process. In this model, each event sequencebelonging to a cluster is generated via the same Hawkes process with specificparameters, and different clusters correspond to different Hawkes processes.The prior distribution of the Hawkes processes is controlled via a Dirichletdistribution. We learn the model via a maximum likelihood estimator (MLE) andpropose an effective variational Bayesian inference algorithm. We specificallyanalyze the resulting EM-type algorithm in the context of inner-outeriterations and discuss several inner iteration allocation strategies. Theidentifiability of our model, the convergence of our learning method, and itssample complexity are analyzed in both theoretical and empirical ways, whichdemonstrate the superiority of our method to other competitors. The proposedmethod learns the number of clusters automatically and is robust to modelmisspecification. Experiments on both synthetic and real-world data show thatour method can learn diverse triggering patterns hidden in asynchronous eventsequences and achieve encouraging performance on clustering purity andconsistency.

Recurrent Poisson Factorization for Temporal Recommendation

  Poisson factorization is a probabilistic model of users and items forrecommendation systems, where the so-called implicit consumer data is modeledby a factorized Poisson distribution. There are many variants of Poissonfactorization methods who show state-of-the-art performance on real-worldrecommendation tasks. However, most of them do not explicitly take into accountthe temporal behavior and the recurrent activities of users which is essentialto recommend the right item to the right user at the right time. In this paper,we introduce Recurrent Poisson Factorization (RPF) framework that generalizesthe classical PF methods by utilizing a Poisson process for modeling theimplicit feedback. RPF treats time as a natural constituent of the model andbrings to the table a rich family of time-sensitive factorization models. Toelaborate, we instantiate several variants of RPF who are capable of handlingdynamic user preferences and item specification (DRPF), modeling thesocial-aspect of product adoption (SRPF), and capturing the consumptionheterogeneity among users and items (HRPF). We also develop a variationalalgorithm for approximate posterior inference that scales up to massive datasets. Furthermore, we demonstrate RPF's superior performance over manystate-of-the-art methods on synthetic dataset, and large scale real-worlddatasets on music streaming logs, and user-item interactions in M-Commerceplatforms.

Joint Modeling of Event Sequence and Time Series with Attentional Twin  Recurrent Neural Networks

  A variety of real-world processes (over networks) produce sequences of datawhose complex temporal dynamics need to be studied. More especially, the eventtimestamps can carry important information about the underlying networkdynamics, which otherwise are not available from the time-series evenly sampledfrom continuous signals. Moreover, in most complex processes, event sequencesand evenly-sampled times series data can interact with each other, whichrenders joint modeling of those two sources of data necessary. To tackle theabove problems, in this paper, we utilize the rich framework of (temporal)point processes to model event data and timely update its intensity function bythe synergic twin Recurrent Neural Networks (RNNs). In the proposedarchitecture, the intensity function is synergistically modulated by one RNNwith asynchronous events as input and another RNN with time series as input.Furthermore, to enhance the interpretability of the model, the attentionmechanism for the neural point process is introduced. The whole model withevent type and timestamp prediction output layers can be trained end-to-end andallows a black-box treatment for modeling the intensity. We substantiate thesuperiority of our model in synthetic data and three real-world benchmarkdatasets.

Modeling The Intensity Function Of Point Process Via Recurrent Neural  Networks

  Event sequence, asynchronously generated with random timestamp, is ubiquitousamong applications. The precise and arbitrary timestamp can carry importantclues about the underlying dynamics, and has lent the event data fundamentallydifferent from the time-series whereby series is indexed with fixed and equaltime interval. One expressive mathematical tool for modeling event is pointprocess. The intensity functions of many point processes involve twocomponents: the background and the effect by the history. Due to its inherentspontaneousness, the background can be treated as a time series while the otherneed to handle the history events. In this paper, we model the background by aRecurrent Neural Network (RNN) with its units aligned with time series indexeswhile the history effect is modeled by another RNN whose units are aligned withasynchronous events to capture the long-range dynamics. The whole model withevent type and timestamp prediction output layers can be trained end-to-end.Our approach takes an RNN perspective to point process, and models itsbackground and history effect. For utility, our method allows a black-boxtreatment for modeling the intensity which is often a pre-defined parametricform in point processes. Meanwhile end-to-end training opens the venue forreusing existing rich techniques in deep network for point process modeling. Weapply our model to the predictive maintenance problem using a log dataset bymore than 1000 ATMs from a global bank headquartered in North America.

Hawkes Processes for Invasive Species Modeling and Management

  The spread of invasive species to new areas threatens the stability ofecosystems and causes major economic losses in agriculture and forestry. Wepropose a novel approach to minimizing the spread of an invasive species givena limited intervention budget. We first model invasive species propagationusing Hawkes processes, and then derive closed-form expressions forcharacterizing the effect of an intervention action on the invasion process. Weuse this to obtain an optimal intervention plan based on an integer programmingformulation, and compare the optimal plan against severalecologically-motivated heuristic strategies used in practice. We present anempirical study of two variants of the invasive control problem: minimizing thefinal rate of invasions, and minimizing the number of invasions at the end of agiven time horizon. Our results show that the optimized intervention achievesnearly the same level of control that would be attained by completelyeradicating the species, with a 20% cost saving. Additionally, we design aheuristic intervention strategy based on a combination of the density and lifestage of the invasive individuals, and find that it comes surprisingly close tothe optimized strategy, suggesting that this could serve as a good rule ofthumb in invasive species management.

Visually Explainable Recommendation

  Images account for a significant part of user decisions in many applicationscenarios, such as product images in e-commerce, or user image posts in socialnetworks. It is intuitive that user preferences on the visual patterns of image(e.g., hue, texture, color, etc) can be highly personalized, and this providesus with highly discriminative features to make personalized recommendations.  Previous work that takes advantage of images for recommendation usuallytransforms the images into latent representation vectors, which are adopted bya recommendation component to assist personalized user/item profiling andrecommendation. However, such vectors are hardly useful in terms of providingvisual explanations to users about why a particular item is recommended, andthus weakens the explainability of recommendation systems.  As a step towards explainable recommendation models, we propose visuallyexplainable recommendation based on attentive neural networks to model the userattention on images, under the supervision of both implicit feedback andtextual reviews. By this, we can not only provide recommendation results to theusers, but also tell the users why an item is recommended by providingintuitive visual highlights in a personalized manner. Experimental results showthat our models are not only able to improve the recommendation performance,but also can provide persuasive visual explanations for the users to take therecommendations.

