Proceedings of the Eighteenth Conference on Uncertainty in Artificial  Intelligence (2002)

  This is the Proceedings of the Eighteenth Conference on Uncertainty inArtificial Intelligence, which was held in Alberta, Canada, August 1-4 2002

Human-Level Intelligence or Animal-Like Abilities?

  The vision systems of the eagle and the snake outperform everything that wecan make in the laboratory, but snakes and eagles cannot build an eyeglass or atelescope or a microscope. (Judea Pearl)

Objection-Based Causal Networks

  This paper introduces the notion of objection-based causal networks whichresemble probabilistic causal networks except that they are quantified usingobjections. An objection is a logical sentence and denotes a condition underwhich a, causal dependency does not exist. Objection-based causal networksenjoy almost all the properties that make probabilistic causal networkspopular, with the added advantage that objections are, arguably more intuitivethan probabilities.

A Standard Approach for Optimizing Belief Network Inference using Query  DAGs

  This paper proposes a novel, algorithm-independent approach to optimizingbelief network inference. rather than designing optimizations on an algorithmby algorithm basis, we argue that one should use an unoptimized algorithm togenerate a Q-DAG, a compiled graphical representation of the belief network,and then optimize the Q-DAG and its evaluator instead. We present a set ofQ-DAG optimizations that supplant optimizations designed for traditionalinference algorithms, including zero compression, network pruning and caching.We show that our Q-DAG optimizations require time linear in the Q-DAG size, andsignificantly simplify the process of designing algorithms for optimizingbelief network inference.

Conditioning Methods for Exact and Approximate Inference in Causal  Networks

  We present two algorithms for exact and approximate inference in causalnetworks. The first algorithm, dynamic conditioning, is a refinement of cutsetconditioning that has linear complexity on some networks for which cutsetconditioning is exponential. The second algorithm, B-conditioning, is analgorithm for approximate inference that allows one to trade-off the quality ofapproximations with the computation time. We also present some experimentalresults illustrating the properties of the proposed algorithms.

Argument Calculus and Networks

  A major reason behind the success of probability calculus is that itpossesses a number of valuable tools, which are based on the notion ofprobabilistic independence. In this paper, I identify a notion of logicalindependence that makes some of these tools available to a class ofpropositional databases, called argument databases. Specifically, I suggest agraphical representation of argument databases, called argument networks, whichresemble Bayesian networks. I also suggest an algorithm for reasoning withargument networks, which resembles a basic algorithm for reasoning withBayesian networks. Finally, I show that argument networks have severalapplications: Nonmonotonic reasoning, truth maintenance, and diagnosis.

EDML: A Method for Learning Parameters in Bayesian Networks

  We propose a method called EDML for learning MAP parameters in binaryBayesian networks under incomplete data. The method assumes Beta priors and canbe used to learn maximum likelihood parameters when the priors areuninformative. EDML exhibits interesting behaviors, especially when compared toEM. We introduce EDML, explain its origin, and study some of its propertiesboth analytically and empirically.

A Differential Approach to Inference in Bayesian Networks

  We present a new approach for inference in Bayesian networks, which is mainlybased on partial differentiation. According to this approach, one compiles aBayesian network into a multivariate polynomial and then computes the partialderivatives of this polynomial with respect to each variable. We show that oncesuch derivatives are made available, one can compute in constant-time answersto a large class of probabilistic queries, which are central to classicalinference, parameter estimation, model validation and sensitivity analysis. Wepresent a number of complexity results relating to the compilation of suchpolynomials and to the computation of their partial derivatives. We argue thatthe combined simplicity, comprehensiveness and computational complexity of thepresented framework is unique among existing frameworks for inference inBayesian networks.

Any-Space Probabilistic Inference

  We have recently introduced an any-space algorithm for exact inference inBayesian networks, called Recursive Conditioning, RC, which allows one to tradespace with time at increments of X-bytes, where X is the number of bytes neededto cache a floating point number. In this paper, we present three keyextensions of RC. First, we modify the algorithm so it applies to more generalfactorization of probability distributions, including (but not limited to)Bayesian network factorizations. Second, we present a forgetting mechanismwhich reduces the space requirements of RC considerably and then compare suchrequirmenets with those of variable elimination on a number of realisticnetworks, showing orders of magnitude improvements in certain cases. Third, wepresent a version of RC for computing maximum a posteriori hypotheses (MAP),which turns out to be the first MAP algorithm allowing a smooth time-spacetradeoff. A key advantage of presented MAP algorithm is that it does not haveto start from scratch each time a new query is presented, but can reuse some ofits computations across multiple queries, leading to significant savings inceratain cases.

Dynamic Jointrees

  It is well known that one can ignore parts of a belief network when computinganswers to certain probabilistic queries. It is also well known that theignorable parts (if any) depend on the specific query of interest and,therefore, may change as the query changes. Algorithms based on jointrees,however, do not seem to take computational advantage of these facts given thatthey typically construct jointrees for worst-case queries; that is, queries forwhich every part of the belief network is considered relevant. To address thislimitation, we propose in this paper a method for reconfiguring jointreesdynamically as the query changes. The reconfiguration process aims atmaintaining a jointree which corresponds to the underlying belief network afterit has been pruned given the current query. Our reconfiguration method ismarked by three characteristics: (a) it is based on a non-classical definitionof jointrees; (b) it is relatively efficient; and (c) it can reuse some of thecomputations performed before a jointree is reconfigured. We presentpreliminary experimental results which demonstrate significant savings overusing static jointrees when query changes are considerable.

Action Networks: A Framework for Reasoning about Actions and Change  under Uncertainty

  This work proposes action networks as a semantically well-founded frameworkfor reasoning about actions and change under uncertainty. Action networks addtwo primitives to probabilistic causal networks: controllable variables andpersistent variables. Controllable variables allow the representation ofactions as directly setting the value of specific events in the domain, subjectto preconditions. Persistent variables provide a canonical model of persistenceaccording to which both the state of a variable and the causal mechanismdictating its value persist over time unless intervened upon by an action (orits consequences). Action networks also allow different methods for quantifyingthe uncertainty in causal relationships, which go beyond traditionalprobabilistic quantification. This paper describes both recent results and workin progress.

On the Relation between Kappa Calculus and Probabilistic Reasoning

  We study the connection between kappa calculus and probabilistic reasoning indiagnosis applications. Specifically, we abstract a probabilistic beliefnetwork for diagnosing faults into a kappa network and compare the ordering offaults computed using both methods. We show that, at least for the exampleexamined, the ordering of faults coincide as long as all the causal relationsin the original probabilistic network are taken into account. We also provide aformal analysis of some network structures where the two methods will differ.Both kappa rankings and infinitesimal probabilities have been used extensivelyto study default reasoning and belief revision. But little has been done onutilizing their connection as outlined above. This is partly because therelation between kappa and probability calculi assumes that probabilities arearbitrarily close to one (or zero). The experiments in this paper investigatethis relation when this assumption is not satisfied. The reported results haveimportant implications on the use of kappa rankings to enhance the knowledgeengineering of uncertainty models.

Approximating the Partition Function by Deleting and then Correcting for  Model Edges

  We propose an approach for approximating the partition function which isbased on two steps: (1) computing the partition function of a simplified modelwhich is obtained by deleting model edges, and (2) rectifying the result byapplying an edge-by-edge correction. The approach leads to an intuitiveframework in which one can trade-off the quality of an approximation with thecomplexity of computing it. It also includes the Bethe free energyapproximation as a degenerate case. We develop the approach theoretically inthis paper and provide a number of empirical results that reveal its practicalutility.

On the Robustness of Most Probable Explanations

  In Bayesian networks, a Most Probable Explanation (MPE) is a completevariable instantiation with a highest probability given the current evidence.In this paper, we discuss the problem of finding robustness conditions of theMPE under single parameter changes. Specifically, we ask the question: How muchchange in a single network parameter can we afford to apply while keeping theMPE unchanged? We will describe a procedure, which is the first of its kind,that computes this answer for each parameter in the Bayesian network variablein time O(n exp(w)), where n is the number of network variables and w is itstreewidth.

On Bayesian Network Approximation by Edge Deletion

  We consider the problem of deleting edges from a Bayesian network for thepurpose of simplifying models in probabilistic inference. In particular, wepropose a new method for deleting network edges, which is based on the evidenceat hand. We provide some interesting bounds on the KL-divergence betweenoriginal and approximate networks, which highlight the impact of given evidenceon the quality of approximation and shed some light on good and bad candidatesfor edge deletion. We finally demonstrate empirically the promise of theproposed edge deletion technique as a basis for approximate inference.

Exploiting Evidence in Probabilistic Inference

  We define the notion of compiling a Bayesian network with evidence andprovide a specific approach for evidence-based compilation, which makes use oflogical processing. The approach is practical and advantageous in a number ofapplication areas-including maximum likelihood estimation, sensitivityanalysis, and MAP computations-and we provide specific empirical results in thedomain of genetic linkage analysis. We also show that the approach isapplicable for networks that do not contain determinism, and show that itempirically subsumes the performance of the quickscore algorithm when appliedto noisy-or networks.

Dual Decomposition from the Perspective of Relax, Compensate and then  Recover

  Relax, Compensate and then Recover (RCR) is a paradigm for approximateinference in probabilistic graphical models that has previously providedtheoretical and practical insights on iterative belief propagation and some ofits generalizations. In this paper, we characterize the technique of dualdecomposition in the terms of RCR, viewing it as a specific way to compensatefor relaxed equivalence constraints. Among other insights gathered from thisperspective, we propose novel heuristics for recovering relaxed equivalenceconstraints with the goal of incrementally tightening dual decompositionapproximations, all the way to reaching exact solutions. We also showempirically that recovering equivalence constraints can sometimes tighten thecorresponding approximation (and obtaining exact results), without increasingmuch the complexity of inference.

On the Relative Expressiveness of Bayesian and Neural Networks

  A neural network computes a function. A central property of neural networksis that they are "universal approximators:" for a given continuous function,there exists a neural network that can approximate it arbitrarily well, givenenough neurons (and some additional assumptions). In contrast, a Bayesiannetwork is a model, but each of its queries can be viewed as computing afunction. In this paper, we identify some key distinctions between thefunctions computed by neural networks and those by marginal Bayesian networkqueries, showing that the former are more expressive than the latter. Moreover,we propose a simple augmentation to Bayesian networks (a testing operator),which enables their marginal queries to become "universal approximators."

Query DAGs: A Practical Paradigm for Implementing Belief Network  Inference

  We describe a new paradigm for implementing inference in belief networks,which relies on compiling a belief network into an arithmetic expression calleda Query DAG (Q-DAG). Each non-leaf node of a Q-DAG represents a numericoperation, a number, or a symbol for evidence. Each leaf node of a Q-DAGrepresents the answer to a network query, that is, the probability of someevent of interest. It appears that Q-DAGs can be generated using any of thealgorithms for exact inference in belief networks --- we show how they can begenerated using clustering and conditioning algorithms. The time and spacecomplexity of a Q-DAG generation algorithm is no worse than the time complexityof the inference algorithm on which it is based; that of a Q-DAG on-lineevaluation algorithm is linear in the size of the Q-DAG, and such inferenceamounts to a standard evaluation of the arithmetic expression it represents.The main value of Q-DAGs is in reducing the software and hardware resourcesrequired to utilize belief networks in on-line, real-world applications. Theproposed framework also facilitates the development of on-line inference ondifferent software and hardware platforms, given the simplicity of the Q-DAGevaluation algorithm. This paper describes this new paradigm for probabilisticinference, explaining how it works, its uses, and outlines some of the researchdirections that it leads to.

On the tractable counting of theory models and its application to belief  revision and truth maintenance

  We introduced decomposable negation normal form (DNNF) recently as atractable form of propositional theories, and provided a number of powerfullogical operations that can be performed on it in polynomial time. We alsopresented an algorithm for compiling any conjunctive normal form (CNF) intoDNNF and provided a structure-based guarantee on its space and time complexity.We present in this paper a linear-time algorithm for converting an orderedbinary decision diagram (OBDD) representation of a propositional theory into anequivalent DNNF, showing that DNNFs scale as well as OBDDs. We also identify asubclass of DNNF which we call deterministic DNNF, d-DNNF, and show that theprevious complexity guarantees on compiling DNNF continue to hold for thisstricter subclass, which has stronger properties. In particular, we present anew operation on d-DNNF which allows us to count its models under theassertion, retraction and flipping of every literal by traversing the d-DNNFtwice. That is, after such traversal, we can test in constant-time: theentailment of any literal by the d-DNNF, and the consistency of the d-DNNFunder the retraction or flipping of any literal. We demonstrate thesignificance of these new operations by showing how they allow us to implementlinear-time, complete truth maintenance systems and linear-time, completebelief revision systems for two important classes of propositional theories.

Compilation of Propositional Weighted Bases

  In this paper, we investigate the extent to which knowledge compilation canbe used to improve inference from propositional weighted bases. We present ageneral notion of compilation of a weighted base that is parametrized by anyequivalence--preserving compilation function. Both negative and positiveresults are presented. On the one hand, complexity results are identified,showing that the inference problem from a compiled weighted base is asdifficult as in the general case, when the prime implicates, Horn cover orrenamable Horn cover classes are targeted. On the other hand, we show that theinference problem becomes tractable whenever DNNF-compilations are used andclausal queries are considered. Moreover, we show that the set of all preferredmodels of a DNNF-compilation of a weighted base can be computed in timepolynomial in the output size. Finally, we sketch how our results can be usedin model-based diagnosis in order to compute the most probable diagnoses of asystem.

Node Splitting: A Scheme for Generating Upper Bounds in Bayesian  Networks

  We formulate in this paper the mini-bucket algorithm for approximateinference in terms of exact inference on an approximate model produced bysplitting nodes in a Bayesian network. The new formulation leads to a number oftheoretical and practical implications. First, we show that branchand- boundsearch algorithms that use minibucket bounds may operate in a drasticallyreduced search space. Second, we show that the proposed formulation inspiresnew minibucket heuristics and allows us to analyze existing heuristics from anew perspective. Finally, we show that this new formulation allows mini-bucketapproximations to benefit from recent advances in exact inference, allowing oneto significantly increase the reach of these approximations.

A Variational Approach for Approximating Bayesian Networks by Edge  Deletion

  We consider in this paper the formulation of approximate inference inBayesian networks as a problem of exact inference on an approximate networkthat results from deleting edges (to reduce treewidth). We have shown inearlier work that deleting edges calls for introducing auxiliary networkparameters to compensate for lost dependencies, and proposed intuitiveconditions for determining these parameters. We have also shown that our methodcorresponds to IBP when enough edges are deleted to yield a polytree, andcorresponds to some generalizations of IBP when fewer edges are deleted. Inthis paper, we propose a different criteria for determining auxiliaryparameters based on optimizing the KL-divergence between the original andapproximate networks. We discuss the relationship between the two methods forselecting parameters, shedding new light on IBP and its generalizations. Wealso discuss the application of our new method to approximating inferenceproblems which are exponential in constrained treewidth, including MAP andnonmyopic value of information.

Sensitivity Analysis in Bayesian Networks: From Single to Multiple  Parameters

  Previous work on sensitivity analysis in Bayesian networks has focused onsingle parameters, where the goal is to understand the sensitivity of queriesto single parameter changes, and to identify single parameter changes thatwould enforce a certain query constraint. In this paper, we expand the work tomultiple parameters which may be in the CPT of a single variable, or the CPTsof multiple variables. Not only do we identify the solution space of multipleparameter changes that would be needed to enforce a query constraint, but wealso show how to find the optimal solution, that is, the one which disturbs thecurrent probability distribution the least (with respect to a specific measureof disturbance). We characterize the computational complexity of our newtechniques and discuss their applications to developing and debugging Bayesiannetworks, and to the problem of reasoning about the value (reliability) of newinformation.

Lifted Relax, Compensate and then Recover: From Approximate to Exact  Lifted Probabilistic Inference

  We propose an approach to lifted approximate inference for first-orderprobabilistic models, such as Markov logic networks. It is based on performingexact lifted inference in a simplified first-order model, which is found byrelaxing first-order constraints, and then compensating for the relaxation.These simplified models can be incrementally improved by carefully recoveringconstraints that have been relaxed, also at the first-order level. This leadsto a spectrum of approximations, with lifted belief propagation on one end, andexact lifted inference on the other. We discuss how relaxation, compensation,and recovery can be performed, all at the firstorder level, and showempirically that our approach substantially improves on the approximations ofboth propositional solvers and lifted belief propagation.

New Advances and Theoretical Insights into EDML

  EDML is a recently proposed algorithm for learning MAP parameters in Bayesiannetworks. In this paper, we present a number of new advances and insights onthe EDML algorithm. First, we provide the multivalued extension of EDML,originally proposed for Bayesian networks over binary variables. Next, weidentify a simplified characterization of EDML that further implies a simplefixed-point algorithm for the convex optimization problem that underlies it.This characterization further reveals a connection between EDML and EM: a fixedpoint of EDML is a fixed point of EM, and vice versa. We thus identify also anew characterization of EM fixed points, but in the semantics of EDML. Finally,we propose a hybrid EDML/EM algorithm that takes advantage of the improvedempirical convergence behavior of EDML, while maintaining the monotonicimprovement property of EM.

New Advances in Inference by Recursive Conditioning

  Recursive Conditioning (RC) was introduced recently as the first any-spacealgorithm for inference in Bayesian networks which can trade time for space byvarying the size of its cache at the increment needed to store a floating pointnumber. Under full caching, RC has an asymptotic time and space complexitywhich is comparable to mainstream algorithms based on variable elimination andclustering (exponential in the network treewidth and linear in its size). Weshow two main results about RC in this paper. First, we show that its actualspace requirements under full caching are much more modest than those needed bymainstream methods and study the implications of this finding. Second, we showthat RC can effectively deal with determinism in Bayesian networks by employingstandard logical techniques, such as unit resolution, allowing a significantreduction in its time requirements in certain cases. We illustrate our resultsusing a number of benchmark networks, including the very challenging ones thatarise in genetic linkage analysis.

Reasoning about Bayesian Network Classifiers

  Bayesian network classifiers are used in many fields, and one common class ofclassifiers are naive Bayes classifiers. In this paper, we introduce anapproach for reasoning about Bayesian network classifiers in which weexplicitly convert them into Ordered Decision Diagrams (ODDs), which are thenused to reason about the properties of these classifiers. Specifically, wepresent an algorithm for converting any naive Bayes classifier into an ODD, andwe show theoretically and experimentally that this algorithm can give us an ODDthat is tractable in size even given an intractable number of instances. SinceODDs are tractable representations of classifiers, our algorithm allows us toefficiently test the equivalence of two naive Bayes classifiers andcharacterize discrepancies between them. We also show a number of additionalresults including a count of distinct classifiers that can be induced bychanging some CPT in a naive Bayes classifier, and the range of allowablechanges to a CPT which keeps the current classifier unchanged.

Solving MAP Exactly using Systematic Search

  MAP is the problem of finding a most probable instantiation of a set ofvariables in a Bayesian network given some evidence. Unlike computing posteriorprobabilities, or MPE (a special case of MAP), the time and space complexity ofstructural solutions for MAP are not only exponential in the network treewidth,but in a larger parameter known as the "constrained" treewidth. In practice,this means that computing MAP can be orders of magnitude more expensive thancomputing posterior probabilities or MPE. This paper introduces a new, simpleupper bound on the probability of a MAP solution, which admits a tradeoffbetween the bound quality and the time needed to compute it. The bound is shownto be generally much tighter than those of other methods of comparablecomplexity. We use this proposed upper bound to develop a branch-and-boundsearch algorithm for solving MAP exactly. Experimental results demonstrate thatthe search algorithm is able to solve many problems that are far beyond thereach of any structure-based method for MAP. For example, we show that theproposed algorithm can compute MAP exactly and efficiently for some networkswhose constrained treewidth is more than 40.

Approximating MAP using Local Search

  MAP is the problem of finding a most probable instantiation of a set ofvariables in a Bayesian network, given evidence. Unlike computing marginals,posteriors, and MPE (a special case of MAP), the time and space complexity ofMAP is not only exponential in the network treewidth, but also in a largerparameter known as the "constrained" treewidth. In practice, this means thatcomputing MAP can be orders of magnitude more expensive thancomputingposteriors or MPE. Thus, practitioners generally avoid MAPcomputations, resorting instead to approximating them by the most likely valuefor each MAP variableseparately, or by MPE.We present a method forapproximating MAP using local search. This method has space complexity which isexponential onlyin the treewidth, as is the complexity of each search step. Weinvestigate the effectiveness of different local searchmethods and severalinitialization strategies and compare them to otherapproximationschemes.Experimental results show that local search provides a much moreaccurate approximation of MAP, while requiring few search steps.Practically,this means that the complexity of local search is often exponential only intreewidth as opposed to the constrained treewidth, making approximating MAP asefficient as other computations.

On the Complexity and Approximation of Binary Evidence in Lifted  Inference

  Lifted inference algorithms exploit symmetries in probabilistic models tospeed up inference. They show impressive performance when calculatingunconditional probabilities in relational models, but often resort tonon-lifted inference when computing conditional probabilities. The reason isthat conditioning on evidence breaks many of the model's symmetries, which canpreempt standard lifting techniques. Recent theoretical results show, forexample, that conditioning on evidence which corresponds to binary relations is#P-hard, suggesting that no lifting is to be expected in the worst case. Inthis paper, we balance this negative result by identifying the Boolean rank ofthe evidence as a key parameter for characterizing the complexity ofconditioning in lifted inference. In particular, we show that conditioning onbinary evidence with bounded Boolean rank is efficient. This opens up thepossibility of approximating evidence by a low-rank Boolean matrixfactorization, which we investigate both theoretically and empirically.

Skolemization for Weighted First-Order Model Counting

  First-order model counting emerged recently as a novel reasoning task, at thecore of efficient algorithms for probabilistic logics. We present aSkolemization algorithm for model counting problems that eliminates existentialquantifiers from a first-order logic theory without changing its weighted modelcount. For certain subsets of first-order logic, lifted model counters wereshown to run in time polynomial in the number of objects in the domain ofdiscourse, where propositional model counters require exponential time.However, these guarantees apply only to Skolem normal form theories (i.e., noexistential quantifiers) as the presence of existential quantifiers reduceslifted model counters to propositional ones. Since textbook Skolemization isnot sound for model counting, these restrictions precluded efficient modelcounting for directed models, such as probabilistic logic programs, which relyon existential quantification. Our Skolemization procedure extends theapplicability of first-order model counters to these representations. Moreover,it simplifies the design of lifted model counting algorithms.

On the Role of Canonicity in Bottom-up Knowledge Compilation

  We consider the problem of bottom-up compilation of knowledge bases, which isusually predicated on the existence of a polytime function for combiningcompilations using Boolean operators (usually called an Apply function). Whilesuch a polytime Apply function is known to exist for certain languages (e.g.,OBDDs) and not exist for others (e.g., DNNF), its existence for certainlanguages remains unknown. Among the latter is the recently introduced languageof Sentential Decision Diagrams (SDDs), for which a polytime Apply functionexists for unreduced SDDs, but remains unknown for reduced ones (i.e. canonicalSDDs). We resolve this open question in this paper and consider some of itstheoretical and practical implications. Some of the findings we report questionthe common wisdom on the relationship between bottom-up compilation, languagecanonicity and the complexity of the Apply function.

When do Numbers Really Matter?

  Common wisdom has it that small distinctions in the probabilities quantifyinga Bayesian network do not matter much for the resultsof probabilistic queries.However, one can easily develop realistic scenarios under which smallvariations in network probabilities can lead to significant changes in computedqueries. A pending theoretical question is then to analytically characterizeparameter changes that do or do not matter. In this paper, we study thesensitivity of probabilistic queries to changes in network parameters and provesome tight bounds on the impact that such parameters can have on queries. Ouranalytical results pinpoint some interesting situations under which parameterchanges do or do not matter. These results are important for knowledgeengineers as they help them identify influential network parameters. They arealso important for approximate inference algorithms that preprocessnetwork CPTsto eliminate small distinctions in probabilities.

On Relaxing Determinism in Arithmetic Circuits

  The past decade has seen a significant interest in learning tractableprobabilistic representations. Arithmetic circuits (ACs) were among the firstproposed tractable representations, with some subsequent representations beinginstances of ACs with weaker or stronger properties. In this paper, we providea formal basis under which variants on ACs can be compared, and where theprecise roles and semantics of their various properties can be made moretransparent. This allows us to place some recent developments on ACs in aclearer perspective and to also derive new results for ACs. This includes anexponential separation between ACs with and without determinism; completenessand incompleteness results; and tractability results (or lack thereof) whencomputing most probable explanations (MPEs).

On Compiling DNNFs without Determinism

  State-of-the-art knowledge compilers generate deterministic subsets of DNNF,which have been recently shown to be exponentially less succinct than DNNF. Inthis paper, we propose a new method to compile DNNFs without enforcingdeterminism necessarily. Our approach is based on compiling deterministic DNNFswith the addition of auxiliary variables to the input formula. These variablesare then existentially quantified from the deterministic structure in lineartime, which would lead to a DNNF that is equivalent to the input formula andnot necessarily deterministic. On the theoretical side, we show that the newmethod could generate exponentially smaller DNNFs than deterministic ones, evenby adding a single auxiliary variable. Further, we show that various existingtechniques that introduce auxiliary variables to the input formulas can beemployed in our framework. On the practical side, we empirically demonstratethat our new method can significantly advance DNNF compilation on certainbenchmarks.

A Symbolic Approach to Explaining Bayesian Network Classifiers

  We propose an approach for explaining Bayesian network classifiers, which isbased on compiling such classifiers into decision functions that have atractable and symbolic form. We introduce two types of explanations for why aclassifier may have classified an instance positively or negatively and suggestalgorithms for computing these explanations. The first type of explanationidentifies a minimal set of the currently active features that is responsiblefor the current classification, while the second type of explanation identifiesa minimal set of features whose current state (active or not) is sufficient forthe classification. We consider in particular the compilation of Naive andLatent-Tree Bayesian network classifiers into Ordered Decision Diagrams (ODDs),providing a context for evaluating our proposal using case studies andexperiments based on classifiers from the literature.

