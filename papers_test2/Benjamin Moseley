The Complexity of Scheduling for p-norms of Flow and Stretch

  We consider computing optimal k-norm preemptive schedules of jobs that arriveover time. In particular, we show that computing the optimal k-norm of flowschedule, is strongly NP-hard for k in (0, 1) and integers k in (1, infinity).Further we show that computing the optimal k-norm of stretch schedule, isstrongly NP-hard for k in (0, 1) and integers k in (1, infinity).

Scalable K-Means++

  Over half a century old and showing no signs of aging, k-means remains one ofthe most popular data processing algorithms. As is well-known, a properinitialization of k-means is crucial for obtaining a good final solution. Therecently proposed k-means++ initialization algorithm achieves this, obtainingan initial set of centers that is provably close to the optimum solution. Amajor downside of the k-means++ is its inherent sequential nature, which limitsits applicability to massive data: one must make k passes over the data to finda good initial set of centers. In this work we show how to drastically reducethe number of passes needed to obtain, in parallel, a good initialization. Thisis unlike prevailing efforts on parallelizing k-means that have mostly focusedon the post-initialization phases of k-means. We prove that our proposedinitialization algorithm k-means|| obtains a nearly optimal solution after alogarithmic number of passes, and then show that in practice a constant numberof passes suffices. Experimental evaluation on real-world large-scale datademonstrates that k-means|| outperforms k-means++ in both sequential andparallel settings.

An O(log log m)-competitive Algorithm for Online Machine Minimization

  This paper considers the online machine minimization problem, a basic realtime scheduling problem. The setting for this problem consists of n jobs thatarrive over time, where each job has a deadline by which it must be completed.The goal is to design an online scheduler that feasibly schedules the jobs on anearly minimal number of machines. An algorithm is c-machine optimal if thealgorithm will feasibly schedule a collection of jobs on cm machines if thereexists a feasible schedule on m machines. For over two decades the best knownresult was a O(log P)-machine optimal algorithm, where P is the ratio of themaximum to minimum job size. In a recent breakthrough, a O(log m)-machineoptimal algorithm was given. In this paper, we exponentially improve on thisrecent result by giving a O(log log m)-machine optimal algorithm.

Fast approximate simulation of seismic waves with deep learning

  We simulate the response of acoustic seismic waves in horizontally layeredmedia using a deep neural network. In contrast to traditional finite-differencemodelling techniques our network is able to directly approximate the recordedseismic response at multiple receiver locations in a single inference step,without needing to iteratively model the seismic wavefield through time. Thisresults in an order of magnitude reduction in simulation time from the order of1 s for FD modelling to the order of 0.1 s using our approach. Such a speedimprovement could lead to real-time seismic simulation applications and benefitseismic inversion algorithms based on forward modelling, such as full waveforminversion. Our proof of concept deep neural network is trained using 50,000synthetic examples of seismic waves propagating through different 2Dhorizontally layered velocity models. We discuss how our approach could beextended to arbitrary velocity models. Our deep neural network design isinspired by the WaveNet architecture used for speech synthesis. We alsoinvestigate using deep neural networks for simulating the full seismicwavefield and for carrying out seismic inversion directly.

Online Scheduling to Minimize the Maximum Delay Factor

  In this paper two scheduling models are addressed. First is the standardmodel (unicast) where requests (or jobs) are independent. The other is thebroadcast model where broadcasting a page can satisfy multiple outstandingrequests for that page. We consider online scheduling of requests when theyhave deadlines. Unlike previous models, which mainly consider the objective ofmaximizing throughput while respecting deadlines, here we focus on schedulingall the given requests with the goal of minimizing the maximum {\em delayfactor}.We prove strong lower bounds on the achievable competitive ratios fordelay factor scheduling even with unit-time requests.For the unicast model wegive algorithms that are $(1 + \eps)$-speed $O({1 \over \eps})$-competitive inboth the single machine and multiple machine settings. In the broadcast modelwe give an algorithm for similar-sized pages that is $(2+ \eps)$-speed $O({1\over \eps^2})$-competitive. For arbitrary page sizes we give an algorithm thatis $(4+\eps)$-speed $O({1 \over \eps^2})$-competitive.

Longest Wait First for Broadcast Scheduling

  We consider online algorithms for broadcast scheduling. In the pull-basedbroadcast model there are $n$ unit-sized pages of information at a server andrequests arrive online for pages. When the server transmits a page $p$, alloutstanding requests for that page are satisfied. The longest-wait-first} (LWF)algorithm is a natural algorithm that has been shown to have good empiricalperformance. In this paper we make two main contributions to the analysis ofLWF and broadcast scheduling. \begin{itemize} \item We give an intuitive andeasy to understand analysis of LWF which shows that it is$O(1/\eps^2)$-competitive for average flow-time with $(4+\eps)$ speed. Using amore involved analysis, we show that LWF is $O(1/\eps^3)$-competitive foraverage flow-time with $(3.4+\epsilon)$ speed. \item We show that a naturalextension of LWF is O(1)-speed O(1)-competitive for more general objectivefunctions such as average delay-factor and $L_k$ norms of delay-factor (forfixed $k$). \end{itemize}

Scheduling to Minimize Energy and Flow Time in Broadcast Scheduling

  In this paper we initiate the study of minimizing power consumption in thebroadcast scheduling model. In this setting there is a wireless transmitter.Over time requests arrive at the transmitter for pages of information. Multiplerequests may be for the same page. When a page is transmitted, all requests forthat page receive the transmission simulteneously. The speed the transmittersends data at can be dynamically scaled to conserve energy. We consider theproblem of minimizing flow time plus energy, the most popular scheduling metricconsidered in the standard scheduling model when the scheduler is energy aware.We will assume that the power consumed is modeled by an arbitrary convexfunction. For this problem there is a $\Omega(n)$ lower bound. Due to the lowerbound, we consider the resource augmentation model of Gupta \etal\cite{GuptaKP10}. Using resource augmentation, we give a scalable algorithm.Our result also gives a scalable non-clairvoyant algorithm for minimizingweighted flow time plus energy in the standard scheduling model.

Fast Clustering using MapReduce

  Clustering problems have numerous applications and are becoming morechallenging as the size of the data increases. In this paper, we considerdesigning clustering algorithms that can be used in MapReduce, the most popularprogramming environment for processing large datasets. We focus on thepractical and popular clustering problems, $k$-center and $k$-median. Wedevelop fast clustering algorithms with constant factor approximationguarantees. From a theoretical perspective, we give the first analysis thatshows several clustering algorithms are in $\mathcal{MRC}^0$, a theoreticalMapReduce class introduced by Karloff et al. \cite{KarloffSV10}. Our algorithmsuse sampling to decrease the data size and they run a time consuming clusteringalgorithm such as local search or Lloyd's algorithm on the resulting data set.Our algorithms have sufficient flexibility to be used in practice since theyrun in a constant number of MapReduce rounds. We complement these results byperforming experiments using our algorithms. We compare the empiricalperformance of our algorithms to several sequential and parallel algorithms forthe $k$-median problem. The experiments show that our algorithms' solutions aresimilar to or better than the other algorithms' solutions. Furthermore, on datasets that are sufficiently large, our algorithms are faster than the otherparallel algorithms that we tested.

Quantum Secrecy in Thermal States

  We propose to perform quantum key distribution using quantum correlationsoccurring within thermal states produced by low power sources such as LED's.These correlations are exploited through the Hanbury Brown and Twiss effect. Webuild an optical central broadcast protocol using a superluminescent diodewhich allows switching between laser and thermal regimes, enabling us toprovide experimental key rates in both regimes. We provide a theoreticalanalysis and show that quantum secrecy is possible, even in high noisesituations.

Bargaining for Revenue Shares on Tree Trading Networks

  We study trade networks with a tree structure, where a seller with a singleindivisible good is connected to buyers, each with some value for the good, viaa unique path of intermediaries. Agents in the tree make multiplicative revenueshare offers to their parent nodes, who choose the best offer and offer part ofit to their parent, and so on; the winning path is determined by who finallymakes the highest offer to the seller. In this paper, we investigate how theserevenue shares might be set via a natural bargaining process between agents onthe tree, specifically, egalitarian bargaining between endpoints of each edgein the tree. We investigate the fixed point of this system of bargainingequations and prove various desirable for this solution concept, including (i)existence, (ii) uniqueness, (iii) efficiency, (iv) membership in the core, (v)strict monotonicity, (vi) polynomial-time computability to any given accuracy.Finally, we present numerical evidence that asynchronous dynamics with randomlyordered updates always converges to the fixed point, indicating that the fixedpoint shares might arise from decentralized bargaining amongst agents on thetrade network.

Backprop with Approximate Activations for Memory-efficient Network  Training

  Larger and deeper neural network architectures deliver improved accuracy on avariety of tasks, but also require a large amount of memory for training tostore intermediate activations for back-propagation. We introduce anapproximation strategy to significantly reduce this memory footprint, withminimal effect on training performance and negligible computational cost. Ourmethod replaces intermediate activations with lower-precision approximations tofree up memory, after the full-precision versions have been used forcomputation in subsequent layers in the forward pass. Only these approximateactivations are retained for use in the backward pass. Compared to naivelow-precision computation, our approach limits the accumulation of errorsacross layers and allows the use of much lower-precision approximations withoutaffecting training accuracy. Experiments on CIFAR and ImageNet show that ourmethod yields performance comparable to full-precision training, while storingactivations at a fraction of the memory cost with 8- and even 4-bit fixed-pointprecision.

Online Scheduling on Identical Machines using SRPT

  Due to its optimality on a single machine for the problem of minimizingaverage flow time, Shortest-Remaining-Processing-Time (\srpt) appears to be themost natural algorithm to consider for the problem of minimizing average flowtime on multiple identical machines. It is known that $\srpt$ achieves the bestpossible competitive ratio on multiple machines up to a constant factor. Usingresource augmentation, $\srpt$ is known to achieve total flow time at most thatof the optimal solution when given machines of speed $2- \frac{1}{m}$. Further,it is known that $\srpt$'s competitive ratio improves as the speed increases;$\srpt$ is $s$-speed $\frac{1}{s}$-competitive when $s \geq 2- \frac{1}{m}$.  However, a gap has persisted in our understanding of $\srpt$. Before thiswork, the performance of $\srpt$ was not known when $\srpt$ is given$(1+\eps)$-speed when $0 < \eps < 1-\frac{1}{m}$, even though it has beenthought that $\srpt$ is $(1+\eps)$-speed $O(1)$-competitive for over a decade.Resolving this question was suggested in Open Problem 2.9 from the survey"Online Scheduling" by Pruhs, Sgall, and Torng \cite{PruhsST}, and we answerthe question in this paper. We show that $\srpt$ is \emph{scalable} on $m$identical machines. That is, we show $\srpt$ is $(1+\eps)$-speed$O(\frac{1}{\eps})$-competitive for $\eps >0$. We complement this by showingthat $\srpt$ is $(1+\eps)$-speed $O(\frac{1}{\eps^2})$-competitive for theobjective of minimizing the $\ell_k$-norms of flow time on $m$ identicalmachines. Both of our results rely on new potential functions that capture thestructure of \srpt. Our results, combined with previous work, show that $\srpt$is the best possible online algorithm in essentially every aspect whenmigration is permissible.

Minimizing Maximum Response Time and Delay Factor in Broadcast  Scheduling

  We consider online algorithms for pull-based broadcast scheduling. In thissetting there are n pages of information at a server and requests for pagesarrive online. When the server serves (broadcasts) a page p, all outstandingrequests for that page are satisfied. We study two related metrics, namelymaximum response time (waiting time) and maximum delay-factor and theirweighted versions. We obtain the following results in the worst-case onlinecompetitive model.  - We show that FIFO (first-in first-out) is 2-competitive even when the pagesizes are different. Previously this was known only for unit-sized pages [10]via a delicate argument. Our proof differs from [10] and is perhaps moreintuitive.  - We give an online algorithm for maximum delay-factor that isO(1/eps^2)-competitive with (1+\eps)-speed for unit-sized pages and with(2+\eps)-speed for different sized pages. This improves on the algorithm in[12] which required (2+\eps)-speed and (4+\eps)-speed respectively. In additionwe show that the algorithm and analysis can be extended to obtain the sameresults for maximum weighted response time and delay factor.  - We show that a natural greedy algorithm modeled after LWF(Longest-Wait-First) is not O(1)-competitive for maximum delay factor with anyconstant speed even in the setting of standard scheduling with unit-sized jobs.This complements our upper bound and demonstrates the importance of thetradeoff made in our algorithm.

Greed Works - Online Algorithms For Unrelated Machine Stochastic  Scheduling

  This paper establishes the first performance guarantees for a combinatorialonline algorithm that schedules stochastic, nonpreemptive jobs on unrelatedmachines to minimize the expected total weighted completion time. Prior work onunrelated machine scheduling with stochastic jobs was restricted to the offlinecase, and required sophisticated linear or convex programming relaxations forthe assignment of jobs to machines. The algorithm introduced in this paper isbased on a purely combinatorial assignment of jobs to machines, hence it alsoworks online. The performance bounds are of the same order of magnitude asthose of earlier work, and depend linearly on an upper bound $\Delta$ on thesquared coefficient of variation of the jobs' processing times. They are$4+2\Delta$ when there are no release dates, and $12+6\Delta$ when jobs arereleased over time. For the special case of deterministic processing times,without and with release times, this paper shows that the same combinatorialgreedy algorithm has a competitive ratio of 4 and 6, respectively. As to thetechnical contribution, the paper shows for the first time how dual fittingtechniques can be used for stochastic and nonpreemptive scheduling problems.

Online Non-preemptive Scheduling on Unrelated Machines with Rejections

  When a computer system schedules jobs there is typically a significant costassociated with preempting a job during execution. This cost can be from theexpensive task of saving the memory's state and loading data into and out ofmemory. It is desirable to schedule jobs non-preemptively to avoid the costs ofpreemption. There is a need for non-preemptive system schedulers on desktops,servers and data centers. Despite this need, there is a gap between theory andpractice. Indeed, few non-preemptive \emph{online} schedulers are known to havestrong foundational guarantees. This gap is likely due to strong lower boundson any online algorithm for popular objectives. Indeed, typical worst caseanalysis approaches, and even resource augmented approaches such as speedaugmentation, result in all algorithms having poor performance guarantees. Thispaper considers on-line non-preemptive scheduling problems in the worst-caserejection model where the algorithm is allowed to reject a small fraction ofjobs. By rejecting only a few jobs, this paper shows that the strong lowerbounds can be circumvented. This approach can be used to discover algorithmicscheduling policies with desirable worst-case guarantees. Specifically, thepaper presents algorithms for the following two objectives: minimizing thetotal flow-time and minimizing the total weighted flow-time plus energy underthe speed-scaling mechanism. The algorithms have a small constant competitiveratio while rejecting only a constant fraction of jobs. Beyond specificresults, the paper asserts that alternative models beyond speed augmentationshould be explored to aid in the discovery of good schedulers in the face ofthe requirement of being online and non-preemptive.

Online Non-Preemptive Scheduling to Minimize Weighted Flow-time on  Unrelated Machines

  In this paper, we consider the online problem of scheduling independent jobs\emph{non-preemptively} so as to minimize the weighted flow-time on a set ofunrelated machines. There has been a considerable amount of work on thisproblem in the preemptive setting where several competitive algorithms areknown in the classical competitive model. %Using the speed augmentation model,Anand et al. showed that the greedy algorithm is$O\left(\frac{1}{\epsilon}\right)$-competitive in the preemptive setting. Inthe non-preemptive setting, Lucarelli et al. showed that there exists a stronglower bound for minimizing weighted flow-time even on a single machine.However, the problem in the non-preemptive setting admits a strong lower bound.Recently, Lucarelli et al. presented an algorithm that achieves a$O\left(\frac{1}{\epsilon^2}\right)$-competitive ratio when the algorithm isallowed to reject $\epsilon$-fraction of total weight of jobs and$\epsilon$-speed augmentation. They further showed that speed augmentationalone is insufficient to derive any competitive algorithm. An intriguing openquestion is whether there exists a scalable competitive algorithm that rejectsa small fraction of total weights.  In this paper, we affirmatively answer this question. Specifically, we showthat there exists a $O\left(\frac{1}{\epsilon^3}\right)$-competitive algorithmfor minimizing weighted flow-time on a set of unrelated machine that rejects atmost $O(\epsilon)$-fraction of total weight of jobs. The design and analysis ofthe algorithm is based on the primal-dual technique. Our result asserts thatalternative models beyond speed augmentation should be explored when designingonline schedulers in the non-preemptive setting in an effort to find provablygood algorithms.

Pre-Synaptic Pool Modification (PSPM): A Supervised Learning Procedure  for Spiking Neural Networks

  A central question in neuroscience is how to develop realistic models thatpredict output firing behavior based on provided external stimulus. Given a setof external inputs and a set of output spike trains, the objective is todiscover a network structure which can accomplish the transformation asaccurately as possible. Due to the difficulty of this problem in its mostgeneral form, approximations have been made in previous work. Pastapproximations have sacrificed network size, recurrence, allowed spiked count,or have imposed layered network structure. Here we present a learning rulewithout these sacrifices, which produces a weight matrix of a leakyintegrate-and-fire (LIF) network to match the output activity of bothdeterministic LIF networks as well as probabilistic integrate-and-fire (PIF)networks. Inspired by synaptic scaling, our pre-synaptic pool modification(PSPM) algorithm outputs deterministic, fully recurrent spiking neural networksthat can provide a novel generative model for given spike trains. Similarity inoutput spike trains is evaluated with a variety of metrics including avan-Rossum like measure and a numerical comparison of inter-spike intervaldistributions. Application of our algorithm to randomly generated networksimproves similarity to the reference spike trains on both of these statedmeasures. In addition, we generated LIF networks that operate near criticalitywhen trained on critical PIF outputs. Our results establish that learning rulesbased on synaptic homeostasis can be used to represent input-outputrelationships in fully recurrent spiking neural networks.

Efficient nonmyopic active search with applications in drug and  materials discovery

  Active search is a learning paradigm for actively identifying as many membersof a given class as possible. A critical target scenario is high-throughputscreening for scientific discovery, such as drug or materials discovery. Inthis paper, we approach this problem in Bayesian decision framework. We firstderive the Bayesian optimal policy under a natural utility, and establish atheoretical hardness of active search, proving that the optimal policy can notbe approximated for any constant ratio. We also study the batch setting for thefirst time, where a batch of $b>1$ points can be queried at each iteration. Wegive an asymptotic lower bound, linear in batch size, on the adaptivity gap:how much we could lose if we query $b$ points at a time for $t$ iterations,instead of one point at a time for $bt$ iterations. We then introduce a novelapproach to nonmyopic approximations of the optimal policy that admitsefficient computation. Our proposed policy can automatically trade offexploration and exploitation, without relying on any tuning parameters. We alsogeneralize our policy to batch setting, and propose two approaches to tacklethe combinatorial search challenge. We evaluate our proposed policies on alarge database of drug discovery and materials science. Results demonstrate thesuperior performance of our proposed policy in both sequential and batchsetting; the nonmyopic behavior is also illustrated in various aspects.

On Functional Aggregate Queries with Additive Inequalities

  Motivated by fundamental applications in databases and relational machinelearning, we formulate and study the problem of answering functional aggregatequeries (FAQ) in which some of the input factors are defined by a collection ofadditive inequalities between variables. We refer to these queries as FAQ-AIfor short.  To answer FAQ-AI in the Boolean semiring, we define relaxed treedecompositions and relaxed submodular and fractional hypertree widthparameters. We show that an extension of the InsideOut algorithm usingChazelle's geometric data structure for solving the semigroup range searchproblem can answer Boolean FAQ-AI in time given by these new width parameters.This new algorithm achieves lower complexity than known solutions for FAQ-AI.It also recovers some known results in database query answering.  Our second contribution is a relaxation of the set of polymatroids that givesrise to the counting version of the submodular width, denoted by #subw. Thisnew width is sandwiched between the submodular and the fractional hypertreewidths. Any FAQ and FAQ-AI over one semiring can be answered in timeproportional to #subw and respectively to the relaxed version of #subw.  We present three applications of our FAQ-AI framework to relational machinelearning: k-means clustering, training linear support vector machines, andtraining models using non-polynomial loss. These optimization problems can besolved over a database asymptotically faster than computing the join of thedatabase relations.

The Atacama Cosmology Telescope: Cosmological parameters from three  seasons of data

  We present constraints on cosmological and astrophysical parameters fromhigh-resolution microwave background maps at 148 GHz and 218 GHz made by theAtacama Cosmology Telescope (ACT) in three seasons of observations from 2008 to2010. A model of primary cosmological and secondary foreground parameters isfit to the map power spectra and lensing deflection power spectrum, includingcontributions from both the thermal Sunyaev-Zeldovich (tSZ) effect and thekinematic Sunyaev-Zeldovich (kSZ) effect, Poisson and correlated anisotropyfrom unresolved infrared sources, radio sources, and the correlation betweenthe tSZ effect and infrared sources. The power ell^2 C_ell/2pi of the thermalSZ power spectrum at 148 GHz is measured to be 3.4 +\- 1.4 muK^2 at ell=3000,while the corresponding amplitude of the kinematic SZ power spectrum has a 95%confidence level upper limit of 8.6 muK^2. Combining ACT power spectra with theWMAP 7-year temperature and polarization power spectra, we find excellentconsistency with the LCDM model. We constrain the number of effectiverelativistic degrees of freedom in the early universe to be Neff=2.79 +\- 0.56,in agreement with the canonical value of Neff=3.046 for three masslessneutrinos. We constrain the sum of the neutrino masses to be Sigma m_nu < 0.39eV at 95% confidence when combining ACT and WMAP 7-year data with BAO andHubble constant measurements. We constrain the amount of primordial helium tobe Yp = 0.225 +\- 0.034, and measure no variation in the fine structureconstant alpha since recombination, with alpha/alpha0 = 1.004 +/- 0.005. Wealso find no evidence for any running of the scalar spectral index, dns/dlnk =-0.004 +\- 0.012.

