Conducting Truthful Surveys, Cheaply

  We consider the problem of conducting a survey with the goal of obtaining anunbiased estimator of some population statistic when individuals have unknowncosts (drawn from a known prior) for participating in the survey. Individualsmust be compensated for their participation and are strategic agents, and sothe payment scheme must incentivize truthful behavior. We derive optimaltruthful mechanisms for this problem for the two goals of minimizing thevariance of the estimator given a fixed budget, and minimizing the expectedcost of the survey given a fixed variance goal.

Potential Networks, Contagious Communities, and Understanding Social  Network Structure

  In this paper we study how the network of agents adopting a particulartechnology relates to the structure of the underlying network over which thetechnology adoption spreads. We develop a model and show that the network ofagents adopting a particular technology may have characteristics that differsignificantly from the social network of agents over which the technologyspreads. For example, the network induced by a cascade may have a heavy-taileddegree distribution even if the original network does not.  This provides evidence that online social networks created by technologyadoption over an underlying social network may look fundamentally differentfrom social networks and indicates that using data from many online socialnetworks may mislead us if we try to use it to directly infer the structure ofsocial networks. Our results provide an alternate explanation for certainproperties repeatedly observed in data sets, for example: heavy-tailed degreedistribution, network densification, shrinking diameter, and network communityprofile. These properties could be caused by a sort of `sampling bias' ratherthan by attributes of the underlying social structure. By generating networksusing cascades over traditional network models that do not themselves containthese properties, we can nevertheless reliably produce networks that containall these properties.  An opportunity for interesting future research is developing new methods thatcorrectly infer underlying network structure from data about a network that isgenerated via a cascade spread over the underlying network.

Finding Overlapping Communities in Social Networks: Toward a Rigorous  Approach

  A "community" in a social network is usually understood to be a group ofnodes more densely connected with each other than with the rest of the network.This is an important concept in most domains where networks arise: social,technological, biological, etc. For many years algorithms for findingcommunities implicitly assumed communities are nonoverlapping (leading to useof clustering-based approaches) but there is increasing interest in findingoverlapping communities. A barrier to finding communities is that the solutionconcept is often defined in terms of an NP-complete problem such as Clique orHierarchical Clustering.  This paper seeks to initiate a rigorous approach to the problem of findingoverlapping communities, where "rigorous" means that we clearly state thefollowing: (a) the object sought by our algorithm (b) the assumptions about theunderlying network (c) the (worst-case) running time.  Our assumptions about the network lie between worst-case and average-case. Anaverage case analysis would require a precise probabilistic model of thenetwork, on which there is currently no consensus. However, some plausibleassumptions about network parameters can be gleaned from a long body of work inthe sociology community spanning five decades focusing on the study ofindividual communities and ego-centric networks. Thus our assumptions aresomewhat "local" in nature. Nevertheless they suffice to permit a rigorousanalysis of running time of algorithms that recover global structure.  Our algorithms use random sampling similar to that in property testing andalgorithms for dense graphs. However, our networks are not necessarily densegraphs, not even in local neighborhoods.  Our algorithms explore a local-global relationship between ego-centric andsocio-centric networks that we hope will provide a fruitful framework forfuture work both in computer science and sociology.

Graph Isomorphism and the Lasserre Hierarchy

  In this paper we show lower bounds for a certain large class of algorithmssolving the Graph Isomorphism problem, even on expander graph instances.Spielman [25] shows an algorithm for isomorphism of strongly regular expandergraphs that runs in time exp(O(n^(1/3)) (this bound was recently improved toexpf O(n^(1/5) [5]). It has since been an open question to remove therequirement that the graph be strongly regular. Recent algorithmic results showthat for many problems the Lasserre hierarchy works surprisingly well when theunderlying graph has expansion properties. Moreover, recent work of Atseriasand Maneva [3] shows that k rounds of the Lasserre hierarchy is ageneralization of the k-dimensional Weisfeiler-Lehman algorithm for GraphIsomorphism. These two facts combined make the Lasserre hierarchy a goodcandidate for solving graph isomorphism on expander graphs. Our main resultrules out this promising direction by showing that even Omega(n) rounds of theLasserre semidefinite program hierarchy fail to solve the Graph Isomorphismproblem even on expander graphs.

A Descending Price Auction for Matching Markets

  This work presents a descending-price-auction algorithm to obtain the maximummarket-clearing price vector (MCP) in unit-demand matching markets with m itemsby exploiting the combinatorial structure. With a shrewd choice of goods forwhich the prices are reduced in each step, the algorithm only uses thecombinatorial structure, which avoids solving LPs and enjoys a stronglypolynomial runtime of $O(m^4)$. Critical to the algorithm is determining theset of under-demanded goods for which we reduce the prices simultaneously ineach step of the algorithm. This we accomplish by choosing the subset of goodsthat maximize a skewness function, which makes the bipartite graph seriesconverges to the combinatorial structure at the maximum MCP in $O(m^2)$ steps.A graph coloring algorithm is proposed to find the set of goods with themaximal skewness value that yields $O(m^4)$ complexity.

Eliciting Expertise without Verification

  A central question of crowd-sourcing is how to elicit expertise from agents.This is even more difficult when answers cannot be directly verified. A keychallenge is that sophisticated agents may strategically withhold effort orinformation when they believe their payoff will be based upon comparison withother agents whose reports will likely omit this information due to lack ofeffort or expertise.  Our work defines a natural model for this setting based on the assumptionthat \emph{more sophisticated agents know the beliefs of less sophisticatedagents}.  We then provide a mechanism design framework for this setting. From thisframework, we design several novel mechanisms, for both the single and multiplequestion settings, that (1) encourage agents to invest effort and provide theirinformation honestly; (2) output a correct "hierarchy" of the information whenagents are rational.

Water from Two Rocks: Maximizing the Mutual Information

  We build a natural connection between the learning problem, co-training, andforecast elicitation without verification (related to peer-prediction) andaddress them simultaneously using the same information theoretic approach.  In co-training/multiview learning, the goal is to aggregate two views of datainto a prediction for a latent label. We show how to optimally combine twoviews of data by reducing the problem to an optimization problem. Our workgives a unified and rigorous approach to the general setting.  In forecast elicitation without verification we seek to design a mechanismthat elicits high quality forecasts from agents in the setting where themechanism does not have access to the ground truth. By assuming the agents'information is independent conditioning on the outcome, we propose mechanismswhere truth-telling is a strict equilibrium for both the single-task andmulti-task settings. Our multi-task mechanism additionally has the propertythat the truth-telling equilibrium pays better than any other strategy profileand strictly better than any other "non-permutation" strategy profile when theprior satisfies some mild conditions.

Social Learning with Questions

  This work studies sequential social learning (also known as Bayesianobservational learning), and how private communication can enable agents toavoid herding to the wrong action/state. Starting from the seminal BHW(Bikhchandani, Hirshleifer, and Welch, 1992) model where asymptotic learningdoes not occur, we allow agents to ask private and finite questions to abounded subset of their predecessors. While retaining the publicly observedhistory of the agents and their Bayes rationality from the BHW model, wefurther assume that both the ability to ask questions and the questionsthemselves are common knowledge. Then interpreting asking questions aspartitioning information sets, we study whether asymptotic learning can beachieved with finite capacity questions. Restricting our attention to thenetwork where every agent is only allowed to query her immediate predecessor,an explicit construction shows that a 1-bit question from each agent is enoughto enable asymptotic learning.

On the Complexity of Nash Equilibria of Action-Graph Games

  We consider the problem of computing Nash Equilibria of action-graph games(AGGs). AGGs, introduced by Bhat and Leyton-Brown, is a succinct representationof games that encapsulates both "local" dependencies as in graphical games, andpartial indifference to other agents' identities as in anonymous games, whichoccur in many natural settings. This is achieved by specifying a graph on theset of actions, so that the payoff of an agent for selecting a strategy dependsonly on the number of agents playing each of the neighboring strategies in theaction graph. We present a Polynomial Time Approximation Scheme for computingmixed Nash equilibria of AGGs with constant treewidth and a constant number ofagent types (and an arbitrary number of strategies), together with hardnessresults for the cases when either the treewidth or the number of agent types isunconstrained. In particular, we show that even if the action graph is a tree,but the number of agent-types is unconstrained, it is NP-complete to decide theexistence of a pure-strategy Nash equilibrium and PPAD-complete to compute amixed Nash equilibrium (even an approximate one); similarly for symmetric AGGs(all agents belong to a single type), if we allow arbitrary treewidth. Thesehardness results suggest that, in some sense, our PTAS is as strong of apositive result as one can expect.

General Hardness Amplification of Predicates and Puzzles

  We give new proofs for the hardness amplification of efficiently samplablepredicates and of weakly verifiable puzzles which generalize to new settings.More concretely, in the first part of the paper, we give a new proof of Yao'sXOR-Lemma that additionally applies to related theorems in the cryptographicsetting. Our proof seems simpler than previous ones, yet immediatelygeneralizes to statements similar in spirit such as the extraction lemma usedto obtain pseudo-random generators from one-way functions [Hastad, Impagliazzo,Levin, Luby, SIAM J. on Comp. 1999].  In the second part of the paper, we give a new proof of hardnessamplification for weakly verifiable puzzles, which is more general thanprevious ones in that it gives the right bound even for an arbitrary monotonefunction applied to the checking circuit of the underlying puzzle.  Both our proofs are applicable in many settings of interactive cryptographicprotocols because they satisfy a property that we call "non-rewinding". Inparticular, we show that any weak cryptographic protocol whose security isgiven by the unpredictability of single bits can be strengthened with a naturalinformation theoretic protocol. As an example, we show how these theorems solvethe main open question from [Halevi and Rabin, TCC2008] concerning bitcommitment.

Social Learning in a Changing World

  We study a model of learning on social networks in dynamic environments,describing a group of agents who are each trying to estimate an underlyingstate that varies over time, given access to weak signals and the estimates oftheir social network neighbors.  We study three models of agent behavior. In the "fixed response" model,agents use a fixed linear combination to incorporate information from theirpeers into their own estimate. This can be thought of as an extension of theDeGroot model to a dynamic setting. In the "best response" model, playerscalculate minimum variance linear estimators of the underlying state.  We show that regardless of the initial configuration, fixed response dynamicsconverge to a steady state, and that the same holds for best response on thecomplete graph. We show that best response dynamics can, in the long term, leadto estimators with higher variance than is achievable using well chosen fixedresponses.  The "penultimate prediction" model is an elaboration of the best responsemodel. While this model only slightly complicates the computations required ofthe agents, we show that in some cases it greatly increases the efficiency oflearning, and on complete graphs is in fact optimal, in a strong sense.

Characterizing Strategic Cascades on Networks

  Transmission of disease, spread of information and rumors, adoption of newproducts, and many other network phenomena can be fruitfully modeled ascascading processes, where actions chosen by nodes influence the subsequentbehavior of neighbors in the network graph. Current literature on cascadestends to assume nodes choose myopically based on the state of choices alreadytaken by other nodes. We examine the possibility of strategic choice, whereagents representing nodes anticipate the choices of others who have not yetdecided, and take into account their own influence on such choices. Our studyemploys the framework of Chierichetti et al. [2012], who (under assumption ofmyopic node behavior) investigate the scheduling of node decisions to promotecascades of product adoptions preferred by the scheduler. We show that whennodes behave strategically, outcomes can be extremely different. We exhibitcases where in the strategic setting 100% of agents adopt, but in the myopicsetting only an arbitrarily small epsilon % do. Conversely, we present caseswhere in the strategic setting 0% of agents adopt, but in the myopic setting(100-epsilon)% do, for any constant epsilon > 0. Additionally, we prove someproperties of cascade processes with strategic agents, both in general and forparticular classes of graphs.

Buying Private Data without Verification

  We consider the problem of designing a survey to aggregate non-verifiableinformation from a privacy-sensitive population: an analyst wants to computesome aggregate statistic from the private bits held by each member of apopulation, but cannot verify the correctness of the bits reported byparticipants in his survey. Individuals in the population are strategic agentswith a cost for privacy, \ie, they not only account for the payments theyexpect to receive from the mechanism, but also their privacy costs from anyinformation revealed about them by the mechanism's outcome---the computedstatistic as well as the payments---to determine their utilities. How can theanalyst design payments to obtain an accurate estimate of the populationstatistic when individuals strategically decide both whether to participate andwhether to truthfully report their sensitive information?  We design a differentially private peer-prediction mechanism that supportsaccurate estimation of the population statistic as a Bayes-Nash equilibrium insettings where agents have explicit preferences for privacy. The mechanismrequires knowledge of the marginal prior distribution on bits $b_i$, but doesnot need full knowledge of the marginal distribution on the costs $c_i$,instead requiring only an approximate upper bound. Our mechanism guarantees$\epsilon$-differential privacy to each agent $i$ against any adversary who canobserve the statistical estimate output by the mechanism, as well as thepayments made to the $n-1$ other agents $j\neq i$. Finally, we show that withslightly more structured assumptions on the privacy cost functions of eachagent, the cost of running the survey goes to $0$ as the number of agentsdiverges.

Identifying the Major Sources of Variance in Transaction Latencies:  Towards More Predictable Databases

  Decades of research have sought to improve transaction processing performanceand scalability in database management systems (DBMSs). However, significantlyless attention has been dedicated to the predictability of performance: howoften individual transactions exhibit execution latency far from the mean?Performance predictability is vital when transaction processing lies on thecritical path of a complex enterprise software or an interactive web service,as well as in emerging database-as-a-service markets where customers contractfor guaranteed levels of performance. In this paper, we take several stepstowards achieving more predictable database systems. First, we propose aprofiling framework called VProfiler that, given the source code of a DBMS, isable to identify the dominant sources of variance in transaction latency.VProfiler automatically instruments the DBMS source code to deconstruct theoverall variance of transaction latencies into variances and covariances of theexecution time of individual functions, which in turn provide insight into theroot causes of variance. Second, we use VProfiler to analyze MySQL and Postgres- two of the most popular and complex open-source database systems. Our casestudies reveal that the primary causes of variance in MySQL and Postgres arelock scheduling and centralized logging, respectively. Finally, based onVProfiler's findings, we further focus on remedying the performance variance ofMySQL by (1) proposing a new lock scheduling algorithm, called Variance-AwareTransaction Scheduling (VATS), (2) enhancing the buffer pool replacementpolicy, and (3) identifying tuning parameters that can reduce variancesignificantly. Our experimental results show that our schemes reduce overalltransaction latency variance by 37% on average (and up to 64%) withoutcompromising throughput or mean latency.

Putting Peer Prediction Under the Micro(economic)scope and Making  Truth-telling Focal

  Peer-prediction is a (meta-)mechanism which, given any proper scoring rule,produces a mechanism to elicit privately-held, non-verifiable information fromself-interested agents. Formally, truth-telling is a strict Nash equilibrium ofthe mechanism. Unfortunately, there may be other equilibria as well (includinguninformative equilibria where all players simply report the same fixed signal,regardless of their true signal) and, typically, the truth-telling equilibriumdoes not have the highest expected payoff. The main result of this paper is toshow that, in the symmetric binary setting, by tweaking peer-prediction, inpart by carefully selecting the proper scoring rule it is based on, we can makethe truth-telling equilibrium focal---that is, truth-telling has higherexpected payoff than any other equilibrium.  Along the way, we prove the following: in the setting where agents receivebinary signals we 1) classify all equilibria of the peer-prediction mechanism;2) introduce a new technical tool for understanding scoring rules, which allowsus to make truth-telling pay better than any other informative equilibrium; 3)leverage this tool to provide an optimal version of the previous result; thatis, we optimize the gap between the expected payoff of truth-telling and otherinformative equilibria; and 4) show that with a slight modification to the peerprediction framework, we can, in general, make the truth-telling equilibriumfocal---that is, truth-telling pays more than any other equilibrium (includingthe uninformative equilibria).

Equilibrium Selection in Information Elicitation without Verification  via Information Monotonicity

  Peer-prediction is a mechanism which elicits privately-held, non-variableinformation from self-interested agents---formally, truth-telling is a strictBayes Nash equilibrium of the mechanism. The original Peer-prediction mechanismsuffers from two main limitations: (1) the mechanism must know the "commonprior" of agents' signals; (2) additional undesirable and non-truthfulequilibria exist which often have a greater expected payoff than thetruth-telling equilibrium. A series of results has successfully weakened theknown common prior assumption. However, the equilibrium multiplicity issueremains a challenge.  In this paper, we address the above two problems. In the setting where acommon prior exists but is not known to the mechanism we show (1) a generalnegative result applying to a large class of mechanisms showing truth-tellingcan never pay strictly more in expectation than a particular set of equilibriawhere agents collude to "relabel" the signals and tell the truth afterrelabeling signals; (2) provide a mechanism that has no information about thecommon prior but where truth-telling pays as much in expectation as anyrelabeling equilibrium and pays strictly more than any other symmetricequilibrium; (3) moreover in our mechanism, if the number of agents issufficiently large, truth-telling pays similarly to any equilibrium close to a"relabeling" equilibrium and pays strictly more than any equilibrium that isnot close to a relabeling equilibrium.

An Information Theoretic Framework For Designing Information Elicitation  Mechanisms That Reward Truth-telling

  In the setting where information cannot be verified, we propose a simple yetpowerful information theoretical framework---the Mutual InformationParadigm---for information elicitation mechanisms. Our framework pays everyagent a measure of mutual information between her signal and a peer's signal.We require that the mutual information measurement has the key property thatany "data processing" on the two random variables will decrease the mutualinformation between them. We identify such information measures that generalizeShannon mutual information.  Our Mutual Information Paradigm overcomes the two main challenges ininformation elicitation without verification: (1) how to incentivize effort andavoid agents colluding to report random or identical responses (2) how tomotivate agents who believe they are in the minority to report truthfully.  Aided by the information measures we found, (1) we use the paradigm to designa family of novel mechanisms where truth-telling is a dominant strategy and anyother strategy will decrease every agent's expected payment (in themulti-question, detail free, minimal setting where the number of questions islarge); (2) we show the versatility of our framework by providing a unifiedtheoretical understanding of existing mechanisms---Peer Prediction [Miller2005], Bayesian Truth Serum [Prelec 2004], and Dasgupta and Ghosh [2013]---bymapping them into our framework such that theoretical results of those existingmechanisms can be reconstructed easily.  We also give an impossibility result which illustrates, in a certain sense,the the optimality of our framework.

Don't Be Greedy: Leveraging Community Structure to Find High Quality  Seed Sets for Influence Maximization

  We consider the problem of maximizing the spread of influence in a socialnetwork by choosing a fixed number of initial seeds --- a central problem inthe study of network cascades. The majority of existing work on this problem,formally referred to as the influence maximization problem, is designed forsubmodular cascades. Despite the empirical evidence that many cascades arenon-submodular, little work has been done focusing on non-submodular influencemaximization.  We propose a new heuristic for solving the influence maximization problem andshow via simulations on real-world and synthetic networks that our algorithmoutputs more influential seed sets than the state-of-the-art greedy algorithmin many natural cases, with average improvements of 7% for submodular cascades,and 55% for non-submodular cascades. Our heuristic uses a dynamic programmingapproach on a hierarchical decomposition of the social network to leverage therelation between the spread of cascades and the community structure of socialnetworks. We verify the importance of network structure by showing the qualityof the hierarchical decomposition impacts the quality of seed set output by ouralgorithm. We also present "worst-case" theoretical results proving that incertain settings our algorithm outputs seed sets that are a factor of$\Theta(\sqrt{n})$ more influential than those of the greedy algorithm, where$n$ is the number of nodes in the network. Finally, we generalize our algorithmto a message passing version that can be used to find seed sets that have atleast as much influence as the dynamic programming algorithms.

Beyond Worst-Case (In)approximability of Nonsubmodular Influence  Maximization

  We consider the problem of maximizing the spread of influence in a socialnetwork by choosing a fixed number of initial seeds, formally referred to asthe influence maximization problem. It admits a $(1-1/e)$-factor approximationalgorithm if the influence function is submodular. Otherwise, in the worstcase, the problem is NP-hard to approximate to within a factor of$N^{1-\varepsilon}$. This paper studies whether this worst-case hardness resultcan be circumvented by making assumptions about either the underlying networktopology or the cascade model. All of our assumptions are motivated by manyreal life social network cascades.  First, we present strong inapproximability results for a very restrictedclass of networks called the (stochastic) hierarchical blockmodel, a specialcase of the well-studied (stochastic) blockmodel in which relationships betweenblocks admit a tree structure. We also provide a dynamic-program basedpolynomial time algorithm which optimally computes a directed variant of theinfluence maximization problem on hierarchical blockmodel networks. Ouralgorithm indicates that the inapproximability result is due to thebidirectionality of influence between agent-blocks.  Second, we present strong inapproximability results for a class of influencefunctions that are "almost" submodular, called 2-quasi-submodular. Ourinapproximability results hold even for any 2-quasi-submodular $f$ fixed inadvance. This result also indicates that the "threshold" between submodularityand nonsubmodularity is sharp, regarding the approximability of influencemaximization.

Characterizing Adversarial Subspaces Using Local Intrinsic  Dimensionality

  Deep Neural Networks (DNNs) have recently been shown to be vulnerable againstadversarial examples, which are carefully crafted instances that can misleadDNNs to make errors during prediction. To better understand such attacks, acharacterization is needed of the properties of regions (the so-called'adversarial subspaces') in which adversarial examples lie. We tackle thischallenge by characterizing the dimensional properties of adversarial regions,via the use of Local Intrinsic Dimensionality (LID). LID assesses thespace-filling capability of the region surrounding a reference example, basedon the distance distribution of the example to its neighbors. We first provideexplanations about how adversarial perturbation can affect the LIDcharacteristic of adversarial regions, and then show empirically that LIDcharacteristics can facilitate the distinction of adversarial examplesgenerated using state-of-the-art attacks. As a proof-of-concept, we show that apotential application of LID is to distinguish adversarial examples, and thepreliminary results show that it can outperform several state-of-the-artdetection measures by large margins for five attack strategies considered inthis paper across three benchmark datasets. Our analysis of the LIDcharacteristic for adversarial regions not only motivates new directions ofeffective adversarial defense, but also opens up more challenges for developingnew attacks to better understand the vulnerabilities of DNNs.

Optimal Testing of Reed-Muller Codes

  We consider the problem of testing if a given function f : F_2^n -> F_2 isclose to any degree d polynomial in n variables, also known as the Reed-Mullertesting problem. The Gowers norm is based on a natural 2^{d+1}-query test forthis property. Alon et al. [AKKLR05] rediscovered this test and showed that itaccepts every degree d polynomial with probability 1, while it rejectsfunctions that are Omega(1)-far with probability Omega(1/(d 2^{d})). We give anasymptotically optimal analysis of this test, and show that it rejectsfunctions that are (even only) Omega(2^{-d})-far with Omega(1)-probability (sothe rejection probability is a universal constant independent of d and n). Thisimplies a tight relationship between the (d+1)st Gowers norm of a function andits maximal correlation with degree d polynomials, when the correlation isclose to 1. Our proof works by induction on n and yields a new analysis of eventhe classical Blum-Luby-Rubinfeld [BLR93] linearity test, for the setting offunctions mapping F_2^n to F_2. The optimality follows from a tighter analysisof counterexamples to the "inverse conjecture for the Gowers norm" constructedby [GT09,LMS08]. Our result has several implications. First, it shows that theGowers norm test is tolerant, in that it also accepts close codewords. Second,it improves the parameters of an XOR lemma for polynomials given by Viola andWigderson [VW07]. Third, it implies a "query hierarchy" result for propertytesting of affine-invariant properties. That is, for every function q(n), itgives an affine-invariant property that is testable with O(q(n))-queries, butnot with o(q(n))-queries, complementing an analogous result of [GKNR09] forgraph properties.

Constrained Non-Monotone Submodular Maximization: Offline and Secretary  Algorithms

  Constrained submodular maximization problems have long been studied, withnear-optimal results known under a variety of constraints when the submodularfunction is monotone. The case of non-monotone submodular maximization is lessunderstood: the first approximation algorithms even for the unconstraintedsetting were given by Feige et al. (FOCS '07). More recently, Lee et al. (STOC'09, APPROX '09) show how to approximately maximize non-monotone submodularfunctions when the constraints are given by the intersection of p matroidconstraints; their algorithm is based on local-search procedures that considerp-swaps, and hence the running time may be n^Omega(p), implying their algorithmis polynomial-time only for constantly many matroids. In this paper, we givealgorithms that work for p-independence systems (which generalize constraintsgiven by the intersection of p matroids), where the running time is poly(n,p).Our algorithm essentially reduces the non-monotone maximization problem tomultiple runs of the greedy algorithm previously used in the monotone case.  Our idea of using existing algorithms for monotone functions to solve thenon-monotone case also works for maximizing a submodular function with respectto a knapsack constraint: we get a simple greedy-based constant-factorapproximation for this problem.  With these simpler algorithms, we are able to adapt our approach toconstrained non-monotone submodular maximization to the (online) secretarysetting, where elements arrive one at a time in random order, and the algorithmmust make irrevocable decisions about whether or not to select each element asit arrives. We give constant approximations in this secretary setting when thealgorithm is constrained subject to a uniform matroid or a partition matroid,and give an O(log k) approximation when it is constrained by a general matroidof rank k.

How Complex Contagions Spread Quickly in the Preferential Attachment  Model and Other Time-Evolving Networks

  In this paper, we study the spreading speed of complex contagions in a socialnetwork. A $k$-complex contagion starts from a set of initially infected seedssuch that any node with at least $k$ infected neighbors gets infected. Simplecontagions, i.e., $k=1$, quickly spread to the entire network in small worldgraphs. However, fast spreading of complex contagions appears to be less likelyand more delicate; the successful cases depend crucially on the networkstructure~\cite{G08,Ghasemiesfeh:2013:CCW}.  Our main result shows that complex contagions can spread fast in a generalfamily of time-evolving networks that includes the preferential attachmentmodel~\cite{barabasi99emergence}. We prove that if the initial seeds are chosenas the oldest nodes in a network of this family, a $k$-complex contagion coversthe entire network of $n$ nodes in $O(\log n)$ steps. We show that the choiceof the initial seeds is crucial. If the initial seeds are uniformly randomlychosen in the PA model, even with a polynomial number of them, a complexcontagion would stop prematurely. The oldest nodes in a preferential attachmentmodel are likely to have high degrees. However, we remark that it is actuallynot the power law degree distribution per se that facilitates fast spreading ofcomplex contagions, but rather the evolutionary graph structure of such models.Some members of the said family do not even have a power-law distribution.  We also prove that complex contagions are fast in the copymodel~\cite{KumarRaRa00}, a variant of the preferential attachment family.  Finally, we prove that when a complex contagion starts from an arbitrary setof initial seeds on a general graph, determining if the number of infectedvertices is above a given threshold is $\mathbf{P}$-complete. Thus, one cannothope to categorize all the settings in which complex contagions percolate in agraph.

Complex Contagions in Kleinberg's Small World Model

  Complex contagions describe diffusion of behaviors in a social network insettings where spreading requires the influence by two or more neighbors. In a$k$-complex contagion, a cluster of nodes are initially infected, andadditional nodes become infected in the next round if they have at least $k$already infected neighbors. It has been argued that complex contagions bettermodel behavioral changes such as adoption of new beliefs, fashion trends orexpensive technology innovations. This has motivated rigorous understanding ofspreading of complex contagions in social networks. Despite simple contagions($k=1$) that spread fast in all small world graphs, how complex contagionsspread is much less understood. Previous work~\cite{Ghasemiesfeh:2013:CCW}analyzes complex contagions in Kleinberg's small worldmodel~\cite{kleinberg00small} where edges are randomly added according to aspatial distribution (with exponent $\gamma$) on top of a two dimensional gridstructure. It has been shown in~\cite{Ghasemiesfeh:2013:CCW} that the speed ofcomplex contagions differs exponentially when $\gamma=0$ compared to when$\gamma=2$.  In this paper, we fully characterize the entire parameter space of $\gamma$except at one point, and provide upper and lower bounds for the speed of$k$-complex contagions. We study two subtly different variants of Kleinberg'ssmall world model and show that, with respect to complex contagions, theybehave differently. For each model and each $k \geq 2$, we show that there isan intermediate range of values, such that when $\gamma$ takes any of thesevalues, a $k$-complex contagion spreads quickly on the corresponding graph, ina polylogarithmic number of rounds. However, if $\gamma$ is outside this range,then a $k$-complex contagion requires a polynomial number of rounds to spreadto the entire network.

