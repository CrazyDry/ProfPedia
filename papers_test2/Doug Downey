Definition Modeling: Learning to define word embeddings in natural  language

  Distributed representations of words have been shown to capture lexicalsemantics, as demonstrated by their effectiveness in word similarity andanalogical relation tasks. But, these tasks only evaluate lexical semanticsindirectly. In this paper, we study whether it is possible to utilizedistributed representations to generate dictionary definitions of words, as amore direct and transparent representation of the embeddings' semantics. Weintroduce definition modeling, the task of generating a definition for a givenword and its embedding. We present several definition model architectures basedon recurrent neural networks, and experiment with the models over multiple datasets. Our results show that a model that controls dependencies between the wordbeing defined and the definition words performs significantly better, and thata character-level convolution layer designed to leverage morphology cancomplement word-level embeddings. Finally, an error analysis suggests that theerrors made by a definition model may provide insight into the shortcomings ofword embeddings.

Construction of the Literature Graph in Semantic Scholar

  We describe a deployed scalable system for organizing published scientificliterature into a heterogeneous graph to facilitate algorithmic manipulationand discovery. The resulting literature graph consists of more than 280M nodes,representing papers, authors, entities and various interactions between them(e.g., authorships, citations, entity mentions). We reduce literature graphconstruction into familiar NLP tasks (e.g., entity extraction and linking),point out research challenges due to differences from standard formulations ofthese tasks, and report empirical results for each task. The methods describedin this paper are used to enable semantic features in www.semanticscholar.org

A new evaluation framework for topic modeling algorithms based on  synthetic corpora

  Topic models are in widespread use in natural language processing and beyond.Here, we propose a new framework for the evaluation of probabilistic topicmodeling algorithms based on synthetic corpora containing an unambiguouslydefined ground truth topic structure. The major innovation of our approach isthe ability to quantify the agreement between the planted and inferred topicstructures by comparing the assigned topic labels at the level of the tokens.In experiments, our approach yields novel insights about the relative strengthsof topic models as corpus characteristics vary, and the first evidence of an"undetectable phase" for topic models when the planted structure is weak. Wealso establish the practical relevance of the insights gained for syntheticcorpora by predicting the performance of topic modeling algorithms inclassification tasks in real-world corpora.

AQuA: An Adversarially Authored Question-Answer Dataset for Common Sense

  Commonsense reasoning is a critical AI capability, but it is difficult toconstruct challenging datasets that test common sense. Recent neuralquestion-answering systems, based on large pre-trained models of language, havealready achieved near-human-level performance on commonsense knowledgebenchmarks. These systems do not possess human-level common sense, but are ableto exploit limitations of the datasets to achieve human-level scores.  We introduce the AQuA dataset, an adversarially-constructed evaluationdataset for testing common sense. AQuA forms a challenging extension to therecently-proposed SWAG dataset, which tests commonsense knowledge usingsentence-completion questions that describe situations observed in video. Toproduce a more difficult dataset, we introduce a novel procedure for questionacquisition in which workers author questions designed to target weaknesses ofstate-of-the-art neural question answering systems. Workers are rewarded forsubmissions that models fail to answer correctly both before and afterfine-tuning (in cross-validation). We create 2.8k questions via this procedureand evaluate the performance of multiple state-of-the-art question answeringsystems on our dataset. We observe a significant gap between human performance,which is 95.3%, and the performance of the best baseline accuracy of 65.3% bythe OpenAI GPT model.

