Random DFAs are Efficiently PAC Learnable

  This paper has been withdrawn due to an error found by Dana Angluin and LevReyzin.

Query learning of derived $ω$-tree languages in polynomial time

  We present the first polynomial time algorithm to learn nontrivial classes oflanguages of infinite trees. Specifically, our algorithm uses membership andequivalence queries to learn classes of $\omega$-tree languages derived fromweak regular $\omega$-word languages in polynomial time. The method is ageneral polynomial time reduction of learning a class of derived $\omega$-treelanguages to learning the underlying class of $\omega$-word languages, for anyclass of $\omega$-word languages recognized by a deterministic B\"{u}chiacceptor. Our reduction, combined with the polynomial time learning algorithmof Maler and Pnueli [1995] for the class of weak regular $\omega$-wordlanguages yields the main result. We also show that subset queries that returncounterexamples can be implemented in polynomial time using subset queries thatreturn no counterexamples for deterministic or non-deterministic finite wordacceptors, and deterministic or non-deterministic B\"{u}chi $\omega$-wordacceptors.  A previous claim of an algorithm to learn regular $\omega$-trees due toJayasrirani, Begam and Thomas [2008] is unfortunately incorrect, as shown inAngluin [2016].

The computational power of population protocols

  We consider the model of population protocols introduced by Angluin et al.,in which anonymous finite-state agents stably compute a predicate of themultiset of their inputs via two-way interactions in the all-pairs family ofcommunication networks. We prove that all predicates stably computable in thismodel (and certain generalizations of it) are semilinear, answering a centralopen question about the power of the model. Removing the assumption of two-wayinteraction, we also consider several variants of the model in which agentscommunicate by anonymous message-passing where the recipient of each message ischosen by an adversary and the sender is not identified to the recipient. Theseone-way models are distinguished by whether messages are delivered immediatelyor after a delay, whether a sender can record that it has sent a message, andwhether a recipient can queue incoming messages, refusing to accept newmessages until it has had a chance to send out messages of its own. Wecharacterize the classes of predicates stably computable in each of theseone-way models using natural subclasses of the semilinear predicates.

Regular omega-Languages with an Informative Right Congruence

  A regular language is almost fully characterized by its right congruencerelation. Indeed, a regular language can always be recognized by a DFAisomorphic to the automaton corresponding to its right congruence, henceforththe Rightcon automaton. The same does not hold for regular omega-languages. Theright congruence of a regular omega-language is not informative enough; manyregular omega-languages have a trivial right congruence, and in general it isnot always possible to define an omega-automaton recognizing a given languagethat is isomorphic to the rightcon automaton.  The class of weak regular omega-languages does have an informative rightcongruence. That is, any weak regular omega-language can always be recognizedby a deterministic B\"uchi automaton that is isomorphic to the rightconautomaton. Weak regular omega-languages reside in the lower levels of theexpressiveness hierarchy of regular omega-languages. Are there more expressivesub-classes of regular omega languages that have an informative rightcongruence? Can we fully characterize the class of languages with a trivialright congruence? In this paper we try to place some additional pieces of thisbig puzzle.

Families of DFAs as Acceptors of $ω$-Regular Languages

  Families of DFAs (FDFAs) provide an alternative formalism for recognizing$\omega$-regular languages. The motivation for introducing them was a desiredcorrelation between the automaton states and right congruence relations, in amanner similar to the Myhill-Nerode theorem for regular languages. Thiscorrelation is beneficial for learning algorithms, and indeed it was recentlyshown that $\omega$-regular languages can be learned from membership andequivalence queries, using FDFAs as the acceptors.  In this paper, we look into the question of how suitable FDFAs are fordefining omega-regular languages. Specifically, we look into the complexity ofperforming Boolean operations, such as complementation and intersection, onFDFAs, the complexity of solving decision problems, such as emptiness andlanguage containment, and the succinctness of FDFAs compared to standarddeterministic and nondeterministic $\omega$-automata.  We show that FDFAs enjoy the benefits of deterministic automata with respectto Boolean operations and decision problems. Namely, they can all be performedin nondeterministic logarithmic space. We provide polynomial translations ofdeterministic B\"uchi and co-B\"uchi automata to FDFAs and of FDFAs tonondeterministic B\"uchi automata (NBAs). We show that translation of an NBA toan FDFA may involve an exponential blowup. Last, we show that FDFAs are moresuccinct than deterministic parity automata (DPAs) in the sense thattranslating a DPA to an FDFA can always be done with only a polynomialincrease, yet the other direction involves an inevitable exponential blowup inthe worst case.

Learning and Verifying Quantified Boolean Queries by Example

  To help a user specify and verify quantified queries --- a class of databasequeries known to be very challenging for all but the most expert users --- onecan question the user on whether certain data objects are answers ornon-answers to her intended query. In this paper, we analyze the number ofquestions needed to learn or verify qhorn queries, a special class of Booleanquantified queries whose underlying form is conjunctions of quantified Hornexpressions. We provide optimal polynomial-question and polynomial-timelearning and verification algorithms for two subclasses of the class qhorn withupper constant limits on a query's causal density.

Context-Free Transductions with Neural Stacks

  This paper analyzes the behavior of stack-augmented recurrent neural network(RNN) models. Due to the architectural similarity between stack RNNs andpushdown transducers, we train stack RNN models on a number of tasks, includingstring reversal, context-free language modelling, and cumulative XORevaluation. Examining the behavior of our networks, we show thatstack-augmented RNNs can discover intuitive stack-based strategies for solvingour tasks. However, stack RNNs are more difficult to train than classicalarchitectures such as LSTMs. Rather than employ stack-based strategies, morecomplex networks often find approximate solutions by using the stack asunstructured memory.

