Algorithm Portfolio Design: Theory vs. Practice

  Stochastic algorithms are among the best for solving computationally hardsearch and reasoning problems. The runtime of such procedures is characterizedby a random variable. Different algorithms give rise to different probabilitydistributions. One can take advantage of such differences by combining severalalgorithms into a portfolio, and running them in parallel or interleaving themon a single processor. We provide a detailed evaluation of the portfolioapproach on distributions of hard combinatorial search problems. We show underwhat conditions the protfolio approach can have a dramatic computationaladvantage over the best traditional methods.

Playing games against nature: optimal policies for renewable resource  allocation

  In this paper we introduce a class of Markov decision processes that arise asa natural model for many renewable resource allocation problems. Upon extendingresults from the inventory control literature, we prove that they admit aclosed form solution and we show how to exploit this structure to speed up itscomputation. We consider the application of the proposed framework to severalproblems arising in very different domains, and as part of the ongoing effortin the emerging field of Computational Sustainability we discuss in detail itsapplication to the Northern Pacific Halibut marine fishery. Our approach isapplied to a model based on real world data, obtaining a policy with aguaranteed lower bound on the utility function that is structurally verydifferent from the one currently employed.

Maximizing the Spread of Cascades Using Network Design

  We introduce a new optimization framework to maximize the expected spread ofcascades in networks. Our model allows a rich set of actions that directlymanipulate cascade dynamics by adding nodes or edges to the network. Ourmotivating application is one in spatial conservation planning, where a cascademodels the dispersal of wild animals through a fragmented landscape. We proposea mixed integer programming (MIP) formulation that combines elements fromnetwork design and stochastic optimization. Our approach results in solutionswith stochastic optimality guarantees and points to conservation strategiesthat are fundamentally different from naive approaches.

Uniform Solution Sampling Using a Constraint Solver As an Oracle

  We consider the problem of sampling from solutions defined by a set of hardconstraints on a combinatorial space. We propose a new sampling technique that,while enforcing a uniform exploration of the search space, leverages thereasoning power of a systematic constraint solver in a black-box scheme. Wepresent a series of challenging domains, such as energy barriers and highlyasymmetric spaces, that reveal the difficulties introduced by hard constraints.We demonstrate that standard approaches such as Simulated Annealing and GibbsSampling are greatly affected, while our new technique can overcome many ofthese difficulties. Finally, we show that our sampling scheme naturally definesa new approximate model counting technique, which we empirically show to bevery accurate on a range of benchmark problems.

A Bayesian Approach to Tackling Hard Computational Problems

  We are developing a general framework for using learned Bayesian models fordecision-theoretic control of search and reasoningalgorithms. We illustrate theapproach on the specific task of controlling both general and domain-specificsolvers on a hard class of structured constraint satisfaction problems. Asuccessful strategyfor reducing the high (and even infinite) variance inrunning time typically exhibited by backtracking search algorithms is to cutoff and restart the search if a solution is not found within a certainamount oftime. Previous work on restart strategies have employed fixed cut off values.We show how to create a dynamic cut off strategy by learning a Bayesian modelthat predicts the ultimate length of a trial based on observing the earlybehavior of the search algorithm. Furthermore, we describe the generalconditions under which a dynamic restart strategy can outperform thetheoretically optimal fixed strategy.

Taming the Curse of Dimensionality: Discrete Integration by Hashing and  Optimization

  Integration is affected by the curse of dimensionality and quickly becomesintractable as the dimensionality of the problem grows. We propose a randomizedalgorithm that, with high probability, gives a constant-factor approximation ofa general discrete integral defined over an exponentially large set. Thisalgorithm relies on solving only a small number of instances of a discretecombinatorial optimization problem subject to randomly generated parityconstraints used as a hash function. As an application, we demonstrate thatwith a small number of MAP queries we can efficiently approximate the partitionfunction of discrete graphical models, which can in turn be used, for instance,for marginal computation or model selection.

Optimization With Parity Constraints: From Binary Codes to Discrete  Integration

  Many probabilistic inference tasks involve summations over exponentiallylarge sets. Recently, it has been shown that these problems can be reduced tosolving a polynomial number of MAP inference queries for a model augmented withrandomly generated parity constraints. By exploiting a connection withmax-likelihood decoding of binary codes, we show that these optimizations arecomputationally hard. Inspired by iterative message passing decodingalgorithms, we propose an Integer Linear Programming (ILP) formulation for theproblem, enhanced with new sparsification techniques to improve decodingperformance. By solving the ILP through a sequence of LP relaxations, we getboth lower and upper bounds on the partition function, which hold with highprobability and are much tighter than those obtained with variational methods.

Variable Elimination in the Fourier Domain

  The ability to represent complex high dimensional probability distributionsin a compact form is one of the key insights in the field of graphical models.Factored representations are ubiquitous in machine learning and lead to majorcomputational advantages. We explore a different type of compact representationbased on discrete Fourier representations, complementing the classical approachbased on conditional independencies. We show that a large class ofprobabilistic graphical models have a compact Fourier representation. Thistheoretical result opens up an entirely new way of approximating a probabilitydistribution. We demonstrate the significance of this approach by applying itto the variable elimination algorithm. Compared with the traditional bucketrepresentation and other approximate inference algorithms, we obtainsignificant improvements.

Solving Marginal MAP Problems with NP Oracles and Parity Constraints

  Arising from many applications at the intersection of decision making andmachine learning, Marginal Maximum A Posteriori (Marginal MAP) Problems unifythe two main classes of inference, namely maximization (optimization) andmarginal inference (counting), and are believed to have higher complexity thanboth of them. We propose XOR_MMAP, a novel approach to solve the Marginal MAPProblem, which represents the intractable counting subproblem with queries toNP oracles, subject to additional parity constraints. XOR_MMAP provides aconstant factor approximation to the Marginal MAP Problem, by encoding it as asingle optimization in polynomial size of the original problem. We evaluate ourapproach in several machine learning and decision making applications, and showthat our approach outperforms several state-of-the-art Marginal MAP solvers.

XOR-Sampling for Network Design with Correlated Stochastic Events

  Many network optimization problems can be formulated as stochastic networkdesign problems in which edges are present or absent stochastically.Furthermore, protective actions can guarantee that edges will remain present.We consider the problem of finding the optimal protection strategy under abudget limit in order to maximize some connectivity measurements of thenetwork. Previous approaches rely on the assumption that edges are independent.In this paper, we consider a more realistic setting where multiple edges arenot independent due to natural disasters or regional events that make thestates of multiple edges stochastically correlated. We use Markov Random Fieldsto model the correlation and define a new stochastic network design framework.We provide a novel algorithm based on Sample Average Approximation (SAA)coupled with a Gibbs or XOR sampler. The experimental results on real roadnetwork data show that the policies produced by SAA with the XOR sampler havehigher quality and lower variance compared to SAA with Gibbs sampler.

Multi-Entity Dependence Learning with Rich Context via Conditional  Variational Auto-encoder

  Multi-Entity Dependence Learning (MEDL) explores conditional correlationsamong multiple entities. The availability of rich contextual informationrequires a nimble learning scheme that tightly integrates with deep neuralnetworks and has the ability to capture correlation structures amongexponentially many outcomes. We propose MEDL_CVAE, which encodes a conditionalmultivariate distribution as a generating process. As a result, the variationallower bound of the joint likelihood can be optimized via a conditionalvariational auto-encoder and trained end-to-end on GPUs. Our MEDL_CVAE wasmotivated by two real-world applications in computational sustainability: onestudies the spatial correlation among multiple bird species using the eBirddata and the other models multi-dimensional landscape composition and humanfootprint in the Amazon rainforest with satellite images. We show thatMEDL_CVAE captures rich dependency structures, scales better than previousmethods, and further improves on the joint likelihood taking advantage of verylarge datasets that are beyond the capacity of previous methods.

End-to-End Learning for the Deep Multivariate Probit Model

  The multivariate probit model (MVP) is a popular classic model for studyingbinary responses of multiple entities. Nevertheless, the computationalchallenge of learning the MVP model, given that its likelihood involvesintegrating over a multidimensional constrained space of latent variables,significantly limits its application in practice. We propose a flexible deepgeneralization of the classic MVP, the Deep Multivariate Probit Model (DMVP),which is an end-to-end learning scheme that uses an efficient parallel samplingprocess of the multivariate probit model to exploit GPU-boosted deep neuralnetworks. We present both theoretical and empirical analysis of the convergencebehavior of DMVP's sampling process with respect to the resolution of thecorrelation structure. We provide convergence guarantees for DMVP and ourempirical analysis demonstrates the advantages of DMVP's sampling compared withstandard MCMC-based methods. We also show that when applied to multi-entitymodelling problems, which are natural DMVP applications, DMVP trains fasterthan classical MVP, by at least an order of magnitude, captures richcorrelations among entities, and further improves the joint likelihood ofentities compared with several competitive models.

Bias Reduction via End-to-End Shift Learning: Application to Citizen  Science

  Citizen science projects are successful at gathering rich datasets forvarious applications. However, the data collected by citizen scientists areoften biased --- in particular, aligned more with the citizens' preferencesthan with scientific objectives. We propose the Shift Compensation Network(SCN), an end-to-end learning scheme which learns the shift from the scientificobjectives to the biased data while compensating for the shift by re-weightingthe training data. Applied to bird observational data from the citizen scienceproject eBird, we demonstrate how SCN quantifies the data distribution shiftand outperforms supervised learning models that do not address the data bias.Compared with competing models in the context of covariate shift, we furtherdemonstrate the advantage of SCN in both its effectiveness and its capabilityof handling massive high-dimensional data.

On the Erdos Discrepancy Problem

  According to the Erd\H{o}s discrepancy conjecture, for any infinite $\pm 1$sequence, there exists a homogeneous arithmetic progression of unboundeddiscrepancy. In other words, for any $\pm 1$ sequence $(x_1,x_2,...)$ and adiscrepancy $C$, there exist integers $m$ and $d$ such that $|\sum_{i=1}^m x_{i\cdot d}| > C$. This is an $80$-year-old open problem and recent developmentproved that this conjecture is true for discrepancies up to $2$. Paul Erd\H{o}salso conjectured that this property of unbounded discrepancy even holds for therestricted case of completely multiplicative sequences (CMSs), namely sequences$(x_1,x_2,...)$ where $x_{a \cdot b} = x_{a} \cdot x_{b}$ for any $a,b \geq 1$.The longest CMS with discrepancy $2$ has been proven to be of size $246$. Inthis paper, we prove that any completely multiplicative sequence of size$127,646$ or more has discrepancy at least $4$, proving the Erd\H{o}sdiscrepancy conjecture for CMSs of discrepancies up to $3$. In addition, weprove that this bound is tight and increases the size of the longest knownsequence of discrepancy $3$ from $17,000$ to $127,645$. Finally, we provideinductive construction rules as well as streamlining methods to improve thelower bounds for sequences of higher discrepancies.

Phase-Mapper: An AI Platform to Accelerate High Throughput Materials  Discovery

  High-Throughput materials discovery involves the rapid synthesis,measurement, and characterization of many different but structurally-relatedmaterials. A key problem in materials discovery, the phase map identificationproblem, involves the determination of the crystal phase diagram from thematerials' composition and structural characterization data. We presentPhase-Mapper, a novel AI platform to solve the phase map identification problemthat allows humans to interact with both the data and products of AIalgorithms, including the incorporation of human feedback to constrain orinitialize solutions. Phase-Mapper affords incorporation of any spectraldemixing algorithm, including our novel solver, AgileFD, which is based on aconvolutive non-negative matrix factorization algorithm. AgileFD canincorporate constraints to capture the physics of the materials as well ashuman feedback. We compare three solver variants with previously proposedmethods in a large-scale experiment involving 20 synthetic systems,demonstrating the efficacy of imposing physical constrains using AgileFD.Phase-Mapper has also been used by materials scientists to solve a wide varietyof phase diagrams, including the previously unsolved Nb-Mn-V oxide system,which is provided here as an illustrative example.

Automated Phase Mapping with AgileFD and its Application to Light  Absorber Discovery in the V-Mn-Nb Oxide System

  Rapid construction of phase diagrams is a central tenet of combinatorialmaterials science with accelerated materials discovery efforts often hamperedby challenges in interpreting combinatorial x-ray diffraction datasets, whichwe address by developing AgileFD, an artificial intelligence algorithm thatenables rapid phase mapping from a combinatorial library of x-ray diffractionpatterns. AgileFD models alloying-based peak shifting through a novel expansionof convolutional nonnegative matrix factorization, which not only improves theidentification of constituent phases but also maps their concentration andlattice parameter as a function of composition. By incorporating Gibbs phaserule into the algorithm, physically meaningful phase maps are obtained withunsupervised operation, and more refined solutions are attained by injectingexpert knowledge of the system. The algorithm is demonstrated throughinvestigation of the V-Mn-Nb oxide system where decomposition of eight oxidephases, including two with substantial alloying, provides the first phase mapfor this pseudo-ternary system. This phase map enables interpretation ofhigh-throughput band gap data, leading to the discovery of new solar lightabsorbers and the alloying-based tuning of the direct-allowed band-gap energyof MnV2O6. The open-source family of AgileFD algorithms can be implemented intoa broad range of high throughput workflows to accelerate materials discovery.

Automatic Detection and Compression for Passive Acoustic Monitoring of  the African Forest Elephant

  In this work, we consider applying machine learning to the analysis andcompression of audio signals in the context of monitoring elephants insub-Saharan Africa. Earth's biodiversity is increasingly under threat bysources of anthropogenic change (e.g. resource extraction, land use change, andclimate change) and surveying animal populations is critical for developingconservation strategies. However, manually monitoring tropical forests or deepoceans is intractable. For species that communicate acoustically, researchershave argued for placing audio recorders in the habitats as a cost-effective andnon-invasive method, a strategy known as passive acoustic monitoring (PAM). Incollaboration with conservation efforts, we construct a large labeled datasetof passive acoustic recordings of the African Forest Elephant viacrowdsourcing, compromising thousands of hours of recordings in the wild. Usingstate-of-the-art techniques in artificial intelligence we improve uponpreviously proposed methods for passive acoustic monitoring for classificationand segmentation. In real-time detection of elephant calls, network bandwidthquickly becomes a bottleneck and efficient ways to compress the data areneeded. Most audio compression schemes are aimed at human listeners and areunsuitable for low-frequency elephant calls. To remedy this, we provide a novelend-to-end differentiable method for compression of audio signals that can beadapted to acoustic monitoring of any species and dramatically improves overnaive coding strategies.

Search for CP violation in the decay $D^+ \to π^-π^+π^+$

  A search for CP violation in the phase space of the decay$D^+\to\pi^-\pi^+\pi^+$ is reported using $pp$ collision data, corresponding toan integrated luminosity of 1.0 fb$^{-1}$, collected by the LHCb experiment ata centre-of-mass energy of 7 TeV. The Dalitz plot distributions for $3.1\times10^6$ $D^+$ and $D^-$ candidates are compared with binned and unbinnedmodel-independent techniques. No evidence for CP violation is found.

