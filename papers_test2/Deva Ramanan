Dual coordinate solvers for large-scale structural SVMs

  This manuscript describes a method for training linear SVMs (including binarySVMs, SVM regression, and structural SVMs) from large, out-of-core trainingdatasets. Current strategies for large-scale learning fall into one of twocamps; batch algorithms which solve the learning problem given a finitedatasets, and online algorithms which can process out-of-core datasets. Theformer typically requires datasets small enough to fit in memory. The latter isoften phrased as a stochastic optimization problem; such algorithms enjoystrong theoretical properties but often require manual tuned annealingschedules, and may converge slowly for problems with large output spaces (e.g.,structural SVMs). We discuss an algorithm for an "intermediate" regime in whichthe data is too large to fit in memory, but the active constraints (supportvectors) are small enough to remain in memory. In this case, one can designrather efficient learning algorithms that are as stable as batch algorithms,but capable of processing out-of-core datasets. We have developed such aMATLAB-based solver and used it to train a collection of recognition systemsfor articulated pose estimation, facial analysis, 3D object recognition, andaction classification, all with publicly-available code. This writeup describesthe solver in detail.

Egocentric Pose Recognition in Four Lines of Code

  We tackle the problem of estimating the 3D pose of an individual's upperlimbs (arms+hands) from a chest mounted depth-camera. Importantly, we considerpose estimation during everyday interactions with objects. Past work shows thatstrong pose+viewpoint priors and depth-based features are crucial for robustperformance. In egocentric views, hands and arms are observable within a welldefined volume in front of the camera. We call this volume an egocentricworkspace. A notable property is that hand appearance correlates with workspacelocation. To exploit this correlation, we classify arm+hand configurations in aglobal egocentric coordinate frame, rather than a local scanning window. Thisgreatly simplify the architecture and improves performance. We propose anefficient pipeline which 1) generates synthetic workspace exemplars fortraining using a virtual chest-mounted camera whose intrinsic parameters matchour physical camera, 2) computes perspective-aware depth features on thisentire volume and 3) recognizes discrete arm+hand pose classes through a sparsemulti-class SVM. Our method provides state-of-the-art hand pose recognitionperformance from egocentric RGB-D images in real-time.

Do We Need More Training Data?

  Datasets for training object recognition systems are steadily increasing insize. This paper investigates the question of whether existing detectors willcontinue to improve as data grows, or saturate in performance due to limitedmodel complexity and the Bayes risk associated with the feature spaces in whichthey operate. We focus on the popular paradigm of discriminatively trainedtemplates defined on oriented gradient features. We investigate the performanceof mixtures of templates as the number of mixture components and the amount oftraining data grows. Surprisingly, even with proper treatment of regularizationand "outliers", the performance of classic mixture models appears to saturatequickly ($\sim$10 templates and $\sim$100 positive training examples pertemplate). This is not a limitation of the feature space as compositionalmixtures that share template parameters via parts and that can synthesize newtemplates not encountered during training yield significantly betterperformance. Based on our analysis, we conjecture that the greatest gains indetection performance will continue to derive from improved representations andlearning algorithms that can make efficient use of large datasets.

Depth-based hand pose estimation: methods, data, and challenges

  Hand pose estimation has matured rapidly in recent years. The introduction ofcommodity depth sensors and a multitude of practical applications have spurrednew advances. We provide an extensive analysis of the state-of-the-art,focusing on hand pose estimation from a single depth frame. To do so, we haveimplemented a considerable number of systems, and will release all software andevaluation code. We summarize important conclusions here: (1) Pose estimationappears roughly solved for scenes with isolated hands. However, methods stillstruggle to analyze cluttered scenes where hands may be interacting with nearbyobjects and surfaces. To spur further progress we introduce a challenging newdataset with diverse, cluttered scenes. (2) Many methods evaluate themselveswith disparate criteria, making comparisons difficult. We define a consistentevaluation criteria, rigorously motivated by human experiments. (3) Weintroduce a simple nearest-neighbor baseline that outperforms most existingsystems. This implies that most systems do not generalize beyond their trainingsets. This also reinforces the under-appreciated point that training data is asimportant as the model itself. We conclude with directions for future progress.

Multi-scale recognition with DAG-CNNs

  We explore multi-scale convolutional neural nets (CNNs) for imageclassification. Contemporary approaches extract features from a single outputlayer. By extracting features from multiple layers, one can simultaneouslyreason about high, mid, and low-level features during classification. Theresulting multi-scale architecture can itself be seen as a feed-forward modelthat is structured as a directed acyclic graph (DAG-CNNs). We use DAG-CNNs tolearn a set of multiscale features that can be effectively shared betweencoarse and fine-grained classification tasks. While fine-tuning such modelshelps performance, we show that even "off-the-self" multiscale features performquite well. We present extensive analysis and demonstrate state-of-the-artclassification performance on three standard scene benchmarks (SUN397, MIT67,and Scene15). In terms of the heavily benchmarked MIT67 and Scene15 datasets,our results reduce the lowest previously-reported error by 23.9% and 9.5%,respectively.

The Open World of Micro-Videos

  Micro-videos are six-second videos popular on social media networks withseveral unique properties. Firstly, because of the authoring process, theycontain significantly more diversity and narrative structure than existingcollections of video "snippets". Secondly, because they are often captured byhand-held mobile cameras, they contain specialized viewpoints includingthird-person, egocentric, and self-facing views seldom seen in traditionalproduced video. Thirdly, due to to their continuous production and publicationon social networks, aggregate micro-video content contains interestingopen-world dynamics that reflects the temporal evolution of tag topics. Theseaspects make micro-videos an appealing well of visual data for developinglarge-scale models for video understanding. We analyze a novel dataset ofmicro-videos labeled with 58 thousand tags. To analyze this data, we introduceviewpoint-specific and temporally-evolving models for video understanding,defined over state-of-the-art motion and deep visual features. We conclude thatour dataset opens up new research opportunities for large-scale video analysis,novel viewpoints, and open-world dynamics.

PixelNet: Towards a General Pixel-level Architecture

  We explore architectures for general pixel-level prediction problems, fromlow-level edge detection to mid-level surface normal estimation to high-levelsemantic segmentation. Convolutional predictors, such as thefully-convolutional network (FCN), have achieved remarkable success byexploiting the spatial redundancy of neighboring pixels through convolutionalprocessing. Though computationally efficient, we point out that such approachesare not statistically efficient during learning precisely because spatialredundancy limits the information learned from neighboring pixels. Wedemonstrate that (1) stratified sampling allows us to add diversity duringbatch updates and (2) sampled multi-scale features allow us to explore morenonlinear predictors (multiple fully-connected layers followed by ReLU) thatimprove overall accuracy. Finally, our objective is to show how a architecturecan get performance better than (or comparable to) the architectures designedfor a particular task. Interestingly, our single architecture producesstate-of-the-art results for semantic segmentation on PASCAL-Context, surfacenormal estimation on NYUDv2 dataset, and edge detection on BSDS withoutcontextual post-processing.

3D Human Pose Estimation = 2D Pose Estimation + Matching

  We explore 3D human pose estimation from a single RGB image. While manyapproaches try to directly predict 3D pose from image measurements, we explorea simple architecture that reasons through intermediate 2D pose predictions.Our approach is based on two key observations (1) Deep neural nets haverevolutionized 2D pose estimation, producing accurate 2D predictions even forposes with self occlusions. (2) Big-data sets of 3D mocap data are now readilyavailable, making it tempting to lift predicted 2D poses to 3D through simplememorization (e.g., nearest neighbors). The resulting architecture is trivialto implement with off-the-shelf 2D pose estimation systems and 3D mocaplibraries. Importantly, we demonstrate that such methods outperform almost allstate-of-the-art 3D pose estimation systems, most of which directly try toregress 3D pose from 2D measurements.

PixelNet: Representation of the pixels, by the pixels, and for the  pixels

  We explore design principles for general pixel-level prediction problems,from low-level edge detection to mid-level surface normal estimation tohigh-level semantic segmentation. Convolutional predictors, such as thefully-convolutional network (FCN), have achieved remarkable success byexploiting the spatial redundancy of neighboring pixels through convolutionalprocessing. Though computationally efficient, we point out that such approachesare not statistically efficient during learning precisely because spatialredundancy limits the information learned from neighboring pixels. Wedemonstrate that stratified sampling of pixels allows one to (1) add diversityduring batch updates, speeding up learning; (2) explore complex nonlinearpredictors, improving accuracy; and (3) efficiently train state-of-the-artmodels tabula rasa (i.e., "from scratch") for diverse pixel-labeling tasks. Oursingle architecture produces state-of-the-art results for semantic segmentationon PASCAL-Context dataset, surface normal estimation on NYUDv2 depth dataset,and edge detection on BSDS.

ActionVLAD: Learning spatio-temporal aggregation for action  classification

  In this work, we introduce a new video representation for actionclassification that aggregates local convolutional features across the entirespatio-temporal extent of the video. We do so by integrating state-of-the-arttwo-stream networks with learnable spatio-temporal feature aggregation. Theresulting architecture is end-to-end trainable for whole-video classification.We investigate different strategies for pooling across space and time andcombining signals from the different streams. We find that: (i) it is importantto pool jointly across space and time, but (ii) appearance and motion streamsare best aggregated into their own separate representations. Finally, we showthat our representation outperforms the two-stream base architecture by a largemargin (13% relative) as well as out-performs other baselines with comparablebase architectures on HMDB51, UCF101, and Charades video classificationbenchmarks.

Tracking as Online Decision-Making: Learning a Policy from Streaming  Videos with Reinforcement Learning

  We formulate tracking as an online decision-making process, where a trackingagent must follow an object despite ambiguous image frames and a limitedcomputational budget. Crucially, the agent must decide where to look in theupcoming frames, when to reinitialize because it believes the target has beenlost, and when to update its appearance model for the tracked object. Suchdecisions are typically made heuristically. Instead, we propose to learn anoptimal decision-making policy by formulating tracking as a partiallyobservable decision-making process (POMDP). We learn policies with deepreinforcement learning algorithms that need supervision (a reward signal) onlywhen the track has gone awry. We demonstrate that sparse rewards allow us toquickly train on massive datasets, several orders of magnitude more than pastwork. Interestingly, by treating the data source of Internet videos asunlimited streams, we both learn and evaluate our trackers in a single, unifiedcomputational stream.

PixelNN: Example-based Image Synthesis

  We present a simple nearest-neighbor (NN) approach that synthesizeshigh-frequency photorealistic images from an "incomplete" signal such as alow-resolution image, a surface normal map, or edges. Current state-of-the-artdeep generative models designed for such conditional image synthesis lack twoimportant things: (1) they are unable to generate a large set of diverseoutputs, due to the mode collapse problem. (2) they are not interpretable,making it difficult to control the synthesized output. We demonstrate that NNapproaches potentially address such limitations, but suffer in accuracy onsmall datasets. We design a simple pipeline that combines the best of bothworlds: the first stage uses a convolutional neural network (CNN) to maps theinput to a (overly-smoothed) image, and the second stage uses a pixel-wisenearest neighbor method to map the smoothed output to multiple high-quality,high-frequency outputs in a controllable manner. We demonstrate our approachfor various input modalities, and for various domains ranging from human facesto cats-and-dogs to shoes and handbags.

Attentional Pooling for Action Recognition

  We introduce a simple yet surprisingly powerful model to incorporateattention in action recognition and human object interaction tasks. Ourproposed attention module can be trained with or without extra supervision, andgives a sizable boost in accuracy while keeping the network size andcomputational cost nearly the same. It leads to significant improvements overstate of the art base architecture on three standard action recognitionbenchmarks across still images and videos, and establishes new state of the arton MPII dataset with 12.5% relative improvement. We also perform an extensiveanalysis of our attention module both empirically and analytically. In terms ofthe latter, we introduce a novel derivation of bottom-up and top-down attentionas low-rank approximations of bilinear pooling methods (typically used forfine-grained classification). From this perspective, our attention formulationsuggests a novel characterization of action recognition as a fine-grainedrecognition problem.

Patch Correspondences for Interpreting Pixel-level CNNs

  We present compositional nearest neighbors (CompNN), a simple approach tovisually interpreting distributed representations learned by a convolutionalneural network (CNN) for pixel-level tasks (e.g., image synthesis andsegmentation). It does so by reconstructing both a CNN's input and output imageby copy-pasting corresponding patches from the training set with similarfeature embeddings. To do so efficiently, it makes of a patch-match-basedalgorithm that exploits the fact that the patch representations learned by aCNN for pixel level tasks vary smoothly. Finally, we show that CompNN can beused to establish semantic correspondences between two images and controlproperties of the output image by modifying the images contained in thetraining set. We present qualitative and quantitative experiments for semanticsegmentation and image-to-image translation that demonstrate that CompNN is agood tool for interpreting the embeddings learned by pixel-level CNNs.

Active Learning with Partial Feedback

  In the large-scale multiclass setting, assigning labels often consists ofanswering multiple questions to drill down through a hierarchy of classes.Here, the labor required per annotation scales with the number of questionsasked. We propose active learning with partial feedback. In this setup, thelearner asks the annotator if a chosen example belongs to a (possiblycomposite) chosen class. The answer eliminates some classes, leaving the agentwith a partial label. Success requires (i) a sampling strategy to choose(example, class) pairs, and (ii) learning from partial labels. Experiments onthe TinyImageNet dataset demonstrate that our most effective method achieves a26% relative improvement (8.1% absolute) in top1 classification accuracy for a250k (or 30%) binary question budget, compared to a naive baseline. Our workmay also impact traditional data annotation. For example, our best method fullyannotates TinyImageNet with only 482k (with EDC though, ERC is 491) binaryquestions (vs 827k for naive method).

Cross-Domain Image Matching with Deep Feature Maps

  We investigate the problem of automatically determining what type of shoeleft an impression found at a crime scene. This recognition problem is madedifficult by the variability in types of crime scene evidence (ranging fromtraces of dust or oil on hard surfaces to impressions made in soil) and thelack of comprehensive databases of shoe outsole tread patterns. We find thatmid-level features extracted by pre-trained convolutional neural nets aresurprisingly effective descriptors for this specialized domains. However, thechoice of similarity measure for matching exemplars to a query image isessential to good performance. For matching multi-channel deep features, wepropose the use of multi-channel normalized cross-correlation and analyze itseffectiveness. Our proposed metric significantly improves performance inmatching crime scene shoeprints to laboratory test impressions. We also showits effectiveness in other cross-domain image retrieval problems: matchingfacade images to segmentation labels and aerial photos to map images. Finally,we introduce a discriminatively trained variant and fine-tune our systemthrough our proposed metric, obtaining state-of-the-art performance.

Active Testing: An Efficient and Robust Framework for Estimating  Accuracy

  Much recent work on visual recognition aims to scale up learning to massive,noisily-annotated datasets. We address the problem of scaling- up theevaluation of such models to large-scale datasets with noisy labels. Currentprotocols for doing so require a human user to either vet (re-annotate) a smallfraction of the test set and ignore the rest, or else correct errors inannotation as they are found through manual inspection of results. In thiswork, we re-formulate the problem as one of active testing, and examinestrategies for efficiently querying a user so as to obtain an accu- rateperformance estimate with minimal vetting. We demonstrate the effectiveness ofour proposed active testing framework on estimating two performance metrics,Precision@K and mean Average Precision, for two popular computer vision tasks,multi-label classification and instance segmentation. We further show that ourapproach is able to save significant human annotation effort and is more robustthan alternative evaluation protocols.

Recycle-GAN: Unsupervised Video Retargeting

  We introduce a data-driven approach for unsupervised video retargeting thattranslates content from one domain to another while preserving the style nativeto a domain, i.e., if contents of John Oliver's speech were to be transferredto Stephen Colbert, then the generated content/speech should be in StephenColbert's style. Our approach combines both spatial and temporal informationalong with adversarial losses for content translation and style preservation.In this work, we first study the advantages of using spatiotemporal constraintsover spatial constraints for effective retargeting. We then demonstrate theproposed approach for the problems where information in both space and timematters such as face-to-face translation, flower-to-flower, wind and cloudsynthesis, sunrise and sunset.

DistInit: Learning Video Representations without a Single Labeled Video

  Video recognition models have progressed significantly over the past fewyears, evolving from shallow classifiers trained on hand-crafted features todeep spatiotemporal networks. However, labeled video data required to trainsuch models has not been able to keep up with the ever increasing depth andsophistication of these networks. In this work we propose an alternativeapproach to learning video representations that requires no semanticallylabeled videos, and instead leverages the years of effort in collecting andlabeling large and clean still-image datasets. We do so by usingstate-of-the-art models pre-trained on image datasets as "teachers" to trainvideo models in a distillation framework. We demonstrate that our method learnstruly spatiotemporal features, despite being trained only using supervisionfrom still-image networks. Moreover, it learns good representations acrossdifferent input modalities, using completely uncurated raw video data sourcesand with different 2D teacher models. Our method obtains strong transferperformance, outperforming standard techniques for bootstrapping videoarchitectures from image-based models and obtains competitive performance withstate-of-the-art approaches for video action recognition.

Towards Segmenting Everything That Moves

  Video analysis is the task of perceiving the world as it changes. Often,though, most of the world doesn't change all that much: it's boring. For manyapplications such as action detection or robotic interaction, segmenting allmoving objects is a crucial first step. While this problem has beenwell-studied in the field of spatiotemporal segmentation, virtually none of theprior works use learning-based approaches, despite significant advances insingle-frame instance segmentation. We propose the first deep-learning basedapproach for video instance segmentation. Our two-stream models' architectureis based on Mask R-CNN, but additionally takes optical flow as input toidentify moving objects. It then combines the motion and appearance cues tocorrect motion estimation mistakes and capture the full extent of objects. Weshow state-of-the-art results on the Freiburg Berkeley Motion Segmentationdataset by a wide margin. One potential worry with learning-based methods isthat they might overfit to the particular type of objects that they have beentrained on. While current recognition systems tend to be limited to a "closedworld" of N objects on which they are trained, our model seems to segmentalmost anything that moves.

Microsoft COCO: Common Objects in Context

  We present a new dataset with the goal of advancing the state-of-the-art inobject recognition by placing the question of object recognition in the contextof the broader question of scene understanding. This is achieved by gatheringimages of complex everyday scenes containing common objects in their naturalcontext. Objects are labeled using per-instance segmentations to aid in preciseobject localization. Our dataset contains photos of 91 objects types that wouldbe easily recognizable by a 4 year old. With a total of 2.5 million labeledinstances in 328k images, the creation of our dataset drew upon extensive crowdworker involvement via novel user interfaces for category detection, instancespotting and instance segmentation. We present a detailed statistical analysisof the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we providebaseline performance analysis for bounding box and segmentation detectionresults using a Deformable Parts Model.

3D Hand Pose Detection in Egocentric RGB-D Images

  We focus on the task of everyday hand pose estimation from egocentricviewpoints. For this task, we show that depth sensors are particularlyinformative for extracting near-field interactions of the camera wearer withhis/her environment. Despite the recent advances in full-body pose estimationusing Kinect-like sensors, reliable monocular hand pose estimation in RGB-Dimages is still an unsolved problem. The problem is considerably exacerbatedwhen analyzing hands performing daily activities from a first-person viewpoint,due to severe occlusions arising from object manipulations and a limitedfield-of-view. Our system addresses these difficulties by exploiting strongpriors over viewpoint and pose in a discriminative tracking-by-detectionframework. Our priors are operationalized through a photorealistic syntheticmodel of egocentric scenes, which is used to generate training data forlearning depth-based pose classifiers. We evaluate our approach on an annotateddataset of real egocentric object manipulation scenes and compare to bothcommercial and academic approaches. Our method provides state-of-the-artperformance for both hand detection and pose estimation in egocentric RGB-Dimages.

Bottom-Up and Top-Down Reasoning with Hierarchical Rectified Gaussians

  Convolutional neural nets (CNNs) have demonstrated remarkable performance inrecent history. Such approaches tend to work in a unidirectional bottom-upfeed-forward fashion. However, practical experience and biological evidencetells us that feedback plays a crucial role, particularly for detailed spatialunderstanding tasks. This work explores bidirectional architectures that alsoreason with top-down feedback: neural units are influenced by both lower andhigher-level units.  We do so by treating units as rectified latent variables in a quadraticenergy function, which can be seen as a hierarchical Rectified Gaussian model(RGs). We show that RGs can be optimized with a quadratic program (QP), thatcan in turn be optimized with a recurrent neural network (with rectified linearunits). This allows RGs to be trained with GPU-optimized gradient descent. Froma theoretical perspective, RGs help establish a connection between CNNs andhierarchical probabilistic models. From a practical perspective, RGs are wellsuited for detailed spatial tasks that can benefit from top-down reasoning. Weillustrate them on the challenging task of keypoint localization underocclusions, where local bottom-up evidence may be misleading. We demonstratestate-of-the-art results on challenging benchmarks.

Finding Tiny Faces

  Though tremendous strides have been made in object recognition, one of theremaining open challenges is detecting small objects. We explore three aspectsof the problem in the context of finding small faces: the role of scaleinvariance, image resolution, and contextual reasoning. While most recognitionapproaches aim to be scale-invariant, the cues for recognizing a 3px tall faceare fundamentally different than those for recognizing a 300px tall face. Wetake a different approach and train separate detectors for different scales. Tomaintain efficiency, detectors are trained in a multi-task fashion: they makeuse of features extracted from multiple layers of single (deep) featurehierarchy. While training detectors for large objects is straightforward, thecrucial challenge remains training detectors for small objects. We show thatcontext is crucial, and define templates that make use of massively-largereceptive fields (where 99% of the template extends beyond the object ofinterest). Finally, we explore the role of scale in pre-trained deep networks,providing ways to extrapolate networks tuned for limited scales to ratherextreme ranges. We demonstrate state-of-the-art results onmassively-benchmarked face datasets (FDDB and WIDER FACE). In particular, whencompared to prior art on WIDER FACE, our results reduce error by a factor of 2(our models produce an AP of 82% while prior art ranges from 29-64%).

Tinkering Under the Hood: Interactive Zero-Shot Learning with Net  Surgery

  We consider the task of visual net surgery, in which a CNN can bereconfigured without extra data to recognize novel concepts that may be omittedfrom the training set. While most prior work make use of linguistic cues forsuch "zero-shot" learning, we do so by using a pictorial languagerepresentation of the training set, implicitly learned by a CNN, to generalizeto new classes. To this end, we introduce a set of visualization techniquesthat better reveal the activation patterns and relations between groups of CNNfilters. We next demonstrate that knowledge of pictorial languages can be usedto rewire certain CNN neurons into a part model, which we call a pictoriallanguage classifier. We demonstrate the robustness of simple PLCs by applyingthem in a weakly supervised manner: labeling unlabeled concepts for visualclasses present in the training data. Specifically we show that a PLC built ontop of a CNN trained for ImageNet classification can localize humans in Graz-02and determine the pose of birds in PASCAL-VOC without extra labeled data oradditional training. We then apply PLCs in an interactive zero-shot manner,demonstrating that pictorial languages are expressive enough to detect a set ofvisual classes in MS-COCO that never appear in the ImageNet training set.

Need for Speed: A Benchmark for Higher Frame Rate Object Tracking

  In this paper, we propose the first higher frame rate video dataset (calledNeed for Speed - NfS) and benchmark for visual object tracking. The datasetconsists of 100 videos (380K frames) captured with now commonly availablehigher frame rate (240 FPS) cameras from real world scenarios. All frames areannotated with axis aligned bounding boxes and all sequences are manuallylabelled with nine visual attributes - such as occlusion, fast motion,background clutter, etc. Our benchmark provides an extensive evaluation of manyrecent and state-of-the-art trackers on higher frame rate sequences. We rankedeach of these trackers according to their tracking accuracy and real-timeperformance. One of our surprising conclusions is that at higher frame rates,simple trackers such as correlation filters outperform complex methods based ondeep networks. This suggests that for practical applications (such as inrobotics or embedded vision), one needs to carefully tradeoff bandwidthconstraints associated with higher frame rate acquisition, computational costsof real-time analysis, and the required application accuracy. Our dataset andbenchmark allows for the first time (to our knowledge) systematic explorationof such issues, and will be made available to allow for further research inthis space.

Expecting the Unexpected: Training Detectors for Unusual Pedestrians  with Adversarial Imposters

  As autonomous vehicles become an every-day reality, high-accuracy pedestriandetection is of paramount practical importance. Pedestrian detection is ahighly researched topic with mature methods, but most datasets focus on commonscenes of people engaged in typical walking poses on sidewalks. But performanceis most crucial for dangerous scenarios, such as children playing in the streetor people using bicycles/skateboards in unexpected ways. Such "in-the-tail"data is notoriously hard to observe, making both training and testingdifficult. To analyze this problem, we have collected a novel annotated datasetof dangerous scenarios called the Precarious Pedestrian dataset. Even given adedicated collection effort, it is relatively small by contemporary standards(around 1000 images). To allow for large-scale data-driven learning, we explorethe use of synthetic data generated by a game engine. A significant challengeis selected the right "priors" or parameters for synthesis: we would likerealistic data with poses and object configurations that mimic true PrecariousPedestrians. Inspired by Generative Adversarial Networks (GANs), we generate amassive amount of synthetic data and train a discriminative classifier toselect a realistic subset, which we deem the Adversarial Imposters. Wedemonstrate that this simple pipeline allows one to synthesize realistictraining data by making use of rendering/animation engines within a GANframework. Interestingly, we also demonstrate that such data can be used torank algorithms, suggesting that Adversarial Imposters can also be used for"in-the-tail" validation at test-time, a notoriously difficult challenge forreal-world deployment.

Predictive-Corrective Networks for Action Detection

  While deep feature learning has revolutionized techniques for static-imageunderstanding, the same does not quite hold for video processing. Architecturesand optimization techniques used for video are largely based off those forstatic images, potentially underutilizing rich video information. In this work,we rethink both the underlying network architecture and the stochastic learningparadigm for temporal data. To do so, we draw inspiration from classic theoryon linear dynamic systems for modeling time series. By extending such models toinclude nonlinear mappings, we derive a series of novel recurrent neuralnetworks that sequentially make top-down predictions about the future and thencorrect those predictions with bottom-up observations. Predictive-correctivenetworks have a number of desirable properties: (1) they can adaptively focuscomputation on "surprising" frames where predictions require large corrections,(2) they simplify learning in that only "residual-like" corrective terms needto be learned over time and (3) they naturally decorrelate an input data streamin a hierarchical fashion, producing a more reliable signal for learning ateach layer of a network. We provide an extensive analysis of our lightweightand interpretable framework, and demonstrate that our model is competitive withthe two-stream network on three challenging datasets without the need forcomputationally expensive optical flow.

Comparing Apples and Oranges: Off-Road Pedestrian Detection on the NREC  Agricultural Person-Detection Dataset

  Person detection from vehicles has made rapid progress recently with theadvent of multiple highquality datasets of urban and highway driving, yet nolarge-scale benchmark is available for the same problem in off-road oragricultural environments. Here we present the NREC AgriculturalPerson-Detection Dataset to spur research in these environments. It consists oflabeled stereo video of people in orange and apple orchards taken from twoperception platforms (a tractor and a pickup truck), along with vehicleposition data from RTK GPS. We define a benchmark on part of the dataset thatcombines a total of 76k labeled person images and 19k sampled person-freeimages. The dataset highlights several key challenges of the domain, includingvarying environment, substantial occlusion by vegetation, people in motion andin non-standard poses, and people seen from a variety of distances; meta-dataare included to allow targeted evaluation of each of these effects. Finally, wepresent baseline detection performance results for three leading approachesfrom urban pedestrian detection and our own convolutional neural networkapproach that benefits from the incorporation of additional image context. Weshow that the success of existing approaches on urban data does not transferdirectly to this domain.

Unconstrained Face Detection and Open-Set Face Recognition Challenge

  Face detection and recognition benchmarks have shifted toward more difficultenvironments. The challenge presented in this paper addresses the next step inthe direction of automatic detection and identification of people from outdoorsurveillance cameras. While face detection has shown remarkable success inimages collected from the web, surveillance cameras include more diverseocclusions, poses, weather conditions and image blur. Although faceverification or closed-set face identification have surpassed humancapabilities on some datasets, open-set identification is much more complex asit needs to reject both unknown identities and false accepts from the facedetector. We show that unconstrained face detection can approach high detectionrates albeit with moderate false accept rates. By contrast, open-set facerecognition is currently weak and requires much more attention.

Learning Policies for Adaptive Tracking with Deep Feature Cascades

  Visual object tracking is a fundamental and time-critical vision task. Recentyears have seen many shallow tracking methods based on real-time pixel-basedcorrelation filters, as well as deep methods that have top performance but needa high-end GPU. In this paper, we learn to improve the speed of deep trackerswithout losing accuracy. Our fundamental insight is to take an adaptiveapproach, where easy frames are processed with cheap features (such as pixelvalues), while challenging frames are processed with invariant but expensivedeep features. We formulate the adaptive tracking problem as a decision-makingprocess, and learn an agent to decide whether to locate objects with highconfidence on an early layer, or continue processing subsequent layers of anetwork. This significantly reduces the feed-forward cost for easy frames withdistinct or slow-moving objects. We train the agent offline in a reinforcementlearning fashion, and further demonstrate that learning all deep layers (so asto provide good features for adaptive tracking) can lead to near real-timeaverage tracking speed of 23 fps on a single CPU while achievingstate-of-the-art performance. Perhaps most tellingly, our approach provides a100X speedup for almost 50% of the time, indicating the power of an adaptiveapproach.

Brute-Force Facial Landmark Analysis With A 140,000-Way Classifier

  We propose a simple approach to visual alignment, focusing on theillustrative task of facial landmark estimation. While most prior work treatsthis as a regression problem, we instead formulate it as a discrete $K$-wayclassification task, where a classifier is trained to return one of $K$discrete alignments. One crucial benefit of a classifier is the ability toreport back a (softmax) distribution over putative alignments. We demonstratethat this distribution is a rich representation that can be marginalized (togenerate uncertainty estimates over groups of landmarks) and conditioned on (toincorporate top-down context, provided by temporal constraints in a videostream or an interactive human user). Such capabilities are difficult tointegrate into classic regression-based approaches. We study performance as afunction of the number of classes $K$, including the extreme "exemplar class"setting where $K$ is equal to the number of training examples (140K in oursetting). Perhaps surprisingly, we show that classifiers can still be learnedin this setting. When compared to prior work in classification, our $K$ isunprecedentedly large, including many "fine-grained" classes that are verysimilar. We address these issues by using a multi-label loss function thatallows for training examples to be non-uniformly shared across discreteclasses. We perform a comprehensive experimental analysis of our method onstandard benchmarks, demonstrating state-of-the-art results for facialalignment in videos.

Online Model Distillation for Efficient Video Inference

  High-quality computer vision models typically address the problem ofunderstanding the general distribution of real-world images. However, mostcameras observe only a very small fraction of this distribution. This offersthe possibility of achieving more efficient inference by specializing compact,low-cost models to the specific distribution of frames observed by a singlecamera. In this paper, we employ the technique of model distillation(supervising a low-cost student model using the output of a high-cost teacher)to specialize accurate, low-cost semantic segmentation models to a target videostream. Rather than learn a specialized student model on offline data from thevideo stream, we train the student in an online fashion on the live video,intermittently running the teacher to provide a target for learning. Onlinemodel distillation yields semantic segmentation models that closely approximatetheir Mask R-CNN teacher with 7 to 17x lower inference runtime cost (11 to 26xin FLOPs), even when the target video's distribution is non-stationary. Ourmethod requires no offline pretraining on the target video stream, and achieveshigher accuracy and lower cost than solutions based on flow or video objectsegmentation. We also provide a new video dataset for evaluating the efficiencyof inference over long running video streams.

Photo-Sketching: Inferring Contour Drawings from Images

  Edges, boundaries and contours are important subjects of study in bothcomputer graphics and computer vision. On one hand, they are the 2D elementsthat convey 3D shapes, on the other hand, they are indicative of occlusionevents and thus separation of objects or semantic concepts. In this paper, weaim to generate contour drawings, boundary-like drawings that capture theoutline of the visual scene. Prior art often cast this problem as boundarydetection. However, the set of visual cues presented in the boundary detectionoutput are different from the ones in contour drawings, and also the artisticstyle is ignored. We address these issues by collecting a new dataset ofcontour drawings and proposing a learning-based method that resolves diversityin the annotation and, unlike boundary detectors, can work with imperfectalignment of the annotation and the actual ground truth. Our method surpassesprevious methods quantitatively and qualitatively. Surprisingly, when our modelfine-tunes on BSDS500, we achieve the state-of-the-art performance in salientboundary detection, suggesting contour drawing might be a scalable alternativeto boundary annotation, which at the same time is easier and more interestingfor annotators to draw.

