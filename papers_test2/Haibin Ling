StructVIO : Visual-inertial Odometry with Structural Regularity of  Man-made Environments

  We propose a novel visual-inertial odometry approach that adopts structuralregularity in man-made environments. Instead of using Manhattan worldassumption, we use Atlanta world model to describe such regularity. An Atlantaworld is a world that contains multiple local Manhattan worlds with differentheading directions. Each local Manhattan world is detected on-the-fly, andtheir headings are gradually refined by the state estimator when newobservations are coming. With fully exploration of structural lines thataligned with each local Manhattan worlds, our visual-inertial odometry methodbecome more accurate and robust, as well as much more flexible to differentkinds of complex man-made environments. Through extensive benchmark tests andreal-world tests, the results show that the proposed approach outperformsexisting visual-inertial systems in large-scale man-made environments

A Comparative Study of Object Trackers for Infrared Flying Bird Tracking

  Bird strikes present a huge risk for aircraft, especially since traditionalairport bird surveillance is mainly dependent on inefficient human observation.Computer vision based technology has been proposed to automatically detectbirds, determine bird flying trajectories, and predict aircraft takeoff delays.However, the characteristics of bird flight using imagery and the performanceof existing methods applied to flying bird task are not well known. Therefore,we perform infrared flying bird tracking experiments using 12 state-of-the-artalgorithms on a real BIRDSITE-IR dataset to obtain useful clues and recommendfeature analysis. We also develop a Struck-scale method to demonstrate theeffectiveness of multiple scale sampling adaption in handling the object offlying bird with varying shape and scale. The general analysis can be used todevelop specialized bird tracking methods for airport safety, wildness andurban bird population studies.

SANet: Structure-Aware Network for Visual Tracking

  Convolutional neural network (CNN) has drawn increasing interest in visualtracking owing to its powerfulness in feature extraction. Most existingCNN-based trackers treat tracking as a classification problem. However, thesetrackers are sensitive to similar distractors because their CNN models mainlyfocus on inter-class classification. To address this problem, we useself-structure information of object to distinguish it from distractors.Specifically, we utilize recurrent neural network (RNN) to model objectstructure, and incorporate it into CNN to improve its robustness to similardistractors. Considering that convolutional layers in different levelscharacterize the object from different perspectives, we use multiple RNNs tomodel object structure in different levels respectively. Extensive experimentson three benchmarks, OTB100, TC-128 and VOT2015, show that the proposedalgorithm outperforms other methods. Code is released athttp://www.dabi.temple.edu/~hbling/code/SANet/SANet.html.

Planar Object Tracking in the Wild: A Benchmark

  Planar object tracking is an actively studied problem in vision-based roboticapplications. While several benchmarks have been constructed for evaluatingstate-of-the-art algorithms, there is a lack of video sequences captured in thewild rather than in constrained laboratory environment. In this paper, wepresent a carefully designed planar object tracking benchmark containing 210videos of 30 planar objects sampled in the natural environment. In particular,for each object, we shoot seven videos involving various challenging factors,namely scale change, rotation, perspective distortion, motion blur, occlusion,out-of-view, and unconstrained. The ground truth is carefully annotatedsemi-manually to ensure the quality. Moreover, eleven state-of-the-artalgorithms are evaluated on the benchmark using two evaluation metrics, withdetailed analysis provided for the evaluation results. We expect the proposedbenchmark to benefit future studies on planar object tracking.

Dense Recurrent Neural Networks for Scene Labeling

  Recently recurrent neural networks (RNNs) have demonstrated the ability toimprove scene labeling through capturing long-range dependencies among imageunits. In this paper, we propose dense RNNs for scene labeling by exploringvarious long-range semantic dependencies among image units. In comparison withexisting RNN based approaches, our dense RNNs are able to capture richercontextual dependencies for each image unit via dense connections between eachpair of image units, which significantly enhances their discriminative power.Besides, to select relevant and meanwhile restrain irrelevant dependencies foreach unit from dense connections, we introduce an attention model into denseRNNs. The attention model enables automatically assigning more importance tohelpful dependencies while less weight to unconcerned dependencies. Integratingwith convolutional neural networks (CNNs), our method achieves state-of-the-artperformances on the PASCAL Context, MIT ADE20K and SiftFlow benchmarks.

A Single-shot-per-pose Camera-Projector Calibration System For Imperfect  Planar Targets

  Existing camera-projector calibration methods typically warp feature pointsfrom a camera image to a projector image using estimated homographies, andoften suffer from errors in camera parameters and noise due to imperfectplanarity of the calibration target. In this paper we propose a simple yetrobust solution that explicitly deals with these challenges. Following thestructured light (SL) camera-project calibration framework, a carefullydesigned correspondence algorithm is built on top of the De Bruijn patterns.Such correspondence is then used for initial camera-projector calibration.Then, to gain more robustness against noises, especially those from animperfect planar calibration board, a bundle adjustment algorithm is developedto jointly optimize the estimated camera and projector models. Aside from therobustness, our solution requires only one shot of SL pattern for eachcalibration board pose, which is much more convenient than multi-shot solutionsin practice. Data validations are conducted on both synthetic and realdatasets, and our method shows clear advantages over existing methods in allexperiments.

Vision Meets Drones: A Challenge

  In this paper we present a large-scale visual object detection and trackingbenchmark, named VisDrone2018, aiming at advancing visual understanding taskson the drone platform. The images and video sequences in the benchmark werecaptured over various urban/suburban areas of 14 different cities across Chinafrom north to south. Specifically, VisDrone2018 consists of 263 video clips and10,209 images (no overlap with video clips) with rich annotations, includingobject bounding boxes, object categories, occlusion, truncation ratios, etc.With intensive amount of effort, our benchmark has more than 2.5 millionannotated instances in 179,264 images/video frames. Being the largest suchdataset ever published, the benchmark enables extensive evaluation andinvestigation of visual analysis algorithms on the drone platform. Inparticular, we design four popular tasks with the benchmark, including objectdetection in images, object detection in videos, single object tracking, andmulti-object tracking. All these tasks are extremely challenging in theproposed dataset due to factors such as occlusion, large scale and posevariation, and fast motion. We hope the benchmark largely boost the researchand development in visual analysis on drone platforms.

Privacy-Protective-GAN for Face De-identification

  Face de-identification has become increasingly important as the image sourcesare explosively growing and easily accessible. The advance of new facerecognition techniques also arises people's concern regarding the privacyleakage. The mainstream pipelines of face de-identification are mostly based onthe k-same framework, which bears critiques of low effectiveness and poorvisual quality. In this paper, we propose a new framework calledPrivacy-Protective-GAN (PP-GAN) that adapts GAN with novel verificator andregulator modules specially designed for the face de-identification problem toensure generating de-identified output with retained structure similarityaccording to a single input. We evaluate the proposed approach in terms ofprivacy protection, utility preservation, and structure similarity. Ourapproach not only outperforms existing face de-identification techniques butalso provides a practical framework of adapting GAN with priors of domainknowledge.

Scene Parsing via Dense Recurrent Neural Networks with Attentional  Selection

  Recurrent neural networks (RNNs) have shown the ability to improve sceneparsing through capturing long-range dependencies among image units. In thispaper, we propose dense RNNs for scene labeling by exploring various long-rangesemantic dependencies among image units. Different from existing RNN basedapproaches, our dense RNNs are able to capture richer contextual dependenciesfor each image unit by enabling immediate connections between each pair ofimage units, which significantly enhances their discriminative power. Besides,to select relevant dependencies and meanwhile to restrain irrelevant ones foreach unit from dense connections, we introduce an attention model into denseRNNs. The attention model allows automatically assigning more importance tohelpful dependencies while less weight to unconcerned dependencies. Integratingwith convolutional neural networks (CNNs), we develop an end-to-end scenelabeling system. Extensive experiments on three large-scale benchmarksdemonstrate that the proposed approach can improve the baselines by largemargins and outperform other state-of-the-art algorithms.

Feature Pyramid and Hierarchical Boosting Network for Pavement Crack  Detection

  Pavement crack detection is a critical task for insuring road safety. Manualcrack detection is extremely time-consuming. Therefore, an automatic road crackdetection method is required to boost this progress. However, it remains achallenging task due to the intensity inhomogeneity of cracks and complexity ofthe background, e.g., the low contrast with surrounding pavements and possibleshadows with similar intensity. Inspired by recent advances of deep learning incomputer vision, we propose a novel network architecture, named Feature Pyramidand Hierarchical Boosting Network (FPHBN), for pavement crack detection. Theproposed network integrates semantic information to low-level features forcrack detection in a feature pyramid way. And, it balances the contribution ofboth easy and hard samples to loss by nested sample reweighting in ahierarchical way. To demonstrate the superiority and generality of the proposedmethod, we evaluate the proposed method on five crack datasets and compare itwith state-of-the-art crack detection, edge detection, semantic segmentationmethods. Extensive experiments show that the proposed method outperforms thesestate-of-the-art methods in terms of accuracy and generality.

Generic Multiview Visual Tracking

  Recent progresses in visual tracking have greatly improved the trackingperformance. However, challenges such as occlusion and view change remainobstacles in real world deployment. A natural solution to these challenges isto use multiple cameras with multiview inputs, though existing systems aremostly limited to specific targets (e.g. human), static cameras, and/or cameracalibration. To break through these limitations, we propose a generic multiviewtracking (GMT) framework that allows camera movement, while requiring neitherspecific object model nor camera calibration. A key innovation in our frameworkis a cross-camera trajectory prediction network (TPN), which implicitly anddynamically encodes camera geometric relations, and hence addresses missingtarget issues such as occlusion. Moreover, during tracking, we assembleinformation across different cameras to dynamically update a novelcollaborative correlation filter (CCF), which is shared among cameras toachieve robustness against view change. The two components are integrated intoa correlation filter tracking framework, where the features are trained offlineusing existing single view tracking datasets. For evaluation, we firstcontribute a new generic multiview tracking dataset (GMTD) with carefulannotations, and then run experiments on GMTD and the PETS2009 datasets. Onboth datasets, the proposed GMT algorithm shows clear advantages overstate-of-the-art ones.

FAMNet: Joint Learning of Feature, Affinity and Multi-dimensional  Assignment for Online Multiple Object Tracking

  Data association-based multiple object tracking (MOT) involves multipleseparated modules processed or optimized differently, which results in complexmethod design and requires non-trivial tuning of parameters. In this paper, wepresent an end-to-end model, named FAMNet, where Feature extraction, Affinityestimation and Multi-dimensional assignment are refined in a single network.All layers in FAMNet are designed differentiable thus can be optimized jointlyto learn the discriminative features and higher-order affinity model for robustMOT, which is supervised by the loss directly from the assignment ground truth.We also integrate single object tracking technique and a dedicated targetmanagement scheme into the FAMNet-based tracking system to further recoverfalse negatives and inhibit noisy target candidates generated by the externaldetector. The proposed method is evaluated on a diverse set of benchmarksincluding MOT2015, MOT2017, KITTI-Car and UA-DETRAC, and achieves promisingperformance on all of them in comparison with state-of-the-arts.

Adaptive Objectness for Object Tracking

  Object tracking is a long standing problem in vision. While great effortshave been spent to improve tracking performance, a simple yet reliable priorknowledge is left unexploited: the target object in tracking must be an objectother than non-object. The recently proposed and popularized objectness measureprovides a natural way to model such prior in visual tracking. Thus motivated,in this paper we propose to adapt objectness for visual object tracking.Instead of directly applying an existing objectness measure that is generic andhandles various objects and environments, we adapt it to be compatible to thespecific tracking sequence and object. More specifically, we use the newlyproposed BING objectness as the base, and then train an object-adaptiveobjectness for each tracking task. The training is implemented by using anadaptive support vector machine that integrates information from the specifictracking target into the BING measure. We emphasize that the benefit of theproposed adaptive objectness, named ADOBING, is generic. To show this, wecombine ADOBING with seven top performed trackers in recent evaluations. We runthe ADOBING-enhanced trackers with their base trackers on two popularbenchmarks, the CVPR2013 benchmark (50 sequences) and the Princeton TrackingBenchmark (100 sequences). On both benchmarks, our methods not onlyconsistently improve the base trackers, but also achieve the best knownperformances. Noting that the way we integrate objectness in visual tracking isgeneric and straightforward, we expect even more improvement by usingtracker-specific objectness.

DeepSaliency: Multi-Task Deep Neural Network Model for Salient Object  Detection

  A key problem in salient object detection is how to effectively model thesemantic properties of salient objects in a data-driven manner. In this paper,we propose a multi-task deep saliency model based on a fully convolutionalneural network (FCNN) with global input (whole raw images) and global output(whole saliency maps). In principle, the proposed saliency model takes adata-driven strategy for encoding the underlying saliency prior information,and then sets up a multi-task learning scheme for exploring the intrinsiccorrelations between saliency detection and semantic image segmentation.Through collaborative feature learning from such two correlated tasks, theshared fully convolutional layers produce effective features for objectperception. Moreover, it is capable of capturing the semantic information onsalient objects across different levels using the fully convolutional layers,which investigate the feature-sharing properties of salient object detectionwith great feature redundancy reduction. Finally, we present a graph Laplacianregularized nonlinear regression model for saliency refinement. Experimentalresults demonstrate the effectiveness of our approach in comparison with thestate-of-the-art approaches.

A Richly Annotated Dataset for Pedestrian Attribute Recognition

  In this paper, we aim to improve the dataset foundation for pedestrianattribute recognition in real surveillance scenarios. Recognition of humanattributes, such as gender, and clothes types, has great prospects in realapplications. However, the development of suitable benchmark datasets forattribute recognition remains lagged behind. Existing human attribute datasetsare collected from various sources or an integration of pedestrianre-identification datasets. Such heterogeneous collection poses a big challengeon developing high quality fine-grained attribute recognition algorithms.Furthermore, human attribute recognition are generally severely affected byenvironmental or contextual factors, such as viewpoints, occlusions and bodyparts, while existing attribute datasets barely care about them. To tacklethese problems, we build a Richly Annotated Pedestrian (RAP) dataset from realmulti-camera surveillance scenarios with long term collection, where datasamples are annotated with not only fine-grained human attributes but alsoenvironmental and contextual factors. RAP has in total 41,585 pedestriansamples, each of which is annotated with 72 attributes as well as viewpoints,occlusions, body parts information. To our knowledge, the RAP dataset is thelargest pedestrian attribute dataset, which is expected to greatly promote thestudy of large-scale attribute recognition systems. Furthermore, we empiricallyanalyze the effects of different environmental and contextual factors onpedestrian attribute recognition. Experimental results demonstrate thatviewpoints, occlusions and body parts information could assist attributerecognition a lot in real applications.

Multi-level Contextual RNNs with Attention Model for Scene Labeling

  Context in image is crucial for scene labeling while existing methods onlyexploit local context generated from a small surrounding area of an image patchor a pixel, by contrast long-range and global contextual information isignored. To handle this issue, we in this work propose a novel approach forscene labeling by exploring multi-level contextual recurrent neural networks(ML-CRNNs). Specifically, we encode three kinds of contextual cues, i.e., localcontext, global context and image topic context in structural recurrent neuralnetworks (RNNs) to model long-range local and global dependencies in image. Inthis way, our method is able to `see' the image in terms of both long-rangelocal and holistic views, and make a more reliable inference for imagelabeling. Besides, we integrate the proposed contextual RNNs into hierarchicalconvolutional neural networks (CNNs), and exploit dependence relationships inmultiple levels to provide rich spatial and semantic information. Moreover, wenovelly adopt an attention model to effectively merge multiple levels and showthat it outperforms average- or max-pooling fusion strategies. Extensiveexperiments demonstrate that the proposed approach achieves newstate-of-the-art results on the CamVid, SiftFlow and Stanford-backgrounddatasets.

Transductive Zero-Shot Learning with a Self-training dictionary approach

  As an important and challenging problem in computer vision, zero-shotlearning (ZSL) aims at automatically recognizing the instances from unseenobject classes without training data. To address this problem, ZSL is usuallycarried out in the following two aspects: 1) capturing the domain distributionconnections between seen classes data and unseen classes data; and 2) modelingthe semantic interactions between the image feature space and the labelembedding space. Motivated by these observations, we propose a bidirectionalmapping based semantic relationship modeling scheme that seeks for crossmodalknowledge transfer by simultaneously projecting the image features and labelembeddings into a common latent space. Namely, we have a bidirectionalconnection relationship that takes place from the image feature space to thelatent space as well as from the label embedding space to the latent space. Todeal with the domain shift problem, we further present a transductive learningapproach that formulates the class prediction problem in an iterative refiningprocess, where the object classification capacity is progressively reinforcedthrough bootstrapping-based model updating over highly reliable instances.Experimental results on three benchmark datasets (AwA, CUB and SUN) demonstratethe effectiveness of the proposed approach against the state-of-the-artapproaches.

Parallel Tracking and Verifying: A Framework for Real-Time and High  Accuracy Visual Tracking

  Being intensively studied, visual tracking has seen great recent advances ineither speed (e.g., with correlation filters) or accuracy (e.g., with deepfeatures). Real-time and high accuracy tracking algorithms, however, remainscarce. In this paper we study the problem from a new perspective and present anovel parallel tracking and verifying (PTAV) framework, by taking advantage ofthe ubiquity of multi-thread techniques and borrowing from the success ofparallel tracking and mapping in visual SLAM. Our PTAV framework typicallyconsists of two components, a tracker T and a verifier V, working in parallelon two separate threads. The tracker T aims to provide a super real-timetracking inference and is expected to perform well most of the time; bycontrast, the verifier V checks the tracking results and corrects T whenneeded. The key innovation is that, V does not work on every frame but onlyupon the requests from T; on the other end, T may adjust the tracking accordingto the feedback from V. With such collaboration, PTAV enjoys both the highefficiency provided by T and the strong discriminative power by V. In ourextensive experiments on popular benchmarks including OTB2013, OTB2015, TC128and UAV20L, PTAV achieves the best tracking accuracy among all real-timetrackers, and in fact performs even better than many deep learning basedsolutions. Moreover, as a general framework, PTAV is very flexible and hasgreat rooms for improvement and generalization.

Parallel Tracking and Verifying

  Being intensively studied, visual object tracking has witnessed greatadvances in either speed (e.g., with correlation filters) or accuracy (e.g.,with deep features). Real-time and high accuracy tracking algorithms, however,remain scarce. In this paper we study the problem from a new perspective andpresent a novel parallel tracking and verifying (PTAV) framework, by takingadvantage of the ubiquity of multi-thread techniques and borrowing ideas fromthe success of parallel tracking and mapping in visual SLAM. The proposed PTAVframework is typically composed of two components, a (base) tracker T and averifier V, working in parallel on two separate threads. The tracker T aims toprovide a super real-time tracking inference and is expected to perform wellmost of the time; by contrast, the verifier V validates the tracking resultsand corrects T when needed. The key innovation is that, V does not work onevery frame but only upon the requests from T; on the other end, T may adjustthe tracking according to the feedback from V. With such collaboration, PTAVenjoys both the high efficiency provided by T and the strong discriminativepower by V. Meanwhile, to adapt V to object appearance changes over time, wemaintain a dynamic target template pool for adaptive verification, resulting infurther performance improvements. In our extensive experiments on popularbenchmarks including OTB2015, TC128, UAV20L and VOT2016, PTAV achieves the besttracking accuracy among all real-time trackers, and in fact even outperformsmany deep learning based algorithms. Moreover, as a general framework, PTAV isvery flexible with great potentials for future improvement and generalization.

Weighted Bilinear Coding over Salient Body Parts for Person  Re-identification

  Deep convolutional neural networks (CNNs) have demonstrated dominantperformance in person re-identification (Re-ID). Existing CNN based methodsutilize global average pooling (GAP) to aggregate intermediate convolutionalfeatures for Re-ID. However, this strategy only considers the first-orderstatistics of local features and treats local features at different locationsequally important, leading to sub-optimal feature representation. To deal withthese issues, we propose a novel \emph{weighted bilinear coding} (WBC) modelfor local feature aggregation in CNN networks to pursue more representative anddiscriminative feature representations. In specific, bilinear coding is used toencode the channel-wise feature correlations to capture richer featureinteractions. Meanwhile, a weighting scheme is applied on the bilinear codingto adaptively adjust the weights of local features at different locations basedon their importance in recognition, further improving the discriminability offeature aggregation. To handle the spatial misalignment issue, we use a salientpart net to derive salient body parts, and apply the WBC model on each part.The final representation, formed by concatenating the WBC eoncoded features ofeach part, is both discriminative and resistant to spatial misalignment.Experiments on three benchmarks including Market-1501, DukeMTMC-reID and CUHK03evidence the favorable performance of our method against other state-of-the-artmethods.

Graph Correspondence Transfer for Person Re-identification

  In this paper, we propose a graph correspondence transfer (GCT) approach forperson re-identification. Unlike existing methods, the GCT model formulatesperson re-identification as an off-line graph matching and on-linecorrespondence transferring problem. In specific, during training, the GCTmodel aims to learn off-line a set of correspondence templates from positivetraining pairs with various pose-pair configurations via patch-wise graphmatching. During testing, for each pair of test samples, we select a fewtraining pairs with the most similar pose-pair configurations as references,and transfer the correspondences of these references to test pair for featuredistance calculation. The matching score is derived by aggregating distancesfrom different references. For each probe image, the gallery image with thehighest matching score is the re-identifying result. Compared to existingalgorithms, our GCT can handle spatial misalignment caused by large variationsin view angles and human poses owing to the benefits of patch-wise graphmatching. Extensive experiments on five benchmarks including VIPeR, Road,PRID450S, 3DPES and CUHK01 evidence the superior performance of GCT model overother state-of-the-art methods.

MTFH: A Matrix Tri-Factorization Hashing Framework for Efficient  Cross-Modal Retrieval

  Hashing has recently sparked a great revolution in cross-modal retrieval dueto its low storage cost and high query speed. Most existing cross-modal hashingmethods learn unified hash codes in a common Hamming space to represent allmulti-modal data and make them intuitively comparable. However, such unifiedhash codes could inherently sacrifice their representation scalability becausethe data from different modalities may not have one-to-one correspondence andcould be stored more efficiently by different hash codes of unequal lengths. Tomitigate this problem, this paper proposes a generalized and flexiblecross-modal hashing framework, termed Matrix Tri-Factorization Hashing (MTFH),which not only preserves the semantic similarity between the multi-modal datapoints, but also works seamlessly in various settings including paired orunpaired multi-modal data, and equal or varying hash length encoding scenarios.Specifically, MTFH exploits an efficient objective function to jointly learnthe flexible modality-specific hash codes with different length settings, whilesimultaneously excavating two semantic correlation matrices to ensureheterogeneous data comparable. As a result, the derived hash codes are moresemantically meaningful for various challenging cross-modal retrieval tasks.Extensive experiments evaluated on public benchmark datasets highlight thesuperiority of MTFH under various retrieval scenarios and show its verycompetitive performance with the state-of-the-arts.

LaSOT: A High-quality Benchmark for Large-scale Single Object Tracking

  In this paper, we present LaSOT, a high-quality benchmark for Large-scaleSingle Object Tracking. LaSOT consists of 1,400 sequences with more than 3.5Mframes in total. Each frame in these sequences is carefully and manuallyannotated with a bounding box, making LaSOT the largest, to the best of ourknowledge, densely annotated tracking benchmark. The average video length ofLaSOT is more than 2,500 frames, and each sequence comprises various challengesderiving from the wild where target objects may disappear and re-appear againin the view. By releasing LaSOT, we expect to provide the community with alarge-scale dedicated benchmark with high quality for both the training of deeptrackers and the veritable evaluation of tracking algorithms. Moreover,considering the close connections of visual appearance and natural language, weenrich LaSOT by providing additional language specification, aiming atencouraging the exploration of natural linguistic feature for tracking. Athorough experimental evaluation of 35 tracking algorithms on LaSOT ispresented with detailed analysis, and the results demonstrate that there isstill a big room for improvements.

Siamese Cascaded Region Proposal Networks for Real-Time Visual Tracking

  Region proposal networks (RPN) have been recently combined with the Siamesenetwork for tracking, and shown excellent accuracy with high efficiency.Nevertheless, previously proposed one-stage Siamese-RPN trackers degenerate inpresence of similar distractors and large scale variation. Addressing theseissues, we propose a multi-stage tracking framework, Siamese Cascaded RPN(C-RPN), which consists of a sequence of RPNs cascaded from deep high-level toshallow low-level layers in a Siamese network. Compared to previous solutions,C-RPN has several advantages: (1) Each RPN is trained using the outputs of RPNin the previous stage. Such process stimulates hard negative sampling,resulting in more balanced training samples. Consequently, the RPNs aresequentially more discriminative in distinguishing difficult background (i.e.,similar distractors). (2) Multi-level features are fully leveraged through anovel feature transfer block (FTB) for each RPN, further improving thediscriminability of C-RPN using both high-level semantic and low-level spatialinformation. (3) With multiple steps of regressions, C-RPN progressivelyrefines the location and shape of the target in each RPN with adjusted anchorboxes in the previous stage, which makes localization more accurate. C-RPN istrained end-to-end with the multi-task loss function. In inference, C-RPN isdeployed as it is, without any temporal adaption, for real-time tracking. Inextensive experiments on OTB-2013, OTB-2015, VOT-2016, VOT-2017, LaSOT andTrackingNet, C-RPN consistently achieves state-of-the-art results and runs inreal-time.

Online Multi-Object Tracking with Instance-Aware Tracker and Dynamic  Model Refreshment

  Recent progresses in model-free single object tracking (SOT) algorithms havelargely inspired applying SOT to \emph{multi-object tracking} (MOT) to improvethe robustness as well as relieving dependency on external detector. However,SOT algorithms are generally designed for distinguishing a target from itsenvironment, and hence meet problems when a target is spatially mixed withsimilar objects as observed frequently in MOT. To address this issue, in thispaper we propose an instance-aware tracker to integrate SOT techniques for MOTby encoding awareness both within and between target models. In particular, weconstruct each target model by fusing information for distinguishing targetboth from background and other instances (tracking targets). To conserveuniqueness of all target models, our instance-aware tracker considers responsemaps from all target models and assigns spatial locations exclusively tooptimize the overall accuracy. Another contribution we make is a dynamic modelrefreshing strategy learned by a convolutional neural network. This strategyhelps to eliminate initialization noise as well as to adapt to the variation oftarget size and appearance. To show the effectiveness of the proposed approach,it is evaluated on the popular MOT15 and MOT16 challenge benchmarks. On bothbenchmarks, our approach achieves the best overall performances in comparisonwith published results.

PFLD: A Practical Facial Landmark Detector

  Being accurate, efficient, and compact is essential to a facial landmarkdetector for practical use. To simultaneously consider the three concerns, thispaper investigates a neat model with promising detection accuracy under wildenvironments e.g., unconstrained pose, expression, lighting, and occlusionconditions) and super real-time speed on a mobile device. More concretely, wecustomize an end-to-end single stage network associated with accelerationtechniques. During the training phase, for each sample, rotation information isestimated for geometrically regularizing landmark localization, which is thenNOT involved in the testing phase. A novel loss is designed to, besidesconsidering the geometrical regularization, mitigate the issue of dataimbalance by adjusting weights of samples to different states, such as largepose, extreme lighting, and occlusion, in the training set. Extensiveexperiments are conducted to demonstrate the efficacy of our design and revealits superior performance over state-of-the-art alternatives on widely-adoptedchallenging benchmarks, i.e., 300W (including iBUG, LFPW, AFW, HELEN, andXM2VTS) and AFLW. Our model can be merely 2.1Mb of size and reach over 140 fpsper face on a mobile phone (Qualcomm ARM 845 processor) with high precision,making it attractive for large-scale or real-time applications. We have madeour practical system based on PFLD 0.25X model publicly available at\url{http://sites.google.com/view/xjguo/fld} for encouraging comparisons andimprovements from the community.

End-to-end Projector Photometric Compensation

  Projector photometric compensation aims to modify a projector input imagesuch that it can compensate for disturbance from the appearance of projectionsurface. In this paper, for the first time, we formulate the compensationproblem as an end-to-end learning problem and propose a convolutional neuralnetwork, named CompenNet, to implicitly learn the complex compensationfunction. CompenNet consists of a UNet-like backbone network and an autoencodersubnet. Such architecture encourages rich multi-level interactions between thecamera-captured projection surface image and the input image, and thus capturesboth photometric and environment information of the projection surface. Inaddition, the visual details and interaction information are carried to deeperlayers along the multi-level skip convolution layers. The architecture is ofparticular importance for the projector compensation task, for which only asmall training dataset is allowed in practice. Another contribution we make isa novel evaluation benchmark, which is independent of system setup and thusquantitatively verifiable. Such benchmark is not previously available, to ourbest knowledge, due to the fact that conventional evaluation requests thehardware system to actually project the final results. Our key idea, motivatedfrom our end-to-end problem formulation, is to use a reasonable surrogate toavoid such projection process so as to be setup-independent. Our method isevaluated carefully on the benchmark, and the results show that our end-to-endlearning solution outperforms state-of-the-arts both qualitatively andquantitatively by a significant margin.

M2Det: A Single-Shot Object Detector based on Multi-Level Feature  Pyramid Network

  Feature pyramids are widely exploited by both the state-of-the-art one-stageobject detectors (e.g., DSSD, RetinaNet, RefineDet) and the two-stage objectdetectors (e.g., Mask R-CNN, DetNet) to alleviate the problem arising fromscale variation across object instances. Although these object detectors withfeature pyramids achieve encouraging results, they have some limitations due tothat they only simply construct the feature pyramid according to the inherentmulti-scale, pyramidal architecture of the backbones which are actuallydesigned for object classification task. Newly, in this work, we present amethod called Multi-Level Feature Pyramid Network (MLFPN) to construct moreeffective feature pyramids for detecting objects of different scales. First, wefuse multi-level features (i.e. multiple layers) extracted by backbone as thebase feature. Second, we feed the base feature into a block of alternatingjoint Thinned U-shape Modules and Feature Fusion Modules and exploit thedecoder layers of each u-shape module as the features for detecting objects.Finally, we gather up the decoder layers with equivalent scales (sizes) todevelop a feature pyramid for object detection, in which every feature mapconsists of the layers (features) from multiple levels. To evaluate theeffectiveness of the proposed MLFPN, we design and train a powerful end-to-endone-stage object detector we call M2Det by integrating it into the architectureof SSD, which gets better detection performance than state-of-the-art one-stagedetectors. Specifically, on MS-COCO benchmark, M2Det achieves AP of 41.0 atspeed of 11.8 FPS with single-scale inference strategy and AP of 44.2 withmulti-scale inference strategy, which is the new state-of-the-art results amongone-stage detectors. The code will be made available on\url{https://github.com/qijiezhao/M2Det.

Robust and Efficient Graph Correspondence Transfer for Person  Re-identification

  Spatial misalignment caused by variations in poses and viewpoints is one ofthe most critical issues that hinders the performance improvement in existingperson re-identification (Re-ID) algorithms. To address this problem, in thispaper, we present a robust and efficient graph correspondence transfer (REGCT)approach for explicit spatial alignment in Re-ID. Specifically, we propose toestablish the patch-wise correspondences of positive training pairs via graphmatching. By exploiting both spatial and visual contexts of human appearance ingraph matching, meaningful semantic correspondences can be obtained. Tocircumvent the cumbersome \emph{on-line} graph matching in testing phase, wepropose to transfer the \emph{off-line} learned patch-wise correspondences fromthe positive training pairs to test pairs. In detail, for each test pair, thetraining pairs with similar pose-pair configurations are selected asreferences. The matching patterns (i.e., the correspondences) of the selectedreferences are then utilized to calculate the patch-wise feature distances ofthis test pair. To enhance the robustness of correspondence transfer, we designa novel pose context descriptor to accurately model human body configurations,and present an approach to measure the similarity between a pair of posecontext descriptors. Meanwhile, to improve testing efficiency, we propose acorrespondence template ensemble method using the voting mechanism, whichsignificantly reduces the amount of patch-wise matchings involved in distancecalculation. With aforementioned strategies, the REGCT model can effectivelyand efficiently handle the spatial misalignment problem in Re-ID. Extensiveexperiments on five challenging benchmarks, including VIPeR, Road, PRID450S,3DPES and CUHK01, evidence the superior performance of REGCT over otherstate-of-the-art approaches.

