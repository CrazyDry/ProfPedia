Synchronous Context-Free Grammars and Optimal Linear Parsing Strategies

  Synchronous Context-Free Grammars (SCFGs), also known as syntax-directedtranslation schemata, are unlike context-free grammars in that they do not havea binary normal form. In general, parsing with SCFGs takes space and timepolynomial in the length of the input strings, but with the degree of thepolynomial depending on the permutations of the SCFG rules. We consider linearparsing strategies, which add one nonterminal at a time. We show that for agiven input permutation, the problems of finding the linear parsing strategywith the minimum space and time complexity are both NP-hard.

Human languages order information efficiently

  Most languages use the relative order between words to encode meaningrelations. Languages differ, however, in what orders they use and how theseorders are mapped onto different meanings. We test the hypothesis that, despitethese differences, human languages might constitute different `solutions' tocommon pressures of language use. Using Monte Carlo simulations over data fromfive languages, we find that their word orders are efficient for processing interms of both dependency length and local lexical probability. This suggeststhat biases originating in how the brain understands language stronglyconstrain how human languages change over generations.

Feature-based Decipherment for Large Vocabulary Machine Translation

  Orthographic similarities across languages provide a strong signal forprobabilistic decipherment, especially for closely related language pairs. Theexisting decipherment models, however, are not well-suited for exploiting theseorthographic similarities. We propose a log-linear model with latent variablesthat incorporates orthographic similarity features. Maximum likelihood trainingis computationally expensive for the proposed log-linear model. To address thischallenge, we perform approximate inference via MCMC sampling and contrastivedivergence. Our results show that the proposed log-linear model withcontrastive divergence scales to large vocabularies and outperforms theexisting generative decipherment models by exploiting the orthographicfeatures.

Exploring phrase-compositionality in skip-gram models

  In this paper, we introduce a variation of the skip-gram model which jointlylearns distributed word vector representations and their way of composing toform phrase embeddings. In particular, we propose a learning procedure thatincorporates a phrase-compositionality function which can capture how we wantto compose phrases vectors from their component word vectors. Our experimentsshow improvement in word and phrase similarity tasks as well as syntactic taskslike dependency parsing using the proposed joint models.

AMR-to-text generation as a Traveling Salesman Problem

  The task of AMR-to-text generation is to generate grammatical text thatsustains the semantic meaning for a given AMR graph. We at- tack the task byfirst partitioning the AMR graph into smaller fragments, and then generatingthe translation for each fragment, before finally deciding the order by solvingan asymmetric generalized traveling salesman problem (AGTSP). A Maximum Entropyclassifier is trained to estimate the traveling costs, and a TSP solver is usedto find the optimized solution. The final model reports a BLEU score of 22.44on the SemEval-2016 Task8 dataset.

AMR-to-text Generation with Synchronous Node Replacement Grammar

  This paper addresses the task of AMR-to-text generation by leveragingsynchronous node replacement grammar. During training, graph-to-string rulesare learned using a heuristic extraction algorithm. At test time, a graphtransducer is applied to collapse input AMRs and generate output sentences.Evaluated on SemEval-2016 Task 8, our method gives a BLEU score of 25.62, whichis the best reported so far.

Addressing the Data Sparsity Issue in Neural AMR Parsing

  Neural attention models have achieved great success in different NLP tasks.How- ever, they have not fulfilled their promise on the AMR parsing task due tothe data sparsity issue. In this paper, we de- scribe a sequence-to-sequencemodel for AMR parsing and present different ways to tackle the data sparsityproblem. We show that our methods achieve significant improvement over abaseline neural atten- tion model and our results are also compet- itiveagainst state-of-the-art systems that do not use extra linguistic resources.

Neural Transition-based Syntactic Linearization

  The task of linearization is to find a grammatical order given a set ofwords. Traditional models use statistical methods. Syntactic linearizationsystems, which generate a sentence along with its syntactic tree, have shownstate-of-the-art performance. Recent work shows that a multi-layer LSTMlanguage model outperforms competitive statistical syntactic linearizationsystems without using syntax. In this paper, we study neural syntacticlinearization, building a transition-based syntactic linearizer leveraging afeed-forward neural network, observing significantly better results compared toLSTM language models on this task.

Semantic Neural Machine Translation using AMR

  It is intuitive that semantic representations can be useful for machinetranslation, mainly because they can help in enforcing meaning preservation andhandling data sparsity (many sentences correspond to one meaning) of machinetranslation models. On the other hand, little work has been done on leveragingsemantics for neural machine translation (NMT). In this work, we study theusefulness of AMR (short for abstract meaning representation) on NMT.Experiments on a standard English-to-German dataset show that incorporating AMRas additional knowledge can significantly improve a strong attention-basedsequence-to-sequence neural translation model.

Convergence of the EM Algorithm for Gaussian Mixtures with Unbalanced  Mixing Coefficients

  The speed of convergence of the Expectation Maximization (EM) algorithm forGaussian mixture model fitting is known to be dependent on the amount ofoverlap among the mixture components. In this paper, we study the impact ofmixing coefficients on the convergence of EM. We show that when the mixturecomponents exhibit some overlap, the convergence of EM becomes slower as thedynamic range among the mixing coefficients increases. We propose adeterministic anti-annealing algorithm, that significantly improves the speedof convergence of EM for such mixtures with unbalanced mixing coefficients. Theproposed algorithm is compared against other standard optimization techniqueslike BFGS, Conjugate Gradient, and the traditional EM algorithm. Finally, wepropose a similar deterministic anti-annealing based algorithm for theDirichlet process mixture model and demonstrate its advantages over theconventional variational Bayesian approach.

Parsing Linear Context-Free Rewriting Systems with Fast Matrix  Multiplication

  We describe a matrix multiplication recognition algorithm for a subset ofbinary linear context-free rewriting systems (LCFRS) with running time$O(n^{\omega d})$ where $M(m) = O(m^{\omega})$ is the running time for $m\times m$ matrix multiplication and $d$ is the "contact rank" of the LCFRS --the maximal number of combination and non-combination points that appear in thegrammar rules. We also show that this algorithm can be used as a subroutine toget a recognition algorithm for general binary LCFRS with running time$O(n^{\omega d + 1})$. The currently best known $\omega$ is smaller than$2.38$. Our result provides another proof for the best known result for parsingmildly context sensitive formalisms such as combinatory categorial grammars,head grammars, linear indexed grammars, and tree adjoining grammars, which canbe parsed in time $O(n^{4.76})$. It also shows that inversion transductiongrammars can be parsed in time $O(n^{5.76})$. In addition, binary LCFRSsubsumes many other formalisms and types of grammars, for some of which we alsoimprove the asymptotic complexity of parsing.

Sense Embedding Learning for Word Sense Induction

  Conventional word sense induction (WSI) methods usually represent eachinstance with discrete linguistic features or cooccurrence features, and traina model for each polysemous word individually. In this work, we propose tolearn sense embeddings for the WSI task. In the training stage, our methodinduces several sense centroids (embedding) for each polysemous word. In thetesting stage, our method represents each instance as a contextual vector, andinduces its sense by finding the nearest sense centroid in the embedding space.The advantages of our method are (1) distributed sense vectors are taken as theknowledge representations which are trained discriminatively, and usually havebetter performance than traditional count-based distributional models, and (2)a general model for the whole vocabulary is jointly trained to induce sensecentroids under the mutlitask learning framework. Evaluated on SemEval-2010 WSIdataset, our method outperforms all participants and most of the recentstate-of-the-art methods. We further verify the two advantages by comparingwith carefully designed baselines.

A Graph-to-Sequence Model for AMR-to-Text Generation

  The problem of AMR-to-text generation is to recover a text representing thesame meaning as an input AMR graph. The current state-of-the-art method uses asequence-to-sequence model, leveraging LSTM for encoding a linearized AMRstructure. Although being able to model non-local semantic information, asequence LSTM can lose information from the AMR graph structure, and thus faceschallenges with large graphs, which result in long sequences. We introduce aneural graph-to-sequence model, using a novel LSTM structure for directlyencoding graph-level semantics. On a standard benchmark, our model showssuperior results to existing methods in the literature.

N-ary Relation Extraction using Graph State LSTM

  Cross-sentence $n$-ary relation extraction detects relations among $n$entities across multiple sentences. Typical methods formulate an input as a\textit{document graph}, integrating various intra-sentential andinter-sentential dependencies. The current state-of-the-art method splits theinput graph into two DAGs, adopting a DAG-structured LSTM for each. Thoughbeing able to model rich linguistic knowledge by leveraging graph edges,important information can be lost in the splitting procedure. We propose agraph-state LSTM model, which uses a parallel state to model each word,recurrently enriching state values via message passing. Compared with DAGLSTMs, our graph LSTM keeps the original graph structure, and speeds upcomputation by allowing more parallelization. On a standard benchmark, ourmodel shows the best result in the literature.

Exploring Graph-structured Passage Representation for Multi-hop Reading  Comprehension with Graph Neural Networks

  Multi-hop reading comprehension focuses on one type of factoid question,where a system needs to properly integrate multiple pieces of evidence tocorrectly answer a question. Previous work approximates global evidence withlocal coreference information, encoding coreference chains with DAG-styled GRUlayers within a gated-attention reader. However, coreference is limited inproviding information for rich inference. We introduce a new method for betterconnecting global evidence, which forms more complex graphs compared to DAGs.To perform evidence integration on our graphs, we investigate two recent graphneural networks, namely graph convolutional network (GCN) and graph recurrentnetwork (GRN). Experiments on two standard datasets show that richer globalinformation leads to better answers. Our method performs better than allpublished results on these datasets.

Automated Analysis and Prediction of Job Interview Performance

  We present a computational framework for automatically quantifying verbal andnonverbal behaviors in the context of job interviews. The proposed framework istrained by analyzing the videos of 138 interview sessions with 69internship-seeking undergraduates at the Massachusetts Institute of Technology(MIT). Our automated analysis includes facial expressions (e.g., smiles, headgestures, facial tracking points), language (e.g., word counts, topicmodeling), and prosodic information (e.g., pitch, intonation, and pauses) ofthe interviewees. The ground truth labels are derived by taking a weightedaverage over the ratings of 9 independent judges. Our framework canautomatically predict the ratings for interview traits such as excitement,friendliness, and engagement with correlation coefficients of 0.75 or higher,and can quantify the relative importance of prosody, language, and facialexpressions. By analyzing the relative feature weights learned by theregression models, our framework recommends to speak more fluently, use lessfiller words, speak as "we" (vs. "I"), use more unique words, and smile more.We also find that the students who were rated highly while answering the firstinterview question were also rated highly overall (i.e., first impressionmatters). Finally, our MIT Interview dataset will be made available to otherresearchers to further validate and expand our findings.

