Response to "Visual Dialogue without Vision or Dialogue" (Massiceti et  al., 2018)

  In a recent workshop paper, Massiceti et al. presented a baseline model andsubsequent critique of Visual Dialog (Das et al., CVPR 2017) that raises whatwe believe to be unfounded concerns about the dataset and evaluation. Thisarticle intends to rebut the critique and clarify potential confusions forpractitioners and future participants in the Visual Dialog challenge.

Lemotif: Abstract Visual Depictions of your Emotional States in Life

  We present Lemotif. Lemotif generates a motif for your emotional life. Youtell Lemotif a little bit about your day -- what were salient events or aspectsand how they made you feel. Lemotif will generate a lemotif -- a creativeabstract visual depiction of your emotions and their sources. Over time,Lemotif can create visual motifs to capture a summary of your emotional statesover arbitrary periods of time -- making patterns in your emotions and theirsources apparent, presenting opportunities to take actions, and measure theireffectiveness. The underlying principles in Lemotif are that the lemotif should(1) separate out the sources of the emotions, (2) depict these sourcesvisually, (3) depict the emotions visually, and (4) have a creative aspect tothem. We verify via human studies that each of these factors contributes to theproposed lemotifs being favored over corresponding baselines.

Collecting Image Description Datasets using Crowdsourcing

  We describe our two new datasets with images described by humans. Both thedatasets were collected using Amazon Mechanical Turk, a crowdsourcing platform.The two datasets contain significantly more descriptions per image than otherexisting datasets. One is based on a popular image description dataset calledthe UIUC Pascal Sentence Dataset, whereas the other is based on the AbstractScenes dataset con- taining images made from clipart objects. In this paper wedescribe our interfaces, analyze some properties of and show exampledescriptions from our two datasets.

Graph R-CNN for Scene Graph Generation

  We propose a novel scene graph generation model called Graph R-CNN, that isboth effective and efficient at detecting objects and their relations inimages. Our model contains a Relation Proposal Network (RePN) that efficientlydeals with the quadratic number of potential relations between objects in animage. We also propose an attentional Graph Convolutional Network (aGCN) thateffectively captures contextual information between objects and relations.Finally, we introduce a new evaluation metric that is more holistic andrealistic than existing metrics. We report state-of-the-art performance onscene graph generation as evaluated using both existing and our proposedmetrics.

Human-Machine CRFs for Identifying Bottlenecks in Holistic Scene  Understanding

  Recent trends in image understanding have pushed for holistic sceneunderstanding models that jointly reason about various tasks such as objectdetection, scene recognition, shape analysis, contextual reasoning, and localappearance based classifiers. In this work, we are interested in understandingthe roles of these different tasks in improved scene understanding, inparticular semantic segmentation, object detection and scene recognition.Towards this goal, we "plug-in" human subjects for each of the variouscomponents in a state-of-the-art conditional random field model. Comparisonsamong various hybrid human-machine CRFs give us indications of how much "headroom" there is to improve scene understanding by focusing research efforts onvarious individual tasks.

CIDEr: Consensus-based Image Description Evaluation

  Automatically describing an image with a sentence is a long-standingchallenge in computer vision and natural language processing. Due to recentprogress in object detection, attribute classification, action recognition,etc., there is renewed interest in this area. However, evaluating the qualityof descriptions has proven to be challenging. We propose a novel paradigm forevaluating image descriptions that uses human consensus. This paradigm consistsof three main parts: a new triplet-based method of collecting human annotationsto measure consensus, a new automated metric (CIDEr) that captures consensus,and two new datasets: PASCAL-50S and ABSTRACT-50S that contain 50 sentencesdescribing each image. Our simple metric captures human judgment of consensusbetter than existing metrics across sentences generated by various sources. Wealso evaluate five state-of-the-art image description approaches using this newprotocol and provide a benchmark for future comparisons. A version of CIDErnamed CIDEr-D is available as a part of MS COCO evaluation server to enablesystematic evaluation and benchmarking.

Don't Just Listen, Use Your Imagination: Leveraging Visual Common Sense  for Non-Visual Tasks

  Artificial agents today can answer factual questions. But they fall short onquestions that require common sense reasoning. Perhaps this is because mostexisting common sense databases rely on text to learn and represent knowledge.But much of common sense knowledge is unwritten - partly because it tends notto be interesting enough to talk about, and partly because some common sense isunnatural to articulate in text. While unwritten, it is not unseen. In thispaper we leverage semantic common sense knowledge learned from images - i.e.visual common sense - in two textual tasks: fill-in-the-blank and visualparaphrasing. We propose to "imagine" the scene behind the text, and leveragevisual cues from the "imagined" scenes in addition to textual cues whileanswering these questions. We imagine the scenes as a visual abstraction. Ourapproach outperforms a strong text-only baseline on these tasks. Our proposedtasks can serve as benchmarks to quantitatively evaluate progress in solvingtasks that go "beyond recognition". Our code and datasets are publiclyavailable.

Visual Word2Vec (vis-w2v): Learning Visually Grounded Word Embeddings  Using Abstract Scenes

  We propose a model to learn visually grounded word embeddings (vis-w2v) tocapture visual notions of semantic relatedness. While word embeddings trainedusing text have been extremely successful, they cannot uncover notions ofsemantic relatedness implicit in our visual world. For instance, although"eats" and "stares at" seem unrelated in text, they share semantics visually.When people are eating something, they also tend to stare at the food.Grounding diverse relations like "eats" and "stares at" into vision remainschallenging, despite recent progress in vision. We note that the visualgrounding of words depends on semantics, and not the literal pixels. We thususe abstract scenes created from clipart to provide the visual grounding. Wefind that the embeddings we learn capture fine-grained, visually groundednotions of semantic relatedness. We show improvements over text-only wordembeddings (word2vec) on three tasks: common-sense assertion classification,visual paraphrasing and text-based image retrieval. Our code and datasets areavailable online.

Counting Everyday Objects in Everyday Scenes

  We are interested in counting the number of instances of object classes innatural, everyday images. Previous counting approaches tackle the problem inrestricted domains such as counting pedestrians in surveillance videos. Countscan also be estimated from outputs of other vision tasks like object detection.In this work, we build dedicated models for counting designed to tackle thelarge variance in counts, appearances, and scales of objects found in naturalscenes. Our approach is inspired by the phenomenon of subitizing - the abilityof humans to make quick assessments of counts given a perceptual signal, forsmall count values. Given a natural scene, we employ a divide and conquerstrategy while incorporating context across the scene to adapt the subitizingidea to counting. Our approach offers consistent improvements over numerousbaseline approaches for counting on the PASCAL VOC 2007 and COCO datasets.Subsequently, we study how counting can be used to improve object detection. Wethen show a proof of concept application of our counting methods to the task ofVisual Question Answering, by studying the `how many?' questions in the VQA andCOCO-QA datasets.

Joint Unsupervised Learning of Deep Representations and Image Clusters

  In this paper, we propose a recurrent framework for Joint UnsupervisedLEarning (JULE) of deep representations and image clusters. In our framework,successive operations in a clustering algorithm are expressed as steps in arecurrent process, stacked on top of representations output by a ConvolutionalNeural Network (CNN). During training, image clusters and representations areupdated jointly: image clustering is conducted in the forward pass, whilerepresentation learning in the backward pass. Our key idea behind thisframework is that good representations are beneficial to image clustering andclustering results provide supervisory signals to representation learning. Byintegrating two processes into a single model with a unified weighted tripletloss and optimizing it end-to-end, we can obtain not only more powerfulrepresentations, but also more precise image clusters. Extensive experimentsshow that our method outperforms the state-of-the-art on image clusteringacross a variety of image datasets. Moreover, the learned representationsgeneralize well when transferred to other tasks.

Visual Storytelling

  We introduce the first dataset for sequential vision-to-language, and explorehow this data may be used for the task of visual storytelling. The firstrelease of this dataset, SIND v.1, includes 81,743 unique photos in 20,211sequences, aligned to both descriptive (caption) and story language. Weestablish several strong baselines for the storytelling task, and motivate anautomatic metric to benchmark progress. Modelling concrete description as wellas figurative and social language, as provided in this dataset and thestorytelling task, has the potential to move artificial intelligence from basicunderstandings of typical visual scenes towards more and more human-likeunderstanding of grounded event structure and subjective expression.

Leveraging Visual Question Answering for Image-Caption Ranking

  Visual Question Answering (VQA) is the task of taking as input an image and afree-form natural language question about the image, and producing an accurateanswer. In this work we view VQA as a "feature extraction" module to extractimage and caption representations. We employ these representations for the taskof image-caption ranking. Each feature dimension captures (imagines) whether afact (question-answer pair) could plausibly be true for the image and caption.This allows the model to interpret images and captions from a wide variety ofperspectives. We propose score-level and representation-level fusion models toincorporate VQA knowledge in an existing state-of-the-art VQA-agnosticimage-caption ranking model. We find that incorporating and reasoning aboutconsistency between images and captions significantly improves performance.Concretely, our model improves state-of-the-art on caption retrieval by 7.1%and on image retrieval by 4.4% on the MSCOCO dataset.

Hierarchical Question-Image Co-Attention for Visual Question Answering

  A number of recent works have proposed attention models for Visual QuestionAnswering (VQA) that generate spatial maps highlighting image regions relevantto answering the question. In this paper, we argue that in addition to modeling"where to look" or visual attention, it is equally important to model "whatwords to listen to" or question attention. We present a novel co-attentionmodel for VQA that jointly reasons about image and question attention. Inaddition, our model reasons about the question (and consequently the image viathe co-attention mechanism) in a hierarchical fashion via a novel 1-dimensionalconvolution neural networks (CNN). Our model improves the state-of-the-art onthe VQA dataset from 60.3% to 60.5%, and from 61.6% to 63.3% on the COCO-QAdataset. By using ResNet, the performance is further improved to 62.1% for VQAand 65.4% for COCO-QA.

Human Attention in Visual Question Answering: Do Humans and Deep  Networks Look at the Same Regions?

  We conduct large-scale studies on `human attention' in Visual QuestionAnswering (VQA) to understand where humans choose to look to answer questionsabout images. We design and test multiple game-inspired novelattention-annotation interfaces that require the subject to sharpen regions ofa blurred image to answer a question. Thus, we introduce the VQA-HAT (HumanATtention) dataset. We evaluate attention maps generated by state-of-the-artVQA models against human attention both qualitatively (via visualizations) andquantitatively (via rank-order correlation). Overall, our experiments show thatcurrent attention models in VQA do not seem to be looking at the same regionsas humans.

Human Attention in Visual Question Answering: Do Humans and Deep  Networks Look at the Same Regions?

  We conduct large-scale studies on `human attention' in Visual QuestionAnswering (VQA) to understand where humans choose to look to answer questionsabout images. We design and test multiple game-inspired novelattention-annotation interfaces that require the subject to sharpen regions ofa blurred image to answer a question. Thus, we introduce the VQA-HAT (HumanATtention) dataset. We evaluate attention maps generated by state-of-the-artVQA models against human attention both qualitatively (via visualizations) andquantitatively (via rank-order correlation). Overall, our experiments show thatcurrent attention models in VQA do not seem to be looking at the same regionsas humans.

Question Relevance in VQA: Identifying Non-Visual And False-Premise  Questions

  Visual Question Answering (VQA) is the task of answering natural-languagequestions about images. We introduce the novel problem of determining therelevance of questions to images in VQA. Current VQA models do not reason aboutwhether a question is even related to the given image (e.g. What is the capitalof Argentina?) or if it requires information from external resources to answercorrectly. This can break the continuity of a dialogue in human-machineinteraction. Our approaches for determining relevance are composed of twostages. Given an image and a question, (1) we first determine whether thequestion is visual or not, (2) if visual, we determine whether the question isrelevant to the given image or not. Our approaches, based on LSTM-RNNs, VQAmodel uncertainty, and caption-question similarity, are able to outperformstrong baselines on both relevance tasks. We also present human studies showingthat VQA models augmented with such question relevance reasoning are perceivedas more intelligent, reasonable, and human-like.

Analyzing the Behavior of Visual Question Answering Models

  Recently, a number of deep-learning based models have been proposed for thetask of Visual Question Answering (VQA). The performance of most models isclustered around 60-70%. In this paper we propose systematic methods to analyzethe behavior of these models as a first step towards recognizing theirstrengths and weaknesses, and identifying the most fruitful directions forprogress. We analyze two models, one each from two major classes of VQA models-- with-attention and without-attention and show the similarities anddifferences in the behavior of these models. We also analyze the winning entryof the VQA Challenge 2016.  Our behavior analysis reveals that despite recent progress, today's VQAmodels are "myopic" (tend to fail on sufficiently novel instances), often "jumpto conclusions" (converge on a predicted answer after 'listening' to just halfthe question), and are "stubborn" (do not change their answers across images).

Sort Story: Sorting Jumbled Images and Captions into Stories

  Temporal common sense has applications in AI tasks such as QA, multi-documentsummarization, and human-AI communication. We propose the task of sequencing --given a jumbled set of aligned image-caption pairs that belong to a story, thetask is to sort them such that the output sequence forms a coherent story. Wepresent multiple approaches, via unary (position) and pairwise (order)predictions, and their ensemble-based combinations, achieving strong results onthis task. We use both text-based and image-based features, which depictcomplementary improvements. Using qualitative examples, we demonstrate that ourmodels have learnt interesting aspects of temporal common sense.

Deep Learning the City : Quantifying Urban Perception At A Global Scale

  Computer vision methods that quantify the perception of urban environment areincreasingly being used to study the relationship between a city's physicalappearance and the behavior and health of its residents. Yet, the throughput ofcurrent methods is too limited to quantify the perception of cities across theworld. To tackle this challenge, we introduce a new crowdsourced datasetcontaining 110,988 images from 56 cities, and 1,170,000 pairwise comparisonsprovided by 81,630 online volunteers along six perceptual attributes: safe,lively, boring, wealthy, depressing, and beautiful. Using this data, we train aSiamese-like convolutional neural architecture, which learns from a jointclassification and ranking loss, to predict human judgments of pairwise imagecomparisons. Our results show that crowdsourcing combined with neural networkscan produce urban perception data at the global scale.

Measuring Machine Intelligence Through Visual Question Answering

  As machines have become more intelligent, there has been a renewed interestin methods for measuring their intelligence. A common approach is to proposetasks for which a human excels, but one which machines find difficult. However,an ideal task should also be easy to evaluate and not be easily gameable. Webegin with a case study exploring the recently popular task of image captioningand its limitations as a task for measuring machine intelligence. Analternative and more promising task is Visual Question Answering that tests amachine's ability to reason about language and vision. We describe a datasetunprecedented in size created for the task that contains over 760,000 humangenerated questions about images. Using around 10 million human generatedanswers, machines may be easily evaluated.

Towards Transparent AI Systems: Interpreting Visual Question Answering  Models

  Deep neural networks have shown striking progress and obtainedstate-of-the-art results in many AI research fields in the recent years.However, it is often unsatisfying to not know why they predict what they do. Inthis paper, we address the problem of interpreting Visual Question Answering(VQA) models. Specifically, we are interested in finding what part of the input(pixels in images or words in questions) the VQA model focuses on whileanswering the question. To tackle this problem, we use two visualizationtechniques -- guided backpropagation and occlusion -- to find important wordsin the question and important regions in the image. We then present qualitativeand quantitative analyses of these importance maps. We found that even withoutexplicit attention mechanisms, VQA models may sometimes be implicitly attendingto relevant regions in the image, and often to appropriate words in thequestion.

Grad-CAM: Why did you say that?

  We propose a technique for making Convolutional Neural Network (CNN)-basedmodels more transparent by visualizing input regions that are 'important' forpredictions -- or visual explanations. Our approach, called Gradient-weightedClass Activation Mapping (Grad-CAM), uses class-specific gradient informationto localize important regions. These localizations are combined with existingpixel-space visualizations to create a novel high-resolution andclass-discriminative visualization called Guided Grad-CAM. These methods helpbetter understand CNN-based models, including image captioning and visualquestion answering (VQA) models. We evaluate our visual explanations bymeasuring their ability to discriminate between classes, to inspire trust inhumans, and their correlation with occlusion maps. Grad-CAM provides a new wayto understand CNN-based models.  We have released code, an online demo hosted on CloudCV, and a full versionof this extended abstract.

Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image  Captioning

  Attention-based neural encoder-decoder frameworks have been widely adoptedfor image captioning. Most methods force visual attention to be active forevery generated word. However, the decoder likely requires little to no visualinformation from the image to predict non-visual words such as "the" and "of".Other words that may seem visual can often be predicted reliably just from thelanguage model e.g., "sign" after "behind a red stop" or "phone" following"talking on a cell". In this paper, we propose a novel adaptive attention modelwith a visual sentinel. At each time step, our model decides whether to attendto the image (and if so, to which regions) or to the visual sentinel. The modeldecides whether to attend to the image and where, in order to extractmeaningful information for sequential word generation. We test our method onthe COCO image captioning 2015 challenge dataset and Flickr30K. Our approachsets the new state-of-the-art by a significant margin.

Context-aware Captions from Context-agnostic Supervision

  We introduce an inference technique to produce discriminative context-awareimage captions (captions that describe differences between images or visualconcepts) using only generic context-agnostic training data (captions thatdescribe a concept or an image in isolation). For example, given images andcaptions of "siamese cat" and "tiger cat", we generate language that describesthe "siamese cat" in a way that distinguishes it from "tiger cat". Our keynovelty is that we show how to do joint inference over a language model that iscontext-agnostic and a listener which distinguishes closely-related concepts.We first apply our technique to a justification task, namely to describe why animage contains a particular fine-grained category as opposed to anotherclosely-related category of the CUB-200-2011 dataset. We then studydiscriminative image captioning to generate language that uniquely refers toone of two semantically-similar images in the COCO dataset. Evaluations withdiscriminative ground truth for justification and human studies fordiscriminative image captioning reveal that our approach outperforms baselinegenerative and speaker-listener approaches for discrimination.

LR-GAN: Layered Recursive Generative Adversarial Networks for Image  Generation

  We present LR-GAN: an adversarial image generation model which takes scenestructure and context into account. Unlike previous generative adversarialnetworks (GANs), the proposed GAN learns to generate image background andforegrounds separately and recursively, and stitch the foregrounds on thebackground in a contextually relevant manner to produce a complete naturalimage. For each foreground, the model learns to generate its appearance, shapeand pose. The whole model is unsupervised, and is trained in an end-to-endmanner with gradient descent methods. The experiments demonstrate that LR-GANcan generate more natural images with objects that are more human recognizablethan DCGAN.

Sound-Word2Vec: Learning Word Representations Grounded in Sounds

  To be able to interact better with humans, it is crucial for machines tounderstand sound - a primary modality of human perception. Previous works haveused sound to learn embeddings for improved generic textual similarityassessment. In this work, we treat sound as a first-class citizen, studyingdownstream textual tasks which require aural grounding. To this end, we proposesound-word2vec - a new embedding scheme that learns specialized word embeddingsgrounded in sounds. For example, we learn that two seemingly (semantically)unrelated concepts, like leaves and paper are similar due to the similarrustling sounds they make. Our embeddings prove useful in textual tasksrequiring aural reasoning like text-based sound retrieval and discovering foleysound effects (used in movies). Moreover, our embedding space capturesinteresting dependencies between words and onomatopoeia and outperforms priorwork on aurally-relevant word relatedness datasets such as AMEN and ASLex.

Punny Captions: Witty Wordplay in Image Descriptions

  Wit is a form of rich interaction that is often grounded in a specificsituation (e.g., a comment in response to an event). In this work, we attemptto build computational models that can produce witty descriptions for a givenimage. Inspired by a cognitive account of humor appreciation, we employlinguistic wordplay, specifically puns, in image descriptions. We develop twoapproaches which involve retrieving witty descriptions for a given image from alarge corpus of sentences, or generating them via an encoder-decoder neuralnetwork architecture. We compare our approach against meaningful baselineapproaches via human studies and show substantial improvements. We find thatwhen a human is subject to similar constraints as the model regarding wordusage and style, people vote the image descriptions generated by our model tobe slightly wittier than human-written witty descriptions. Unsurprisingly,humans are almost always wittier than the model when they are free to choosethe vocabulary, style, etc.

C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0  Dataset

  Visual Question Answering (VQA) has received a lot of attention over the pastcouple of years. A number of deep learning models have been proposed for thistask. However, it has been shown that these models are heavily driven bysuperficial correlations in the training data and lack compositionality -- theability to answer questions about unseen compositions of seen concepts. Thiscompositionality is desirable and central to intelligence. In this paper, wepropose a new setting for Visual Question Answering where the testquestion-answer pairs are compositionally novel compared to trainingquestion-answer pairs. To facilitate developing models under this setting, wepresent a new compositional split of the VQA v1.0 dataset, which we callCompositional VQA (C-VQA). We analyze the distribution of questions and answersin the C-VQA splits. Finally, we evaluate several existing VQA models underthis new setting and show that the performances of these models degrade by asignificant amount compared to the original VQA setting.

Cooperative Learning with Visual Attributes

  Learning paradigms involving varying levels of supervision have received alot of interest within the computer vision and machine learning communities.The supervisory information is typically considered to come from a humansupervisor -- a "teacher" figure. In this paper, we consider an alternatesource of supervision -- a "peer" -- i.e. a different machine. We introducecooperative learning, where two agents trying to learn the same visualconcepts, but in potentially different environments using different sources ofdata (sensors), communicate their current knowledge of these concepts to eachother. Given the distinct sources of data in both agents, the mode ofcommunication between the two agents is not obvious. We propose the use ofvisual attributes -- semantic mid-level visual properties such as furry,wooden, etc.-- as the mode of communication between the agents. Our experimentsin three domains -- objects, scenes, and animals -- demonstrate that ourproposed cooperative learning approach improves the performance of both agentsas compared to their performance if they were to learn in isolation. Ourapproach is particularly applicable in scenarios where privacy, security and/orbandwidth constraints restrict the amount and type of information the twoagents can exchange.

ParlAI: A Dialog Research Software Platform

  We introduce ParlAI (pronounced "par-lay"), an open-source software platformfor dialog research implemented in Python, available at http://parl.ai. Itsgoal is to provide a unified framework for sharing, training and testing ofdialog models, integration of Amazon Mechanical Turk for data collection, humanevaluation, and online/reinforcement learning; and a repository of machinelearning models for comparing with others' models, and improving upon existingarchitectures. Over 20 tasks are supported in the first release, includingpopular datasets such as SQuAD, bAbI tasks, MCTest, WikiQA, QACNN, QADailyMail,CBT, bAbI Dialog, Ubuntu, OpenSubtitles and VQA. Several models are integrated,including neural models such as memory networks, seq2seq and attentive LSTMs.

Deal or No Deal? End-to-End Learning for Negotiation Dialogues

  Much of human dialogue occurs in semi-cooperative settings, where agents withdifferent goals attempt to agree on common decisions. Negotiations requirecomplex communication and reasoning skills, but success is easy to measure,making this an interesting task for AI. We gather a large dataset ofhuman-human negotiations on a multi-issue bargaining task, where agents whocannot observe each other's reward functions must reach an agreement (or adeal) via natural language dialogue. For the first time, we show it is possibleto train end-to-end models for negotiation, which must learn both linguisticand reasoning skills with no annotated dialogue states. We also introducedialogue rollouts, in which the model plans ahead by simulating possiblecomplete continuations of the conversation, and find that this techniquedramatically improves performance. Our code and dataset are publicly available(https://github.com/facebookresearch/end-to-end-negotiator).

Active Learning for Visual Question Answering: An Empirical Study

  We present an empirical study of active learning for Visual QuestionAnswering, where a deep VQA model selects informative question-image pairs froma pool and queries an oracle for answers to maximally improve its performanceunder a limited query budget. Drawing analogies from human learning, we explorecramming (entropy), curiosity-driven (expected model change), and goal-driven(expected error reduction) active learning approaches, and propose a fast andeffective goal-driven active learning scoring function to pick question-imagepairs for deep VQA models under the Bayesian Neural Network framework. We findthat deep VQA models need large amounts of training data before they can startasking informative questions. But once they do, all three approaches outperformthe random selection baseline and achieve significant query savings. For thescenario where the model is allowed to ask generic questions about images butis evaluated only on specific questions (e.g., questions whose answer is eitheryes or no), our proposed goal-driven scoring function performs the best.

Embodied Question Answering

  We present a new AI task -- Embodied Question Answering (EmbodiedQA) -- wherean agent is spawned at a random location in a 3D environment and asked aquestion ("What color is the car?"). In order to answer, the agent must firstintelligently navigate to explore the environment, gather information throughfirst-person (egocentric) vision, and then answer the question ("orange").  This challenging task requires a range of AI skills -- active perception,language understanding, goal-driven navigation, commonsense reasoning, andgrounding of language into actions. In this work, we develop the environments,end-to-end-trained reinforcement learning agents, and evaluation protocols forEmbodiedQA.

Neural Baby Talk

  We introduce a novel framework for image captioning that can produce naturallanguage explicitly grounded in entities that object detectors find in theimage. Our approach reconciles classical slot filling approaches (that aregenerally better grounded in images) with modern neural captioning approaches(that are generally more natural sounding and accurate). Our approach firstgenerates a sentence `template' with slot locations explicitly tied to specificimage regions. These slots are then filled in by visual concepts identified inthe regions by object detectors. The entire architecture (sentence templategeneration and slot filling with object detectors) is end-to-enddifferentiable. We verify the effectiveness of our proposed model on differentimage captioning tasks. On standard image captioning and novel objectcaptioning, our model reaches state-of-the-art on both COCO and Flickr30kdatasets. We also demonstrate that our model has unique advantages when thetrain and test distributions of scene compositions -- and hence language priorsof associated captions -- are different. Code has been made available at:https://github.com/jiasenlu/NeuralBabyTalk

Audio Visual Scene-Aware Dialog (AVSD) Challenge at DSTC7

  Scene-aware dialog systems will be able to have conversations with usersabout the objects and events around them. Progress on such systems can be madeby integrating state-of-the-art technologies from multiple research areasincluding end-to-end dialog systems visual dialog, and video description. Weintroduce the Audio Visual Scene Aware Dialog (AVSD) challenge and dataset. Inthis challenge, which is one track of the 7th Dialog System TechnologyChallenges (DSTC7) workshop1, the task is to build a system that generatesresponses in a dialog about an input video

Talk the Walk: Navigating New York City through Grounded Dialogue

  We introduce "Talk The Walk", the first large-scale dialogue dataset groundedin action and perception. The task involves two agents (a "guide" and a"tourist") that communicate via natural language in order to achieve a commongoal: having the tourist navigate to a given target location. The task anddataset, which are described in detail, are challenging and their full solutionis an open problem that we pose to the community. We (i) focus on the task oftourist localization and develop the novel Masked Attention for SpatialConvolutions (MASC) mechanism that allows for grounding tourist utterances intothe guide's map, (ii) show it yields significant improvements for both emergentand natural language communication, and (iii) using this method, we establishnon-trivial baselines on the full task.

Pythia v0.1: the Winning Entry to the VQA Challenge 2018

  This document describes Pythia v0.1, the winning entry from Facebook AIResearch (FAIR)'s A-STAR team to the VQA Challenge 2018.  Our starting point is a modular re-implementation of the bottom-up top-down(up-down) model. We demonstrate that by making subtle but important changes tothe model architecture and the learning rate schedule, fine-tuning imagefeatures, and adding data augmentation, we can significantly improve theperformance of the up-down model on VQA v2.0 dataset -- from 65.67% to 70.22%.  Furthermore, by using a diverse ensemble of models trained with differentfeatures and on different datasets, we are able to significantly improve overthe 'standard' way of ensembling (i.e. same model with different random seeds)by 1.31%. Overall, we achieve 72.27% on the test-std split of the VQA v2.0dataset. Our code in its entirety (training, evaluation, data-augmentation,ensembling) and pre-trained models are publicly available at:https://github.com/facebookresearch/pythia

Choose Your Neuron: Incorporating Domain Knowledge through  Neuron-Importance

  Individual neurons in convolutional neural networks supervised forimage-level classification tasks have been shown to implicitly learnsemantically meaningful concepts ranging from simple textures and shapes towhole or partial objects - forming a "dictionary" of concepts acquired throughthe learning process. In this work we introduce a simple, efficient zero-shotlearning approach based on this observation. Our approach, which we call NeuronImportance-AwareWeight Transfer (NIWT), learns to map domain knowledge aboutnovel "unseen" classes onto this dictionary of learned concepts and thenoptimizes for network parameters that can effectively combine these concepts -essentially learning classifiers by discovering and composing learned semanticconcepts in deep networks. Our approach shows improvements over previousapproaches on the CUBirds and AWA2 generalized zero-shot learning benchmarks.We demonstrate our approach on a diverse set of semantic inputs as externaldomain knowledge including attributes and natural language captions. Moreoverby learning inverse mappings, NIWT can provide visual and textual explanationsfor the predictions made by the newly learned classifiers and provide neuronnames. Our code is available athttps://github.com/ramprs/neuron-importance-zsl.

Neural Modular Control for Embodied Question Answering

  We present a modular approach for learning policies for navigation over longplanning horizons from language input. Our hierarchical policy operates atmultiple timescales, where the higher-level master policy proposes subgoals tobe executed by specialized sub-policies. Our choice of subgoals iscompositional and semantic, i.e. they can be sequentially combined in arbitraryorderings, and assume human-interpretable descriptions (e.g. 'exit room', 'findkitchen', 'find refrigerator', etc.).  We use imitation learning to warm-start policies at each level of thehierarchy, dramatically increasing sample efficiency, followed by reinforcementlearning. Independent reinforcement learning at each level of hierarchy enablessub-policies to adapt to consequences of their actions and recover from errors.Subsequent joint hierarchical training enables the master policy to adapt tothe sub-policies.

TarMAC: Targeted Multi-Agent Communication

  We explore a collaborative multi-agent reinforcement learning setting where ateam of agents attempts to solve cooperative tasks in partially-observableenvironments. In this scenario, learning an effective communication protocol iskey. We propose a communication architecture that allows for targetedcommunication, where agents learn both what messages to send and who to sendthem to, solely from downstream task-specific reward without any communicationsupervision. Additionally, we introduce a multi-stage communication approachwhere the agents co-ordinate via multiple rounds of communication before takingactions in the environment. We evaluate our approach on a diverse set ofcooperative multi-agent tasks, of varying difficulties, with varying number ofagents, in a variety of environments ranging from 2D grid layouts of shapes andsimulated traffic junctions to complex 3D indoor environments. We demonstratethe benefits of targeted as well as multi-stage communication. Moreover, weshow that the targeted communication strategies learned by agents are bothinterpretable and intuitive.

Do Explanations make VQA Models more Predictable to a Human?

  A rich line of research attempts to make deep neural networks moretransparent by generating human-interpretable 'explanations' of their decisionprocess, especially for interactive tasks like Visual Question Answering (VQA).In this work, we analyze if existing explanations indeed make a VQA model --its responses as well as failures -- more predictable to a human. Surprisingly,we find that they do not. On the other hand, we find that human-in-the-loopapproaches that treat the model as a black-box do.

Dialog System Technology Challenge 7

  This paper introduces the Seventh Dialog System Technology Challenges (DSTC),which use shared datasets to explore the problem of building dialog systems.Recently, end-to-end dialog modeling approaches have been applied to variousdialog tasks. The seventh DSTC (DSTC7) focuses on developing technologiesrelated to end-to-end dialog systems for (1) sentence selection, (2) sentencegeneration and (3) audio visual scene aware dialog. This paper summarizes theoverall setup and results of DSTC7, including detailed descriptions of thedifferent tracks and provided datasets. We also describe overall trends in thesubmitted systems and the key results. Each track introduced new datasets andparticipants achieved impressive results using state-of-the-art end-to-endtechnologies.

Audio-Visual Scene-Aware Dialog

  We introduce the task of scene-aware dialog. Given a follow-up question in anongoing dialog about a video, our goal is to generate a complete and naturalresponse to a question given (a) an input video, and (b) the history ofprevious turns in the dialog. To succeed, agents must ground the semantics inthe video and leverage contextual cues from the history of the dialog to answerthe question. To benchmark this task, we introduce the Audio Visual Scene-AwareDialog (AVSD) dataset. For each of more than 11,000 videos of human actions forthe Charades dataset. Our dataset contains a dialog about the video, plus afinal summary of the video by one of the dialog participants. We train severalbaseline systems for this task and evaluate the performance of the trainedmodels using several qualitative and quantitative metrics. Our results indicatethat the models must comprehend all the available inputs (video, audio,question and dialog history) to perform well on this dataset.

Embodied Multimodal Multitask Learning

  Recent efforts on training visual navigation agents conditioned on languageusing deep reinforcement learning have been successful in learning policies fordifferent multimodal tasks, such as semantic goal navigation and embodiedquestion answering. In this paper, we propose a multitask model capable ofjointly learning these multimodal tasks, and transferring knowledge of wordsand their grounding in visual objects across the tasks. The proposed model usesa novel Dual-Attention unit to disentangle the knowledge of words in thetextual representations and visual concepts in the visual representations, andalign them with each other. This disentangled task-invariant alignment ofrepresentations facilitates grounding and knowledge transfer across both tasks.We show that the proposed model outperforms a range of baselines on both tasksin simulated 3D environments. We also show that this disentanglement ofrepresentations makes our model modular, interpretable, and allows for transferto instructions containing new words by leveraging object detectors.

Taking a HINT: Leveraging Explanations to Make Vision and Language  Models More Grounded

  Many vision and language models suffer from poor visual grounding - oftenfalling back on easy-to-learn language priors rather than associating languagewith visual concepts. In this work, we propose a generic framework which wecall Human Importance-aware Network Tuning (HINT) that effectively leverageshuman supervision to improve visual grounding. HINT constrains deep networks tobe sensitive to the same input regions as humans. Crucially, our approachoptimizes the alignment between human attention maps and gradient-based networkimportances - ensuring that models learn not just to look at but rather rely onvisual concepts that humans found relevant for a task when making predictions.We demonstrate our approach on Visual Question Answering and Image Captioningtasks, achieving state of-the-art for the VQA-CP dataset which penalizesover-reliance on language priors.

Cycle-Consistency for Robust Visual Question Answering

  Despite significant progress in Visual Question Answering over the years,robustness of today's VQA models leave much to be desired. We introduce a newevaluation protocol and associated dataset (VQA-Rephrasings) and show thatstate-of-the-art VQA models are notoriously brittle to linguistic variations inquestions. VQA-Rephrasings contains 3 human-provided rephrasings for 40kquestions spanning 40k images from the VQA v2.0 validation dataset. As a steptowards improving robustness of VQA models, we propose a model-agnosticframework that exploits cycle consistency. Specifically, we train a model tonot only answer a question, but also generate a question conditioned on theanswer, such that the answer predicted for the generated question is the sameas the ground truth answer to the original question. Without the use ofadditional annotations, we show that our approach is significantly more robustto linguistic variations than state-of-the-art VQA models, when evaluated onthe VQA-Rephrasings dataset. In addition, our approach outperformsstate-of-the-art approaches on the standard VQA and Visual Question Generationtasks on the challenging VQA v2.0 dataset.

Probabilistic Neural-symbolic Models for Interpretable Visual Question  Answering

  We propose a new class of probabilistic neural-symbolic models, that havesymbolic functional programs as a latent, stochastic variable. Instantiated inthe context of visual question answering, our probabilistic formulation offerstwo key conceptual advantages over prior neural-symbolic models for VQA.Firstly, the programs generated by our model are more understandable whilerequiring lesser number of teaching examples. Secondly, we show that one canpose counterfactual scenarios to the model, to probe its beliefs on theprograms that could lead to a specified answer given an image. Our results onthe CLEVR and SHAPES datasets verify our hypotheses, showing that the modelgets better program (and answer) prediction accuracy even in the low dataregime, and allows one to probe the coherence and consistency of reasoningperformed.

Learning Dynamics Model in Reinforcement Learning by Incorporating the  Long Term Future

  In model-based reinforcement learning, the agent interleaves between modellearning and planning. These two components are inextricably intertwined. Ifthe model is not able to provide sensible long-term prediction, the executedplanner would exploit model flaws, which can yield catastrophic failures. Thispaper focuses on building a model that reasons about the long-term future anddemonstrates how to use this for efficient planning and exploration. To thisend, we build a latent-variable autoregressive model by leveraging recent ideasin variational inference. We argue that forcing latent variables to carryfuture information through an auxiliary task substantially improves long-termpredictions. Moreover, by planning in the latent space, the planner's solutionis ensured to be within regions where the model is valid. An explorationstrategy can be devised by searching for unlikely trajectories under the model.Our method achieves higher reward faster compared to baselines on a variety oftasks and environments in both the imitation learning and model-basedreinforcement learning settings.

Trick or TReAT: Thematic Reinforcement for Artistic Typography

  An approach to make text visually appealing and memorable is semanticreinforcement - the use of visual cues alluding to the context or theme inwhich the word is being used to reinforce the message (e.g., Google Doodles).We present a computational approach for semantic reinforcement called TReAT -Thematic Reinforcement for Artistic Typography. Given an input word (e.g. exam)and a theme (e.g. education), the individual letters of the input word arereplaced by cliparts relevant to the theme which visually resemble the letters- adding creative context to the potentially boring input word. We use anunsupervised approach to learn a latent space to represent letters and clipartsand compute similarities between the two. Human studies show that participantscan reliably recognize the word as well as the theme in our outputs (TReATs)and find them more creative compared to meaningful baselines.

Align2Ground: Weakly Supervised Phrase Grounding Guided by Image-Caption  Alignment

  We address the problem of grounding free-form textual phrases by using weaksupervision from image-caption pairs. We propose a novel end-to-end model thatuses caption-to-image retrieval as a `downstream' task to guide the process ofphrase localization. Our method, as a first step, infers the latentcorrespondences between regions-of-interest (RoIs) and phrases in the captionand creates a discriminative image representation using these matched RoIs. Ina subsequent step, this (learned) representation is aligned with the caption.Our key contribution lies in building this `caption-conditioned' image encodingwhich tightly couples both the tasks and allows the weak supervision toeffectively guide visual grounding. We provide an extensive empirical andqualitative analysis to investigate the different components of our proposedmodel and compare it with competitive baselines. For phrase localization, wereport an improvement of 4.9% (absolute) over the prior state-of-the-art on theVisualGenome dataset. We also report results that are at par with thestate-of-the-art on the downstream caption-to-image retrieval task on COCO andFlickr30k datasets.

