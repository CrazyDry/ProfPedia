Postprocessing for Iterative Differentially Private Algorithms

  Iterative algorithms for differential privacy run for a fixed number ofiterations, where each iteration learns some information from data and producesan intermediate output. However, the algorithm only releases the output of thelast iteration, and from which the accuracy of algorithm is judged. In thispaper, we propose a post-processing algorithm that seeks to improve theaccuracy by incorporating the knowledge on the data contained in intermediateoutputs.

A New Class of Private Chi-Square Tests

  In this paper, we develop new test statistics for private hypothesis testing.These statistics are designed specifically so that their asymptoticdistributions, after accounting for noise added for privacy concerns, match theasymptotics of the classical (non-private) chi-square tests for testing if themultinomial data parameters lie in lower dimensional manifolds (examplesinclude goodness of fit and independence testing). Empirically, these new teststatistics outperform prior work, which focused on noisy versions of existingstatistics.

Differentially Private Confidence Intervals for Empirical Risk  Minimization

  The process of data mining with differential privacy produces results thatare affected by two types of noise: sampling noise due to data collection andprivacy noise that is designed to prevent the reconstruction of sensitiveinformation. In this paper, we consider the problem of designing confidenceintervals for the parameters of a variety of differentially private machinelearning models. The algorithms can provide confidence intervals that satisfydifferential privacy (as well as the more recently proposed concentrateddifferential privacy) and can be used with existing differentially privatemechanisms that train models using objective perturbation and outputperturbation.

Worst-Case Background Knowledge for Privacy-Preserving Data Publishing

  Recent work has shown the necessity of considering an attacker's backgroundknowledge when reasoning about privacy in data publishing. However, inpractice, the data publisher does not know what background knowledge theattacker possesses. Thus, it is important to consider the worst-case. In thispaper, we initiate a formal study of worst-case background knowledge. Wepropose a language that can express any background knowledge about the data. Weprovide a polynomial time algorithm to measure the amount of disclosure ofsensitive information in the worst case, given that the attacker has at most aspecified number of pieces of information in this language. We also provide amethod to efficiently sanitize the data so that the amount of disclosure in theworst case is less than a specified threshold.

A Framework for Extracting Semantic Guarantees from Privacy

  Statistical privacy views privacy definitions as contracts that guide thebehavior of algorithms that take in sensitive data and produce sanitized data.For most existing privacy definitions, it is not clear what they actuallyguarantee.  In this paper, we propose the first (to the best of our knowledge) frameworkfor extracting semantic guarantees from privacy definitions. That is, insteadof answering narrow questions such as "does privacy definition Y protect X?"the goal is to answer the more general question "what does privacy definition Yprotect?"  The privacy guarantees we can extract are Bayesian in nature and deal withchanges in an attacker's beliefs. The key to our framework is an object we callthe row cone. Every privacy definition has a row cone, which is a convex setthat describes all the ways an attacker's prior beliefs can be turned intoposterior beliefs after observing an output of an algorithm satisfying thatprivacy definition.  The framework can be applied to privacy definitions or even to individualalgorithms to identify the types of inferences they defend against. Weillustrate the use of our framework with analyses of several definitions andalgorithms for which we can derive previously unknown semantics. These includerandomized response, FRAPP, and several algorithms that add integer-valuednoise to their inputs.

Revisiting Differentially Private Hypothesis Tests for Categorical Data

  In this paper, we consider methods for performing hypothesis tests on dataprotected by a statistical disclosure control technology known as differentialprivacy. Previous approaches to differentially private hypothesis testingeither perturbed the test statistic with random noise having large variance(and resulted in a significant loss of power) or added smaller amounts of noisedirectly to the data but failed to adjust the test in response to the addednoise (resulting in biased, unreliable $p$-values). In this paper, we develop avariety of practical hypothesis tests that address these problems. Using adifferent asymptotic regime that is more suited to hypothesis testing withprivacy, we show a modified equivalence between chi-squared tests andlikelihood ratio tests. We then develop differentially private likelihood ratioand chi-squared tests for a variety of applications on tabular data (i.e.,independence, sample proportions, and goodness-of-fit tests). Experimentalevaluations on small and large datasets using a wide variety of privacysettings demonstrate the practicality and reliability of our methods.

Unifying Adversarial Training Algorithms with Flexible Deep Data  Gradient Regularization

  Many previous proposals for adversarial training of deep neural nets haveincluded di- rectly modifying the gradient, training on a mix of original andadversarial examples, using contractive penalties, and approximately optimizingconstrained adversarial ob- jective functions. In this paper, we show theseproposals are actually all instances of optimizing a general, regularizedobjective we call DataGrad. Our proposed DataGrad framework, which can beviewed as a deep extension of the layerwise contractive au- toencoder penalty,cleanly simplifies prior work and easily allows extensions such as adversarialtraining with multi-task cues. In our experiments, we find that the deep gra-dient regularization of DataGrad (which also has L1 and L2 flavors ofregularization) outperforms alternative forms of regularization, includingclassical L1, L2, and multi- task, both on the original dataset as well as onadversarial sets. Furthermore, we find that combining multi-task optimizationwith DataGrad adversarial training results in the most robust performance.

LightDP: Towards Automating Differential Privacy Proofs

  The growing popularity and adoption of differential privacy in academic andindustrial settings has resulted in the development of increasinglysophisticated algorithms for releasing information while preserving privacy.Accompanying this phenomenon is the natural rise in the development andpublication of incorrect algorithms, thus demonstrating the necessity of formalverification tools. However, existing formal methods for differential privacyface a dilemma: methods based on customized logics can verify sophisticatedalgorithms but come with a steep learning curve and significant annotationburden on the programmers, while existing programming platforms lack expressivepower for some sophisticated algorithms.  In this paper, we present LightDP, a simple imperative language that strikesa better balance between expressive power and usability. The core of LightDP isa novel relational type system that separates relational reasoning from privacybudget calculations. With dependent types, the type system is powerful enoughto verify sophisticated algorithms where the composition theorem falls short.In addition, the inference engine of LightDP infers most of the proof details,and even searches for the proof with minimal privacy cost bound when multipleproofs exist. We show that LightDP verifies sophisticated algorithms withlittle manual effort.

Predicting Demographics of High-Resolution Geographies with Geotagged  Tweets

  In this paper, we consider the problem of predicting demographics ofgeographic units given geotagged Tweets that are composed within these units.Traditional survey methods that offer demographics estimates are usuallylimited in terms of geographic resolution, geographic boundaries, and timeintervals. Thus, it would be highly useful to develop computational methodsthat can complement traditional survey methods by offering demographicsestimates at finer geographic resolutions, with flexible geographic boundaries(i.e. not confined to administrative boundaries), and at different timeintervals. While prior work has focused on predicting demographics and healthstatistics at relatively coarse geographic resolutions such as the county-levelor state-level, we introduce an approach to predict demographics at finergeographic resolutions such as the blockgroup-level. For the task of predictinggender and race/ethnicity counts at the blockgroup-level, an approach adaptedfrom prior work to our problem achieves an average correlation of 0.389(gender) and 0.569 (race) on a held-out test dataset. Our approach outperformsthis prior approach with an average correlation of 0.671 (gender) and 0.692(race).

Learning to Extract Semantic Structure from Documents Using Multimodal  Fully Convolutional Neural Network

  We present an end-to-end, multimodal, fully convolutional network forextracting semantic structures from document images. We consider documentsemantic structure extraction as a pixel-wise segmentation task, and propose aunified model that classifies pixels based not only on their visual appearance,as in the traditional page segmentation task, but also on the content ofunderlying text. Moreover, we propose an efficient synthetic documentgeneration process that we use to generate pretraining data for our network.Once the network is trained on a large set of synthetic documents, we fine-tunethe network on unlabeled real documents using a semi-supervised approach. Wesystematically study the optimum network architecture and show that both ourmultimodal approach and the synthetic data pretraining significantly boost theperformance.

Prolongation of SMAP to Spatio-temporally Seamless Coverage of  Continental US Using a Deep Learning Neural Network

  The Soil Moisture Active Passive (SMAP) mission has delivered valuablesensing of surface soil moisture since 2015. However, it has a short time spanand irregular revisit schedule. Utilizing a state-of-the-art time-series deeplearning neural network, Long Short-Term Memory (LSTM), we created a systemthat predicts SMAP level-3 soil moisture data with atmospheric forcing,model-simulated moisture, and static physiographic attributes as inputs. Thesystem removes most of the bias with model simulations and improves predictedmoisture climatology, achieving small test root-mean-squared error (<0.035) andhigh correlation coefficient >0.87 for over 75\% of Continental United States,including the forested Southeast. As the first application of LSTM inhydrology, we show the proposed network avoids overfitting and is robust forboth temporal and spatial extrapolation tests. LSTM generalizes well acrossregions with distinct climates and physiography. With high fidelity to SMAP,LSTM shows great potential for hindcasting, data assimilation, and weatherforecasting.

Conducting Credit Assignment by Aligning Local Representations

  Using back-propagation and its variants to train deep networks is oftenproblematic for new users. Issues such as exploding gradients, vanishinggradients, and high sensitivity to weight initialization strategies often makenetworks difficult to train, especially when users are experimenting with newarchitectures. Here, we present Local Representation Alignment (LRA), atraining procedure that is much less sensitive to bad initializations, does notrequire modifications to the network architecture, and can be adapted tonetworks with highly nonlinear and discrete-valued activation functions.Furthermore, we show that one variation of LRA can start with a nullinitialization of network weights and still successfully train networks with awide variety of nonlinearities, including tanh, ReLU-6, softplus, signum andothers that may draw their inspiration from biology.  A comprehensive set of experiments on MNIST and the much harder Fashion MNISTdata sets show that LRA can be used to train networks robustly and effectively,succeeding even when back-propagation fails and outperforming other alternativelearning algorithms, such as target propagation and feedback alignment.

Differentially Private Hierarchical Count-of-Counts Histograms

  We consider the problem of privately releasing a class of queries that wecall hierarchical count-of-counts histograms. Count-of-counts histogramspartition the rows of an input table into groups (e.g., group of people in thesame household), and for every integer j report the number of groups of size j.Hierarchical count-of-counts queries report count-of-counts histograms atdifferent granularities as per hierarchy defined on an attribute in the inputdata (e.g., geographical location of a household at the national, state andcounty levels). In this paper, we introduce this problem, along withappropriate error metrics and propose a differentially private solution thatgenerates count-of-counts histograms that are consistent across all levels ofthe hierarchy.

Adversarial Training for Community Question Answer Selection Based on  Multi-scale Matching

  Community-based question answering (CQA) websites represent an importantsource of information. As a result, the problem of matching the most valuableanswers to their corresponding questions has become an increasingly popularresearch topic. We frame this task as a binary (relevant/irrelevant)classification problem, and present an adversarial training framework toalleviate label imbalance issue. We employ a generative model to iterativelysample a subset of challenging negative samples to fool our classificationmodel. Both models are alternatively optimized using REINFORCE algorithm. Theproposed method is completely different from previous ones, where negativesamples in training set are directly used or uniformly down-sampled. Further,we propose using Multi-scale Matching which explicitly inspects the correlationbetween words and ngrams of different levels of granularity. We evaluate theproposed method on SemEval 2016 and SemEval 2017 datasets and achievesstate-of-the-art or similar performance.

Detecting Violations of Differential Privacy

  The widespread acceptance of differential privacy has led to the publicationof many sophisticated algorithms for protecting privacy. However, due to thesubtle nature of this privacy definition, many such algorithms have bugs thatmake them violate their claimed privacy. In this paper, we consider the problemof producing counterexamples for such incorrect algorithms. The counterexamplesare designed to be short and human-understandable so that the counterexamplegenerator can be used in the development process -- a developer could quicklyexplore variations of an algorithm and investigate where they break down. Ourapproach is statistical in nature. It runs a candidate algorithm many times anduses statistical tests to try to detect violations of differential privacy. Anevaluation on a variety of incorrect published algorithms validates theusefulness of our approach: it correctly rejects incorrect algorithms andprovides counterexamples for them within a few seconds.

Detecting Outliers in Data with Correlated Measures

  Advances in sensor technology have enabled the collection of large-scaledatasets. Such datasets can be extremely noisy and often contain a significantamount of outliers that result from sensor malfunction or human operationfaults. In order to utilize such data for real-world applications, it iscritical to detect outliers so that models built from these datasets will notbe skewed by outliers.  In this paper, we propose a new outlier detection method that utilizes thecorrelations in the data (e.g., taxi trip distance vs. trip time). Differentfrom existing outlier detection methods, we build a robust regression modelthat explicitly models the outliers and detects outliers simultaneously withthe model fitting.  We validate our approach on real-world datasets against methods specificallydesigned for each dataset as well as the state of the art outlier detectors.Our outlier detection method achieves better performances, demonstrating therobustness and generality of our method. Last, we report interesting casestudies on some outliers that result from atypical events.

Concentrated Differentially Private Gradient Descent with Adaptive  per-Iteration Privacy Budget

  Iterative algorithms, like gradient descent, are common tools for solving avariety of problems, such as model fitting. For this reason, there is interestin creating differentially private versions of them. However, their conversionto differentially private algorithms is often naive. For instance, a fixednumber of iterations are chosen, the privacy budget is split evenly among them,and at each iteration, parameters are updated with a noisy gradient. In thispaper, we show that gradient-based algorithms can be improved by a more carefulallocation of privacy budget per iteration. Intuitively, at the beginning ofthe optimization, gradients are expected to be large, so that they do not needto be measured as accurately. However, as the parameters approach their optimalvalues, the gradients decrease and hence need to be measured more accurately.We add a basic line-search capability that helps the algorithm decide when moreaccurate gradient measurements are necessary. Our gradient descent algorithmworks with the recently introduced zCDP version of differential privacy. Itoutperforms prior algorithms for model fitting and is competitive with thestate-of-the-art for $(\epsilon,\delta)$-differential privacy, a strictlyweaker definition than zCDP.

TextContourNet: a Flexible and Effective Framework for Improving Scene  Text Detection Architecture with a Multi-task Cascade

  We study the problem of extracting text instance contour information fromimages and use it to assist scene text detection. We propose a novel andeffective framework for this and experimentally demonstrate that: (1) A CNNthat can be effectively used to extract instance-level text contour fromnatural images. (2) The extracted contour information can be used for betterscene text detection. We propose two ways for learning the contour tasktogether with the scene text detection: (1) as an auxiliary task and (2) asmulti-task cascade. Extensive experiments with different benchmark datasetsdemonstrate that both designs improve the performance of a state-of-the-artscene text detector and that a multi-task cascade design achieves the bestperformance.

Proving Differential Privacy with Shadow Execution

  Recent work on formal verification of differential privacy shows a trendtoward usability and expressiveness -- generating a correctness proof ofsophisticated algorithm while minimizing the annotation burden on programmers.Sometimes, combining those two requires substantial changes to program logics:one recent paper is able to verify Report Noisy Max automatically, but itinvolves a complex verification system using customized program logics andverifiers.  In this paper, we propose a new proof technique, called shadow execution, andembed it into a language called ShadowDP. ShadowDP uses shadow execution togenerate proofs of differential privacy with very few programmer annotationsand without relying on customized logics and verifiers. In addition toverifying Report Noisy Max, we show that it can verify a new variant of SparseVector that reports the gap between some noisy query answers and the noisythreshold. Moreover, ShadowDP reduces the complexity of verification: for allof the algorithms we have evaluated, type checking and verification in totaltakes at most 3 seconds, while prior work takes minutes on the same algorithms.

Large Scale Scene Text Verification with Guided Attention

  Many tasks are related to determining if a particular text string exists inan image. In this work, we propose a new framework that learns this task in anend-to-end way. The framework takes an image and a text string as input andthen outputs the probability of the text string being present in the image.This is the first end-to-end framework that learns such relationships betweentext and images in scene text area. The framework does not require explicitscene text detection or recognition and thus no bounding box annotations areneeded for it. It is also the first work in scene text area that tackles suh aweakly labeled problem. Based on this framework, we developed a model calledGuided Attention. Our designed model achieves much better results than severalstate-of-the-art scene text reading based solutions for a challenging StreetView Business Matching task. The task tries to find correct business names forstorefront images and the dataset we collected for it is substantially larger,and more challenging than existing scene text dataset. This new real-world taskprovides a new perspective for studying scene text related problems. We alsodemonstrate the uniqueness of our task via a comparison between our problem anda typical Visual Question Answering problem.

ET-Lasso: Efficient Tuning of Lasso for High-Dimensional Data

  The L1 regularization (Lasso) has proven to be a versatile tool to selectrelevant features and estimate the model coefficients simultaneously. Despiteits popularity, it is very challenging to guarantee the feature selectionconsistency of Lasso. One way to improve the feature selection consistency isto select an ideal tuning parameter. Traditional tuning criteria mainly focuson minimizing the estimated prediction error or maximizing the posterior modelprobability, such as cross-validation and BIC, which may either betime-consuming or fail to control the false discovery rate (FDR) when thenumber of features is extremely large. The other way is to introducepseudo-features to learn the importance of the original ones. Recently, theKnockoff filter is proposed to control the FDR when performing featureselection. However, its performance is sensitive to the choice of the expectedFDR threshold. Motivated by these ideas, we propose a new method usingpseudo-features to obtain an ideal tuning parameter. In particular, we presentthe Efficient Tuning of Lasso (ET-Lasso) to separate active and inactivefeatures by adding permuted features as pseudo-features in linear models. Thepseudo-features are constructed to be inactive by nature, which can be used toobtain a cutoff to select the tuning parameter that separates active andinactive features. Experimental studies on both simulations and real-world dataapplications are provided to show that ET-Lasso can effectively and efficientlyselect active features under a wide range of different scenarios.

Continual Learning of Recurrent Neural Networks by Locally Aligning  Distributed Representations

  Temporal models based on recurrent neural networks have proven to be quitepowerful in a wide variety of applications, including language modeling andspeech processing. However, training these models relies on back-propagationthrough time, which entails unfolding the network over many time steps, makingthe process of conducting credit assignment considerably more challenging.Furthermore, the nature of back-propagation itself does not permit the use ofnon-differentiable activation functions and is inherently sequential, makingparallelization of the training process very difficult.  In this work, we propose the Parallel Temporal Neural Coding Network(P-TNCN), a biologically inspired model trained by the learning algorithm knownas Local Representation Alignment, that aims to resolve the difficulties thatplague recurrent networks trained by back-propagation through time. Mostnotably, this architecture requires neither unrolling nor the derivatives ofits internal activation functions. We compare our model and learning procedureto other online back-propagation-through-time alternatives (which tend to becomputationally expensive), including real-time recurrent learning, echo statenetworks, and unbiased online recurrent optimization, and show that itoutperforms them on sequence benchmarks such as Bouncing MNIST, a new benchmarkwe call Bouncing NotMNIST, and Penn Treebank. Notably, our approach can, insome instances, outperform full back-propagation through time and variants suchas sparse attentive back-tracking.  Significantly, the hidden unit correction phase of P-TNCN allows it to adaptto new datasets even if its synaptic weights are held fixed (zero-shotadaptation) and facilitates retention of prior knowledge when faced with a tasksequence. We present results that show the P-TNCN's ability to conductzero-shot adaptation and continual sequence modeling.

