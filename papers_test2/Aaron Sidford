A Simple, Combinatorial Algorithm for Solving SDD Systems in  Nearly-Linear Time

  In this paper, we present a simple combinatorial algorithm that solvessymmetric diagonally dominant (SDD) linear systems in nearly-linear time. Ituses very little of the machinery that previously appeared to be necessary fora such an algorithm. It does not require recursive preconditioning, spectralsparsification, or even the Chebyshev Method or Conjugate Gradient. Afterconstructing a "nice" spanning tree of a graph associated with the linearsystem, the entire algorithm consists of the repeated application of a simple(non-recursive) update rule, which it implements using a lightweight datastructure. The algorithm is numerically stable and can be implemented withoutthe increased bit-precision required by previous solvers. As such, thealgorithm has the fastest known running time under the standard unit-cost RAMmodel. We hope that the simplicity of the algorithm and the insights yielded byits analysis will be useful in both theory and practice.

Accelerated Methods for Non-Convex Optimization

  We present an accelerated gradient method for non-convex optimizationproblems with Lipschitz continuous first and second derivatives. The methodrequires time $O(\epsilon^{-7/4} \log(1/ \epsilon) )$ to find an$\epsilon$-stationary point, meaning a point $x$ such that $\|\nabla f(x)\| \le\epsilon$. The method improves upon the $O(\epsilon^{-2} )$ complexity ofgradient descent and provides the additional second-order guarantee that$\nabla^2 f(x) \succeq -O(\epsilon^{1/2})I$ for the computed $x$. Furthermore,our method is Hessian free, i.e. it only requires gradient computations, and istherefore suitable for large scale applications.

Efficient Convex Optimization with Membership Oracles

  We consider the problem of minimizing a convex function over a convex setgiven access only to an evaluation oracle for the function and a membershiporacle for the set. We give a simple algorithm which solves this problem with$\tilde{O}(n^2)$ oracle calls and $\tilde{O}(n^3)$ additional arithmeticoperations. Using this result, we obtain more efficient reductions among thefive basic oracles for convex sets and functions defined by Gr\"otschel, Lovaszand Schrijver.

A Markov Chain Theory Approach to Characterizing the Minimax Optimality  of Stochastic Gradient Descent (for Least Squares)

  This work provides a simplified proof of the statistical minimax optimalityof (iterate averaged) stochastic gradient descent (SGD), for the special caseof least squares. This result is obtained by analyzing SGD as a stochasticprocess and by sharply characterizing the stationary covariance matrix of thisprocess. The finite rate optimality characterization captures the constantfactors and addresses model mis-specification.

Near-optimal method for highly smooth convex optimization

  We propose a near-optimal method for highly smooth convex optimization. Moreprecisely, in the oracle model where one obtains the $p^{th}$ order Taylorexpansion of a function at the query point, we propose a method with rate ofconvergence $\tilde{O}(1/k^{\frac{ 3p +1}{2}})$ after $k$ queries to the oraclefor any convex function whose $p^{th}$ order derivative is Lipschitz.

Efficient Accelerated Coordinate Descent Methods and Faster Algorithms  for Solving Linear Systems

  In this paper we show how to accelerate randomized coordinate descent methodsand achieve faster convergence rates without paying per-iteration costs inasymptotic running time. In particular, we show how to generalize andefficiently implement a method proposed by Nesterov, giving faster asymptoticrunning times for various algorithms that use standard coordinate descent as ablack box. In addition to providing a proof of convergence for this new generalmethod, we show that it is numerically stable, efficiently implementable, andin certain regimes, asymptotically optimal.  To highlight the computational power of this algorithm, we show how it canused to create faster linear system solvers in several regimes:  - We show how this method achieves a faster asymptotic runtime than conjugategradient for solving a broad class of symmetric positive definite systems ofequations.  - We improve the best known asymptotic convergence guarantees for Kaczmarzmethods, a popular technique for image reconstruction and solvingoverdetermined systems of equations, by accelerating a randomized algorithm ofStrohmer and Vershynin.  - We achieve the best known running time for solving Symmetric DiagonallyDominant (SDD) system of equations in the unit-cost RAM model, obtaining an O(mlog^{3/2} n (log log n)^{1/2} log (log n / eps)) asymptotic running time byaccelerating a recent solver by Kelner et al.  Beyond the independent interest of these solvers, we believe they highlightthe versatility of the approach of this paper and we hope that they will openthe door for further algorithmic improvements in the future.

Path Finding II : An Ã•(m sqrt(n)) Algorithm for the Minimum Cost Flow  Problem

  In this paper we present an $\tilde{O}(m\sqrt{n}\log^{O(1)}U)$ time algorithmfor solving the maximum flow problem on directed graphs with $m$ edges, $n$vertices, and capacity ratio $U$. This improves upon the previous fastestrunning time of$O(m\min\left(n^{2/3},m^{1/2}\right)\log\left(n^{2}/m\right)\log U)$ achievedover 15 years ago by Goldberg and Rao. In the special case of solving densedirected unit capacity graphs our algorithm improves upon the previous fastestrunning times of of $O(\min\{m^{3/2},mn^{^{2/3}}\})$ achieved by Even andTarjan and Karzanov over 35 years ago and of $\tilde{O}(m^{10/7})$ achievedrecently by M\k{a}dry.  We achieve these results through the development and application of a newgeneral interior point method that we believe is of independent interest. Thenumber of iterations required by this algorithm is better than that predictedby analyzing the best self-concordant barrier of the feasible region. Byapplying this method to the linear programming formulations of maximum flow,minimum cost flow, and lossy generalized minimum cost flow and applyinganalysis by Daitch and Spielman we achieve running time of$\tilde{O}(m\sqrt{n}\log^{O(1)}(U/\epsilon))$ for these problems as well.Furthermore, our algorithm is parallelizable and using a recent nearly lineartime work polylogarithmic depth Laplacian system solver of Spielman and Peng weachieve a $\tilde{O}(\sqrt{n}\log^{O(1)}(U/\epsilon))$ depth algorithm and$\tilde{O}(m\sqrt{n}\log^{O(1)}(U/\epsilon))$ work algorithm for solving theseproblems.

Geometric Median in Nearly Linear Time

  In this paper we provide faster algorithms for solving the geometric medianproblem: given $n$ points in $\mathbb{R}^{d}$ compute a point that minimizesthe sum of Euclidean distances to the points. This is one of the oldestnon-trivial problems in computational geometry yet despite an abundance ofresearch the previous fastest algorithms for computing a$(1+\epsilon)$-approximate geometric median were $O(d\cdotn^{4/3}\epsilon^{-8/3})$ by Chin et. al,$\tilde{O}(d\exp{\epsilon^{-4}\log\epsilon^{-1}})$ by Badoiu et. al,$O(nd+\mathrm{poly}(d,\epsilon^{-1})$ by Feldman and Langberg, and$O((nd)^{O(1)}\log\frac{1}{\epsilon})$ by Parrilo and Sturmfels and Xue and Ye.  In this paper we show how to compute a $(1+\epsilon)$-approximate geometricmedian in time $O(nd\log^{3}\frac{1}{\epsilon})$ and $O(d\epsilon^{-2})$. Whileour $O(d\epsilon^{-2})$ is a fairly straightforward application of stochasticsubgradient descent, our $O(nd\log^{3}\frac{1}{\epsilon})$ time algorithm is anovel long step interior point method. To achieve this running time we startwith a simple $O((nd)^{O(1)}\log\frac{1}{\epsilon})$ time interior point methodand show how to improve it, ultimately building an algorithm that is quitenon-standard from the perspective of interior point literature. Our result isone of very few cases we are aware of outperforming traditional interior pointtheory and the only we are aware of using interior point methods to obtain anearly linear time algorithm for a canonical optimization problem thattraditionally requires superlinear time. We hope our work leads to furtherimprovements in this line of research.

Efficient Structured Matrix Recovery and Nearly-Linear Time Algorithms  for Solving Inverse Symmetric $M$-Matrices

  In this paper we show how to recover a spectral approximations to broadclasses of structured matrices using only a polylogarithmic number of adaptivelinear measurements to either the matrix or its inverse. Leveraging this resultwe obtain faster algorithms for variety of linear algebraic problems. Keyresults include:  $\bullet$ A nearly linear time algorithm for solving the inverse of symmetric$M$-matrices, a strict superset of Laplacians and SDD matrices.  $\bullet$ An $\tilde{O}(n^2)$ time algorithm for solving $n \times n$ linearsystems that are constant spectral approximations of Laplacians or moregenerally, SDD matrices.  $\bullet$ An $\tilde{O}(n^2)$ algorithm to recover a spectral approximationof a $n$-vertex graph using only $\tilde{O}(1)$ matrix-vector multiplies withits Laplacian matrix.  The previous best results for each problem either used a trivial number ofqueries to exactly recover the matrix or a trivial $O(n^\omega)$ running time,where $\omega$ is the matrix multiplication constant.  We achieve these results by generalizing recent semidefinite programmingbased linear sized sparsifier results of Lee and Sun (2017) and providingiterative methods inspired by the semistreaming sparsification results ofKapralov, Lee, Musco, Musco and Sidford (2014) and input sparsity time linearsystem solving results of Li, Miller, and Peng (2013). We hope that byinitiating study of these natural problems, expanding the robustness and scopeof recent nearly linear time linear system solving research, and providinggeneral matrix recovery machinery this work may serve as a stepping stone forfaster algorithms.

Deterministic Approximation of Random Walks in Small Space

  We give a deterministic, nearly logarithmic-space algorithm that given anundirected graph $G$, a positive integer $r$, and a set $S$ of vertices,approximates the conductance of $S$ in the $r$-step random walk on $G$ towithin a factor of $1+\epsilon$, where $\epsilon>0$ is an arbitrarily smallconstant. More generally, our algorithm computes an $\epsilon$-spectralapproximation to the normalized Laplacian of the $r$-step walk.  Our algorithm combines the derandomized square graph operation (Rozenman andVadhan, 2005), which we recently used for solving Laplacian systems in nearlylogarithmic space (Murtagh, Reingold, Sidford, and Vadhan, 2017), with ideasfrom (Cheng, Cheng, Liu, Peng, and Teng, 2015), which gave an algorithm that istime-efficient (while ours is space-efficient) and randomized (while ours isdeterministic) for the case of even $r$ (while ours works for all $r$). Alongthe way, we provide some new results that generalize technical machinery andyield improvements over previous work. First, we obtain a nearly linear-timerandomized algorithm for computing a spectral approximation to the normalizedLaplacian for odd $r$. Second, we define and analyze a generalization of thederandomized square for irregular graphs and for sparsifying the product of twodistinct graphs. As part of this generalization, we also give a stronglyexplicit construction of expander graphs of every size.

An Almost-Linear-Time Algorithm for Approximate Max Flow in Undirected  Graphs, and its Multicommodity Generalizations

  In this paper, we introduce a new framework for approximately solving flowproblems in capacitated, undirected graphs and apply it to provideasymptotically faster algorithms for the maximum $s$-$t$ flow and maximumconcurrent multicommodity flow problems. For graphs with $n$ vertices and $m$edges, it allows us to find an $\epsilon$-approximate maximum $s$-$t$ flow intime $O(m^{1+o(1)}\epsilon^{-2})$, improving on the previous best bound of$\tilde{O}(mn^{1/3} poly(1/\epsilon))$. Applying the same framework in themulticommodity setting solves a maximum concurrent multicommodity flow problemwith $k$ commodities in $O(m^{1+o(1)}\epsilon^{-2}k^2)$ time, improving on theexisting bound of $\tilde{O}(m^{4/3} poly(k,\epsilon^{-1})$.  Our algorithms utilize several new technical tools that we believe may be ofindependent interest:  - We give a non-Euclidean generalization of gradient descent and providebounds on its performance. Using this, we show how to reduce approximatemaximum flow and maximum concurrent flow to the efficient construction ofoblivious routings with a low competitive ratio.  - We define and provide an efficient construction of a new type of flowsparsifier. In addition to providing the standard properties of a cutsparsifier our construction allows for flows in the sparse graph to be routed(very efficiently) in the original graph with low congestion.  - We give the first almost-linear-time construction of an$O(m^{o(1)})$-competitive oblivious routing scheme. No previous such algorithmran in time better than $\tilde{{\Omega}}(mn)$.  We also note that independently Jonah Sherman produced an almost linear timealgorithm for maximum flow and we thank him for coordinating submissions.

Variance Reduced Value Iteration and Faster Algorithms for Solving  Markov Decision Processes

  In this paper we provide faster algorithms for approximately solvingdiscounted Markov Decision Processes in multiple parameter regimes. Given adiscounted Markov Decision Process (DMDP) with $|S|$ states, $|A|$ actions,discount factor $\gamma\in(0,1)$, and rewards in the range $[-M, M]$, we showhow to compute an $\epsilon$-optimal policy, with probability $1 - \delta$ intime \[ \tilde{O}\left( \left(|S|^2 |A| + \frac{|S| |A|}{(1 - \gamma)^3}\right)  \log\left( \frac{M}{\epsilon} \right) \log\left( \frac{1}{\delta} \right)\right) ~ . \] This contribution reflects the first nearly linear time, nearlylinearly convergent algorithm for solving DMDPs for intermediate values of$\gamma$.  We also show how to obtain improved sublinear time algorithms provided we cansample from the transition function in $O(1)$ time. Under this assumption weprovide an algorithm which computes an $\epsilon$-optimal policy withprobability $1 - \delta$ in time \[ \tilde{O} \left(\frac{|S| |A| M^2}{(1 -\gamma)^4 \epsilon^2} \log \left(\frac{1}{\delta}\right) \right) ~. \]  Lastly, we extend both these algorithms to solve finite horizon MDPs. Ouralgorithms improve upon the previous best for approximately computing optimalpolicies for fixed-horizon MDPs in multiple parameter regimes.  Interestingly, we obtain our results by a careful modification of approximatevalue iteration. We show how to combine classic approximate value iterationanalysis with new techniques in variance reduction. Our fastest algorithmsleverage further insights to ensure that our algorithms make monotonic progresstowards the optimal value. This paper is one of few instances in using samplingto obtain a linearly convergent linear programming algorithm and we hope thatthe analysis may be useful more broadly.

Un-regularizing: approximate proximal point and faster stochastic  algorithms for empirical risk minimization

  We develop a family of accelerated stochastic algorithms that minimize sumsof convex functions. Our algorithms improve upon the fastest running time forempirical risk minimization (ERM), and in particular linear least-squaresregression, across a wide range of problem settings. To achieve this, weestablish a framework based on the classical proximal point algorithm. Namely,we provide several algorithms that reduce the minimization of a strongly convexfunction to approximate minimizations of regularizations of the function. Usingthese results, we accelerate recent fast stochastic algorithms in a black-boxfashion. Empirically, we demonstrate that the resulting algorithms exhibitnotions of stability that are advantageous in practice. Both in theory and inpractice, the provided algorithms reap the computational benefits of adding alarge strongly convex regularization term, without incurring a correspondingbias to the original problem.

Principal Component Projection Without Principal Component Analysis

  We show how to efficiently project a vector onto the top principal componentsof a matrix, without explicitly computing these components. Specifically, weintroduce an iterative algorithm that provably computes the projection usingfew calls to any black-box routine for ridge regression.  By avoiding explicit principal component analysis (PCA), our algorithm is thefirst with no runtime dependence on the number of top principal components. Weshow that it can be used to give a fast iterative method for the popularprincipal component regression problem, giving the first major runtimeimprovement over the naive method of combining PCA with regression.  To achieve our results, we first observe that ridge regression can be used toobtain a "smooth projection" onto the top principal components. We then sharpenthis approximation to true projection using a low-degree polynomialapproximation to the matrix step function. Step function approximation is atopic of long-term interest in scientific computing. We extend prior theory byconstructing polynomials with simple iterative structure and rigorouslyanalyzing their behavior under limited precision.

Accelerating Stochastic Gradient Descent For Least Squares Regression

  There is widespread sentiment that it is not possible to effectively utilizefast gradient methods (e.g. Nesterov's acceleration, conjugate gradient, heavyball) for the purposes of stochastic optimization due to their instability anderror accumulation, a notion made precise in d'Aspremont 2008 and Devolder,Glineur, and Nesterov 2014. This work considers these issues for the specialcase of stochastic approximation for the least squares regression problem, andour main result refutes the conventional wisdom by showing that accelerationcan be made robust to statistical errors. In particular, this work introducesan accelerated stochastic gradient method that provably achieves the minimaxoptimal statistical risk faster than stochastic gradient descent. Critical tothe analysis is a sharp characterization of accelerated stochastic gradientdescent as a stochastic process. We hope this characterization gives insightstowards the broader question of designing simple and effective acceleratedstochastic methods for more general convex and non-convex optimizationproblems.

"Convex Until Proven Guilty": Dimension-Free Acceleration of Gradient  Descent on Non-Convex Functions

  We develop and analyze a variant of Nesterov's accelerated gradient descent(AGD) for minimization of smooth non-convex functions. We prove that one of twocases occurs: either our AGD variant converges quickly, as if the function wasconvex, or we produce a certificate that the function is "guilty" of beingnon-convex. This non-convexity certificate allows us to exploit negativecurvature and obtain deterministic, dimension-free acceleration of convergencefor non-convex functions. For a function $f$ with Lipschitz continuous gradientand Hessian, we compute a point $x$ with $\|\nabla f(x)\| \le \epsilon$ in$O(\epsilon^{-7/4} \log(1/ \epsilon) )$ gradient and function evaluations.Assuming additionally that the third derivative is Lipschitz, we require only$O(\epsilon^{-5/3} \log(1/ \epsilon) )$ evaluations.

Derandomization Beyond Connectivity: Undirected Laplacian Systems in  Nearly Logarithmic Space

  We give a deterministic $\tilde{O}(\log n)$-space algorithm for approximatelysolving linear systems given by Laplacians of undirected graphs, andconsequently also approximating hitting times, commute times, and escapeprobabilities for undirected graphs. Previously, such systems were known to besolvable by randomized algorithms using $O(\log n)$ space (Doron, Le Gall, andTa-Shma, 2017) and hence by deterministic algorithms using $O(\log^{3/2} n)$space (Saks and Zhou, FOCS 1995 and JCSS 1999).  Our algorithm combines ideas from time-efficient Laplacian solvers (Spielmanand Teng, STOC `04; Peng and Spielman, STOC `14) with ideas used to show thatUndirected S-T Connectivity is in deterministic logspace (Reingold, STOC `05and JACM `08; Rozenman and Vadhan, RANDOM `05).

Lower Bounds for Finding Stationary Points I

  We prove lower bounds on the complexity of finding $\epsilon$-stationarypoints (points $x$ such that $\|\nabla f(x)\| \le \epsilon$) of smooth,high-dimensional, and potentially non-convex functions $f$. We consideroracle-based complexity measures, where an algorithm is given access to thevalue and all derivatives of $f$ at a query point $x$. We show that for any(potentially randomized) algorithm $\mathsf{A}$, there exists a function $f$with Lipschitz $p$th order derivatives such that $\mathsf{A}$ requires at least$\epsilon^{-(p+1)/p}$ queries to find an $\epsilon$-stationary point. Our lowerbounds are sharp to within constants, and they show that gradient descent,cubic-regularized Newton's method, and generalized $p$th order regularizationare worst-case optimal within their natural function classes.

Lower Bounds for Finding Stationary Points II: First-Order Methods

  We establish lower bounds on the complexity of finding $\epsilon$-stationarypoints of smooth, non-convex high-dimensional functions using first-ordermethods. We prove that deterministic first-order methods, even applied toarbitrarily smooth functions, cannot achieve convergence rates in $\epsilon$better than $\epsilon^{-8/5}$, which is within$\epsilon^{-1/15}\log\frac{1}{\epsilon}$ of the best known rate for suchmethods. Moreover, for functions with Lipschitz first and second derivatives,we prove no deterministic first-order method can achieve convergence ratesbetter than $\epsilon^{-12/7}$, while $\epsilon^{-2}$ is a lower bound forfunctions with only Lipschitz gradient. For convex functions with Lipschitzgradient, accelerated gradient descent achieves the rate$\epsilon^{-1}\log\frac{1}{\epsilon}$, showing that finding stationary pointsis easier given convexity.

Leverage Score Sampling for Faster Accelerated Regression and ERM

  Given a matrix $\mathbf{A}\in\mathbb{R}^{n\times d}$ and a vector $b\in\mathbb{R}^{d}$, we show how to compute an $\epsilon$-approximate solutionto the regression problem $ \min_{x\in\mathbb{R}^{d}}\frac{1}{2} \|\mathbf{A} x- b\|_{2}^{2} $ in time $ \tilde{O} ((n+\sqrt{d\cdot\kappa_{\text{sum}}})\cdots\cdot\log\epsilon^{-1}) $ where$\kappa_{\text{sum}}=\mathrm{tr}\left(\mathbf{A}^{\top}\mathbf{A}\right)/\lambda_{\min}(\mathbf{A}^{T}\mathbf{A})$and $s$ is the maximum number of non-zero entries in a row of $\mathbf{A}$. Ouralgorithm improves upon the previous best running time of $ \tilde{O}((n+\sqrt{n \cdot\kappa_{\text{sum}}})\cdot s\cdot\log\epsilon^{-1})$.  We achieve our result through a careful combination of leverage scoresampling techniques, proximal point methods, and accelerated coordinatedescent. Our method not only matches the performance of previous methods, butfurther improves whenever leverage scores of rows are small (up topolylogarithmic factors). We also provide a non-linear generalization of theseresults that improves the running time for solving a broader class of ERMproblems.

Perron-Frobenius Theory in Nearly Linear Time: Positive Eigenvectors,  M-matrices, Graph Kernels, and Other Applications

  In this paper we provide nearly linear time algorithms for several problemsclosely associated with the classic Perron-Frobenius theorem, includingcomputing Perron vectors, i.e. entrywise non-negative eigenvectors ofnon-negative matrices, and solving linear systems in asymmetric M-matrices, ageneralization of Laplacian systems. The running times of our algorithms dependnearly linearly on the input size and polylogarithmically on the desiredaccuracy and problem condition number.  Leveraging these results we also provide improved running times for a broaderrange of problems including computing random walk-based graph kernels,computing Katz centrality, and more. The running times of our algorithmsimprove upon previously known results which either depended polynomially on thecondition number of the problem, required quadratic time, or only applied tospecial cases.  We obtain these results by providing new iterative methods for reducing theseproblems to solving linear systems in Row-Column Diagonally Dominant (RCDD)matrices. Our methods are related to the classic shift-and-invertpreconditioning technique for eigenvector computation and constitute the firstalternative to the result in Cohen et al. (2016) for reducing stationarydistribution computation and solving directed Laplacian systems to solving RCDDsystems.

A Rank-1 Sketch for Matrix Multiplicative Weights

  We show that a simple randomized sketch of the matrix multiplicative weight(MMW) update enjoys the same regret bounds as MMW, up to a small constantfactor. Unlike MMW, where every step requires full matrix exponentiation, oursteps require only a single product of the form $e^A b$, which the Lanczosmethod approximates efficiently. Our key technique is to view the sketch as arandomized mirror projection, and perform mirror descent analysis on theexpected projection. Our sketch solves the online eigenvector problem,improving the best known complexity bounds. We also apply this sketch to asimple no-regret scheme for semidefinite programming in saddle-point form,where it matches the best known guarantees.

Subquadratic Submodular Function Minimization

  Submodular function minimization (SFM) is a fundamental discrete optimizationproblem which generalizes many well known problems, has applications in variousfields, and can be solved in polynomial time. Owing to applications in computervision and machine learning, fast SFM algorithms are highly desirable. Thecurrent fastest algorithms [Lee, Sidford, Wong, FOCS 2015] run in $O(n^{2}\lognM\cdot\textrm{EO} +n^{3}\log^{O(1)}nM)$ time and $O(n^{3}\log^{2}n\cdot\textrm{EO} +n^{4}\log^{O(1)}n$) time respectively, where $M$ is the largestabsolute value of the function (assuming the range is integers) and$\textrm{EO}$ is the time taken to evaluate the function on any set. Althoughthe best known lower bound on the query complexity is only $\Omega(n)$, thecurrent shortest non-deterministic proof certifying the optimum value of afunction requires $\Theta(n^{2})$ function evaluations.  The main contribution of this paper are subquadratic SFM algorithms. Forinteger-valued submodular functions, we give an SFM algorithm which runs in$O(nM^{3}\log n\cdot\textrm{EO})$ time giving the first nearly linear timealgorithm in any known regime. For real-valued submodular functions with rangein $[-1,1]$, we give an algorithm which in$\tilde{O}(n^{5/3}\cdot\textrm{EO}/\varepsilon^{2})$ time returns an$\varepsilon$-additive approximate solution. At the heart of it, our algorithmsare projected stochastic subgradient descent methods on the Lovasz extension ofsubmodular functions where we crucially exploit submodularity and datastructures to obtain fast, i.e. sublinear time subgradient updates. . Thelatter is crucial for beating the $n^{2}$ bound as we show that algorithmswhich access only subgradients of the Lovasz extension, and these include thetheoretically best algorithms mentioned above, must make $\Omega(n)$subgradient calls (even for functions whose range is $\{-1,0,1\}$).

Almost-Linear-Time Algorithms for Markov Chains and New Spectral  Primitives for Directed Graphs

  In this paper we introduce a notion of spectral approximation for directedgraphs. While there are many potential ways one might define approximation fordirected graphs, most of them are too strong to allow sparse approximations ingeneral. In contrast, we prove that for our notion of approximation, suchsparsifiers do exist, and we show how to compute them in almost linear time.  Using this notion of approximation, we provide a general framework forsolving asymmetric linear systems that is broadly inspired by the work of[Peng-Spielman, STOC`14]. Applying this framework in conjunction with oursparsification algorithm, we obtain an almost linear time algorithm for solvingdirected Laplacian systems associated with Eulerian Graphs. Using this solverin the recent framework of [Cohen-Kelner-Peebles-Peng-Sidford-Vladu, FOCS`16],we obtain almost linear time algorithms for solving a directed Laplacian linearsystem, computing the stationary distribution of a Markov chain, computingexpected commute times in a directed graph, and more.  For each of these problems, our algorithms improves the previous best runningtimes of $O((nm^{3/4} + n^{2/3} m) \log^{O(1)} (n \kappa \epsilon^{-1}))$ to$O((m + n2^{O(\sqrt{\log{n}\log\log{n}})}) \log^{O(1)} (n \kappa\epsilon^{-1}))$ where $n$ is the number of vertices in the graph, $m$ is thenumber of edges, $\kappa$ is a natural condition number associated with theproblem, and $\epsilon$ is the desired accuracy. We hope these results open thedoor for further studies into directed spectral graph theory, and will serve asa stepping stone for designing a new generation of fast algorithms for directedgraphs.

Single Pass Spectral Sparsification in Dynamic Streams

  We present the first single pass algorithm for computing spectral sparsifiersof graphs in the dynamic semi-streaming model. Given a single pass over astream containing insertions and deletions of edges to a graph G, our algorithmmaintains a randomized linear sketch of the incidence matrix of G intodimension O((1/epsilon^2) n polylog(n)). Using this sketch, at any point, thealgorithm can output a (1 +/- epsilon) spectral sparsifier for G with highprobability.  While O((1/epsilon^2) n polylog(n)) space algorithms are known for computing"cut sparsifiers" in dynamic streams [AGM12b, GKP12] and spectral sparsifiersin "insertion-only" streams [KL11], prior to our work, the best known singlepass algorithm for maintaining spectral sparsifiers in dynamic streams requiredsketches of dimension Omega((1/epsilon^2) n^(5/3)) [AGM14].  To achieve our result, we show that, using a coarse sparsifier of G and alinear sketch of G's incidence matrix, it is possible to sample edges byeffective resistance, obtaining a spectral sparsifier of arbitrary precision.Sampling from the sketch requires a novel application of ell_2/ell_2 sparserecovery, a natural extension of the ell_0 methods used for cut sparsifiers in[AGM12b]. Recent work of [MP12] on row sampling for matrix approximation givesa recursive approach for obtaining the required coarse sparsifiers.  Under certain restrictions, our approach also extends to the problem ofmaintaining a spectral approximation for a general matrix A^T A given a streamof updates to rows in A.

Uniform Sampling for Matrix Approximation

  Random sampling has become a critical tool in solving massive matrixproblems. For linear regression, a small, manageable set of data rows can berandomly selected to approximate a tall, skinny data matrix, improvingprocessing time significantly. For theoretical performance guarantees, each rowmust be sampled with probability proportional to its statistical leveragescore. Unfortunately, leverage scores are difficult to compute.  A simple alternative is to sample rows uniformly at random. While this oftenworks, uniform sampling will eliminate critical row information for manynatural instances. We take a fresh look at uniform sampling by examining whatinformation it does preserve. Specifically, we show that uniform samplingyields a matrix that, in some sense, well approximates a large fraction of theoriginal. While this weak form of approximation is not enough for solvinglinear regression directly, it is enough to compute a better approximation.  This observation leads to simple iterative row sampling algorithms for matrixapproximation that run in input-sparsity time and preserve row structure andsparsity at all intermediate steps. In addition to an improved understanding ofuniform sampling, our main proof introduces a structural result of independentinterest: we show that every matrix can be made to have low coherence byreweighting a small subset of its rows.

Path Finding I :Solving Linear Programs with Ã•(sqrt(rank)) Linear  System Solves

  In this paper we present a new algorithm for solving linear programs thatrequires only $\tilde{O}(\sqrt{rank(A)}L)$ iterations to solve a linear programwith $m$ constraints, $n$ variables, and constraint matrix $A$, and bitcomplexity $L$. Each iteration of our method consists of solving $\tilde{O}(1)$linear systems and additional nearly linear time computation.  Our method improves upon the previous best iteration bound by factor of$\tilde{\Omega}((m/rank(A))^{1/4})$ for methods with polynomial time computableiterations and by $\tilde{\Omega}((m/rank(A))^{1/2})$ for methods which solveat most $\tilde{O}(1)$ linear systems in each iteration. Our method isparallelizable and amenable to linear algebraic techniques for accelerating thelinear system solver. As such, up to polylogarithmic factors we either match orimprove upon the best previous running times in both depth and work fordifferent ratios of $m$ and $rank(A)$.  Moreover, our method matches up to polylogarithmic factors a theoreticallimit established by Nesterov and Nemirovski in 1994 regarding the use of a"universal barrier" for interior point methods, thereby resolving along-standing open question regarding the running time of polynomial timeinterior point methods for linear programming.

Streaming PCA: Matching Matrix Bernstein and Near-Optimal Finite Sample  Guarantees for Oja's Algorithm

  This work provides improved guarantees for streaming principle componentanalysis (PCA). Given $A_1, \ldots, A_n\in \mathbb{R}^{d\times d}$ sampledindependently from distributions satisfying $\mathbb{E}[A_i] = \Sigma$ for$\Sigma \succeq \mathbf{0}$, this work provides an $O(d)$-space linear-timesingle-pass streaming algorithm for estimating the top eigenvector of $\Sigma$.The algorithm nearly matches (and in certain cases improves upon) the accuracyobtained by the standard batch method that computes top eigenvector of theempirical covariance $\frac{1}{n} \sum_{i \in [n]} A_i$ as analyzed by thematrix Bernstein inequality. Moreover, to achieve constant accuracy, ouralgorithm improves upon the best previous known sample complexities ofstreaming algorithms by either a multiplicative factor of $O(d)$ or$1/\mathrm{gap}$ where $\mathrm{gap}$ is the relative distance between the toptwo eigenvalues of $\Sigma$.  These results are achieved through a novel analysis of the classic Oja'salgorithm, one of the oldest and most popular algorithms for streaming PCA. Inparticular, this work shows that simply picking a random initial point $w_0$and applying the update rule $w_{i + 1} = w_i + \eta_i A_i w_i$ suffices toaccurately estimate the top eigenvector, with a suitable choice of $\eta_i$. Webelieve our result sheds light on how to efficiently perform streaming PCA bothin theory and in practice and we hope that our analysis may serve as the basisfor analyzing many variants and extensions of streaming PCA.

Efficient Inverse Maintenance and Faster Algorithms for Linear  Programming

  In this paper, we consider the following inverse maintenance problem: given$A \in \mathbb{R}^{n\times d}$ and a number of rounds $r$, we receive a$n\times n$ diagonal matrix $D^{(k)}$ at round $k$ and we wish to maintain anefficient linear system solver for $A^{T}D^{(k)}A$ under the assumption$D^{(k)}$ does not change too rapidly. This inverse maintenance problem is thecomputational bottleneck in solving multiple optimization problems. We show howto solve this problem with $\tilde{O}(nnz(A)+d^{\omega})$ preprocessing timeand amortized $\tilde{O}(nnz(A)+d^{2})$ time per round, improving upon previousrunning times for solving this problem.  Consequently, we obtain the fastest known running times for solving multipleproblems including, linear programming and computing a rounding of a polytope.In particular given a feasible point in a linear program with $d$ variables,$n$ constraints, and constraint matrix $A\in\mathbb{R}^{n\times d}$, we showhow to solve the linear program in time$\tilde{O}(nnz(A)+d^{2})\sqrt{d}\log(\epsilon^{-1}))$. We achieve our resultsthrough a novel combination of classic numerical techniques of low rank update,preconditioning, and fast matrix multiplication as well as recent work onsubspace embeddings and spectral sparsification that we hope will be ofindependent interest.

Competing with the Empirical Risk Minimizer in a Single Pass

  In many estimation problems, e.g. linear and logistic regression, we wish tominimize an unknown objective given only unbiased samples of the objectivefunction. Furthermore, we aim to achieve this using as few samples as possible.In the absence of computational constraints, the minimizer of a sample averageof observed data -- commonly referred to as either the empirical risk minimizer(ERM) or the $M$-estimator -- is widely regarded as the estimation strategy ofchoice due to its desirable statistical convergence properties. Our goal inthis work is to perform as well as the ERM, on every problem, while minimizingthe use of computational resources such as running time and space usage.  We provide a simple streaming algorithm which, under standard regularityassumptions on the underlying problem, enjoys the following properties:  * The algorithm can be implemented in linear time with a single pass of theobserved data, using space linear in the size of a single sample.  * The algorithm achieves the same statistical rate of convergence as theempirical risk minimizer on every problem, even considering constant factors.  * The algorithm's performance depends on the initial error at a rate thatdecreases super-polynomially.  * The algorithm is easily parallelizable.  Moreover, we quantify the (finite-sample) rate at which the algorithm becomescompetitive with the ERM.

Efficient Algorithms for Large-scale Generalized Eigenvector Computation  and Canonical Correlation Analysis

  This paper considers the problem of canonical-correlation analysis (CCA)(Hotelling, 1936) and, more broadly, the generalized eigenvector problem for apair of symmetric matrices. These are two fundamental problems in data analysisand scientific computing with numerous applications in machine learning andstatistics (Shi and Malik, 2000; Hardoon et al., 2004; Witten et al., 2009).  We provide simple iterative algorithms, with improved runtimes, for solvingthese problems that are globally linearly convergent with moderate dependencieson the condition numbers and eigenvalue gaps of the matrices involved.  We obtain our results by reducing CCA to the top-$k$ generalized eigenvectorproblem. We solve this problem through a general framework that simply requiresblack box access to an approximate linear system solver. Instantiating thisframework with accelerated gradient descent we obtain a running time of$O(\frac{z k \sqrt{\kappa}}{\rho} \log(1/\epsilon) \log\left(k\kappa/\rho\right))$ where $z$ is the total number of nonzero entries,$\kappa$ is the condition number and $\rho$ is the relative eigenvalue gap ofthe appropriate matrices.  Our algorithm is linear in the input size and the number of components $k$ upto a $\log(k)$ factor. This is essential for handling large-scale matrices thatappear in practice. To the best of our knowledge this is the first suchalgorithm with global linear convergence. We hope that our results promptfurther research and ultimately improve the practical running time forperforming these important data analysis procedures on large data sets.

Faster Algorithms for Computing the Stationary Distribution, Simulating  Random Walks, and More

  In this paper, we provide faster algorithms for computing various fundamentalquantities associated with random walks on a directed graph, including thestationary distribution, personalized PageRank vectors, hitting times, andescape probabilities. In particular, on a directed graph with $n$ vertices and$m$ edges, we show how to compute each quantity in time$\tilde{O}(m^{3/4}n+mn^{2/3})$, where the $\tilde{O}$ notation suppressespolylogarithmic factors in $n$, the desired accuracy, and the appropriatecondition number (i.e. the mixing time or restart probability).  Our result improves upon the previous fastest running times for theseproblems; previous results either invoke a general purpose linear system solveron a $n\times n$ matrix with $m$ non-zero entries, or depend polynomially onthe desired error or natural condition number associated with the problem (i.e.the mixing time or restart probability). For sparse graphs, we obtain a runningtime of $\tilde{O}(n^{7/4})$, breaking the $O(n^{2})$ barrier of the bestrunning time one could hope to achieve using fast matrix multiplication.  We achieve our result by providing a similar running time improvement forsolving directed Laplacian systems, a natural directed or asymmetric analog ofthe well studied symmetric or undirected Laplacian systems. We show how tosolve such systems in time $\tilde{O}(m^{3/4}n+mn^{2/3})$, and efficientlyreduce a broad range of problems to solving $\tilde{O}(1)$ directed Laplaciansystems on Eulerian graphs. We hope these results and our analysis open thedoor for further study into directed spectral graph theory.

Approximating Cycles in Directed Graphs: Fast Algorithms for Girth and  Roundtrip Spanners

  The girth of a graph, i.e. the length of its shortest cycle, is a fundamentalgraph parameter. Unfortunately all known algorithms for computing, evenapproximately, the girth and girth-related structures in directed weighted$m$-edge and $n$-node graphs require $\Omega(\min\{n^{\omega}, mn\})$ time (for$2\leq\omega<2.373$). In this paper, we drastically improve these runtimes asfollows:  * Multiplicative Approximations in Nearly Linear Time: We give an algorithmthat in $\widetilde{O}(m)$ time computes an $\widetilde{O}(1)$-multiplicativeapproximation of the girth as well as an $\widetilde{O}(1)$-multiplicativeroundtrip spanner with $\widetilde{O}(n)$ edges with high probability (w.h.p).  * Nearly Tight Additive Approximations: For unweighted graphs and any $\alpha\in (0,1)$ we give an algorithm that in $\widetilde{O}(mn^{1 - \alpha})$ timecomputes an $O(n^\alpha)$-additive approximation of the girth w.h.p, andpartially derandomize it. We show that the runtime of our algorithm cannot besignificantly improved without a breakthrough in combinatorial Boolean matrixmultiplication.  Our main technical contribution to achieve these results is the first nearlylinear time algorithm for computing roundtrip covers, a directed graphdecomposition concept key to previous roundtrip spanner constructions.Previously it was not known how to compute these significantly faster than$\Omega(\min\{n^\omega, mn\})$ time. Given the traditional difficulty inefficiently processing directed graphs, we hope our techniques may find furtherapplications.

Stability of the Lanczos Method for Matrix Function Approximation

  The ubiquitous Lanczos method can approximate $f(A)x$ for any symmetric $n\times n$ matrix $A$, vector $x$, and function $f$. In exact arithmetic, themethod's error after $k$ iterations is bounded by the error of the bestdegree-$k$ polynomial uniformly approximating $f(x)$ on the range$[\lambda_{min}(A), \lambda_{max}(A)]$. However, despite decades of work, ithas been unclear if this powerful guarantee holds in finite precision.  We resolve this problem, proving that when $\max_{x \in [\lambda_{min},\lambda_{max}]}|f(x)| \le C$, Lanczos essentially matches the exact arithmeticguarantee if computations use roughly $\log(nC\|A\|)$ bits of precision. Ourproof extends work of Druskin and Knizhnerman [DK91], leveraging the stabilityof the classic Chebyshev recurrence to bound the stability of any polynomialapproximating $f(x)$.  We also study the special case of $f(A) = A^{-1}$, where stronger guaranteeshold. In exact arithmetic Lanczos performs as well as the best polynomialapproximating $1/x$ at each of $A$'s eigenvalues, rather than on the fulleigenvalue range. In seminal work, Greenbaum gives an approach to extendingthis bound to finite precision: she proves that finite precision Lanczos andthe related CG method match any polynomial approximating $1/x$ in a tiny rangearound each eigenvalue [Gre89].  For $A^{-1}$, this bound appears stronger than ours. However, we exhibitmatrices with condition number $\kappa$ where exact arithmetic Lanczosconverges in $polylog(\kappa)$ iterations, but Greenbaum's bound predicts$\Omega(\kappa^{1/5})$ iterations. It thus cannot offer significant improvementover the $O(\kappa^{1/2})$ bound achievable via our result. Our analysis raisesthe question of if convergence in less than $poly(\kappa)$ iterations can beexpected in finite precision, even for matrices with clustered, skewed, orotherwise favorable eigenvalue distributions.

Efficient $\widetilde{O}(n/Îµ)$ Spectral Sketches for the  Laplacian and its Pseudoinverse

  In this paper we consider the problem of efficiently computing$\epsilon$-sketches for the Laplacian and its pseudoinverse. Given a Laplacianand an error tolerance $\epsilon$, we seek to construct a function $f$ suchthat for any vector $x$ (chosen obliviously from $f$), with high probability$(1-\epsilon) x^\top A x \leq f(x) \leq (1 + \epsilon) x^\top A x$ where $A$ iseither the Laplacian or its pseudoinverse. Our goal is to construct such asketch $f$ efficiently and to store it in the least space possible.  We provide nearly-linear time algorithms that, when given a Laplacian matrix$\mathcal{L} \in \mathbb{R}^{n \times n}$ and an error tolerance $\epsilon$,produce $\tilde{O}(n/\epsilon)$-size sketches of both $\mathcal{L}$ and itspseudoinverse. Our algorithms improve upon the previous best sketch size of$\widetilde{O}(n / \epsilon^{1.6})$ for sketching the Laplacian form by Andoniet al (2015) and $O(n / \epsilon^2)$ for sketching the Laplacian pseudoinverseby Batson, Spielman, and Srivastava (2008).  Furthermore we show how to compute all-pairs effective resistances from$\widetilde{O}(n/\epsilon)$ size sketch in $\widetilde{O}(n^2/\epsilon)$ time.This improves upon the previous best running time of$\widetilde{O}(n^2/\epsilon^2)$ by Spielman and Srivastava (2008).

Near-Optimal Time and Sample Complexities for Solving Discounted Markov  Decision Process with a Generative Model

  In this paper we consider the problem of computing an $\epsilon$-optimalpolicy of a discounted Markov Decision Process (DMDP) provided we can onlyaccess its transition function through a generative sampling model that givenany state-action pair samples from the transition function in $O(1)$ time.Given such a DMDP with states $S$, actions $A$, discount factor$\gamma\in(0,1)$, and rewards in range $[0, 1]$ we provide an algorithm whichcomputes an $\epsilon$-optimal policy with probability $1 - \delta$ where\emph{both} the time spent and number of sample taken are upper bounded by \[O\left[\frac{|S||A|}{(1-\gamma)^3 \epsilon^2} \log\left(\frac{|S||A|}{(1-\gamma)\delta \epsilon}  \right)  \log\left(\frac{1}{(1-\gamma)\epsilon}\right)\right] ~. \] For fixed valuesof $\epsilon$, this improves upon the previous best known bounds by a factor of$(1 - \gamma)^{-1}$ and matches the sample complexity lower bounds proved inAzar et al. (2013) up to logarithmic factors. We also extend our method tocomputing $\epsilon$-optimal policies for finite-horizon MDP with a generativemodel and provide a nearly matching sample complexity lower bound.

Coordinate Methods for Accelerating $\ell_\infty$ Regression and Faster  Approximate Maximum Flow

  In this paper we provide faster algorithms for approximately solving$\ell_{\infty}$ regression, a fundamental problem prevalent in bothcombinatorial and continuous optimization. In particular we provide anaccelerated coordinate descent method which converges in $k$ iterations at a$O\left(\frac{1}{k}\right)$ rate independent of the dimension of the problem,and whose iterations can be implemented cheaply for many structured matrices.Our algorithm can be viewed as an alternative approach to the recentbreakthrough result of Sherman which achieves a similar running timeimprovement over classic algorithmic approaches, i.e. smoothing and gradientdescent, which either converge at a $O\left(\frac{1}{\sqrt{k}}\right)$ rate orhave running times with a worse dependence on problem parameters.  We show how to use our algorithm to achieve a runtime of $\tilde{O}\left(m +\frac{\sqrt{ns}}{\epsilon}\right)$ to compute an $\epsilon$-approximate maximumflow, for an undirected graph with $m$ edges, $n$ vertices, and where $s$ isthe squared $\ell_2$ norm of the congestion of any optimal flow. As $s = O(m)$this yields a running time of $\tilde{O}\left(m +\frac{\sqrt{nm}}{\epsilon}\right)$, generically improving upon the previousbest known runtime of $\tilde{O}\left(\frac{m}{\epsilon}\right)$ by Shermanwhenever the graph is slightly dense. Moreover, we show how to achieve improvedexact algorithms for maximum flow on a variety of unit capacity graphs.  We achieve these results by providing an accelerated coordinate descentmethod exploiting dynamic measures of coordinate smoothness for smoothed$\ell_{\infty}$ regression. Our analysis leverages the structure of the Hessianof the smoothed problem via a bound on its trace, as well as techniques forexploiting sparsity of the constraint matrix for faster sampling and smoothnessestimates.

Towards Optimal Running Times for Optimal Transport

  In this work, we provide faster algorithms for approximating the optimaltransport distance, e.g. earth mover's distance, between two discreteprobability distributions $\mu, \nu \in \Delta^n$. Given a cost function $C :[n] \times [n] \to \mathbb{R}_{\geq 0}$ where $C(i,j) \leq 1$ quantifies thepenalty of transporting a unit of mass from $i$ to $j$, we show how to computea coupling $X$ between $r$ and $c$ in time $\widetilde{O}\left(n^2 /\epsilon\right)$ whose expected transportation cost is within an additive $\epsilon$ ofoptimal. This improves upon the previously best known running time for thisproblem of $\widetilde{O}\left(\text{min}\left\{ n^{9/4}/\epsilon,n^2/\epsilon^2 \right\}\right)$.  We achieve our results by providing reductions from optimal transport tocanonical optimization problems for which recent algorithmic efforts haveprovided nearly-linear time algorithms. Leveraging nearly linear timealgorithms for solving packing linear programs and for solving the matrixbalancing problem, we obtain two separate proofs of our stated running time.Further, one of our algorithms is easily parallelized and can be implementedwith depth $\widetilde{O}(1/\epsilon)$.  Moreover, we show that further algorithmic improvements to our result wouldbe surprising in the sense that any improvement would yield an $o(n^{2.5})$algorithm for \textit{maximum cardinality bipartite matching}, for whichcurrently the only known algorithms for achieving such a result are based onfast-matrix multiplication.

Dynamic Streaming Spectral Sparsification in Nearly Linear Time and  Space

  In this paper we consider the problem of computing spectral approximations tographs in the single pass dynamic streaming model. We provide a linearsketching based solution that given a stream of edge insertions and deletionsto a $n$-node undirected graph, uses $\tilde O(n)$ space, processes each updatein $\tilde O(1)$ time, and with high probability recovers a spectral sparsifierin $\tilde O(n)$ time. Prior to our work, state of the art results either usednear optimal $\tilde O(n)$ space complexity, but brute-force $\Omega(n^2)$recovery time [Kapralov et al.'14], or with subquadratic runtime, butpolynomially suboptimal space complexity [Ahn et al.'14, Kapralov et al.'19].  Our main technical contribution is a novel method for `bucketing' vertices ofthe input graph into clusters that allows fast recovery of edges ofsufficiently large effective resistance. Our algorithm first buckets verticesof the graph by performing ball-carving using (an approximation to) itseffective resistance metric, and then recovers the high effective resistanceedges from a sketched version of an electrical flow between vertices in abucket, taking nearly linear time in the number of vertices overall. Thisprocess is performed at different geometric scales to recover a sample of edgeswith probabilities proportional to effective resistances and obtain an actualsparsifier of the input graph.  This work provides both the first efficient $\ell_2$-sparse recoveryalgorithm for graphs and new primitives for manipulating the effectiveresistance embedding of a graph, both of which we hope have furtherapplications.

Robust Shift-and-Invert Preconditioning: Faster and More Sample  Efficient Algorithms for Eigenvector Computation

  We provide faster algorithms and improved sample complexities forapproximating the top eigenvector of a matrix.  Offline Setting: Given an $n \times d$ matrix $A$, we show how to compute an$\epsilon$ approximate top eigenvector in time $\tilde O ( [nnz(A) + \frac{d\cdot sr(A)}{gap^2}]\cdot \log 1/\epsilon )$ and $\tilde O([\frac{nnz(A)^{3/4}(d \cdot sr(A))^{1/4}}{\sqrt{gap}}]\cdot \log1/\epsilon )$. Here $sr(A)$ is thestable rank and $gap$ is the multiplicative eigenvalue gap. By separating the$gap$ dependence from $nnz(A)$ we improve on the classic power and Lanczosmethods. We also improve prior work using fast subspace embeddings andstochastic optimization, giving significantly improved dependencies on $sr(A)$and $\epsilon$. Our second running time improves this further when $nnz(A) \le\frac{d\cdot sr(A)}{gap^2}$.  Online Setting: Given a distribution $D$ with covariance matrix $\Sigma$ anda vector $x_0$ which is an $O(gap)$ approximate top eigenvector for $\Sigma$,we show how to refine to an $\epsilon$ approximation using $\tildeO(\frac{v(D)}{gap^2} + \frac{v(D)}{gap \cdot \epsilon})$ samples from $D$. Here$v(D)$ is a natural variance measure. Combining our algorithm with previouswork to initialize $x_0$, we obtain a number of improved sample complexity andruntime results. For general distributions, we achieve asymptotically optimalaccuracy as a function of sample size as the number of samples grows large.  Our results center around a robust analysis of the classic method ofshift-and-invert preconditioning to reduce eigenvector computation toapproximately solving a sequence of linear systems. We then apply fast SVRGbased approximate system solvers to achieve our claims. We believe our resultssuggest the general effectiveness of shift-and-invert based approaches andimply that further computational gains may be reaped in practice.

Faster Eigenvector Computation via Shift-and-Invert Preconditioning

  We give faster algorithms and improved sample complexities for estimating thetop eigenvector of a matrix $\Sigma$ -- i.e. computing a unit vector $x$ suchthat $x^T \Sigma x \ge (1-\epsilon)\lambda_1(\Sigma)$:  Offline Eigenvector Estimation: Given an explicit $A \in \mathbb{R}^{n \timesd}$ with $\Sigma = A^TA$, we show how to compute an $\epsilon$ approximate topeigenvector in time $\tilde O([nnz(A) + \frac{d*sr(A)}{gap^2} ]* \log1/\epsilon )$ and $\tilde O([\frac{nnz(A)^{3/4} (d*sr(A))^{1/4}}{\sqrt{gap}} ]* \log 1/\epsilon )$. Here $nnz(A)$ is the number of nonzeros in $A$, $sr(A)$is the stable rank, $gap$ is the relative eigengap. By separating the $gap$dependence from the $nnz(A)$ term, our first runtime improves upon theclassical power and Lanczos methods. It also improves prior work using fastsubspace embeddings [AC09, CW13] and stochastic optimization [Sha15c], givingsignificantly better dependencies on $sr(A)$ and $\epsilon$. Our second runningtime improves these further when $nnz(A) \le \frac{d*sr(A)}{gap^2}$.  Online Eigenvector Estimation: Given a distribution $D$ with covariancematrix $\Sigma$ and a vector $x_0$ which is an $O(gap)$ approximate topeigenvector for $\Sigma$, we show how to refine to an $\epsilon$ approximationusing $ O(\frac{var(D)}{gap*\epsilon})$ samples from $D$. Here $var(D)$ is anatural notion of variance. Combining our algorithm with previous work toinitialize $x_0$, we obtain improved sample complexity and runtime resultsunder a variety of assumptions on $D$.  We achieve our results using a general framework that we believe is ofindependent interest. We give a robust analysis of the classic method ofshift-and-invert preconditioning to reduce eigenvector computation toapproximately solving a sequence of linear systems. We then apply faststochastic variance reduced gradient (SVRG) based system solvers to achieve ourclaims.

A Faster Cutting Plane Method and its Implications for Combinatorial and  Convex Optimization

  We improve upon the running time for finding a point in a convex set given aseparation oracle. In particular, given a separation oracle for a convex set$K\subset \mathbb{R}^n$ contained in a box of radius $R$, we show how to eitherfind a point in $K$ or prove that $K$ does not contain a ball of radius$\epsilon$ using an expected $O(n\log(nR/\epsilon))$ oracle evaluations andadditional time $O(n^3\log^{O(1)}(nR/\epsilon))$. This matches the oraclecomplexity and improves upon the $O(n^{\omega+1}\log(nR/\epsilon))$ additionaltime of the previous fastest algorithm achieved over 25 years ago by Vaidya forthe current matrix multiplication constant $\omega<2.373$ when$R/\epsilon=n^{O(1)}$.  Using a mix of standard reductions and new techniques, our algorithm yieldsimproved runtimes for solving classic problems in continuous and combinatorialoptimization:  Submodular Minimization: Our weakly and strongly polynomial time algorithmshave runtimes of $O(n^2\log nM\cdot\text{EO}+n^3\log^{O(1)}nM)$ and$O(n^3\log^2 n\cdot\text{EO}+n^4\log^{O(1)}n)$, improving upon the previousbest of $O((n^4\text{EO}+n^5)\log M)$ and $O(n^5\text{EO}+n^6)$.  Matroid Intersection: Our runtimes are $O(nrT_{\text{rank}}\log n\log (nM)+n^3\log^{O(1)}(nM))$ and $O(n^2\log (nM) T_{\text{ind}}+n^3 \log^{O(1)}(nM))$, achieving the first quadratic bound on the query complexity for theindependence and rank oracles. In the unweighted case, this is the firstimprovement since 1986 for independence oracle.  Submodular Flow: Our runtime is $O(n^2\lognCU\cdot\text{EO}+n^3\log^{O(1)}nCU)$, improving upon the previous bests from15 years ago roughly by a factor of $O(n^4)$.  Semidefinite Programming: Our runtime is $\tilde{O}(n(n^2+m^{\omega}+S))$,improving upon the previous best of $\tilde{O}(n(n^{\omega}+m^{\omega}+S))$ forthe regime where the number of nonzeros $S$ is small.

Routing under Balance

  We introduce the notion of balance for directed graphs: a weighted directedgraph is $\alpha$-balanced if for every cut $S \subseteq V$, the total weightof edges going from $S$ to $V\setminus S$ is within factor $\alpha$ of thetotal weight of edges going from $V\setminus S$ to $S$. Several importantfamilies of graphs are nearly balanced, in particular, Eulerian graphs (with$\alpha = 1$) and residual graphs of $(1+\epsilon)$-approximate undirectedmaximum flows (with $\alpha=O(1/\epsilon)$).  We use the notion of balance to give a more fine-grained understanding ofseveral well-studied routing questions that are considerably harder in directedgraphs. We first revisit oblivious routings in directed graphs. Our mainalgorithmic result is an oblivious routing scheme for single-source instancesthat achieve an $O(\alpha \cdot \log^3 n / \log \log n)$ competitive ratio. Inthe process, we make several technical contributions which may be ofindependent interest. In particular, we give an efficient algorithm forcomputing low-radius decompositions of directed graphs parameterized bybalance. We also define and construct low-stretch arborescences, ageneralization of low-stretch spanning trees to directed graphs.  On the negative side, we present new lower bounds for oblivious routingproblems on directed graphs. We show that the competitive ratio of obliviousrouting algorithms for directed graphs is $\Omega(n)$ in general; this resultimproves upon the long-standing best known lower bound of $\Omega(\sqrt{n})$given by Hajiaghayi, Kleinberg, Leighton and R\"acke in 2006. We also show thatour restriction to single-source instances is necessary by showing an$\Omega(\sqrt{n})$ lower bound for multiple-source oblivious routing inEulerian graphs.  We also give a fast algorithm for the maximum flow problem in balanceddirected graphs.

Parallelizing Stochastic Gradient Descent for Least Squares Regression:  mini-batching, averaging, and model misspecification

  This work characterizes the benefits of averaging schemes widely used inconjunction with stochastic gradient descent (SGD). In particular, this workprovides a sharp analysis of: (1) mini-batching, a method of averaging manysamples of a stochastic gradient to both reduce the variance of the stochasticgradient estimate and for parallelizing SGD and (2) tail-averaging, a methodinvolving averaging the final few iterates of SGD to decrease the variance inSGD's final iterate. This work presents non-asymptotic excess risk bounds forthese schemes for the stochastic approximation problem of least squaresregression.  Furthermore, this work establishes a precise problem-dependent extent towhich mini-batch SGD yields provable near-linear parallelization speedups overSGD with batch size one. This allows for understanding learning rate versusbatch size tradeoffs for the final iterate of an SGD method. These results arethen utilized in providing a highly parallelizable SGD method that obtains theminimax risk with nearly the same number of serial updates as batch gradientdescent, improving significantly over existing SGD methods. A non-asymptoticanalysis of communication efficient parallelization schemes such asmodel-averaging/parameter mixing methods is then provided.  Finally, this work sheds light on some fundamental differences in SGD'sbehavior when dealing with agnostic noise in the (non-realizable) least squaresregression problem. In particular, the work shows that the stepsizes thatensure minimax risk for the agnostic case must be a function of the noiseproperties.  This paper builds on the operator view of analyzing SGD methods, introducedby Defossez and Bach (2015), followed by developing a novel analysis inbounding these operators to characterize the excess risk. These techniques areof broader interest in analyzing computational aspects of stochasticapproximation.

Spectrum Approximation Beyond Fast Matrix Multiplication: Algorithms and  Hardness

  Understanding the singular value spectrum of a matrix $A \in \mathbb{R}^{n\times n}$ is a fundamental task in countless applications. In matrixmultiplication time, it is possible to perform a full SVD and directly computethe singular values $\sigma_1,...,\sigma_n$. However, little is known aboutalgorithms that break this runtime barrier.  Using tools from stochastic trace estimation, polynomial approximation, andfast system solvers, we show how to efficiently isolate different ranges of$A$'s spectrum and approximate the number of singular values in these ranges.We thus effectively compute a histogram of the spectrum, which can stand in forthe true singular values in many applications.  We use this primitive to give the first algorithms for approximating a wideclass of symmetric matrix norms in faster than matrix multiplication time. Forexample, we give a $(1 + \epsilon)$ approximation algorithm for theSchatten-$1$ norm (the nuclear norm) running in just $\tilde O((nnz(A)n^{1/3} +n^2)\epsilon^{-3})$ time for $A$ with uniform row sparsity or $\tildeO(n^{2.18} \epsilon^{-3})$ time for dense matrices. The runtime scales smoothlyfor general Schatten-$p$ norms, notably becoming $\tilde O (p \cdot nnz(A)\epsilon^{-3})$ for any $p \ge 2$.  At the same time, we show that the complexity of spectrum approximation isinherently tied to fast matrix multiplication in the small $\epsilon$ regime.We prove that achieving milder $\epsilon$ dependencies in our algorithms wouldimply faster than matrix multiplication time triangle detection for generalgraphs. This further implies that highly accurate algorithms running insubcubic time yield subcubic time matrix multiplication. As an application ofour bounds, we show that precisely computing all effective resistances in agraph in less than matrix multiplication time is likely difficult, barring amajor algorithmic breakthrough.

Solving Directed Laplacian Systems in Nearly-Linear Time through Sparse  LU Factorizations

  We show how to solve directed Laplacian systems in nearly-linear time. Givena linear system in an $n \times n$ Eulerian directed Laplacian with $m$ nonzeroentries, we show how to compute an $\epsilon$-approximate solution in time $O(m\log^{O(1)} (n) \log (1/\epsilon))$. Through reductions from [Cohen et al.FOCS'16] , this gives the first nearly-linear time algorithms for computing$\epsilon$-approximate solutions to row or column diagonally dominant linearsystems (including arbitrary directed Laplacians) and computing$\epsilon$-approximations to various properties of random walks on directedgraphs, including stationary distributions, personalized PageRank vectors,hitting times, and escape probabilities. These bounds improve upon the recentalmost-linear algorithms of [Cohen et al. STOC'17], which gave an algorithm tosolve Eulerian Laplacian systems in time $O((m+n2^{O(\sqrt{\log n \log \logn})})\log^{O(1)}(n \epsilon^{-1}))$.  To achieve our results, we provide a structural result that we believe is ofindependent interest. We show that Laplacians of all strongly connecteddirected graphs have sparse approximate LU-factorizations. That is, for everysuch directed Laplacian $ {\mathbf{L}}$, there is a lower triangular matrix$\boldsymbol{\mathit{{\mathfrak{L}}}}$ and an upper triangular matrix$\boldsymbol{\mathit{{\mathfrak{U}}}}$, each with at most $\tilde{O}(n)$nonzero entries, such that their product $\boldsymbol{\mathit{{\mathfrak{L}}}}\boldsymbol{\mathit{{\mathfrak{U}}}}$ spectrally approximates $ {\mathbf{L}}$in an appropriate norm. This claim can be viewed as an analogue of recent workon sparse Cholesky factorizations of Laplacians of undirected graphs. We showhow to construct such factorizations in nearly-linear time and prove that, onceconstructed, they yield nearly-linear time algorithms for solving directedLaplacian systems.

Exploiting Numerical Sparsity for Efficient Learning : Faster  Eigenvector Computation and Regression

  In this paper, we obtain improved running times for regression and topeigenvector computation for numerically sparse matrices. Given a data matrix $A\in \mathbb{R}^{n \times d}$ where every row $a \in \mathbb{R}^d$ has$\|a\|_2^2 \leq L$ and numerical sparsity at most $s$, i.e. $\|a\|_1^2 /\|a\|_2^2 \leq s$, we provide faster algorithms for these problems in manyparameter settings.  For top eigenvector computation, we obtain a running time of $\tilde{O}(nd +r(s + \sqrt{r s}) / \mathrm{gap}^2)$ where $\mathrm{gap} > 0$ is the relativegap between the top two eigenvectors of $A^\top A$ and $r$ is the stable rankof $A$. This running time improves upon the previous best unaccelerated runningtime of $O(nd + r d / \mathrm{gap}^2)$ as it is always the case that $r \leq d$and $s \leq d$.  For regression, we obtain a running time of $\tilde{O}(nd + (nL / \mu)\sqrt{s nL / \mu})$ where $\mu > 0$ is the smallest eigenvalue of $A^\top A$.This running time improves upon the previous best unaccelerated running time of$\tilde{O}(nd + n L d / \mu)$. This result expands the regimes where regressioncan be solved in nearly linear time from when $L/\mu = \tilde{O}(1)$ to when $L/ \mu = \tilde{O}(d^{2/3} / (sn)^{1/3})$.  Furthermore, we obtain similar improvements even when row norms and numericalsparsities are non-uniform and we show how to achieve even faster running timesby accelerating using approximate proximal point [Frostig et. al. 2015] /catalyst [Lin et. al. 2015]. Our running times depend only on the size of theinput and natural numerical measures of the matrix, i.e. eigenvalues and$\ell_p$ norms, making progress on a key open problem regarding optimal runningtimes for efficient large-scale learning.

