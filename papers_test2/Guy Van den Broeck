Efficient Algorithms for Bayesian Network Parameter Learning from  Incomplete Data

  We propose an efficient family of algorithms to learn the parameters of aBayesian network from incomplete data. In contrast to textbook approaches suchas EM and the gradient method, our approach is non-iterative, yields closedform parameter estimates, and eliminates the need for inference in a Bayesiannetwork. Our approach provides consistent parameter estimates for missing dataproblems that are MCAR, MAR, and in some cases, MNAR. Empirically, our approachis orders of magnitude faster than EM (as our approach requires no inference).Given sufficient data, we learn parameters that can be orders of magnitude moreaccurate.

Lifted Relax, Compensate and then Recover: From Approximate to Exact  Lifted Probabilistic Inference

  We propose an approach to lifted approximate inference for first-orderprobabilistic models, such as Markov logic networks. It is based on performingexact lifted inference in a simplified first-order model, which is found byrelaxing first-order constraints, and then compensating for the relaxation.These simplified models can be incrementally improved by carefully recoveringconstraints that have been relaxed, also at the first-order level. This leadsto a spectrum of approximations, with lifted belief propagation on one end, andexact lifted inference on the other. We discuss how relaxation, compensation,and recovery can be performed, all at the firstorder level, and showempirically that our approach substantially improves on the approximations ofboth propositional solvers and lifted belief propagation.

On the Complexity and Approximation of Binary Evidence in Lifted  Inference

  Lifted inference algorithms exploit symmetries in probabilistic models tospeed up inference. They show impressive performance when calculatingunconditional probabilities in relational models, but often resort tonon-lifted inference when computing conditional probabilities. The reason isthat conditioning on evidence breaks many of the model's symmetries, which canpreempt standard lifting techniques. Recent theoretical results show, forexample, that conditioning on evidence which corresponds to binary relations is#P-hard, suggesting that no lifting is to be expected in the worst case. Inthis paper, we balance this negative result by identifying the Boolean rank ofthe evidence as a key parameter for characterizing the complexity ofconditioning in lifted inference. In particular, we show that conditioning onbinary evidence with bounded Boolean rank is efficient. This opens up thepossibility of approximating evidence by a low-rank Boolean matrixfactorization, which we investigate both theoretically and empirically.

Skolemization for Weighted First-Order Model Counting

  First-order model counting emerged recently as a novel reasoning task, at thecore of efficient algorithms for probabilistic logics. We present aSkolemization algorithm for model counting problems that eliminates existentialquantifiers from a first-order logic theory without changing its weighted modelcount. For certain subsets of first-order logic, lifted model counters wereshown to run in time polynomial in the number of objects in the domain ofdiscourse, where propositional model counters require exponential time.However, these guarantees apply only to Skolem normal form theories (i.e., noexistential quantifiers) as the presence of existential quantifiers reduceslifted model counters to propositional ones. Since textbook Skolemization isnot sound for model counting, these restrictions precluded efficient modelcounting for directed models, such as probabilistic logic programs, which relyon existential quantification. Our Skolemization procedure extends theapplicability of first-order model counters to these representations. Moreover,it simplifies the design of lifted model counting algorithms.

Tractability through Exchangeability: A New Perspective on Efficient  Probabilistic Inference

  Exchangeability is a central notion in statistics and probability theory. Theassumption that an infinite sequence of data points is exchangeable is at thecore of Bayesian statistics. However, finite exchangeability as a statisticalproperty that renders probabilistic inference tractable is lesswell-understood. We develop a theory of finite exchangeability and its relationto tractable probabilistic inference. The theory is complementary to that ofindependence and conditional independence. We show that tractable inference inprobabilistic models with high treewidth and millions of variables can beunderstood using the notion of finite (partial) exchangeability. We also showthat existing lifted inference algorithms implicitly utilize a combination ofconditional independence and partial exchangeability.

On the Role of Canonicity in Bottom-up Knowledge Compilation

  We consider the problem of bottom-up compilation of knowledge bases, which isusually predicated on the existence of a polytime function for combiningcompilations using Boolean operators (usually called an Apply function). Whilesuch a polytime Apply function is known to exist for certain languages (e.g.,OBDDs) and not exist for others (e.g., DNNF), its existence for certainlanguages remains unknown. Among the latter is the recently introduced languageof Sentential Decision Diagrams (SDDs), for which a polytime Apply functionexists for unreduced SDDs, but remains unknown for reduced ones (i.e. canonicalSDDs). We resolve this open question in this paper and consider some of itstheoretical and practical implications. Some of the findings we report questionthe common wisdom on the relationship between bottom-up compilation, languagecanonicity and the complexity of the Apply function.

Lifted Probabilistic Inference for Asymmetric Graphical Models

  Lifted probabilistic inference algorithms have been successfully applied to alarge number of symmetric graphical models. Unfortunately, the majority ofreal-world graphical models is asymmetric. This is even the case for relationalrepresentations when evidence is given. Therefore, more recent work in thecommunity moved to making the models symmetric and then applying existinglifted inference algorithms. However, this approach has two shortcomings.First, all existing over-symmetric approximations require a relationalrepresentation such as Markov logic networks. Second, the induced symmetriesoften change the distribution significantly, making the computed probabilitieshighly biased. We present a framework for probabilistic sampling-basedinference that only uses the induced approximate symmetries to propose steps ina Metropolis-Hastings style Markov chain. The framework, therefore, leads toimproved probability estimates while remaining unbiased. Experimentsdemonstrate that the approach outperforms existing MCMC algorithms.

Lifted Variable Elimination: A Novel Operator and Completeness Results

  Various methods for lifted probabilistic inference have been proposed, butour understanding of these methods and the relationships between them is stilllimited, compared to their propositional counterparts. The only existingtheoretical characterization of lifting is for weighted first-order modelcounting (WFOMC), which was shown to be complete domain-lifted for the class of2-logvar models. This paper makes two contributions to lifted variableelimination (LVE). First, we introduce a novel inference operator called groupinversion. Second, we prove that LVE augmented with this operator is completein the same sense as WFOMC.

Algebraic Model Counting

  Weighted model counting (WMC) is a well-known inference task on knowledgebases, used for probabilistic inference in graphical models. We introducealgebraic model counting (AMC), a generalization of WMC to a semiringstructure. We show that AMC generalizes many well-known tasks in a variety ofdomains such as probabilistic inference, soft constraints and network anddatabase analysis. Furthermore, we investigate AMC from a knowledge compilationperspective and show that all AMC tasks can be evaluated using sd-DNNFcircuits. We identify further characteristics of AMC instances that allow forthe use of even more succinct circuits.

Probabilistic Program Abstractions

  Abstraction is a fundamental tool for reasoning about complex systems.Program abstraction has been utilized to great effect for analyzingdeterministic programs. At the heart of program abstraction is the relationshipbetween a concrete program, which is difficult to analyze, and an abstractprogram, which is more tractable. Program abstractions, however, are typicallynot probabilistic. We generalize non-deterministic program abstractions toprobabilistic program abstractions by explicitly quantifying thenon-deterministic choices. Our framework upgrades key definitions andproperties of abstractions to the probabilistic context. We also discusspreliminary ideas for performing inference on probabilistic abstractions andgeneral probabilistic programs.

On Robust Trimming of Bayesian Network Classifiers

  This paper considers the problem of removing costly features from a Bayesiannetwork classifier. We want the classifier to be robust to these changes, andmaintain its classification behavior. To this end, we propose a closenessmetric between Bayesian classifiers, called the expected classificationagreement (ECA). Our corresponding trimming algorithm finds an optimal subsetof features and a new classification threshold that maximize the expectedagreement, subject to a budgetary constraint. It utilizes new theoreticalinsights to perform branch-and-bound search in the space of feature sets, whilecomputing bounds on the ECA. Our experiments investigate both the runtime costof trimming and its effect on the robustness and accuracy of the finalclassifier.

Learning Logistic Circuits

  This paper proposes a new classification model called logistic circuits. OnMNIST and Fashion datasets, our learning algorithm outperforms neural networksthat have an order of magnitude more parameters. Yet, logistic circuits have adistinct origin in symbolic AI, forming a discriminative counterpart toprobabilistic-logical circuits such as ACs, SPNs, and PSDDs. We show thatparameter learning for logistic circuits is convex optimization, and that asimple local search algorithm can induce strong model structures from data.

Inference and learning in probabilistic logic programs using weighted  Boolean formulas

  Probabilistic logic programs are logic programs in which some of the factsare annotated with probabilities. This paper investigates how classicalinference and learning tasks known from the graphical model community can betackled for probabilistic logic programs. Several such tasks such as computingthe marginals given evidence and learning from (partial) interpretations havenot really been addressed for probabilistic logic programs before.  The first contribution of this paper is a suite of efficient algorithms forvarious inference tasks. It is based on a conversion of the program and thequeries and evidence to a weighted Boolean formula. This allows us to reducethe inference tasks to well-studied tasks such as weighted model counting,which can be solved using state-of-the-art methods known from the graphicalmodel and knowledge compilation literature. The second contribution is analgorithm for parameter estimation in the learning from interpretationssetting. The algorithm employs Expectation Maximization, and is built on top ofthe developed inference algorithms.  The proposed approach is experimentally evaluated. The results show that theinference algorithms improve upon the state-of-the-art in probabilistic logicprogramming and that it is indeed possible to learn the parameters of aprobabilistic logic program from interpretations.

Symmetric Weighted First-Order Model Counting

  The FO Model Counting problem (FOMC) is the following: given a sentence$\Phi$ in FO and a number $n$, compute the number of models of $\Phi$ over adomain of size $n$; the Weighted variant (WFOMC) generalizes the problem byassociating a weight to each tuple and defining the weight of a model to be theproduct of weights of its tuples. In this paper we study the complexity of thesymmetric WFOMC, where all tuples of a given relation have the same weight. Ourmotivation comes from an important application, inference in Knowledge Baseswith soft constraints, like Markov Logic Networks, but the problem is also ofindependent theoretical interest. We study both the data complexity, and thecombined complexity of FOMC and WFOMC. For the data complexity we prove theexistence of an FO$^{3}$ formula for which FOMC is #P$_1$-complete, and theexistence of a Conjunctive Query for which WFOMC is #P$_1$-complete. We alsoprove that all $\gamma$-acyclic queries have polynomial time data complexity.For the combined complexity, we prove that, for every fragment FO$^{k}$, $k\geq2$, the combined complexity of FOMC (or WFOMC) is #P-complete.

Inference in Probabilistic Logic Programs using Weighted CNF's

  Probabilistic logic programs are logic programs in which some of the factsare annotated with probabilities. Several classical probabilistic inferencetasks (such as MAP and computing marginals) have not yet received a lot ofattention for this formalism. The contribution of this paper is that we developefficient inference algorithms for these tasks. This is based on a conversionof the probabilistic logic program and the query and evidence to a weighted CNFformula. This allows us to reduce the inference tasks to well-studied taskssuch as weighted model counting. To solve such tasks, we employstate-of-the-art methods. We consider multiple methods for the conversion ofthe programs as well as for inference on the weighted CNF. The resultingapproach is evaluated experimentally and shown to improve upon thestate-of-the-art in probabilistic logic programming.

Understanding the Complexity of Lifted Inference and Asymmetric Weighted  Model Counting

  In this paper we study lifted inference for the Weighted First-Order ModelCounting problem (WFOMC), which counts the assignments that satisfy a givensentence in first-order logic (FOL); it has applications in StatisticalRelational Learning (SRL) and Probabilistic Databases (PDB). We present severalresults. First, we describe a lifted inference algorithm that generalizes priorapproaches in SRL and PDB. Second, we provide a novel dichotomy result for anon-trivial fragment of FO CNF sentences, showing that for each sentence theWFOMC problem is either in PTIME or #P-hard in the size of the input domain; weprove that, in the first case our algorithm solves the WFOMC problem in PTIME,and in the second case it fails. Third, we present several properties of thealgorithm. Finally, we discuss limitations of lifted inference for symmetricprobabilistic databases (where the weights of ground literals depend only onthe relation name, and not on the constants of the domain), and prove theimpossibility of a dichotomy result for the complexity of probabilisticinference for the entire language FOL.

Knowledge Compilation of Logic Programs Using Approximation Fixpoint  Theory

  To appear in Theory and Practice of Logic Programming (TPLP), Proceedings ofICLP 2015  Recent advances in knowledge compilation introduced techniques to compile\emph{positive} logic programs into propositional logic, essentially exploitingthe constructive nature of the least fixpoint computation. This approach hasseveral advantages over existing approaches: it maintains logical equivalence,does not require (expensive) loop-breaking preprocessing or the introduction ofauxiliary variables, and significantly outperforms existing algorithms.Unfortunately, this technique is limited to \emph{negation-free} programs. Inthis paper, we show how to extend it to general logic programs under thewell-founded semantics.  We develop our work in approximation fixpoint theory, an algebraicalframework that unifies semantics of different logics. As such, our algebraicalresults are also applicable to autoepistemic logic, default logic and abstractdialectical frameworks.

Quantifying Causal Effects on Query Answering in Databases

  The notion of actual causation, as formalized by Halpern and Pearl, has beenrecently applied to relational databases, to characterize and compute actualcauses for possibly unexpected answers to monotone queries. Causes take theform of database tuples, and can be ranked according to their causalresponsibility, a numerical measure of their relevance as a cause to the queryanswer. In this work we revisit this notion, introducing and making a case foran alternative measure of causal contribution, that of causal effect. Themeasure generalizes actual causes, and can be applied beyond monotone queries.We show that causal effect provides intuitive and intended results.

New Liftable Classes for First-Order Probabilistic Inference

  Statistical relational models provide compact encodings of probabilisticdependencies in relational domains, but result in highly intractable graphicalmodels. The goal of lifted inference is to carry out probabilistic inferencewithout needing to reason about each individual separately, by instead treatingexchangeable, undistinguished objects as a whole. In this paper, we study thedomain recursion inference rule, which, despite its central role in earlytheoretical results on domain-lifted inference, has later been believedredundant. We show that this rule is more powerful than expected, and in factsignificantly extends the range of models for which lifted inference runs intime polynomial in the number of individuals in the domain. This includes anopen problem called S4, the symmetric transitivity model, and a first-orderlogic encoding of the birthday paradox. We further identify new classes S2FO2and S2RU of domain-liftable theories, which respectively subsume FO2 andrecursively unary theories, the largest classes of domain-liftable theoriesknown so far, and show that using domain recursion can achieve exponentialspeedup even in theories that cannot fully be lifted with the existing set ofinference rules.

Don't Fear the Bit Flips: Optimized Coding Strategies for Binary  Classification

  After being trained, classifiers must often operate on data that has beencorrupted by noise. In this paper, we consider the impact of such noise on thefeatures of binary classifiers. Inspired by tools for classifier robustness, weintroduce the same classification probability (SCP) to measure the resultingdistortion on the classifier outputs. We introduce a low-complexity estimate ofthe SCP based on quantization and polynomial multiplication. We also studychannel coding techniques based on replication error-correcting codes. Incontrast to the traditional channel coding approach, where error-correction ismeant to preserve the data and is agnostic to the application, our schemesspecifically aim to maximize the SCP (equivalently minimizing the distortion ofthe classifier output) for the same redundancy overhead.

A Semantic Loss Function for Deep Learning with Symbolic Knowledge

  This paper develops a novel methodology for using symbolic knowledge in deeplearning. From first principles, we derive a semantic loss function thatbridges between neural output vectors and logical constraints. This lossfunction captures how close the neural network is to satisfying the constraintson its output. An experimental evaluation shows that it effectively guides thelearner to achieve (near-)state-of-the-art results on semi-supervisedmulti-class classification. Moreover, it significantly increases the ability ofthe neural network to predict structured objects, such as rankings and paths.These discrete concepts are tremendously difficult to learn, and benefit from atight integration of deep learning and symbolic reasoning methods.

Approximate Knowledge Compilation by Online Collapsed Importance  Sampling

  We introduce collapsed compilation, a novel approximate inference algorithmfor discrete probabilistic graphical models. It is a collapsed samplingalgorithm that incrementally selects which variable to sample next based on thepartial sample obtained so far. This online collapsing, together with knowledgecompilation inference on the remaining variables, naturally exploits localstructure and context- specific independence in the distribution. Theseproperties are naturally exploited in exact inference, but are difficult toharness for approximate inference. More- over, by having a partially compiledcircuit available during sampling, collapsed compilation has access to a highlyeffective proposal distribution for importance sampling. Our experimentalevaluation shows that collapsed compilation performs well on standardbenchmarks. In particular, when the amount of exact inference is equallylimited, collapsed compilation is competitive with the state of the art, andoutperforms it on several benchmarks.

On Constrained Open-World Probabilistic Databases

  Increasing amounts of available data have led to a heightened need forrepresenting large-scale probabilistic knowledge bases. One approach is to usea probabilistic database, a model with strong assumptions that allow forefficiently answering many interesting queries. Recent work on open-worldprobabilistic databases strengthens the semantics of these probabilisticdatabases by discarding the assumption that any information not present in thedata must be false. While intuitive, these semantics are not sufficientlyprecise to give reasonable answers to queries. We propose overcoming theseissues by using constraints to restrict this open world. We provide analgorithm for one class of queries, and establish a basic hardness result foranother. Finally, we propose an efficient and tight approximation for a largeclass of queries.

What to Expect of Classifiers? Reasoning about Logistic Regression with  Missing Features

  While discriminative classifiers often yield strong predictive performance,missing feature values at prediction time can still be a challenge. Classifiersmay not behave as expected under certain ways of substituting the missingvalues, since they inherently make assumptions about the data distribution theywere trained on. In this paper, we propose a novel framework that classifiesexamples with missing features by computing the expected prediction on a givenfeature distribution. We then use geometric programming to learn a naive Bayesdistribution that embeds a given logistic regression classifier and canefficiently take its expected predictions. Empirical evaluations show that ourmodel achieves the same performance as the logistic regression with allfeatures observed, and outperforms standard imputation techniques when featuresgo missing during prediction time. Furthermore, we demonstrate that our methodcan be used to generate 'sufficient explanations' of logistic regressionclassifications, by removing features that do not affect the classification.

Generating and Sampling Orbits for Lifted Probabilistic Inference

  Lifted inference scales to large probability models by exploiting symmetry.However, existing exact lifted inference techniques do not apply to generalfactor graphs, as they require a relational representation. In this work weprovide a theoretical framework and algorithm for performing exact liftedinference on symmetric factor graphs by computing colored graph automorphisms,as is often done for approximate lifted inference. Our key insight is torepresent variable assignments directly in the colored factor graph encoding.This allows us to generate representatives and compute the size of each orbitof the symmetric distribution. In addition to exact inference, we use thisencoding to implement an MCMC algorithm that explores the space of orbitsquickly by uniform orbit sampling.

Efficient Search-Based Weighted Model Integration

  Weighted model integration (WMI) extends Weighted model counting (WMC) to theintegration of functions over mixed discrete-continuous domains. It has showntremendous promise for solving inference problems in graphical models andprobabilistic programming. Yet, state-of-the-art tools for WMI are limited interms of performance and ignore the independence structure that is crucial toimproving efficiency. To address this limitation, we propose an efficient modelintegration algorithm for theories with tree primal graphs. We exploit thesparse graph structure by using search to performing integration. Our algorithmgreatly improves the computational efficiency on such problems and exploitscontext-specific independence between variables. Experimental results showdramatic speedups compared to existing WMI solvers on problems with tree-shapeddependencies.

Symbolic Exact Inference for Discrete Probabilistic Programs

  The computational burden of probabilistic inference remains a hurdle forapplying probabilistic programming languages to practical problems of interest.In this work, we provide a semantic and algorithmic foundation for efficientexact inference on discrete-valued finite-domain imperative probabilisticprograms. We leverage and generalize efficient inference procedures forBayesian networks, which exploit the structure of the network to decompose theinference task, thereby avoiding full path enumeration. To do this, we firstcompile probabilistic programs to a symbolic representation. Then we adapttechniques from the probabilistic logic programming and artificial intelligencecommunities in order to perform inference on the symbolic representation. Weformalize our approach, prove it sound, and experimentally validate it againstexisting exact and approximate inference techniques. We show that our inferenceapproach is competitive with inference procedures specialized for Bayesiannetworks, thereby expanding the class of probabilistic programs which can bepractically analyzed.

Domain Recursion for Lifted Inference with Existential Quantifiers

  In recent work, we proved that the domain recursion inference rule makesdomain-lifted inference possible on several relational probability models(RPMs) for which the best known time complexity used to be exponential. We alsoidentified two classes of RPMs for which inference becomes domain lifted whenusing domain recursion. These two classes subsume the largest lifted classesthat were previously known. In this paper, we show that domain recursion canalso be applied to models with existential quantifiers. Currently, all liftedinference algorithms assume that existential quantifiers have been removed inpre-processing by Skolemization. We show that besides introducing potentiallyinconvenient negative weights, Skolemization may increase the time complexityof inference. We give two example models where domain recursion can replaceSkolemization, avoids the need for dealing with negative numbers, and reducesthe time complexity of inference. These two examples may be interesting fromthree theoretical aspects: 1- they provide a better and deeper understanding ofdomain recursion and, in general, (lifted) inference, 2- they may serve asevidence that there are larger classes of models for which domain recursion cansatisfyingly replace Skolemization, and 3- they may serve as evidence thatbetter Skolemization techniques exist.

Active Inductive Logic Programming for Code Search

  Modern search techniques either cannot efficiently incorporate human feedbackto refine search results or to express structural or semantic properties ofdesired code. The key insight of our interactive code search technique ALICE isthat user feedback could be actively incorporated to allow users to easilyexpress and refine search queries. We design a query language to model thestructure and semantics of code as logic facts. Given a code example with userannotations, ALICE automatically extracts a logic query from features that aretagged as important. Users can refine the search query by labeling one or moreexamples as desired (positive) or irrelevant (negative). ALICE then infers anew logic query that separates the positives from negative examples via activeinductive logic programming. Our comprehensive and systematic simulationexperiment shows that ALICE removes a large number of false positives quicklyby actively incorporating user feedback. Its search algorithm is also robust tonoise and user labeling mistakes. Our choice of leveraging both positive andnegative examples and the nested containment structure of selected code iseffective in refining search queries. Compared with an existing technique,Critics, ALICE does not require a user to manually construct a search patternand yet achieves comparable precision and recall with fewer search iterationson average. A case study with users shows that ALICE is easy to use and helpsexpress complex code patterns.

