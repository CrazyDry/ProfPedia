Contextual Motifs: Increasing the Utility of Motifs using Contextual  Data

  Motifs are a powerful tool for analyzing physiological waveform data.Standard motif methods, however, ignore important contextual information (e.g.,what the patient was doing at the time the data were collected). We hypothesizethat these additional contextual data could increase the utility of motifs.Thus, we propose an extension to motifs, contextual motifs, that incorporatescontext. Recognizing that, oftentimes, context may be unobserved orunavailable, we focus on methods to jointly infer motifs and context. Appliedto both simulated and real physiological data, our proposed approach improvesupon existing motif methods in terms of the discriminative utility of thediscovered motifs. In particular, we discovered contextual motifs in continuousglucose monitor (CGM) data collected from patients with type 1 diabetes.Compared to their contextless counterparts, these contextual motifs led tobetter predictions of hypo- and hyperglycemic events. Our results suggest thateven when inferred, context is useful in both a long- and short-term predictionhorizon when processing and interpreting physiological waveform data.

Learning Credible Models

  In many settings, it is important that a model be capable of providingreasons for its predictions (i.e., the model must be interpretable). However,the model's reasoning may not conform with well-established knowledge. In suchcases, while interpretable, the model lacks \textit{credibility}. In this work,we formally define credibility in the linear setting and focus on techniquesfor learning models that are both accurate and credible. In particular, wepropose a regularization penalty, expert yielded estimates (EYE), thatincorporates expert knowledge about well-known relationships among covariatesand the outcome of interest. We give both theoretical and empirical resultscomparing our proposed method to several other regularization techniques.Across a range of settings, experiments on both synthetic and real data showthat models learned using the EYE penalty are significantly more credible thanthose learned using other penalties. Applied to a large-scale patient riskstratification task, our proposed technique results in a model whose topfeatures overlap significantly with known clinical risk factors, while stillachieving good predictive performance.

Learning the Probability of Activation in the Presence of Latent  Spreaders

  When an infection spreads in a community, an individual's probability ofbecoming infected depends on both her susceptibility and exposure to thecontagion through contact with others. While one often has knowledge regardingan individual's susceptibility, in many cases, whether or not an individual'scontacts are contagious is unknown. We study the problem of predicting if anindividual will adopt a contagion in the presence of multiple modes ofinfection (exposure/susceptibility) and latent neighbor influence. We present agenerative probabilistic model and a variational inference method to learn theparameters of our model. Through a series of experiments on synthetic data, wemeasure the ability of the proposed model to identify latent spreaders, andpredict the risk of infection. Applied to a real dataset of 20,000 hospitalpatients, we demonstrate the utility of our model in predicting the onset of ahealthcare associated infection using patient room-sharing and nurse-sharingnetworks. Our model outperforms existing benchmarks and provides actionableinsights for the design and implementation of targeted interventions to curbthe spread of infection.

The Advantage of Doubling: A Deep Reinforcement Learning Approach to  Studying the Double Team in the NBA

  During the 2017 NBA playoffs, Celtics coach Brad Stevens was faced with adifficult decision when defending against the Cavaliers: "Do you double andrisk giving up easy shots, or stay at home and do the best you can?" It's atough call, but finding a good defensive strategy that effectively incorporatesdoubling can make all the difference in the NBA. In this paper, we analyzedouble teaming in the NBA, quantifying the trade-off between risk and reward.Using player trajectory data pertaining to over 643,000 possessions, weidentified when the ball handler was double teamed. Given these data and thecorresponding outcome (i.e., was the defense successful), we used deepreinforcement learning to estimate the quality of the defensive actions. Wepresent qualitative and quantitative results summarizing our learned defensivestrategy for defending. We show that our policy value estimates are predictiveof points per possession and win percentage. Overall, the proposed frameworkrepresents a step toward a more comprehensive understanding of defensivestrategies in the NBA.

Learning to Exploit Invariances in Clinical Time-Series Data using  Sequence Transformer Networks

  Recently, researchers have started applying convolutional neural networks(CNNs) with one-dimensional convolutions to clinical tasks involvingtime-series data. This is due, in part, to their computational efficiency,relative to recurrent neural networks and their ability to efficiently exploitcertain temporal invariances, (e.g., phase invariance). However, it iswell-established that clinical data may exhibit many other types of invariances(e.g., scaling). While preprocessing techniques, (e.g., dynamic time warping)may successfully transform and align inputs, their use often requires one toidentify the types of invariances in advance. In contrast, we propose the useof Sequence Transformer Networks, an end-to-end trainable architecture thatlearns to identify and account for invariances in clinical time-series data.Applied to the task of predicting in-hospital mortality, our proposed approachachieves an improvement in the area under the receiver operating characteristiccurve (AUROC) relative to a baseline CNN (AUROC=0.851 vs. AUROC=0.838). Ourresults suggest that a variety of valuable invariances can be learned directlyfrom the data.

Leveraging Clinical Time-Series Data for Prediction: A Cautionary Tale

  In healthcare, patient risk stratification models are often learned usingtime-series data extracted from electronic health records. When extracting datafor a clinical prediction task, several formulations exist, depending on howone chooses the time of prediction and the prediction horizon. In this paper,we show how the formulation can greatly impact both model performance andclinical utility. Leveraging a publicly available ICU dataset, we consider twoclinical prediction tasks: in-hospital mortality, and hypokalemia. Throughthese case studies, we demonstrate the necessity of evaluating models using anoutcome-independent reference point, since choosing the time of predictionrelative to the event can result in unrealistic performance. Further, anoutcome-independent scheme outperforms an outcome-dependent scheme on bothtasks (In-Hospital Mortality AUROC .882 vs. .831; Serum Potassium: AUROC .829vs. .740) when evaluated on test sets that mimic real-world use.

Clinically Meaningful Comparisons Over Time: An Approach to Measuring  Patient Similarity based on Subsequence Alignment

  Longitudinal patient data has the potential to improve clinical riskstratification models for disease. However, chronic diseases that progressslowly over time are often heterogeneous in their clinical presentation.Patients may progress through disease stages at varying rates. This leads topathophysiological misalignment over time, making it difficult to consistentlycompare patients in a clinically meaningful way. Furthermore, patients presentclinically for the first time at different stages of disease. This eliminatesthe possibility of simply aligning patients based on their initialpresentation. Finally, patient data may be sampled at different rates due todifferences in schedules or missed visits. To address these challenges, wepropose a robust measure of patient similarity based on subsequence alignment.Compared to global alignment techniques that do not account forpathophysiological misalignment, focusing on the most relevant subsequencesallows for an accurate measure of similarity between patients. We demonstratethe utility of our approach in settings where longitudinal data, while useful,are limited and lack a clear temporal alignment for comparison. Applied to thetask of stratifying patients for risk of progression to probable Alzheimer'sDisease, our approach outperforms models that use only snapshot data (AUROC of0.839 vs. 0.812) and models that use global alignment techniques (AUROC of0.822). Our results support the hypothesis that patients' trajectories areuseful for quantifying inter-patient similarities and that using subsequencematching and can help account for heterogeneity and misalignment inlongitudinal data.

Deep Multi-Output Forecasting: Learning to Accurately Predict Blood  Glucose Trajectories

  In many forecasting applications, it is valuable to predict not only thevalue of a signal at a certain time point in the future, but also the valuesleading up to that point. This is especially true in clinical applications,where the future state of the patient can be less important than the patient'soverall trajectory. This requires multi-step forecasting, a forecasting variantwhere one aims to predict multiple values in the future simultaneously.Standard methods to accomplish this can propagate error from prediction toprediction, reducing quality over the long term. In light of these challenges,we propose multi-output deep architectures for multi-step forecasting in whichwe explicitly model the distribution of future values of the signal over aprediction horizon. We apply these techniques to the challenging and clinicallyrelevant task of blood glucose forecasting. Through a series of experiments ona real-world dataset consisting of 550K blood glucose measurements, wedemonstrate the effectiveness of our proposed approaches in capturing theunderlying signal dynamics. Compared to existing shallow and deep methods, wefind that our proposed approaches improve performance individually and capturecomplementary information, leading to a large improvement over the baselinewhen combined (4.87 vs. 5.31 absolute percentage error (APE)). Overall, theresults suggest the efficacy of our proposed approach in predicting bloodglucose level and multi-step forecasting more generally.

A Domain Guided CNN Architecture for Predicting Age from Structural  Brain Images

  Given the wide success of convolutional neural networks (CNNs) applied tonatural images, researchers have begun to apply them to neuroimaging data. Todate, however, exploration of novel CNN architectures tailored to neuroimagingdata has been limited. Several recent works fail to leverage the 3D structureof the brain, instead treating the brain as a set of independent 2D slices.Approaches that do utilize 3D convolutions rely on architectures developed forobject recognition tasks in natural 2D images. Such architectures makeassumptions about the input that may not hold for neuroimaging. For example,existing architectures assume that patterns in the brain exhibit translationinvariance. However, a pattern in the brain may have different meaningdepending on where in the brain it is located. There is a need to explore novelarchitectures that are tailored to brain images. We present two simplemodifications to existing CNN architectures based on brain image structure.Applied to the task of brain age prediction, our network achieves a meanabsolute error (MAE) of 1.4 years and trains 30% faster than a CNN baselinethat achieves a MAE of 1.6 years. Our results suggest that lessons learned fromdeveloping models on natural images may not directly transfer to neuroimagingtasks. Instead, there remains a large space of unexplored questions regardingmodel development in this area, whose answers may differ from conventionalwisdom.

