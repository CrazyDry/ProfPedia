Constructing Abstraction Hierarchies Using a Skill-Symbol Loop

  We describe a framework for building abstraction hierarchies whereby an agentalternates skill- and representation-acquisition phases to construct a sequenceof increasingly abstract Markov decision processes. Our formulation builds onrecent results showing that the appropriate abstract representation of aproblem is specified by the agent's skills. We describe how such a hierarchycan be used for fast planning, and illustrate the construction of anappropriate hierarchy for the Taxi domain.

Hidden Parameter Markov Decision Processes: A Semiparametric Regression  Approach for Discovering Latent Task Parametrizations

  Control applications often feature tasks with similar, but not identical,dynamics. We introduce the Hidden Parameter Markov Decision Process (HiP-MDP),a framework that parametrizes a family of related dynamical systems with alow-dimensional set of latent factors, and introduce a semiparametricregression approach for learning its structure from data. In the controlsetting, we show that a learned HiP-MDP rapidly identifies the dynamics of anew task instance, allowing an agent to flexibly adapt to task variations.

Reinforcement Learning with Parameterized Actions

  We introduce a model-free algorithm for learning in Markov decision processeswith parameterized actions-discrete actions with continuous parameters. At eachstep the agent must select both which action to use and which parameters to usewith that action. We introduce the Q-PAMDP algorithm for learning in thesedomains, show that it converges to a local optimum, and compare it to directpolicy search in the goal-scoring and Platform domains.

Mean Actor Critic

  We propose a new algorithm, Mean Actor-Critic (MAC), for discrete-actioncontinuous-state reinforcement learning. MAC is a policy gradient algorithmthat uses the agent's explicit representation of all action values to estimatethe gradient of the policy, rather than using only the actions that wereactually executed. We prove that this approach reduces variance in the policygradient estimate relative to traditional actor-critic methods. We showempirical results on two control domains and on six Atari games, where MAC iscompetitive with state-of-the-art policy search algorithms.

Active Exploration for Learning Symbolic Representations

  We introduce an online active exploration algorithm for data-efficientlylearning an abstract symbolic model of an environment. Our algorithm is dividedinto two parts: the first part quickly generates an intermediate Bayesiansymbolic model from the data that the agent has collected so far, which theagent can then use along with the second part to guide its future explorationtowards regions of the state space that the model is uncertain about. We showthat our algorithm outperforms random and greedy exploration policies on twodifferent computer game domains. The first domain is an Asteroids-inspired gamewith complex dynamics but basic logical structure. The second is the TreasureGame, with simpler dynamics but more complex logical structure.

Discovering Options for Exploration by Minimizing Cover Time

  One of the main challenges in reinforcement learning is solving tasks withsparse reward. We show that the difficulty of discovering a distant rewardingstate in an MDP is bounded by the expected cover time of a random walk over thegraph induced by the MDP's transition dynamics. We therefore propose toaccelerate exploration by constructing options that minimize cover time. Theproposed algorithm finds an option which provably diminishes the expectednumber of steps to visit every state in the state space by a uniform randomwalk. We show empirically that the proposed algorithm improves the learningtime in several domains with sparse rewards.

Learning Parameterized Skills

  We introduce a method for constructing skills capable of solving tasks drawnfrom a distribution of parameterized reinforcement learning problems. Themethod draws example tasks from a distribution of interest and uses thecorresponding learned policies to estimate the topology of thelower-dimensional piecewise-smooth manifold on which the skill policies lie.This manifold models how policy parameters change as task parameters vary. Themethod identifies the number of charts that compose the manifold and thenapplies non-linear regression in each chart to construct a parameterized skillby predicting policy parameters from task parameters. We evaluate our method onan underactuated simulated robotic arm tasked with learning to accurately throwdarts at a parameterized target location.

Transfer Learning Across Patient Variations with Hidden Parameter Markov  Decision Processes

  Due to physiological variation, patients diagnosed with the same conditionmay exhibit divergent, but related, responses to the same treatments. HiddenParameter Markov Decision Processes (HiP-MDPs) tackle this transfer-learningproblem by embedding these tasks into a low-dimensional space. However, theoriginal formulation of HiP-MDP had a critical flaw: the embedding uncertaintywas modeled independently of the agent's state uncertainty, requiring anunnatural training procedure in which all tasks visited every part of the statespace---possible for robots that can be moved to a particular location,impossible for human patients. We update the HiP-MDP framework and extend it tomore robustly develop personalized medicine strategies for HIV treatment.

Robust and Efficient Transfer Learning with Hidden-Parameter Markov  Decision Processes

  We introduce a new formulation of the Hidden Parameter Markov DecisionProcess (HiP-MDP), a framework for modeling families of related tasks usinglow-dimensional latent embeddings. Our new framework correctly models the jointuncertainty in the latent parameters and the state space. We also replace theoriginal Gaussian Process-based model with a Bayesian Neural Network, enablingmore scalable inference. Thus, we expand the scope of the HiP-MDP toapplications with higher dimensions and more complex dynamics.

Communicating Robot Arm Motion Intent Through Mixed Reality Head-mounted  Displays

  Efficient motion intent communication is necessary for safe and collaborativework environments with collocated humans and robots. Humans efficientlycommunicate their motion intent to other humans through gestures, gaze, andsocial cues. However, robots often have difficulty efficiently communicatingtheir motion intent to humans via these methods. Many existing methods forrobot motion intent communication rely on 2D displays, which require the humanto continually pause their work and check a visualization. We propose a mixedreality head-mounted display visualization of the proposed robot motion overthe wearer's real-world view of the robot and its environment. To evaluate theeffectiveness of this system against a 2D display visualization and against novisualization, we asked 32 participants to labeled different robot arm motionsas either colliding or non-colliding with blocks on a table. We found a 16%increase in accuracy with a 62% decrease in the time it took to complete thetask compared to the next best system. This demonstrates that a mixed-realityHMD allows a human to more quickly and accurately tell where the robot is goingto move than the compared baselines.

Hybrid Bayesian Eigenobjects: Combining Linear Subspace and Deep Network  Methods for 3D Robot Vision

  We introduce Hybrid Bayesian Eigenobjects (HBEOs), a novel representation for3D objects designed to allow a robot to jointly estimate the pose, class, andfull 3D geometry of a novel object observed from a single viewpoint in a singlepractical framework. By combining both linear subspace methods and deepconvolutional prediction, HBEOs efficiently learn nonlinear objectrepresentations without directly regressing into high-dimensional space. HBEOsalso remove the onerous and generally impractical necessity of input datavoxelization prior to inference. We experimentally evaluate the suitability ofHBEOs to the challenging task of joint pose, class, and shape inference onnovel objects and show that, compared to preceding work, HBEOs offerdramatically improved performance in all three tasks along with several ordersof magnitude faster runtime performance.

Scanning the Internet for ROS: A View of Security in Robotics Research

  Because robots can directly perceive and affect the physical world, securityissues take on particular importance. In this paper, we describe the results ofour work on scanning the entire IPv4 address space of the Internet forinstances of the Robot Operating System (ROS), a widely used robotics platformfor research. Our results identified that a number of hosts supporting ROS areexposed to the public Internet, thereby allowing anyone to access roboticsensors and actuators. As a proof of concept, and with consent, we were able toread image sensor information and move the robot of a research group in a USuniversity. This paper gives an overview of our findings, including thegeographic distribution of publicly-accessible platforms, the sorts of sensorand actuator data that is available, as well as the different kinds of robotsand sensors that our scan uncovered. Additionally, we offer recommendations onbest practices to mitigate these security issues in the future.

Finding Options that Minimize Planning Time

  We formalize the problem of selecting the optimal set of options for planningas that of computing the smallest set of options so that planning converges inless than a given maximum of value-iteration passes. We first show that theproblem is NP-hard, even if the task is constrained to be deterministic---thefirst such complexity result for option discovery. We then present the firstpolynomial-time boundedly suboptimal approximation algorithm for this setting,and empirically evaluate it against both the optimal options and arepresentative collection of heuristic approaches in simple grid-based domainsincluding the classic four-rooms problem.

Planning for Decentralized Control of Multiple Robots Under Uncertainty

  We describe a probabilistic framework for synthesizing control policies forgeneral multi-robot systems, given environment and sensor models and a costfunction. Decentralized, partially observable Markov decision processes(Dec-POMDPs) are a general model of decision processes where a team of agentsmust cooperate to optimize some objective (specified by a shared reward or costfunction) in the presence of uncertainty, but where communication limitationsmean that the agents cannot share their state, so execution must proceed in adecentralized fashion. While Dec-POMDPs are typically intractable to solve forreal-world problems, recent research on the use of macro-actions in Dec-POMDPshas significantly increased the size of problem that can be practically solvedas a Dec-POMDP. We describe this general model, and show how, in contrast tomost existing methods that are specialized to a particular problem class, itcan synthesize control policies that use whatever opportunities forcoordination are present in the problem, while balancing off uncertainty inoutcomes, sensor information, and information about other agents. We use threevariations on a warehouse task to show that a single planner of this type cangenerate cooperative behavior using task allocation, direct communication, andsignaling, as appropriate.

Learning Multi-Level Hierarchies with Hindsight

  Multi-level hierarchies have the potential to accelerate learning in sparsereward tasks because they can divide a problem into a set of short horizonsubproblems. In order to realize this potential, Hierarchical ReinforcementLearning (HRL) algorithms need to be able to learn the multiple levels within ahierarchy in parallel, so these simpler subproblems can be solvedsimultaneously. Yet most existing HRL methods that can learn hierarchies arenot able to efficiently learn multiple levels of policies at the same time,particularly in continuous domains. To address this problem, we introduce aframework that can learn multiple levels of policies in parallel. Our approachconsists of two main components: (i) a particular hierarchical architecture and(ii) a method for jointly learning multiple levels of policies. The hierarchiesproduced by our framework are comprised of a set of nested, goal-conditionedpolicies that use the state space to decompose a task into short subtasks. Allpolicies in the hierarchy are learned simultaneously using two types ofhindsight transitions. We demonstrate experimentally in both grid world andsimulated robotics domains that our approach can significantly acceleratelearning relative to other non-hierarchical and hierarchical methods. Indeed,our framework is the first to successfully learn 3-level hierarchies inparallel in tasks with continuous state and action spaces.

