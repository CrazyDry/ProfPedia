A Single Model Explains both Visual and Auditory Precortical Coding

  Precortical neural systems encode information collected by the senses, butthe driving principles of the encoding used have remained a subject of debate.We present a model of retinal coding that is based on three constraints:information preservation, minimization of the neural wiring, and responseequalization. The resulting novel version of sparse principal componentsanalysis successfully captures a number of known characteristics of the retinalcoding system, such as center-surround receptive fields, color opponencychannels, and spatiotemporal responses that correspond to magnocellular andparvocellular pathways. Furthermore, when trained on auditory data, the samemodel learns receptive fields well fit by gammatone filters, commonly used tomodel precortical auditory coding. This suggests that efficient coding may be aunifying principle of precortical encoding across modalities.

Example Selection For Dictionary Learning

  In unsupervised learning, an unbiased uniform sampling strategy is typicallyused, in order that the learned features faithfully encode the statisticalstructure of the training data. In this work, we explore whether active exampleselection strategies - algorithms that select which examples to use, based onthe current estimate of the features - can accelerate learning. Specifically,we investigate effects of heuristic and saliency-inspired selection algorithmson the dictionary learning task with sparse activations. We show that someselection algorithms do improve the speed of learning, and we speculate on whythey might work.

Skeleton Key: Image Captioning by Skeleton-Attribute Decomposition

  Recently, there has been a lot of interest in automatically generatingdescriptions for an image. Most existing language-model based approaches forthis task learn to generate an image description word by word in its originalword order. However, for humans, it is more natural to locate the objects andtheir relationships first, and then elaborate on each object, describingnotable attributes. We present a coarse-to-fine method that decomposes theoriginal image description into a skeleton sentence and its attributes, andgenerates the skeleton sentence and attribute phrases separately. By thisdecomposition, our method can generate more accurate and novel descriptionsthan the previous state-of-the-art. Experimental results on the MS-COCO and alarger scale Stock3M datasets show that our algorithm yields consistentimprovements across different evaluation metrics, especially on the SPICEmetric, which has much higher correlation with human ratings than theconventional metrics. Furthermore, our algorithm can generate descriptions withvaried length, benefiting from the separate control of the skeleton andattributes. This enables image description generation that better accommodatesuser preferences.

Recognizing and Curating Photo Albums via Event-Specific Image  Importance

  Automatic organization of personal photos is a problem with many real worldap- plications, and can be divided into two main tasks: recognizing the eventtype of the photo collection, and selecting interesting images from thecollection. In this paper, we attempt to simultaneously solve both tasks:album-wise event recognition and image- wise importance prediction. Wecollected an album dataset with both event type labels and image importancelabels, refined from an existing CUFED dataset. We propose a hybrid systemconsisting of three parts: A siamese network-based event-specific imageimportance prediction, a Convolutional Neural Network (CNN) that recognizes theevent type, and a Long Short-Term Memory (LSTM)-based sequence level eventrecognizer. We propose an iterative updating procedure for event type and imageimportance score prediction. We experimentally verified that image importancescore prediction and event type recognition can each help the performance ofthe other.

DeepJ: Style-Specific Music Generation

  Recent advances in deep neural networks have enabled algorithms to composemusic that is comparable to music composed by humans. However, few algorithmsallow the user to generate music with tunable parameters. The ability to tuneproperties of generated music will yield more practical benefits for aidingartists, filmmakers, and composers in their creative tasks. In this paper, weintroduce DeepJ - an end-to-end generative model that is capable of composingmusic conditioned on a specific mixture of composer styles. Our innovationsinclude methods to learn musical style and music dynamics. We use our model todemonstrate a simple technique for controlling the style of generated music asa proof of concept. Evaluation of our model using human raters shows that wehave improved over the Biaxial LSTM approach.

Basic Level Categorization Facilitates Visual Object Recognition

  Recent advances in deep learning have led to significant progress in thecomputer vision field, especially for visual object recognition tasks. Thefeatures useful for object classification are learned by feed-forward deepconvolutional neural networks (CNNs) automatically, and they are shown to beable to predict and decode neural representations in the ventral visual pathwayof humans and monkeys. However, despite the huge amount of work on optimizingCNNs, there has not been much research focused on linking CNNs with guidingprinciples from the human visual cortex. In this work, we propose a networkoptimization strategy inspired by both of the developmental trajectory ofchildren's visual object recognition capabilities, and Bar (2003), whohypothesized that basic level information is carried in the fast magnocellularpathway through the prefrontal cortex (PFC) and then projected back to inferiortemporal cortex (IT), where subordinate level categorization is achieved. Weinstantiate this idea by training a deep CNN to perform basic level objectcategorization first, and then train it on subordinate level categorization. Weapply this idea to training AlexNet (Krizhevsky et al., 2012) on the ILSVRC2012 dataset and show that the top-5 accuracy increases from 80.13% to 82.14%,demonstrating the effectiveness of the method. We also show that subsequenttransfer learning on smaller datasets gives superior results.

Deep Active Object Recognition by Joint Label and Action Prediction

  An active object recognition system has the advantage of being able to act inthe environment to capture images that are more suited for training and thatlead to better performance at test time. In this paper, we propose a deepconvolutional neural network for active object recognition that simultaneouslypredicts the object label, and selects the next action to perform on the objectwith the aim of improving recognition performance. We treat active objectrecognition as a reinforcement learning problem and derive the cost function totrain the network for joint prediction of the object label and the action. Agenerative model of object similarities based on the Dirichlet distribution isproposed and embedded in the network for encoding the state of the system. Thetraining is carried out by simultaneously minimizing the label and actionprediction errors using gradient descent. We empirically show that the proposednetwork is able to predict both the object label and the actions on GERMS, adataset for active object recognition. We compare the test label predictionaccuracy of the proposed model with Dirichlet and Naive Bayes state encoding.The results of experiments suggest that the proposed model equipped withDirichlet state encoding is superior in performance, and selects images thatlead to better training and higher accuracy of label prediction at test time.

Central and peripheral vision for scene recognition: A  neurocomputational modeling exploration

  What are the roles of central and peripheral vision in human scenerecognition? Larson and Loschky (2009) showed that peripheral visioncontributes more than central vision in obtaining maximum scene recognitionaccuracy. However, central vision is more efficient for scene recognition thanperipheral, based on the amount of visual area needed for accurate recognition.In this study, we model and explain the results of Larson and Loschky (2009)using a neurocomputational modeling approach. We show that the advantage ofperipheral vision in scene recognition, as well as the efficiency advantage forcentral vision, can be replicated using state-of-the-art deep neural networkmodels. In addition, we propose and provide support for the hypothesis that theperipheral advantage comes from the inherent usefulness of peripheral features.This result is consistent with data presented by Thibaut, Tran, Szaffarczyk,and Boucart (2014), who showed that patients with central vision loss can stillcategorize natural scenes efficiently. Furthermore, by using a deepmixture-of-experts model ("The Deep Model," or TDM) that receives central andperipheral visual information on separate channels simultaneously, we show thatthe peripheral advantage emerges naturally in the learning process: Whentrained to categorize scenes, the model weights the peripheral pathway morethan the central pathway. As we have seen in our previous modeling work,learning creates a transform that spreads different scene categories intodifferent regions in representational space. Finally, we visualize the featuresfor the two pathways, and find that different preferences for scene categoriesemerge for the two pathways during the training process.

Hierarchical Cellular Automata for Visual Saliency

  Saliency detection, finding the most important parts of an image, has becomeincreasingly popular in computer vision. In this paper, we introduceHierarchical Cellular Automata (HCA) -- a temporally evolving model tointelligently detect salient objects. HCA consists of two main components:Single-layer Cellular Automata (SCA) and Cuboid Cellular Automata (CCA). As anunsupervised propagation mechanism, Single-layer Cellular Automata can exploitthe intrinsic relevance of similar regions through interactions with neighbors.Low-level image features as well as high-level semantic information extractedfrom deep neural networks are incorporated into the SCA to measure thecorrelation between different image patches. With these hierarchical deepfeatures, an impact factor matrix and a coherence matrix are constructed tobalance the influences on each cell's next state. The saliency values of allcells are iteratively updated according to a well-defined update rule.Furthermore, we propose CCA to integrate multiple saliency maps generated bySCA at different scales in a Bayesian framework. Therefore, single-layerpropagation and multi-layer integration are jointly modeled in our unified HCA.Surprisingly, we find that the SCA can improve all existing methods that weapplied it to, resulting in a similar precision level regardless of theoriginal results. The CCA can act as an efficient pixel-wise aggregationalgorithm that can integrate state-of-the-art methods, resulting in even betterresults. Extensive experiments on four challenging datasets demonstrate thatthe proposed algorithm outperforms state-of-the-art conventional methods and iscompetitive with deep learning based approaches.

Belief Tree Search for Active Object Recognition

  Active Object Recognition (AOR) has been approached as an unsupervisedlearning problem, in which optimal trajectories for object inspection are notknown and are to be discovered by reducing label uncertainty measures ortraining with reinforcement learning. Such approaches have no guarantees of thequality of their solution. In this paper, we treat AOR as a PartiallyObservable Markov Decision Process (POMDP) and find near-optimal policies ontraining data using Belief Tree Search (BTS) on the corresponding belief MarkovDecision Process (MDP). AOR then reduces to the problem of knowledge transferfrom near-optimal policies on training set to the test set. We train a LongShort Term Memory (LSTM) network to predict the best next action on thetraining set rollouts. We sho that the proposed AOR method generalizes well tonovel views of familiar objects and also to novel objects. We compare thissupervised scheme against guided policy search, and find that the LSTM networkreaches higher recognition accuracy compared to the guided policy method. Wefurther look into optimizing the observation function to increase the totalcollected reward of optimal policy. In AOR, the observation function is knownonly approximately. We propose a gradient-based method update to thisapproximate observation function to increase the total reward of any policy. Weshow that by optimizing the observation function and retraining the supervisedLSTM network, the AOR performance on the test set improves significantly.

Deep-ESN: A Multiple Projection-encoding Hierarchical Reservoir  Computing Framework

  As an efficient recurrent neural network (RNN) model, reservoir computing(RC) models, such as Echo State Networks, have attracted widespread attentionin the last decade. However, while they have had great success with time seriesdata [1], [2], many time series have a multiscale structure, which asingle-hidden-layer RC model may have difficulty capturing. In this paper, wepropose a novel hierarchical reservoir computing framework we call Deep EchoState Networks (Deep-ESNs). The most distinctive feature of a Deep-ESN is itsability to deal with time series through hierarchical projections.Specifically, when an input time series is projected into the high-dimensionalecho-state space of a reservoir, a subsequent encoding layer (e.g., a PCA,autoencoder, or a random projection) can project the echo-state representationsinto a lower-dimensional space. These low-dimensional representations can thenbe processed by another ESN. By using projection layers and encoding layersalternately in the hierarchical framework, a Deep-ESN can not only attenuatethe effects of the collinearity problem in ESNs, but also fully take advantageof the temporal kernel property of ESNs to explore multiscale dynamics of timeseries. To fuse the multiscale representations obtained by each reservoir, weadd connections from each encoding layer to the last output layer. Theoreticalanalyses prove that stability of a Deep-ESN is guaranteed by the echo stateproperty (ESP), and the time complexity is equivalent to a conventional ESN.Experimental results on some artificial and real world time series demonstratethat Deep-ESNs can capture multiscale dynamics, and outperform both standardESNs and previous hierarchical ESN-based models.

