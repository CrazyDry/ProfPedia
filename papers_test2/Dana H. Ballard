Atari-HEAD: Atari Human Eye-Tracking and Demonstration Dataset

  We introduce a large-scale dataset of human actions and eye movements whileplaying Atari videos games. The dataset currently has 44 hours of gameplay datafrom 16 games and a total of 2.97 million demonstrated actions. Human subjectsplayed games in a frame-by-frame manner to allow enough decision time in orderto obtain near-optimal decisions. This dataset could be potentially used forresearch in imitation learning, reinforcement learning, and visual saliency.

AGIL: Learning Attention from Human for Visuomotor Tasks

  When intelligent agents learn visuomotor behaviors from human demonstrations,they may benefit from knowing where the human is allocating visual attention,which can be inferred from their gaze. A wealth of information regardingintelligent decision making is conveyed by human gaze allocation; hence,exploiting such information has the potential to improve the agents'performance. With this motivation, we propose the AGIL (Attention GuidedImitation Learning) framework. We collect high-quality human action and gazedata while playing Atari games in a carefully controlled experimental setting.Using these data, we first train a deep neural network that can predict humangaze positions and visual attention with high accuracy (the gaze network) andthen train another network to predict human actions (the policy network).Incorporating the learned attention model from the gaze network into the policynetwork significantly improves the action prediction accuracy and taskperformance.

An Initial Attempt of Combining Visual Selective Attention with Deep  Reinforcement Learning

  Visual attention serves as a means of feature selection mechanism in theperceptual system. Motivated by Broadbent's leaky filter model of selectiveattention, we evaluate how such mechanism could be implemented and affect thelearning process of deep reinforcement learning. We visualize and analyze thefeature maps of DQN on a toy problem Catch, and propose an approach to combinevisual selective attention with deep reinforcement learning. We experiment withoptical flow-based attention and A2C on Atari games. Experiment results showthat visual selective attention could lead to improvements in terms of sampleefficiency on tested games. An intriguing relation between attention and batchnormalization is also discovered.

