Noun-Phrase Analysis in Unrestricted Text for Information Retrieval

  Information retrieval is an important application area of natural-languageprocessing where one encounters the genuine challenge of processing largequantities of unrestricted natural-language text. This paper reports on theapplication of a few simple, yet robust and efficient noun-phrase analysistechniques to create better indexing phrases for information retrieval. Inparticular, we describe a hybrid approach to the extraction of meaningful(continuous or discontinuous) subcompounds from complex noun phrases using bothcorpus statistics and linguistic heuristics. Results of experiments show thatindexing based on such extracted subcompounds improves both recall andprecision in an information retrieval system. The noun-phrase analysistechniques are also potentially useful for book indexing and automaticthesaurus extraction.

Exploiting Context to Identify Lexical Atoms -- A Statistical View of  Linguistic Context

  Interpretation of natural language is inherently context-sensitive. Mostwords in natural language are ambiguous and their meanings are heavilydependent on the linguistic context in which they are used. The study oflexical semantics can not be separated from the notion of context. This papertakes a contextual approach to lexical semantics and studies the linguisticcontext of lexical atoms, or "sticky" phrases such as "hot dog". Since suchlexical atoms may occur frequently in unrestricted natural language text,recognizing them is crucial for understanding naturally-occurring text. Thepaper proposes several heuristic approaches to exploiting the linguisticcontext to identify lexical atoms from arbitrary natural language text.

Fast Statistical Parsing of Noun Phrases for Document Indexing

  Information Retrieval (IR) is an important application area of NaturalLanguage Processing (NLP) where one encounters the genuine challenge ofprocessing large quantities of unrestricted natural language text. While mucheffort has been made to apply NLP techniques to IR, very few NLP techniqueshave been evaluated on a document collection larger than several megabytes.Many NLP techniques are simply not efficient enough, and not robust enough, tohandle a large amount of text. This paper proposes a new probabilistic modelfor noun phrase parsing, and reports on the application of such a parsingtechnique to enhance document indexing. The effectiveness of using syntacticphrases provided by the parser to supplement single words for indexing isevaluated with a 250 megabytes document collection. The experiment's resultsshow that supplementing single words with syntactic phrases for indexingconsistently and significantly improves retrieval performance.

Numerical Facet Range Partition: Evaluation Metric and Methods

  Faceted navigation is a very useful component in today's search engines. Itis especially useful when user has an exploratory information need or prefercertain attribute values than others. Existing work has tried to optimizefaceted systems in many aspects, but little work has been done on optimizingnumerical facet ranges (e.g., price ranges of product). In this paper, weintroduce for the first time the research problem on numerical facet rangepartition and formally frame it as an optimization problem. To enablequantitative evaluation of a partition algorithm, we propose an evaluationmetric to be applied to search engine logs. We further propose two rangepartition algorithms that computationally optimize the defined metric.Experimental results on a two-month search log from a major e-Commerce engineshow that our proposed method can significantly outperform baseline.

Identifying Compromised Accounts on Social Media Using Statistical Text  Analysis

  Compromised social media accounts are legitimate user accounts that have beenhijacked by a third (malicious) party and can cause various kinds of damage.Early detection of such compromised accounts is very important in order tocontrol the damage. In this work we propose a novel general framework fordiscovering compromised accounts by utilizing statistical text analysis. Theframework is built on the observation that users will use language that ismeasurably different from the language that a hacker (or spammer) would use,when the account is compromised. We use the framework to develop specificalgorithms based on language modeling and use the similarity of language modelsof users and spammers as features in a supervised learning setup to identifycompromised accounts. Evaluation results on a large Twitter corpus of over 129million tweets show promising results of the proposed approach.

JIM: Joint Influence Modeling for Collective Search Behavior

  Previous work has shown that popular trending events are important externalfactors which pose significant influence on user search behavior and alsoprovided a way to computationally model this influence. However, their problemformulation was based on the strong assumption that each event poses itsinfluence independently. This assumption is unrealistic as there are manycorrelated events in the real world which influence each other and thus, wouldpose a joint influence on the user search behavior rather than posing influenceindependently. In this paper, we study this novel problem of Modeling the JointInfluences posed by multiple correlated events on user search behavior. Wepropose a Joint Influence Model based on the Multivariate Hawkes Process whichcaptures the inter-dependency among multiple events in terms of their influenceupon user search behavior. We evaluate the proposed Joint Influence Model usingtwo months query-log data from https://search.yahoo.com/. Experimental resultsshow that the model can indeed capture the temporal dynamics of the jointinfluence over time and also achieves superior performance over differentbaseline methods when applied to solve various interesting prediction problemsas well as real-word application scenarios, e.g., query auto-completion.

Preference-based Graphic Models for Collaborative Filtering

  Collaborative filtering is a very useful general technique for exploiting thepreference patterns of a group of users to predict the utility of items to aparticular user. Previous research has studied several probabilistic graphicmodels for collaborative filtering with promising results. However, while thesemodels have succeeded in capturing the similarity among users and items in oneway or the other, none of them has considered the fact that users with similarinterests in items can have very different rating patterns; some users tend toassign a higher rating to all items than other users. In this paper, we proposeand study of two new graphic models that address the distinction between userpreferences and ratings. In one model, called the decoupled model, we introducetwo different variables to decouple a users preferences FROM his ratings. INthe other, called the preference model, we model the orderings OF itemspreferred BY a USER, rather than the USERs numerical ratings of items.Empirical study over two datasets of movie ratings shows that appropriatemodeling of the distinction between user preferences and ratings improves theperformance substantially and consistently. Specifically, the proposeddecoupled model outperforms all five existing approaches that we compare withsignificantly, but the preference model is not very successful. These resultssuggest that explicit modeling of the underlying user preferences is veryimportant for collaborative filtering, but we can not afford ignoring therating information completely.

Modeling Diverse Relevance Patterns in Ad-hoc Retrieval

  Assessing relevance between a query and a document is challenging in ad-hocretrieval due to its diverse patterns, i.e., a document could be relevant to aquery as a whole or partially as long as it provides sufficient information forusers' need. Such diverse relevance patterns require an ideal retrieval modelto be able to assess relevance in the right granularity adaptively.Unfortunately, most existing retrieval models compute relevance at a singlegranularity, either document-wide or passage-level, or use fixed combinationstrategy, restricting their ability in capturing diverse relevance patterns. Inthis work, we propose a data-driven method to allow relevance signals atdifferent granularities to compete with each other for final relevanceassessment. Specifically, we propose a HIerarchical Neural maTching model(HiNT) which consists of two stacked components, namely local matching layerand global decision layer. The local matching layer focuses on producing a setof local relevance signals by modeling the semantic matching between a queryand each passage of a document. The global decision layer accumulates localsignals into different granularities and allows them to compete with each otherto decide the final relevance score. Experimental results demonstrate that ourHiNT model outperforms existing state-of-the-art retrieval models significantlyon benchmark ad-hoc retrieval datasets.

Non-Autoregressive Machine Translation with Auxiliary Regularization

  As a new neural machine translation approach, Non-Autoregressive machineTranslation (NAT) has attracted attention recently due to its high efficiencyin inference. However, the high efficiency has come at the cost of notcapturing the sequential dependency on the target side of translation, whichcauses NAT to suffer from two kinds of translation errors: 1) repeatedtranslations (due to indistinguishable adjacent decoder hidden states), and 2)incomplete translations (due to incomplete transfer of source side informationvia the decoder hidden states).  In this paper, we propose to address these two problems by improving thequality of decoder hidden representations via two auxiliary regularizationterms in the training process of an NAT model. First, to make the hidden statesmore distinguishable, we regularize the similarity between consecutive hiddenstates based on the corresponding target tokens. Second, to force the hiddenstates to contain all the information in the source sentence, we leverage thedual nature of translation tasks (e.g., English to German and German toEnglish) and minimize a backward reconstruction error to ensure that the hiddenstates of the NAT decoder are able to recover the source side sentence.Extensive experiments conducted on several benchmark datasets show that bothregularization strategies are effective and can alleviate the issues ofrepeated translations and incomplete translations in NAT models. The accuracyof NAT models is therefore improved significantly over the state-of-the-art NATmodels with even better efficiency for inference.

On Application of Learning to Rank for E-Commerce Search

  E-Commerce (E-Com) search is an emerging important new application ofinformation retrieval. Learning to Rank (LETOR) is a general effective strategyfor optimizing search engines, and is thus also a key technology for E-Comsearch. While the use of LETOR for web search has been well studied, its usefor E-Com search has not yet been well explored. In this paper, we discuss thepractical challenges in applying learning to rank methods to E-Com search,including the challenges in feature representation, obtaining reliablerelevance judgments, and optimally exploiting multiple user feedback signalssuch as click rates, add-to-cart ratios, order rates, and revenue. We studythese new challenges using experiments on industry data sets and report severalinteresting findings that can provide guidance on how to optimally apply LETORto E-Com search: First, popularity-based features defined solely on productitems are very useful and LETOR methods were able to effectively optimize theircombination with relevance-based features. Second, query attribute sparsityraises challenges for LETOR, and selecting features to reduce/avoid sparsity isbeneficial. Third, while crowdsourcing is often useful for obtaining relevancejudgments for Web search, it does not work as well for E-Com search due todifficulty in eliciting sufficiently fine grained relevance judgments. Finally,among the multiple feedback signals, the order rate is found to be the mostrobust training objective, followed by click rate, while add-to-cart ratioseems least robust, suggesting that an effective practical strategy may be toinitially use click rates for training and gradually shift to using order ratesas they become available.

