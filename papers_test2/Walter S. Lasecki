Effect of Group Means on the Probability of Consensus

  In this study, groups who could not reach a consensus were investigated usingthe group polarization paradigm. The purpose was to explore the conditionsleading to intragroup disagreement and attitude change following disagreementamong 269 participants. Analysis indicated that the probability of consensuswas low when the group means differed from the grand mean of the entire sample.When small differences among group members were found, depolarization (reversedirection of polarization) followed disagreement. These results suggested thegroups which deviated most from the population tendency were the most likely tocause within-group disagreement, while within-group variances determined thedirection of attitude change following disagreement within the group.

Social Technologies for Developing Collective Intelligence in Networked  Society

  The scientific problem in our project is defined as a question: how socialtechnologies could contribute to the development of smart and inclusivesociety? The subject of our research are networked projects (virtual CIsystems) which include collective decision making tools and innovationmechanisms allowing and encouraging individual and team creativity,entrepreneurship, online collaboration, new forms of self-regulation andself-governance, self-configuration of communities by considering theseprojects as being catalyst for emergence of CI. The answers to thesetheoretical questions could have huge practical implications by influencingmore reasonable and sophisticated application of social technologies inpractice.

Collective intelligence in Massive Online Dialogues

  The emergence and ongoing development of Web 2.0 technologies have enablednew and advanced forms of collective intelligence at unprecedented scales,allowing large numbers of individuals to act collectively and create highquality intellectual artifacts. However, little is known about how and whenthey indeed promote collective intelligence. In this manuscript, we provide asurvey of the automated tools developed to analyze discourse-centric collectiveintelligence. By conducting a thematic analysis of the current researchdirection, a set of gaps and limitations are identified.

Crowd Memory: Learning in the Collective

  Crowd algorithms often assume workers are inexperienced and thus fail toadapt as workers in the crowd learn a task. These assumptions fundamentallylimit the types of tasks that systems based on such algorithms can handle. Thispaper explores how the crowd learns and remembers over time in the context ofhuman computation, and how more realistic assumptions of worker experience maybe used when designing new systems. We first demonstrate that the crowd canrecall information over time and discuss possible implications of crowd memoryin the design of crowd algorithms. We then explore crowd learning during acontinuous control task. Recent systems are able to disguise dynamic groups ofworkers as crowd agents to support continuous tasks, but have not yetconsidered how such agents are able to learn over time. We show, using areal-time gaming setting, that crowd agents can learn over time, and `remember'by passing strategies from one generation of workers to the next, despite highturnover rates in the workers comprising them. We conclude with a discussion offuture research directions for crowd memory and learning.

Architectures of Virtual Decision-Making: The Emergence of Gender  Discrimination on a Crowdfunding Website

  The increasing relevance of Internet-based markets requires a sustainedinvestigation into the relationship between design and user behavior. Thisresearch begins within the sociology of quantification and markets toinvestigate the impacts of basic design decisions on user behavior andindividual success on a widely used crowdfunding website. This study looks atone common design feature, publishing recipients' sex, on the probability ofreceiving funding. Following research in the sociology of gender, these effectsare defined along individual, behavioral, and structural dimensions. Theresults reveal that before teachers' sex was published, gender discriminationwas weak and inconsistent. However, afterward gender discrimination increasesby an order of magnitude and becomes systematized. Contrary to expectation,donors did not discriminate by sex category, but by teachers' structuralposition and the kinds of language they used. Implications for research ongender discrimination, priming, and online behavior are discussed.

Less-is-more in a 5-star rating system: an experimental study of human  combined decisions in a multi-armed bandit problem

  Given the rapid proliferation of advanced information technologies, includingthe Internet, modern humans can easily access vast amount of sociallytransmitted information. Intuitively, this situation is isomorphic to someeusocial insects that are known to solve the exploration-exploitation dilemmacollectively through information transfer (e.g., honeybees [Seeley et al.,1991]; and ants [Shaffer, Sasaki & Pratt, 2013]). Yet, in contrast from theeusocial insects, whose colonies are composed of kin, human collectiveperformance may be affected by an inherent free-rider problem [Bolton & Harris,1999; Kameda, Tsukasaki, Hastie & Berg, 2011]. Specifically, in groupsinvolving non-kin members, it is expected that free-riders, who allow others tosearch for better alternatives and then exploit their findings through sociallearning ("information scroungers"), will frequently appear, and consequentlyundermine the advantage of collective intelligence [Rogers, 1998; Kameda &Nakanishi, 2003].

When none of us perform better than all of us together: the role of  analogical decision rules in groups

  During social interactions, groups develop collective competencies that(ideally) should assist groups to outperform average standalone individualmembers (weak cognitive synergy) or the best performing member in the group(strong cognitive synergy). In two experimental studies we manipulate the typeof decision rule used in group decision-making (identify the best vs.collaborative), and the way in which the decision rules are induced (direct vs.analogical) and we test the effect of these two manipulations on the emergenceof strong and weak cognitive synergy. Our most important results indicate thatan analogically induced decision rule (imitate-the-successful heuristic) inwhich groups have to identify the best member and build on his/her performance(take-the-best heuristic) is the most conducive for strong cognitive synergy.Our studies bring evidence for the role of analogy-making in groups as well asthe role of fast-and-frugal heuristics for group decision-making.

Analytical reasoning task reveals limits of social learning in networks

  Social learning -by observing and copying others- is a highly successfulcultural mechanism for adaptation, outperforming individual informationacquisition and experience. Here, we investigate social learning in the contextof the uniquely human capacity for reflective, analytical reasoning. A hallmarkof the human mind is our ability to engage analytical reasoning, and suppressfalse associative intuitions. Through a set of lab-based network experiments,we find that social learning fails to propagate this cognitive strategy. Whenpeople make false intuitive conclusions, and are exposed to the analytic outputof their peers, they recognize and adopt this correct output. But they fail toengage analytical reasoning in similar subsequent tasks. Thus, humans exhibitan 'unreflective copying bias,' which limits their social learning to theoutput, rather than the process, of their peers' reasoning -even when doing sorequires minimal effort and no technical skill. In contrast to much recent workon observation-based social learning, which emphasizes the propagation ofsuccessful behavior through copying, our findings identify a limit on the powerof social networks in situations that require analytical reasoning.

Transient Leadership and Collective Cell Movement in Early Diverged  Multicellular Animals

  Collective motion of cells is critical to some of the most vital tasksincluding wound healing, development, and immune response [Friedl and Gilmour2009; Tokarski et al. 2012; Lee et al. 2012; Beltman et al. 2009], and iscommon to many pathological processes including cancer cell invasion andteratogenesis [Khalil and Friedl 2010]. The extensive understanding of movementby single cells [R{\o}rth 2011; Insall and Machesky 2011; Houk et al. 2012] isinsufficient to predict the behavior of cellular groups [Theveneau et al. 2013;Trepat, X. and Fredberg 2011], and identifying underlying rules of coordinationin collective cell migration is still evasive. Few of the supposed benefits ofcollective motion have ever been tested at the cellular scale. As an example,though collective sensing allows for larger groups to exhibit greater accuracyin navigation [Simons 2004; Berdahl et al. 2013] and group taxis is possiblethrough the leadership of only a few individuals [Couzin et al. 2005], sucheffects have never been investigated in collective cell migration. We willinvestigate collective motion and decision-making in a primitive multicellularanimal, Trichoplax adhaerens to understand how intercellular coordinationaffects animal behavior and how migration accuracy scales with cellular groupsize.

Tuning the Diversity of Open-Ended Responses from the Crowd

  Crowdsourcing can solve problems that current fully automated systems cannot.Its effectiveness depends on the reliability, accuracy, and speed of the crowdworkers that drive it. These objectives are frequently at odds with oneanother. For instance, how much time should workers be given to discover andpropose new solutions versus deliberate over those currently proposed? How dowe determine if discovering a new answer is appropriate at all? And how do wemanage workers who lack the expertise or attention needed to provide usefulinput to a given task? We present a mechanism that uses distinct payoffs forthree possible worker actions---propose,vote, or abstain---to provide workerswith the necessary incentives to guarantee an effective (or even optimal)balance between searching for new answers, assessing those currently available,and, when they have insufficient expertise or insight for the task at hand,abstaining. We provide a novel game theoretic analysis for this mechanism andtest it experimentally on an image---labeling problem and show that it allows asystem to reliably control the balance betweendiscovering new answers andconverging to existing ones.

Understanding Task Design Trade-offs in Crowdsourced Paraphrase  Collection

  Linguistically diverse datasets are critical for training and evaluatingrobust machine learning systems, but data collection is a costly process thatoften requires experts. Crowdsourcing the process of paraphrase generation isan effective means of expanding natural language datasets, but there has beenlimited analysis of the trade-offs that arise when designing tasks. In thispaper, we present the first systematic study of the key factors incrowdsourcing paraphrase collection. We consider variations in instructions,incentives, data domains, and workflows. We manually analyzed paraphrases forcorrectness, grammaticality, and linguistic diversity. Our observations providenew insight into the trade-offs between accuracy and diversity in crowdresponses that arise as a result of task design, providing guidance for futureparaphrase generation procedures.

Analyzing Assumptions in Conversation Disentanglement Research Through  the Lens of a New Dataset and Model

  Disentangling conversations mixed together in a single stream of messages isa difficult task with no large annotated datasets. We created a new datasetthat is 25 times the size of any previous publicly available resource, hassamples of conversation from 152 points in time across a decade, and isannotated with both threads and a within-thread reply-structure graph. We alsodeveloped a new neural network model, which extracts conversation threadssubstantially more accurately than prior work. Using our annotated data and ourmodel we tested assumptions in prior work, revealing major issues inheuristically constructed resources, and identifying how small datasets havebiased our understanding of multi-party multi-conversation chat.

Dialog System Technology Challenge 7

  This paper introduces the Seventh Dialog System Technology Challenges (DSTC),which use shared datasets to explore the problem of building dialog systems.Recently, end-to-end dialog modeling approaches have been applied to variousdialog tasks. The seventh DSTC (DSTC7) focuses on developing technologiesrelated to end-to-end dialog systems for (1) sentence selection, (2) sentencegeneration and (3) audio visual scene aware dialog. This paper summarizes theoverall setup and results of DSTC7, including detailed descriptions of thedifferent tracks and provided datasets. We also describe overall trends in thesubmitted systems and the key results. Each track introduced new datasets andparticipants achieved impressive results using state-of-the-art end-to-endtechnologies.

Information Spread in a Connected World

  In the following work, we compare the spread of information by word-of-mouth(WOM) to the spread of information through search engines. We assume that theinitial acknowledgement of new information derives from social interactions butthat solid opinions are only formed after further evaluation through searchengines. Search engines can be viewed as central hubs that connect informationpresented in relevant websites to searchers. Since they construct newconnections between searchers and information in every query performed, thenetwork structure is less relevant. Although models of viral spread of ideashave been inspected in many previous works [1], [2], [3], [4], [5], [6], [7],[8], [9], [10], [11], [12], [13], only few assume the acceptance of a novelconcept to be solely based on the evaluation of the opinions of others [8],[5]. Following this approach, combined with that of models of informationspread with threshold [1] that claim the propagation in a network to occur onlyif a threshold of neighbors hold an opinion, the proposed work adds a newtheoretical perspective that is relevant to the daily use of search engines asa major information search tool. We continue by presenting some justificationsbased on experimentations. Last we discuss possible outcomes of over use ofsearch engines vs. WOM, and suggest an hypothesis that such overuse mightactually narrow the collective information set.

Open Collaboration for Innovation: Principles and Performance

  The principles of open collaboration for innovation (and production), oncedistinctive to open source software, are now found in many other ventures. Someof these ventures are internet-based: Wikipedia, online forums and communities.Others are off-line: in medicine, science, and everyday life. Such ventureshave been affecting traditional firms, and may represent a new organizationalform. Despite the impact of such ventures, questions remain about theiroperating principles and performance. Here we define open collaboration (OC),the underlying set of principles, and propose that it is a robust engine forinnovation and production. First, we review multiple OC ventures and identifyfour defining principles. In all instances, participants create goods andservices of economic value, they exchange and reuse each other's work, theylabor purposefully with just loose coordination, and they permit anyone tocontribute and consume. These principles distinguish OC from otherorganizational forms, such as firms or cooperatives. Next, we turn toperformance. To understand the performance of OC, we develop a computationalmodel, combining innovation theory with recent evidence on human cooperation.We identify and investigate three elements that affect performance: thecooperativeness of participants, the diversity of their needs, and the degreeto which the goods are rival (subtractable). Through computational experiments,we find that OC performs well even in seemingly harsh environments: whencooperators are a minority, free riders are present, diversity is lacking, orgoods are rival. We conclude that OC is viable and likely to expand into newdomains. The findings also inform the discussion on new organizational forms,collaborative and communal.

Crowdsourcing for Participatory Democracies: Efficient Elicitation of  Social Choice Functions

  We present theoretical and empirical results demonstrating the usefulness ofvoting rules for participatory democracies. We first give algorithms whichefficiently elicit \epsilon-approximations to two prominent voting rules: theBorda rule and the Condorcet winner. This result circumvents previousprohibitive lower bounds and is surprisingly strong: even if the number ofideas is as large as the number of participants, each participant will onlyhave to make a logarithmic number of comparisons, an exponential improvementover the linear number of comparisons previously needed. We demonstrate theapproach in an experiment in Finland's recent off-road traffic law reform,observing that the total number of comparisons needed to achieve a fixed\epsilon approximation is linear in the number of ideas and that the constantis not large.  Finally, we note a few other experimental observations which support the useof voting rules for aggregation. First, we observe that rating, one of thecommon alternatives to ranking, manifested effects of bias in our data. Second,we show that very few of the topics lacked a Condorcet winner, one of theprominent negative results in voting. Finally, we show data hinting at apotential future direction: the use of partial rankings as opposed to pairwisecomparisons to further decrease the elicitation time.

Influence Process Structural Learning and the Emergence of Collective  Intelligence

  Recent work [Hazy 2012] has demonstrated computationally that collectivesthat are organized into networks which govern the flow of resources can learnto recognize newly emerging opportunities distributed in the environment. Thispaper argues that the system does this through a process analogous to neuralnetwork learning with relative status playing the role of synaptic weights.Hazy showed computationally that learning of this type can occur even whenresource allocation decision makers have no direct visibility into theenvironment, have no direct understanding of the opportunity, and are notinvolved in their exploitation except to the extent that they evaluate thesuccess or failure of funded projects. Effectively, the system of interactionslearns which individuals have the best access to information and otherresources within the ecosystem. Hazy [2012] calls this previously unidentifiedemergence phenomenon: Influence Process Structural Learning (IPSL). In theprior model of IPSL, a three-tiered organizational structure was predeterminedin the model design [Hazy 2012]. These initial conditions delimit the extent towhich the emergence of collective intelligence can be posited because the modelitself assumes a defined structure. This work contributes to the field byextending the IPSL argument for collective intelligence to a holistic emergenceargument. It begins by briefly reviewing previously published work. Itcontinues the conversation by adding two additional steps: Firstly, it showshow a three-tier organizing structure might emerge through known complexitymechanisms. In this case the mechanism identified is preferential attachment[Barabasi 2002]. Secondly, the paper shows how collective intelligence canemerge within a system of agents when the influence structure among theseagents is treated as a the genetic algorithm.

Collective Intelligence in Citizen Science -- A Study of Performers and  Talkers

  The recent emergence of online citizen science is illustrative of anefficient and effective means to harness the crowd in order to achieve a rangeof scientific discoveries. Fundamentally, citizen science projects draw uponcrowds of non-expert volunteers to complete short Tasks, which can vary indomain and complexity. However, unlike most human-computational systems,participants in these systems, the `citizen scientists' are volunteers, wherebyno incentives, financial or otherwise, are offered. Furthermore, encouraged bycitizen science platforms such as Zooniverse, online communities have emerged,providing them with an environment to discuss, share ideas, and solve problems.In fact, it is the result of these forums that has enabled a number ofscientific discoveries to be made. In this paper we explore the phenomenon ofcollective intelligence via the relationship between the activities of onlinecitizen science communities and the discovery of scientific knowledge. Weperform a cross-project analysis of ten Zooniverse citizen science projects andanalyse the behaviour of users with regards to their Task completion activityand participation in discussion and discover collective behaviour amongsthighly active users. Whilst our findings have implications for future citizenscience design, we also consider the wider implications for understandingcollective intelligence research in general.

Human Communication Systems Evolve by Cultural Selection

  Human communication systems, such as language, evolve culturally; theircomponents undergo reproduction and variation. However, a role for selection incultural evolutionary dynamics is less clear. Often neutral evolution (alsoknown as 'drift') models, are used to explain the evolution of humancommunication systems, and cultural evolution more generally. Under thisaccount, cultural change is unbiased: for instance, vocabulary, baby names andpottery designs have been found to spread through random copying.  While drift is the null hypothesis for models of cultural evolution it doesnot always adequately explain empirical results. Alternative models includecultural selection, which assumes variant adoption is biased. Theoreticalmodels of human communication argue that during conversation interlocutors arebiased to adopt the same labels and other aspects of linguistic representation(including prosody and syntax). This basic alignment mechanism has beenextended by computer simulation to account for the emergence of linguisticconventions. When agents are biased to match the linguistic behavior of theirinterlocutor, a single variant can propagate across an entire population ofinteracting computer agents. This behavior-matching account operates at thelevel of the individual. We call it the Conformity-biased model. Under adifferent selection account, called content-biased selection, functionalselection or replicator selection, variant adoption depends upon the intrinsicvalue of the particular variant (e.g., ease of learning or use). This secondalternative account operates at the level of the cultural variant. FollowingBoyd and Richerson we call it the Content-biased model. The present paper teststhe drift model and the two biased selection models' ability to explain thespread of communicative signal variants in an experimental micro-society.

When is a crowd wise?

  Numerous studies and anecdotes demonstrate the "wisdom of the crowd," thesurprising accuracy of a group's aggregated judgments. Less is known, however,about the generality of crowd wisdom. For example, are crowds wise even iftheir members have systematic judgmental biases, or can influence each otherbefore members render their judgments? If so, are there situations in which wecan expect a crowd to be less accurate than skilled individuals? We provide aprecise but general definition of crowd wisdom: A crowd is wise if a linearaggregate, for example a mean, of its members' judgments is closer to thetarget value than a randomly, but not necessarily uniformly, sampled member ofthe crowd. Building on this definition, we develop a theoretical framework forexamining, a priori, when and to what degree a crowd will be wise. Wesystematically investigate the boundary conditions for crowd wisdom within thisframework and determine conditions under which the accuracy advantage forcrowds is maximized. Our results demonstrate that crowd wisdom is highlyrobust: Even if judgments are biased and correlated, one would need to nearlydeterministically select only a highly skilled judge before an individual'sjudgment could be expected to be more accurate than a simple averaging of thecrowd. Our results also provide an accuracy rationale behind the need fordiversity of judgments among group members. Contrary to folk explanations ofcrowd wisdom which hold that judgments should ideally be independent so thaterrors cancel out, we find that crowd wisdom is maximized when judgmentssystematically differ as much as possible. We re-analyze data from twopublished studies that confirm our theoretical results.

Wisdom of the Confident: Using Social Interactions to Eliminate the Bias  in Wisdom of the Crowds

  Human groups can perform extraordinary accurate estimations compared toindividuals by simply using the mean, median or geometric mean of theindividual estimations [Galton 1907, Surowiecki 2005, Page 2008]. However, thisis true only for some tasks and in general these collective estimations showstrong biases. The method fails also when allowing for social interactions,which makes the collective estimation worse as individuals tend to converge tothe biased result [Lorenz et al. 2011]. Here we show that there is a brightside of this apparently negative impact of social interactions into collectiveintelligence. We found that some individuals resist the social influence and,when using the median of this subgroup, we can eliminate the bias of the wisdomof the full crowd. To find this subgroup of individuals more confident in theirprivate estimations than in the social influence, we model individuals asestimators that combine private and social information with different relativeweights [Perez-Escudero & de Polavieja 2011, Arganda et al. 2012]. We thencomputed the geometric mean for increasingly smaller groups by eliminatingthose using in their estimations higher values of the social influence weight.The trend obtained in this procedure gives unbiased results, in contrast to thesimpler method of computing the median of the complete group. Our results showthat, while a simple operation like the mean, median or geometric mean of agroup may not allow groups to make good estimations, a more complex operationtaking into account individuality in the social dynamics can lead to a bettercollective intelligence.

On Manipulation in Prediction Markets When Participants Influence  Outcomes Directly

  Prediction markets are often used as mechanisms to aggregate informationabout a future event, for example, whether a candidate will win an election.The event is typically assumed to be exogenous. In reality, participants mayinfluence the outcome, and therefore (1) running the prediction market couldchange the incentives of participants in the process that creates the outcome(for example, agents may want to change their vote in an election), and (2)simple results such as the myopic incentive compatibility of proper scoringrules no longer hold in the prediction market itself. We introduce a model ofgames of this kind, where agents first trade in a prediction market and thentake an action that influences the market outcome. Our two-stage two-playermodel, despite its simplicity, captures two aspects of real-world predictionmarkets: (1) agents may directly influence the outcome, (2) some of the agentsinstrumental in deciding the outcome may not take part in the predictionmarket. We show that this game has two different types of perfect Bayesianequilibria, which we term LPP and HPP, depending on the values of the beliefparameters: in the LPP domain, equilibrium prices reveal expected marketoutcomes conditional on the participants' private information, whereas HPPequilibria are collusive -- participants effectively coordinate in anuninformative and untruthful way.

"Is there anything else I can help you with?": Challenges in Deploying  an On-Demand Crowd-Powered Conversational Agent

  Intelligent conversational assistants, such as Apple's Siri, Microsoft'sCortana, and Amazon's Echo, have quickly become a part of our digital life.However, these assistants have major limitations, which prevents users fromconversing with them as they would with human dialog partners. This limits ourability to observe how users really want to interact with the underlyingsystem. To address this problem, we developed a crowd-powered conversationalassistant, Chorus, and deployed it to see how users and workers would interacttogether when mediated by the system. Chorus sophisticatedly converses with endusers over time by recruiting workers on demand, which in turn decide whatmight be the best response for each user sentence. Up to the first month of ourdeployment, 59 users have held conversations with Chorus during 320conversational sessions. In this paper, we present an account of Chorus'deployment, with a focus on four challenges: (i) identifying when conversationsare over, (ii) malicious users and workers, (iii) on-demand recruiting, and(iv) settings in which consensus is not enough. Our observations could assistthe deployment of crowd-powered conversation systems and crowd-powered systemsin general.

