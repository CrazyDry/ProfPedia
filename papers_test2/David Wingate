The Infinite Latent Events Model

  We present the Infinite Latent Events Model, a nonparametric hierarchicalBayesian distribution over infinite dimensional Dynamic Bayesian Networks withbinary state representations and noisy-OR-like transitions. The distributioncan be used to learn structure in discrete timeseries data by simultaneouslyinferring a set of latent events, which events fired at each timestep, and howthose events are causally linked. We illustrate the model on a soundfactorization task, a network topology identification task, and a video gametask.

Round Table Discussion at the Final Session of FPCP 2008: The Future of  Flavor Physics and CP

  The final session of FPCP 2008 consisted of a round-table discussion amongpanelists and audience. The panelists included Jeffrey Appel(moderator), MartinBeneke, George W.S. Hou, David Kirkby, Dmitri Tsybychev, Matt Wingate, and TakuYamanaka. What follows is an edited transcript of the session.

Automated Variational Inference in Probabilistic Programming

  We present a new algorithm for approximate inference in probabilisticprograms, based on a stochastic gradient for variational programs. This methodis efficient without restrictions on the probabilistic program; it isparticularly practical for distributions which are not analytically tractable,including highly structured distributions that arise in probabilistic programs.We show how to automatically derive mean-field probabilistic programs andoptimize them, and demonstrate that our perspective improves inferenceefficiency over other algorithms.

Graph Neural Processes: Towards Bayesian Graph Neural Networks

  We introduce Graph Neural Processes (GNP), inspired by the recent work inconditional and latent neural processes. A Graph Neural Process is defined as aConditional Neural Process that operates on arbitrary graph data. It takesfeatures of sparsely observed context points as input, and outputs adistribution over target points. We demonstrate graph neural processes in edgeimputation and discuss benefits and drawbacks of the method for otherapplication areas. One major benefit of GNPs is the ability to quantifyuncertainty in deep learning on graph structures. An additional benefit of thismethod is the ability to extend graph neural networks to inputs of dynamicsized graphs.

Video Extrapolation with an Invertible Linear Embedding

  We predict future video frames from complex dynamic scenes, using aninvertible neural network as the encoder of a nonlinear dynamic system withlatent linear state evolution. Our invertible linear embedding (ILE)demonstrates successful learning, prediction and latent state inference. Incontrast to other approaches, ILE does not use any explicit reconstruction lossor simplistic pixel-space assumptions. Instead, it leverages invertibility tooptimize the likelihood of image sequences exactly, albeit indirectly.Comparison with a state-of-the-art method demonstrates the viability of ourapproach.

Bottom hadron mass splittings in the static limit from 2+1 flavour  lattice QCD

  Dynamical 2+1 flavour lattice QCD is used to calculate the splittings betweenthe masses of mesons and baryons containing a single static heavy quark anddomain-wall light and strange quarks. Our calculations are based on thedynamical domain-wall gauge field configurations generated by the RBC and UKQCDcollaborations at a spatial volume of (2.7 fm)^3 and a range of quark masseswith a lightest value corresponding to a (partially-quenched) pion mass of 275MeV. When extrapolated to the physical values of the light quark masses, theresults of our calculations are generally in good agreement with experimentaldeterminations in the bottom sector. However, the static limit splittingsbetween the Omega_b^- baryon and other bottom hadrons tend to slightlyunderestimate those obtained using the recent D-zero measurement of theOmega_b^-.

Bottom hadrons from lattice QCD with domain wall and NRQCD fermions

  Dynamical 2+1 flavor lattice QCD is used to calculate the masses of bottomhadrons, including B mesons, singly and doubly bottom baryons, and for thefirst time also the triply-bottom baryon Omega_bbb. The domain wall action isused for the up-, down-, and strange quarks (both valence and sea), while thebottom quark is implemented with non-relativistic QCD. A calculation of thebottomonium spectrum is also presented.

A Bayesian Sampling Approach to Exploration in Reinforcement Learning

  We present a modular approach to reinforcement learning that uses a Bayesianrepresentation of the uncertainty over models. The approach, BOSS (Best ofSampled Set), drives exploration by sampling multiple models from the posteriorand selecting actions optimistically. It extends previous work by providing arule for deciding when to resample and how to combine the models. We show thatour algorithm achieves nearoptimal reward with high probability with a samplecomplexity that is low relative to the speed at which the posteriordistribution converges during learning. We demonstrate that BOSS performs quitefavorably compared to state-of-the-art reinforcement-learning approaches andillustrate its flexibility by pairing it with a non-parametric model thatgeneralizes across states.

Predictive Linear-Gaussian Models of Stochastic Dynamical Systems

  Models of dynamical systems based on predictive state representations (PSRs)are defined strictly in terms of observable quantities, in contrast withtraditional models (such as Hidden Markov Models) that use latent variables orstatespace representations. In addition, PSRs have an effectively infinitememory, allowing them to model some systems that finite memory-based modelscannot. Thus far, PSR models have primarily been developed for domains withdiscrete observations. Here, we develop the Predictive Linear-Gaussian (PLG)model, a class of PSR models for domains with continuous observations. We showthat PLG models subsume Linear Dynamical System models (also called Kalmanfilter models or state-space models) while using fewer parameters. We alsointroduce an algorithm to estimate PLG parameters from data, and contrast itwith standard Expectation Maximization (EM) algorithms used to estimate Kalmanfilter parameters. We show that our algorithm is a consistent estimationprocedure and present preliminary empirical results suggesting that ouralgorithm outperforms EM, particularly as the model dimension increases.

Form factors for Lambda_b -> Lambda transitions from lattice QCD

  The rare baryonic decays $\Lambda_b \to \Lambda \mu^+ \mu^-$ and $\Lambda_b\to \Lambda \gamma$ can complement rare $B$ meson decays in constraining modelsof new physics. In this work, we calculate the relevant $\Lambda_b \to \Lambda$transition form factors at leading order in heavy-quark effective theory usinglattice QCD. Our analysis is based on RBC/UKQCD gauge field ensembles with 2+1flavors of domain-wall fermions, and with lattice spacings of $a\approx 0.11$fm and $a\approx 0.08$ fm. We compute appropriate ratios of three-point andtwo-point correlation functions for a wide range of source-sink separations,and extrapolate to infinite separation in order to eliminate excited-statecontamination. We then extrapolate the form factors to the continuum limit andto the physical values of the light-quark masses.

Lambda_b -> Lambda l+ l- form factors and differential branching  fraction from lattice QCD

  We present the first lattice QCD determination of the $\Lambda_b \to \Lambda$transition form factors that govern the rare baryonic decays $\Lambda_b \to\Lambda l^+ l^-$ at leading order in heavy-quark effective theory. Ourcalculations are performed with 2+1 flavors of domain-wall fermions, at twolattice spacings and with pion masses down to 227 MeV. Three-point functionswith a wide range of source-sink separations are used to extract theground-state contributions. The form factors are extrapolated to the physicalvalues of the light-quark masses and to the continuum limit. We use our resultsto calculate the differential branching fractions for $\Lambda_b \to \Lambdal^+ l^-$ with $l=e,\mu,\tau$ within the standard model. We find agreement witha recent CDF measurement of the $\Lambda_b \to \Lambda \mu^+ \mu^-$differential branching fraction.

What can you do with a rock? Affordance extraction via word embeddings

  Autonomous agents must often detect affordances: the set of behaviors enabledby a situation. Affordance detection is particularly helpful in domains withlarge action spaces, allowing the agent to prune its search space by avoidingfutile behaviors. This paper presents a method for affordance extraction viaword embeddings trained on a Wikipedia corpus. The resulting word vectors aretreated as a common knowledge database which can be queried using linearalgebra. We apply this method to a reinforcement learning agent in a text-onlyenvironment and show that affordance-based action selection improvesperformance most of the time. Our method increases the computational complexityof each learning step but significantly reduces the total number of stepsneeded. In addition, the agent's action selections begin to resemble those ahuman would choose.

Probabilistic programs for inferring the goals of autonomous agents

  Intelligent systems sometimes need to infer the probable goals of people,cars, and robots, based on partial observations of their motion. This paperintroduces a class of probabilistic programs for formulating and solving theseproblems. The formulation uses randomized path planning algorithms as the basisfor probabilistic models of the process by which autonomous agents plan toachieve their goals. Because these path planning algorithms do not havetractable likelihood functions, new inference algorithms are needed. This paperproposes two Monte Carlo techniques for these "likelihood-free" models, one ofwhich can use likelihood estimates from neural networks to accelerateinference. The paper demonstrates efficacy on three simple examples, each usingunder 50 lines of probabilistic code.

Embedding Grammars

  Classic grammars and regular expressions can be used for a variety ofpurposes, including parsing, intent detection, and matching. However, thecomparisons are performed at a structural level, with constituent elements(words or characters) matched exactly. Recent advances in word embeddings showthat semantically related words share common features in a vector-spacerepresentation, suggesting the possibility of a hybrid grammar and wordembedding. In this paper, we blend the structure of standard context-freegrammars with the semantic generalization capabilities of word embeddings tocreate hybrid semantic grammars. These semantic grammars generalize thespecific terminals used by the programmer to other words and phrases withrelated meanings, allowing the construction of compact grammars that match anentire region of the vector space rather than matching specific elements.

General machine-learning surrogate models for materials prediction

  Surrogate machine-learning models are transforming computational materialsscience by predicting properties of materials with the accuracy of ab initiomethods at a fraction of the computational cost. We demonstrate surrogatemodels that simultaneously interpolate energies of different materials on adataset of 10 binary alloys (AgCu, AlFe, AlMg, AlNi, AlTi, CoNi, CuFe, CuNi,FeV, NbNi) with 10 different species and all possible fcc, bcc and hcpstructures up to 8 atoms in the unit cell, 15 950 structures in total. We findthat prediction errors remain unaffected when increasing the number ofsimultaneously modeled alloys. Several state-of-the-art materialsrepresentations and learning algorithms were found to qualitatively agree, withprediction errors as low as 1 meV/atom.

Modeling Theory of Mind for Autonomous Agents with Probabilistic  Programs

  As autonomous agents become more ubiquitous, they will eventually have toreason about the mental state of other agents, including those agents' beliefs,desires and goals - so-called theory of mind reasoning. We introduce acollection of increasingly complex theory of mind models of a "chaser" pursuinga "runner", known as the Chaser-Runner model. We show that our implementationis a relatively straightforward theory of mind model that can capture a varietyof rich behaviors, which in turn, increase runner detection rates relative tobasic (non-theory-of-mind) models. In addition, our paper demonstrates that (1)using a planning-as-inference formulation based on nested importance samplingresults in agents simultaneously reasoning about other agents' plans andcrafting counter-plans, (2) probabilistic programming is a natural way todescribe models in which each uses complex primitives such as path planners tomake decisions, and (3) allocating additional computation to perform nestedreasoning about agents result in lower-variance estimates of expected utility.

$Λ_b \to p l^- \barν$ form factors from lattice QCD with  static b quarks

  We present a lattice QCD calculation of form factors for the decay $\Lambda_b\to p \mu^- \bar{\nu}$, which is a promising channel for determining the CKMmatrix element $|V_{ub}|$ at the Large Hadron Collider. In this initial studywe work in the limit of static b quarks, where the number of independent formfactors reduces to two. We use dynamical domain-wall fermions for the lightquarks, and perform the calculation at two different lattice spacings and atmultiple values of the light-quark masses in a single large volume. Using ourform factor results, we calculate the $\Lambda_b \to p \mu^- \bar{\nu}$differential decay rate in the range $14 GeV^2 \leq q^2 \leq q^2_{max}$, andobtain the integral $\int_{14 GeV^2}^{q^2_{max}} [d\Gamma/dq^2] dq^2 /|V_{ub}|^2 = 15.3 \pm 4.2 ps^{-1}$. Combined with future experimental data,this will give a novel determination of $|V_{ub}|$ with about 15\% theoreticaluncertainty. The uncertainty is dominated by the use of the staticapproximation for the b quark, and can be reduced further by performing thelattice calculation with a more sophisticated heavy-quark action.

Estimating Human Intent for Physical Human-Robot Co-Manipulation

  Human teams can be exceptionally efficient at adapting and collaboratingduring manipulation tasks using shared mental models. However, the same sharedmental models that can be used by humans to perform robust low-level force andmotion control during collaborative manipulation tasks are non-existent forrobots. For robots to perform collaborative tasks with people naturally andefficiently, understanding and predicting human intent is necessary. However,humans are difficult to predict and model. We have completed an exploratorystudy recording motion and force for 20 human dyads moving an object in tandemin order to better understand how they move and how their movement can bepredicted. In this paper, we show how past motion data can be used to predicthuman intent. In order to predict human intent, which we equate with the humanteam's velocity for a short time horizon, we used a neural network. Using theprevious 150 time steps at a rate of 200 Hz, human intent can be predicted forthe next 50 time steps with a mean squared error of 0.02 (m/s)^2. We also showthat human intent can be estimated in a human-robot dyad. This work is animportant first step in enabling future work of integrating human intentestimation on a robot controller to execute a short-term collaborativetrajectory.

