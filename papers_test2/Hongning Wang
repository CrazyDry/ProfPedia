Explainable Recommendation via Multi-Task Learning in Opinionated Text  Data

  Explaining automatically generated recommendations allows users to make moreinformed and accurate decisions about which results to utilize, and thereforeimproves their satisfaction. In this work, we develop a multi-task learningsolution for explainable recommendation. Two companion learning tasks of userpreference modeling for recommendation} and \textit{opinionated contentmodeling for explanation are integrated via a joint tensor factorization. As aresult, the algorithm predicts not only a user's preference over a list ofitems, i.e., recommendation, but also how the user would appreciate aparticular item at the feature level, i.e., opinionated textual explanation.Extensive experiments on two large collections of Amazon and Yelp reviewsconfirmed the effectiveness of our solution in both recommendation andexplanation tasks, compared with several existing recommendation algorithms.And our extensive user study clearly demonstrates the practical value of theexplainable recommendations generated by our algorithm.

Learning Contextual Bandits in a Non-stationary Environment

  Multi-armed bandit algorithms have become a reference solution for handlingthe explore/exploit dilemma in recommender systems, and many other importantreal-world problems, such as display advertisement. However, such algorithmsusually assume a stationary reward distribution, which hardly holds in practiceas users' preferences are dynamic. This inevitably costs a recommender systemconsistent suboptimal performance. In this paper, we consider the situationwhere the underlying distribution of reward remains unchanged over (possiblyshort) epochs and shifts at unknown time instants. In accordance, we propose acontextual bandit algorithm that detects possible changes of environment basedon its reward estimation confidence and updates its arm selection strategyrespectively. Rigorous upper regret bound analysis of the proposed algorithmdemonstrates its learning effectiveness in such a non-trivial environment.Extensive empirical evaluations on both synthetic and real-world datasets forrecommendation confirm its practical utility in a changing environment.

Efficient Exploration of Gradient Space for Online Learning to Rank

  Online learning to rank (OL2R) optimizes the utility of returned searchresults based on implicit feedback gathered directly from users. To improve theestimates, OL2R algorithms examine one or more exploratory gradient directionsand update the current ranker if a proposed one is preferred by users via aninterleaved test. In this paper, we accelerate the online learning process byefficient exploration in the gradient space. Our algorithm, named as Null SpaceGradient Descent, reduces the exploration space to only the \emph{null space}of recent poorly performing gradients. This prevents the algorithm fromrepeatedly exploring directions that have been discouraged by the most recentinteractions with users. To improve sensitivity of the resulting interleavedtest, we selectively construct candidate rankers to maximize the chance thatthey can be differentiated by candidate ranking documents in the current query;and we use historically difficult queries to identify the best ranker when tieoccurs in comparing the rankers. Extensive experimental comparisons with thestate-of-the-art OL2R algorithms on several public benchmarks confirmed theeffectiveness of our proposal algorithm, especially in its fast learningconvergence and promising ranking quality at an early stage.

