Sentiment Classification using Images and Label Embeddings

  In this project we analysed how much semantic information images carry, andhow much value image data can add to sentiment analysis of the text associatedwith the images. To better understand the contribution from images, we comparedmodels which only made use of image data, models which only made use of textdata, and models which combined both data types. We also analysed if thisapproach could help sentiment classifiers generalize to unknown sentiments.

Mid-level Elements for Object Detection

  Building on the success of recent discriminative mid-level elements, wepropose a surprisingly simple approach for object detection which performscomparable to the current state-of-the-art approaches on PASCAL VOC comp-3detection challenge (no external data). Through extensive experiments andablation analysis, we show how our approach effectively improves upon theHOG-based pipelines by adding an intermediate mid-level representation for thetask of object detection. This representation is easily interpretable andallows us to visualize what our object detector "sees". We also discuss theinsights our approach shares with CNN-based methods, such as sharingrepresentation between categories helps.

Autonomous Ingress of a UAV through a window using Monocular Vision

  The use of autonomous UAVs for surveillance purposes and other reconnaissancetasks is increasingly becoming popular and convenient.These tasks requires theability to successfully ingress through the rectangular openings or windows ofthe target structure.In this paper, a method to robustly detect the window inthe surrounding using basic image processing techniques and efficient distancemeasure, is proposed.Furthermore, a navigation scheme which incorporates thisdetection method for performing navigation task has also been proposed.Thewhole navigation task is performed and tested in the simulation environmentGAZEBO.

Parametric Synthesis of Text on Stylized Backgrounds using PGGANs

  We describe a novel method of generating high-resolution real-world images oftext where the style and textual content of the images are describedparametrically. Our method combines text to image retrieval techniques withprogressive growing of Generative Adversarial Networks (PGGANs) to achieveconditional generation of photo-realistic images that reflect specific styles,as well as artifacts seen in real-world images. We demonstrate our method inthe context of automotive license plates. We assess the impact of varying thenumber of training images of each style on the fidelity of the generated style,and demonstrate the quality of the generated images using license platerecognition systems.

Training Region-based Object Detectors with Online Hard Example Mining

  The field of object detection has made significant advances riding on thewave of region-based ConvNets, but their training procedure still includes manyheuristics and hyperparameters that are costly to tune. We present a simple yetsurprisingly effective online hard example mining (OHEM) algorithm for trainingregion-based ConvNet detectors. Our motivation is the same as it has alwaysbeen -- detection datasets contain an overwhelming number of easy examples anda small number of hard examples. Automatic selection of these hard examples canmake training more effective and efficient. OHEM is a simple and intuitivealgorithm that eliminates several heuristics and hyperparameters in common use.But more importantly, it yields consistent and significant boosts in detectionperformance on benchmarks like PASCAL VOC 2007 and 2012. Its effectivenessincreases as datasets become larger and more difficult, as demonstrated by theresults on the MS COCO dataset. Moreover, combined with complementary advancesin the field, OHEM leads to state-of-the-art results of 78.9% and 76.3% mAP onPASCAL VOC 2007 and 2012 respectively.

Contribution of Kaluza-Klein modes to vacuum energy in models with large  extra dimensions $&$ the Cosmological constant

  In this paper, the generation of topological energy in models with largeextra dimensions is investigated. The origin of this energy is attributed to atopological deformation of the standard Minkowski vacuum due tocompactification of extra dimensions. This deformation is seen to give rise toan effective, finite energy density due to massive Kaluza-Klein modes ofgravitation. It's renormalized value is seen to depend on the size of the extradimensions instead of the UV cut-off of the theory. It is shown that if thisenergy density is to contribute to the observed cosmological constant, therewill be extremely stringent bounds on the number of extra dimensions and theirsize.

Beyond Skip Connections: Top-Down Modulation for Object Detection

  In recent years, we have seen tremendous progress in the field of objectdetection. Most of the recent improvements have been achieved by targetingdeeper feedforward networks. However, many hard object categories such asbottle, remote, etc. require representation of fine details and not justcoarse, semantic representations. But most of these fine details are lost inthe early convolutional layers. What we need is a way to incorporate finerdetails from lower layers into the detection architecture. Skip connectionshave been proposed to combine high-level and low-level features, but we arguethat selecting the right features from low-level requires top-down contextualinformation. Inspired by the human visual pathway, in this paper we proposetop-down modulations as a way to incorporate fine details into the detectionframework. Our approach supplements the standard bottom-up, feedforward ConvNetwith a top-down modulation (TDM) network, connected using lateral connections.These connections are responsible for the modulation of lower layer filters,and the top-down network handles the selection and integration of contextualinformation and low-level features. The proposed TDM architecture provides asignificant boost on the COCO testdev benchmark, achieving 28.6 AP for VGG16,35.2 AP for ResNet101, and 37.3 for InceptionResNetv2 network, without anybells and whistles (e.g., multi-scale, iterative box refinement, etc.).

Cross-stitch Networks for Multi-task Learning

  Multi-task learning in Convolutional Networks has displayed remarkablesuccess in the field of recognition. This success can be largely attributed tolearning shared representations from multiple supervisory tasks. However,existing multi-task approaches rely on enumerating multiple networkarchitectures specific to the tasks at hand, that do not generalize. In thispaper, we propose a principled approach to learn shared representations inConvNets using multi-task learning. Specifically, we propose a new sharingunit: "cross-stitch" unit. These units combine the activations from multiplenetworks and can be trained end-to-end. A network with cross-stitch units canlearn an optimal combination of shared and task-specific representations. Ourproposed method generalizes across multiple tasks and shows dramaticallyimproved performance over baseline methods for categories with few trainingexamples.

A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection

  How do we learn an object detector that is invariant to occlusions anddeformations? Our current solution is to use a data-driven strategy -- collectlarge-scale datasets which have object instances under different conditions.The hope is that the final classifier can use these examples to learninvariances. But is it really possible to see all the occlusions in a dataset?We argue that like categories, occlusions and object deformations also follow along-tail. Some occlusions and deformations are so rare that they hardlyhappen; yet we want to learn a model invariant to such occurrences. In thispaper, we propose an alternative solution. We propose to learn an adversarialnetwork that generates examples with occlusions and deformations. The goal ofthe adversary is to generate examples that are difficult for the objectdetector to classify. In our framework both the original detector and adversaryare learned in a joint manner. Our experimental results indicate a 2.3% mAPboost on VOC07 and a 2.6% mAP boost on VOC2012 object detection challengecompared to the Fast-RCNN pipeline. We also release the code for this paper.

Stability of traveling, pre-tensioned, heavy cables

  We study the dynamics of an inclined tensioned, heavy cable traveling with aconstant speed in the vertical plane. The cable is modeled as a beam resistingbending and shear. The governing equation for the transverse in-planevibrations of the cable are derived through the Newton-Euler method. The cabledynamics is also studied in the limit of zero bending stiffness. In all cases,application of en- ergy balance reveals that the total energy of the systemfluctuates even though the oscillations are small and bounded in time,indicating that the system is nonconser- vative. A comprehensive stabilityanalysis is carried out in the parameter space of inclination, traveling speed,pre-tension, bending rigidity and the slenderness of the cable. Effect ofdamping is also considered. We conclude that, while pre-tension, rigidity andslenderness enhance the stability of the traveling cable, the angle ofinclination affects the stability adversely. These results may act asguidelines for safer design and operation.

Neural Signatures for Licence Plate Re-identification

  The problem of vehicle licence plate re-identification is generallyconsidered as a one-shot image retrieval problem. The objective of this task isto learn a feature representation (called a "signature") for licence plates.Incoming licence plate images are converted to signatures and matched to apreviously collected template database through a distance measure. Then, theinput image is recognized as the template whose signature is "nearest" to theinput signature. The template database is restricted to contain only a singlesignature per unique licence plate for our problem.  We measure the performance of deep convolutional net-based features adaptedfrom face recognition on this task. In addition, we also test a hybrid approachcombining the Fisher vector with a neural network-based embedding called "f2nn"trained with the Triplet loss function. We find that the hybrid approachperforms comparably while providing computational benefits. The signaturegenerated by the hybrid approach also shows higher generalizability to datasetsmore dissimilar to the training corpus.

Multi-task learning for Joint Language Understanding and Dialogue State  Tracking

  This paper presents a novel approach for multi-task learning of languageunderstanding (LU) and dialogue state tracking (DST) in task-oriented dialoguesystems. Multi-task training enables the sharing of the neural network layersresponsible for encoding the user utterance for both LU and DST and improvesperformance while reducing the number of network parameters. In our proposedframework, DST operates on a set of candidate values for each slot that hasbeen mentioned so far. These candidate sets are generated using LU slotannotations for the current user utterance, dialogue acts corresponding to thepreceding system utterance and the dialogue state estimated for the previousturn, enabling DST to handle slots with a large or unbounded set of possiblevalues and deal with slot values not seen during training. Furthermore, tobridge the gap between training and inference, we investigate the use ofscheduled sampling on LU output for the current user utterance as well as theDST output for the preceding turn.

Unitarity constraints on the stabilized Randall-Sundrum scenario

  Recently proposed stabilization mechanism of the Randall-Sundrum metric givesrise to a scalar radion, which couples universally to matter with a weakinteraction ($\simeq 1$ TeV) scale. Demanding that gauge boson scattering asdescribed by the effective low enerrgy theory be unitary upto a given scaleleads to significant constraints on the mass of such a radion.

An Implementation of Faster RCNN with Study for Region Sampling

  We adapted the join-training scheme of Faster RCNN framework from Caffe toTensorFlow as a baseline implementation for object detection. Our code is madepublicly available. This report documents the simplifications made to theoriginal pipeline, with justifications from ablation analysis on both PASCALVOC 2007 and COCO 2014. We further investigated the role of non-maximalsuppression (NMS) in selecting regions-of-interest (RoIs) for regionclassification, and found that a biased sampling toward small regions helpsperformance and can achieve on-par mAP to NMS-based sampling when convergedsufficiently.

PyText: A Seamless Path from NLP research to production

  We introduce PyText - a deep learning based NLP modeling framework built onPyTorch. PyText addresses the often-conflicting requirements of enabling rapidexperimentation and of serving models at scale. It achieves this by providingsimple and extensible interfaces for model components, and by using PyTorch'scapabilities of exporting models for inference via the optimized Caffe2execution engine. We report our own experience of migrating experimentation andproduction workflows to PyText, which enabled us to iterate faster on novelmodeling ideas and then seamlessly ship them at industrial scale.

Visual Features for Context-Aware Speech Recognition

  Automatic transcriptions of consumer-generated multi-media content such as"Youtube" videos still exhibit high word error rates. Such data typicallyoccupies a very broad domain, has been recorded in challenging conditions, withcheap hardware and a focus on the visual modality, and may have beenpost-processed or edited. In this paper, we extend our earlier work on adaptingthe acoustic model of a DNN-based speech recognition system to an RNN languagemodel and show how both can be adapted to the objects and scenes that can beautomatically detected in the video. We are working on a corpus of "how-to"videos from the web, and the idea is that an object that can be seen ("car"),or a scene that is being detected ("kitchen") can be used to condition bothmodels on the "context" of the recording, thereby reducing perplexity andimproving transcription. We achieve good improvements in both cases and compareand analyze the respective reductions in word error rate. We expect that ourresults can be used for any type of speech processing in which "context"information is available, for example in robotics, man-machine interaction, orwhen indexing large audio-visual archives, and should ultimately help to bringtogether the "video-to-text" and "speech-to-text" communities.

An Efficient Approach to Encoding Context for Spoken Language  Understanding

  In task-oriented dialogue systems, spoken language understanding, or SLU,refers to the task of parsing natural language user utterances into semanticframes. Making use of context from prior dialogue history holds the key to moreeffective SLU. State of the art approaches to SLU use memory networks to encodecontext by processing multiple utterances from the dialogue at each turn,resulting in significant trade-offs between accuracy and computationalefficiency. On the other hand, downstream components like the dialogue statetracker (DST) already keep track of the dialogue state, which can serve as asummary of the dialogue history. In this work, we propose an efficient approachto encoding context from prior utterances for SLU. More specifically, ourarchitecture includes a separate recurrent neural network (RNN) based encodingmodule that accumulates dialogue context to guide the frame parsing sub-tasksand can be shared between SLU and DST. In our experiments, we demonstrate theeffectiveness of our approach on dialogues from two domains.

Design and Development of Underwater Vehicle: ANAHITA

  Anahita is an autonomous underwater vehicle which is currently beingdeveloped by interdisciplinary team of students at Indian Institute ofTechnology(IIT) Kanpur with aim to provide a platform for research in AUV toundergraduate students. This is the second vehicle which is being designed byAUV-IITK team to participate in 6th NIOT-SAVe competition organized by theNational Institute of Ocean Technology, Chennai. The Vehicle has beencompletely redesigned with the major improvements in modularity and ease ofaccess of all the components, keeping the design very compact and efficient.New advancements in the vehicle include, power distribution system andmonitoring system. The sensors include the inertial measurement units (IMU),hydrophone array, a depth sensor, and two RGB cameras. The current vehiclefeatures hot swappable battery pods giving a huge advantage over the previousvehicle, for longer runtime.

Learning Exploration Policies for Navigation

  Numerous past works have tackled the problem of task-driven navigation. But,how to effectively explore a new environment to enable a variety of down-streamtasks has received much less attention. In this work, we study how agents canautonomously explore realistic and complex 3D environments without the contextof task-rewards. We propose a learning-based approach and investigate differentpolicy architectures, reward functions, and training paradigms. We find thatthe use of policies with spatial memory that are bootstrapped with imitationlearning and finally finetuned with coverage rewards derived purely fromon-board sensors can be effective at exploring novel environments. We show thatour learned exploration policies can explore better than classical approachesbased on geometry alone and generic learning-based exploration techniques.Finally, we also show how such task-agnostic exploration can be used fordown-stream tasks. Code and Videos are available at:https://sites.google.com/view/exploration-for-nav.

Revisiting Unreasonable Effectiveness of Data in Deep Learning Era

  The success of deep learning in vision can be attributed to: (a) models withhigh capacity; (b) increased computational power; and (c) availability oflarge-scale labeled data. Since 2012, there have been significant advances inrepresentation capabilities of the models and computational capabilities ofGPUs. But the size of the biggest dataset has surprisingly remained constant.What will happen if we increase the dataset size by 10x or 100x? This papertakes a step towards clearing the clouds of mystery surrounding therelationship between `enormous data' and visual deep learning. By exploitingthe JFT-300M dataset which has more than 375M noisy labels for 300M images, weinvestigate how the performance of current vision tasks would change if thisdata was used for representation learning. Our paper delivers some surprising(and some expected) findings. First, we find that the performance on visiontasks increases logarithmically based on volume of training data size. Second,we show that representation learning (or pre-training) still holds a lot ofpromise. One can improve performance on many vision tasks by just training abetter base model. Finally, as expected, we present new state-of-the-artresults for different vision tasks including image classification, objectdetection, semantic segmentation and human pose estimation. Our sincere hope isthat this inspires vision community to not undervalue the data and developcollective efforts in building larger datasets.

Beating the Multiplicative Weights Update Algorithm

  Multiplicative weights update algorithms have been used extensively indesigning iterative algorithms for many computational tasks. The core idea isto maintain a distribution over a set of experts and update this distributionin an online fashion based on the parameters of the underlying optimizationproblem. In this report, we study the behavior of a special MWU algorithm usedfor generating a global coin flip in the presence of an adversary that tampersthe experts' advice. Specifically, we focus our attention on two adversarialstrategies: (1) non-adaptive, in which the adversary chooses a fixed set ofexperts a priori and corrupts their advice in each round; and (2) adaptive, inwhich this set is chosen as the rounds of the algorithm progress. We formulatethese adversarial strategies as being greedy in terms of trying to maximize theshare of the corrupted experts in the final weighted advice the MWU computesand provide the underlying optimization problem that needs to be solved toachieve this goal. We provide empirical results to show that in the presence ofeither of the above adversaries, the MWU algorithm takes $\mathcal{O}(n)$rounds in expectation to produce the desired output. This result compares wellwith the current state of the art of $\mathcal{O}(n^3)$ for the generalByzantine consensus problem. Finally, we briefly discuss the extension of theseadversarial strategies for a general MWU algorithm and provide an outline forthe framework in that setting.

Higgs-Axion interplay and anomalous magnetic phase diagram in TlCuCl$_3$

  What is so unique in TlCuCl3 which drives so many unique magnetic features inthis compound? To study these properties, here we employ a combination ofab-initio band structure, tight-binding model, and an effective quantum fieldtheory. Within a density-functional theory (DFT) calculation, we find anunexpected bulk Dirac cone without spin-orbit coupling (SOC). Tracing back toits origin, we identify, for the first time, the presence of aSu-Schrieffer-Heeger (SSH) like dimerized Cu chain lying in the 3D crystalstructure. The SSH chain, combined with SOC, stipulates an anisotropic 3D Diraccone where chiral and helical states are intertwined. As a Heisenberginteraction is introduced, we show that the dimerized Cu sublattices of the SSHchain condensate into spin-singlet, dimerized magnets. In the magnetic groundstate, we also find a topological phase, distinguished by the axion angle.Finally, to study how the topological axion term couples to magneticexcitations, we derive a Chern-Simons-Ginzburg-Landau action from the 3D SSHHamiltonian. We find that axion term provides an additional mass term to theHiggs mode, and a lifetime to paramagnons, which are independent of the quantumcritical physics. The axion-Higgs interplay can be probed with electric andmagnetic field applied parallel or anti-parallel to each other.

τPolarization asymmetry in $B \to X_s τ^+ τ^-$ in SUSY models  with large $tanβ$

  Rare B decays provides an opportunity to probe for new physics beyond thestandard model. the effective Hamiltonian for the decay $b \to s l^+ l^-$predicts the characteristic polarization for the final state lepton. Leptonpolarization has, in addition to a longitudinal component $P_L$, two orthogonalcomponents $P_T$ and $P_N$ lying in and perpendicular to the decay plane. Inthis article we perform a study of the $\tau$-polarisation asymmetry in thecase of SUSY models with large $\tan\beta$ in the inclusive decay $B \to X_s\tau^+ \tau^-$.

Longitudinal Polarization in $K_L \to μ^+ μ^-$ in MSSM with large  $tanβ$

  A complete experiment on decay $K_L \to l^+ l^-$ will not only consist ofmeasurement of the decay rates but also lepton polarization etc. Theseadditional observations will yield tests of CP invariance in these decays. In$K_L$ and $K_S$ decays, the e mode is slower than the $\mu$ mode by roughly$(m_e/m_\mu)^2$ \cite{sehgal1}. As well discussed in literature \cite{herczeg}the Standard Model contribution to the lepton polarization is of order $2\times \sim 10^{-3}$. We show that in MSSM with large \tanbeta and light higgsmasses ($\sim 2 M_W$), the longitudinal lepton polarization in $K_L \to \mu^+\mu^-$ can be enhanced to a higher value, of about $10^{-2}$.

Neutrinos as Source of Ultra High Energy Cosmic Rays in Extra Dimensions

  If the neutrinos are to be identified with the primary source of ultra-highenergy cosmic rays(UHECR), their interaction on relic neutrinos is of greatimportance in understanding their long intergalactic journey. In theories withlarge compact dimensions, the exchange of a tower of massive spin-2 gravitons(Kaluza-Klein excitations) gives extra contribution to $\nu\bar{\nu}\longrightarrow f\bar{f}$ and $\gamma\gamma$ processes along with the openingof a new channel for the neutrinos to annihilate with the relic cosmic neutrinobackground $\nu\bar{\nu} \longrightarrow G_{kk}$ to produce bulk gravitons inthe extra dimensions. This will affect their attenuation. We compute thecontribution of these Kaluza-Klein excitations to the above processes and findthat for parameters of the theory constrained by supernova cooling, thecontribution does indeed become the dominant contribution above $\sqrt{s}\simeq 300$ GeV.

Two loop radion correction to $K_L$ - $K_S$ mass difference in the  stabilised Randall-Sundrum brane world scenario

  In the stabilised Randall-Sundrum brane world scenario, the radion can havephenomenologically testable effects, which can be measured against preciselymeasured electroweak physics data. We investigate the effect of two loop radioncorrections to $K_L$ - $K_S$ mass difference to set a bound on the radion massand vacuum expectation value. It is found that the leading two loop correctionsare of the order $[Log(\frac{\Lambda^2}{m_\phi^2}) ]^2$ where $\Lambda$ is thecut-off scale ${\cal O}(\sim$TeV) and $m_\phi$ is the radion mass.

Neutrino Masses from Non-minimal Gravitational Interactions of Massive  Neutral Fermions

  A new mechanism is proposed for generating neutrino masses radiativelythrough a non-minimal coupling to gravity of fermionic bilinears involvingmassive neutral fermions. Such coupling terms can arise in theories where thegravity sector is augmented by a scalar field. They necessarily violate theprinciple of equivalence, but such violations are not ruled out by presentexperiments. It is shown that the proposed mechanism is realised mostconvincingly in theories of the Randall- Sundrum type, where gravity couplesstrongly in the TeV range. The mechanism has the potential for solving both thesolar and atmospheric neutrino problems. The smallness of neutrino masses inthis scenario is due to the fact that the interaction of the massive neutralfermions arises entirely from higher-dimensional operators in the effectiveLagrangian.

Radiation from a charged particle and radiation reaction -- revisited

  We study the electromagnetic fields of an arbitrarily moving charged particleand the radiation reaction on the charged particle using a novel approach. Wefirst show that the fields of an arbitrarily moving charged particle in aninertial frame can be related in a simple manner to the fields of a uniformlyaccelerated charged particle in its rest frame. Since the latter field isstatic and easily obtainable, it is possible to derive the fields of anarbitrarily moving charged particle by a coordinate transformation. Moreimportantly, this formalism allows us to calculate the self-force on a chargedparticle in a remarkably simple manner. We show that the original expressionfor this force, obtained by Dirac, can be rederived with much less computationand in an intuitively simple manner using our formalism.

Unsupervised Discovery of Mid-Level Discriminative Patches

  The goal of this paper is to discover a set of discriminative patches whichcan serve as a fully unsupervised mid-level visual representation. The desiredpatches need to satisfy two requirements: 1) to be representative, they need tooccur frequently enough in the visual world; 2) to be discriminative, they needto be different enough from the rest of the visual world. The patches couldcorrespond to parts, objects, "visual phrases", etc. but are not restricted tobe any one of them. We pose this as an unsupervised discriminative clusteringproblem on a huge dataset of image patches. We use an iterative procedure whichalternates between clustering and training discriminative classifiers, whileapplying careful cross-validation at each step to prevent overfitting. Thepaper experimentally demonstrates the effectiveness of discriminative patchesas an unsupervised mid-level visual representation, suggesting that it could beused in place of visual words for many tasks. Furthermore, discriminativepatches can also be used in a supervised regime, such as scene classification,where they demonstrate state-of-the-art performance on the MIT Indoor-67dataset.

Designing Deep Networks for Surface Normal Estimation

  In the past few years, convolutional neural nets (CNN) have shown incrediblepromise for learning visual representations. In this paper, we use CNNs for thetask of predicting surface normals from a single image. But what is the rightarchitecture we should use? We propose to build upon the decades of hard workin 3D scene understanding, to design new CNN architecture for the task ofsurface normal estimation. We show by incorporating several constraints(man-made, manhattan world) and meaningful intermediate representations (roomlayout, edge labels) in the architecture leads to state of the art performanceon surface normal estimation. We also show that our network is quite robust andshow state of the art results on other datasets as well without anyfine-tuning.

Dense Optical Flow Prediction from a Static Image

  Given a scene, what is going to move, and in what direction will it move?Such a question could be considered a non-semantic form of action prediction.In this work, we present a convolutional neural network (CNN) based approachfor motion prediction. Given a static image, this CNN predicts the futuremotion of each and every pixel in the image in terms of optical flow. Our CNNmodel leverages the data in tens of thousands of realistic videos to train ourmodel. Our method relies on absolutely no human labeling and is able to predictmotion based on the context of the scene. Because our CNN model makes noassumptions about the underlying scene, it can predict future optical flow on adiverse set of scenarios. We outperform all previous approaches by largemargins.

Unsupervised Learning of Visual Representations using Videos

  Is strong supervision necessary for learning a good visual representation? Dowe really need millions of semantically-labeled images to train a ConvolutionalNeural Network (CNN)? In this paper, we present a simple yet surprisinglypowerful approach for unsupervised learning of CNN. Specifically, we usehundreds of thousands of unlabeled videos from the web to learn visualrepresentations. Our key idea is that visual tracking provides the supervision.That is, two patches connected by a track should have similar visualrepresentation in deep feature space since they probably belong to the sameobject or object part. We design a Siamese-triplet network with a ranking lossfunction to train this CNN representation. Without using a single image fromImageNet, just using 100K unlabeled videos and the VOC 2012 dataset, we trainan ensemble of unsupervised networks that achieves 52% mAP (no bounding boxregression). This performance comes tantalizingly close to itsImageNet-supervised counterpart, an ensemble which achieves a mAP of 54.4%. Wealso show that our unsupervised network can perform competitively in othertasks such as surface-normal estimation.

In Defense of the Direct Perception of Affordances

  The field of functional recognition or affordance estimation from images hasseen a revival in recent years. As originally proposed by Gibson, theaffordances of a scene were directly perceived from the ambient light: in otherwords, functional properties like sittable were estimated directly fromincoming pixels. Recent work, however, has taken a mediated approach in whichaffordances are derived by first estimating semantics or geometry and thenreasoning about the affordances. In a tribute to Gibson, this paper exploreshis theory of affordances as originally proposed. We propose two approaches fordirect perception of affordances and show that they obtain good results and canout-perform mediated approaches. We hope this paper can rekindle discussionaround direct perception and its implications in the long term.

Webly Supervised Learning of Convolutional Networks

  We present an approach to utilize large amounts of web data for learningCNNs. Specifically inspired by curriculum learning, we present a two-stepapproach for CNN training. First, we use easy images to train an initial visualrepresentation. We then use this initial CNN and adapt it to harder, morerealistic images by leveraging the structure of data and categories. Wedemonstrate that our two-stage CNN outperforms a fine-tuned CNN trained onImageNet on Pascal VOC 2012. We also demonstrate the strength of weblysupervised learning by localizing objects in web images and training a R-CNNstyle detector. It achieves the best performance on VOC 2007 where no VOCtraining data is used. Finally, we show our approach is quite robust to noiseand performs comparably even when we use image search results from March 2013(pre-CNN image search era).

Unsupervised Visual Representation Learning by Context Prediction

  This work explores the use of spatial context as a source of free andplentiful supervisory signal for training a rich visual representation. Givenonly a large, unlabeled image collection, we extract random pairs of patchesfrom each image and train a convolutional neural net to predict the position ofthe second patch relative to the first. We argue that doing well on this taskrequires the model to learn to recognize objects and their parts. Wedemonstrate that the feature representation learned using this within-imagecontext indeed captures visual similarity across images. For example, thisrepresentation allows us to perform unsupervised visual discovery of objectslike cats, people, and even birds from the Pascal VOC 2011 detection dataset.Furthermore, we show that the learned ConvNet can be used in the R-CNNframework and provides a significant boost over a randomly-initialized ConvNet,resulting in state-of-the-art performance among algorithms which use onlyPascal-provided training set annotations.

Actions ~ Transformations

  What defines an action like "kicking ball"? We argue that the true meaning ofan action lies in the change or transformation an action brings to theenvironment. In this paper, we propose a novel representation for actions bymodeling an action as a transformation which changes the state of theenvironment before the action happens (precondition) to the state after theaction (effect). Motivated by recent advancements of video representation usingdeep learning, we design a Siamese network which models the action as atransformation on a high-level feature space. We show that our model givesimprovements on standard action recognition datasets including UCF101 andHMDB51. More importantly, our approach is able to generalize beyond learnedaction categories and shows significant performance improvement oncross-category generalization on our new ACT dataset.

Generative Image Modeling using Style and Structure Adversarial Networks

  Current generative frameworks use end-to-end learning and generate images bysampling from uniform noise distribution. However, these approaches ignore themost basic principle of image formation: images are product of: (a) Structure:the underlying 3D model; (b) Style: the texture mapped onto structure. In thispaper, we factorize the image generation process and propose Style andStructure Generative Adversarial Network (S^2-GAN). Our S^2-GAN has twocomponents: the Structure-GAN generates a surface normal map; the Style-GANtakes the surface normal map as input and generates the 2D image. Apart from areal vs. generated loss function, we use an additional loss with computedsurface normals from generated images. The two GANs are first trainedindependently, and then merged together via joint learning. We show our S^2-GANmodel is interpretable, generates more realistic images and can be used tolearn unsupervised RGBD representations.

A Fast Unified Model for Parsing and Sentence Understanding

  Tree-structured neural networks exploit valuable syntactic parse informationas they interpret the meanings of sentences. However, they suffer from two keytechnical problems that make them slow and unwieldy for large-scale NLP tasks:they usually operate on parsed sentences and they do not directly supportbatched computation. We address these issues by introducing the Stack-augmentedParser-Interpreter Neural Network (SPINN), which combines parsing andinterpretation within a single tree-sequence hybrid model by integratingtree-structured sentence interpretation into the linear sequential structure ofa shift-reduce parser. Our model supports batched computation for a speedup ofup to 25 times over other tree-structured models, and its integrated parser canoperate on unparsed data with little loss in accuracy. We evaluate it on theStanford NLI entailment task and show that it significantly outperforms othersentence-encoding models.

Learning a Predictable and Generative Vector Representation for Objects

  What is a good vector representation of an object? We believe that it shouldbe generative in 3D, in the sense that it can produce new 3D objects; as wellas be predictable from 2D, in the sense that it can be perceived from 2Dimages. We propose a novel architecture, called the TL-embedding network, tolearn an embedding space with these properties. The network consists of twocomponents: (a) an autoencoder that ensures the representation is generative;and (b) a convolutional network that ensures the representation is predictable.This enables tackling a number of tasks including voxel prediction from 2Dimages and 3D model retrieval. Extensive experimental analysis demonstrates theusefulness and versatility of this embedding.

Marr Revisited: 2D-3D Alignment via Surface Normal Prediction

  We introduce an approach that leverages surface normal predictions, alongwith appearance cues, to retrieve 3D models for objects depicted in 2D stillimages from a large CAD object library. Critical to the success of our approachis the ability to recover accurate surface normals for objects in the depictedscene. We introduce a skip-network model built on the pre-trained Oxford VGGconvolutional neural network (CNN) for surface normal prediction. Our modelachieves state-of-the-art accuracy on the NYUv2 RGB-D dataset for surfacenormal prediction, and recovers fine object detail compared to previousmethods. Furthermore, we develop a two-stream network over the input image andpredicted surface normals that jointly learns pose and style for CAD modelretrieval. When using the predicted surface normals, our two-stream networkmatches prior work using surface normals computed from RGB-D images on the taskof pose prediction, and achieves state of the art when using RGB-D input.Finally, our two-stream network allows us to retrieve CAD models that bettermatch the style and pose of a depicted object compared with baselineapproaches.

An Uncertain Future: Forecasting from Static Images using Variational  Autoencoders

  In a given scene, humans can often easily predict a set of immediate futureevents that might happen. However, generalized pixel-level anticipation incomputer vision systems is difficult because machine learning struggles withthe ambiguity inherent in predicting the future. In this paper, we focus onpredicting the dense trajectory of pixels in a scene, specifically what willmove in the scene, where it will travel, and how it will deform over the courseof one second. We propose a conditional variational autoencoder as a solutionto this problem. In this framework, direct inference from the image shapes thedistribution of possible trajectories, while latent variables encode anynecessary information that is not available in the image. We show that ourmethod is able to successfully predict events in a wide variety of scenes andcan produce multiple different predictions when the future is ambiguous. Ouralgorithm is trained on thousands of diverse, realistic videos and requiresabsolutely no human labeling. In addition to non-semantic action prediction, wefind that our method learns a representation that is applicable to semanticvision tasks.

Pose from Action: Unsupervised Learning of Pose Features based on Motion

  Human actions are comprised of a sequence of poses. This makes videos ofhumans a rich and dense source of human poses. We propose an unsupervisedmethod to learn pose features from videos that exploits a signal which iscomplementary to appearance and can be used as supervision: motion. The keyidea is that humans go through poses in a predictable manner while performingactions. Hence, given two poses, it should be possible to model the motion thatcaused the change between them. We represent each of the poses as a feature ina CNN (Appearance ConvNet) and generate a motion encoding from optical flowmaps using a separate CNN (Motion ConvNet). The data for this task isautomatically generated allowing us to train without human supervision. Wedemonstrate the strength of the learned representation by finetuning thetrained model for Pose Estimation on the FLIC dataset, for static image actionrecognition on PASCAL and for action recognition in videos on UCF101 andHMDB51.

PixelNet: Towards a General Pixel-level Architecture

  We explore architectures for general pixel-level prediction problems, fromlow-level edge detection to mid-level surface normal estimation to high-levelsemantic segmentation. Convolutional predictors, such as thefully-convolutional network (FCN), have achieved remarkable success byexploiting the spatial redundancy of neighboring pixels through convolutionalprocessing. Though computationally efficient, we point out that such approachesare not statistically efficient during learning precisely because spatialredundancy limits the information learned from neighboring pixels. Wedemonstrate that (1) stratified sampling allows us to add diversity duringbatch updates and (2) sampled multi-scale features allow us to explore morenonlinear predictors (multiple fully-connected layers followed by ReLU) thatimprove overall accuracy. Finally, our objective is to show how a architecturecan get performance better than (or comparable to) the architectures designedfor a particular task. Interestingly, our single architecture producesstate-of-the-art results for semantic segmentation on PASCAL-Context, surfacenormal estimation on NYUDv2 dataset, and edge detection on BSDS withoutcontextual post-processing.

Learning to Push by Grasping: Using multiple tasks for effective  learning

  Recently, end-to-end learning frameworks are gaining prevalence in the fieldof robot control. These frameworks input states/images and directly predict thetorques or the action parameters. However, these approaches are often critiqueddue to their huge data requirements for learning a task. The argument of thedifficulty in scalability to multiple tasks is well founded, since trainingthese tasks often require hundreds or thousands of examples. But do end-to-endapproaches need to learn a unique model for every task? Intuitively, it seemsthat sharing across tasks should help since all tasks require some commonunderstanding of the environment. In this paper, we attempt to take the nextstep in data-driven end-to-end learning frameworks: move from the realm oftask-specific models to joint learning of multiple robot tasks. In anastonishing result we show that models with multi-task learning tend to performbetter than task-specific models trained with same amounts of data. Forexample, a deep-network learned with 2.5K grasp and 2.5K push examples performsbetter on grasping than a network trained on 5K grasp examples.

Supervision via Competition: Robot Adversaries for Learning Tasks

  There has been a recent paradigm shift in robotics to data-driven learningfor planning and control. Due to large number of experiences required fortraining, most of these approaches use a self-supervised paradigm: usingsensors to measure success/failure. However, in most cases, these sensorsprovide weak supervision at best. In this work, we propose an adversariallearning framework that pits an adversary against the robot learning the task.In an effort to defeat the adversary, the original robot learns to perform thetask with more robustness leading to overall improved performance. We show thatthis adversarial framework forces the the robot to learn a better graspingmodel in order to overcome the adversary. By grasping 82% of presented novelobjects compared to 68% without an adversary, we demonstrate the utility ofcreating adversaries. We also demonstrate via experiments that having robots inadversarial setting might be a better learning strategy as compared to havingcollaborative multiple robots.

The More You Know: Using Knowledge Graphs for Image Classification

  One characteristic that sets humans apart from modern learning-based computervision algorithms is the ability to acquire knowledge about the world and usethat knowledge to reason about the visual world. Humans can learn about thecharacteristics of objects and the relationships that occur between them tolearn a large variety of visual concepts, often with few examples. This paperinvestigates the use of structured prior knowledge in the form of knowledgegraphs and shows that using this knowledge improves performance on imageclassification. We build on recent work on end-to-end learning on graphs,introducing the Graph Search Neural Network as a way of efficientlyincorporating large knowledge graphs into a vision classification pipeline. Weshow in a number of experiments that our method outperforms standard neuralnetwork baselines for multi-label classification.

Asynchronous Temporal Fields for Action Recognition

  Actions are more than just movements and trajectories: we cook to eat and wehold a cup to drink from it. A thorough understanding of videos requires goingbeyond appearance modeling and necessitates reasoning about the sequence ofactivities, as well as the higher-level constructs such as intentions. But howdo we model and reason about these? We propose a fully-connected temporal CRFmodel for reasoning over various aspects of activities that includes objects,actions, and intentions, where the potentials are predicted by a deep network.End-to-end training of such structured models is a challenging endeavor: Forinference and learning we need to construct mini-batches consisting of wholevideos, leading to mini-batches with only a few videos. This causeshigh-correlation between data points leading to breakdown of the backpropalgorithm. To address this challenge, we present an asynchronous variationalinference method that allows efficient end-to-end training. Our method achievesa classification mAP of 22.4% on the Charades benchmark, outperforming thestate-of-the-art (17.2% mAP), and offers equal gains on the task of temporallocalization.

PixelNet: Representation of the pixels, by the pixels, and for the  pixels

  We explore design principles for general pixel-level prediction problems,from low-level edge detection to mid-level surface normal estimation tohigh-level semantic segmentation. Convolutional predictors, such as thefully-convolutional network (FCN), have achieved remarkable success byexploiting the spatial redundancy of neighboring pixels through convolutionalprocessing. Though computationally efficient, we point out that such approachesare not statistically efficient during learning precisely because spatialredundancy limits the information learned from neighboring pixels. Wedemonstrate that stratified sampling of pixels allows one to (1) add diversityduring batch updates, speeding up learning; (2) explore complex nonlinearpredictors, improving accuracy; and (3) efficiently train state-of-the-artmodels tabula rasa (i.e., "from scratch") for diverse pixel-labeling tasks. Oursingle architecture produces state-of-the-art results for semantic segmentationon PASCAL-Context dataset, surface normal estimation on NYUDv2 depth dataset,and edge detection on BSDS.

ActionVLAD: Learning spatio-temporal aggregation for action  classification

  In this work, we introduce a new video representation for actionclassification that aggregates local convolutional features across the entirespatio-temporal extent of the video. We do so by integrating state-of-the-arttwo-stream networks with learnable spatio-temporal feature aggregation. Theresulting architecture is end-to-end trainable for whole-video classification.We investigate different strategies for pooling across space and time andcombining signals from the different streams. We find that: (i) it is importantto pool jointly across space and time, but (ii) appearance and motion streamsare best aggregated into their own separate representations. Finally, we showthat our representation outperforms the two-stream base architecture by a largemargin (13% relative) as well as out-performs other baselines with comparablebase architectures on HMDB51, UCF101, and Charades video classificationbenchmarks.

What's in a Question: Using Visual Questions as a Form of Supervision

  Collecting fully annotated image datasets is challenging and expensive. Manytypes of weak supervision have been explored: weak manual annotations, websearch results, temporal continuity, ambient sound and others. We focus on oneparticular unexplored mode: visual questions that are asked about images. Thekey observation that inspires our work is that the question itself providesuseful information about the image (even without the answer being available).For instance, the question "what is the breed of the dog?" informs the AI thatthe animal in the scene is a dog and that there is only one dog present. Wemake three contributions: (1) providing an extensive qualitative andquantitative analysis of the information contained in human visual questions,(2) proposing two simple but surprisingly effective modifications to thestandard visual question answering models that allow them to make use of weaksupervision in the form of unanswered questions associated with images and (3)demonstrating that a simple data augmentation strategy inspired by our insightsresults in a 7.1% improvement on the standard VQA benchmark.

