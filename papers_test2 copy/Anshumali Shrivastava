Optimal Densification for Fast and Accurate Minwise Hashing

  Minwise hashing is a fundamental and one of the most successful hashingalgorithm in the literature. Recent advances based on the idea ofdensification~\cite{Proc:OneHashLSH_ICML14,Proc:Shrivastava_UAI14} have shownthat it is possible to compute $k$ minwise hashes, of a vector with $d$nonzeros, in mere $(d + k)$ computations, a significant improvement over theclassical $O(dk)$. These advances have led to an algorithmic improvement in thequery complexity of traditional indexing algorithms based on minwise hashing.Unfortunately, the variance of the current densification techniques isunnecessarily high, which leads to significantly poor accuracy compared tovanilla minwise hashing, especially when the data is sparse. In this paper, weprovide a novel densification scheme which relies on carefully tailored2-universal hashes. We show that the proposed scheme is variance-optimal, andwithout losing the runtime efficiency, it is significantly more accurate thanexisting densification techniques. As a result, we obtain a significantlyefficient hashing scheme which has the same variance and collision probabilityas minwise hashing. Experimental evaluations on real sparse andhigh-dimensional datasets validate our claims. We believe that given thesignificant advantages, our method will replace minwise hashing implementationsin practice.

Graph Kernels via Functional Embedding

  We propose a representation of graph as a functional object derived from thepower iteration of the underlying adjacency matrix. The proposed functionalrepresentation is a graph invariant, i.e., the functional remains unchangedunder any reordering of the vertices. This property eliminates the difficultyof handling exponentially many isomorphic forms. Bhattacharyya kernelconstructed between these functionals significantly outperforms thestate-of-the-art graph kernels on 3 out of the 4 standard benchmark graphclassification datasets, demonstrating the superiority of our approach. Theproposed methodology is simple and runs in time linear in the number of edges,which makes our kernel more efficient and scalable compared to many widelyadopted graph kernels with running time cubic in the number of vertices.

Revisiting Winner Take All (WTA) Hashing for Sparse Datasets

  WTA (Winner Take All) hashing has been successfully applied in many largescale vision applications. This hashing scheme was tailored to take advantageof the comparative reasoning (or order based information), which showedsignificant accuracy improvements. In this paper, we identify a subtle issuewith WTA, which grows with the sparsity of the datasets. This issue limits thediscriminative power of WTA. We then propose a solution for this problem basedon the idea of Densification which provably fixes the issue. Our experimentsshow that Densified WTA Hashing outperforms Vanilla WTA both in imageclassification and retrieval tasks consistently and significantly.

Exact Weighted Minwise Hashing in Constant Time

  Weighted minwise hashing (WMH) is one of the fundamental subroutine, requiredby many celebrated approximation algorithms, commonly adopted in industrialpractice for large scale-search and learning. The resource bottleneck of thealgorithms is the computation of multiple (typically a few hundreds tothousands) independent hashes of the data. The fastest hashing algorithm is byIoffe \cite{Proc:Ioffe_ICDM10}, which requires one pass over the entire datavector, $O(d)$ ($d$ is the number of non-zeros), for computing one hash.However, the requirement of multiple hashes demands hundreds or thousandspasses over the data. This is very costly for modern massive dataset.  In this work, we break this expensive barrier and show an expected constantamortized time algorithm which computes $k$ independent and unbiased WMH intime $O(k)$ instead of $O(dk)$ required by Ioffe's method. Moreover, ourproposal only needs a few bits (5 - 9 bits) of storage per hash value comparedto around $64$ bits required by the state-of-art-methodologies. Experimentalevaluations, on real datasets, show that for computing 500 WMH, our proposalcan be 60000x faster than the Ioffe's method without losing any accuracy. Ourmethod is also around 100x faster than approximate heuristics capitalizing onthe efficient "densified" one permutation hashing schemes\cite{Proc:OneHashLSH_ICML14}. Given the simplicity of our approach and itssignificant advantages, we hope that it will replace existing implementationsin practice.

Arrays of (locality-sensitive) Count Estimators (ACE): High-Speed  Anomaly Detection via Cache Lookups

  Anomaly detection is one of the frequent and important subroutines deployedin large-scale data processing systems. Even being a well-studied topic,existing techniques for unsupervised anomaly detection require storingsignificant amounts of data, which is prohibitive from memory and latencyperspective. In the big-data world existing methods fail to address the new setof memory and latency constraints. In this paper, we propose ACE (Arrays of(locality-sensitive) Count Estimators) algorithm that can be 60x faster thanthe ELKI package~\cite{DBLP:conf/ssd/AchtertBKSZ09}, which has the fastestimplementation of the unsupervised anomaly detection algorithms. ACE algorithmrequires less than $4MB$ memory, to dynamically compress the full datainformation into a set of count arrays. These tiny $4MB$ arrays of counts aresufficient for unsupervised anomaly detection. At the core of the ACEalgorithm, there is a novel statistical estimator which is derived from thesampling view of Locality Sensitive Hashing(LSH). This view is significantlydifferent and efficient than the widely popular view of LSH for near-neighborsearch. We show the superiority of ACE algorithm over 11 popular baselines on 3benchmark datasets, including the KDD-Cup99 data which is the largest availablebenchmark comprising of more than half a million entries with ground truthanomaly labels.

Probabilistic Blocking with An Application to the Syrian Conflict

  Entity resolution seeks to merge databases as to remove duplicate entrieswhere unique identifiers are typically unknown. We review modern blockingapproaches for entity resolution, focusing on those based upon localitysensitive hashing (LSH). First, we introduce $k$-means locality sensitivehashing (KLSH), which is based upon the information retrieval literature andclusters similar records into blocks using a vector-space representation andprojections. Second, we introduce a subquadratic variant of LSH to theliterature, known as Densified One Permutation Hashing (DOPH). Third, wepropose a weighted variant of DOPH. We illustrate each method on an applicationto a subset of the ongoing Syrian conflict, giving a discussion of each method.

Hashing Algorithms for Large-Scale Learning

  In this paper, we first demonstrate that b-bit minwise hashing, whoseestimators are positive definite kernels, can be naturally integrated withlearning algorithms such as SVM and logistic regression. We adopt a simplescheme to transform the nonlinear (resemblance) kernel into linear (innerproduct) kernel; and hence large-scale problems can be solved extremelyefficiently. Our method provides a simple effective solution to large-scalelearning in massive and extremely high-dimensional datasets, especially whendata do not fit in memory.  We then compare b-bit minwise hashing with the Vowpal Wabbit (VW) algorithm(which is related the Count-Min (CM) sketch). Interestingly, VW has the samevariances as random projections. Our theoretical and empirical comparisonsillustrate that usually $b$-bit minwise hashing is significantly more accurate(at the same storage) than VW (and random projections) in binary data.Furthermore, $b$-bit minwise hashing can be combined with VW to achieve furtherimprovements in terms of training speed, especially when $b$ is large.

Training Logistic Regression and SVM on 200GB Data Using b-Bit Minwise  Hashing and Comparisons with Vowpal Wabbit (VW)

  We generated a dataset of 200 GB with 10^9 features, to test our recent b-bitminwise hashing algorithms for training very large-scale logistic regressionand SVM. The results confirm our prior work that, compared with the VW hashingalgorithm (which has the same variance as random projections), b-bit minwisehashing is substantially more accurate at the same storage. For example, withmerely 30 hashed values per data point, b-bit minwise hashing can achievesimilar accuracies as VW with 2^14 hashed values per data point.  We demonstrate that the preprocessing cost of b-bit minwise hashing isroughly on the same order of magnitude as the data loading time. Furthermore,by using a GPU, the preprocessing cost can be reduced to a small fraction ofthe data loading time.  Minwise hashing has been widely used in industry, at least in the context ofsearch. One reason for its popularity is that one can efficiently simulatepermutations by (e.g.,) universal hashing. In other words, there is no need tostore the permutation matrix. In this paper, we empirically verify thispractice, by demonstrating that even using the simplest 2-universal hashingdoes not degrade the learning performance.

Coding for Random Projections

  The method of random projections has become very popular for large-scaleapplications in statistical learning, information retrieval, bio-informaticsand other applications. Using a well-designed coding scheme for the projecteddata, which determines the number of bits needed for each projected value andhow to allocate these bits, can significantly improve the effectiveness of thealgorithm, in storage cost as well as computational speed. In this paper, westudy a number of simple coding schemes, focusing on the task of similarityestimation and on an application to training linear classifiers. We demonstratethat uniform quantization outperforms the standard existing influential method(Datar et. al. 2004). Indeed, we argue that in many cases coding with just asmall number of bits suffices. Furthermore, we also develop a non-uniform 2-bitcoding scheme that generally performs well in practice, as confirmed by ourexperiments on training linear support vector machines (SVM).

Coding for Random Projections and Approximate Near Neighbor Search

  This technical note compares two coding (quantization) schemes for randomprojections in the context of sub-linear time approximate near neighbor search.The first scheme is based on uniform quantization while the second schemeutilizes a uniform quantization plus a uniformly random offset (which has beenpopular in practice). The prior work compared the two schemes in the context ofsimilarity estimation and training linear classifiers, with the conclusion thatthe step of random offset is not necessary and may hurt the performance(depending on the similarity level). The task of near neighbor search isrelated to similarity estimation with importance distinctions and requires ownstudy. In this paper, we demonstrate that in the context of near neighborsearch, the step of random offset is not needed either and may hurt theperformance (sometimes significantly so, depending on the similarity and otherparameters).

Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search  (MIPS)

  We present the first provably sublinear time algorithm for approximate\emph{Maximum Inner Product Search} (MIPS). Our proposal is also the firsthashing algorithm for searching with (un-normalized) inner product as theunderlying similarity measure. Finding hashing schemes for MIPS was consideredhard. We formally show that the existing Locality Sensitive Hashing (LSH)framework is insufficient for solving MIPS, and then we extend the existing LSHframework to allow asymmetric hashing schemes. Our proposal is based on aninteresting mathematical phenomenon in which inner products, after independentasymmetric transformations, can be converted into the problem of approximatenear neighbor search. This key observation makes efficient sublinear hashingscheme for MIPS possible. In the extended asymmetric LSH (ALSH) framework, weprovide an explicit construction of provably fast hashing scheme for MIPS. Theproposed construction and the extended LSH framework could be of independenttheoretical interest. Our proposed algorithm is simple and easy to implement.We evaluate the method, for retrieving inner products, in the collaborativefiltering task of item recommendations on Netflix and Movielens datasets.

Improved Densification of One Permutation Hashing

  The existing work on densification of one permutation hashing reduces thequery processing cost of the $(K,L)$-parameterized Locality Sensitive Hashing(LSH) algorithm with minwise hashing, from $O(dKL)$ to merely $O(d + KL)$,where $d$ is the number of nonzeros of the data vector, $K$ is the number ofhashes in each hash table, and $L$ is the number of hash tables. While that isa substantial improvement, our analysis reveals that the existing densificationscheme is sub-optimal. In particular, there is no enough randomness in thatprocedure, which affects its accuracy on very sparse datasets.  In this paper, we provide a new densification procedure which is provablybetter than the existing scheme. This improvement is more significant for verysparse datasets which are common over the web. The improved technique has thesame cost of $O(d + KL)$ for query processing, thereby making it strictlypreferable over the existing procedure. Experimental evaluations on publicdatasets, in the task of hashing based near neighbor search, support ourtheoretical findings.

Improved Asymmetric Locality Sensitive Hashing (ALSH) for Maximum Inner  Product Search (MIPS)

  Recently it was shown that the problem of Maximum Inner Product Search (MIPS)is efficient and it admits provably sub-linear hashing algorithms. Asymmetrictransformations before hashing were the key in solving MIPS which was otherwisehard. In the prior work, the authors use asymmetric transformations whichconvert the problem of approximate MIPS into the problem of approximate nearneighbor search which can be efficiently solved using hashing. In this work, weprovide a different transformation which converts the problem of approximateMIPS into the problem of approximate cosine similarity search which can beefficiently solved using signed random projections. Theoretical analysis showthat the new scheme is significantly better than the original scheme for MIPS.Experimental evaluations strongly support the theoretical findings.

Asymmetric Minwise Hashing

  Minwise hashing (Minhash) is a widely popular indexing scheme in practice.Minhash is designed for estimating set resemblance and is known to besuboptimal in many applications where the desired measure is set overlap (i.e.,inner product between binary vectors) or set containment. Minhash has inherentbias towards smaller sets, which adversely affects its performance inapplications where such a penalization is not desirable. In this paper, wepropose asymmetric minwise hashing (MH-ALSH), to provide a solution to thisproblem. The new scheme utilizes asymmetric transformations to cancel the biasof traditional minhash towards smaller sets, making the final "collisionprobability" monotonic in the inner product. Our theoretical comparisons showthat for the task of retrieving with binary inner products asymmetric minhashis provably better than traditional minhash and other recently proposed hashingalgorithms for general inner products. Thus, we obtain an algorithmicimprovement over existing approaches in the literature. Experimentalevaluations on four publicly available high-dimensional datasets validate ourclaims and the proposed scheme outperforms, often significantly, other hashingalgorithms on the task of near neighbor retrieval with set containment. Ourproposal is simple and easy to implement in practice.

Blocking Methods Applied to Casualty Records from the Syrian Conflict

  Estimation of death counts and associated standard errors is of greatimportance in armed conflict such as the ongoing violence in Syria, as well ashistorical conflicts in Guatemala, Per\'u, Colombia, Timor Leste, and Kosovo.For example, statistical estimates of death counts were cited as importantevidence in the trial of General Efra\'in R\'ios Montt for acts of genocide inGuatemala. Estimation relies on both record linkage and multiple systemsestimation. A key first step in this process is identifying ways to partitionthe records such that they are computationally manageable. This step isreferred to as blocking and is a major challenge for the Syrian database sinceit is sparse in the number of duplicate records and feature poor in itsattributes. As a consequence, we propose locality sensitive hashing (LSH)methods to overcome these challenges. We demonstrate the computationalsuperiority and error rates of these methods by comparing our proposed approachwith others in the literature. We conclude with a discussion of many challengesof merging LSH with record linkage to achieve an estimate of the number ofuniquely documented deaths in the Syrian conflict.

Near-Isometric Binary Hashing for Large-scale Datasets

  We develop a scalable algorithm to learn binary hash codes for indexinglarge-scale datasets. Near-isometric binary hashing (NIBH) is a data-dependenthashing scheme that quantizes the output of a learned low-dimensional embeddingto obtain a binary hash code. In contrast to conventional hashing schemes,which typically rely on an $\ell_2$-norm (i.e., average distortion)minimization, NIBH is based on a $\ell_{\infty}$-norm (i.e., worst-casedistortion) minimization that provides several benefits, including superiordistance, ranking, and near-neighbor preservation performance. We develop apractical and efficient algorithm for NIBH based on column generation thatscales well to large datasets. A range of experimental evaluations demonstratethe superiority of NIBH over ten state-of-the-art binary hashing schemes.

A New Unbiased and Efficient Class of LSH-Based Samplers and Estimators  for Partition Function Computation in Log-Linear Models

  Log-linear models are arguably the most successful class of graphical modelsfor large-scale applications because of their simplicity and tractability.Learning and inference with these models require calculating the partitionfunction, which is a major bottleneck and intractable for large state spaces.Importance Sampling (IS) and MCMC-based approaches are lucrative. However, thecondition of having a "good" proposal distribution is often not satisfied inpractice.  In this paper, we add a new dimension to efficient estimation via sampling.We propose a new sampling scheme and an unbiased estimator that estimates thepartition function accurately in sub-linear time. Our samples are generated innear-constant time using locality sensitive hashing (LSH), and so arecorrelated and unnormalized. We demonstrate the effectiveness of our proposedapproach by comparing the accuracy and speed of estimating the partitionfunction against other state-of-the-art estimation techniques including IS andthe efficient variant of Gumbel-Max sampling. With our efficient samplingscheme, we accurately train real-world language models using only 1-2% ofcomputations.

MISSION: Ultra Large-Scale Feature Selection using Count-Sketches

  Feature selection is an important challenge in machine learning. It plays acrucial role in the explainability of machine-driven decisions that are rapidlypermeating throughout modern society. Unfortunately, the explosion in the sizeand dimensionality of real-world datasets poses a severe challenge to standardfeature selection algorithms. Today, it is not uncommon for datasets to havebillions of dimensions. At such scale, even storing the feature vector isimpossible, causing most existing feature selection methods to fail.Workarounds like feature hashing, a standard approach to large-scale machinelearning, helps with the computational feasibility, but at the cost of losingthe interpretability of features. In this paper, we present MISSION, a novelframework for ultra large-scale feature selection that performs stochasticgradient descent while maintaining an efficient representation of the featuresin memory using a Count-Sketch data structure. MISSION retains the simplicityof feature hashing without sacrificing the interpretability of the featureswhile using only O(log^2(p)) working memory. We demonstrate that MISSIONaccurately and efficiently performs feature selection on real-world,large-scale datasets with billions of dimensions.

Compressing Gradient Optimizers via Count-Sketches

  Many popular first-order optimization methods (e.g., Momentum, AdaGrad, Adam)accelerate the convergence rate of deep learning models. However, thesealgorithms require auxiliary parameters, which cost additional memoryproportional to the number of parameters in the model. The problem is becomingmore severe as deep learning models continue to grow larger in order to learnfrom complex, large-scale datasets. Our proposed solution is to maintain alinear sketch to compress the auxiliary variables. We demonstrate that ourtechnique has the same performance as the full-sized baseline, while usingsignificantly less space for the auxiliary variables. Theoretically, we provethat count-sketch optimization maintains the SGD convergence rate, whilegracefully reducing memory usage for large-models. On the large-scale 1-BillionWord dataset, we save 25% of the memory used during training (8.6 GB instead of11.7 GB) by compressing the Adam optimizer in the Embedding and Softmax layerswith negligible accuracy and performance loss. For an Amazon extremeclassification task with over 49.5 million classes, we also reduce the trainingtime by 38%, by increasing the mini-batch size 3.5x using our count-sketchoptimizer.

RACE: Sub-Linear Memory Sketches for Approximate Near-Neighbor Search on  Streaming Data

  We present the first sublinear memory sketch which can be queried to find the$v$ nearest neighbors in a dataset. Our online sketching algorithm can compressan $N$-element dataset to a sketch of size $O(N^b \log^3{N})$ in $O(N^{b+1}\log^3{N})$ time, where $b < 1$ when the query satisfies a data-dependentnear-neighbor stability condition.  We achieve data-dependent sublinear space by combining recent advances inlocality sensitive hashing (LSH)-based estimators with compressed sensing. Ourresults shed new light on the memory-accuracy tradeoff for near-neighborsearch. The techniques presented reveal a deep connection between thefundamental compressed sensing (or heavy hitters) recovery problem andnear-neighbor search, leading to new insight for geometric search problems andimplications for sketching algorithms.

Using Local Experiences for Global Motion Planning

  Sampling-based planners are effective in many real-world applications such asrobotics manipulation, navigation, and even protein modeling. However, it isoften challenging to generate a collision-free path in environments where keyareas are hard to sample. In the absence of any prior information,sampling-based planners are forced to explore uniformly or heuristically, whichcan lead to degraded performance. One way to improve performance is to useprior knowledge of environments to adapt the sampling strategy to the problemat hand. In this work, we decompose the workspace into local primitives,memorizing local experiences by these primitives in the form of local samplers,and store them in a database. We synthesize an efficient global sampler byretrieving local experiences relevant to the given situation. Our methodtransfers knowledge effectively between diverse environments that share localprimitives and speeds up the performance dramatically. Our results show, interms of solution time, an improvement of multiple orders of magnitude in twotraditionally challenging high-dimensional problems compared tostate-of-the-art approaches.

b-Bit Minwise Hashing in Practice: Large-Scale Batch and Online Learning  and Using GPUs for Fast Preprocessing with Simple Hash Functions

  In this paper, we study several critical issues which must be tackled beforeone can apply b-bit minwise hashing to the volumes of data often usedindustrial applications, especially in the context of search.  1. (b-bit) Minwise hashing requires an expensive preprocessing step thatcomputes k (e.g., 500) minimal values after applying the correspondingpermutations for each data vector. We developed a parallelization scheme usingGPUs and observed that the preprocessing time can be reduced by a factor of20-80 and becomes substantially smaller than the data loading time.  2. One major advantage of b-bit minwise hashing is that it can substantiallyreduce the amount of memory required for batch learning. However, as onlinealgorithms become increasingly popular for large-scale learning in the contextof search, it is not clear if b-bit minwise yields significant improvements forthem. This paper demonstrates that $b$-bit minwise hashing provides aneffective data size/dimension reduction scheme and hence it can dramaticallyreduce the data loading time for each epoch of the online training process.This is significant because online learning often requires many (e.g., 10 to100) epochs to reach a sufficient accuracy.  3. Another critical issue is that for very large data sets it becomesimpossible to store a (fully) random permutation matrix, due to its spacerequirements. Our paper is the first study to demonstrate that $b$-bit minwisehashing implemented using simple hash functions, e.g., the 2-universal (2U) and4-universal (4U) hash families, can produce very similar learning results asusing fully random permutations. Experiments on datasets of up to 200GB arepresented.

A New Space for Comparing Graphs

  Finding a new mathematical representations for graph, which allows directcomparison between different graph structures, is an open-ended researchdirection. Having such a representation is the first prerequisite for a varietyof machine learning algorithms like classification, clustering, etc., overgraph datasets. In this paper, we propose a symmetric positive semidefinitematrix with the $(i,j)$-{th} entry equal to the covariance between normalizedvectors $A^ie$ and $A^je$ ($e$ being vector of all ones) as a representationfor graph with adjacency matrix $A$. We show that the proposed matrixrepresentation encodes the spectrum of the underlying adjacency matrix and italso contains information about the counts of small sub-structures present inthe graph such as triangles and small paths. In addition, we show that thismatrix is a \emph{"graph invariant"}. All these properties make the proposedmatrix a suitable object for representing graphs.  The representation, being a covariance matrix in a fixed dimensional metricspace, gives a mathematical embedding for graphs. This naturally leads to ameasure of similarity on graph objects. We define similarity between two givengraphs as a Bhattacharya similarity measure between their correspondingcovariance matrix representations. As shown in our experimental study on thetask of social network classification, such a similarity measure outperformsother widely used state-of-the-art methodologies. Our proposed method is alsocomputationally efficient. The computation of both the matrix representationand the similarity value can be performed in operations linear in the number ofedges. This makes our method scalable in practice.  We believe our theoretical and empirical results provide evidence forstudying truncated power iterations, of the adjacency matrix, to characterizesocial networks.

In Defense of MinHash Over SimHash

  MinHash and SimHash are the two widely adopted Locality Sensitive Hashing(LSH) algorithms for large-scale data processing applications. Deciding whichLSH to use for a particular problem at hand is an important question, which hasno clear answer in the existing literature. In this study, we provide atheoretical answer (validated by experiments) that MinHash virtually alwaysoutperforms SimHash when the data are binary, as common in practice such assearch.  The collision probability of MinHash is a function of resemblance similarity($\mathcal{R}$), while the collision probability of SimHash is a function ofcosine similarity ($\mathcal{S}$). To provide a common basis for comparison, weevaluate retrieval results in terms of $\mathcal{S}$ for both MinHash andSimHash. This evaluation is valid as we can prove that MinHash is a valid LSHwith respect to $\mathcal{S}$, by using a general inequality $\mathcal{S}^2\leq\mathcal{R}\leq \frac{\mathcal{S}}{2-\mathcal{S}}$. Our worst case analysis canshow that MinHash significantly outperforms SimHash in high similarity region.  Interestingly, our intensive experiments reveal that MinHash is alsosubstantially better than SimHash even in datasets where most of the datapoints are not too similar to each other. This is partly because, in practicaldata, often $\mathcal{R}\geq \frac{\mathcal{S}}{z-\mathcal{S}}$ holds where $z$is only slightly larger than 2 (e.g., $z\leq 2.1$). Our restricted worst caseanalysis by assuming $\frac{\mathcal{S}}{z-\mathcal{S}}\leq \mathcal{R}\leq\frac{\mathcal{S}}{2-\mathcal{S}}$ shows that MinHash indeed significantlyoutperforms SimHash even in low similarity region.  We believe the results in this paper will provide valuable guidelines forsearch in practice, especially when the data are sparse.

2-Bit Random Projections, NonLinear Estimators, and Approximate Near  Neighbor Search

  The method of random projections has become a standard tool for machinelearning, data mining, and search with massive data at Web scale. The effectiveuse of random projections requires efficient coding schemes for quantizing(real-valued) projected data into integers. In this paper, we focus on a simple2-bit coding scheme. In particular, we develop accurate nonlinear estimators ofdata similarity based on the 2-bit strategy. This work will have importantpractical applications. For example, in the task of near neighbor search, acrucial step (often called re-ranking) is to compute or estimate datasimilarities once a set of candidate data points have been identified by hashtable techniques. This re-ranking step can take advantage of the proposedcoding scheme and estimator.  As a related task, in this paper, we also study a simple uniform quantizationscheme for the purpose of building hash tables with projected data. Ouranalysis shows that typically only a small number of bits are needed. Forexample, when the target similarity level is high, 2 or 3 bits might besufficient. When the target similarity level is not so high, it is preferableto use only 1 or 2 bits. Therefore, a 2-bit scheme appears to be overall a goodchoice for the task of sublinear time approximate near neighbor search via hashtables.  Combining these results, we conclude that 2-bit random projections should berecommended for approximate near neighbor search and similarity estimation.Extensive experimental results are provided.

Scalable and Sustainable Deep Learning via Randomized Hashing

  Current deep learning architectures are growing larger in order to learn fromcomplex datasets. These architectures require giant matrix multiplicationoperations to train millions of parameters. Conversely, there is anothergrowing trend to bring deep learning to low-power, embedded devices. The matrixoperations, associated with both training and testing of deep networks, arevery expensive from a computational and energy standpoint. We present a novelhashing based technique to drastically reduce the amount of computation neededto train and test deep networks. Our approach combines recent ideas fromadaptive dropouts and randomized hashing for maximum inner product search toselect the nodes with the highest activation efficiently. Our new algorithm fordeep learning reduces the overall computational cost of forward andback-propagation by operating on significantly fewer (sparse) nodes. As aconsequence, our algorithm uses only 5% of the total multiplications, whilekeeping on average within 1% of the accuracy of the original model. A uniqueproperty of the proposed hashing based back-propagation is that the updates arealways sparse. Due to the sparse gradient updates, our algorithm is ideallysuited for asynchronous and parallel training leading to near linear speedupwith increasing number of cores. We demonstrate the scalability andsustainability (energy efficiency) of our proposed algorithm via rigorousexperimental evaluations on several real datasets.

SSH (Sketch, Shingle, & Hash) for Indexing Massive-Scale Time Series

  Similarity search on time series is a frequent operation in large-scaledata-driven applications. Sophisticated similarity measures are standard fortime series matching, as they are usually misaligned. Dynamic Time Warping orDTW is the most widely used similarity measure for time series because itcombines alignment and matching at the same time. However, the alignment makesDTW slow. To speed up the expensive similarity search with DTW, branch andbound based pruning strategies are adopted. However, branch and bound basedpruning are only useful for very short queries (low dimensional time series),and the bounds are quite weak for longer queries. Due to the loose boundsbranch and bound pruning strategy boils down to a brute-force search.  To circumvent this issue, we design SSH (Sketch, Shingle, & Hashing), anefficient and approximate hashing scheme which is much faster than thestate-of-the-art branch and bound searching technique: the UCR suite. SSH usesa novel combination of sketching, shingling and hashing techniques to produce(probabilistic) indexes which align (near perfectly) with DTW similaritymeasure. The generated indexes are then used to create hash buckets forsub-linear search. Our results show that SSH is very effective for longer timesequence and prunes around 95% candidates, leading to the massive speedup insearch with DTW. Empirical results on two large-scale benchmark time seriesdata show that our proposed method can be around 20 times faster than thestate-of-the-art package (UCR suite) without any significant loss in accuracy.

Sub-Linear Privacy-Preserving Near-Neighbor Search with Untrusted Server  on Large-Scale Datasets

  In Near-Neighbor Search (NNS), a new client queries a database (held by aserver) for the most similar data (near-neighbors) given a certain similaritymetric. The Privacy-Preserving variant (PP-NNS) requires that neither servernor the client shall learn information about the other party's data except whatcan be inferred from the outcome of NNS. The overwhelming growth in the size ofcurrent datasets and the lack of a truly secure server in the online worldrender the existing solutions impractical; either due to their highcomputational requirements or non-realistic assumptions which potentiallycompromise privacy. PP-NNS having query time {\it sub-linear} in the size ofthe database has been suggested as an open research direction by Li et al.(CCSW'15). In this paper, we provide the first such algorithm, called SecureLocality Sensitive Indexing (SLSI) which has a sub-linear query time and theability to handle honest-but-curious parties. At the heart of our proposal liesa secure binary embedding scheme generated from a novel probabilistictransformation over locality sensitive hashing family. We provide informationtheoretic bound for the privacy guarantees and support our theoretical claimsusing substantial empirical evidence on real-world datasets.

Accelerating Dependency Graph Learning from Heterogeneous Categorical  Event Streams via Knowledge Transfer

  Dependency graph, as a heterogeneous graph representing the intrinsicrelationships between different pairs of system entities, is essential to manydata analysis applications, such as root cause diagnosis, intrusion detection,etc. Given a well-trained dependency graph from a source domain and an immaturedependency graph from a target domain, how can we extract the entity anddependency knowledge from the source to enhance the target? One way is todirectly apply a mature dependency graph learned from a source domain to thetarget domain. But due to the domain variety problem, directly using the sourcedependency graph often can not achieve good performance. Traditional transferlearning methods mainly focus on numerical data and are not applicable.  In this paper, we propose ACRET, a knowledge transfer based model foraccelerating dependency graph learning from heterogeneous categorical eventstreams. In particular, we first propose an entity estimation model to filterout irrelevant entities from the source domain based on entity embedding andmanifold learning. Only the entities with statistically high correlations aretransferred to the target domain. On the surviving entities, we propose adependency construction model for constructing the unbiased dependencyrelationships by solving a two-constraint optimization problem. Theexperimental results on synthetic and real-world datasets demonstrate theeffectiveness and efficiency of ACRET. We also apply ACRET to a real enterprisesecurity system for intrusion detection. Our method is able to achieve superiordetection performance at least 20 days lead lag time in advance with more than70% accuracy.

FLASH: Randomized Algorithms Accelerated over CPU-GPU for Ultra-High  Dimensional Similarity Search

  We present FLASH (\textbf{F}ast \textbf{L}SH \textbf{A}lgorithm for\textbf{S}imilarity search accelerated with \textbf{H}PC), a similarity searchsystem for ultra-high dimensional datasets on a single machine, that does notrequire similarity computations and is tailored for high-performance computingplatforms. By leveraging a LSH style randomized indexing procedure andcombining it with several principled techniques, such as reservoir sampling,recent advances in one-pass minwise hashing, and count based estimations, wereduce the computational and parallelization costs of similarity search, whileretaining sound theoretical guarantees.  We evaluate FLASH on several real, high-dimensional datasets from differentdomains, including text, malicious URL, click-through prediction, socialnetworks, etc. Our experiments shed new light on the difficulties associatedwith datasets having several million dimensions. Current state-of-the-artimplementations either fail on the presented scale or are orders of magnitudeslower than FLASH. FLASH is capable of computing an approximate k-NN graph,from scratch, over the full webspam dataset (1.3 billion nonzeros) in less than10 seconds. Computing a full k-NN graph in less than 10 seconds on the webspamdataset, using brute-force ($n^2D$), will require at least 20 teraflops. Weprovide CPU and GPU implementations of FLASH for replicability of our results.

Unique Entity Estimation with Application to the Syrian Conflict

  Entity resolution identifies and removes duplicate entities in large, noisydatabases and has grown in both usage and new developments as a result ofincreased data availability. Nevertheless, entity resolution has tradeoffsregarding assumptions of the data generation process, error rates, andcomputational scalability that make it a difficult task for real applications.In this paper, we focus on a related problem of unique entity estimation, whichis the task of estimating the unique number of entities and associated standarderrors in a data set with duplicate entities. Unique entity estimation sharesmany fundamental challenges of entity resolution, namely, that thecomputational cost of all-to-all entity comparisons is intractable for largedatabases. To circumvent this computational barrier, we propose an efficient(near-linear time) estimation algorithm based on locality sensitive hashing.Our estimator, under realistic assumptions, is unbiased and has provably lowvariance compared to existing random sampling based approaches. In addition, weempirically show its superiority over the state-of-the-art estimators on threereal applications. The motivation for our work is to derive an accurateestimate of the documented, identifiable deaths in the ongoing Syrian conflict.Our methodology, when applied to the Syrian data set, provides an estimate of$191,874 \pm 1772$ documented, identifiable deaths, which is very close to theHuman Rights Data Analysis Group (HRDAG) estimate of 191,369. Our work providesan example of challenges and efforts involved in solving a real, noisychallenging problem where modeling assumptions may not hold.

Scaling-up Split-Merge MCMC with Locality Sensitive Sampling (LSS)

  Split-Merge MCMC (Monte Carlo Markov Chain) is one of the essential andpopular variants of MCMC for problems when an MCMC state consists of an unknownnumber of components. It is well known that state-of-the-art methods forsplit-merge MCMC do not scale well. Strategies for rapid mixing requires smartand informative proposals to reduce the rejection rate. However, all knownsmart proposals involve expensive operations to suggest informativetransitions. As a result, the cost of each iteration is prohibitive for massivescale datasets. It is further known that uninformative but computationallyefficient proposals, such as random split-merge, leads to extremely slowconvergence. This tradeoff between mixing time and per update cost seems hardto get around.  In this paper, we show a sweet spot. We leverage some unique properties ofweighted MinHash, which is a popular LSH, to design a novel class ofsplit-merge proposals which are significantly more informative than randomsampling but at the same time efficient to compute. Overall, we obtain asuperior tradeoff between convergence and per update cost. As a directconsequence, our proposals are around 6X faster than the state-of-the-artsampling methods on two large real datasets KDDCUP and PubMed with severalmillions of entities and thousands of clusters.

Want to bring a community together? Create more sub-communities

  Understanding overlapping community structures is crucial for networkanalysis and prediction. AGM (Affiliation Graph Model) is one of the favoritemodels for explaining the densely overlapped community structures. In thispaper, we thoroughly re-investigate the assumptions made by the AGM model onreal datasets. We find that the AGM model is not sufficient to explain severalempirical behaviors observed in popular real-world networks. To our surprise,all our experimental results can be explained by a parameter-free hypothesis,leading to more straightforward modeling than AGM which has many parameters.Based on these findings, we propose a parameter-free Jaccard-based AffiliationGraph (JAG) model which models the probability of edge as a network specificconstant times the Jaccard similarity between community sets associated withthe individuals. Our modeling is significantly simpler than AGM, and iteliminates the need of associating a parameter, the probability value, witheach community. Furthermore, JAG model naturally explains why (and in factwhen) overlapping communities are densely connected. Based on theseobservations, we propose a new community-driven friendship formation process,which mathematically recovers the JAG model. JAG is the first model that pointstowards a direct causal relationship between tight connections in the givencommunity with the number of overlapping communities inside it. Thus, \emph{themost effective way to bring a community together is to form moresub-communities within it.} The community detection algorithm based on ourmodeling demonstrates a significantly simple algorithm with state-of-the-artaccuracy on six real-world network datasets compared to the existing linkanalysis based methods.

Extreme Classification in Log Memory

  We present Merged-Averaged Classifiers via Hashing (MACH) forK-classification with ultra-large values of K. Compared to traditionalone-vs-all classifiers that require O(Kd) memory and inference cost, MACH onlyneed O(d log K) (d is dimensionality )memory while only requiring O(K log K + dlog K) operation for inference. MACH is a generic K-classification algorithm,with provably theoretical guarantees, which requires O(log K) memory withoutany assumption on the relationship between classes. MACH uses universal hashingto reduce classification with a large number of classes to few independentclassification tasks with small (constant) number of classes. We providetheoretical quantification of discriminability-memory tradeoff. With MACH wecan train ODP dataset with 100,000 classes and 400,000 features on a singleTitan X GPU, with the classification accuracy of 19.28%, which is thebest-reported accuracy on this dataset. Before this work, the best performingbaseline is a one-vs-all classifier that requires 40 billion parameters (160 GBmodel size) and achieves 9% accuracy. In contrast, MACH can achieve 9% accuracywith 480x reduction in the model size (of mere 0.3GB). With MACH, we alsodemonstrate complete training of fine-grained imagenet dataset (compressed size104GB), with 21,000 classes, on a single GPU. To the best of our knowledge,this is the first work to demonstrate complete training of these extreme-classdatasets on a single Titan X.

Better accuracy with quantified privacy: representations learned via  reconstructive adversarial network

  The remarkable success of machine learning, especially deep learning, hasproduced a variety of cloud-based services for mobile users. Such servicesrequire an end user to send data to the service provider, which presents aserious challenge to end-user privacy. To address this concern, prior workseither add noise to the data or send features extracted from the raw data. Theystruggle to balance between the utility and privacy because added noise reducesutility and raw data can be reconstructed from extracted features. This workrepresents a methodical departure from prior works: we balance between ameasure of privacy and another of utility by leveraging adversarial learning tofind a sweeter tradeoff. We design an encoder that optimizes against thereconstruction error (a measure of privacy), adversarially by a Decoder, andthe inference accuracy (a measure of utility) by a Classifier. The result isRAN, a novel deep model with a new training algorithm that automaticallyextracts features for classification that are both private and useful. It turnsout that adversarially forcing the extracted features to only conveys theintended information required by classification leads to an implicitregularization leading to better classification accuracy than the originalmodel which completely ignores privacy. Thus, we achieve better privacy withbetter utility, a surprising possibility in machine learning! We conductedextensive experiments on five popular datasets over four training schemes, anddemonstrate the superiority of RAN compared with existing alternatives.

SLIDE : In Defense of Smart Algorithms over Hardware Acceleration for  Large-Scale Deep Learning Systems

  Deep Learning (DL) algorithms are the central focus of modern machinelearning systems. As data volumes keep growing, it has become customary totrain large neural networks with hundreds of millions of parameters with enoughcapacity to memorize these volumes and obtain state-of-the-art accuracy. To getaround the costly computations associated with large models and data, thecommunity is increasingly investing in specialized hardware for model training.However, with the end of Moore's law, there is a limit to such scaling. Theprogress on the algorithmic front has failed to demonstrate a direct advantageover powerful hardware such as NVIDIA-V100 GPUs. This paper provides anexception. We propose SLIDE (Sub-LInear Deep learning Engine) that uniquelyblends smart randomized algorithms, which drastically reduce the computationduring both training and inference, with simple multi-core parallelism on amodest CPU. SLIDE is an auspicious illustration of the power of smartrandomized algorithms over CPUs in outperforming the best available GPU with anoptimized implementation. Our evaluations on large industry-scale datasets,with some large fully connected architectures, show that training with SLIDE ona 44 core CPU is more than 2.7 times (2 hours vs. 5.5 hours) faster than thesame network trained using Tensorflow on Tesla V100 at any given accuracylevel. We provide codes and benchmark scripts for reproducibility.

