Decentralized Erasure Codes for Distributed Networked Storage

  We consider the problem of constructing an erasure code for storage over anetwork when the data sources are distributed. Specifically, we assume thatthere are n storage nodes with limited memory and k<n sources generating thedata. We want a data collector, who can appear anywhere in the network, toquery any k storage nodes and be able to retrieve the data. We introduceDecentralized Erasure Codes, which are linear codes with a specific randomizedstructure inspired by network coding on random bipartite graphs. We show thatdecentralized erasure codes are optimally sparse, and lead to reducedcommunication, storage and computation cost over random linear coding.

Probabilistic Analysis of Linear Programming Decoding

  We initiate the probabilistic analysis of linear programming (LP) decoding oflow-density parity-check (LDPC) codes. Specifically, we show that for a randomLDPC code ensemble, the linear programming decoder of Feldman et al. succeedsin correcting a constant fraction of errors with high probability. The fractionof correctable errors guaranteed by our analysis surpasses previousnon-asymptotic results for LDPC codes, and in particular exceeds the bestprevious finite-length result on LP decoding by a factor greater than ten. Thisimprovement stems in part from our analysis of probabilistic bit-flippingchannels, as opposed to adversarial channels. At the core of our analysis is anovel combinatorial characterization of LP decoding success, based on thenotion of a generalized matching. An interesting by-product of our analysis isto establish the existence of ``probabilistic expansion'' in random bipartitegraphs, in which one requires only that almost every (as opposed to every) setof a certain size expands, for sets much larger than in the classicalworst-case setting.

Geographic Gossip: Efficient Averaging for Sensor Networks

  Gossip algorithms for distributed computation are attractive due to theirsimplicity, distributed nature, and robustness in noisy and uncertainenvironments. However, using standard gossip algorithms can lead to asignificant waste in energy by repeatedly recirculating redundant information.For realistic sensor network model topologies like grids and random geometricgraphs, the inefficiency of gossip schemes is related to the slow mixing timesof random walks on the communication graph. We propose and analyze analternative gossiping scheme that exploits geographic information. By utilizinggeographic routing combined with a simple resampling method, we demonstratesubstantial gains over previously proposed gossip protocols. For regular graphssuch as the ring or grid, our algorithm improves standard gossip by factors of$n$ and $\sqrt{n}$ respectively. For the more challenging case of randomgeometric graphs, our algorithm computes the true average to accuracy$\epsilon$ using $O(\frac{n^{1.5}}{\sqrt{\log n}} \log \epsilon^{-1})$ radiotransmissions, which yields a $\sqrt{\frac{n}{\log n}}$ factor improvement overstandard gossip algorithms. We illustrate these theoretical results withexperimental comparisons between our algorithm and standard methods as appliedto various classes of random fields.

A Survey on Network Codes for Distributed Storage

  Distributed storage systems often introduce redundancy to increasereliability. When coding is used, the repair problem arises: if a node storingencoded information fails, in order to maintain the same level of reliabilitywe need to create encoded information at a new node. This amounts to a partialrecovery of the code, whereas conventional erasure coding focuses on thecomplete recovery of the information from a subset of encoded packets. Theconsideration of the repair network traffic gives rise to new designchallenges. Recently, network coding techniques have been instrumental inaddressing these challenges, establishing that maintenance bandwidth can bereduced by orders of magnitude compared to standard erasure codes. This paperprovides an overview of the research results on this topic.

Reweighted LP Decoding for LDPC Codes

  We introduce a novel algorithm for decoding binary linear codes by linearprogramming. We build on the LP decoding algorithm of Feldman et al. andintroduce a post-processing step that solves a second linear program thatreweights the objective function based on the outcome of the original LPdecoder output. Our analysis shows that for some LDPC ensembles we can improvethe provable threshold guarantees compared to standard LP decoding. We alsoshow significant empirical performance gains for the reweighted LP decodingalgorithm with very small additional computational complexity.

Femtocaching and Device-to-Device Collaboration: A New Architecture for  Wireless Video Distribution

  We present a new architecture to handle the ongoing explosive increase in thedemand for video content in wireless networks. It is based on distributedcaching of the content in femto-basestations with small or non-existingbackhaul capacity but with considerable storage space, called helper nodes. Wealso consider using the mobile terminals themselves as caching helpers, whichcan distribute video through device-to-device communications. This approachallows an improvement in the video throughput without deployment of anyadditional infrastructure. The new architecture can improve video throughput byone to two orders-of-magnitude.

Local Graph Coloring and Index Coding

  We present a novel upper bound for the optimal index coding rate. Our bounduses a graph theoretic quantity called the local chromatic number. We show howa good local coloring can be used to create a good index code. The localcoloring is used as an alignment guide to assign index coding vectors from ageneral position MDS code. We further show that a natural LP relaxation yieldsan even stronger index code. Our bounds provably outperform the state of theart on index coding but at most by a constant factor.

Graph Theory versus Minimum Rank for Index Coding

  We obtain novel index coding schemes and show that they provably outperformall previously known graph theoretic bounds proposed so far. Further, weestablish a rather strong negative result: all known graph theoretic bounds arewithin a logarithmic factor from the chromatic number. This is in strikingcontrast to minrank since prior work has shown that it can outperform thechromatic number by a polynomial factor in some cases. The conclusion is thatall known graph theoretic bounds are not much stronger than the chromaticnumber.

Gradient Coding

  We propose a novel coding theoretic framework for mitigating stragglers indistributed learning. We show how carefully replicating data blocks and codingacross gradients can provide tolerance to failures and stragglers forSynchronous Gradient Descent. We implement our schemes in python (using MPI) torun on Amazon EC2, and show how we compare against baseline approaches inrunning time and generalization error.

On Approximation Guarantees for Greedy Low Rank Optimization

  We provide new approximation guarantees for greedy low rank matrix estimationunder standard assumptions of restricted strong convexity and smoothness. Ournovel analysis also uncovers previously unknown connections between the lowrank estimation and combinatorial optimization, so much so that our bounds arereminiscent of corresponding approximation bounds in submodular maximization.Additionally, we also provide statistical recovery guarantees. Finally, wepresent empirical comparison of greedy estimation with established baselines ontwo important real-world problems.

From Dumb Wireless Sensors to Smart Networks using Network Coding

  The vision of wireless sensor networks is one of a smart collection of tiny,dumb devices. These motes may be individually cheap, unintelligent, imprecise,and unreliable. Yet they are able to derive strength from numbers, renderingthe whole to be strong, reliable and robust. Our approach is to adopt adistributed and randomized mindset and rely on in network processing andnetwork coding. Our general abstraction is that nodes should act only locallyand independently, and the desired global behavior should arise as a collectiveproperty of the network. We summarize our work and present how these ideas canbe applied for communication and storage in sensor networks.

Order-Optimal Consensus through Randomized Path Averaging

  Gossip algorithms have recently received significant attention, mainlybecause they constitute simple and robust message-passing schemes fordistributed information processing over networks. However for many topologiesthat are realistic for wireless ad-hoc and sensor networks (like grids andrandom geometric graphs), the standard nearest-neighbor gossip converges asslowly as flooding ($O(n^2)$ messages).  A recently proposed algorithm called geographic gossip improves gossipefficiency by a $\sqrt{n}$ factor, by exploiting geographic information toenable multi-hop long distance communications. In this paper we prove that avariation of geographic gossip that averages along routed paths, improvesefficiency by an additional $\sqrt{n}$ factor and is order optimal ($O(n)$messages) for grids and random geometric graphs.  We develop a general technique (travel agency method) based on Markov chainmixing time inequalities, which can give bounds on the performance ofrandomized message-passing algorithms operating over various graph topologies.

Lower Bounds on the Rate-Distortion Function of LDGM Codes

  A recent line of work has focused on the use of low-density generator matrix(LDGM) codes for lossy source coding. In this paper, wedevelop a generictechnique for deriving lower bounds on the rate-distortion functions of binarylinear codes, with particular interest on the effect of bounded degrees. Theunderlying ideas can be viewing as the source coding analog of the classicalresult of Gallager, providing bounds for channel coding over the binarysymmetric channel using bounded degree LDPC codes. We illustrate this methodfor different random ensembles of LDGM codes, including the check-regularensemble and bit-check-regular ensembles, by deriving explicit lower bounds ontheir rate-distortion performance as a function of the degrees.

Exact MAP Inference by Avoiding Fractional Vertices

  Given a graphical model, one essential problem is MAP inference, that is,finding the most likely configuration of states according to the model.Although this problem is NP-hard, large instances can be solved in practice. Amajor open question is to explain why this is true. We give a natural conditionunder which we can provably perform MAP inference in polynomial time. Werequire that the number of fractional vertices in the LP relaxation exceedingthe optimal solution is bounded by a polynomial in the problem size. Thisresolves an open question by Dimakis, Gohari, and Wainwright. In contrast, forgeneral LP relaxations of integer programs, known techniques can only handle aconstant number of fractional vertices whose value exceeds the optimalsolution. We experimentally verify this condition and demonstrate how efficientvarious integer programming methods are at removing fractional solutions.

Network Coding for Distributed Storage Systems

  Peer-to-peer distributed storage systems provide reliable access to datathrough redundancy spread over nodes across the Internet. A key goal is tominimize the amount of bandwidth used to maintain that redundancy. Storing afile using an erasure code, in fragments spread across nodes, promises torequire less redundancy and hence less maintenance bandwidth than simplereplication to provide the same level of reliability. However, since fragmentsmust be periodically replaced as nodes fail, a key question is how to generatea new fragment in a distributed way while transferring as little data aspossible across the network.  In this paper, we introduce a general technique to analyze storagearchitectures that combine any form of coding and replication, as well aspresenting two new schemes for maintaining redundancy using erasure codes.First, we show how to optimally generate MDS fragments directly from existingfragments in the system. Second, we introduce a new scheme called RegeneratingCodes which use slightly larger fragments than MDS but have lower overallbandwidth use. We also show through simulation that in realistic environments,Regenerating Codes can reduce maintenance bandwidth use by 25 percent or morecompared with the best previous design--a hybrid of replication and erasurecodes--while simplifying system architecture.

Guessing Facets: Polytope Structure and Improved LP Decoding

  In this paper we investigate the structure of the fundamental polytope usedin the Linear Programming decoding introduced by Feldman, Karger andWainwright. We begin by showing that for expander codes, every fractionalpseudocodeword always has at least a constant fraction of non-integral bits. Wethen prove that for expander codes, the active set of any fractionalpseudocodeword is smaller by a constant fraction than the active set of anycodeword. We further exploit these geometrical properties to devise an improveddecoding algorithm with the same complexity order as LP decoding that provablyperforms better, for any blocklength. It proceeds by guessing facets of thepolytope, and then resolving the linear program on these facets. While the LPdecoder succeeds only if the ML codeword has the highest likelihood over allpseudocodewords, we prove that the proposed algorithm, when applied to suitableexpander codes, succeeds unless there exist a certain number ofpseudocodewords, all adjacent to the ML codeword on the LP decoding polytope,and with higher likelihood than the ML codeword. We then describe an extendedalgorithm, still with polynomial complexity, that succeeds as long as there areat most polynomially many pseudocodewords above the ML codeword.

Network Coding for Distributed Storage Systems

  Distributed storage systems provide reliable access to data throughredundancy spread over individually unreliable nodes. Application scenariosinclude data centers, peer-to-peer storage systems, and storage in wirelessnetworks. Storing data using an erasure code, in fragments spread across nodes,requires less redundancy than simple replication for the same level ofreliability. However, since fragments must be periodically replaced as nodesfail, a key question is how to generate encoded fragments in a distributed waywhile transferring as little data as possible across the network.  For an erasure coded system, a common practice to repair from a node failureis for a new node to download subsets of data stored at a number of survivingnodes, reconstruct a lost coded block using the downloaded data, and store itat the new node. We show that this procedure is sub-optimal. We introduce thenotion of regenerating codes, which allow a new node to download\emph{functions} of the stored data from the surviving nodes. We show thatregenerating codes can significantly reduce the repair bandwidth. Further, weshow that there is a fundamental tradeoff between storage and repair bandwidthwhich we theoretically characterize using flow arguments on an appropriatelyconstructed graph. By invoking constructive results in network coding, weintroduce regenerating codes that can achieve any point in this optimaltradeoff.

Near-Optimal Detection in MIMO Systems using Gibbs Sampling

  In this paper we study a Markov Chain Monte Carlo (MCMC) Gibbs sampler forsolving the integer least-squares problem. In digital communication the problemis equivalent to performing Maximum Likelihood (ML) detection in Multiple-InputMultiple-Output (MIMO) systems. While the use of MCMC methods for such problemshas already been proposed, our method is novel in that we optimize the"temperature" parameter so that in steady state, i.e. after the Markov chainhas mixed, there is only polynomially (rather than exponentially) smallprobability of encountering the optimal solution. More precisely, we obtain thelargest value of the temperature parameter for this to occur, since the higherthe temperature, the faster the mixing. This is in contrast to simulatedannealing techniques where, rather than being held fixed, the temperatureparameter is tended to zero. Simulations suggest that the resulting Gibbssampler provides a computationally efficient way of achieving approximative MLdetection in MIMO systems having a huge number of transmit and receivedimensions. In fact, they further suggest that the Markov chain is rapidlymixing. Thus, it has been observed that even in cases were ML detection using,e.g. sphere decoding becomes infeasible, the Gibbs sampler can still offer anear-optimal solution using much less computations.

From Centralized to Decentralized Coded Caching

  We consider the problem of designing decentralized schemes for coded caching.In this problem there are $K$ users each caching $M$ files out of a library of$N$ total files. The question is to minimize $R$, the number of broadcasttransmissions to satisfy all the user demands. Decentralized schemes allow thecreation of each cache independently, allowing users to join or leave withoutdependencies. Previous work showed that to achieve a coding gain $g$, i.e. $R\leq K (1-M/N)/g$ transmissions, each file has to be divided into number ofsubpackets that is exponential in $g$.  In this work we propose a simple translation scheme that converts anyconstant rate centralized scheme into a random decentralized placement schemethat guarantees a target coding gain of $g$. If the file size in the originalconstant rate centralized scheme is subexponential in $K$, then the file sizefor the resulting scheme is subexponential in $g$. When new users join, therest of the system remains the same. However, we require an additionalcommunication overhead of $O(\log K)$ bits to determine the new user's cachestate. We also show that the worst-case rate guarantee degrades only by aconstant factor due to the dynamics of user arrival and departure.

Stay on path: PCA along graph paths

  We introduce a variant of (sparse) PCA in which the set of feasible supportsets is determined by a graph. In particular, we consider the followingsetting: given a directed acyclic graph $G$ on $p$ vertices corresponding tovariables, the non-zero entries of the extracted principal component mustcoincide with vertices lying along a path in $G$.  From a statistical perspective, information on the underlying network maypotentially reduce the number of observations required to recover thepopulation principal component. We consider the canonical estimator whichoptimally exploits the prior knowledge by solving a non-convex quadraticmaximization on the empirical covariance. We introduce a simple network andanalyze the estimator under the spiked covariance model. We show that sideinformation potentially improves the statistical complexity.  We propose two algorithms to approximate the solution of the constrainedquadratic maximization, and recover a component with the desired properties. Weempirically evaluate our schemes on synthetic and real datasets.

Bipartite Correlation Clustering -- Maximizing Agreements

  In Bipartite Correlation Clustering (BCC) we are given a complete bipartitegraph $G$ with `+' and `-' edges, and we seek a vertex clustering thatmaximizes the number of agreements: the number of all `+' edges within clustersplus all `-' edges cut across clusters. BCC is known to be NP-hard.  We present a novel approximation algorithm for $k$-BCC, a variant of BCC withan upper bound $k$ on the number of clusters. Our algorithm outputs a$k$-clustering that provably achieves a number of agreements within amultiplicative ${(1-\delta)}$-factor from the optimal, for any desired accuracy$\delta$. It relies on solving a combinatorially constrained bilinearmaximization on the bi-adjacency matrix of $G$. It runs in time exponential in$k$ and $\delta^{-1}$, but linear in the size of the input.  Further, we show that, in the (unconstrained) BCC setting, an${(1-\delta)}$-approximation can be achieved by $O(\delta^{-1})$ clustersregardless of the size of the graph. In turn, our $k$-BCC algorithm implies anEfficient PTAS for the BCC objective of maximizing agreements.

Compressed Sensing using Generative Models

  The goal of compressed sensing is to estimate a vector from anunderdetermined system of noisy linear measurements, by making use of priorknowledge on the structure of vectors in the relevant domain. For almost allresults in this literature, the structure is represented by sparsity in awell-chosen basis. We show how to achieve guarantees similar to standardcompressed sensing but without employing sparsity at all. Instead, we supposethat vectors lie near the range of a generative model $G: \mathbb{R}^k \to\mathbb{R}^n$. Our main theorem is that, if $G$ is $L$-Lipschitz, then roughly$O(k \log L)$ random Gaussian measurements suffice for an $\ell_2/\ell_2$recovery guarantee. We demonstrate our results using generative models frompublished variational autoencoder and generative adversarial networks. Ourmethod can use $5$-$10$x fewer measurements than Lasso for the same accuracy.

Gossip Algorithms for Distributed Signal Processing

  Gossip algorithms are attractive for in-network processing in sensor networksbecause they do not require any specialized routing, there is no bottleneck orsingle point of failure, and they are robust to unreliable wireless networkconditions. Recently, there has been a surge of activity in the computerscience, control, signal processing, and information theory communities,developing faster and more robust gossip algorithms and deriving theoreticalperformance guarantees. This article presents an overview of recent work in thearea. We describe convergence rate results, which are related to the number oftransmitted messages and thus the amount of energy consumed in the network forgossiping. We discuss issues related to gossiping over wireless links,including the effects of quantization and noise, and we illustrate the use ofgossip algorithms for canonical signal processing tasks including distributedestimation, source localization, and compression.

Geographic Gossip: Efficient Aggregation for Sensor Networks

  Gossip algorithms for aggregation have recently received significantattention for sensor network applications because of their simplicity androbustness in noisy and uncertain environments. However, gossip algorithms canwaste significant energy by essentially passing around redundant informationmultiple times. For realistic sensor network model topologies like grids andrandom geometric graphs, the inefficiency of gossip schemes is caused by slowmixing times of random walks on those graphs. We propose and analyze analternative gossiping scheme that exploits geographic information. By utilizinga simple resampling method, we can demonstrate substantial gains overpreviously proposed gossip protocols. In particular, for random geometricgraphs, our algorithm computes the true average to accuracy $1/n^a$ using$O(n^{1.5}\sqrt{\log n})$ radio transmissions, which reduces the energyconsumption by a $\sqrt{\frac{n}{\log n}}$ factor over standard gossipalgorithms.

The Impact of Mobility on Gossip Algorithms

  The influence of node mobility on the convergence time of averaging gossipalgorithms in networks is studied. It is shown that a small number of fullymobile nodes can yield a significant decrease in convergence time. A method isdeveloped for deriving lower bounds on the convergence time by merging nodesaccording to their mobility pattern. This method is used to show that if theagents have one-dimensional mobility in the same direction the convergence timeis improved by at most a constant. Upper bounds are obtained on the convergencetime using techniques from the theory of Markov chains and show that simplemodels of mobility can dramatically accelerate gossip as long as the mobilitypaths significantly overlap. Simulations verify that different mobilitypatterns can have significantly different effects on the convergence ofdistributed algorithms.

LP Decoding meets LP Decoding: A Connection between Channel Coding and  Compressed Sensing

  This is a tale of two linear programming decoders, namely channel codinglinear programming decoding (CC-LPD) and compressed sensing linear programmingdecoding (CS-LPD). So far, they have evolved quite independently. The aim ofthe present paper is to show that there is a tight connection between, on theone hand, CS-LPD based on a zero-one measurement matrix over the reals and, onthe other hand, CC-LPD of the binary linear code that is obtained by viewingthis measurement matrix as a binary parity-check matrix. This connection allowsone to translate performance guarantees from one setup to the other.

Searching for Minimum Storage Regenerating Codes

  Regenerating codes allow distributed storage systems to recover from the lossof a storage node while transmitting the minimum possible amount of data acrossthe network. We present a systematic computer search for optimal systematicregenerating codes. To search the space of potential codes, we reduce thepotential search space in several ways. We impose an additional symmetrycondition on codes that we consider. We specify codes in a simple alternativeway, using additional recovered coefficients rather than transmissioncoefficients and place codes into equivalence classes to avoid redundantchecking. Our main finding is a few optimal systematic minimum storageregenerating codes for $n=5$ and $k=3$, over several finite fields. No suchcodes were previously known and the matching of the information theoreticcut-set bound was an open problem.

On the Delay of Network Coding over Line Networks

  We analyze a simple network where a source and a receiver are connected by aline of erasure channels of different reliabilities. Recent prior work hasshown that random linear network coding can achieve the min-cut capacity andtherefore the asymptotic rate is determined by the worst link of the linenetwork. In this paper we investigate the delay for transmitting a batch ofpackets, which is a function of all the erasure probabilities and the number ofpackets in the batch. We show a monotonicity result on the delay function andderive simple expressions which characterize the expected delay behavior ofline networks. Further, we use a martingale bounded differences argument toshow that the actual delay is tightly concentrated around its expectation.

Security in Distributed Storage Systems by Communicating a Logarithmic  Number of Bits

  We investigate the problem of maintaining an encoded distributed storagesystem when some nodes contain adversarial errors. Using the error-correctioncapabilities that are built into the existing redundancy of the system, wepropose a simple linear hashing scheme to detect errors in the storage nodes.Our main result is that for storing a data object of total size $\size$ usingan $(n,k)$ MDS code over a finite field $\F_q$, up to$t_1=\lfloor(n-k)/2\rfloor$ errors can be detected, with probability of failuresmaller than $1/ \size$, by communicating only $O(n(n-k)\log \size)$ bits to atrusted verifier. Our result constructs small projections of the data thatpreserve the errors with high probability and builds on a pseudorandomgenerator that fools linear functions. The transmission rate achieved by ourscheme is asymptotically equal to the min-cut capacity between the source andany receiver.

Efficient Algorithms for Renewable Energy Allocation to Delay Tolerant  Consumers

  We investigate the problem of allocating energy from renewable sources toflexible consumers in electricity markets. We assume there is a renewableenergy supplier that provides energy according to a time-varying (and possiblyunpredictable) supply process. The plant must serve consumers within aspecified delay window, and incurs a cost of drawing energy from other(possibly non-renewable) sources if its own supply is not sufficient to meetthe deadlines. We formulate two stochastic optimization problems: The firstseeks to minimize the time average cost of using the other sources (and hencestrives for the most efficient utilization of the renewable source). The secondallows the renewable source to dynamically set a price for its service, andseeks to maximize the resulting time average profit. These problems are solvedvia the Lyapunov optimization technique. Our resulting algorithms do notrequire knowledge of the statistics of the time-varying supply and demandprocesses and are robust to arbitrary sample path variations.

Rebuilding for Array Codes in Distributed Storage Systems

  In distributed storage systems that use coding, the issue of minimizing thecommunication required to rebuild a storage node after a failure arises. Weconsider the problem of repairing an erased node in a distributed storagesystem that uses an EVENODD code. EVENODD codes are maximum distance separable(MDS) array codes that are used to protect against erasures, and only requireXOR operations for encoding and decoding. We show that when there are tworedundancy nodes, to rebuild one erased systematic node, only 3/4 of theinformation needs to be transmitted. Interestingly, in many cases, the requireddisk I/O is also minimized.

Repair Optimal Erasure Codes through Hadamard Designs

  In distributed storage systems that employ erasure coding, the issue ofminimizing the total {\it communication} required to exactly rebuild a storagenode after a failure arises. This repair bandwidth depends on the structure ofthe storage code and the repair strategies used to restore the lost data.Designing high-rate maximum-distance separable (MDS) codes that achieve theoptimum repair communication has been a well-known open problem. In this work,we use Hadamard matrices to construct the first explicit 2-parity MDS storagecode with optimal repair properties for all single node failures, including theparities. Our construction relies on a novel method of achieving perfectinterference alignment over finite fields with a finite file size, or number ofextensions. We generalize this construction to design $m$-parity MDS codes thatachieve the optimum repair communication for single systematic node failuresand show that there is an interesting connection between our $m$-parity codesand the systematic-repair optimal permutation-matrix based codes of Tamo {\itet al.} \cite{Tamo} and Cadambe {\it et al.} \cite{PermCodes_ISIT, PermCodes}.

Distributed Storage Codes through Hadamard Designs

  In distributed storage systems that employ erasure coding, the issue ofminimizing the total {\it repair bandwidth} required to exactly regenerate astorage node after a failure arises. This repair bandwidth depends on thestructure of the storage code and the repair strategies used to restore thelost data. Minimizing it requires that undesired data during a repair align inthe smallest possible spaces, using the concept of interference alignment (IA).Here, a points-on-a-lattice representation of the symbol extension IA ofCadambe {\it et al.} provides cues to perfect IA instances which we combinewith fundamental properties of Hadamard matrices to construct a new storagecode with favorable repair properties. Specifically, we build an explicit$(k+2,k)$ storage code over $\mathbb{GF}(3)$, whose single systematic nodefailures can be repaired with bandwidth that matches exactly the theoreticalminimum. Moreover, the repair of single parity node failures generates at mostthe same repair bandwidth as any systematic node failure. Our code can tolerateany single node failure and any pair of failures that involves at most onesystematic failure.

Simple Regenerating Codes: Network Coding for Cloud Storage

  Network codes designed specifically for distributed storage systems have thepotential to provide dramatically higher storage efficiency for the sameavailability. One main challenge in the design of such codes is the exactrepair problem: if a node storing encoded information fails, in order tomaintain the same level of reliability we need to create encoded information ata new node. One of the main open problems in this emerging area has been thedesign of simple coding schemes that allow exact and low cost repair of failednodes and have high data rates. In particular, all prior known explicitconstructions have data rates bounded by 1/2.  In this paper we introduce the first family of distributed storage codes thathave simple look-up repair and can achieve arbitrarily high rates. Ourconstructions are very simple to implement and perform exact repair by simpleXORing of packets. We experimentally evaluate the proposed codes in a realisticcloud storage simulator and show significant benefits in both performance andreliability compared to replication and standard Reed-Solomon codes.

Allocations for Heterogenous Distributed Storage

  We study the problem of storing a data object in a set of data nodes thatfail independently with given probabilities. Our problem is a naturalgeneralization of a homogenous storage allocation problem where all the nodeshad the same reliability and is naturally motivated for peer-to-peer and cloudstorage systems with different types of nodes. Assuming optimal erasure coding(MDS), the goal is to find a storage allocation (i.e, how much to store in eachnode) to maximize the probability of successful recovery. This problem turnsout to be a challenging combinatorial optimization problem. In this work weintroduce an approximation framework based on large deviation inequalities andconvex optimization. We propose two approximation algorithms and study theasymptotic performance of the resulting allocations.

Wireless Device-to-Device Communications with Distributed Caching

  We introduce a novel wireless device-to-device (D2D) collaborationarchitecture that exploits distributed storage of popular content to enablefrequency reuse. We identify a fundamental conflict between collaborationdistance and interference and show how to optimize the transmission power tomaximize frequency reuse. Our analysis depends on the user content requeststatistics which are modeled by a Zipf distribution. Our main result is aclosed form expression of the optimal collaboration distance as a function ofthe content reuse distribution parameters. We show that if the Zipf exponent ofthe content reuse distribution is greater than 1, it is possible to have anumber of D2D interference-free collaboration pairs that scales linearly in thenumber of nodes. If the Zipf exponent is smaller than 1, we identify the bestpossible scaling in the number of D2D collaborating links. Surprisingly, a verysimple distributed caching policy achieves the optimal scaling behavior andtherefore there is no need to centrally coordinate what each node is caching.

Scaling Behaviors of Wireless Device-to-Device Communications with  Distributed Caching

  We analyze a novel architecture for caching popular video content to enablewireless device-to-device collaboration. We focus on the asymptotic scalingcharacteristics and show how they depends on video content popularitystatistics. We identify a fundamental conflict between collaboration distanceand interference and show how to optimize the transmission power to maximizefrequency reuse. Our main result is a closed form expression of the optimalcollaboration distance as a function of the model parameters. Under the commonassumption of a Zipf distribution for content reuse, we show that if the Zipfexponent is greater than 1, it is possible to have a number of D2Dinterference-free collaboration pairs that scales linearly in the number ofnodes. If the Zipf exponent is smaller than 1, we identify the best possiblescaling in the number of D2D collaborating links. Surprisingly, a very simpledistributed caching policy achieves the optimal scaling behavior and thereforethere is no need to centrally coordinate what each node is caching.

On the Delay Advantage of Coding in Packet Erasure Networks

  We consider the delay of network coding compared to routing withretransmissions in packet erasure networks with probabilistic erasures. Weinvestigate the sub-linear term in the block delay required for unicasting $n$packets and show that there is an unbounded gap between network coding androuting. In particular, we show that delay benefit of network coding scales atleast as $\sqrt{n}$. Our analysis of the delay function for the routingstrategy involves a major technical challenge of computing the expectation ofthe maximum of two negative binomial random variables. This problem has beenstudied previously and we derive the first exact characterization which may beof independent interest. We also use a martingale bounded differences argumentto show that the actual coding delay is tightly concentrated around itsexpectation.

Network Codes for Real-Time Applications

  We consider the scenario of broadcasting for real-time applications and lossrecovery via instantly decodable network coding. Past work focused onminimizing the completion delay, which is not the right objective for real-timeapplications that have strict deadlines. In this work, we are interested infinding a code that is instantly decodable by the maximum number of users.First, we prove that this problem is NP-Hard in the general case. Then weconsider the practical probabilistic scenario, where users have i.i.d. lossprobability and the number of packets is linear or polynomial in the number ofusers. In this scenario, we provide a polynomial-time (in the number of users)algorithm that finds the optimal coded packet. The proposed algorithm isevaluated using both simulation and real network traces of a real-time Androidapplication. Both results show that the proposed coding scheme significantlyoutperforms the state-of-the-art baselines: an optimal repetition code and aCOPE-like greedy scheme.

Base-Station Assisted Device-to-Device Communications for  High-Throughput Wireless Video Networks

  We propose a new scheme for increasing the throughput of video files incellular communications systems. This scheme exploits (i) the redundancy ofuser requests as well as (ii) the considerable storage capacity of smartphonesand tablets. Users cache popular video files and - after receiving requestsfrom other users - serve these requests via device-to-device localizedtransmissions. The file placement is optimal when a central control knows apriori the locations of wireless devices when file requests occur. However,even a purely random caching scheme shows only a minor performance losscompared to such a genie-aided scheme. We then analyze the optimalcollaboration distance, trading off frequency reuse with the probability offinding a requested file within the collaboration distance. We show that animprovement of spectral efficiency of one to two orders of magnitude ispossible, even if there is not very high redundancy in video requests.

Locality and Availability in Distributed Storage

  This paper studies the problem of code symbol availability: a code symbol issaid to have $(r, t)$-availability if it can be reconstructed from $t$ disjointgroups of other symbols, each of size at most $r$. For example, $3$-replicationsupports $(1, 2)$-availability as each symbol can be read from its $t= 2$ other(disjoint) replicas, i.e., $r=1$. However, the rate of replication must vanishlike $\frac{1}{t+1}$ as the availability increases.  This paper shows that it is possible to construct codes that can support ascaling number of parallel reads while keeping the rate to be an arbitrarilyhigh constant. It further shows that this is possible with the minimum distancearbitrarily close to the Singleton bound. This paper also presents a bounddemonstrating a trade-off between minimum distance, availability and locality.Our codes match the aforementioned bound and their construction relies oncombinatorial objects called resolvable designs.  From a practical standpoint, our codes seem useful for distributed storageapplications involving hot data, i.e., the information which is frequentlyaccessed by multiple processes in parallel.

Bounding Multiple Unicasts through Index Coding and Locally Repairable  Codes

  We establish a duality result between linear index coding and LocallyRepairable Codes (LRCs). Specifically, we show that a natural extension of LRCswe call Generalized Locally Repairable Codes (GLCRs) are exactly dual to linearindex codes. In a GLRC, every node is decodable from a specific set of othernodes and these sets induce a recoverability directed graph. We show that thedual linear subspace of a GLRC is a solution to an index coding instance wherethe side information graph is this GLRC recoverability graph. We show that theGLRC rate is equivalent to the complementary index coding rate, i.e. the numberof transmissions saved by coding. Our second result uses this duality toestablish a new upper bound for the multiple unicast network coding problem. Inmultiple unicast network coding, we are given a directed acyclic graph and rsources that want to send independent messages to r corresponding destinations.Our new upper bound is efficiently computable and relies on a strongapproximation result for complementary index coding. We believe that our boundcould lead to a logarithmic approximation factor for multiple unicast networkcoding if a plausible connection we state is verified.

Batch Codes through Dense Graphs without Short Cycles

  Consider a large database of $n$ data items that need to be stored using $m$servers. We study how to encode information so that a large number $k$ of readrequests can be performed in parallel while the rate remains constant (andideally approaches one). This problem is equivalent to the design of multisetBatch Codes introduced by Ishai, Kushilevitz, Ostrovsky and Sahai [17].  We give families of multiset batch codes with asymptotically optimal rates ofthe form $1-1/\text{poly}(k)$ and a number of servers $m$ scaling polynomiallyin the number of read requests $k$. An advantage of our batch codeconstructions over most previously known multiset batch codes is explicit anddeterministic decoding algorithms and asymptotically optimal fault tolerance.  Our main technical innovation is a graph-theoretic method of designingmultiset batch codes using dense bipartite graphs with no small cycles. Wemodify prior graph constructions of dense, high-girth graphs to obtain ourbatch code results. We achieve close to optimal tradeoffs between theparameters for bipartite graph based batch codes.

On the Information Theoretic Limits of Learning Ising Models

  We provide a general framework for computing lower-bounds on the samplecomplexity of recovering the underlying graphs of Ising models, given i.i.dsamples. While there have been recent results for specific graph classes, theseinvolve fairly extensive technical arguments that are specialized to eachspecific graph class. In contrast, we isolate two key graph-structuralingredients that can then be used to specify sample complexity lower-bounds.Presence of these structural properties makes the graph class hard to learn. Wederive corollaries of our main result that not only recover existing recentresults, but also provide lower bounds for novel graph classes not consideredpreviously. We also extend our framework to the random graph setting and derivecorollaries for Erd\H{o}s-R\'{e}nyi graphs in a certain dense setting.

Index Coding with Coded Side-Information

  This letter investigates a new class of index coding problems. One senderbroadcasts packets to multiple users, each desiring a subset, by exploitingprior knowledge of linear combinations of packets. We refer to this class ofproblems as index coding with coded side-information. Our aim is tocharacterize the minimum index code length that the sender needs to transmit tosimultaneously satisfy all user requests. We show that the optimal binaryvector index code length is equal to the minimum rank (minrank) of a matrixwhose elements consist of the sets of desired packet indices and side-information encoding matrices. This is the natural extension of matrix minrankin the presence of coded side information. Using the derived expression, wepropose a greedy randomized algorithm to minimize the rank of the derivedmatrix.

FrogWild! -- Fast PageRank Approximations on Graph Engines

  We propose FrogWild, a novel algorithm for fast approximation of highPageRank vertices, geared towards reducing network costs of running traditionalPageRank algorithms. Our algorithm can be seen as a quantized version of poweriteration that performs multiple parallel random walks over a directed graph.One important innovation is that we introduce a modification to the GraphLabframework that only partially synchronizes mirror vertices. This partialsynchronization vastly reduces the network traffic generated by traditionalPageRank algorithms, thus greatly reducing the per-iteration cost of PageRank.On the other hand, this partial synchronization also creates dependenciesbetween the random walks used to estimate PageRank. Our main theoreticalinnovation is the analysis of the correlations introduced by this partialsynchronization process and a bound establishing that our approximation isclose to the true PageRank vector.  We implement our algorithm in GraphLab and compare it against the defaultPageRank implementation. We show that our algorithm is very fast, performingeach iteration in less than one second on the Twitter graph and can be up to 7xfaster compared to the standard GraphLab PageRank implementation.

Sparse PCA via Bipartite Matchings

  We consider the following multi-component sparse PCA problem: given a set ofdata points, we seek to extract a small number of sparse components withdisjoint supports that jointly capture the maximum possible variance. Thesecomponents can be computed one by one, repeatedly solving the single-componentproblem and deflating the input data matrix, but as we show this greedyprocedure is suboptimal. We present a novel algorithm for sparse PCA thatjointly optimizes multiple disjoint components. The extracted features capturevariance that lies within a multiplicative factor arbitrarily close to 1 fromthe optimal. Our algorithm is combinatorial and computes the desired componentsby solving multiple instances of the bipartite maximum weight matching problem.Its complexity grows as a low order polynomial in the ambient dimension of theinput data matrix, but exponentially in its rank. However, it can beeffectively applied on a low-dimensional sketch of the data; this allows us toobtain polynomial-time approximation guarantees via spectral bounds. Weevaluate our algorithm on real data-sets and empirically demonstrate that inmany cases it outperforms existing, deflation-based approaches.

Single Pass PCA of Matrix Products

  In this paper we present a new algorithm for computing a low rankapproximation of the product $A^TB$ by taking only a single pass of the twomatrices $A$ and $B$. The straightforward way to do this is to (a) first sketch$A$ and $B$ individually, and then (b) find the top components using PCA on thesketch. Our algorithm in contrast retains additional summary information about$A,B$ (e.g. row and column norms etc.) and uses this additional information toobtain an improved approximation from the sketches. Our main analytical resultestablishes a comparable spectral norm guarantee to existing two-pass methods;in addition we also provide results from an Apache Spark implementation thatshows better computational and statistical performance on real-world andsynthetic evaluation datasets.

Restricted Strong Convexity Implies Weak Submodularity

  We connect high-dimensional subset selection and submodular maximization. Ourresults extend the work of Das and Kempe (2011) from the setting of linearregression to arbitrary objective functions. For greedy feature selection, thisconnection allows us to obtain strong multiplicative performance bounds onseveral methods without statistical modeling assumptions. We also deriverecovery guarantees of this form under standard assumptions. Our work showsthat greedy algorithms perform within a constant factor from the best possiblesubset-selection solution for a broad class of general objective functions. Ourmethods allow a direct control over the number of obtained features as opposedto regularization parameters that only implicitly control sparsity. Our prooftechnique uses the concept of weak submodularity initially defined by Das andKempe. We draw a connection between convex analysis and submodular set functiontheory which may be of independent interest for other statistical learningapplications that have combinatorial structure.

Identifying Best Interventions through Online Importance Sampling

  Motivated by applications in computational advertising and systems biology,we consider the problem of identifying the best out of several possible softinterventions at a source node $V$ in an acyclic causal directed graph, tomaximize the expected value of a target node $Y$ (located downstream of $V$).Our setting imposes a fixed total budget for sampling under variousinterventions, along with cost constraints on different types of interventions.We pose this as a best arm identification bandit problem with $K$ arms whereeach arm is a soft intervention at $V,$ and leverage the information leakageamong the arms to provide the first gap dependent error and simple regretbounds for this problem. Our results are a significant improvement over thetraditional best arm identification results. We empirically show that ouralgorithms outperform the state of the art in the Flow Cytometry data-set, andalso apply our algorithm for model interpretation of the Inception-v3 deep netthat classifies images.

