Probabilistic Motion Estimation Based on Temporal Coherence

  We develop a theory for the temporal integration of visual motion motivatedby psychophysical experiments. The theory proposes that input data aretemporally grouped and used to predict and estimate the motion flows in theimage sequence. This temporal grouping can be considered a generalization ofthe data association techniques used by engineers to study motion sequences.Our temporal-grouping theory is expressed in terms of the Bayesiangeneralization of standard Kalman filtering. To implement the theory we derivea parallel network which shares some properties of cortical networks. Computersimulations of this network demonstrate that our theory qualitatively accountsfor psychophysical experiments on motion occlusion and motion outliers.

Complexity of Representation and Inference in Compositional Models with  Part Sharing

  This paper describes serial and parallel compositional models of multipleobjects with part sharing. Objects are built by part-subpart compositions andexpressed in terms of a hierarchical dictionary of object parts. These partsare represented on lattices of decreasing sizes which yield an executivesummary description. We describe inference and learning algorithms for thesemodels. We analyze the complexity of this model in terms of computation time(for serial computers) and numbers of nodes (e.g., "neurons") for parallelcomputers. In particular, we compute the complexity gains by part sharing andits dependence on how the dictionary scales with the level of the hierarchy. Weexplore three regimes of scaling behavior where the dictionary size (i)increases exponentially with the level, (ii) is determined by an unsupervisedcompositional learning algorithm applied to real data, (iii) decreasesexponentially with scale. This analysis shows that in some regimes the use ofshared parts enables algorithms which can perform inference in time linear inthe number of levels for an exponential number of objects. In other regimespart sharing has little advantage for serial computers but can give linearprocessing on parallel computers.

Robust Estimation of 3D Human Poses from a Single Image

  Human pose estimation is a key step to action recognition. We propose amethod of estimating 3D human poses from a single image, which works inconjunction with an existing 2D pose/joint detector. 3D pose estimation ischallenging because multiple 3D poses may correspond to the same 2D pose afterprojection due to the lack of depth information. Moreover, current 2D poseestimators are usually inaccurate which may cause errors in the 3D estimation.We address the challenges in three ways: (i) We represent a 3D pose as a linearcombination of a sparse set of bases learned from 3D human skeletons. (ii) Weenforce limb length constraints to eliminate anthropomorphically implausibleskeletons. (iii) We estimate a 3D pose by minimizing the $L_1$-norm errorbetween the projection of the 3D pose and the corresponding 2D detection. The$L_1$-norm loss term is robust to inaccurate 2D joint estimations. We use thealternating direction method (ADM) to solve the optimization problemefficiently. Our approach outperforms the state-of-the-arts on three benchmarkdatasets.

The Secrets of Salient Object Segmentation

  In this paper we provide an extensive evaluation of fixation prediction andsalient object segmentation algorithms as well as statistics of major datasets.Our analysis identifies serious design flaws of existing salient objectbenchmarks, called the dataset design bias, by over emphasizing thestereotypical concepts of saliency. The dataset design bias does not onlycreate the discomforting disconnection between fixations and salient objectsegmentation, but also misleads the algorithm designing. Based on our analysis,we propose a new high quality dataset that offers both fixation and salientobject segmentation ground-truth. With fixations and salient object beingpresented simultaneously, we are able to bridge the gap between fixations andsalient objects, and propose a novel method for salient object segmentation.Finally, we report significant benchmark progress on three existing datasets ofsegmenting salient objects

Learning Deep Structured Models

  Many problems in real-world applications involve predicting several randomvariables which are statistically related. Markov random fields (MRFs) are agreat mathematical tool to encode such relationships. The goal of this paper isto combine MRFs with deep learning algorithms to estimate complexrepresentations while taking into account the dependencies between the outputrandom variables. Towards this goal, we propose a training algorithm that isable to learn structured models jointly with deep features that form the MRFpotentials. Our approach is efficient as it blends learning and inference andmakes use of GPU acceleration. We demonstrate the effectiveness of ouralgorithm in the tasks of predicting words from noisy images, as well asmulti-class classification of Flickr photographs. We show that joint learningof the deep features and the MRF parameters results in significant performancegains.

Explain Images with Multimodal Recurrent Neural Networks

  In this paper, we present a multimodal Recurrent Neural Network (m-RNN) modelfor generating novel sentence descriptions to explain the content of images. Itdirectly models the probability distribution of generating a word givenprevious words and the image. Image descriptions are generated by sampling fromthis distribution. The model consists of two sub-networks: a deep recurrentneural network for sentences and a deep convolutional network for images. Thesetwo sub-networks interact with each other in a multimodal layer to form thewhole m-RNN model. The effectiveness of our model is validated on threebenchmark datasets (IAPR TC-12, Flickr 8K, and Flickr 30K). Our modeloutperforms the state-of-the-art generative method. In addition, the m-RNNmodel can be applied to retrieval tasks for retrieving images or sentences, andachieves significant performance improvement over the state-of-the-art methodswhich directly optimize the ranking objective function for retrieval.

Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image  Segmentation

  Deep convolutional neural networks (DCNNs) trained on a large number ofimages with strong pixel-level annotations have recently significantly pushedthe state-of-art in semantic image segmentation. We study the more challengingproblem of learning DCNNs for semantic image segmentation from either (1)weakly annotated training data such as bounding boxes or image-level labels or(2) a combination of few strongly labeled and many weakly labeled images,sourced from one or multiple datasets. We develop Expectation-Maximization (EM)methods for semantic image segmentation model training under these weaklysupervised and semi-supervised settings. Extensive experimental evaluationshows that the proposed techniques can learn models delivering competitiveresults on the challenging PASCAL VOC 2012 image segmentation benchmark, whilerequiring significantly less annotation effort. We share source codeimplementing the proposed system athttps://bitbucket.org/deeplab/deeplab-public.

Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs  and a Discriminatively Trained Domain Transform

  Deep convolutional neural networks (CNNs) are the backbone of state-of-artsemantic image segmentation systems. Recent work has shown that complementingCNNs with fully-connected conditional random fields (CRFs) can significantlyenhance their object localization accuracy, yet dense CRF inference iscomputationally expensive. We propose replacing the fully-connected CRF withdomain transform (DT), a modern edge-preserving filtering method in which theamount of smoothing is controlled by a reference edge map. Domain transformfiltering is several times faster than dense CRF inference and we show that ityields comparable semantic segmentation results, accurately capturing objectboundaries. Importantly, our formulation allows learning the reference edge mapfrom intermediate CNN features instead of using the image gradient magnitude asin standard DT filtering. This produces task-specific edges in an end-to-endtrainable system optimizing the target semantic segmentation quality.

Attention to Scale: Scale-aware Semantic Image Segmentation

  Incorporating multi-scale features in fully convolutional neural networks(FCNs) has been a key element to achieving state-of-the-art performance onsemantic image segmentation. One common way to extract multi-scale features isto feed multiple resized input images to a shared deep network and then mergethe resulting features for pixelwise classification. In this work, we proposean attention mechanism that learns to softly weight the multi-scale features ateach pixel location. We adapt a state-of-the-art semantic image segmentationmodel, which we jointly train with multi-scale input images and the attentionmodel. The proposed attention model not only outperforms average- andmax-pooling, but allows us to diagnostically visualize the importance offeatures at different positions and scales. Moreover, we show that adding extrasupervision to the output at each scale is essential to achieving excellentperformance when merging multi-scale features. We demonstrate the effectivenessof our model with extensive experiments on three challenging datasets,including PASCAL-Person-Part, PASCAL VOC 2012 and a subset of MS-COCO 2014.

PASCAL Boundaries: A Class-Agnostic Semantic Boundary Dataset

  In this paper, we address the boundary detection task motivated by theambiguities in current definition of edge detection. To this end, we generate alarge database consisting of more than 10k images (which is 20x bigger thanexisting edge detection databases) along with ground truth boundaries between459 semantic classes including both foreground objects and different types ofbackground, and call it the PASCAL Boundaries dataset, which will be releasedto the community. In addition, we propose a novel deep network-basedmulti-scale semantic boundary detector and name it Multi-scale Deep SemanticBoundary Detector (M-DSBD). We provide baselines using models that were trainedon edge detection and show that they transfer reasonably to the task ofboundary detection. Finally, we point to various important research problemsthat this dataset can be used for.

Exploiting Symmetry and/or Manhattan Properties for 3D Object Structure  Estimation from Single and Multiple Images

  Many man-made objects have intrinsic symmetries and Manhattan structure. Byassuming an orthographic projection model, this paper addresses the estimationof 3D structures and camera projection using symmetry and/or Manhattanstructure cues, which occur when the input is single- or multiple-image fromthe same category, e.g., multiple different cars. Specifically, analysis on thesingle image case implies that Manhattan alone is sufficient to recover thecamera projection, and then the 3D structure can be reconstructed uniquelyexploiting symmetry. However, Manhattan structure can be difficult to observefrom a single image due to occlusion. To this end, we extend to themultiple-image case which can also exploit symmetry but does not requireManhattan axes. We propose a novel rigid structure from motion method,exploiting symmetry and using multiple images from the same category as input.Experimental results on the Pascal3D+ dataset show that our methodsignificantly outperforms baseline methods.

Object Recognition with and without Objects

  While recent deep neural networks have achieved a promising performance onobject recognition, they rely implicitly on the visual contents of the wholeimage. In this paper, we train deep neural net- works on the foreground(object) and background (context) regions of images respectively. Consider- inghuman recognition in the same situations, net- works trained on the purebackground without ob- jects achieves highly reasonable recognition performancethat beats humans by a large margin if only given context. However, humansstill outperform networks with pure object available, which indicates networksand human beings have different mechanisms in understanding an image.Furthermore, we straightforwardly combine multiple trained networks to exploredifferent visual cues learned by different networks. Experiments show thatuseful visual hints can be explicitly learned separately and then combined toachieve higher performance, which verifies the advantages of the proposedframework.

Generating Multiple Diverse Hypotheses for Human 3D Pose Consistent with  2D Joint Detections

  We propose a method to generate multiple diverse and valid human posehypotheses in 3D all consistent with the 2D detection of joints in a monocularRGB image. We use a novel generative model uniform (unbiased) in the space ofanatomically plausible 3D poses. Our model is compositional (produces a pose bycombining parts) and since it is restricted only by anatomical constraints itcan generalize to every plausible human 3D pose. Removing the model biasintrinsically helps to generate more diverse 3D pose hypotheses. We argue thatgenerating multiple pose hypotheses is more reasonable than generating only asingle 3D pose based on the 2D joint detection given the depth ambiguity andthe uncertainty due to occlusion and imperfect 2D joint detection. We hope thatthe idea of generating multiple consistent pose hypotheses can give rise to anew line of future work that has not received much attention in the literature.We used the Human3.6M dataset for empirical evaluation.

NormFace: L2 Hypersphere Embedding for Face Verification

  Thanks to the recent developments of Convolutional Neural Networks, theperformance of face verification methods has increased rapidly. In a typicalface verification method, feature normalization is a critical step for boostingperformance. This motivates us to introduce and study the effect ofnormalization during training. But we find this is non-trivial, despitenormalization being differentiable. We identify and study four issues relatedto normalization through mathematical analysis, which yields understanding andhelps with parameter settings. Based on this analysis we propose two strategiesfor training using normalized features. The first is a modification of softmaxloss, which optimizes cosine similarity instead of inner-product. The second isa reformulation of metric learning by introducing an agent vector for eachclass. We show that both strategies, and small variants, consistently improveperformance by between 0.2% to 0.4% on the LFW dataset based on two models.This is significant because the performance of the two models on LFW dataset isclose to saturation at over 98%. Codes and models are released onhttps://github.com/happynear/NormFace

Deep Supervision for Pancreatic Cyst Segmentation in Abdominal CT Scans

  Automatic segmentation of an organ and its cystic region is a prerequisite ofcomputer-aided diagnosis. In this paper, we focus on pancreatic cystsegmentation in abdominal CT scan. This task is important and very useful inclinical practice yet challenging due to the low contrast in boundary, thevariability in location, shape and the different stages of the pancreaticcancer. Inspired by the high relevance between the location of a pancreas andits cystic region, we introduce extra deep supervision into the segmentationnetwork, so that cyst segmentation can be improved with the help of relativelyeasier pancreas segmentation. Under a reasonable transformation function, ourapproach can be factorized into two stages, and each stage can be efficientlyoptimized via gradient back-propagation throughout the deep networks. Wecollect a new dataset with 131 pathological samples, which, to the best of ourknowledge, is the largest set for pancreatic cyst segmentation. Without humanassistance, our approach reports a 63.44% average accuracy, measured by theDice-S{\o}rensen coefficient (DSC), which is higher than the number (60.46%)without deep supervision.

A 3D Coarse-to-Fine Framework for Volumetric Medical Image Segmentation

  In this paper, we adopt 3D Convolutional Neural Networks to segmentvolumetric medical images. Although deep neural networks have been proven to bevery effective on many 2D vision tasks, it is still challenging to apply themto 3D tasks due to the limited amount of annotated 3D data and limitedcomputational resources. We propose a novel 3D-based coarse-to-fine frameworkto effectively and efficiently tackle these challenges. The proposed 3D-basedframework outperforms the 2D counterpart to a large margin since it canleverage the rich spatial infor- mation along all three axes. We conductexperiments on two datasets which include healthy and pathological pancreasesrespectively, and achieve the current state-of-the-art in terms ofDice-S{\o}rensen Coefficient (DSC). On the NIH pancreas segmentation dataset,we outperform the previous best by an average of over 2%, and the worst case isimproved by 7% to reach almost 70%, which indicates the reliability of ourframework in clinical applications.

Single-Shot Object Detection with Enriched Semantics

  We propose a novel single shot object detection network named Detection withEnriched Semantics (DES). Our motivation is to enrich the semantics of objectdetection features within a typical deep detector, by a semantic segmentationbranch and a global activation module. The segmentation branch is supervised byweak segmentation ground-truth, i.e., no extra annotation is required. Inconjunction with that, we employ a global activation module which learnsrelationship between channels and object classes in a self-supervised manner.Comprehensive experimental results on both PASCAL VOC and MS COCO detectiondatasets demonstrate the effectiveness of the proposed method. In particular,with a VGG16 based DES, we achieve an mAP of 81.7 on VOC2007 test and an mAP of32.8 on COCO test-dev with an inference speed of 31.5 milliseconds per image ona Titan Xp GPU. With a lower resolution version, we achieve an mAP of 79.7 onVOC2007 with an inference speed of 13.0 milliseconds per image.

Bridging the Gap Between 2D and 3D Organ Segmentation with Volumetric  Fusion Net

  There has been a debate on whether to use 2D or 3D deep neural networks forvolumetric organ segmentation. Both 2D and 3D models have their advantages anddisadvantages. In this paper, we present an alternative framework, which trains2D networks on different viewpoints for segmentation, and builds a 3DVolumetric Fusion Net (VFN) to fuse the 2D segmentation results. VFN isrelatively shallow and contains much fewer parameters than most 3D networks,making our framework more efficient at integrating 3D information forsegmentation. We train and test the segmentation and fusion modulesindividually, and propose a novel strategy, named cross-cross-augmentation, tomake full use of the limited training data. We evaluate our framework onseveral challenging abdominal organs, and verify its superiority insegmentation accuracy and stability over existing 2D and 3D approaches.

Multi-Scale Spatially-Asymmetric Recalibration for Image Classification

  Convolution is spatially-symmetric, i.e., the visual features are independentof its position in the image, which limits its ability to utilize contextualcues for visual recognition. This paper addresses this issue by introducing arecalibration process, which refers to the surrounding region of each neuron,computes an importance value and multiplies it to the original neural response.Our approach is named multi-scale spatially-asymmetric recalibration (MS-SAR),which extracts visual cues from surrounding regions at multiple scales, anddesigns a weighting scheme which is asymmetric in the spatial domain. MS-SAR isimplemented in an efficient way, so that only small fractions of extraparameters and computations are required. We apply MS-SAR to several popularbuilding blocks, including the residual block and the densely-connected block,and demonstrate its superior performance in both CIFAR and ILSVRC2012classification tasks.

Training Multi-organ Segmentation Networks with Sample Selection by  Relaxed Upper Confident Bound

  Deep convolutional neural networks (CNNs), especially fully convolutionalnetworks, have been widely applied to automatic medical image segmentationproblems, e.g., multi-organ segmentation. Existing CNN-based segmentationmethods mainly focus on looking for increasingly powerful networkarchitectures, but pay less attention to data sampling strategies for trainingnetworks more effectively. In this paper, we present a simple but effectivesample selection method for training multi-organ segmentation networks. Sampleselection exhibits an exploitation-exploration strategy, i.e., exploiting hardsamples and exploring less frequently visited samples. Based on the fact thatvery hard samples might have annotation errors, we propose a new sampleselection policy, named Relaxed Upper Confident Bound (RUCB). Compared withother sample selection policies, e.g., Upper Confident Bound (UCB), it exploitsa range of hard samples rather than being stuck with a small set of very hardones, which mitigates the influence of annotation errors during training. Weapply this new sample selection policy to training a multi-organ segmentationnetwork on a dataset containing 120 abdominal CT scans and show that it boostssegmentation performance significantly.

Joint Shape Representation and Classification for Detecting PDAC

  We aim to detect pancreatic ductal adenocarcinoma (PDAC) in abdominal CTscans, which sheds light on early diagnosis of pancreatic cancer. This is a 3Dvolume classification task with little training data. We propose a two-stageframework, which first segments the pancreas into a binary mask, thencompresses the mask into a shape vector and performs abnormalityclassification. Shape representation and classification are performed in a {\emjoint} manner, both to exploit the knowledge that PDAC often changes the {\bfshape} of the pancreas and to prevent over-fitting. Experiments are performedon $300$ normal scans and $156$ PDAC cases. We achieve a specificity of$90.2\%$ (false alarm occurs on less than $1/10$ normal cases) at a sensitivityof $80.2\%$ (less than $1/5$ PDAC cases are not detected), which show promisefor clinical applications.

Deep Nets: What have they ever done for Vision?

  This is an opinion paper about the strengths and weaknesses of Deep Nets forvision. They are at the center of recent progress on artificial intelligenceand are of growing importance in cognitive science and neuroscience. They haveenormous successes but also clear limitations. There is also only partialunderstanding of their inner workings. It seems unlikely that Deep Nets intheir current form will be the best long-term solution either for buildinggeneral purpose intelligent machines or for understanding the mind/brain, butit is likely that many aspects of them will remain. At present Deep Nets dovery well on specific types of visual tasks and on specific benchmarkeddatasets. But Deep Nets are much less general purpose, flexible, and adaptivethan the human visual system. Moreover, methods like Deep Nets may run intofundamental difficulties when faced with the enormous complexity of naturalimages which can lead to a combinatorial explosion. To illustrate our mainpoints, while keeping the references small, this paper is slightly biasedtowards work from our group.

Multi-Scale Coarse-to-Fine Segmentation for Screening Pancreatic Ductal  Adenocarcinoma

  This paper proposes an intuitive approach to finding pancreatic ductaladenocarcinoma (PDAC), the most common type of pancreatic cancer, by checkingabdominal CT scans. Our idea is named segmentation-for-classification (S4C),which classifies a volume by checking if at least a sufficient number of voxelsis segmented as the tumor. In order to deal with tumors with different scales,we train volumetric segmentation networks with multi-scale inputs, and testthem in a coarse-to-fine flowchart. A post-processing module is used to filterout outliers and reduce false alarms. We perform a case study on our datasetcontaining 439 CT scans, in which 136 cases were diagnosed with PDAC and 303cases are normal. Our approach reports a sensitivity of 94.1% at a specificityof 98.5%, with an average tumor segmentation accuracy of 56.46% over all PDACcases.

PreCo: A Large-scale Dataset in Preschool Vocabulary for Coreference  Resolution

  We introduce PreCo, a large-scale English dataset for coreference resolution.The dataset is designed to embody the core challenges in coreference, such asentity representation, by alleviating the challenge of low overlap betweentraining and test sets and enabling separated analysis of mention detection andmention clustering. To strengthen the training-test overlap, we collect a largecorpus of about 38K documents and 12.4M words which are mostly from thevocabulary of English-speaking preschoolers. Experiments show that with highertraining-test overlap, error analysis on PreCo is more efficient than the oneon OntoNotes, a popular existing dataset. Furthermore, we annotate singletonmentions making it possible for the first time to quantify the influence that amention detector makes on coreference resolution performance. The dataset isfreely available at https://preschool-lab.github.io/PreCo/.

Phase Collaborative Network for Multi-Phase Medical Imaging Segmentation

  Integrating multi-phase information is an effective way of boosting visualrecognition. In this paper, we investigate this problem from the perspective ofmedical imaging analysis, in which two phases in CT scans known as arterial andvenous are combined towards higher segmentation accuracy. To this end, wepropose Phase Collaborative Network (PCN), an end-to-end network which containsboth generative and discriminative modules to formulate phase-to-phaserelations and data-to-label relations, respectively. Experiments are performedon several CT image segmentation datasets. PCN achieves superior performancewith either two phases or only one phase available. Moreover, we empiricallyverify that the accuracy gain comes from the collaboration between phases.

Snapshot Distillation: Teacher-Student Optimization in One Generation

  Optimizing a deep neural network is a fundamental task in computer vision,yet direct training methods often suffer from over-fitting. Teacher-studentoptimization aims at providing complementary cues from a model trainedpreviously, but these approaches are often considerably slow due to thepipeline of training a few generations in sequence, i.e., time complexity isincreased by several times.  This paper presents snapshot distillation (SD), the first framework whichenables teacher-student optimization in one generation. The idea of SD is verysimple: instead of borrowing supervision signals from previous generations, weextract such information from earlier epochs in the same generation, meanwhilemake sure that the difference between teacher and student is sufficiently largeso as to prevent under-fitting. To achieve this goal, we implement SD in acyclic learning rate policy, in which the last snapshot of each cycle is usedas the teacher for all iterations in the next cycle, and the teacher signal issmoothed to provide richer information. In standard image classificationbenchmarks such as CIFAR100 and ILSVRC2012, SD achieves consistent accuracygain without heavy computational overheads. We also verify that modelspre-trained with SD transfers well to object detection and semanticsegmentation in the PascalVOC dataset.

Thickened 2D Networks for 3D Medical Image Segmentation

  There has been a debate in medical image segmentation on whether to use 2D or3D networks, where both pipelines have advantages and disadvantages. This paperpresents a novel approach which thickens the input of a 2D network, so that themodel is expected to enjoy both the stability and efficiency of 2D networks aswell as the ability of 3D networks in modeling volumetric contexts. A majorinformation loss happens when a large number of 2D slices are fused at thefirst convolutional layer, resulting in a relatively weak ability of thenetwork in distinguishing the difference among slices. To alleviate thisdrawback, we propose an effective framework which (i) postpones slice fusionand (ii) adds highway connections from the pre-fusion layer so that theprediction layer receives slice-sensitive auxiliary cues. Experiments onsegmenting a few abdominal targets in particular blood vessels which requirestrong 3D contexts demonstrate the effectiveness of our approach.

Representing Data by a Mixture of Activated Simplices

  We present a new model which represents data as a mixture of simplices.Simplices are geometric structures that generalize triangles. We give a simplegeometric understanding that allows us to learn a simplicial structureefficiently. Our method requires that the data are unit normalized (and thuslie on the unit sphere). We show that under this restriction, building a modelwith simplices amounts to constructing a convex hull inside the sphere whoseboundary facets is close to the data. We call the boundary facets of the convexhull that are close to the data Activated Simplices. While the total number ofbases used to build the simplices is a parameter of the model, the dimensionsof the individual activated simplices are learned from the data. Simplices canhave different dimensions, which facilitates modeling of inhomogeneous datasources. The simplicial structure is bounded --- this is appropriate formodeling data with constraints, such as human elbows can not bend more than 180degrees. The simplices are easy to interpret and extremes within the data canbe discovered among the vertices. The method provides good reconstruction andregularization. It supports good nearest neighbor classification and it allowsrealistic generative models to be constructed. It achieves state-of-the-artresults on benchmark datasets, including 3D poses and digits.

Semantic Image Segmentation with Deep Convolutional Nets and Fully  Connected CRFs

  Deep Convolutional Neural Networks (DCNNs) have recently shown state of theart performance in high level vision tasks, such as image classification andobject detection. This work brings together methods from DCNNs andprobabilistic graphical models for addressing the task of pixel-levelclassification (also called "semantic image segmentation"). We show thatresponses at the final layer of DCNNs are not sufficiently localized foraccurate object segmentation. This is due to the very invariance propertiesthat make DCNNs good for high level tasks. We overcome this poor localizationproperty of deep networks by combining the responses at the final DCNN layerwith a fully connected Conditional Random Field (CRF). Qualitatively, our"DeepLab" system is able to localize segment boundaries at a level of accuracywhich is beyond previous methods. Quantitatively, our method sets the newstate-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching71.6% IOU accuracy in the test set. We show how these results can be obtainedefficiently: Careful network re-purposing and a novel application of the 'hole'algorithm from the wavelet community allow dense computation of neural netresponses at 8 frames per second on a modern GPU.

DeePM: A Deep Part-Based Model for Object Detection and Semantic Part  Localization

  In this paper, we propose a deep part-based model (DeePM) for symbioticobject detection and semantic part localization. For this purpose, we annotatesemantic parts for all 20 object categories on the PASCAL VOC 2012 dataset,which provides information on object pose, occlusion, viewpoint andfunctionality. DeePM is a latent graphical model based on the state-of-the-artR-CNN framework, which learns an explicit representation of the object-partconfiguration with flexible type sharing (e.g., a sideview horse head can beshared by a fully-visible sideview horse and a highly truncated sideview horsewith head and neck only). For comparison, we also present an end-to-endObject-Part (OP) R-CNN which learns an implicit feature representation forjointly mapping an image ROI to the object and part bounding boxes. We evaluatethe proposed methods for both the object and part detection performance onPASCAL VOC 2012, and show that DeePM consistently outperforms OP R-CNN indetecting objects and parts. In addition, it obtains superior performance toFast and Faster R-CNNs in object detection.

DeepLab: Semantic Image Segmentation with Deep Convolutional Nets,  Atrous Convolution, and Fully Connected CRFs

  In this work we address the task of semantic image segmentation with DeepLearning and make three main contributions that are experimentally shown tohave substantial practical merit. First, we highlight convolution withupsampled filters, or 'atrous convolution', as a powerful tool in denseprediction tasks. Atrous convolution allows us to explicitly control theresolution at which feature responses are computed within Deep ConvolutionalNeural Networks. It also allows us to effectively enlarge the field of view offilters to incorporate larger context without increasing the number ofparameters or the amount of computation. Second, we propose atrous spatialpyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPPprobes an incoming convolutional feature layer with filters at multiplesampling rates and effective fields-of-views, thus capturing objects as well asimage context at multiple scales. Third, we improve the localization of objectboundaries by combining methods from DCNNs and probabilistic graphical models.The commonly deployed combination of max-pooling and downsampling in DCNNsachieves invariance but has a toll on localization accuracy. We overcome thisby combining the responses at the final DCNN layer with a fully connectedConditional Random Field (CRF), which is shown both qualitatively andquantitatively to improve localization performance. Our proposed "DeepLab"system sets the new state-of-art at the PASCAL VOC-2012 semantic imagesegmentation task, reaching 79.7% mIOU in the test set, and advances theresults on three other datasets: PASCAL-Context, PASCAL-Person-Part, andCityscapes. All of our code is made publicly available online.

Semi-Supervised Sparse Representation Based Classification for Face  Recognition with Insufficient Labeled Samples

  This paper addresses the problem of face recognition when there is only few,or even only a single, labeled examples of the face that we wish to recognize.Moreover, these examples are typically corrupted by nuisance variables, bothlinear (i.e., additive nuisance variables such as bad lighting, wearing ofglasses) and non-linear (i.e., non-additive pixel-wise nuisance variables suchas expression changes). The small number of labeled examples means that it ishard to remove these nuisance variables between the training and testing facesto obtain good recognition performance. To address the problem we propose amethod called Semi-Supervised Sparse Representation based Classification(S$^3$RC). This is based on recent work on sparsity where faces are representedin terms of two dictionaries: a gallery dictionary consisting of one or moreexamples of each person, and a variation dictionary representing linearnuisance variables (e.g., different lighting conditions, different glasses).The main idea is that (i) we use the variation dictionary to characterize thelinear nuisance variables via the sparsity framework, then (ii) prototype faceimages are estimated as a gallery dictionary via a Gaussian Mixture Model(GMM), with mixed labeled and unlabeled samples in a semi-supervised manner, todeal with the non-linear nuisance variations between labeled and unlabeledsamples. We have done experiments with insufficient labeled samples, even whenthere is only a single labeled sample per person. Our results on the AR,Multi-PIE, CAS-PEAL, and LFW databases demonstrate that the proposed method isable to deliver significantly improved performance over existing methods.

A Fixed-Point Model for Pancreas Segmentation in Abdominal CT Scans

  Deep neural networks have been widely adopted for automatic organsegmentation from abdominal CT scans. However, the segmentation accuracy ofsome small organs (e.g., the pancreas) is sometimes below satisfaction,arguably because deep networks are easily disrupted by the complex and variablebackground regions which occupies a large fraction of the input volume. In thispaper, we formulate this problem into a fixed-point model which uses apredicted segmentation mask to shrink the input region. This is motivated bythe fact that a smaller input region often leads to more accurate segmentation.In the training process, we use the ground-truth annotation to generateaccurate input regions and optimize network weights. On the testing stage, wefix the network parameters and update the segmentation results in an iterativemanner. We evaluate our approach on the NIH pancreas segmentation dataset, andoutperform the state-of-the-art by more than 4%, measured by the averageDice-S{\o}rensen Coefficient (DSC). In addition, we report 62.43% DSC in theworst case, which guarantees the reliability of our approach in clinicalapplications.

Regularizing Face Verification Nets For Pain Intensity Regression

  Limited labeled data are available for the research of estimating facialexpression intensities. For instance, the ability to train deep networks forautomated pain assessment is limited by small datasets with labels ofpatient-reported pain intensities. Fortunately, fine-tuning from adata-extensive pre-trained domain, such as face verification, can alleviatethis problem. In this paper, we propose a network that fine-tunes astate-of-the-art face verification network using a regularized regression lossand additional data with expression labels. In this way, the expressionintensity regression task can benefit from the rich feature representationstrained on a huge amount of data for face verification. The proposedregularized deep regressor is applied to estimate the pain expression intensityand verified on the widely-used UNBC-McMaster Shoulder-Pain dataset, achievingthe state-of-the-art performance. A weighted evaluation metric is also proposedto address the imbalance issue of different pain intensities.

Multi-Context Attention for Human Pose Estimation

  In this paper, we propose to incorporate convolutional neural networks with amulti-context attention mechanism into an end-to-end framework for human poseestimation. We adopt stacked hourglass networks to generate attention maps fromfeatures at multiple resolutions with various semantics. The Conditional RandomField (CRF) is utilized to model the correlations among neighboring regions inthe attention map. We further combine the holistic attention model, whichfocuses on the global consistency of the full human body, and the body partattention model, which focuses on the detailed description for different bodyparts. Hence our model has the ability to focus on different granularity fromlocal salient regions to global semantic-consistent spaces. Additionally, wedesign novel Hourglass Residual Units (HRUs) to increase the receptive field ofthe network. These units are extensions of residual units with a side branchincorporating filters with larger receptive fields, hence features with variousscales are learned and combined within the HRUs. The effectiveness of theproposed multi-context attention mechanism and the hourglass residual units isevaluated on two widely used human pose estimation benchmarks. Our approachoutperforms all existing methods on both benchmarks over all the body parts.

NDDR-CNN: Layerwise Feature Fusing in Multi-Task CNNs by Neural  Discriminative Dimensionality Reduction

  In this paper, we propose a novel Convolutional Neural Network (CNN)structure for general-purpose multi-task learning (MTL), which enablesautomatic feature fusing at every layer from different tasks. This is incontrast with the most widely used MTL CNN structures which empirically orheuristically share features on some specific layers (e.g., share all thefeatures except the last convolutional layer). The proposed layerwise featurefusing scheme is formulated by combining existing CNN components in a novelway, with clear mathematical interpretability as discriminative dimensionalityreduction, which is referred to as Neural Discriminative DimensionalityReduction (NDDR). Specifically, we first concatenate features with the samespatial resolution from different tasks according to their channel dimension.Then, we show that the discriminative dimensionality reduction can be fulfilledby 1x1 Convolution, Batch Normalization, and Weight Decay in one CNN. The useof existing CNN components ensures the end-to-end training and theextensibility of the proposed NDDR layer to various state-of-the-art CNNarchitectures in a "plug-and-play" manner. The detailed ablation analysis showsthat the proposed NDDR layer is easy to train and also robust to differenthyperparameters. Experiments on different task sets with various base networkarchitectures demonstrate the promising performance and desirablegeneralizability of our proposed method. The code of our paper is available athttps://github.com/ethanygao/NDDR-CNN.

Semi-Supervised Multi-Organ Segmentation via Deep Multi-Planar  Co-Training

  In multi-organ segmentation of abdominal CT scans, most existing fullysupervised deep learning algorithms require lots of voxel-wise annotations,which are usually difficult, expensive, and slow to obtain. In comparison,massive unlabeled 3D CT volumes are usually easily accessible. Currentmainstream works to address the semi-supervised biomedical image segmentationproblem are mostly graph-based. By contrast, deep network based semi-supervisedlearning methods have not drawn much attention in this field. In this work, wepropose Deep Multi-Planar Co-Training (DMPCT), whose contributions can bedivided into two folds: 1) The deep model is learned in a co-training stylewhich can mine consensus information from multiple planes like the sagittal,coronal, and axial planes; 2) Multi-planar fusion is applied to generate morereliable pseudo-labels, which alleviates the errors occurring in thepseudo-labels and thus can help to train better segmentation networks.Experiments are done on our newly collected large dataset with 100 unlabeledcases as well as 210 labeled cases where 16 anatomical structures are manuallyannotated by four radiologists and confirmed by a senior expert. The resultssuggest that DMPCT significantly outperforms the fully supervised method bymore than 4% especially when only a small set of annotations is used.

Progressive Recurrent Learning for Visual Recognition

  Computer vision is difficult, partly because the mathematical functionconnecting input and output data is often complex, fuzzy and thus hard tolearn. A currently popular solution is to design a deep neural network andoptimize it on a large-scale dataset. However, as the number of parametersincreases, the generalization ability is often not guaranteed, e.g., the modelcan over-fit due to the limited amount of training data, or fail to convergebecause the desired function is too difficult to learn. This paper presents aneffective framework named progressive recurrent learning (PRL). The core ideais similar to curriculum learning which gradually increases the difficulty oftraining data. We generalize it to a wide range of vision problems that werepreviously considered less proper to apply curriculum learning. PRL starts withinserting a recurrent prediction scheme, based on the motivation of feeding theprediction of a vision model to the same model iteratively, so that theauxiliary cues contained in it can be exploited to improve the quality ofitself. In order to better optimize this framework, we start with providingperfect prediction, i.e., ground-truth, to the second stage, but graduallyreplace it with the prediction of the first stage. In the final status, theground-truth information is not needed any more, so that the entire model workson the real data distribution as in the testing process. We apply PRL to twochallenging visual recognition tasks, namely, object localization and semanticsegmentation, and demonstrate consistent accuracy gain compared to the baselinetraining strategy, especially in the scenarios of more difficult vision tasks.

Elastic Boundary Projection for 3D Medical Imaging Segmentation

  We focus on an important yet challenging problem: using a 2D deep network todeal with 3D segmentation for medical imaging analysis. Existing approacheseither applied multi-view planar (2D) networks or directly used volumetric (3D)networks for this purpose, but both of them are not ideal: 2D networks cannotcapture 3D contexts effectively, and 3D networks are both memory-consuming andless stable arguably due to the lack of pre-trained models.  In this paper, we bridge the gap between 2D and 3D using a novel approachnamed Elastic Boundary Projection (EBP). The key observation is that, althoughthe object is a 3D volume, what we really need in segmentation is to find itsboundary which is a 2D surface. Therefore, we place a number of pivot points inthe 3D space, and for each pivot, we determine its distance to the objectboundary along a dense set of directions. This creates an elastic shell aroundeach pivot which is initialized as a perfect sphere. We train a 2D deep networkto determine whether each ending point falls within the object, and graduallyadjust the shell so that it gradually converges to the actual shape of theboundary and thus achieves the goal of segmentation. EBP allows 3D segmentationwithout cutting the volume into slices or small patches, which stands out fromconventional 2D and 3D approaches. EBP achieves promising accuracy insegmenting several abdominal organs from CT scans.

Towards Accurate Task Accomplishment with Low-Cost Robotic Arms

  Training a robotic arm to accomplish real-world tasks has been attractingincreasing attention in both academia and industry. This work discusses therole of computer vision algorithms in this field. We focus on low-cost arms onwhich no sensors are equipped and thus all decisions are made upon visualrecognition, e.g., real-time 3D pose estimation. This requires annotating a lotof training data, which is not only time-consuming but also laborious.  In this paper, we present an alternative solution, which uses a 3D model tocreate a large number of synthetic data, trains a vision model in this virtualdomain, and applies it to real-world images after domain adaptation. To thisend, we design a semi-supervised approach, which fully leverages the geometricconstraints among keypoints. We apply an iterative algorithm for optimization.Without any annotations on real images, our algorithm generalizes well andproduces satisfying results on 3D pose estimation, which is evaluated on tworeal-world datasets. We also construct a vision-based control system for taskaccomplishment, for which we train a reinforcement learning agent in a virtualenvironment and apply it to the real-world. Moreover, our approach, with merelya 3D model being required, has the potential to generalize to other types ofmulti-rigid-body dynamic systems.

Regional Homogeneity: Towards Learning Transferable Universal  Adversarial Perturbations Against Defenses

  This paper focuses on learning transferable adversarial examples specificallyagainst defense models (models to defense adversarial attacks). In particular,we show that a simple universal perturbation can fool a series ofstate-of-the-art defenses.  Adversarial examples generated by existing attacks are generally hard totransfer to defense models. We observe the property of regional homogeneity inadversarial perturbations and suggest that the defenses are less robust toregionally homogeneous perturbations. Therefore, we propose an effectivetransforming paradigm and a customized gradient transformer module to transformexisting perturbations into regionally homogeneous ones. Without explicitlyforcing the perturbations to be universal, we observe that a well-trainedgradient transformer module tends to output input-independent gradients (henceuniversal) benefiting from the under-fitting phenomenon. Thorough experimentsdemonstrate that our work significantly outperforms the prior art attackingalgorithms (either image-dependent or universal ones) by an average improvementof 14.0% when attacking 9 defenses in the black-box setting. In addition to thecross-model transferability, we also verify that regionally homogeneousperturbations can well transfer across different vision tasks (attacking withthe semantic segmentation task and testing on the object detection task).

Zoom Better to See Clearer: Human and Object Parsing with Hierarchical  Auto-Zoom Net

  Parsing articulated objects, e.g. humans and animals, into semantic parts(e.g. body, head and arms, etc.) from natural images is a challenging andfundamental problem for computer vision. A big difficulty is the largevariability of scale and location for objects and their corresponding parts.Even limited mistakes in estimating scale and location will degrade the parsingoutput and cause errors in boundary details. To tackle these difficulties, wepropose a "Hierarchical Auto-Zoom Net" (HAZN) for object part parsing whichadapts to the local scales of objects and parts. HAZN is a sequence of two"Auto-Zoom Net" (AZNs), each employing fully convolutional networks thatperform two tasks: (1) predict the locations and scales of object instances(the first AZN) or their parts (the second AZN); (2) estimate the part scoresfor predicted object instance or part regions. Our model can adaptively "zoom"(resize) predicted image regions into their proper scales to refine theparsing.  We conduct extensive experiments over the PASCAL part datasets on humans,horses, and cows. For humans, our approach significantly outperforms thestate-of-the-arts by 5% mIOU and is especially better at segmenting smallinstances and small parts. We obtain similar improvements for parsing cows andhorses over alternative methods. In summary, our strategy of first zooming intoobjects and then zooming into parts is very effective. It also enables us toprocess different regions of the image at different scales adaptively so that,for example, we do not need to waste computational resources scaling the entireimage.

Recurrent Saliency Transformation Network: Incorporating Multi-Stage  Visual Cues for Small Organ Segmentation

  We aim at segmenting small organs (e.g., the pancreas) from abdominal CTscans. As the target often occupies a relatively small region in the inputimage, deep neural networks can be easily confused by the complex and variablebackground. To alleviate this, researchers proposed a coarse-to-fine approach,which used prediction from the first (coarse) stage to indicate a smaller inputregion for the second (fine) stage. Despite its effectiveness, this algorithmdealt with two stages individually, which lacked optimizing a global energyfunction, and limited its ability to incorporate multi-stage visual cues.Missing contextual information led to unsatisfying convergence in iterations,and that the fine stage sometimes produced even lower segmentation accuracythan the coarse stage.  This paper presents a Recurrent Saliency Transformation Network. The keyinnovation is a saliency transformation module, which repeatedly converts thesegmentation probability map from the previous iteration as spatial weights andapplies these weights to the current iteration. This brings us two-foldbenefits. In training, it allows joint optimization over the deep networksdealing with different input scales. In testing, it propagates multi-stagevisual information throughout iterations to improve segmentation accuracy.Experiments in the NIH pancreas segmentation dataset demonstrate thestate-of-the-art accuracy, which outperforms the previous best by an average ofover 2%. Much higher accuracies are also reported on several small organs in alarger dataset collected by ourselves. In addition, our approach enjoys betterconvergence properties, making it more efficient and reliable in practice.

DeepVoting: A Robust and Explainable Deep Network for Semantic Part  Detection under Partial Occlusion

  In this paper, we study the task of detecting semantic parts of an object,e.g., a wheel of a car, under partial occlusion. We propose that all modelsshould be trained without seeing occlusions while being able to transfer thelearned knowledge to deal with occlusions. This setting alleviates thedifficulty in collecting an exponentially large dataset to cover occlusionpatterns and is more essential. In this scenario, the proposal-based deepnetworks, like RCNN-series, often produce unsatisfactory results, because boththe proposal extraction and classification stages may be confused by theirrelevant occluders. To address this, [25] proposed a voting mechanism thatcombines multiple local visual cues to detect semantic parts. The semanticparts can still be detected even though some visual cues are missing due toocclusions. However, this method is manually-designed, thus is hard to beoptimized in an end-to-end manner.  In this paper, we present DeepVoting, which incorporates the robustness shownby [25] into a deep network, so that the whole pipeline can be jointlyoptimized. Specifically, it adds two layers after the intermediate features ofa deep network, e.g., the pool-4 layer of VGGNet. The first layer extracts theevidence of local visual cues, and the second layer performs a voting mechanismby utilizing the spatial relationship between visual cues and semantic parts.We also propose an improved version DeepVoting+ by learning visual cues fromcontext outside objects. In experiments, DeepVoting achieves significantlybetter performance than several baseline methods, including Faster-RCNN, forsemantic part detection under occlusion. In addition, DeepVoting enjoysexplainability as the detection results can be diagnosed via looking up thevoting cues.

Adversarial Attacks Beyond the Image Space

  Generating adversarial examples is an intriguing problem and an important wayof understanding the working mechanism of deep neural networks. Most existingapproaches generated perturbations in the image space, i.e., each pixel can bemodified independently. However, in this paper we pay special attention to thesubset of adversarial examples that correspond to meaningful changes in 3Dphysical properties (like rotation and translation, illumination condition,etc.). These adversaries arguably pose a more serious concern, as theydemonstrate the possibility of causing neural network failure by easyperturbations of real-world 3D objects and scenes.  In the contexts of object classification and visual question answering, weaugment state-of-the-art deep neural networks that receive 2D input images witha rendering module (either differentiable or not) in front, so that a 3D scene(in the physical space) is rendered into a 2D image (in the image space), andthen mapped to a prediction (in the output space). The adversarialperturbations can now go beyond the image space, and have clear meanings in the3D physical world. Though image-space adversaries can be interpreted asper-pixel albedo change, we verify that they cannot be well explained alongthese physically meaningful dimensions, which often have a non-local effect.But it is still possible to successfully attack beyond the image space on thephysical space, though this is more difficult than image-space attacks,reflected in lower success rates and heavier perturbations required.

Abdominal multi-organ segmentation with organ-attention networks and  statistical fusion

  Accurate and robust segmentation of abdominal organs on CT is essential formany clinical applications such as computer-aided diagnosis and computer-aidedsurgery. But this task is challenging due to the weak boundaries of organs, thecomplexity of the background, and the variable sizes of different organs. Toaddress these challenges, we introduce a novel framework for multi-organsegmentation by using organ-attention networks with reverse connections(OAN-RCs) which are applied to 2D views, of the 3D CT volume, and outputestimates which are combined by statistical fusion exploiting structuralsimilarity. OAN is a two-stage deep convolutional network, where deep networkfeatures from the first stage are combined with the original image, in a secondstage, to reduce the complex background and enhance the discriminativeinformation for the target organs. RCs are added to the first stage to give thelower layers semantic information thereby enabling them to adapt to the sizesof different organs. Our networks are trained on 2D views enabling us to useholistic information and allowing efficient computation. To compensate for thelimited cross-sectional information of the original 3D volumetric CT,multi-sectional images are reconstructed from the three different 2D viewdirections. Then we combine the segmentation results from the different viewsusing statistical fusion, with a novel term relating the structural similarityof the 2D views to the original 3D structure. To train the network and evaluateresults, 13 structures were manually annotated by four human raters andconfirmed by a senior expert on 236 normal cases. We tested our algorithm andcomputed Dice-Sorensen similarity coefficients and surface distances forevaluating our estimates of the 13 structures. Our experiments show that theproposed approach outperforms 2D- and 3D-patch based state-of-the-art methods.

Iterative Reorganization with Weak Spatial Constraints: Solving  Arbitrary Jigsaw Puzzles for Unsupervised Representation Learning

  Learning visual features from unlabeled image data is an important yetchallenging task, which is often achieved by training a model on someannotation-free information. We consider spatial contexts, for which we solveso-called jigsaw puzzles, i.e., each image is cut into grids and thendisordered, and the goal is to recover the correct configuration. Existingapproaches formulated it as a classification task by defining a fixed mappingfrom a small subset of configurations to a class set, but these approachesignore the underlying relationship between different configurations and alsolimit their application to more complex scenarios. This paper presents a novelapproach which applies to jigsaw puzzles with an arbitrary grid size anddimensionality. We provide a fundamental and generalized principle, that weakercues are easier to be learned in an unsupervised manner and also transferbetter. In the context of puzzle recognition, we use an iterative manner which,instead of solving the puzzle all at once, adjusts the order of the patches ineach step until convergence. In each step, we combine both unary and binaryfeatures on each patch into a cost function judging the correctness of thecurrent configuration. Our approach, by taking similarity between puzzles intoconsideration, enjoys a more reasonable way of learning visual knowledge. Weverify the effectiveness of our approach in two aspects. First, it is able tosolve arbitrarily complex puzzles, including high-dimensional puzzles, thatprior methods are difficult to handle. Second, it serves as a reliable way ofnetwork initialization, which leads to better transfer performance in a fewvisual recognition tasks including image classification, object detection, andsemantic segmentation.

