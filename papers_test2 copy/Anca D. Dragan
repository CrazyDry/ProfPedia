Robot Planning with Mathematical Models of Human State and Action

  Robots interacting with the physical world plan with models of physics. Weadvocate that robots interacting with people need to plan with models ofcognition. This writeup summarizes the insights we have gained in integratingcomputational cognitive models of people into robotics planning and control. Itstarts from a general game-theoretic formulation of interaction, and analyzeshow different approximations result in different useful coordination behaviorsfor the robot during its interaction with people.

Generating Plans that Predict Themselves

  Collaboration requires coordination, and we coordinate by anticipating ourteammates' future actions and adapting to their plan. In some cases, ourteammates' actions early on can give us a clear idea of what the remainder oftheir plan is, i.e. what action sequence we should expect. In others, theymight leave us less confident, or even lead us to the wrong conclusion. Ourgoal is for robot actions to fall in the first category: we want to enablerobots to select their actions in such a way that human collaborators caneasily use them to correctly anticipate what will follow. While previous workhas focused on finding initial plans that convey a set goal, here we focus onfinding two portions of a plan such that the initial portion conveys the finalone. We introduce $t$-\ACty{}: a measure that quantifies the accuracy andconfidence with which human observers can predict the remaining robot plan fromthe overall task goal and the observed initial $t$ actions in the plan. Wecontribute a method for generating $t$-predictable plans: we search for a fullplan that accomplishes the task, but in which the first $t$ actions make it aseasy as possible to infer the remaining ones. The result is often differentfrom the most efficient plan, in which the initial actions might leave a lot ofambiguity as to how the task will be completed. Through an online experimentand an in-person user study with physical robots, we find that our approachoutperforms a traditional efficiency-based planner in objective and subjectivecollaboration metrics.

Goal Inference Improves Objective and Perceived Performance in  Human-Robot Collaboration

  The study of human-robot interaction is fundamental to the design and use ofrobotics in real-world applications. Robots will need to predict and adapt tothe actions of human collaborators in order to achieve good performance andimprove safety and end-user adoption. This paper evaluates a human-robotcollaboration scheme that combines the task allocation and motion levels ofreasoning: the robotic agent uses Bayesian inference to predict the next goalof its human partner from his or her ongoing motion, and re-plans its ownactions in real time. This anticipative adaptation is desirable in manypractical scenarios, where humans are unable or unwilling to take on thecognitive overhead required to explicitly communicate their intent to therobot. A behavioral experiment indicates that the combination of goal inferenceand dynamic task planning significantly improves both objective and perceivedperformance of the human-robot team. Participants were highly sensitive to thedifferences between robot behaviors, preferring to work with a robot thatadapted to their actions over one that did not.

Expressive Robot Motion Timing

  Our goal is to enable robots to \emph{time} their motion in a way that ispurposefully expressive of their internal states, making them more transparentto people. We start by investigating what types of states motion timing iscapable of expressing, focusing on robot manipulation and keeping the pathconstant while systematically varying the timing. We find that users naturallypick up on certain properties of the robot (like confidence), of the motion(like naturalness), or of the task (like the weight of the object that therobot is carrying). We then conduct a hypothesis-driven experiment to tease outthe directions and magnitudes of these effects, and use our findings to developcandidate mathematical models for how users make these inferences from thetiming. We find a strong correlation between the models and real user data,suggesting that robots can leverage these models to autonomously optimize thetiming of their motion to be expressive.

Learning from Richer Human Guidance: Augmenting Comparison-Based  Learning with Feature Queries

  We focus on learning the desired objective function for a robot. Althoughtrajectory demonstrations can be very informative of the desired objective,they can also be difficult for users to provide. Answers to comparison queries,asking which of two trajectories is preferable, are much easier for users, andhave emerged as an effective alternative. Unfortunately, comparisons are farless informative. We propose that there is much richer information that userscan easily provide and that robots ought to leverage. We focus on augmentingcomparisons with feature queries, and introduce a unified formalism fortreating all answers as observations about the true desired reward. We derivean active query selection algorithm, and test these queries in simulation andon real users. We find that richer, feature-augmented queries can extract moreinformation faster, leading to robots that better match user preferences intheir behavior.

Do You Want Your Autonomous Car To Drive Like You?

  With progress in enabling autonomous cars to drive safely on the road, it istime to start asking how they should be driving. A common answer is that theyshould be adopting their users' driving style. This makes the assumption thatusers want their autonomous cars to drive like they drive - aggressive driverswant aggressive cars, defensive drivers want defensive cars. In this paper, weput that assumption to the test. We find that users tend to prefer asignificantly more defensive driving style than their own. Interestingly, theyprefer the style they think is their own, even though their actual drivingstyle tends to be more aggressive. We also find that preferences do depend onthe specific driving scenario, opening the door for new ways of learningdriving style preference.

Probabilistically Safe Robot Planning with Confidence-Based Human  Predictions

  In order to safely operate around humans, robots can employ predictive modelsof human motion. Unfortunately, these models cannot capture the full complexityof human behavior and necessarily introduce simplifying assumptions. As aresult, predictions may degrade whenever the observed human behavior departsfrom the assumed structure, which can have negative implications for safety. Inthis paper, we observe that how "rational" human actions appear under aparticular model can be viewed as an indicator of that model's ability todescribe the human's current motion. By reasoning about this model confidencein a real-time Bayesian framework, we show that the robot can very quicklymodulate its predictions to become more uncertain when the model performspoorly. Building on recent work in provably-safe trajectory planning, weleverage these confidence-aware human motion predictions to generate assuredautonomous robot motion. Our new analysis combines worst-case tracking errorguarantees for the physical robot with probabilistic time-varying humanpredictions, yielding a quantitative, probabilistic safety certificate. Wedemonstrate our approach with a quadcopter navigating around a human.

An Efficient, Generalized Bellman Update For Cooperative Inverse  Reinforcement Learning

  Our goal is for AI systems to correctly identify and act according to theirhuman user's objectives. Cooperative Inverse Reinforcement Learning (CIRL)formalizes this value alignment problem as a two-player game between a humanand robot, in which only the human knows the parameters of the reward function:the robot needs to learn them as the interaction unfolds. Previous work showedthat CIRL can be solved as a POMDP, but with an action space size exponentialin the size of the reward parameter space. In this work, we exploit a specificproperty of CIRL---the human is a full information agent---to derive anoptimality-preserving modification to the standard Bellman update; this reducesthe complexity of the problem by an exponential factor and allows us to relaxCIRL's assumption of human rationality. We apply this update to a variety ofPOMDP solvers and find that it enables us to scale CIRL to non-trivialproblems, with larger reward parameter spaces, and larger action spaces forboth robot and human. In solutions to these larger problems, the human exhibitspedagogic (teaching) behavior, while the robot interprets it as such andattains higher value for the human.

Model Reconstruction from Model Explanations

  We show through theory and experiment that gradient-based explanations of amodel quickly reveal the model itself. Our results speak to a tension betweenthe desire to keep a proprietary model secret and the ability to offer modelexplanations. On the theoretical side, we give an algorithm that provablylearns a two-layer ReLU network in a setting where the algorithm may query thegradient of the model with respect to chosen inputs. The number of queries isindependent of the dimension and nearly optimal in its dependence on the modelsize. Of interest not only from a learning-theoretic perspective, this resulthighlights the power of gradients rather than labels as a learning primitive.Complementing our theory, we give effective heuristics for reconstructingmodels from gradient explanations that are orders of magnitude morequery-efficient than reconstruction attacks relying on prediction interfaces.

Courteous Autonomous Cars

  Typically, autonomous cars optimize for a combination of safety, efficiency,and driving quality. But as we get better at this optimization, we start seeingbehavior go from too conservative to too aggressive. The car's behavior exposesthe incentives we provide in its cost function. In this work, we argue for carsthat are not optimizing a purely selfish cost, but also try to be courteous toother interactive drivers. We formalize courtesy as a term in the objectivethat measures the increase in another driver's cost induced by the autonomouscar's behavior. Such a courtesy term enables the robot car to be aware ofpossible irrationality of the human behavior, and plan accordingly. We analyzethe effect of courtesy in a variety of scenarios. We find, for example, thatcourteous robot cars leave more space when merging in front of a human driver.Moreover, we find that such a courtesy term can help explain real human driverbehavior on the NGSIM dataset.

Social Cohesion in Autonomous Driving

  Autonomous cars can perform poorly for many reasons. They may have perceptionissues, incorrect dynamics models, be unaware of obscure rules of human trafficsystems, or follow certain rules too conservatively. Regardless of the exactfailure mode of the car, often human drivers around the car are behavingcorrectly. For example, even if the car does not know that it should pull overwhen an ambulance races by, other humans on the road will know and will pullover. We propose to make socially cohesive cars that leverage the behavior ofnearby human drivers to act in ways that are safer and more sociallyacceptable. The simple intuition behind our algorithm is that if all the humansare consistently behaving in a particular way, then the autonomous car probablyshould too. We analyze the performance of our algorithm in a variety ofscenarios and conduct a user study to assess people's attitudes towardssocially cohesive cars. We find that people are surprisingly tolerant ofmistakes that cohesive cars might make in order to get the benefits of drivingin a car with a safer, or even just more socially acceptable behavior.

The Social Cost of Strategic Classification

  Consequential decision-making typically incentivizes individuals to behavestrategically, tailoring their behavior to the specifics of the decision rule.A long line of work has therefore sought to counteract strategic behavior bydesigning more conservative decision boundaries in an effort to increaserobustness to the effects of strategic covariate shift. We show that theseefforts benefit the institutional decision maker at the expense of theindividuals being classified. Introducing a notion of social burden, we provethat any increase in institutional utility necessarily leads to a correspondingincrease in social burden. Moreover, we show that the negative externalities ofstrategic classification can disproportionately harm disadvantaged groups inthe population. Our results highlight that strategy-robustness must be weighedagainst considerations of social welfare and fairness.

Cost Functions for Robot Motion Style

  We focus on autonomously generating robot motion for day to day physicaltasks that is expressive of a certain style or emotion. Because we seekgeneralization across task instances and task types, we propose to capturestyle via cost functions that the robot can use to augment its nominal taskcost and task constraints in a trajectory optimization process. We compare twoapproaches to representing such cost functions: a weighted linear combinationof hand-designed features, and a neural network parameterization operating onraw trajectory input. For each cost type, we learn weights for each style fromuser feedback. We contrast these approaches to a nominal motion acrossdifferent tasks and for different styles in a user study, and find that theyboth perform on par with each other, and significantly outperform the baseline.Each approach has its advantages: featurized costs require learning fewerparameters and can perform better on some styles, but neural networkrepresentations do not require expert knowledge to design features and couldeven learn more complex, nuanced costs than an expert can easily design.

Expressing Robot Incapability

  Our goal is to enable robots to express their incapability, and to do so in away that communicates both what they are trying to accomplish and why they areunable to accomplish it. We frame this as a trajectory optimization problem:maximize the similarity between the motion expressing incapability and whatwould amount to successful task execution, while obeying the physical limits ofthe robot. We introduce and evaluate candidate similarity measures, and showthat one in particular generalizes to a range of tasks, while producingexpressive motions that are tailored to each task. Our user study supports thatour approach automatically generates motions expressing incapability thatcommunicate both what and why to end-users, and improve their overallperception of the robot and willingness to collaborate with it in the future.

Establishing Appropriate Trust via Critical States

  In order to effectively interact with or supervise a robot, humans need tohave an accurate mental model of its capabilities and how it acts. Learnedneural network policies make that particularly challenging. We propose anapproach for helping end-users build a mental model of such policies. Our keyobservation is that for most tasks, the essence of the policy is captured in afew critical states: states in which it is very important to take a certainaction. Our user studies show that if the robot shows a human what itsunderstanding of the task's critical states is, then the human can make a moreinformed decision about whether to deploy the policy, and if she does deployit, when she needs to take control from it at execution time.

A Scalable Framework For Real-Time Multi-Robot, Multi-Human Collision  Avoidance

  Robust motion planning is a well-studied problem in the robotics literature,yet current algorithms struggle to operate scalably and safely in the presenceof other moving agents, such as humans. This paper introduces a novel frameworkfor robot navigation that accounts for high-order system dynamics and maintainssafety in the presence of external disturbances, other robots, andnon-deterministic intentional agents. Our approach precomputes a tracking errormargin for each robot, generates confidence-aware human motion predictions, andcoordinates multiple robots with a sequential priority ordering, effectivelyenabling scalable safe trajectory planning and execution. We demonstrate ourapproach in hardware with two robots and two humans. We also showcase ourwork's scalability in a larger simulation.

Learning from Extrapolated Corrections

  Our goal is to enable robots to learn cost functions from user guidance.Often it is difficult or impossible for users to provide full demonstrations,so corrections have emerged as an easier guidance channel. However, when robotslearn cost functions from corrections rather than demonstrations, they have toextrapolate a small amount of information -- the change of a waypoint along theway -- to the rest of the trajectory. We cast this extrapolation problem asonline function approximation, which exposes different ways in which the robotcan interpret what trajectory the person intended, depending on the functionspace used for the approximation. Our simulation results and user study suggestthat using function spaces with non-Euclidean norms can better capture whatusers intend, particularly if environments are uncluttered. This, in turn, canlead to the robot learning a more accurate cost function and improves theuser's subjective perceptions of the robot.

Literal or Pedagogic Human? Analyzing Human Model Misspecification in  Objective Learning

  It is incredibly easy for a system designer to misspecify the objective foran autonomous system ("robot''), thus motivating the desire to have the robotlearn the objective from human behavior instead. Recent work has suggested thatpeople have an interest in the robot performing well, and will thus behavepedagogically, choosing actions that are informative to the robot. In turn,robots benefit from interpreting the behavior by accounting for this pedagogy.In this work, we focus on misspecification: we argue that robots might not knowwhether people are being pedagogic or literal and that it is important to askwhich assumption is safer to make. We cast objective learning into the moregeneral form of a common-payoff game between the robot and human, and provethat in any such game literal interpretation is more robust tomisspecification. Experiments with human data support our theoretical resultsand point to the sensitivity of the pedagogic assumption.

Enabling Robots to Communicate their Objectives

  The overarching goal of this work is to efficiently enable end-users tocorrectly anticipate a robot's behavior in novel situations. Since a robot'sbehavior is often a direct result of its underlying objective function, ourinsight is that end-users need to have an accurate mental model of thisobjective function in order to understand and predict what the robot will do.While people naturally develop such a mental model over time through observingthe robot act, this familiarization process may be lengthy. Our approachreduces this time by having the robot model how people infer objectives fromobserved behavior, and then it selects those behaviors that are maximallyinformative. The problem of computing a posterior over objectives from observedbehavior is known as Inverse Reinforcement Learning (IRL), and has been appliedto robots learning human objectives. We consider the problem where the roles ofhuman and robot are swapped. Our main contribution is to recognize that unlikerobots, humans will not be exact in their IRL inference. We thus introduce twofactors to define candidate approximate-inference models for human learning inthis setting, and analyze them in a user study in the autonomous drivingdomain. We show that certain approximate-inference models lead to the robotgenerating example behaviors that better enable users to anticipate what itwill do in novel situations. Our results also suggest, however, that additionalresearch is needed in modeling how humans extrapolate from examples of robotbehavior.

Pragmatic-Pedagogic Value Alignment

  As intelligent systems gain autonomy and capability, it becomes vital toensure that their objectives match those of their human users; this is known asthe value-alignment problem. In robotics, value alignment is key to the designof collaborative robots that can integrate into human workflows, successfullyinferring and adapting to their users' objectives as they go. We argue that ameaningful solution to value alignment must combine multi-agent decision theorywith rich mathematical models of human cognition, enabling robots to tap intopeople's natural collaborative capabilities. We present a solution to thecooperative inverse reinforcement learning (CIRL) dynamic game based onwell-established cognitive models of decision making and theory of mind. Thesolution captures a key reciprocity relation: the human will not plan heractions in isolation, but rather reason pedagogically about how the robot mightlearn from them; the robot, in turn, can anticipate this and interpret thehuman's actions pragmatically. To our knowledge, this work constitutes thefirst formal analysis of value alignment grounded in empirically validatedcognitive models.

Where Do You Think You're Going?: Inferring Beliefs about Dynamics from  Behavior

  Inferring intent from observed behavior has been studied extensively withinthe frameworks of Bayesian inverse planning and inverse reinforcement learning.These methods infer a goal or reward function that best explains the actions ofthe observed agent, typically a human demonstrator. Another agent can use thisinferred intent to predict, imitate, or assist the human user. However, acentral assumption in inverse reinforcement learning is that the demonstratoris close to optimal. While models of suboptimal behavior exist, they typicallyassume that suboptimal actions are the result of some type of random noise or aknown cognitive bias, like temporal inconsistency. In this paper, we take analternative approach, and model suboptimal behavior as the result of internalmodel misspecification: the reason that user actions might deviate fromnear-optimal actions is that the user has an incorrect set of beliefs about therules -- the dynamics -- governing how actions affect the environment. Ourinsight is that while demonstrated actions may be suboptimal in the real world,they may actually be near-optimal with respect to the user's internal model ofthe dynamics. By estimating these internal beliefs from observed behavior, wearrive at a new method for inferring intent. We demonstrate in simulation andin a user study with 12 participants that this approach enables us to moreaccurately model human intent, and can be used in a variety of applications,including offering assistance in a shared autonomy framework and inferringhuman preferences.

Simplifying Reward Design through Divide-and-Conquer

  Designing a good reward function is essential to robot planning andreinforcement learning, but it can also be challenging and frustrating. Thereward needs to work across multiple different environments, and that oftenrequires many iterations of tuning. We introduce a novel divide-and-conquerapproach that enables the designer to specify a reward separately for eachenvironment. By treating these separate reward functions as observations aboutthe underlying true reward, we derive an approach to infer a common rewardacross all environments. We conduct user studies in an abstract grid worlddomain and in a motion planning domain for a 7-DOF manipulator that measureuser effort and solution quality. We show that our method is faster, easier touse, and produces a higher quality solution than the typical method ofdesigning a reward jointly across all environments. We additionally conduct aseries of experiments that measure the sensitivity of these results todifferent properties of the reward design task, such as the number ofenvironments, the number of feasible solutions per environment, and thefraction of the total features that vary within each environment. We find thatindependent reward design outperforms the standard, joint, reward designprocess but works best when the design problem can be divided into simplersubproblems.

Learning under Misspecified Objective Spaces

  Learning robot objective functions from human input has become increasinglyimportant, but state-of-the-art techniques assume that the human's desiredobjective lies within the robot's hypothesis space. When this is not true, evenmethods that keep track of uncertainty over the objective fail because theyreason about which hypothesis might be correct, and not whether any of thehypotheses are correct. We focus specifically on learning from physical humancorrections during the robot's task execution, where not having a rich enoughhypothesis space leads to the robot updating its objective in ways that theperson did not actually intend. We observe that such corrections appearirrelevant to the robot, because they are not the best way of achieving any ofthe candidate objectives. Instead of naively trusting and learning from everyhuman interaction, we propose robots learn conservatively by reasoning in realtime about how relevant the human's correction is for the robot's hypothesisspace. We test our inference method in an experiment with human interactiondata, and demonstrate that this alleviates unintended learning in an in-personuser study with a 7DoF robot manipulator.

Hierarchical Game-Theoretic Planning for Autonomous Vehicles

  The actions of an autonomous vehicle on the road affect and are affected bythose of other drivers, whether overtaking, negotiating a merge, or avoiding anaccident. This mutual dependence, best captured by dynamic game theory, createsa strong coupling between the vehicle's planning and its predictions of otherdrivers' behavior, and constitutes an open problem with direct implications onthe safety and viability of autonomous driving technology. Unfortunately,dynamic games are too computationally demanding to meet the real-timeconstraints of autonomous driving in its continuous state and action space. Inthis paper, we introduce a novel game-theoretic trajectory planning algorithmfor autonomous driving, that enables real-time performance by hierarchicallydecomposing the underlying dynamic game into a long-horizon "strategic" gamewith simplified dynamics and full information structure, and a short-horizon"tactical" game with full dynamics and a simplified information structure. Thevalue of the strategic game is used to guide the tactical planning, implicitlyextending the planning horizon, pushing the local trajectory optimizationcloser to global solutions, and, most importantly, quantitatively accountingfor the autonomous vehicle and the human driver's ability and incentives toinfluence each other. In addition, our approach admits non-deterministic modelsof human decision-making, rather than relying on perfectly rationalpredictions. Our results showcase richer, safer, and more effective autonomousbehavior in comparison to existing techniques.

Human-AI Learning Performance in Multi-Armed Bandits

  People frequently face challenging decision-making problems in which outcomesare uncertain or unknown. Artificial intelligence (AI) algorithms exist thatcan outperform humans at learning such tasks. Thus, there is an opportunity forAI agents to assist people in learning these tasks more effectively. In thiswork, we use a multi-armed bandit as a controlled setting in which to explorethis direction. We pair humans with a selection of agents and observe how welleach human-agent team performs. We find that team performance can beat bothhuman and agent performance in isolation. Interestingly, we also find that anagent's performance in isolation does not necessarily correlate with thehuman-agent team's performance. A drop in agent performance can lead to adisproportionately large drop in team performance, or in some settings can evenimprove team performance. Pairing a human with an agent that performs slightlybetter than them can make them perform much better, while pairing them with anagent that performs the same can make them them perform much worse. Further,our results suggest that people have different exploration strategies and mightperform better with agents that match their strategy. Overall, optimizinghuman-agent team performance requires going beyond optimizing agentperformance, to understanding how the agent's suggestions will influence humandecision-making.

Shared Autonomy via Deep Reinforcement Learning

  In shared autonomy, user input is combined with semi-autonomous control toachieve a common goal. The goal is often unknown ex-ante, so prior work enablesagents to infer the goal from user input and assist with the task. Such methodstend to assume some combination of knowledge of the dynamics of theenvironment, the user's policy given their goal, and the set of possible goalsthe user might target, which limits their application to real-world scenarios.We propose a deep reinforcement learning framework for model-free sharedautonomy that lifts these assumptions. We use human-in-the-loop reinforcementlearning with neural network function approximation to learn an end-to-endmapping from environmental observation and user input to agent action values,with task reward as the only form of supervision. This approach poses thechallenge of following user commands closely enough to provide the user withreal-time action feedback and thereby ensure high-quality user input, but alsodeviating from the user's actions when they are suboptimal. We balance thesetwo needs by discarding actions whose values fall below some threshold, thenselecting the remaining action closest to the user's input. Controlled studieswith users (n = 12) and synthetic pilots playing a video game, and a pilotstudy with users (n = 4) flying a real quadrotor, demonstrate the ability ofour algorithm to assist users with real-time control tasks in which the agentcannot directly access the user's private information through observations, butreceives a reward signal and user input that both depend on the user's intent.The agent learns to assist the user without access to this private information,implicitly inferring it from the user's input. This paper is a proof of conceptthat illustrates the potential for deep reinforcement learning to enableflexible and practical assistive systems.

