Complexity Measures for Map-Reduce, and Comparison to Parallel Computing

  The programming paradigm Map-Reduce and its main open-source implementation,Hadoop, have had an enormous impact on large scale data processing. Our goal inthis expository writeup is two-fold: first, we want to present some complexitymeasures that allow us to talk about Map-Reduce algorithms formally, andsecond, we want to point out why this model is actually different from othermodels of parallel programming, most notably the PRAM (Parallel Random AccessMemory) model. We are looking for complexity measures that are detailed enoughto make fine-grained distinction between different algorithms, but which alsoabstract away many of the implementation details.

Lower Bounds for Embedding into Distributions over Excluded Minor Graph  Families

  It was shown recently by Fakcharoenphol et al that arbitrary finite metricscan be embedded into distributions over tree metrics with distortion O(log n).It is also known that this bound is tight since there are expander graphs whichcannot be embedded into distributions over trees with better than Omega(log n)distortion.  We show that this same lower bound holds for embeddings into distributionsover any minor excluded family. Given a family of graphs F which excludes minorM where |M|=k, we explicitly construct a family of graphs with treewidth-(k+1)which cannot be embedded into a distribution over F with better than Omega(logn) distortion. Thus, while these minor excluded families of graphs are moreexpressive than trees, they do not provide asymptotically better approximationsin general. An important corollary of this is that graphs of treewidth-k cannotbe embedded into distributions over graphs of treewidth-(k-3) with distortionless than Omega(log n).  We also extend a result of Alon et al by showing that for any k, planargraphs cannot be embedded into distributions over treewidth-k graphs withbetter than Omega(log n) distortion.

Single pass sparsification in the streaming model with edge deletions

  In this paper we give a construction of cut sparsifiers of Benczur and Kargerin the {\em dynamic} streaming setting in a single pass over the data stream.Previous constructions either required multiple passes or were unable to handleedge deletions. We use $\tilde{O}(1/\e^2)$ time for each stream update and$\tilde{O}(n/\e^2)$ time to construct a sparsifier. Our $\e$-sparsifiers have$O(n\log^3 n/\e^2)$ edges. The main tools behind our result are an applicationof sketching techniques of Ahn et al.[SODA'12] to estimate edge connectivitytogether with a novel application of sampling with limited independence andsparse recovery to produce the edges of the sparsifier.

The Hemispheric Asymmetry of Solar Activity During the Twentieth Century  and the Solar Dynamo

  We believe the Babcock--Leighton process of poloidal field generation to bethe main source of irregularity in the solar cycle. The random nature of thisprocess may make the poloidal field in one hemisphere stronger than that in theother hemisphere at the end of a cycle. We expect this to induce an asymmetryin the next sunspot cycle. We look for evidence of this in the observationaldata and then model it theoretically with our dynamo code. Since actual polarfield measurements exist only from 1970s, we use the polar faculae number datarecorded by Sheeley (1991) as a proxy of the polar field and estimate thehemispheric asymmetry of the polar field in different solar minima during themajor part of the twentieth century. This asymmetry is found to have areasonable correlation with the asymmetry of the next cycle. We then run ourdynamo code by feeding information about this asymmetry at the successiveminima and compare with observational data. We find that the theoreticallycomputed asymmetries of different cycles compare favourably with theobservational data, the correlation coefficient being 0.73. Due to the couplingbetween the two hemispheres, any hemispheric asymmetry tends to get attenuatedwith time. The hemispheric asymmetry of a cycle either from observational dataor from theoretical calculation statistically tends to be less than theasymmetry in the polar field (as inferred from the faculae data) in thepreceding minimum. This reduction factor turns out to be 0.38 and 0.60respectively in observational data and theoretical simulation.

Hybrid Keyword Search Auctions

  Search auctions have become a dominant source of revenue generation on theInternet. Such auctions have typically used per-click bidding and pricing. Wepropose the use of hybrid auctions where an advertiser can make aper-impression as well as a per-click bid, and the auctioneer then chooses oneof the two as the pricing mechanism. We assume that the advertiser and theauctioneer both have separate beliefs (called priors) on the click-probabilityof an advertisement. We first prove that the hybrid auction is truthful,assuming that the advertisers are risk-neutral. We then show that this auctionis superior to the existing per-click auction in multiple ways: 1) It takesinto account the risk characteristics of the advertisers. 2) For obscurekeywords, the auctioneer is unlikely to have a very sharp prior on theclick-probabilities. In such situations, the hybrid auction can result insignificantly higher revenue. 3) An advertiser who believes that itsclick-probability is much higher than the auctioneer's estimate can useper-impression bids to correct the auctioneer's prior without incurring anyextra cost. 4) The hybrid auction can allow the advertiser and auctioneer toimplement complex dynamic programming strategies. As Internet commerce matures,we need more sophisticated pricing models to exploit all the information heldby each of the participants. We believe that hybrid auctions could be animportant step in this direction.

Perfect Matchings via Uniform Sampling in Regular Bipartite Graphs

  In this paper we further investigate the well-studied problem of finding aperfect matching in a regular bipartite graph. The first non-trivial algorithm,with running time $O(mn)$, dates back to K\"{o}nig's work in 1916 (here $m=nd$is the number of edges in the graph, $2n$ is the number of vertices, and $d$ isthe degree of each node). The currently most efficient algorithm takes time$O(m)$, and is due to Cole, Ost, and Schirra. We improve this running time to$O(\min\{m, \frac{n^{2.5}\ln n}{d}\})$; this minimum can never be larger than$O(n^{1.75}\sqrt{\ln n})$. We obtain this improvement by proving a uniformsampling theorem: if we sample each edge in a $d$-regular bipartite graphindependently with a probability $p = O(\frac{n\ln n}{d^2})$ then the resultinggraph has a perfect matching with high probability. The proof involves adecomposition of the graph into pieces which are guaranteed to have manyperfect matchings but do not have any small cuts. We then establish acorrespondence between potential witnesses to non-existence of a matching(after sampling) in any piece and cuts of comparable size in that same piece.Karger's sampling theorem for preserving cuts in a graph can now be adapted toprove our uniform sampling theorem for preserving perfect matchings. Using the$O(m\sqrt{n})$ algorithm (due to Hopcroft and Karp) for finding maximummatchings in bipartite graphs on the sampled graph then yields the statedrunning time. We also provide an infinite family of instances to show that ouruniform sampling result is tight up to poly-logarithmic factors (in fact, up to$\ln^2 n$).

The Ratio Index for Budgeted Learning, with Applications

  In the budgeted learning problem, we are allowed to experiment on a set ofalternatives (given a fixed experimentation budget) with the goal of picking asingle alternative with the largest possible expected payoff. Approximationalgorithms for this problem were developed by Guha and Munagala by rounding alinear program that couples the various alternatives together. In this paper wepresent an index for this problem, which we call the ratio index, which alsoguarantees a constant factor approximation. Index-based policies have theadvantage that a single number (i.e. the index) can be computed for eachalternative irrespective of all other alternatives, and the alternative withthe highest index is experimented upon. This is analogous to the famous Gittinsindex for the discounted multi-armed bandit problem.  The ratio index has several interesting structural properties. First, we showthat it can be computed in strongly polynomial time. Second, we show that withthe appropriate discount factor, the Gittins index and our ratio index areconstant factor approximations of each other, and hence the Gittins index alsogives a constant factor approximation to the budgeted learning problem.Finally, we show that the ratio index can be used to create an index-basedpolicy that achieves an O(1)-approximation for the finite horizon version ofthe multi-armed bandit problem. Moreover, the policy does not require anyknowledge of the horizon (whereas we compare its performance against an optimalstrategy that is aware of the horizon). This yields the following surprisingresult: there is an index-based policy that achieves an O(1)-approximation forthe multi-armed bandit problem, oblivious to the underlying discount factor.

Perfect Matchings in Ã•(n^{1.5}) Time in Regular Bipartite Graphs

  We consider the well-studied problem of finding a perfect matching in$d$-regular bipartite graphs with $2n$ vertices and $m = nd$ edges. While thebest-known algorithm for general bipartite graphs (due to Hopcroft and Karp)takes $O(m \sqrt{n})$ time, in regular bipartite graphs, a perfect matching isknown to be computable in $O(m)$ time. Very recently, the $O(m)$ bound wasimproved to $O(\min\{m, \frac{n^{2.5}\ln n}{d}\})$ expected time, an expressionthat is bounded by $\tilde{O}(n^{1.75})$. In this paper, we further improvethis result by giving an $O(\min\{m, \frac{n^2\ln^3 n}{d}\})$ expected timealgorithm for finding a perfect matching in regular bipartite graphs; as afunction of $n$ alone, the algorithm takes expected time $O((n\ln n)^{1.5})$.  To obtain this result, we design and analyze a two-stage sampling scheme thatreduces the problem of finding a perfect matching in a regular bipartite graphto the same problem on a subsampled bipartite graph with $O(n\ln n)$ edges thathas a perfect matching with high probability. The matching is then recoveredusing the Hopcroft-Karp algorithm. While the standard analysis of Hopcroft-Karpgives us an $\tilde{O}(n^{1.5})$ running time, we present a tighter analysisfor our special case that results in the stronger $\tilde{O}(\min\{m,\frac{n^2}{d} \})$ time mentioned earlier.  Our proof of correctness of this sampling scheme uses a new correspondencetheorem between cuts and Hall's theorem ``witnesses'' for a perfect matching ina bipartite graph that we prove. We believe this theorem may be of independentinterest; as another example application, we show that a perfect matching inthe support of an $n \times n$ doubly stochastic matrix with $m$ non-zeroentries can be found in expected time $\tilde{O}(m + n^{1.5})$.

One Tree Suffices: A Simultaneous O(1)-Approximation for Single-Sink  Buy-at-Bulk

  We study the single-sink buy-at-bulk problem with an unknown cost function.We wish to route flow from a set of demand nodes to a root node, where the costof routing x total flow along an edge is proportional to f(x) for some concave,non-decreasing function f satisfying f(0)=0. We present a simple, fast,combinatorial algorithm that takes a set of demands and constructs a singletree T such that for all f the cost f(T) is a 47.45-approximation of theoptimal cost for that f. This is within a factor of 2.33 of the bestapproximation ratio currently achievable when the tree can be optimized for aspecific function. Trees achieving simultaneous O(1)-approximations for allconcave functions were previously not known to exist regardless of computationtime.

Dimension Independent Similarity Computation

  We present a suite of algorithms for Dimension Independent SimilarityComputation (DISCO) to compute all pairwise similarities between very highdimensional sparse vectors. All of our results are provably independent ofdimension, meaning apart from the initial cost of trivially reading in thedata, all subsequent operations are independent of the dimension, thus thedimension can be very large. We study Cosine, Dice, Overlap, and the Jaccardsimilarity measures. For Jaccard similiarity we include an improved version ofMinHash. Our results are geared toward the MapReduce framework. We empiricallyvalidate our theorems at large scale using data from the social networking siteTwitter. At time of writing, our algorithms are live in production attwitter.com.

Biased Assimilation, Homophily and the Dynamics of Polarization

  Are we as a society getting more polarized, and if so, why? We try to answerthis question through a model of opinion formation. Empirical studies haveshown that homophily results in polarization. However, we show that DeGroot'swell-known model of opinion formation based on repeated averaging can never bepolarizing, even if individuals are arbitrarily homophilous. We generalizeDeGroot's model to account for a phenomenon well-known in social psychology asbiased assimilation: when presented with mixed or inconclusive evidence on acomplex issue, individuals draw undue support for their initial positionthereby arriving at a more extreme opinion. We show that in a simple model ofhomophilous networks, our biased opinion formation process results in eitherpolarization, persistent disagreement or consensus depending on how biasedindividuals are. In other words, homophily alone, without biased assimilation,is not sufficient to polarize society. Quite interestingly, biased assimilationalso provides insight into the following related question: do internet basedrecommender algorithms that show us personalized content contribute topolarization? We make a connection between biased assimilation and thepolarizing effects of some random-walk based recommender algorithms that aresimilar in spirit to some commonly used recommender algorithms.

Triadic Consensus: A Randomized Algorithm for Voting in a Crowd

  Typical voting rules do not work well in settings with many candidates. Ifthere are just several hundred candidates, then even a simple task such aschoosing a top candidate becomes impractical. Motivated by the hope ofdeveloping group consensus mechanisms over the internet, where the numbers ofcandidates could easily number in the thousands, we study an urn-based votingrule where each participant acts as a voter and a candidate. We prove that whenparticipants lie in a one-dimensional space, this voting protocol finds a$(1-\epsilon/sqrt{n})$ approximation of the Condorcet winner with highprobability while only requiring an expected $O(\frac{1}{\epsilon^2}\log^2\frac{n}{\epsilon^2})$ comparisons on average per voter. Moreover, this votingprotocol is shown to have a quasi-truthful Nash equilibrium: namely, a Nashequilibrium exists which may not be truthful, but produces a winner with thesame probability distribution as that of the truthful strategy.

Towards large-scale deliberative decision-making: small groups and the  importance of triads

  Though deliberation is a critical component of democratic decision-making,existing deliberative processes do not scale to large groups of people.Motivated by this, we propose a model in which large-scale decision-makingtakes place through a sequence of small group interactions. Our model considersa group of participants, each having an opinion which together form a graph. Weshow that for median graphs, a class of graphs including grids and trees, it ispossible to use a small number of three-person interactions to tightlyapproximate the wisdom of the crowd, defined here to be the generalized medianof participant opinions, even when agents are strategic. Interestingly, we alsoshow that this sharply contrasts with small groups of size two, for which weprove an impossibility result. Specifically, we show that it is impossible touse sequences of two-person interactions satisfying natural axioms to find atight approximation of the generalized median, even when agents arenon-strategic. Our results demonstrate the potential of small groupinteractions for reaching global decision-making properties.

Bidirectional PageRank Estimation: From Average-Case to Worst-Case

  We present a new algorithm for estimating the Personalized PageRank (PPR)between a source and target node on undirected graphs, with sublinearrunning-time guarantees over the worst-case choice of source and target nodes.Our work builds on a recent line of work on bidirectional estimators for PPR,which obtained sublinear running-time guarantees but in an average-case sense,for a uniformly random choice of target node. Crucially, we show how thereversibility of random walks on undirected networks can be exploited toconvert average-case to worst-case guarantees. While past bidirectional methodscombine forward random walks with reverse local pushes, our algorithm combinesforward local pushes with reverse random walks. We also discuss how to modifyour methods to estimate random-walk probabilities for any length distribution,thereby obtaining fast algorithms for estimating general graph diffusions,including the heat kernel, on undirected networks.

Pruning based Distance Sketches with Provable Guarantees on Random  Graphs

  Measuring the distances between vertices on graphs is one of the mostfundamental components in network analysis. Since finding shortest pathsrequires traversing the graph, it is challenging to obtain distance informationon large graphs very quickly. In this work, we present a preprocessingalgorithm that is able to create landmark based distance sketches efficiently,with strong theoretical guarantees. When evaluated on a diverse set of socialand information networks, our algorithm significantly improves over existingapproaches by reducing the number of landmarks stored, preprocessing time, orstretch of the estimated distances.  On Erd\"{o}s-R\'{e}nyi graphs and random power law graphs with degreedistribution exponent $2 < \beta < 3$, our algorithm outputs an exact distancedata structure with space between $\Theta(n^{5/4})$ and $\Theta(n^{3/2})$depending on the value of $\beta$, where $n$ is the number of vertices. Wecomplement the algorithm with tight lower bounds for Erdos-Renyi graphs and thecase when $\beta$ is close to two.

Markets Beyond Nash Welfare for Leontief Utilities

  We study the allocation of divisible goods to competing agents via a marketmechanism, focusing on agents with Leontief utilities. The majority of theeconomics and mechanism design literature has focused on \emph{linear} prices,meaning that the cost of a good is proportional to the quantity purchased.Equilibria for linear prices are known to be exactly the maximum Nash welfareallocations.  \emph{Price curves} allow the cost of a good to be any (increasing) functionof the quantity purchased. We show that price curve equilibria are not limitedto maximum Nash welfare allocations with two main results. First, we show thatan allocation can be supported by strictly increasing price curves if and onlyif it is \emph{group-domination-free}. A similarly characterization holds forweakly increasing price curves. We use this to show that given any allocation,we can compute strictly (or weakly) increasing price curves that support it (orshow that none exist) in polynomial time. These results involve a connection tothe \emph{agent-order matrix} of an allocation, which may have otherapplications. Second, we use duality to show that in the bandwidth allocationsetting, any allocation maximizing a CES welfare function can be supported byprice curves.

Implementing the Lexicographic Maxmin Bargaining Solution

  There has been much work on exhibiting mechanisms that implement variousbargaining solutions, in particular, the Kalai-Smorodinsky solution\cite{moulin1984implementing} and the Nash Bargaining solution. Anotherwell-known and axiomatically well-studied solution is the lexicographic maxminsolution. However, there is no mechanism known for its implementation. To fillthis gap, we construct a mechanism that implements the lexicographic maxminsolution as the unique subgame perfect equilibrium outcome in the n-playersetting. As is standard in the literature on implementation of bargainingsolutions, we use the assumption that any player can grab the entire surplus.Our mechanism consists of a binary game tree, with each node corresponding to asubgame where the players are allowed to choose between two outcomes. Wecharacterize novel combinatorial properties of the lexicographic maxminsolution which are crucial to the design of our mechanism.

Perfect Matchings in O(n \log n) Time in Regular Bipartite Graphs

  In this paper we consider the well-studied problem of finding a perfectmatching in a d-regular bipartite graph on 2n nodes with m=nd edges. Thebest-known algorithm for general bipartite graphs (due to Hopcroft and Karp)takes time O(m\sqrt{n}). In regular bipartite graphs, however, a matching isknown to be computable in O(m) time (due to Cole, Ost and Schirra). In a recentline of work by Goel, Kapralov and Khanna the O(m) time algorithm was improvedfirst to \tilde O(min{m, n^{2.5}/d}) and then to \tilde O(min{m, n^2/d}). Itwas also shown that the latter algorithm is optimal up to polylogarithmicfactors among all algorithms that use non-adaptive uniform sampling to reducethe size of the graph as a first step.  In this paper, we give a randomized algorithm that finds a perfect matchingin a d-regular graph and runs in O(n\log n) time (both in expectation and withhigh probability). The algorithm performs an appropriately truncated randomwalk on a modified graph to successively find augmenting paths. Our algorithmmay be viewed as using adaptive uniform sampling, and is thus able to bypassthe limitations of (non-adaptive) uniform sampling established in earlier work.We also show that randomization is crucial for obtaining o(nd) time algorithmsby establishing an \Omega(nd) lower bound for any deterministic algorithm. Ourtechniques also give an algorithm that successively finds a matching in thesupport of a doubly stochastic matrix in expected time O(n\log^2 n) time, withO(m) pre-processing time; this gives a simple O(m+mn\log^2 n) time algorithmfor finding the Birkhoff-von Neumann decomposition of a doubly stochasticmatrix.

Set K-Cover Algorithms for Energy Efficient Monitoring in Wireless  Sensor Networks

  Wireless sensor networks (WSNs) are emerging as an effective means forenvironment monitoring. This paper investigates a strategy for energy efficientmonitoring in WSNs that partitions the sensors into covers, and then activatesthe covers iteratively in a round-robin fashion. This approach takes advantageof the overlap created when many sensors monitor a single area. Our work buildsupon previous work in "Power Efficient Organization of Wireless SensorNetworks" by Slijepcevic and Potkonjak, where the model is first formulated. Wehave designed three approximation algorithms for a variation of the SET K-COVERproblem, where the objective is to partition the sensors into covers such thatthe number of covers that include an area, summed over all areas, is maximized.The first algorithm is randomized and partitions the sensors, in expectation,within a fraction 1 - 1/e (~.63) of the optimum. We present two otherdeterministic approximation algorithms. One is a distributed greedy algorithmwith a 1/2 approximation ratio and the other is a centralized greedy algorithmwith a 1 - 1/e approximation ratio. We show that it is NP-Complete to guaranteebetter than 15/16 of the optimal coverage, indicating that all three algorithmsperform well with respect to the best approximation algorithm possible.Simulations indicate that in practice, the deterministic algorithms perform farabove their worst case bounds, consistently covering more than 72% of what iscovered by an optimum solution. Simulations also indicate that the increase inlongevity is proportional to the amount of overlap amongst the sensors. Thealgorithms are fast, easy to use, and according to simulations, significantlyincrease the longevity of sensor networks. The randomized algorithm inparticular seems quite practical.

Monotone properties of random geometric graphs have sharp thresholds

  Random geometric graphs result from taking $n$ uniformly distributed pointsin the unit cube, $[0,1]^d$, and connecting two points if their Euclideandistance is at most $r$, for some prescribed $r$. We show that monotoneproperties for this class of graphs have sharp thresholds by reducing theproblem to bounding the bottleneck matching on two sets of $n$ pointsdistributed uniformly in $[0,1]^d$. We present upper bounds on the thresholdwidth, and show that our bound is sharp for $d=1$ and at most a sublogarithmicfactor away for $d\ge2$. Interestingly, the threshold width is much sharper forrandom geometric graphs than for Bernoulli random graphs. Further, a randomgeometric graph is shown to be a subgraph, with high probability, of anotherindependently drawn random geometric graph with a slightly larger radius; thisproperty is shown to have no analogue for Bernoulli random graphs.

An Oblivious O(1)-Approximation for Single Source Buy-at-Bulk

  We consider the single-source (or single-sink) buy-at-bulk problem with anunknown concave cost function. We want to route a set of demands along a graphto or from a designated root node, and the cost of routing x units of flowalong an edge is proportional to some concave, non-decreasing function f suchthat f(0) = 0. We present a polynomial time algorithm that finds a distributionover trees such that the expected cost of a tree for any f is within anO(1)-factor of the optimum cost for that f. The previous best simultaneousapproximation for this problem, even ignoring computation time, was O(log |D|),where D is the multi-set of demand nodes.  We design a simple algorithmic framework using the ellipsoid method thatfinds an O(1)-approximation if one exists, and then construct a separationoracle using a novel adaptation of the Guha, Meyerson, and Munagala algorithmfor the single-sink buy-at-bulk problem that proves an O(1) approximation ispossible for all f. The number of trees in the support of the distributionconstructed by our algorithm is at most 1+log |D|.

Widescope - A social platform for serious conversations on the Web

  There are several web platforms that people use to interact and exchangeideas, such as social networks like Facebook, Twitter, and Google+; Q&A siteslike Quora and Yahoo! Answers; and myriad independent fora. However, there is ascarcity of platforms that facilitate discussion of complex subjects wherepeople with divergent views can easily rationalize their points of view using ashared knowledge base, and leverage it towards shared objectives, e.g. toarrive at a mutually acceptable compromise.  In this paper, as a first step, we present Widescope, a novel collaborativeweb platform for catalyzing shared understanding of the US Federal and Statebudget debates in order to help users reach data-driven consensus about thecomplex issues involved. It aggregates disparate sources of financial data fromdifferent budgets (i.e. from past, present, and proposed) and presents aunified interface using interactive visualizations. It leverages distributedcollaboration to encourage exploration of ideas and debate. Users can proposebudgets ab-initio, support existing proposals, compare between differentbudgets, and collaborate with others in real time.  We hypothesize that such a platform can be useful in bringing people'sthoughts and opinions closer. Toward this, we present preliminary evidence froma simple pilot experiment, using triadic voting (which we also formally analyzeto show that is better than hot-or-not voting), that 5 out of 6 groups of userswith divergent views (conservatives vs liberals) come to a consensus whileaiming to halve the deficit using Widescope. We believe that tools likeWidescope could have a positive impact on other complex, data-driven socialissues.

Efficient Distributed Locality Sensitive Hashing

  Distributed frameworks are gaining increasingly widespread use inapplications that process large amounts of data. One important exampleapplication is large scale similarity search, for which Locality SensitiveHashing (LSH) has emerged as the method of choice, specially when the data ishigh-dimensional. At its core, LSH is based on hashing the data points to anumber of buckets such that similar points are more likely to map to the samebuckets. To guarantee high search quality, the LSH scheme needs a rather largenumber of hash tables. This entails a large space requirement, and in thedistributed setting, with each query requiring a network call per hash bucketlook up, this also entails a big network load. The Entropy LSH scheme proposedby Panigrahy significantly reduces the number of required hash tables bylooking up a number of query offsets in addition to the query itself. Whilethis improves the LSH space requirement, it does not help with (and in factworsens) the search network efficiency, as now each query offset requires anetwork call. In this paper, focusing on the Euclidian space under $l_2$ normand building up on Entropy LSH, we propose the distributed Layered LSH scheme,and prove that it exponentially decreases the network cost, while maintaining agood load balance between different machines. Our experiments also verify thatour scheme results in a significant network traffic reduction that brings aboutlarge runtime improvement in real world applications.

Personalized PageRank to a Target Node

  Personalalized PageRank uses random walks to determine the importance orauthority of nodes in a graph from the point of view of a given source node.Much past work has considered how to compute personalized PageRank from a givensource node to other nodes. In this work we consider the problem of computingpersonalized PageRanks to a given target node from all source nodes. Thisproblem can be interpreted as finding who supports the target or who isinterested in the target.  We present an efficient algorithm for computing personalized PageRank to agiven target up to any given accuracy. We give a simple analysis of ouralgorithm's running time in both the average case and the parameterizedworst-case. We show that for any graph with $n$ nodes and $m$ edges, if thetarget node is randomly chosen and the teleport probability $\alpha$ is given,the algorithm will compute a result with $\epsilon$ error in time$O\left(\frac{1}{\alpha \epsilon} \left(\frac{m}{n} + \log(n)\right)\right)$.This is much faster than the previously proposed method of computingpersonalized PageRank separately from every source node, and it is comparableto the cost of computing personalized PageRank from a single source. We presentresults from experiments on the Twitter graph which show that the constantfactors in our running time analysis are small and our algorithm is efficientin practice.

FAST-PPR: Scaling Personalized PageRank Estimation for Large Graphs

  We propose a new algorithm, FAST-PPR, for estimating personalized PageRank:given start node $s$ and target node $t$ in a directed graph, and given athreshold $\delta$, FAST-PPR estimates the Personalized PageRank $\pi_s(t)$from $s$ to $t$, guaranteeing a small relative error as long $\pi_s(t)>\delta$.Existing algorithms for this problem have a running-time of $\Omega(1/\delta)$;in comparison, FAST-PPR has a provable average running-time guarantee of${O}(\sqrt{d/\delta})$ (where $d$ is the average in-degree of the graph). Thisis a significant improvement, since $\delta$ is often $O(1/n)$ (where $n$ isthe number of nodes) for applications. We also complement the algorithm with an$\Omega(1/\sqrt{\delta})$ lower bound for PageRank estimation, showing that thedependence on $\delta$ cannot be improved.  We perform a detailed empirical study on numerous massive graphs, showingthat FAST-PPR dramatically outperforms existing algorithms. For example, on the2010 Twitter graph with 1.5 billion edges, for target nodes sampled bypopularity, FAST-PPR has a $20$ factor speedup over the state of the art.Furthermore, an enhanced version of FAST-PPR has a $160$ factor speedup on theTwitter graph, and is at least $20$ times faster on all our candidate graphs.

Crowdsourcing for Participatory Democracies: Efficient Elicitation of  Social Choice Functions

  We present theoretical and empirical results demonstrating the usefulness ofvoting rules for participatory democracies. We first give algorithms whichefficiently elicit \epsilon-approximations to two prominent voting rules: theBorda rule and the Condorcet winner. This result circumvents previousprohibitive lower bounds and is surprisingly strong: even if the number ofideas is as large as the number of participants, each participant will onlyhave to make a logarithmic number of comparisons, an exponential improvementover the linear number of comparisons previously needed. We demonstrate theapproach in an experiment in Finland's recent off-road traffic law reform,observing that the total number of comparisons needed to achieve a fixed\epsilon approximation is linear in the number of ideas and that the constantis not large.  Finally, we note a few other experimental observations which support the useof voting rules for aggregation. First, we observe that rating, one of thecommon alternatives to ranking, manifested effects of bias in our data. Second,we show that very few of the topics lacked a Condorcet winner, one of theprominent negative results in voting. Finally, we show data hinting at apotential future direction: the use of partial rankings as opposed to pairwisecomparisons to further decrease the elicitation time.

Personalized PageRank Estimation and Search: A Bidirectional Approach

  We present new algorithms for Personalized PageRank estimation andPersonalized PageRank search. First, for the problem of estimating PersonalizedPageRank (PPR) from a source distribution to a target node, we present a newbidirectional estimator with simple yet strong guarantees on correctness andperformance, and 3x to 8x speedup over existing estimators in experiments on adiverse set of networks. Moreover, it has a clean algebraic structure whichenables it to be used as a primitive for the Personalized PageRank Searchproblem: Given a network like Facebook, a query like "people named John", and asearching user, return the top nodes in the network ranked by PPR from theperspective of the searching user. Previous solutions either score all nodes orscore candidate nodes one at a time, which is prohibitively slow for largecandidate sets. We develop a new algorithm based on our bidirectional PPRestimator which identifies the most relevant results by sampling candidatesbased on their PPR; this is the first solution to PPR search that can find thebest results without iterating through the set of all candidate results.Finally, by combining PPR sampling with sequential PPR estimation and MonteCarlo, we develop practical algorithms for PPR search, and we show viaexperiments that our algorithms are efficient on networks with billions ofedges.

Efficient Algorithms for Personalized PageRank

  We present new, more efficient algorithms for estimating random walk scoressuch as Personalized PageRank from a given source node to one or several targetnodes. These scores are useful for personalized search and recommendations onnetworks including social networks, user-item networks, and the web. Past workhas proposed using Monte Carlo or using linear algebra to estimate scores froma single source to every target, making them inefficient for a single pair. Ourcontribution is a new bidirectional algorithm which combines linear algebra andMonte Carlo to achieve significant speed improvements. On a diverse set of sixgraphs, our algorithm is 70x faster than past state-of-the-art algorithms. Wealso present theoretical analysis: while past algorithms require $\Omega(n)$time to estimate a random walk score of typical size $\frac{1}{n}$ on an$n$-node graph to a given constant accuracy, our algorithm requires only$O(\sqrt{m})$ expected time for an average target, where $m$ is the number ofedges, and is provably accurate.  In addition to our core bidirectional estimator for personalized PageRank, wepresent an alternative algorithm for undirected graphs, a generalization toarbitrary walk lengths and Markov Chains, an algorithm for personalized searchranking, and an algorithm for sampling random paths from a given source to agiven set of targets. We expect our bidirectional methods can be extended inother ways and will be useful subroutines in other graph analysis problems.

Approximate Personalized PageRank on Dynamic Graphs

  We propose and analyze two algorithms for maintaining approximatePersonalized PageRank (PPR) vectors on a dynamic graph, where edges are addedor deleted. Our algorithms are natural dynamic versions of two known localvariations of power iteration. One, Forward Push, propagates probability massforwards along edges from a source node, while the other, Reverse Push,propagates local changes backwards along edges from a target. In bothvariations, we maintain an invariant between two vectors, and when an edge isupdated, our algorithm first modifies the vectors to restore the invariant,then performs any needed local push operations to restore accuracy.  For Reverse Push, we prove that for an arbitrary directed graph in a randomedge model, or for an arbitrary undirected graph, given a uniformly randomtarget node $t$, the cost to maintain a PPR vector to $t$ of additive error$\varepsilon$ as $k$ edges are updated is $O(k + \bar{d} / \varepsilon)$, where$\bar{d}$ is the average degree of the graph. This is $O(1)$ work per update,plus the cost of computing a reverse vector once on a static graph. For ForwardPush, we show that on an arbitrary undirected graph, given a uniformly randomstart node $s$, the cost to maintain a PPR vector from $s$ of degree-normalizederror $\varepsilon$ as $k$ edges are updated is $O(k + 1 / \varepsilon)$, whichis again $O(1)$ per update plus the cost of computing a PPR vector once on astatic graph.

The Core of the Participatory Budgeting Problem

  In participatory budgeting, communities collectively decide on the allocationof public tax dollars for local public projects. In this work, we consider thequestion of fairly aggregating the preferences of community members todetermine an allocation of funds to projects. This problem is different fromstandard fair resource allocation because of public goods: The allocated goodsbenefit all users simultaneously. Fairness is crucial in participatory decisionmaking, since generating equitable outcomes is an important goal of theseprocesses. We argue that the classic game theoretic notion of core capturesfairness in the setting. To compute the core, we first develop a novelcharacterization of a public goods market equilibrium called the Lindahlequilibrium, which is always a core solution. We then provide the first (to ourknowledge) polynomial time algorithm for computing such an equilibrium for abroad set of utility functions; our algorithm also generalizes (in anon-trivial way) the well-known concept of proportional fairness. We use ourtheoretical insights to perform experiments on real participatory budgetingvoting data. We empirically show that the core can be efficiently computed forutility functions that naturally model our practical setting, and examine therelation of the core with the familiar welfare objective. Finally, we addressconcerns of incentives and mechanism design by developing a randomizedapproximately dominant-strategy truthful mechanism building on the exponentialmechanism from differential privacy.

Re-incentivizing Discovery: Mechanisms for Partial-Progress Sharing in  Research

  An essential primitive for an efficient research ecosystem is\emph{partial-progress sharing} (PPS) -- whereby a researcher sharesinformation immediately upon making a breakthrough. This helps preventduplication of work; however there is evidence that existing reward structuresin research discourage partial-progress sharing. Ensuring PPS is especiallyimportant for new online collaborative-research platforms, which involve manyresearchers working on large, multi-stage problems.  We study the problem of incentivizing information-sharing in research, undera stylized model: non-identical agents work independently on subtasks of alarge project, with dependencies between subtasks captured via an acyclicsubtask-network. Each subtask carries a reward, given to the first agent whopublicly shares its solution. Agents can choose which subtasks to work on, andmore importantly, when to reveal solutions to completed subtasks. Under thismodel, we uncover the strategic rationale behind certain anecdotal phenomena.Moreover, for any acyclic subtask-network, and under a general model ofagent-subtask completion times, we give sufficient conditions that ensure PPSis incentive-compatible for all agents.  One surprising finding is that rewards which are approximately proportionalto perceived task-difficulties are sufficient to ensure PPS in all acyclicsubtask-networks. The fact that there is no tension between local fairness andglobal information-sharing in multi-stage projects is encouraging, as itsuggests practical mechanisms for real-world settings. Finally, we show thatPPS is necessary, and in many cases, sufficient, to ensure a high rate ofprogress in research.

Iterative Local Voting for Collective Decision-making in Continuous  Spaces

  Many societal decision problems lie in high-dimensional continuous spaces notamenable to the voting techniques common for their discrete orsingle-dimensional counterparts. These problems are typically discretizedbefore running an election or decided upon through negotiation byrepresentatives. We propose a algorithm called {\sc Iterative Local Voting} forcollective decision-making in this setting. In this algorithm, voters aresequentially sampled and asked to modify a candidate solution within some localneighborhood of its current value, as defined by a ball in some chosen norm,with the size of the ball shrinking at a specified rate.  We first prove the convergence of this algorithm under appropriate choices ofneighborhoods to Pareto optimal solutions with desirable fairness properties incertain natural settings: when the voters' utilities can be expressed in termsof some form of distance from their ideal solution, and when these utilitiesare additively decomposable across dimensions. In many of these cases, weobtain convergence to the societal welfare maximizing solution.  We then describe an experiment in which we test our algorithm for thedecision of the U.S. Federal Budget on Mechanical Turk with over 2,000 workers,employing neighborhoods defined by $\mathcal{L}^1, \mathcal{L}^2$ and$\mathcal{L}^\infty$ balls. We make several observations that inform futureimplementations of such a procedure.

Sequential Deliberation for Social Choice

  In large scale collective decision making, social choice is a normative studyof how one ought to design a protocol for reaching consensus. However, ininstances where the underlying decision space is too large or complex forordinal voting, standard voting methods of social choice may be impractical.How then can we design a mechanism - preferably decentralized, simple,scalable, and not requiring any special knowledge of the decision space - toreach consensus? We propose sequential deliberation as a natural solution tothis problem. In this iterative method, successive pairs of agents bargain overthe decision space using the previous decision as a disagreement alternative.We describe the general method and analyze the quality of its outcome when thespace of preferences define a median graph. We show that sequentialdeliberation finds a 1.208- approximation to the optimal social cost on suchgraphs, coming very close to this value with only a small constant number ofagents sampled from the population. We also show lower bounds on simplerclasses of mechanisms to justify our design choices. We further show thatsequential deliberation is ex-post Pareto efficient and has truthful reportingas an equilibrium of the induced extensive form game. We finally show that forgeneral metric spaces, the second moment of of the distribution of social costof the outcomes produced by sequential deliberation is also bounded.

Markets for Public Decision-making

  A public decision-making problem consists of a set of issues, each withmultiple possible alternatives, and a set of competing agents, each with apreferred alternative for each issue. We study adaptations of market economiesto this setting, focusing on binary issues. Issues have prices, and each agentis endowed with artificial currency that she can use to purchase probabilityfor her preferred alternatives (we allow randomized outcomes). We first showthat when each issue has a single price that is common to all agents, marketequilibria can be arbitrarily bad. This negative result motivates a differentapproach. We present a novel technique called "pairwise issue expansion", whichtransforms any public decision-making instance into an equivalent Fishermarket, the simplest type of private goods market. This is done by expandingeach issue into many goods: one for each pair of agents who disagree on thatissue. We show that the equilibrium prices in the constructed Fisher marketyield a "pairwise pricing equilibrium" in the original public decision-makingproblem which maximizes Nash welfare. More broadly, pairwise issue expansionuncovers a powerful connection between the public decision-making and privategoods settings; this immediately yields several interesting results aboutpublic decisions markets, and furthers the hope that we will be able to find asimple iterative voting protocol that leads to near-optimum decisions.

Relating Metric Distortion and Fairness of Social Choice Rules

  One way of evaluating social choice (voting) rules is through a utilitariandistortion framework. In this model, we assume that agents submit full rankingsover the alternatives, and these rankings are generated from underlying, butunknown, quantitative costs. The \emph{distortion} of a social choice rule isthen the ratio of the total social cost of the chosen alternative to theoptimal social cost of any alternative; since the true costs are unknown, weconsider the worst-case distortion over all possible underlying costs.Analogously, we can consider the worst-case \emph{fairness ratio} of a socialchoice rule by comparing a useful notion of fairness (based on approximatemajorization) for the chosen alternative to that of the optimal alternative.With an additional metric assumption -- that the costs equal theagent-alternative distances in some metric space -- it is known that theCopeland rule achieves both a distortion and fairness ratio of at most 5. Forother rules, only bounds on the distortion are known, e.g., the popular SingleTransferable Vote (STV) rule has distortion $O(\log m)$, where $m$ is thenumber of alternatives. We prove that the distinct notions of distortion andfairness ratio are in fact closely linked -- within an additive factor of 2 forany voting rule -- and thus STV also achieves an $O(\log m)$ fairness ratio. Wefurther extend the notions of distortion and fairness ratio to social choicerules choosing a \emph{set} of alternatives. By relating the distortion ofsingle-winner rules to multiple-winner rules, we establish that RecursiveCopeland achieves a distortion of 5 and a fairness ratio of at most 7 forchoosing a set of alternatives.

Source Routing and Scheduling in Packet Networks

  We study {\em routing} and {\em scheduling} in packet-switched networks. Weassume an adversary that controls the injection time, source, and destinationfor each packet injected. A set of paths for these packets is {\em admissible}if no link in the network is overloaded. We present the first on-line routingalgorithm that finds a set of admissible paths whenever this is feasible. Ouralgorithm calculates a path for each packet as soon as it is injected at itssource using a simple shortest path computation. The length of a link reflectsits current congestion. We also show how our algorithm can be implemented undertoday's Internet routing paradigms.  When the paths are known (either given by the adversary or computed as above)our goal is to schedule the packets along the given paths so that the packetsexperience small end-to-end delays. The best previous delay bounds fordeterministic and distributed scheduling protocols were exponential in the pathlength. In this paper we present the first deterministic and distributedscheduling protocol that guarantees a polynomial end-to-end delay for everypacket.  Finally, we discuss the effects of combining routing with scheduling. Wefirst show that some unstable scheduling protocols remain unstable no matterhow the paths are chosen. However, the freedom to choose paths can make adifference. For example, we show that a ring with parallel links is stable forall greedy scheduling protocols if paths are chosen intelligently, whereas thisis not the case if the adversary specifies the paths.

Graph Sparsification via Refinement Sampling

  A graph G'(V,E') is an \eps-sparsification of G for some \eps>0, if every(weighted) cut in G' is within (1\pm \eps) of the corresponding cut in G. Acelebrated result of Benczur and Karger shows that for every undirected graphG, an \eps-sparsification with O(n\log n/\e^2) edges can be constructed inO(m\log^2n) time. Applications to modern massive data sets often constrainalgorithms to use computation models that restrict random access to the input.The semi-streaming model, in which the algorithm is constrained to use \tildeO(n) space, has been shown to be a good abstraction for analyzing graphalgorithms in applications to large data sets. Recently, a semi-streamingalgorithm for graph sparsification was presented by Anh and Guha; the totalrunning time of their implementation is \Omega(mn), too large for applicationswhere both space and time are important. In this paper, we introduce a newtechnique for graph sparsification, namely refinement sampling, that gives an\tilde{O}(m) time semi-streaming algorithm for graph sparsification.  Specifically, we show that refinement sampling can be used to design aone-pass streaming algorithm for sparsification that takes O(\log\log n) timeper edge, uses O(\log^2 n) space per node, and outputs an \eps-sparsifier withO(n\log^3 n/\eps^2) edges. At a slightly increased space and time complexity,we can reduce the sparsifier size to O(n \log n/\e^2) edges matching theBenczur-Karger result, while improving upon the Benczur-Karger runtime form=\omega(n\log^3 n). Finally, we show that an \eps-sparsifier with O(n \logn/\eps^2) edges can be constructed in two passes over the data and O(m) timewhenever m =\Omega(n^{1+\delta}) for some constant \delta>0. As a by-product ofour approach, we also obtain an O(m\log\log n+n \log n) time streamingalgorithm to compute a sparse k-connectivity certificate of a graph.

Fast Incremental and Personalized PageRank

  In this paper, we analyze the efficiency of Monte Carlo methods forincremental computation of PageRank, personalized PageRank, and similar randomwalk based methods (with focus on SALSA), on large-scale dynamically evolvingsocial networks. We assume that the graph of friendships is stored indistributed shared memory, as is the case for large social networks such asTwitter.  For global PageRank, we assume that the social network has $n$ nodes, and $m$adversarially chosen edges arrive in a random order. We show that with a resetprobability of $\epsilon$, the total work needed to maintain an accurateestimate (using the Monte Carlo method) of the PageRank of every node at alltimes is $O(\frac{n\ln m}{\epsilon^{2}})$. This is significantly better thanall known bounds for incremental PageRank. For instance, if we naivelyrecompute the PageRanks as each edge arrives, the simple power iteration methodneeds $\Omega(\frac{m^2}{\ln(1/(1-\epsilon))})$ total time and the Monte Carlomethod needs $O(mn/\epsilon)$ total time; both are prohibitively expensive.Furthermore, we also show that we can handle deletions equally efficiently.  We then study the computation of the top $k$ personalized PageRanks startingfrom a seed node, assuming that personalized PageRanks follow a power-law withexponent $\alpha < 1$. We show that if we store $R>q\ln n$ random walksstarting from every node for large enough constant $q$ (using the approachoutlined for global PageRank), then the expected number of calls made to thedistributed social network database is $O(k/(R^{(1-\alpha)/\alpha}))$.  We also present experimental results from the social networking site,Twitter, verifying our assumptions and analyses. The overall result is thatthis algorithm is fast enough for real-time queries over a dynamic socialnetwork.

Similarity Search and Locality Sensitive Hashing using TCAMs

  Similarity search methods are widely used as kernels in various machinelearning applications. Nearest neighbor search (NNS) algorithms are often usedto retrieve similar entries, given a query. While there exist efficienttechniques for exact query lookup using hashing, similarity search using exactnearest neighbors is known to be a hard problem and in high dimensions, bestknown solutions offer little improvement over a linear scan. Fast solutions tothe approximate NNS problem include Locality Sensitive Hashing (LSH) basedtechniques, which need storage polynomial in $n$ with exponent greater than$1$, and query time sublinear, but still polynomial in $n$, where $n$ is thesize of the database. In this work we present a new technique of solving theapproximate NNS problem in Euclidean space using a Ternary Content AddressableMemory (TCAM), which needs near linear space and has O(1) query time. In fact,this method also works around the best known lower bounds in the cell probemodel for the query time using a data structure near linear in the size of thedata base. TCAMs are high performance associative memories widely used innetworking applications such as access control lists. A TCAM can query for abit vector within a database of ternary vectors, where every bit positionrepresents $0$, $1$ or $*$. The $*$ is a wild card representing either a $0$ ora $1$. We leverage TCAMs to design a variant of LSH, called Ternary LocalitySensitive Hashing (TLSH) wherein we hash database entries represented byvectors in the Euclidean space into $\{0,1,*\}$. By using the addedfunctionality of a TLSH scheme with respect to the $*$ character, we solve aninstance of the approximate nearest neighbor problem with 1 TCAM access andstorage nearly linear in the size of the database. We believe that this workcan open new avenues in very high speed data mining.

Liquidity in Credit Networks: A Little Trust Goes a Long Way

  Credit networks represent a way of modeling trust between entities in anetwork. Nodes in the network print their own currency and trust each other fora certain amount of each other's currency. This allows the network to serve asa decentralized payment infrastructure---arbitrary payments can be routedthrough the network by passing IOUs between trusting nodes in their respectivecurrencies---and obviates the need for a common currency. Nodes can repeatedlytransact with each other and pay for the transaction using trusted currency. Anatural question to ask in this setting is: how long can the network sustainliquidity, i.e., how long can the network support the routing of paymentsbefore credit dries up? We answer this question in terms of the long termfailure probability of transactions for various network topologies and creditvalues.  We prove that the transaction failure probability is independent of the pathalong which transactions are routed. We show that under symmetric transactionrates, the transaction failure probability in a number of well-known graphfamilies goes to zero as the size, density or credit capacity of the networkincreases. We also show via simulations that even networks of small size andcredit capacity can route transactions with high probability if they arewell-connected. Further, we characterize a centralized currency system as aspecial type of a star network (one where edges to the root have infinitecredit capacity, and transactions occur only between leaf nodes) and computethe steady-state transaction failure probability in a centralized system. Weshow that liquidity in star networks, complete graphs and Erd\"{o}s-R\'{e}nyinetworks is comparable to that in equivalent centralized currency systems; thuswe do not lose much liquidity in return for their robustness and decentralizedproperties.

Metric Distortion of Social Choice Rules: Lower Bounds and Fairness  Properties

  We study social choice rules under the utilitarian distortion framework, withan additional metric assumption on the agents' costs over the alternatives. Inthis approach, these costs are given by an underlying metric on the set of allagents plus alternatives. Social choice rules have access to only the ordinalpreferences of agents but not the latent cardinal costs that induce them.Distortion is then defined as the ratio between the social cost (typically thesum of agent costs) of the alternative chosen by the mechanism at hand, andthat of the optimal alternative chosen by an omniscient algorithm. Theworst-case distortion of a social choice rule is, therefore, a measure of howclose it always gets to the optimal alternative without any knowledge of theunderlying costs. Under this model, it has been conjectured that Ranked Pairs,the well-known weighted-tournament rule, achieves a distortion of at most 3[Anshelevich et al. 2015]. We disprove this conjecture by constructing asequence of instances which shows that the worst-case distortion of RankedPairs is at least 5. Our lower bound on the worst case distortion of RankedPairs matches a previously known upper bound for the Copeland rule, provingthat in the worst case, the simpler Copeland rule is at least as good as RankedPairs. And as long as we are limited to (weighted or unweighted) tournamentrules, we demonstrate that randomization cannot help achieve an expectedworst-case distortion of less than 3. Using the concept of approximatemajorization within the distortion framework, we prove that Copeland andRandomized Dictatorship achieve low constant factor fairness-ratios (5 and 3respectively), which is a considerable generalization of similar results forthe sum of costs and single largest cost objectives. In addition to all of theabove, we outline several interesting directions for further research in thisspace.

When Hashes Met Wedges: A Distributed Algorithm for Finding High  Similarity Vectors

  Finding similar user pairs is a fundamental task in social networks, withnumerous applications in ranking and personalization tasks such as linkprediction and tie strength detection. A common manifestation of usersimilarity is based upon network structure: each user is represented by avector that represents the user's network connections, where pairwise cosinesimilarity among these vectors defines user similarity. The predominant taskfor user similarity applications is to discover all similar pairs that have apairwise cosine similarity value larger than a given threshold $\tau$. Incontrast to previous work where $\tau$ is assumed to be quite close to 1, wefocus on recommendation applications where $\tau$ is small, but stillmeaningful. The all pairs cosine similarity problem is computationallychallenging on networks with billions of edges, and especially so for settingswith small $\tau$. To the best of our knowledge, there is no practical solutionfor computing all user pairs with, say $\tau = 0.2$ on large social networks,even using the power of distributed algorithms.  Our work directly addresses this challenge by introducing a new algorithm ---WHIMP --- that solves this problem efficiently in the MapReduce model. The keyinsight in WHIMP is to combine the "wedge-sampling" approach of Cohen-Lewis forapproximate matrix multiplication with the SimHash random projection techniquesof Charikar. We provide a theoretical analysis of WHIMP, proving that it hasnear optimal communication costs while maintaining computation cost comparablewith the state of the art. We also empirically demonstrate WHIMP's scalabilityby computing all highly similar pairs on four massive data sets, and show thatit accurately finds high similarity pairs. In particular, we note that WHIMPsuccessfully processes the entire Twitter network, which has tens of billionsof edges.

On Evaluating and Comparing Open Domain Dialog Systems

  Conversational agents are exploding in popularity. However, much work remainsin the area of non goal-oriented conversations, despite significant growth inresearch interest over recent years. To advance the state of the art inconversational AI, Amazon launched the Alexa Prize, a 2.5-million dollaruniversity competition where sixteen selected university teams builtconversational agents to deliver the best social conversational experience.Alexa Prize provided the academic community with the unique opportunity toperform research with a live system used by millions of users. The subjectivityassociated with evaluating conversations is key element underlying thechallenge of building non-goal oriented dialogue systems. In this paper, wepropose a comprehensive evaluation strategy with multiple metrics designed toreduce subjectivity by selecting metrics which correlate well with humanjudgement. The proposed metrics provide granular analysis of the conversationalagents, which is not captured in human ratings. We show that these metrics canbe used as a reasonable proxy for human judgment. We provide a mechanism tounify the metrics for selecting the top performing agents, which has also beenapplied throughout the Alexa Prize competition. To our knowledge, to date it isthe largest setting for evaluating agents with millions of conversations andhundreds of thousands of ratings from users. We believe that this work is astep towards an automatic evaluation process for conversational AIs.

Random Dictators with a Random Referee: Constant Sample Complexity  Mechanisms for Social Choice

  We study social choice mechanisms in an implicit utilitarian framework with ametric constraint, where the goal is to minimize \textit{Distortion}, the worstcase social cost of an ordinal mechanism relative to underlying cardinalutilities. We consider two additional desiderata: Constant sample complexityand Squared Distortion. Constant sample complexity means that the mechanism(potentially randomized) only uses a constant number of ordinal queriesregardless of the number of voters and alternatives. Squared Distortion is ameasure of variance of the Distortion of a randomized mechanism.  Our primary contribution is the first social choice mechanism with constantsample complexity \textit{and} constant Squared Distortion (which also impliesconstant Distortion). We call the mechanism Random Referee, because it uses arandom agent to compare two alternatives that are the favorites of two otherrandom agents. We prove that the use of a comparison query is necessary: nomechanism that only elicits the top-k preferred alternatives of voters (forconstant k) can have Squared Distortion that is sublinear in the number ofalternatives. We also prove that unlike any top-k only mechanism, theDistortion of Random Referee meaningfully improves on benign metric spaces,using the Euclidean plane as a canonical example. Finally, among top-1 onlymechanisms, we introduce Random Oligarchy. The mechanism asks just 3 queriesand is essentially optimal among the class of such mechanisms with respect toDistortion.  In summary, we demonstrate the surprising power of constant sample complexitymechanisms generally, and just three random voters in particular, to providesome of the best known results in the implicit utilitarian framework.

