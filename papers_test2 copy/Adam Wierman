The Empirical Implications of Privacy-Aware Choice

  This paper initiates the study of the testable implications of choice data insettings where agents have privacy preferences. We adapt the standardconceptualization of consumer choice theory to a situation where the consumeris aware of, and has preferences over, the information revealed by her choices.The main message of the paper is that little can be inferred about consumers'preferences once we introduce the possibility that the consumer has concernsabout privacy. This holds even when consumers' privacy preferences are assumedto be monotonic and separable. This motivates the consideration of strongerassumptions and, to that end, we introduce an additive model for privacypreferences that does have testable implications.

A Tale of Two Metrics: Simultaneous Bounds on Competitiveness and Regret

  We consider algorithms for "smoothed online convex optimization" problems, avariant of the class of online convex optimization problems that is stronglyrelated to metrical task systems. Prior literature on these problems hasfocused on two performance metrics: regret and the competitive ratio. Thereexist known algorithms with sublinear regret and known algorithms with constantcompetitive ratios; however, no known algorithm achieves both simultaneously.We show that this is due to a fundamental incompatibility between these twometrics - no algorithm (deterministic or randomized) can achieve sublinearregret and a constant competitive ratio, even in the case when the objectivefunctions are linear. However, we also exhibit an algorithm that, for theimportant special case of one-dimensional decision spaces, provides sublinearregret while maintaining a competitive ratio that grows arbitrarily slowly.

Percolation thresholds on 2D Voronoi networks and Delaunay  triangulations

  The site percolation threshold for the random Voronoi network is determinednumerically for the first time, with the result p_c = 0.71410 +/- 0.00002,using Monte-Carlo simulation on periodic systems of up to 40000 sites. Theresult is very close to the recent theoretical estimate p_c = 0.7151 of Neher,Mecke, and Wagner. For the bond threshold on the Voronoi network, we find p_c =0.666931 +/- 0.000005, implying that for its dual, the Delaunay triangulation,p_c = 0.333069 +/- 0.000005. These results rule out the conjecture by Hsu andHuang that the bond thresholds are 2/3 and 1/3 respectively, but support theconjecture of Wierman that for fully triangulated lattices other than theregular triangular lattice, the bond threshold is less than 2 sin pi/18 =0.3473.

Characterizing the Impact of the Workload on the Value of Dynamic  Resizing in Data Centers

  Energy consumption imposes a significant cost for data centers; yet much ofthat energy is used to maintain excess service capacity during periods ofpredictably low load. Resultantly, there has recently been interest indeveloping designs that allow the service capacity to be dynamically resized tomatch the current workload. However, there is still much debate about the valueof such approaches in real settings. In this paper, we show that the value ofdynamic resizing is highly dependent on statistics of the workload process. Inparticular, both slow time-scale non-stationarities of the workload (e.g., thepeak-to-mean ratio) and the fast time-scale stochasticity (e.g., the burstinessof arrivals) play key roles. To illustrate the impact of these factors, wecombine optimization-based modeling of the slow time-scale with stochasticmodeling of the fast time scale. Within this framework, we provide bothanalytic and numerical results characterizing when dynamic resizing does (anddoes not) provide benefits.

Incentives for P2P-Assisted Content Distribution: If You Can't Beat 'Em,  Join 'Em

  The rapid growth of content distribution on the Internet has brought with itproportional increases in the costs of distributing content. Adding todistribution costs is the fact that digital content is easily duplicable, andhence can be shared in an illicit peer-to-peer (P2P) manner that generates norevenue for the content provider. In this paper, we study whether the contentprovider can recover lost revenue through a more innovative approach todistribution. In particular, we evaluate the benefits of a hybridrevenue-sharing system that combines a legitimate P2P swarm and a centralizedclient-server approach. We show how the revenue recovered by the contentprovider using a server-supported legitimate P2P swarm can exceed that of themonopolistic scheme by an order of magnitude. Our analytical results areobtained in a fluid model, and supported by stochastic simulations.

The Empirical Implications of Rank in Bimatrix Games

  We study the structural complexity of bimatrix games, formalized via rank,from an empirical perspective. We consider a setting where we have data onplayer behavior in diverse strategic situations, but where we do not observethe relevant payoff functions. We prove that high complexity (high rank) hasempirical consequences when arbitrary data is considered. Additionally, weprove that, in more restrictive classes of data (termed laminar), anyobservation is rationalizable using a low-rank game: specifically a zero-sumgame. Hence complexity as a structural property of a game is not alwaystestable. Finally, we prove a general result connecting the structure of thefeasible data sets with the highest rank that may be needed to rationalize aset of observations.

Approximate dynamic programming using fluid and diffusion approximations  with applications to power management

  Neuro-dynamic programming is a class of powerful techniques for approximatingthe solution to dynamic programming equations. In their most computationallyattractive formulations, these techniques provide the approximate solution onlywithin a prescribed finite-dimensional function class. Thus, the question thatalways arises is how should the function class be chosen? The goal of thispaper is to propose an approach using the solutions to associated fluid anddiffusion approximations. In order to illustrate this approach, the paperfocuses on an application to dynamic speed scaling for power management incomputer processors.

The Cost of an Epidemic over a Complex Network: A Random Matrix Approach

  In this paper we quantify the total economic impact of an epidemic over acomplex network using tools from random matrix theory. Incorporating the directand indirect costs of infection, we calculate the disease cost in the largegraph limit for an SIS (Susceptible - Infected - Susceptible) infectionprocess. We also give an upper bound on this cost for arbitrary finite graphsand illustrate both calculated costs using extensive simulations on random andreal-world networks. We extend these calculations by considering the totalsocial cost of an epidemic, accounting for both the immunization and diseasecosts for various immunization strategies and determining the optimalimmunization. Our work focuses on the transient behavior of the epidemic, incontrast to previous research, which typically focuses on determining thesteady-state system equilibrium.

On the Existence of Low-Rank Explanations for Mixed Strategy Behavior

  Nash equilibrium is used as a model to explain the observed behavior ofplayers in strategic settings. For example, in many empirical applications weobserve player behavior, and the problem is to determine if there exist payoffsfor the players for which the equilibrium corresponds to observed playerbehavior. Computational complexity of Nash equilibria is an importantconsideration in this framework. If the instance of the model that explainsobserved player behavior requires players to have solved a computationally hardproblem, then the explanation provided is questionable. In this paper weprovide conditions under which Nash equilibrium is a reasonable explanation forstrategic behavior, i.e., conditions under which observed behavior of playerscan be explained by games in which Nash equilibria are easy to compute. Weidentify three structural conditions and show that if the data set of observedbehavior satisfies any of these conditions, then it is consistent with payoffmatrices for which the observed Nash equilibria could have been computedefficiently. Our conditions admit large and structurally complex data sets ofobserved behavior, showing that even with complexity considerations, Nashequilibrium is often a reasonable model.

Distributional Analysis for Model Predictive Deferrable Load Control

  Deferrable load control is essential for handling the uncertaintiesassociated with the increasing penetration of renewable generation. Modelpredictive control has emerged as an effective approach for deferrable loadcontrol, and has received considerable attention. In particular, previous workhas analyzed the average-case performance of model predictive deferrable loadcontrol. However, to this point, distributional analysis of model predictivedeferrable load control has been elusive. In this paper, we prove strongconcentration results on the distribution of the load variance obtained bymodel predictive deferrable load control. These concentration results highlightthat the typical performance of model predictive deferrable load control istightly concentrated around the average-case performance.

The Role of a Market Maker in Networked Cournot Competition

  We study the role of a market maker (or market operator) in a transmissionconstrained electricity market. We model the market as a one-shot networkedCournot competition where generators supply quantity bids and load servingentities provide downward sloping inverse demand functions. This mimics theoperation of a spot market in a deregulated market structure. In this paper, wefocus on possible mechanisms employed by the market maker to balance demand andsupply. In particular, we consider three candidate objective functions that themarket maker optimizes - social welfare, residual social welfare, and consumersurplus. We characterize the existence of Generalized Nash Equilibrium (GNE) inthis setting and demonstrate that market outcomes at equilibrium can be verydifferent under the candidate objective functions.

Online Convex Optimization Using Predictions

  Making use of predictions is a crucial, but under-explored, area of onlinealgorithms. This paper studies a class of online optimization problems where wehave external noisy predictions available. We propose a stochastic predictionerror model that generalizes prior models in the learning and stochasticcontrol communities, incorporates correlation among prediction errors, andcaptures the fact that predictions improve as time passes. We prove thatachieving sublinear regret and constant competitive ratio for online algorithmsrequires the use of an unbounded prediction window in adversarial settings, butthat under more realistic stochastic prediction error models it is possible touse Averaging Fixed Horizon Control (AFHC) to simultaneously achieve sublinearregret and constant competitive ratio in expectation using only aconstant-sized prediction window. Furthermore, we show that the performance ofAFHC is tightly concentrated around its mean.

Greening Multi-Tenant Data Center Demand Response

  Data centers have emerged as promising resources for demand response,particularly for emergency demand response (EDR), which saves the power gridfrom incurring blackouts during emergency situations. However, currently, datacenters typically participate in EDR by turning on backup (diesel) generators,which is both expensive and environmentally unfriendly. In this paper, we focuson "greening" demand response in multi-tenant data centers, i.e., colocationdata centers, by designing a pricing mechanism through which the data centeroperator can efficiently extract load reductions from tenants during emergencyperiods to fulfill energy reduction requirement for EDR. In particular, wepropose a pricing mechanism for both mandatory and voluntary EDR programs,ColoEDR, that is based on parameterized supply function bidding and providesprovably near-optimal efficiency guarantees, both when tenants are price-takingand when they are price-anticipating. In addition to analytic results, weextend the literature on supply function mechanism design, and evaluate ColoEDRusing trace-based simulation studies. These validate the efficiency analysisand conclude that the pricing mechanism is both beneficial to the environmentand to the data center operator (by decreasing the need for backup dieselgeneration), while also aiding tenants (by providing payments for loadreductions).

Optimizing Energy Storage Participation in Emerging Power Markets

  The growing amount of intermittent renewables in power generation createschallenges for real-time matching of supply and demand in the power grid.Emerging ancillary power markets provide new incentives to consumers (e.g.,electrical vehicles, data centers, and others) to perform demand response tohelp stabilize the electricity grid. A promising class of potential demandresponse providers includes energy storage systems (ESSs). This paper evaluatesthe benefits of using various types of novel ESS technologies for a variety ofemerging smart grid demand response programs, such as regulation servicesreserves (RSRs), contingency reserves, and peak shaving. We model, formulateand solve optimization problems to maximize the net profit of ESSs in providingeach demand response. Our solution selects the optimal power and energycapacities of the ESS, determines the optimal reserve value to provide as wellas the ESS real-time operational policy for program participation. Our resultshighlight that applying ultra-capacitors and flywheels in RSR has the potentialto be up to 30 times more profitable than using common battery technologiessuch as LI and LA batteries for peak shaving.

Joint Data Purchasing and Data Placement in a Geo-Distributed Data  Market

  This paper studies two design tasks faced by a geo-distributed cloud datamarket: which data to purchase (data purchasing) and where to place/replicatethe data for delivery (data placement). We show that the joint problem of datapurchasing and data placement within a cloud data market can be viewed as afacility location problem, and is thus NP-hard. However, we give a provablyoptimal algorithm for the case of a data market made up of a single datacenter, and then generalize the structure from the single data center settingin order to develop a near-optimal, polynomial-time algorithm for ageo-distributed data market. The resulting design, Datum, decomposes the jointpurchasing and placement problem into two subproblems, one for data purchasingand one for data placement, using a transformation of the underlying bandwidthcosts. We show, via a case study, that Datum is near-optimal (within 1.6%) inpractical settings.

Distributed optimization decomposition for joint economic dispatch and  frequency regulation

  Economic dispatch and frequency regulation are typically viewed asfundamentally different problems in power systems and, hence, are typicallystudied separately. In this paper, we frame and study a joint problem that co-optimizes both slow timescale economic dispatch resources and fast timescalefrequency regulation resources. We show how the joint problem can be decomposedwithout loss of optimality into slow and fast timescale sub-problems that haveappealing interpretations as the economic dispatch and frequency regulationproblems respectively. We solve the fast timescale sub-problem using adistributed frequency control algorithm that preserves the stability of thenetwork during transients. We solve the slow timescale sub-problem using anefficient market mechanism that coordinates with the fast timescalesub-problem. We investigate the performance of the decomposition on the IEEE24-bus reliability test system.

Opportunities for Price Manipulation by Aggregators in Electricity  Markets

  Aggregators are playing an increasingly crucial role in the integration ofrenewable generation in power systems. However, the intermittent nature ofrenewable generation makes market interactions of aggregators difficult tomonitor and regulate, raising concerns about potential market manipulation byaggregators. In this paper, we study this issue by quantifying the profit anaggregator can obtain through strategic curtailment of generation in anelectricity market. We show that, while the problem of maximizing the benefitfrom curtailment is hard in general, efficient algorithms exist when thetopology of the network is radial (acyclic). Further, we highlight thatsignificant increases in profit are possible via strategic curtailment inpractical settings.

On the Inefficiency of Forward Markets in Leader-Follower Competition

  Motivated by electricity markets, this paper studies the impact of forwardcontracting in situations where firms have capacity constraints andheterogeneous production lead times. We consider a model with two types offirms - leaders and followers - that choose production at two different times.Followers choose productions in the second stage but can sell forward contractsin the first stage. Our main result is an explicit characterization of theequilibrium outcomes. Classic results on forward contracting suggest that itcan mitigate market power in simple settings; however the results in this papershow that the impact of forward markets in this setting is delicate - forwardcontracting can enhance or mitigate market power. In particular, our resultsshow that leader-follower interactions created by heterogeneous production leadtimes may cause forward markets to be inefficient, even when there are a largenumber of followers. In fact, symmetric equilibria do not necessarily exist dueto differences in market power among the leaders and followers.

On the Role of a Market Maker in Networked Cournot Competition

  We study Cournot competition among firms in a networked marketplace that iscentrally managed by a market maker. In particular, we study a situation inwhich a market maker facilitates trade between geographically separate marketsvia a constrained transport network. Our focus is on understanding theconsequences of the design of the market maker and on providing tools foroptimal design. To that end we provide a characterization of the equilibriumoutcomes of the game between the firms and the market maker. Our resultshighlight that the equilibrium structure is impacted dramatically by the marketmaker's objective - depending on the objective there may be a uniqueequilibrium, multiple equilibria, or no equilibria. Further, the game may be apotential game (as in the case of classical Cournot competition) or not. Beyondcharacterizing the equilibria of the game, we provide an approach for designingthe market maker in order to optimize a design objective (e.g., social welfare)at the equilibrium of the game. Additionally, we use our results to explore thevalue of transport (trade) and the efficiency of the market maker (as comparedto a single, aggregate market).

Thinking Fast and Slow: Optimization Decomposition Across Timescales

  Many real-world control systems, such as the smart grid and humansensorimotor control systems, have decentralized components that react quicklyusing local information and centralized components that react slowly using amore global view. This paper seeks to provide a theoretical framework for howto design controllers that are decomposed across timescales in this way. Theframework is analogous to how the network utility maximization framework usesoptimization decomposition to distribute a global control problem acrossindependent controllers, each of which solves a local problem; except our goalis to decompose a global problem temporally, extracting a timescale separation.Our results highlight that decomposition of a multi-timescale controller into afast timescale, reactive controller and a slow timescale, predictive controllercan be near-optimal in a strong sense. In particular, we exhibit such a design,named Multi-timescale Reflexive Predictive Control (MRPC), which maintains aper-timestep cost within a constant factor of the offline optimal in anadversarial setting.

A Parallelizable Acceleration Framework for Packing Linear Programs

  This paper presents an acceleration framework for packing linear programmingproblems where the amount of data available is limited, i.e., where the numberof constraints m is small compared to the variable dimension n. The frameworkcan be used as a black box to speed up linear programming solvers dramatically,by two orders of magnitude in our experiments. We present worst-case guaranteeson the quality of the solution and the speedup provided by the algorithm,showing that the framework provides an approximately optimal solution whilerunning the original solver on a much smaller problem. The framework can beused to accelerate exact solvers, approximate solvers, and parallel/distributedsolvers. Further, it can be used for both linear programs and integer linearprograms.

Third-Party Data Providers Ruin Simple Mechanisms

  This paper studies the revenue of simple mechanisms in settings where athird-party data provider is present. When no data provider is present, it isknown that simple mechanisms achieve a constant fraction of the revenue ofoptimal mechanisms. The results in this paper demonstrate that this is nolonger true in the presence of a third party data provider who can provide thebidder with a signal that is correlated with the item type. Specifically, weshow that even with a single seller, a single bidder, and a single item ofuncertain type for sale, pricing each item-type separately (the analog of itempricing for multi-item auctions) and bundling all item-types under a singleprice (the analog of grand bundling) can both simultaneously be a logarithmicfactor worse than the optimal revenue. Further, in the presence of a dataprovider, item-type partitioning mechanisms---a more general class ofmechanisms which divide item-types into disjoint groups and offer prices foreach group---still cannot achieve within a $\log \log$ factor of the optimalrevenue.

Smoothed Online Convex Optimization in High Dimensions via Online  Balanced Descent

  We study Smoothed Online Convex Optimization, a version of online convexoptimization where the learner incurs a penalty for changing her actionsbetween rounds. Given a $\Omega(\sqrt{d})$ lower bound on the competitive ratioof any online algorithm, where $d$ is the dimension of the action space, we askunder what conditions this bound can be beaten. We introduce a novelalgorithmic framework for this problem, Online Balanced Descent (OBD), whichworks by iteratively projecting the previous point onto a carefully chosenlevel set of the current cost function so as to balance the switching costs andhitting costs. We demonstrate the generality of the OBD framework by showinghow, with different choices of "balance," OBD can improve upon state-of-the-artperformance guarantees for both competitive ratio and regret, in particular,OBD is the first algorithm to achieve a dimension-free competitive ratio, $3 +O(1/\alpha)$, for locally polyhedral costs, where $\alpha$ measures the"steepness" of the costs. We also prove bounds on the dynamic regret of OBDwhen the balance is performed in the dual space that are dimension-free andimply that OBD has sublinear static regret.

Loyalty Programs in the Sharing Economy: Optimality and Competition

  Loyalty programs are important tools for sharing platforms seeking to growsupply. Online sharing platforms use loyalty programs to heavily subsidizeresource providers, encouraging participation and boosting supply. As thesharing economy has evolved and competition has increased, the design ofloyalty programs has begun to play a crucial role in the pursuit of maximalrevenue. In this paper, we first characterize the optimal loyalty program for aplatform with homogeneous users. We then show that optimal revenue in aheterogeneous market can be achieved by a class of multi-threshold loyaltyprogram (MTLP) which admits a simple implementation-friendly structure. We alsostudy the performance of loyalty programs in a setting with two competingsharing platforms, showing that the degree of heterogeneity is a crucial factorfor both loyalty programs and pricing strategies. Our results show thatsophisticated loyalty programs that reward suppliers via stepwise linearfunctions outperform simple sign-up bonuses, which give them a one time rewardfor participating.

Smoothed Online Optimization for Regression and Control

  We consider Online Convex Optimization (OCO) in the setting where the costsare $m$-strongly convex and the online learner pays a switching cost forchanging decisions between rounds. We show that the recently proposed OnlineBalanced Descent (OBD) algorithm is constant competitive in this setting, withcompetitive ratio $3 + O(1/m)$, irrespective of the ambient dimension.Additionally, we show that when the sequence of cost functions is$\epsilon$-smooth, OBD has near-optimal dynamic regret and maintains strongper-round accuracy. We demonstrate the generality of our approach by showingthat the OBD framework can be used to construct competitive algorithms for avariety of online problems across learning and control, including onlinevariants of ridge regression, logistic regression, maximum likelihoodestimation, and LQR control.

Transparency and Control in Platforms for Networked Markets

  In this work, we analyze the worst case efficiency loss of online platformdesigns under a networked Cournot competition model. Inspired by some of thelargest platforms today, the platform designs considered tradeoffs betweentransparency and control, namely, (i) open access, (ii) controlled allocationand (iii) discriminatory access. Our results show that open access designsincentivize increased production towards perfectly competitive levels and limitefficiency loss, while controlled allocation designs lead to producer-platformincentive misalignment, resulting in low participation and unbounded efficiencyloss. We also show that discriminatory access designs seek a balance betweentransparency and control, and achieve the best of both worlds, maintaining highparticipation rates while limiting efficiency loss. We also study a model ofconsumer search cost which further distinguishes between the three designs.

Peer Effects and Stability in Matching Markets

  Many-to-one matching markets exist in numerous different forms, such ascollege admissions, matching medical interns to hospitals for residencies,assigning housing to college students, and the classic firms and workersmarket. In all these markets, externalities such as complementarities and peereffects severely complicate the preference ordering of each agent. Further,research has shown that externalities lead to serious problems for marketstability and for developing efficient algorithms to find stable matchings. Inthis paper we make the observation that peer effects are often the result ofunderlying social connections, and we explore a formulation of the many-to-onematching market where peer effects are derived from an underlying socialnetwork. The key feature of our model is that it captures peer effects andcomplementarities using utility functions, rather than traditional preferenceordering. With this model and considering a weaker notion of stability, namelytwo-sided exchange stability, we prove that stable matchings always exist andcharacterize the set of stable matchings in terms of social welfare. We alsogive distributed algorithms that are guaranteed to converge to a two-sidedexchange stable matching. To assess the competitive ratio of these algorithmsand to more generally characterize the efficiency of matching markets withexternalities, we provide general bounds on how far the welfare of theworst-case stable matching can be from the welfare of the optimal matching, andfind that the structure of the social network (e.g. how well clustered thenetwork is) plays a large role.

Potential Games are Necessary to Ensure Pure Nash Equilibria in Cost  Sharing Games

  We consider the problem of designing distribution rules to share "welfare"(cost or revenue) among individually strategic agents. There are many knowndistribution rules that guarantee the existence of a (pure) Nash equilibrium inthis setting, e.g., the Shapley value and its weighted variants; however, acharacterization of the space of distribution rules that guarantee theexistence of a Nash equilibrium is unknown. Our work provides an exactcharacterization of this space for a specific class of scalable and separablegames, which includes a variety of applications such as facility location,routing, network formation, and coverage games. Given arbitrary local welfarefunctions W, we prove that a distribution rule guarantees equilibrium existencefor all games (i.e., all possible sets of resources, agent action sets, etc.)if and only if it is equivalent to a generalized weighted Shapley value on some"ground" welfare functions W', which can be distinct from W. However, ifbudget-balance is required in addition to the existence of a Nash equilibrium,then W' must be the same as W. We also provide an alternate characterization ofthis space in terms of "generalized" marginal contributions, which is moreappealing from the point of view of computational tractability. A possiblysurprising consequence of our result is that, in order to guarantee equilibriumexistence in all games with any fixed local welfare functions, it is necessaryto work within the class of potential games.

Prices and Subsidies in the Sharing Economy

  The growth of the sharing economy is driven by the emergence of sharingplatforms, e.g., Uber and Lyft, that match owners looking to share theirresources with customers looking to rent them. The design of such platforms isa complex mixture of economics and engineering, and how to "optimally" designsuch platforms is still an open problem. In this paper, we focus on the designof prices and subsidies in sharing platforms. Our results provide insights intothe tradeoff between revenue maximizing prices and social welfare maximizingprices. Specifically, we introduce a novel model of sharing platforms andcharacterize the profit and social welfare maximizing prices in this model.Further, we bound the efficiency loss under profit maximizing prices, showingthat there is a strong alignment between profit and efficiency in practicalsettings. Our results highlight that the revenue of platforms may be limited inpractice due to supply shortages; thus platforms have a strong incentive toencourage sharing via subsidies. We provide an analytic characterization ofwhen such subsidies are valuable and show how to optimize the size of thesubsidy provided. Finally, we validate the insights from our analysis usingdata from Didi Chuxing, the largest ridesharing platform in China.

Failure Localization in Power Systems via Tree Partitions

  Cascading failures in power systems propagate non-locally, making the controland mitigation of outages extremely hard. In this work, we use the emergingconcept of the tree partition of transmission networks to provide an analyticalcharacterization of line failure localizability in transmission systems. Ourresults rigorously establish the well perceived intuition in power communitythat failures cannot cross bridges, and reveal a finer-grained concept thatencodes more precise information on failure propagations within tree-partitionregions. Specifically, when a non-bridge line is tripped, the impact of thisfailure only propagates within well-defined components, which we refer to ascells, of the tree partition defined by the bridges. In contrast, when a bridgeline is tripped, the impact of this failure propagates globally across thenetwork, affecting the power flow on all remaining transmission lines. Thischaracterization suggests that it is possible to improve the system robustnessby temporarily switching off certain transmission lines, so as to create more,smaller components in the tree partition; thus spatially localizing linefailures and making the grid less vulnerable to large-scale outages. Weillustrate this approach using the IEEE 118-bus test system and demonstratethat switching off a negligible portion of transmission lines allows the impactof line failures to be significantly more localized without substantial changesin line congestion.

Newton Polytopes and Relative Entropy Optimization

  Newton polytopes play a prominent role in the study of sparse polynomialsystems, where they help formalize the idea that the root structure underlyingsparse polynomials of possibly high degree ought to still be "simple." In thispaper we consider sparse polynomial optimization problems, and we seek a deeperunderstanding of the role played by Newton polytopes in this context. Ourinvestigation proceeds by reparametrizing polynomials as signomials -- whichare linear combinations of exponentials of linear functions in the decisionvariable -- and studying the resulting signomial optimization problems.Signomial programs represent an interesting (and generally intractable) classof problems in their own right. We build on recent efforts that providetractable relative entropy convex relaxations to obtain bounds on signomialprograms. We describe several new structural results regarding theserelaxations as well as a range of conditions under which they solve signomialprograms exactly. The facial structure of the associated Newton polytopes playsa prominent role in our analysis. Our results have consequences in twodirections, thus highlighting the utility of the signomial perspective. In onedirection, signomials have no notion of "degree"; therefore, techniquesdeveloped for signomial programs depend only on the particular terms thatappear in a signomial. When specialized to the context of polynomials, weobtain analysis and computational tools that only depend on the particularmonomials that constitute a sparse polynomial. In the other direction,signomials represent a natural generalization of polynomials for which Newtonpolytopes continue to yield valuable insights. In particular, a number ofinvariance properties of Newton polytopes in the context of optimization areonly revealed by adopting the viewpoint of signomials.

Online Inventory Management with Application to Energy Procurement in  Data Centers

  Motivated by the application of energy storage management in electricitymarkets, this paper considers the problem of online linear programming withinventory management constraints. Specifically, a decision maker should satisfysome units of an asset as her demand, either form a market with time-varyingprice or from her own inventory. The decision maker is presented a price inslot-by-slot manner, and must immediately decide the purchased amount with thecurrent price to cover the demand or to store in inventory for covering thefuture demand. The inventory has a limited capacity and its critical role is tobuy and store assets at low price and use the stored assets to cover the demandat high price. The ultimate goal of the decision maker is to cover the demandswhile minimizing the cost of buying assets from the market. We propose BatMan,an online algorithm for simple inventory models, and BatManRate, an extendedversion for the case with rate constraints. Both BatMan and BatManRate achieveoptimal competitive ratios, meaning that no other online algorithm can achievea better theoretical guarantee. To illustrate the results, we use the proposedalgorithms to design and evaluate energy procurement and storage managementstrategies for data centers with a portfolio of energy sources including theelectric grid, local renewable generation, and energy storage systems.

Competitive Online Optimization under Inventory Constraints

  This paper studies online optimization under inventory (budget) constraints.While online optimization is a well-studied topic, versions with inventoryconstraints have proven difficult. We consider a formulation ofinventory-constrained optimization that is a generalization of the classicone-way trading problem and has a wide range of applications. We present a newalgorithmic framework, \textsf{CR-Pursuit}, and prove that it achieves theminimal competitive ratio among all deterministic algorithms (up to aproblem-dependent constant factor) for inventory-constrained onlineoptimization. Our algorithm and its analysis not only simplify and unify thestate-of-the-art results for the standard one-way trading problem, but theyalso establish novel bounds for generalizations including concave revenuefunctions. For example, for one-way trading with price elasticity, the\textsf{CR-Pursuit} algorithm achieves a competitive ratio that is within asmall additive constant (i.e., 1/3) to the lower bound of $\ln \theta+1$, where$\theta$ is the ratio between the maximum and minimum base prices.

Less is More: Real-time Failure Localization in Power Systems

  Cascading failures in power systems exhibit non-local propagation patternswhich make the analysis and mitigation of failures difficult. In this work, wepropose a distributed control framework inspired by the recently proposedconcepts of unified controller and network tree-partition that offers strongguarantees in both the mitigation and localization of cascading failures inpower systems. In this framework, the transmission network is partitioned intoseveral control areas which are connected in a tree structure, and the unifiedcontroller is adopted by generators or controllable loads for fast timescaledisturbance response. After an initial failure, the proposed strategy alwaysprevents successive failures from happening, and regulates the system to thedesired steady state where the impact of initial failures are localized as muchas possible. For extreme failures that cannot be localized, the proposedframework has a configurable design, that progressively involves andcoordinates more control areas for failure mitigation and, as a last resort,imposes minimal load shedding. We compare the proposed control framework withAutomatic Generation Control (AGC) on the IEEE 118-bus test system. Simulationresults show that our novel framework greatly improves the system robustness interms of the N-1 security standard, and localizes the impact of initialfailures in majority of the load profiles that are examined. Moreover, theproposed framework incurs significantly less load loss, if any, compared toAGC, in all of our case studies.

Routing and Staffing when Servers are Strategic

  Traditionally, research focusing on the design of routing and staffingpolicies for service systems has modeled servers as having fixed (possiblyheterogeneous) service rates. However, service systems are generally staffed bypeople. Furthermore, people respond to workload incentives; that is, how hard aperson works can depend both on how much work there is, and how the work isdivided between the people responsible for it. In a service system, the routingand staffing policies control such workload incentives; and so the rate serverswork will be impacted by the system's routing and staffing policies. Thisobservation has consequences when modeling service system performance, and ourobjective is to investigate those consequences.  We do this in the context of the M/M/N queue, which is the canonical modelfor large service systems. First, we present a model for "strategic" serversthat choose their service rate in order to maximize a trade-off between an"effort cost", which captures the idea that servers exert more effort whenworking at a faster rate, and a "value of idleness", which assumes that serversvalue having idle time. Next, we characterize the symmetric Nash equilibriumservice rate under any routing policy that routes based on the server idletime. We find that the system must operate in a quality-driven regime, in whichservers have idle time, in order for an equilibrium to exist, which impliesthat the staffing must have a first-order term that strictly exceeds that ofthe common square-root staffing policy. Then, within the class of policies thatadmit an equilibrium, we (asymptotically) solve the problem of minimizing thetotal cost, when there are linear staffing costs and linear waiting costs.Finally, we end by exploring the question of whether routing policies that arebased on the service rate, instead of the server idle time, can improve systemperformance.

Ionization Electron Signal Processing in Single Phase LArTPCs I.  Algorithm Description and Quantitative Evaluation with MicroBooNE Simulation

  We describe the concept and procedure of drifted-charge extraction developedin the MicroBooNE experiment, a single-phase liquid argon time projectionchamber (LArTPC). This technique converts the raw digitized TPC waveform to thenumber of ionization electrons passing through a wire plane at a given time. Arobust recovery of the number of ionization electrons from both induction andcollection anode wire planes will augment the 3D reconstruction, and isparticularly important for tomographic reconstruction algorithms. A number ofbuilding blocks of the overall procedure are described. The performance of thesignal processing is quantitatively evaluated by comparing extracted chargewith the true charge through a detailed TPC detector simulation taking intoaccount position-dependent induced current inside a single wire region andacross multiple wires. Some areas for further improvement of the performance ofthe charge extraction procedure are also discussed.

Ionization Electron Signal Processing in Single Phase LArTPCs II.  Data/Simulation Comparison and Performance in MicroBooNE

  The single-phase liquid argon time projection chamber (LArTPC) provides alarge amount of detailed information in the form of fine-grained driftedionization charge from particle traces. To fully utilize this information, thedeposited charge must be accurately extracted from the raw digitized waveformsvia a robust signal processing chain. Enabled by the ultra-low noise levelsassociated with cryogenic electronics in the MicroBooNE detector, the preciseextraction of ionization charge from the induction wire planes in asingle-phase LArTPC is qualitatively demonstrated on MicroBooNE data with eventdisplay images, and quantitatively demonstrated via waveform-level andtrack-level metrics. Improved performance of induction plane calorimetry isdemonstrated through the agreement of extracted ionization charge measurementsacross different wire planes for various event topologies. In addition to thecomprehensive waveform-level comparison of data and simulation, a calibrationof the cryogenic electronics response is presented and solutions to variousMicroBooNE-specific TPC issues are discussed. This work presents an importantimprovement in LArTPC signal processing, the foundation of reconstruction andtherefore physics analyses in MicroBooNE.

Comparison of νμ-Ar multiplicity distributions observed by  MicroBooNE to GENIE model predictions

  We measure a large set of observables in inclusive charged current muonneutrino scattering on argon with the MicroBooNE liquid argon time projectionchamber operating at Fermilab. We evaluate three neutrino interaction modelsbased on the widely used GENIE event generator using these observables. Themeasurement uses a data set consisting of neutrino interactions with a finalstate muon candidate fully contained within the MicroBooNE detector. These datawere collected in 2016 with the Fermilab Booster Neutrino Beam, which has anaverage neutrino energy of 800 MeV, using an exposure corresponding to 5E19protons-on-target. The analysis employs fully automatic event selection andcharged particle track reconstruction and uses a data-driven technique toseparate neutrino interactions from cosmic ray background events. We find thatGENIE models consistently describe the shapes of a large number of kinematicdistributions for fixed observed multiplicity.

A Deep Neural Network for Pixel-Level Electromagnetic Particle  Identification in the MicroBooNE Liquid Argon Time Projection Chamber

  We have developed a convolutional neural network (CNN) that can make apixel-level prediction of objects in image data recorded by a liquid argon timeprojection chamber (LArTPC) for the first time. We describe the network design,training techniques, and software tools developed to train this network. Thegoal of this work is to develop a complete deep neural network based datareconstruction chain for the MicroBooNE detector. We show the firstdemonstration of a network's validity on real LArTPC data using MicroBooNEcollection plane images. The demonstration is performed for stopping muon and a$\nu_\mu$ charged current neutral pion data samples.

First Measurement of $ν_μ$ Charged-Current $π^{0}$ Production on  Argon with a LArTPC

  We report the first measurement of the flux-integrated cross section of$\nu_{\mu}$ charged-current single $\pi^{0}$ production on argon. Thismeasurement is performed with the MicroBooNE detector, an 85 ton active massliquid argon time projection chamber exposed to the Booster Neutrino Beam atFermilab. This result on argon is compared to past measurements on lighternuclei to investigate the scaling assumptions used in models of the productionand transport of pions in neutrino-nucleus scattering. The techniques used arean important demonstration of the successful reconstruction and analysis ofneutrino interactions producing electromagnetic final states using a liquidargon time projection chamber operating at the earth's surface.

Rejecting cosmic background for exclusive neutrino interaction studies  with Liquid Argon TPCs; a case study with the MicroBooNE detector

  Cosmic ray (CR) interactions can be a challenging source of background forneutrino oscillation and cross-section measurements in surface detectors. Wepresent methods for CR rejection in measurements of charged-currentquasielastic-like (CCQE-like) neutrino interactions, with a muon and a protonin the final state, measured using liquid argon time projection chambers(LArTPCs). Using a sample of cosmic data collected with the MicroBooNEdetector, mixed with simulated neutrino scattering events, a set of eventselection criteria is developed that produces an event sample with minimalcontribution from CR background. Depending on the selection criteria used apurity between 50% and 80% can be achieved with a signal selection efficiencybetween 50% and 25%, with higher purity coming at the expense of lowerefficiency. While using a specific dataset from the MicroBooNE detector andselection criteria values optimized for CCQE-like events, the conceptspresented here are generic and can be adapted for various studies of exclusive{\nu}{\mu} interactions in LArTPCs.

Design and construction of the MicroBooNE Cosmic Ray Tagger system

  The MicroBooNE detector utilizes a liquid argon time projection chamber(LArTPC) with an 85 t active mass to study neutrino interactions along theBooster Neutrino Beam (BNB) at Fermilab. With a deployment location near groundlevel, the detector records many cosmic muon tracks in each beam-relateddetector trigger that can be misidentified as signals of interest. To reducethese cosmogenic backgrounds, we have designed and constructed a TPC-externalCosmic Ray Tagger (CRT). This sub-system was developed by the Laboratory forHigh Energy Physics (LHEP), Albert Einstein center for fundamental physics,University of Bern. The system utilizes plastic scintillation modules toprovide precise time and position information for TPC-traversing particles.Successful matching of TPC tracks and CRT data will allow us to reducecosmogenic background and better characterize the light collection system andLArTPC data using cosmic muons. In this paper we describe the design andinstallation of the MicroBooNE CRT system and provide an overview of a seriesof tests done to verify the proper operation of the system and its componentsduring installation, commissioning, and physics data-taking.

The DUNE Far Detector Interim Design Report, Volume 2: Single-Phase  Module

  The DUNE IDR describes the proposed physics program and technical designs ofthe DUNE far detector modules in preparation for the full TDR to be publishedin 2019. It is intended as an intermediate milestone on the path to a full TDR,justifying the technical choices that flow down from the high-level physicsgoals through requirements at all levels of the Project. These design choiceswill enable the DUNE experiment to make the ground-breaking discoveries thatwill help to answer fundamental physics questions. Volume 2 describes thesingle-phase module's subsystems, the technical coordination required for itsdesign, construction, installation, and integration, and its organizationalstructure.

The DUNE Far Detector Interim Design Report Volume 1: Physics,  Technology and Strategies

  The DUNE IDR describes the proposed physics program and technical designs ofthe DUNE Far Detector modules in preparation for the full TDR to be publishedin 2019. It is intended as an intermediate milestone on the path to a full TDR,justifying the technical choices that flow down from the high-level physicsgoals through requirements at all levels of the Project. These design choiceswill enable the DUNE experiment to make the ground-breaking discoveries thatwill help to answer fundamental physics questions. Volume 1 contains anexecutive summary that describes the general aims of this document. Theremainder of this first volume provides a more detailed description of the DUNEphysics program that drives the choice of detector technologies. It alsoincludes concise outlines of two overarching systems that have not yet evolvedto consortium structures: computing and calibration. Volumes 2 and 3 of thisIDR describe, for the single-phase and dual-phase technologies, respectively,each detector module's subsystems, the technical coordination required for itsdesign, construction, installation, and integration, and its organizationalstructure.

The DUNE Far Detector Interim Design Report, Volume 3: Dual-Phase Module

  The DUNE IDR describes the proposed physics program and technical designs ofthe DUNE far detector modules in preparation for the full TDR to be publishedin 2019. It is intended as an intermediate milestone on the path to a full TDR,justifying the technical choices that flow down from the high-level physicsgoals through requirements at all levels of the Project. These design choiceswill enable the DUNE experiment to make the ground-breaking discoveries thatwill help to answer fundamental physics questions. Volume 3 describes thedual-phase module's subsystems, the technical coordination required for itsdesign, construction, installation, and integration, and its organizationalstructure.

