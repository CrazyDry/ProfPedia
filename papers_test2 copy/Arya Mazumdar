On the Spread of Random Interleaver

  For a given blocklength we determine the number of interleavers which havespread equal to two. Using this, we find out the probability that a randomlychosen interleaver has spread two. We show that as blocklength increases, thisprobability increases but very quickly converges to the value $1-e^{-2} \approx0.8647$. Subsequently, we determine a lower bound on the probability of aninterleaver having spread at least $s$. We show that this lower bound convergesto the value $e^{-2(s-2)^{2}}$, as the blocklength increases.

Construction of Turbo Code Interleavers from 3-Regular Hamiltonian  Graphs

  In this letter we present a new construction of interleavers for turbo codesfrom 3-regular Hamiltonian graphs. The interleavers can be generated using afew parameters, which can be selected in such a way that the girth of theinterleaver graph (IG) becomes large, inducing a high summary distance. Thesize of the search space for these parameters is derived. The proposedinterleavers themselves work as their de-interleavers.

On a Duality Between Recoverable Distributed Storage and Index Coding

  In this paper, we introduce a model of a single-failure locally recoverabledistributed storage system. This model appears to give rise to a problemseemingly dual of the well-studied index coding problem. The relation betweenthe dimensions of an optimal index code and optimal distributed storage code ofour model has been established in this paper. We also show some extensions tovector codes.

Capacity of Locally Recoverable Codes

  Motivated by applications in distributed storage, the notion of a locallyrecoverable code (LRC) was introduced a few years back. In an LRC, anycoordinate of a codeword is recoverable by accessing only a small number ofother coordinates. While different properties of LRCs have been well-studied,their performance on channels with random erasures or errors has been mostlyunexplored. In this note, we analyze the performance of LRCs over suchstochastic channels. In particular, for input-symmetric discrete memorylesschannels, we give a tight characterization of the gap to Shannon capacity whenLRCs are used over the channel.

Codes in Permutations and Error Correction for Rank Modulation

  Codes for rank modulation have been recently proposed as a means ofprotecting flash memory devices from errors. We study basic coding theoreticproblems for such codes, representing them as subsets of the set ofpermutations of $n$ elements equipped with the Kendall tau distance. We deriveseveral lower and upper bounds on the size of codes. These bounds enable us toestablish the exact scaling of the size of optimal codes for large values of$n$. We also show the existence of codes whose size is within a constant factorof the sphere packing bound for any fixed number of errors.

On the Number of Errors Correctable with Codes on Graphs

  We study ensembles of codes on graphs (generalized low-density parity-check,or LDPC codes) constructed from random graphs and fixed local constrainedcodes, and their extension to codes on hypergraphs. It is known that theaverage minimum distance of codes in these ensembles grows linearly with thecode length. We show that these codes can correct a linearly growing number oferrors under simple iterative decoding algorithms. In particular, we show thatthis property extends to codes constructed by parallel concatenation of Hammingcodes and other codes with small minimum distance. Previously known resultsthat proved this property for graph codes relied on graph expansion andrequired the choice of local codes with large distance relative to theirlength.

Coding for High-Density Recording on a 1-D Granular Magnetic Medium

  In terabit-density magnetic recording, several bits of data can be replacedby the values of their neighbors in the storage medium. As a result, errors inthe medium are dependent on each other and also on the data written. Weconsider a simple one-dimensional combinatorial model of this medium. In ourmodel, we assume a setting where binary data is sequentially written on themedium and a bit can erroneously change to the immediately preceding value. Wederive several properties of codes that correct this type of errors, focusingon bounds on their cardinality.  We also define a probabilistic finite-state channel model of the storagemedium, and derive lower and upper estimates of its capacity. A lower bound isderived by evaluating the symmetric capacity of the channel, i.e., the maximumtransmission rate under the assumption of the uniform input distribution of thechannel. An upper bound is found by showing that the original channel is astochastic degradation of another, related channel model whose capacity we cancompute explicitly.

An Upper Bound On the Size of Locally Recoverable Codes

  In a {\em locally recoverable} or {\em repairable} code, any symbol of acodeword can be recovered by reading only a small (constant) number of othersymbols. The notion of local recoverability is important in the area ofdistributed storage where a most frequent error-event is a single storage nodefailure (erasure). A common objective is to repair the node by downloading datafrom as few other storage node as possible. In this paper, we bound the minimumdistance of a code in terms of its length, size and locality. Unlike previousbounds, our bound follows from a significantly simple analysis and depends onthe size of the alphabet being used. It turns out that the binary Simplex codessatisfy our bound with equality; hence the Simplex codes are the first exampleof a optimal binary locally repairable code family. We also provideachievability results based on random coding and concatenated codes that arenumerically verified to be close to our bounds.

On the Capacity of Memoryless Adversary

  In this paper, we study a model of communication under adversarial noise. Inthis model, the adversary makes online decisions on whether to corrupt atransmitted bit based on only the value of that bit. Like the usual binarysymmetric channel of information theory or the fully adversarial channel ofcombinatorial coding theory, the adversary can, with high probability,introduce at most a given fraction of error.  It is shown that, the capacity (maximum rate of reliable informationtransfer) of such memoryless adversary is strictly below that of the binarysymmetric channel. We give new upper bound on the capacity of such channel --the tightness of this upper bound remains an open question. The main componentof our proof is the careful examination of error-correcting properties of acode with skewed distance distribution.

Storage Capacity of Repairable Networks

  In this paper, we introduce a model of a distributed storage system that islocally recoverable from any single server failure. Unlike the usual localrecovery model of codes for distributed storage, this model accounts for thefact that each server or storage node in a network is connectible to only some,and not all other, nodes. This may happen for reasons such as physicalseparation, inhomogeneity in storage platforms etc. We estimate the storagecapacity of both undirected and directed networks under this model and proposesome constructive schemes. From a coding theory point of view, we show thatthis model is approximately dual of the well-studied index coding problem.  Further in this paper, we extend the above model to handle multiple serverfailures. Among other results, we provide an upper bound on the minimumpairwise distance of a set of words that can be stored in a graph with thelocal repair guarantee. The well-known impossibility bounds on the distance oflocally recoverable codes follow from our result.

Restricted isometry property of random subdictionaries

  We study statistical restricted isometry, a property closely related tosparse signal recovery, of deterministic sensing matrices of size $m \times N$.A matrix is said to have a statistical restricted isometry property (StRIP) oforder $k$ if most submatrices with $k$ columns define a near-isometric map of${\mathbb R}^k$ into ${\mathbb R}^m$. As our main result, we establishsufficient conditions for the StRIP property of a matrix in terms of the mutualcoherence and mean square coherence. We show that for many existingdeterministic families of sampling matrices, $m=O(k)$ rows suffice for$k$-StRIP, which is an improvement over the known estimates of either $m =\Theta(k \log N)$ or $m = \Theta(k\log k)$. We also give examples of matrixfamilies that are shown to have the StRIP property using our sufficientconditions.

Group testing schemes from codes and designs

  In group testing, simple binary-output tests are designed to identify a smallnumber $t$ of defective items that are present in a large population of $N$items. Each test takes as input a group of items and produces a binary outputindicating whether the group is free of the defective items or contains one ormore of them. In this paper we study a relaxation of the combinatorial grouptesting problem. A matrix is called $(t,\epsilon)$-disjunct if it gives rise toa nonadaptive group testing scheme with the property of identifying a uniformlyrandom $t$-set of defective subjects out of a population of size $N$ with falsepositive probability of an item at most $\epsilon$. We establish a newconnection between $(t,\epsilon)$-disjunct matrices and error correcting codesbased on the dual distance of the codes and derive estimates of the parametersof codes that give rise to such schemes. Our methods rely on the moments of thedistance distribution of codes and inequalities for moments of sums ofindependent random variables. We also provide a new connection between grouptesting schemes and combinatorial designs.

A Theoretical Analysis of First Heuristics of Crowdsourced Entity  Resolution

  Entity resolution (ER) is the task of identifying all records in a databasethat refer to the same underlying entity, and are therefore duplicates of eachother. Due to inherent ambiguity of data representation and poor data quality,ER is a challenging task for any automated process. As a remedy, human-poweredER via crowdsourcing has become popular in recent years. Using crowd to answerqueries is costly and time consuming. Furthermore, crowd-answers can often befaulty. Therefore, crowd-based ER methods aim to minimize human participationwithout sacrificing the quality and use a computer generated similarity matrixactively. While, some of these methods perform well in practice, no theoreticalanalysis exists for them, and further their worst case performances do notreflect the experimental findings. This creates a disparity in theunderstanding of the popular heuristics for this problem. In this paper, wemake the first attempt to close this gap. We provide a thorough analysis of theprominent heuristic algorithms for crowd-based ER. We justify experimentalobservations with our analysis and information theoretic lower bounds.

Combinatorial Alphabet-Dependent Bounds for Locally Recoverable Codes

  Locally recoverable (LRC) codes have recently been a focus point of researchin coding theory due to their theoretical appeal and applications indistributed storage systems. In an LRC code, any erased symbol of a codewordcan be recovered by accessing only a small number of other symbols. For LRCcodes over a small alphabet (such as binary), the optimal rate-distancetrade-off is unknown. We present several new combinatorial bounds on LRC codesincluding the locality-aware sphere packing and Plotkin bounds. We also developan approach to linear programming (LP) bounds on LRC codes. The resulting LPbound gives better estimates in examples than the other upper bounds known inthe literature. Further, we provide the tightest known upper bound on the rateof linear LRC codes with a given relative distance, an improvement over theprevious best known bounds.

Robust Gradient Descent via Moment Encoding with LDPC Codes

  This paper considers the problem of implementing large-scale gradient descentalgorithms in a distributed computing setting in the presence of {\emstraggling} processors. To mitigate the effect of the stragglers, it has beenpreviously proposed to encode the data with an erasure-correcting code anddecode at the master server at the end of the computation. We, instead, proposeto encode the second-moment of the data with a low density parity-check (LDPC)code. The iterative decoding algorithms for LDPC codes have very lowcomputational overhead and the number of decoding iterations can be made toautomatically adjust with the number of stragglers in the system. We show thatfor a random model for stragglers, the proposed moment encoding based gradientdescent method can be viewed as the stochastic gradient descent method. Thisallows us to obtain convergence guarantees for the proposed solution.Furthermore, the proposed moment encoding based method is shown to outperformthe existing schemes in a real distributed computing setup.

Bounds on the Rate of Linear Locally Repairable Codes over Small  Alphabets

  Locally repairable codes (LRC) have recently been a subject of intenseresearch due to theoretical appeal and their application in distributed storagesystems. In an LRC, any coordinate of a codeword can be recovered by accessingonly few other coordinates. For LRCs over small alphabet (such as binary), theoptimal rate-distance trade-off is unknown. In this paper we provide thetightest known upper bound on the rate of linear LRCs of a given relativedistance, an improvement over any previous result, in particular\cite{cadambe2013upper}.

Constructions of Rank Modulation Codes

  Rank modulation is a way of encoding information to correct errors in flashmemory devices as well as impulse noise in transmission lines. Modeling rankmodulation involves construction of packings of the space of permutationsequipped with the Kendall tau distance.  We present several general constructions of codes in permutations that covera broad range of code parameters. In particular, we show a number of ways inwhich conventional error-correcting codes can be modified to correct errors inthe Kendall space. Codes that we construct afford simple encoding and decodingalgorithms of essentially the same complexity as required to correct errors inthe Hamming metric. For instance, from binary BCH codes we obtain codescorrecting $t$ Kendall errors in $n$ memory cells that support the order of$n!/(\log_2n!)^t$ messages, for any constant $t= 1,2,...$ We also constructfamilies of codes that correct a number of errors that grows with $n$ atvarying rates, from $\Theta(n)$ to $\Theta(n^{2})$. One of our constructionsgives rise to a family of rank modulation codes for which the trade-off betweenthe number of messages and the number of correctable Kendall errors approachesthe optimal scaling rate. Finally, we list a number of possibilities forconstructing codes of finite length, and give examples of rank modulation codeswith specific parameters.

Construction of Almost Disjunct Matrices for Group Testing

  In a \emph{group testing} scheme, a set of tests is designed to identify asmall number $t$ of defective items among a large set (of size $N$) of items.In the non-adaptive scenario the set of tests has to be designed in one-shot.In this setting, designing a testing scheme is equivalent to the constructionof a \emph{disjunct matrix}, an $M \times N$ matrix where the union of supportsof any $t$ columns does not contain the support of any other column. Inprinciple, one wants to have such a matrix with minimum possible number $M$ ofrows (tests). One of the main ways of constructing disjunct matrices relies on\emph{constant weight error-correcting codes} and their \emph{minimumdistance}. In this paper, we consider a relaxed definition of a disjunct matrixknown as \emph{almost disjunct matrix}. This concept is also studied under thename of \emph{weakly separated design} in the literature. The relaxeddefinition allows one to come up with group testing schemes where aclose-to-one fraction of all possible sets of defective items are identifiable.Our main contribution is twofold. First, we go beyond the minimum distanceanalysis and connect the \emph{average distance} of a constant weight code tothe parameters of an almost disjunct matrix constructed from it. Our secondcontribution is to explicitly construct almost disjunct matrices based on ouraverage distance analysis, that have much smaller number of rows than anyprevious explicit construction of disjunct matrices. The parameters of ourconstruction can be varied to cover a large range of relations for $t$ and $N$.

Update-Efficiency and Local Repairability Limits for Capacity  Approaching Codes

  Motivated by distributed storage applications, we investigate the degree towhich capacity achieving encodings can be efficiently updated when a singleinformation bit changes, and the degree to which such encodings can beefficiently (i.e., locally) repaired when single encoded bit is lost.  Specifically, we first develop conditions under which optimumerror-correction and update-efficiency are possible, and establish that thenumber of encoded bits that must change in response to a change in a singleinformation bit must scale logarithmically in the block-length of the code ifwe are to achieve any nontrivial rate with vanishing probability of error overthe binary erasure or binary symmetric channels. Moreover, we show there existcapacity-achieving codes with this scaling.  With respect to local repairability, we develop tight upper and lower boundson the number of remaining encoded bits that are needed to recover a singlelost bit of the encoding. In particular, we show that if the code-rate is$\epsilon$ less than the capacity, then for optimal codes, the maximum numberof codeword symbols required to recover one lost symbol must scale as$\log1/\epsilon$.  Several variations on---and extensions of---these results are also developed.

Nonadaptive group testing with random set of defectives

  In a group testing scheme, a set of tests is designed to identify a smallnumber $t$ of defective items that are present among a large number $N$ ofitems. Each test takes as input a group of items and produces a binary outputindicating whether any defective item is present in the group. In anon-adaptive scheme designing a testing scheme is equivalent to theconstruction of a disjunct matrix, an $M \times N$ binary matrix where theunion of supports of any $t$ columns does not contain the support of any othercolumn. In this paper we consider the scenario where defective items are randomand follow simple probability distributions. In particular we consider thecases where 1) each item can be defective independently with probability$\frac{t}{N}$ and 2) each $t$-set of items can be defective with uniformprobability. In both cases our aim is to design a testing matrix thatsuccessfully identifies the set of defectives with high probability. Both ofthese models have been studied in the literature before and it is known that$O(t\log N)$ tests are necessary as well as sufficient (via random coding) inboth cases. Our main focus is explicit deterministic construction of the testmatrices amenable to above scenarios. One of the most popular ways ofconstructing test matrices relies on \emph{constant-weight error-correctingcodes} and their minimum distance. We go beyond the minimum distance analysisand connect the average distance of a constant weight code to the parameters ofthe resulting test matrix. With our relaxed requirements, we show that usingexplicit constant-weight codes (e.g., based on algebraic geometry codes) we mayachieve a number of tests equal to $O(t \frac{\log^2 N}{ \log t})$ for both thefirst and the second cases.

Local Partial Clique Covers for Index Coding

  Index coding, or broadcasting with side information, is a network codingproblem of most fundamental importance. In this problem, given a directedgraph, each vertex represents a user with a need of information, and theneighborhood of each vertex represents the side information availability tothat user. The aim is to find an encoding to minimum number of bits (optimalrate) that, when broadcasted, will be sufficient to the need of every user. Notonly the optimal rate is intractable, but it is also very hard to characterizewith some other well-studied graph parameter or with a simpler formulation,such as a linear program. Recently there have been a series of works thataddress this question and provide explicit schemes for index coding as theoptimal value of a linear program with rate given by well-studied propertiessuch as local chromatic number or partial clique-covering number. There hasbeen a recent attempt to combine these existing notions of local chromaticnumber and partial clique covering into a unified notion denoted as the localpartial clique cover (Arbabjolfaei and Kim, 2014).  We present a generalized novel upper-bound (encoding scheme) - in the form ofthe minimum value of a linear program - for optimal index coding. Our boundalso combines the notions of local chromatic number and partial clique coveringinto a new definition of the local partial clique cover, which outperforms boththe previous bounds, as well as beats the previous attempt to combination.  Further, we look at the upper bound derived recently by Thapa et al., 2015,and extend their $n$-$\mathsf{GIC}$ (Generalized Interlinked Cycle)construction to $(k,n)$-$\mathsf{GIC}$ graphs, which are a generalization of$k$-partial cliques.

Clustering with Noisy Queries

  In this paper, we initiate a rigorous theoretical study of clustering withnoisy queries (or a faulty oracle). Given a set of $n$ elements, our goal is torecover the true clustering by asking minimum number of pairwise queries to anoracle. Oracle can answer queries of the form : "do elements $u$ and $v$ belongto the same cluster?" -- the queries can be asked interactively (adaptivequeries), or non-adaptively up-front, but its answer can be erroneous withprobability $p$. In this paper, we provide the first information theoreticlower bound on the number of queries for clustering with noisy oracle in bothsituations. We design novel algorithms that closely match this query complexitylower bound, even when the number of clusters is unknown. Moreover, we designcomputationally efficient algorithms both for the adaptive and non-adaptivesettings. The problem captures/generalizes multiple application scenarios. Itis directly motivated by the growing body of work that use crowdsourcing for{\em entity resolution}, a fundamental and challenging data mining task aimedto identify all records in a database referring to the same entity. Here crowdrepresents the noisy oracle, and the number of queries directly relates to thecost of crowdsourcing. Another application comes from the problem of {\em signedge prediction} in social network, where social interactions can be bothpositive and negative, and one must identify the sign of all pair-wiseinteractions by querying a few pairs. Furthermore, clustering with noisy oracleis intimately connected to correlation clustering, leading to improvementtherein. Finally, it introduces a new direction of study in the popular {\emstochastic block model} where one has an incomplete stochastic block modelmatrix to recover the clusters.

Query Complexity of Clustering with Side Information

  Suppose, we are given a set of $n$ elements to be clustered into $k$(unknown) clusters, and an oracle/expert labeler that can interactively answerpair-wise queries of the form, "do two elements $u$ and $v$ belong to the samecluster?". The goal is to recover the optimum clustering by asking the minimumnumber of queries. In this paper, we initiate a rigorous theoretical study ofthis basic problem of query complexity of interactive clustering, and providestrong information theoretic lower bounds, as well as nearly matching upperbounds. Most clustering problems come with a similarity matrix, which is usedby an automated process to cluster similar points together. Our maincontribution in this paper is to show the dramatic power of side informationaka similarity matrix on reducing the query complexity of clustering. Asimilarity matrix represents noisy pair-wise relationships such as one computedby some function on attributes of the elements. A natural noisy model is wheresimilarity values are drawn independently from some arbitrary probabilitydistribution $f_+$ when the underlying pair of elements belong to the samecluster, and from some $f_-$ otherwise. We show that given such a similaritymatrix, the query complexity reduces drastically from $\Theta(nk)$ (nosimilarity matrix) to $O(\frac{k^2\log{n}}{\cH^2(f_+\|f_-)})$ where $\cH^2$denotes the squared Hellinger divergence. Moreover, this is alsoinformation-theoretic optimal within an $O(\log{n})$ factor. Our algorithms areall efficient, and parameter free, i.e., they work without any knowledge of $k,f_+$ and $f_-$, and only depend logarithmically with $n$. Along the way, ourwork also reveals intriguing connection to popular community detection modelssuch as the {\em stochastic block model}, significantly generalizes them, andopens up many venues for interesting future research.

Storage Capacity as an Information-Theoretic Vertex Cover and the Index  Coding Rate

  Motivated by applications in distributed storage, the storage capacity of agraph was recently defined to be the maximum amount of information that can bestored across the vertices of a graph such that the information at any vertexcan be recovered from the information stored at the neighboring vertices.Computing the storage capacity is a fundamental problem in network coding andis related, or equivalent, to some well-studied problems such as index codingwith side information and generalized guessing games. In this paper, weconsider storage capacity as a natural information-theoretic analogue of theminimum vertex cover of a graph. Indeed, while it was known that storagecapacity is upper bounded by minimum vertex cover, we show that by treating itas such we can get a 3/2 approximation for planar graphs, and a 4/3approximation for triangle-free planar graphs. Since the storage capacity isintimately related to the index coding rate, we get a 2 approximation of indexcoding rate for planar graphs and 3/2 approximation for triangle-free planargraphs. We also show a polynomial time approximation scheme for the indexcoding rate when the alphabet size is constant. We then develop a generalmethod of "gadget covering" to upper bound the storage capacity in terms of theaverage of a set of vertex covers. This method is intuitive and leads to theexact characterization of storage capacity for various families of graphs. Asan illustrative example, we use this approach to derive the exact storagecapacity of cycles-with-chords, a family of graphs related to outerplanargraphs. Finally, we generalize the storage capacity notion to include recoveryfrom partial node failures in distributed storage. We show tight upper andlower bounds on this partial recovery capacity that scales nicely with thefraction of failures in a vertex.

High Dimensional Discrete Integration by Hashing and Optimization

  Recently Ermon et al. (2013) pioneered a way to practically computeapproximations to large scale counting or discrete integration problems byusing random hashes. The hashes are used to reduce the counting problem intomany separate discrete optimization problems. The optimization problems thencan be solved by an NP-oracle such as commercial SAT solvers or integer linearprogramming (ILP) solvers. In particular, Ermon et al. showed that if thedomain of integration is $\{0,1\}^n$ then it is possible to obtain a solutionwithin a factor of $16$ of the optimal (a 16-approximation) by this technique.  In many crucial counting tasks, such as computation of partition function offerromagnetic Potts model, the domain of integration is naturally $\{0,1,\dots,q-1\}^n, q>2$, the hypergrid. The straightforward extension of Ermon et al.'smethod allows a $q^2$-approximation for this problem. For large values of $q$,this is undesirable. In this paper, we show an improved technique to obtain anapproximation factor of $4+O(1/q^2)$ to this problem. We are able to achievethis by using an idea of optimization over multiple bins of the hash functions,that can be easily implemented by inequality constraints, or even inunconstrained way. Also the burden on the NP-oracle is not increased by ourmethod (an ILP solver can still be used). Our method extends to the case whenthe domain of integration is the symmetric group, and as a result we can obtaina $(4+o(1))$-approximation of the {\em permanent of} a matrix. All theseresults hold assuming the existence of an NP-oracle. We provide experimentalsimulation results to support the theoretical guarantees of our algorithms,including comparison to the popular Markov-Chain-Monte-Carlo (MCMC) methods.

Linear Programming Approximations for Index Coding

  Index coding, a source coding problem over broadcast channels, has been asubject of both theoretical and practical interest since its introduction (byBirk and Kol, 1998). In short, the problem can be defined as follows: there isan input $\textbf{x} \triangleq (\textbf{x}_1, \dots, \textbf{x}_n)$, a set of$n$ clients who each desire a single symbol $\textbf{x}_i$ of the input, and abroadcaster whose goal is to send as few messages as possible to all clients sothat each one can recover its desired symbol. Additionally, each client hassome predetermined "side information," corresponding to certain symbols of theinput $\textbf{x}$, which we represent as the "side information graph"$\mathcal{G}$. The graph $\mathcal{G}$ has a vertex $v_i$ for each client and adirected edge $(v_i, v_j)$ indicating that client $i$ knows the $j$th symbol ofthe input. Given a fixed side information graph $\mathcal{G}$, we areinterested in determining or approximating the "broadcast rate" of index codingon the graph, i.e. the fewest number of messages the broadcaster can transmitso that every client gets their desired information.  Using index coding schemes based on linear programs (LPs), we take atwo-pronged approach to approximating the broadcast rate. First, extendingearlier work on planar graphs, we focus on approximating the broadcast rate forspecial graph families such as graphs with small chromatic number and diskgraphs. In certain cases, we are able to show that simple LP-based schemes giveconstant-factor approximations of the broadcast rate, which seem extremelydifficult to obtain in the general case. Second, we provide several LP-basedschemes for the general case which are not constant-factor approximations, butwhich strictly improve on the prior best-known schemes.

Codes on hypergraphs

  Codes on hypergraphs are an extension of the well-studied family of codes onbipartite graphs. Bilu and Hoory (2004) constructed an explicit family of codeson regular t-partite hypergraphs whose minimum distance improves earlierestimates of the distance of bipartite-graph codes. They also suggested adecoding algorithm for such codes and estimated its error-correctingcapability.  In this paper we study two aspects of hypergraph codes. First, we compute theweight enumerators of several ensembles of such codes, establishing conditionsunder which they attain the Gilbert-Varshamov bound and deriving estimates oftheir distance. In particular, we show that this bound is attained by codesconstructed on a fixed bipartite graph with a large spectral gap.  We also suggest a new decoding algorithm of hypergraph codes that corrects aconstant fraction of errors, improving upon the algorithm of Bilu and Hoory.

On linear balancing sets

  Let n be an even positive integer and F be the field \GF(2). A word in F^n iscalled balanced if its Hamming weight is n/2. A subset C \subseteq F^n$ iscalled a balancing set if for every word y \in F^n there is a word x \in C suchthat y + x is balanced. It is shown that most linear subspaces of F^n ofdimension slightly larger than 3/2\log_2(n) are balancing sets. Ageneralization of this result to linear subspaces that are "almost balancing"is also presented. On the other hand, it is shown that the problem of decidingwhether a given set of vectors in F^n spans a balancing set, is NP-hard. Anapplication of linear balancing sets is presented for designing efficienterror-correcting coding schemes in which the codewords are balanced.

Random Subdictionaries and Coherence Conditions for Sparse Signal  Recovery

  The most frequently used condition for sampling matrices employed incompressive sampling is the restricted isometry (RIP) property of the matrixwhen restricted to sparse signals. At the same time, imposing this conditionmakes it difficult to find explicit matrices that support recovery of signalsfrom sketches of the optimal (smallest possible)dimension. A number of attemptshave been made to relax or replace the RIP property in sparse recoveryalgorithms. We focus on the relaxation under which the near-isometry propertyholds for most rather than for all submatrices of the sampling matrix, known asstatistical RIP or StRIP condition. We show that sampling matrices ofdimensions $m\times N$ with maximum coherence $\mu=O((k\log^3 N)^{-1/4})$ andmean square coherence $\bar \mu^2=O(1/(k\log N))$ support stable recovery of$k$-sparse signals using Basis Pursuit. These assumptions are satisfied in manyexamples. As a result, we are able to construct sampling matrices that supportrecovery with low error for sparsity $k$ higher than $\sqrt m,$ which exceedsthe range of parameters of the known classes of RIP matrices.

Compression in the Space of Permutations

  We investigate lossy compression (source coding) of data in the form ofpermutations. This problem has direct applications in the storage of ordinaldata or rankings, and in the analysis of sorting algorithms. We analyze therate-distortion characteristic for the permutation space under the uniformdistribution, and the minimum achievable rate of compression that allows abounded distortion after recovery. Our analysis is with respect to differentpractical and useful distortion measures, including Kendall-tau distance,Spearman's footrule, Chebyshev distance and inversion-$\ell_1$ distance. Weestablish equivalence of source code designs under certain distortions and showsimple explicit code designs that incur low encoding/decoding complexities andare asymptotically optimal. Finally, we show that for the Mallows model, apopular nonuniform ranking model on the permutation space, both the entropy andthe maximum distortion at zero rate are much lower than the uniformcounterparts, which motivates the future design of efficient compressionschemes for this model.

Cooperative Local Repair in Distributed Storage

  Erasure-correcting codes, that support local repair of codeword symbols, haveattracted substantial attention recently for their application in distributedstorage systems. This paper investigates a generalization of the usual locallyrepairable codes. In particular, this paper studies a class of codes with thefollowing property: any small set of codeword symbols can be reconstructed(repaired) from a small number of other symbols. This is referred to ascooperative local repair. The main contribution of this paper is bounds on thetrade-off of the minimum distance and the dimension of such codes, as well asexplicit constructions of families of codes that enable cooperative localrepair. Some other results regarding cooperative local repair are alsopresented, including an analysis for the well-known Hadamard/Simplex codes.

Security in Locally Repairable Storage

  In this paper we extend the notion of {\em locally repairable} codes to {\emsecret sharing} schemes. The main problem that we consider is to find optimalways to distribute shares of a secret among a set of storage-nodes(participants) such that the content of each node (share) can be recovered byusing contents of only few other nodes, and at the same time the secret can bereconstructed by only some allowable subsets of nodes. As a special case, aneavesdropper observing some set of specific nodes (such as less than certainnumber of nodes) does not get any information. In other words, we propose tostudy a locally repairable distributed storage system that is secure against a{\em passive eavesdropper} that can observe some subsets of nodes.  We provide a number of results related to such systems including upper-boundsand achievability results on the number of bits that can be securely storedwith these constraints.

Efficient Rank Aggregation via Lehmer Codes

  We propose a novel rank aggregation method based on converting permutationsinto their corresponding Lehmer codes or other subdiagonal images. Lehmercodes, also known as inversion vectors, are vector representations ofpermutations in which each coordinate can take values not restricted by thevalues of other coordinates. This transformation allows for decoupling of thecoordinates and for performing aggregation via simple scalar median or modecomputations. We present simulation results illustrating the performance ofthis completely parallelizable approach and analytically prove that both themode and median aggregation procedure recover the correct centroid aggregatewith small sample complexity when the permutations are drawn according to thewell-known Mallows models. The proposed Lehmer code approach may also be usedon partial rankings, with similar performance guarantees.

Estimation of Sparsity via Simple Measurements

  We consider several related problems of estimating the 'sparsity' or numberof nonzero elements $d$ in a length $n$ vector $\mathbf{x}$ by observing only$\mathbf{b} = M \odot \mathbf{x}$, where $M$ is a predesigned test matrixindependent of $\mathbf{x}$, and the operation $\odot$ varies between problems.We aim to provide a $\Delta$-approximation of sparsity for some constant$\Delta$ with a minimal number of measurements (rows of $M$). This frameworkgeneralizes multiple problems, such as estimation of sparsity in group testingand compressed sensing. We use techniques from coding theory as well asprobabilistic methods to show that $O(D \log D \log n)$ rows are sufficientwhen the operation $\odot$ is logical OR (i.e., group testing), and nearly thismany are necessary, where $D$ is a known upper bound on $d$. When instead theoperation $\odot$ is multiplication over $\mathbb{R}$ or a finite field$\mathbb{F}_q$, we show that respectively $\Theta(D)$ and $\Theta(D \log_q\frac{n}{D})$ measurements are necessary and sufficient.

Semisupervised Clustering by Queries and Locally Encodable Source Coding

  Source coding is the canonical problem of data compression in informationtheory. In a {\em locally encodable} source coding, each compressed bit dependson only few bits of the input. In this paper, we show that a recently popularmodel of semisupervised clustering is equivalent to locally encodable sourcecoding. In this model, the task is to perform multiclass labeling of unlabeledelements. At the beginning, we can ask in parallel a set of simple queries toan oracle who provides (possibly erroneous) binary answers to the queries. Thequeries cannot involve more than two (or a fixed constant number $\Delta$ of)elements. Now the labeling of all the elements (or clustering) must beperformed based on the (noisy) query answers. The goal is to recover all thecorrect labelings while minimizing the number of such queries. The equivalenceto locally encodable source codes leads us to find lower bounds on the numberof queries required in variety of scenarios. We are also able to showfundamental limitations of pairwise `same cluster' queries - and proposepairwise AND queries, that provably performs better in many situations.

Associative Memory using Dictionary Learning and Expander Decoding

  An associative memory is a framework of content-addressable memory thatstores a collection of message vectors (or a dataset) over a neural networkwhile enabling a neurally feasible mechanism to recover any message in thedataset from its noisy version. Designing an associative memory requiresaddressing two main tasks: 1) learning phase: given a dataset, learn a conciserepresentation of the dataset in the form of a graphical model (or a neuralnetwork), 2) recall phase: given a noisy version of a message vector from thedataset, output the correct message vector via a neurally feasible algorithmover the network learnt during the learning phase. This paper studies theproblem of designing a class of neural associative memories which learns anetwork representation for a large dataset that ensures correction against alarge number of adversarial errors during the recall phase. Specifically, theassociative memories designed in this paper can store dataset containing$\exp(n)$ $n$-length message vectors over a network with $O(n)$ nodes and cantolerate $\Omega(\frac{n}{{\rm polylog} n})$ adversarial errors. This papercarries out this memory design by mapping the learning phase and recall phaseto the tasks of dictionary learning with a square dictionary and iterativeerror correction in an expander code, respectively.

The Geometric Block Model

  To capture the inherent geometric features of many community detectionproblems, we propose to use a new random graph model of communities that wecall a Geometric Block Model. The geometric block model generalizes the randomgeometric graphs in the same way that the well-studied stochastic block modelgeneralizes the Erdos-Renyi random graphs. It is also a natural extension ofrandom community models inspired by the recent theoretical and practicaladvancement in community detection. While being a topic of fundamentaltheoretical interest, our main contribution is to show that many practicalcommunity structures are better explained by the geometric block model. We alsoshow that a simple triangle-counting algorithm to detect communities in thegeometric block model is near-optimal. Indeed, even in the regime where theaverage degree of the graph grows only logarithmically with the number ofvertices (sparse-graph), we show that this algorithm performs extremely well,both theoretically and practically. In contrast, the triangle-countingalgorithm is far from being optimum for the stochastic block model. We simulateour results on both real and synthetic datasets to show superior performance ofboth the new model as well as our algorithm.

Novel Impossibility Results for Group-Testing

  In this work we prove non-trivial impossibility results for perhaps thesimplest non-linear estimation problem, that of {\it Group Testing} (GT), viathe recently developed Madiman-Tetali inequalities. Group Testing concernsitself with identifying a hidden set of $d$ defective items from a set of $n$items via $t$ {disjunctive/pooled} measurements ("group tests"). We considerthe linear sparsity regime, i.e. $d = \delta n$ for any constant $\delta >0$, ahitherto little-explored (though natural) regime. In a standardinformation-theoretic setting, where the tests are required to be non-adaptiveand a small probability of reconstruction error is allowed, our lower bounds on$t$ are the {\it first} that improve over the classical counting lower bound,$t/n \geq H(\delta)$, where $H(\cdot)$ is the binary entropy function. Ascorollaries of our result, we show that (i) for $\delta \gtrsim 0.347$,individual testing is essentially optimal, i.e., $t \geq n(1-o(1))$; and (ii)there is an {adaptivity gap}, since for $\delta \in (0.3471,0.3819)$ known{adaptive} GT algorithms require fewer than $n$ tests to reconstruct ${\calD}$, whereas our bounds imply that the best nonadaptive algorithm mustessentially be individual testing of each element. Perhaps most importantly,our work provides a framework for combining combinatorial andinformation-theoretic methods for deriving non-trivial lower bounds for avariety of non-linear estimation problems.

Representation Learning and Recovery in the ReLU Model

  Rectified linear units, or ReLUs, have become the preferred activationfunction for artificial neural networks. In this paper we consider two basiclearning problems assuming that the underlying data follow a generative modelbased on a ReLU-network -- a neural network with ReLU activations. As aprimarily theoretical study, we limit ourselves to a single-layer network. Thefirst problem we study corresponds to dictionary-learning in the presence ofnonlinearity (modeled by the ReLU functions). Given a set of observationvectors $\mathbf{y}^i \in \mathbb{R}^d, i =1, 2, \dots , n$, we aim to recover$d\times k$ matrix $A$ and the latent vectors $\{\mathbf{c}^i\} \subset\mathbb{R}^k$ under the model $\mathbf{y}^i = \mathrm{ReLU}(A\mathbf{c}^i+\mathbf{b})$, where $\mathbf{b}\in \mathbb{R}^d$ is a random bias. We showthat it is possible to recover the column space of $A$ within an error of$O(d)$ (in Frobenius norm) under certain conditions on the probabilitydistribution of $\mathbf{b}$.  The second problem we consider is that of robust recovery of the signal inthe presence of outliers, i.e., large but sparse noise. In this setting we areinterested in recovering the latent vector $\mathbf{c}$ from its noisynonlinear sketches of the form $\mathbf{v} = \mathrm{ReLU}(A\mathbf{c}) +\mathbf{e}+\mathbf{w}$, where $\mathbf{e} \in \mathbb{R}^d$ denotes theoutliers with sparsity $s$ and $\mathbf{w} \in \mathbb{R}^d$ denote the densebut small noise. This line of work has recently been studied (Soltanolkotabi,2017) without the presence of outliers. For this problem, we show that ageneralized LASSO algorithm is able to recover the signal $\mathbf{c} \in\mathbb{R}^k$ within an $\ell_2$ error of $O(\sqrt{\frac{(k+s)\log d}{d}})$when $A$ is a random Gaussian matrix.

Connectivity in Random Annulus Graphs and the Geometric Block Model

  We provide new connectivity results for {\em vertex-random graphs} or {\emrandom annulus graphs} which are significant generalizations of randomgeometric graphs. Random geometric graphs (RGG) are one of the most basicmodels of random graphs for spatial networks proposed by Gilbert in 1961,shortly after the introduction of the Erd\H{o}s-R\'{en}yi random graphs. Theyresemble social networks in many ways (e.g. by spontaneously creating clusterof nodes with high modularity). The connectivity properties of RGG have beenstudied since its introduction, and analyzing them has been significantlyharder than their Erd\H{o}s-R\'{en}yi counterparts due to correlated edgeformation.  Our next contribution is in using the connectivity of random annulus graphsto provide necessary and sufficient conditions for efficient recovery ofcommunities for {\em the geometric block model} (GBM). The GBM is aprobabilistic model for community detection defined over an RGG in a similarspirit as the popular {\em stochastic block model}, which is defined over anErd\H{o}s-R\'{en}yi random graph. The geometric block model inherits thetransitivity properties of RGGs and thus models communities better than astochastic block model. However, analyzing them requires fresh perspectives asall prior tools fail due to correlation in edge formation. We provide a simpleand efficient algorithm that can recover communities in GBM exactly with highprobability in the regime of connectivity.

Low rank approximation and decomposition of large matrices using error  correcting codes

  Low rank approximation is an important tool used in many applications ofsignal processing and machine learning. Recently, randomized sketchingalgorithms were proposed to effectively construct low rank approximations andobtain approximate singular value decompositions of large matrices. Similarideas were used to solve least squares regression problems. In this paper, weshow how matrices from error correcting codes can be used to find such low rankapproximations and matrix decompositions, and extend the framework to linearleast squares regression problems. The benefits of using these code matricesare the following: (i) They are easy to generate and they reduce randomnesssignificantly. (ii) Code matrices with mild properties satisfy the subspaceembedding property, and have a better chance of preserving the geometry of anentire subspace of vectors. (iii) For parallel and distributed applications,code matrices have significant advantages over structured random matrices andGaussian random matrices. (iv) Unlike Fourier or Hadamard transform matrices,which require sampling $O(k\log k)$ columns for a rank-$k$ approximation, thelog factor is not necessary for certain types of code matrices. That is,$(1+\epsilon)$ optimal Frobenius norm error can be achieved for a rank-$k$approximation with $O(k/\epsilon)$ samples. (v) Fast multiplication is possiblewith structured code matrices, so fast approximations can be achieved forgeneral dense input matrices. (vi) For least squares regression problem$\min\|Ax-b\|_2$ where $A\in \mathbb{R}^{n\times d}$, the $(1+\epsilon)$relative error approximation can be achieved with $O(d/\epsilon)$ samples, withhigh probability, when certain code matrices are used.

Clustering Via Crowdsourcing

  In recent years, crowdsourcing, aka human aided computation has emerged as aneffective platform for solving problems that are considered complex formachines alone. Using human is time-consuming and costly due to monetarycompensations. Therefore, a crowd based algorithm must judiciously use anyinformation computed through an automated process, and ask minimum number ofquestions to the crowd adaptively.  One such problem which has received significant attention is {\em entityresolution}. Formally, we are given a graph $G=(V,E)$ with unknown edge set $E$where $G$ is a union of $k$ (again unknown, but typically large $O(n^\alpha)$,for $\alpha>0$) disjoint cliques $G_i(V_i, E_i)$, $i =1, \dots, k$. The goal isto retrieve the sets $V_i$s by making minimum number of pair-wise queries $V\times V\to\{\pm1\}$ to an oracle (the crowd). When the answer to each query iscorrect, e.g. via resampling, then this reduces to finding connected componentsin a graph. On the other hand, when crowd answers may be incorrect, itcorresponds to clustering over minimum number of noisy inputs. Even, withperfect answers, a simple lower and upper bound of $\Theta(nk)$ on querycomplexity can be shown. A major contribution of this paper is to reduce thequery complexity to linear or even sublinear in $n$ when mild side informationis provided by a machine, and even in presence of crowd errors which are notcorrectable via resampling. We develop new information theoretic lower boundson the query complexity of clustering with side information and errors, and ourupper bounds closely match with them. Our algorithms are naturallyparallelizable, and also give near-optimal bounds on the number of adaptiverounds required to match the query complexity.

