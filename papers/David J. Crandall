DeepDiary: Automatic Caption Generation for Lifelogging Image Streams

  Lifelogging cameras capture everyday life from a first-person perspective,but generate so much data that it is hard for users to browse and organizetheir image collections effectively. In this paper, we propose to use automaticimage captioning algorithms to generate textual representations of thesecollections. We develop and explore novel techniques based on deep learning togenerate captions for both individual images and image streams, using temporalconsistency constraints to create summaries that are both more compact and lessnoisy. We evaluate our techniques with quantitative and qualitative results,and apply captioning to an image retrieval application for finding potentiallyprivate images. Our results suggest that our automatic captioning algorithms,while imperfect, may work well enough to help users manage lifelogging photocollections.

Identifying First-person Camera Wearers in Third-person Videos

  We consider scenarios in which we wish to perform joint scene understanding,object tracking, activity recognition, and other tasks in environments in whichmultiple people are wearing body-worn cameras while a third-person staticcamera also captures the scene. To do this, we need to establish person-levelcorrespondences across first- and third-person videos, which is challengingbecause the camera wearer is not visible from his/her own egocentric video,preventing the use of direct feature matching. In this paper, we propose a newsemi-Siamese Convolutional Neural Network architecture to address this novelchallenge. We formulate the problem as learning a joint embedding space forfirst- and third-person videos that considers both spatial- and motion-domaincues. A new triplet loss function is designed to minimize the distance betweencorrect first- and third-person matches while maximizing the distance betweenincorrect ones. This end-to-end approach performs significantly better thanseveral baselines, in part by learning the first- and third-person featuresoptimized for matching jointly with the distance measure itself.

A Unified Model for Near and Remote Sensing

  We propose a novel convolutional neural network architecture for estimatinggeospatial functions such as population density, land cover, or land use. Inour approach, we combine overhead and ground-level images in an end-to-endtrainable neural network, which uses kernel regression and density estimationto convert features extracted from the ground-level images into a dense featuremap. The output of this network is a dense estimate of the geospatial functionin the form of a pixel-level labeling of the overhead image. To evaluate ourapproach, we created a large dataset of overhead and ground-level images from amajor urban area with three sets of labels: land use, building function, andbuilding age. We find that our approach is more accurate for all tasks, in somecases dramatically so.

A Study of Cross-domain Generative Models applied to Cartoon Series

  We investigate Generative Adversarial Networks (GANs) to model one particularkind of image: frames from TV cartoons. Cartoons are particularly interestingbecause their visual appearance emphasizes the important semantic informationabout a scene while abstracting out the less important details, but eachcartoon series has a distinctive artistic style that performs this abstractionin different ways. We consider a dataset consisting of images from two populartelevision cartoon series, Family Guy and The Simpsons. We examine the abilityof GANs to generate images from each of these two domains, when trainedindependently as well as on both domains jointly. We find that generativemodels may be capable of finding semantic-level correspondences between thesetwo image domains despite the unsupervised setting, even when the training datadoes not give labeled alignments between them.

Minimizing Supervision for Free-space Segmentation

  Identifying "free-space," or safely driveable regions in the scene ahead, isa fundamental task for autonomous navigation. While this task can be addressedusing semantic segmentation, the manual labor involved in creating pixelwiseannotations to train the segmentation model is very costly. Although weaklysupervised segmentation addresses this issue, most methods are not designed forfree-space. In this paper, we observe that homogeneous texture and location aretwo key characteristics of free-space, and develop a novel, practical frameworkfor free-space segmentation with minimal human supervision. Our experimentsshow that our framework performs better than other weakly supervised methodswhile using less supervision. Our work demonstrates the potential forperforming free-space segmentation without tedious and costly manualannotation, which will be important for adapting autonomous driving systems todifferent types of vehicles and environments

Automatic Estimation of Ice Bottom Surfaces from Radar Imagery

  Ground-penetrating radar on planes and satellites now makes it practical tocollect 3D observations of the subsurface structure of the polar ice sheets,providing crucial data for understanding and tracking global climate change.But converting these noisy readings into useful observations is generally doneby hand, which is impractical at a continental scale. In this paper, we proposea computer vision-based technique for extracting 3D ice-bottom surfaces byviewing the task as an inference problem on a probabilistic graphical model. Wefirst generate a seed surface subject to a set of constraints, and thenincorporate additional sources of evidence to refine it via discrete energyminimization. We evaluate the performance of the tracking algorithm on 7topographic sequences (each with over 3000 radar images) collected from theCanadian Arctic Archipelago with respect to human-labeled ground truth.

Fully-Coupled Two-Stream Spatiotemporal Networks for Extremely Low  Resolution Action Recognition

  A major emerging challenge is how to protect people's privacy as cameras andcomputer vision are increasingly integrated into our daily lives, including insmart devices inside homes. A potential solution is to capture and record justthe minimum amount of information needed to perform a task of interest. In thispaper, we propose a fully-coupled two-stream spatiotemporal architecture forreliable human action recognition on extremely low resolution (e.g., 12x16pixel) videos. We provide an efficient method to extract spatial and temporalfeatures and to aggregate them into a robust feature representation for anentire action video sequence. We also consider how to incorporate highresolution videos during training in order to build better low resolutionaction recognition models. We evaluate on two publicly-available datasets,showing significant improvements over the state-of-the-art.

Multi-Task Spatiotemporal Neural Networks for Structured Surface  Reconstruction

  Deep learning methods have surpassed the performance of traditionaltechniques on a wide range of problems in computer vision, but nearly all ofthis work has studied consumer photos, where precisely correct output is oftennot critical. It is less clear how well these techniques may apply onstructured prediction problems where fine-grained output with high precision isrequired, such as in scientific imaging domains. Here we consider the problemof segmenting echogram radar data collected from the polar ice sheets, which ischallenging because segmentation boundaries are often very weak and there is ahigh degree of noise. We propose a multi-task spatiotemporal neural networkthat combines 3D ConvNets and Recurrent Neural Networks (RNNs) to estimate icesurface boundaries from sequences of tomographic radar images. We show that ourmodel outperforms the state-of-the-art on this problem by (1) avoiding the needfor hand-tuned parameters, (2) extracting multiple surfaces (ice-air andice-bed) simultaneously, (3) requiring less non-visual metadata, and (4) beingabout 6 times faster.

Egocentric Vision-based Future Vehicle Localization for Intelligent  Driving Assistance Systems

  Predicting the future location of vehicles is essential for safety-criticalapplications such as advanced driver assistance systems (ADAS) and autonomousdriving. This paper introduces a novel approach to simultaneously predict boththe location and scale of target vehicles in the first-person (egocentric) viewof an ego-vehicle. We present a multi-stream recurrent neural network (RNN)encoder-decoder model that separately captures both object location and scaleand pixel-level observations for future vehicle localization. We show thatincorporating dense optical flow improves prediction results significantlysince it captures information about motion as well as appearance change. Wealso find that explicitly modeling future motion of the ego-vehicle improvesthe prediction accuracy, which could be especially beneficial in intelligentand automated vehicles that have motion planning capability. To evaluate theperformance of our approach, we present a new dataset of first-person videoscollected from a variety of scenarios at road intersections, which areparticularly challenging moments for prediction because vehicle trajectoriesare diverse and dynamic.

Temporal Recurrent Networks for Online Action Detection

  Most work on temporal action detection is formulated as an offline problem,in which the start and end times of actions are determined after the entirevideo is fully observed. However, important real-time applications includingsurveillance and driver assistance systems require identifying actions as soonas each video frame arrives, based only on current and historical observations.In this paper, we propose a novel framework, Temporal Recurrent Network (TRN),to model greater temporal context of a video frame by simultaneously performingonline action detection and anticipation of the immediate future. At eachmoment in time, our approach makes use of both accumulated historical evidenceand predicted future information to better recognize the action that iscurrently occurring, and integrates both of these into a unified end-to-endarchitecture. We evaluate our approach on two popular online action detectiondatasets, HDD and TVSeries, as well as another widely used dataset, THUMOS'14.The results show that TRN significantly outperforms the state-of-the-art.

Unsupervised Traffic Accident Detection in First-Person Videos

  Recognizing abnormal events such as traffic violations and accidents innatural driving scenes is essential for successful autonomous and advanceddriver assistance systems. However, most work on video anomaly detectionsuffers from one of two crucial drawbacks. First, it assumes cameras are fixedand videos have a static background, which is reasonable for surveillanceapplications but not for vehicle-mounted cameras. Second, it poses the problemas one-class classification, which relies on arduous human annotation and onlyrecognizes categories of anomalies that have been explicitly trained. In thispaper, we propose an unsupervised approach for traffic accident detection infirst-person videos. Our major novelty is to detect anomalies by predicting thefuture locations of traffic participants and then monitoring the predictionaccuracy and consistency metrics with three different strategies. To evaluateour approach, we introduce a new dataset of diverse traffic accidents, AnAnAccident Detection (A3D), as well as another publicly-available dataset.Experimental results show that our approach outperforms the state-of-the-art.

Joint Person Segmentation and Identification in Synchronized First- and  Third-person Videos

  In a world of pervasive cameras, public spaces are often captured frommultiple perspectives by cameras of different types, both fixed and mobile. Animportant problem is to organize these heterogeneous collections of videos byfinding connections between them, such as identifying correspondences betweenthe people appearing in the videos and the people holding or wearing thecameras. In this paper, we wish to solve two specific problems: (1) given twoor more synchronized third-person videos of a scene, produce a pixel-levelsegmentation of each visible person and identify corresponding people acrossdifferent views (i.e., determine who in camera A corresponds with whom incamera B), and (2) given one or more synchronized third-person videos as wellas a first-person video taken by a mobile or wearable camera, segment andidentify the camera wearer in the third-person videos. Unlike previous workwhich requires ground truth bounding boxes to estimate the correspondences, weperform person segmentation and identification jointly. We find that solvingthese two problems simultaneously is mutually beneficial, because betterfine-grained segmentation allows us to better perform matching across views,and information from multiple views helps us perform more accuratesegmentation. We evaluate our approach on two challenging datasets ofinteracting people captured from multiple wearable cameras, and show that ourproposed method performs significantly better than the state-of-the-art on bothperson segmentation and identification.

