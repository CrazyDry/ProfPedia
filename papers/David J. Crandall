Identifying First-person Camera Wearers in Third-person Videos

  We consider scenarios in which we wish to perform joint scene understanding,
object tracking, activity recognition, and other tasks in environments in which
multiple people are wearing body-worn cameras while a third-person static
camera also captures the scene. To do this, we need to establish person-level
correspondences across first- and third-person videos, which is challenging
because the camera wearer is not visible from his/her own egocentric video,
preventing the use of direct feature matching. In this paper, we propose a new
semi-Siamese Convolutional Neural Network architecture to address this novel
challenge. We formulate the problem as learning a joint embedding space for
first- and third-person videos that considers both spatial- and motion-domain
cues. A new triplet loss function is designed to minimize the distance between
correct first- and third-person matches while maximizing the distance between
incorrect ones. This end-to-end approach performs significantly better than
several baselines, in part by learning the first- and third-person features
optimized for matching jointly with the distance measure itself.


DeepDiary: Automatic Caption Generation for Lifelogging Image Streams

  Lifelogging cameras capture everyday life from a first-person perspective,
but generate so much data that it is hard for users to browse and organize
their image collections effectively. In this paper, we propose to use automatic
image captioning algorithms to generate textual representations of these
collections. We develop and explore novel techniques based on deep learning to
generate captions for both individual images and image streams, using temporal
consistency constraints to create summaries that are both more compact and less
noisy. We evaluate our techniques with quantitative and qualitative results,
and apply captioning to an image retrieval application for finding potentially
private images. Our results suggest that our automatic captioning algorithms,
while imperfect, may work well enough to help users manage lifelogging photo
collections.


A Unified Model for Near and Remote Sensing

  We propose a novel convolutional neural network architecture for estimating
geospatial functions such as population density, land cover, or land use. In
our approach, we combine overhead and ground-level images in an end-to-end
trainable neural network, which uses kernel regression and density estimation
to convert features extracted from the ground-level images into a dense feature
map. The output of this network is a dense estimate of the geospatial function
in the form of a pixel-level labeling of the overhead image. To evaluate our
approach, we created a large dataset of overhead and ground-level images from a
major urban area with three sets of labels: land use, building function, and
building age. We find that our approach is more accurate for all tasks, in some
cases dramatically so.


A Study of Cross-domain Generative Models applied to Cartoon Series

  We investigate Generative Adversarial Networks (GANs) to model one particular
kind of image: frames from TV cartoons. Cartoons are particularly interesting
because their visual appearance emphasizes the important semantic information
about a scene while abstracting out the less important details, but each
cartoon series has a distinctive artistic style that performs this abstraction
in different ways. We consider a dataset consisting of images from two popular
television cartoon series, Family Guy and The Simpsons. We examine the ability
of GANs to generate images from each of these two domains, when trained
independently as well as on both domains jointly. We find that generative
models may be capable of finding semantic-level correspondences between these
two image domains despite the unsupervised setting, even when the training data
does not give labeled alignments between them.


Minimizing Supervision for Free-space Segmentation

  Identifying "free-space," or safely driveable regions in the scene ahead, is
a fundamental task for autonomous navigation. While this task can be addressed
using semantic segmentation, the manual labor involved in creating pixelwise
annotations to train the segmentation model is very costly. Although weakly
supervised segmentation addresses this issue, most methods are not designed for
free-space. In this paper, we observe that homogeneous texture and location are
two key characteristics of free-space, and develop a novel, practical framework
for free-space segmentation with minimal human supervision. Our experiments
show that our framework performs better than other weakly supervised methods
while using less supervision. Our work demonstrates the potential for
performing free-space segmentation without tedious and costly manual
annotation, which will be important for adapting autonomous driving systems to
different types of vehicles and environments


Automatic Estimation of Ice Bottom Surfaces from Radar Imagery

  Ground-penetrating radar on planes and satellites now makes it practical to
collect 3D observations of the subsurface structure of the polar ice sheets,
providing crucial data for understanding and tracking global climate change.
But converting these noisy readings into useful observations is generally done
by hand, which is impractical at a continental scale. In this paper, we propose
a computer vision-based technique for extracting 3D ice-bottom surfaces by
viewing the task as an inference problem on a probabilistic graphical model. We
first generate a seed surface subject to a set of constraints, and then
incorporate additional sources of evidence to refine it via discrete energy
minimization. We evaluate the performance of the tracking algorithm on 7
topographic sequences (each with over 3000 radar images) collected from the
Canadian Arctic Archipelago with respect to human-labeled ground truth.


Fully-Coupled Two-Stream Spatiotemporal Networks for Extremely Low
  Resolution Action Recognition

  A major emerging challenge is how to protect people's privacy as cameras and
computer vision are increasingly integrated into our daily lives, including in
smart devices inside homes. A potential solution is to capture and record just
the minimum amount of information needed to perform a task of interest. In this
paper, we propose a fully-coupled two-stream spatiotemporal architecture for
reliable human action recognition on extremely low resolution (e.g., 12x16
pixel) videos. We provide an efficient method to extract spatial and temporal
features and to aggregate them into a robust feature representation for an
entire action video sequence. We also consider how to incorporate high
resolution videos during training in order to build better low resolution
action recognition models. We evaluate on two publicly-available datasets,
showing significant improvements over the state-of-the-art.


Multi-Task Spatiotemporal Neural Networks for Structured Surface
  Reconstruction

  Deep learning methods have surpassed the performance of traditional
techniques on a wide range of problems in computer vision, but nearly all of
this work has studied consumer photos, where precisely correct output is often
not critical. It is less clear how well these techniques may apply on
structured prediction problems where fine-grained output with high precision is
required, such as in scientific imaging domains. Here we consider the problem
of segmenting echogram radar data collected from the polar ice sheets, which is
challenging because segmentation boundaries are often very weak and there is a
high degree of noise. We propose a multi-task spatiotemporal neural network
that combines 3D ConvNets and Recurrent Neural Networks (RNNs) to estimate ice
surface boundaries from sequences of tomographic radar images. We show that our
model outperforms the state-of-the-art on this problem by (1) avoiding the need
for hand-tuned parameters, (2) extracting multiple surfaces (ice-air and
ice-bed) simultaneously, (3) requiring less non-visual metadata, and (4) being
about 6 times faster.


Egocentric Vision-based Future Vehicle Localization for Intelligent
  Driving Assistance Systems

  Predicting the future location of vehicles is essential for safety-critical
applications such as advanced driver assistance systems (ADAS) and autonomous
driving. This paper introduces a novel approach to simultaneously predict both
the location and scale of target vehicles in the first-person (egocentric) view
of an ego-vehicle. We present a multi-stream recurrent neural network (RNN)
encoder-decoder model that separately captures both object location and scale
and pixel-level observations for future vehicle localization. We show that
incorporating dense optical flow improves prediction results significantly
since it captures information about motion as well as appearance change. We
also find that explicitly modeling future motion of the ego-vehicle improves
the prediction accuracy, which could be especially beneficial in intelligent
and automated vehicles that have motion planning capability. To evaluate the
performance of our approach, we present a new dataset of first-person videos
collected from a variety of scenarios at road intersections, which are
particularly challenging moments for prediction because vehicle trajectories
are diverse and dynamic.


Temporal Recurrent Networks for Online Action Detection

  Most work on temporal action detection is formulated as an offline problem,
in which the start and end times of actions are determined after the entire
video is fully observed. However, important real-time applications including
surveillance and driver assistance systems require identifying actions as soon
as each video frame arrives, based only on current and historical observations.
In this paper, we propose a novel framework, Temporal Recurrent Network (TRN),
to model greater temporal context of a video frame by simultaneously performing
online action detection and anticipation of the immediate future. At each
moment in time, our approach makes use of both accumulated historical evidence
and predicted future information to better recognize the action that is
currently occurring, and integrates both of these into a unified end-to-end
architecture. We evaluate our approach on two popular online action detection
datasets, HDD and TVSeries, as well as another widely used dataset, THUMOS'14.
The results show that TRN significantly outperforms the state-of-the-art.


Unsupervised Traffic Accident Detection in First-Person Videos

  Recognizing abnormal events such as traffic violations and accidents in
natural driving scenes is essential for successful autonomous and advanced
driver assistance systems. However, most work on video anomaly detection
suffers from one of two crucial drawbacks. First, it assumes cameras are fixed
and videos have a static background, which is reasonable for surveillance
applications but not for vehicle-mounted cameras. Second, it poses the problem
as one-class classification, which relies on arduous human annotation and only
recognizes categories of anomalies that have been explicitly trained. In this
paper, we propose an unsupervised approach for traffic accident detection in
first-person videos. Our major novelty is to detect anomalies by predicting the
future locations of traffic participants and then monitoring the prediction
accuracy and consistency metrics with three different strategies. To evaluate
our approach, we introduce a new dataset of diverse traffic accidents, AnAn
Accident Detection (A3D), as well as another publicly-available dataset.
Experimental results show that our approach outperforms the state-of-the-art.


Joint Person Segmentation and Identification in Synchronized First- and
  Third-person Videos

  In a world of pervasive cameras, public spaces are often captured from
multiple perspectives by cameras of different types, both fixed and mobile. An
important problem is to organize these heterogeneous collections of videos by
finding connections between them, such as identifying correspondences between
the people appearing in the videos and the people holding or wearing the
cameras. In this paper, we wish to solve two specific problems: (1) given two
or more synchronized third-person videos of a scene, produce a pixel-level
segmentation of each visible person and identify corresponding people across
different views (i.e., determine who in camera A corresponds with whom in
camera B), and (2) given one or more synchronized third-person videos as well
as a first-person video taken by a mobile or wearable camera, segment and
identify the camera wearer in the third-person videos. Unlike previous work
which requires ground truth bounding boxes to estimate the correspondences, we
perform person segmentation and identification jointly. We find that solving
these two problems simultaneously is mutually beneficial, because better
fine-grained segmentation allows us to better perform matching across views,
and information from multiple views helps us perform more accurate
segmentation. We evaluate our approach on two challenging datasets of
interacting people captured from multiple wearable cameras, and show that our
proposed method performs significantly better than the state-of-the-art on both
person segmentation and identification.


