What Do Deep CNNs Learn About Objects?

  Deep convolutional neural networks learn extremely powerful image
representations, yet most of that power is hidden in the millions of deep-layer
parameters. What exactly do these parameters represent? Recent work has started
to analyse CNN representations, finding that, e.g., they are invariant to some
2D transformations Fischer et al. (2014), but are confused by particular types
of image noise Nguyen et al. (2014). In this work, we delve deeper and ask: how
invariant are CNNs to object-class variations caused by 3D shape, pose, and
photorealism?


Simultaneous Deep Transfer Across Domains and Tasks

  Recent reports suggest that a generic supervised deep CNN model trained on a
large-scale dataset reduces, but does not remove, dataset bias. Fine-tuning
deep models in a new domain can require a significant amount of labeled data,
which for many applications is simply not available. We propose a new CNN
architecture to exploit unlabeled and sparsely labeled target domain data. Our
approach simultaneously optimizes for domain invariance to facilitate domain
transfer and uses a soft label distribution matching loss to transfer
information between tasks. Our proposed adaptation method offers empirical
performance which exceeds previously published results on two standard
benchmark visual domain adaptation tasks, evaluated across supervised and
semi-supervised adaptation settings.


Improving LSTM-based Video Description with Linguistic Knowledge Mined
  from Text

  This paper investigates how linguistic knowledge mined from large text
corpora can aid the generation of natural language descriptions of videos.
Specifically, we integrate both a neural language model and distributional
semantics trained on large text corpora into a recent LSTM-based architecture
for video description. We evaluate our approach on a collection of Youtube
videos as well as two large movie description datasets showing significant
improvements in grammaticality while modestly improving descriptive quality.


Hierarchical Reinforcement Learning with Hindsight

  Reinforcement Learning (RL) algorithms can suffer from poor sample efficiency
when rewards are delayed and sparse. We introduce a solution that enables
agents to learn temporally extended actions at multiple levels of abstraction
in a sample efficient and automated fashion. Our approach combines universal
value functions and hindsight learning, allowing agents to learn policies
belonging to different time scales in parallel. We show that our method
significantly accelerates learning in a variety of discrete and continuous
tasks.


Adapting control policies from simulation to reality using a pairwise
  loss

  This paper proposes an approach to domain transfer based on a pairwise loss
function that helps transfer control policies learned in simulation onto a real
robot. We explore the idea in the context of a 'category level' manipulation
task where a control policy is learned that enables a robot to perform a mating
task involving novel objects. We explore the case where depth images are used
as the main form of sensor input. Our experimental results demonstrate that
proposed method consistently outperforms baseline methods that train only in
simulation or that combine real and simulated data in a naive way.


Towards Adapting ImageNet to Reality: Scalable Domain Adaptation with
  Implicit Low-rank Transformations

  Images seen during test time are often not from the same distribution as
images used for learning. This problem, known as domain shift, occurs when
training classifiers from object-centric internet image databases and trying to
apply them directly to scene understanding tasks. The consequence is often
severe performance degradation and is one of the major barriers for the
application of classifiers in real-world systems. In this paper, we show how to
learn transform-based domain adaptation classifiers in a scalable manner. The
key idea is to exploit an implicit rank constraint, originated from a
max-margin domain adaptation formulation, to make optimization tractable.
Experiments show that the transformation between domains can be very
efficiently learned from data and easily applied to new categories. This begins
to bridge the gap between large-scale internet image collections and object
images captured in everyday life environments.


Efficient Learning of Domain-invariant Image Representations

  We present an algorithm that learns representations which explicitly
compensate for domain mismatch and which can be efficiently realized as linear
classifiers. Specifically, we form a linear transformation that maps features
from the target (test) domain to the source (training) domain as part of
training the classifier. We optimize both the transformation and classifier
parameters jointly, and introduce an efficient cost function based on
misclassification loss. Our method combines several features previously
unavailable in a single algorithm: multi-class adaptation through
representation learning, ability to map across heterogeneous feature spaces,
and scalability to large datasets. We present experiments on several image
datasets that demonstrate improved accuracy and computational advantages
compared to previous approaches.


Detector Discovery in the Wild: Joint Multiple Instance and
  Representation Learning

  We develop methods for detector learning which exploit joint training over
both weak and strong labels and which transfer learned perceptual
representations from strongly-labeled auxiliary tasks. Previous methods for
weak-label learning often learn detector models independently using latent
variable optimization, but fail to share deep representation knowledge across
classes and usually require strong initialization. Other previous methods
transfer deep representations from domains with strong labels to those with
only weak labels, but do not optimize over individual latent boxes, and thus
may miss specific salient structures for a particular category. We propose a
model that subsumes these previous approaches, and simultaneously trains a
representation and detectors for categories with either weak or strong labels
present. We provide a novel formulation of a joint multiple instance learning
method that includes examples from classification-style data when available,
and also performs domain transfer learning to improve the underlying detector
representation. Our model outperforms known methods on ImageNet-200 detection
with weak labels.


Deep Domain Confusion: Maximizing for Domain Invariance

  Recent reports suggest that a generic supervised deep CNN model trained on a
large-scale dataset reduces, but does not remove, dataset bias on a standard
benchmark. Fine-tuning deep models in a new domain can require a significant
amount of data, which for many applications is simply not available. We propose
a new CNN architecture which introduces an adaptation layer and an additional
domain confusion loss, to learn a representation that is both semantically
meaningful and domain invariant. We additionally show that a domain confusion
metric can be used for model selection to determine the dimension of an
adaptation layer and the best position for the layer in the CNN architecture.
Our proposed adaptation method offers empirical performance which exceeds
previously published results on a standard benchmark visual domain adaptation
task.


Translating Videos to Natural Language Using Deep Recurrent Neural
  Networks

  Solving the visual symbol grounding problem has long been a goal of
artificial intelligence. The field appears to be advancing closer to this goal
with recent breakthroughs in deep learning for natural language grounding in
static images. In this paper, we propose to translate videos directly to
sentences using a unified deep neural network with both convolutional and
recurrent structure. Described video datasets are scarce, and most existing
methods have been applied to toy domains with a small vocabulary of possible
words. By transferring knowledge from 1.2M+ images with category labels and
100,000+ images with captions, our method is able to create sentence
descriptions of open-domain videos with large vocabularies. We compare our
approach with recent work using language generation metrics, subject, verb, and
object prediction accuracy, and a human evaluation.


Sequence to Sequence -- Video to Text

  Real-world videos often have complex dynamics; and methods for generating
open-domain video descriptions should be sensitive to temporal structure and
allow both input (sequence of frames) and output (sequence of words) of
variable length. To approach this problem, we propose a novel end-to-end
sequence-to-sequence model to generate captions for videos. For this we exploit
recurrent neural networks, specifically LSTMs, which have demonstrated
state-of-the-art performance in image caption generation. Our LSTM model is
trained on video-sentence pairs and learns to associate a sequence of video
frames to a sequence of words in order to generate a description of the event
in the video clip. Our model naturally is able to learn the temporal structure
of the sequence of frames as well as the sequence model of the generated
sentences, i.e. a language model. We evaluate several variants of our model
that exploit different visual features on a standard set of YouTube videos and
two movie description datasets (M-VAD and MPII-MD).


Modeling Radiometric Uncertainty for Vision with Tone-mapped Color
  Images

  To produce images that are suitable for display, tone-mapping is widely used
in digital cameras to map linear color measurements into narrow gamuts with
limited dynamic range. This introduces non-linear distortion that must be
undone, through a radiometric calibration process, before computer vision
systems can analyze such photographs radiometrically. This paper considers the
inherent uncertainty of undoing the effects of tone-mapping. We observe that
this uncertainty varies substantially across color space, making some pixels
more reliable than others. We introduce a model for this uncertainty and a
method for fitting it to a given camera or imaging pipeline. Once fit, the
model provides for each pixel in a tone-mapped digital photograph a probability
distribution over linear scene colors that could have induced it. We
demonstrate how these distributions can be useful for visual inference by
incorporating them into estimation algorithms for a representative set of
vision tasks.


Natural Language Object Retrieval

  In this paper, we address the task of natural language object retrieval, to
localize a target object within a given image based on a natural language query
of the object. Natural language object retrieval differs from text-based image
retrieval task as it involves spatial information about objects within the
scene and global scene context. To address this issue, we propose a novel
Spatial Context Recurrent ConvNet (SCRC) model as scoring function on candidate
boxes for object retrieval, integrating spatial configurations and global
scene-level contextual information into the network. Our model processes query
text, local image descriptors, spatial configurations and global context
features through a recurrent network, outputs the probability of the query text
conditioned on each candidate box as a score for the box, and can transfer
visual-linguistic knowledge from image captioning domain to our task.
Experimental results demonstrate that our method effectively utilizes both
local and global information, outperforming previous baseline methods
significantly on different datasets and scenarios, and can exploit large scale
vision and language datasets for knowledge transfer.


Return of Frustratingly Easy Domain Adaptation

  Unlike human learning, machine learning often fails to handle changes between
training (source) and test (target) input distributions. Such domain shifts,
common in practical scenarios, severely damage the performance of conventional
machine learning methods. Supervised domain adaptation methods have been
proposed for the case when the target data have labels, including some that
perform very well despite being "frustratingly easy" to implement. However, in
practice, the target domain is often unlabeled, requiring unsupervised
adaptation. We propose a simple, effective, and efficient method for
unsupervised domain adaptation called CORrelation ALignment (CORAL). CORAL
minimizes domain shift by aligning the second-order statistics of source and
target distributions, without requiring any target labels. Even though it is
extraordinarily simple--it can be implemented in four lines of Matlab
code--CORAL performs remarkably well in extensive evaluations on standard
benchmark datasets.


High precision grasp pose detection in dense clutter

  This paper considers the problem of grasp pose detection in point clouds. We
follow a general algorithmic structure that first generates a large set of
6-DOF grasp candidates and then classifies each of them as a good or a bad
grasp. Our focus in this paper is on improving the second step by using depth
sensor scans from large online datasets to train a convolutional neural
network. We propose two new representations of grasp candidates, and we
quantify the effect of using prior knowledge of two forms: instance or category
knowledge of the object to be grasped, and pretraining the network on simulated
depth data obtained from idealized CAD models. Our analysis shows that a more
informative grasp candidate representation as well as pretraining and prior
knowledge significantly improve grasp detection. We evaluate our approach on a
Baxter Research Robot and demonstrate an average grasp success rate of 93% in
dense clutter. This is a 20% improvement compared to our prior work.


Spatial Semantic Regularisation for Large Scale Object Detection

  Large scale object detection with thousands of classes introduces the problem
of many contradicting false positive detections, which have to be suppressed.
Class-independent non-maximum suppression has traditionally been used for this
step, but it does not scale well as the number of classes grows. Traditional
non-maximum suppression does not consider label- and instance-level
relationships nor does it allow an exploitation of the spatial layout of
detection proposals. We propose a new multi-class spatial semantic
regularisation method based on affinity propagation clustering, which
simultaneously optimises across all categories and all proposed locations in
the image, to improve both the localisation and categorisation of selected
detection proposals. Constraints are shared across the labels through the
semantic WordNet hierarchy. Our approach proves to be especially useful in
large scale settings with thousands of classes, where spatial and semantic
interactions are very frequent and only weakly supervised detectors can be
built due to a lack of bounding box annotations. Detection experiments are
conducted on the ImageNet and COCO dataset, and in settings with thousands of
detected categories. Our method provides a significant precision improvement by
reducing false positives, while simultaneously improving the recall.


Fine-to-coarse Knowledge Transfer For Low-Res Image Classification

  We address the difficult problem of distinguishing fine-grained object
categories in low resolution images. Wepropose a simple an effective deep
learning approach that transfers fine-grained knowledge gained from high
resolution training data to the coarse low-resolution test scenario. Such
fine-to-coarse knowledge transfer has many real world applications, such as
identifying objects in surveillance photos or satellite images where the image
resolution at the test time is very low but plenty of high resolution photos of
similar objects are available. Our extensive experiments on two standard
benchmark datasets containing fine-grained car models and bird species
demonstrate that our approach can effectively transfer fine-detail knowledge to
coarse-detail imagery.


Deep CORAL: Correlation Alignment for Deep Domain Adaptation

  Deep neural networks are able to learn powerful representations from large
quantities of labeled input data, however they cannot always generalize well
across changes in input distributions. Domain adaptation algorithms have been
proposed to compensate for the degradation in performance due to domain shift.
In this paper, we address the case when the target domain is unlabeled,
requiring unsupervised adaptation. CORAL is a "frustratingly easy" unsupervised
domain adaptation method that aligns the second-order statistics of the source
and target distributions with a linear transformation. Here, we extend CORAL to
learn a nonlinear transformation that aligns correlations of layer activations
in deep neural networks (Deep CORAL). Experiments on standard benchmark
datasets show state-of-the-art performance.


Combining Texture and Shape Cues for Object Recognition With Minimal
  Supervision

  We present a novel approach to object classification and detection which
requires minimal supervision and which combines visual texture cues and shape
information learned from freely available unlabeled web search results. The
explosion of visual data on the web can potentially make visual examples of
almost any object easily accessible via web search. Previous unsupervised
methods have utilized either large scale sources of texture cues from the web,
or shape information from data such as crowdsourced CAD models. We propose a
two-stream deep learning framework that combines these cues, with one stream
learning visual texture cues from image search data, and the other stream
learning rich shape information from 3D CAD models. To perform classification
or detection for a novel image, the predictions of the two streams are combined
using a late fusion scheme. We present experiments and visualizations for both
tasks on the standard benchmark PASCAL VOC 2007 to demonstrate that texture and
shape provide complementary information in our model. Our method outperforms
previous web image based models, 3D CAD model based approaches, and weakly
supervised models.


Modeling Relationships in Referential Expressions with Compositional
  Modular Networks

  People often refer to entities in an image in terms of their relationships
with other entities. For example, "the black cat sitting under the table"
refers to both a "black cat" entity and its relationship with another "table"
entity. Understanding these relationships is essential for interpreting and
grounding such natural language expressions. Most prior work focuses on either
grounding entire referential expressions holistically to one region, or
localizing relationships based on a fixed set of categories. In this paper we
instead present a modular deep architecture capable of analyzing referential
expressions into their component parts, identifying entities and relationships
mentioned in the input expression and grounding them all in the scene. We call
this approach Compositional Modular Networks (CMNs): a novel architecture that
learns linguistic analysis and visual inference end-to-end. Our approach is
built around two types of neural modules that inspect local regions and
pairwise interactions between regions. We evaluate CMNs on multiple referential
expression datasets, outperforming state-of-the-art approaches on all tasks.


Top-down Visual Saliency Guided by Captions

  Neural image/video captioning models can generate accurate descriptions, but
their internal process of mapping regions to words is a black box and therefore
difficult to explain. Top-down neural saliency methods can find important
regions given a high-level semantic task such as object classification, but
cannot use a natural language sentence as the top-down input for the task. In
this paper, we propose Caption-Guided Visual Saliency to expose the
region-to-word mapping in modern encoder-decoder networks and demonstrate that
it is learned implicitly from caption training data, without any pixel-level
annotations. Our approach can produce spatial or spatiotemporal heatmaps for
both predicted captions, and for arbitrary query sentences. It recovers
saliency without the overhead of introducing explicit attention layers, and can
be used to analyze a variety of existing model architectures and improve their
design. Evaluation on large-scale video and image datasets demonstrates that
our approach achieves comparable captioning performance with existing methods
while providing more accurate saliency heatmaps. Our code is available at
visionlearninggroup.github.io/caption-guided-saliency/.


Synthetic to Real Adaptation with Generative Correlation Alignment
  Networks

  Synthetic images rendered from 3D CAD models are useful for augmenting
training data for object recognition algorithms. However, the generated images
are non-photorealistic and do not match real image statistics. This leads to a
large domain discrepancy, causing models trained on synthetic data to perform
poorly on real domains. Recent work has shown the great potential of deep
convolutional neural networks to generate realistic images, but has not
utilized generative models to address synthetic-to-real domain adaptation. In
this work, we propose a Deep Generative Correlation Alignment Network (DGCAN)
to synthesize images using a novel domain adaption algorithm. DGCAN leverages a
shape preserving loss and a low level statistic matching loss to minimize the
domain discrepancy between synthetic and real images in deep feature space.
Experimentally, we show training off-the-shelf classifiers on the newly
generated data can significantly boost performance when testing on the real
image domains (PASCAL VOC 2007 benchmark and Office dataset), improving upon
several existing methods.


Adversarial Discriminative Domain Adaptation

  Adversarial learning methods are a promising approach to training robust deep
networks, and can generate complex samples across diverse domains. They also
can improve recognition despite the presence of domain shift or dataset bias:
several adversarial approaches to unsupervised domain adaptation have recently
been introduced, which reduce the difference between the training and test
domain distributions and thus improve generalization performance. Prior
generative approaches show compelling visualizations, but are not optimal on
discriminative tasks and can be limited to smaller shifts. Prior discriminative
approaches could handle larger domain shifts, but imposed tied weights on the
model and did not exploit a GAN-based loss. We first outline a novel
generalized framework for adversarial adaptation, which subsumes recent
state-of-the-art approaches as special cases, and we use this generalized view
to better relate the prior approaches. We propose a previously unexplored
instance of our general framework which combines discriminative modeling,
untied weight sharing, and a GAN loss, which we call Adversarial Discriminative
Domain Adaptation (ADDA). We show that ADDA is more effective yet considerably
simpler than competing domain-adversarial methods, and demonstrate the promise
of our approach by exceeding state-of-the-art unsupervised adaptation results
on standard cross-domain digit classification tasks and a new more difficult
cross-modality object classification task.


R-C3D: Region Convolutional 3D Network for Temporal Activity Detection

  We address the problem of activity detection in continuous, untrimmed video
streams. This is a difficult task that requires extracting meaningful
spatio-temporal features to capture activities, accurately localizing the start
and end times of each activity. We introduce a new model, Region Convolutional
3D Network (R-C3D), which encodes the video streams using a three-dimensional
fully convolutional network, then generates candidate temporal regions
containing activities, and finally classifies selected regions into specific
activities. Computation is saved due to the sharing of convolutional features
between the proposal and the classification pipelines. The entire model is
trained end-to-end with jointly optimized localization and classification
losses. R-C3D is faster than existing methods (569 frames per second on a
single Titan X Maxwell GPU) and achieves state-of-the-art results on THUMOS'14.
We further demonstrate that our model is a general activity detection framework
that does not rely on assumptions about particular dataset properties by
evaluating our approach on ActivityNet and Charades. Our code is available at
http://ai.bu.edu/r-c3d/.


Stable Distribution Alignment Using the Dual of the Adversarial Distance

  Methods that align distributions by minimizing an adversarial distance
between them have recently achieved impressive results. However, these
approaches are difficult to optimize with gradient descent and they often do
not converge well without careful hyperparameter tuning and proper
initialization. We investigate whether turning the adversarial min-max problem
into an optimization problem by replacing the maximization part with its dual
improves the quality of the resulting alignment and explore its connections to
Maximum Mean Discrepancy. Our empirical results suggest that using the dual
formulation for the restricted family of linear discriminators results in a
more stable convergence to a desirable solution when compared with the
performance of a primal min-max GAN-like objective and an MMD objective under
the same restrictions. We test our hypothesis on the problem of aligning two
synthetic point clouds on a plane and on a real-image domain adaptation problem
on digits. In both cases, the dual formulation yields an iterative procedure
that gives more stable and monotonic improvement over time.


VisDA: The Visual Domain Adaptation Challenge

  We present the 2017 Visual Domain Adaptation (VisDA) dataset and challenge, a
large-scale testbed for unsupervised domain adaptation across visual domains.
Unsupervised domain adaptation aims to solve the real-world problem of domain
shift, where machine learning models trained on one domain must be transferred
and adapted to a novel visual domain without additional supervision. The
VisDA2017 challenge is focused on the simulation-to-reality shift and has two
associated tasks: image classification and image segmentation. The goal in both
tracks is to first train a model on simulated, synthetic data in the source
domain and then adapt it to perform well on real image data in the unlabeled
test domain. Our dataset is the largest one to date for cross-domain object
classification, with over 280K images across 12 categories in the combined
training, validation and testing domains. The image segmentation dataset is
also large-scale with over 30K images across 18 categories in the three
domains. We compare VisDA to existing cross-domain adaptation datasets and
provide a baseline performance analysis using various domain adaptation models
that are currently popular in the field.


Adversarial Dropout Regularization

  We present a method for transferring neural representations from label-rich
source domains to unlabeled target domains. Recent adversarial methods proposed
for this task learn to align features across domains by fooling a special
domain critic network. However, a drawback of this approach is that the critic
simply labels the generated features as in-domain or not, without considering
the boundaries between classes. This can lead to ambiguous features being
generated near class boundaries, reducing target classification accuracy. We
propose a novel approach, Adversarial Dropout Regularization (ADR), to
encourage the generator to output more discriminative features for the target
domain. Our key idea is to replace the critic with one that detects
non-discriminative features, using dropout on the classifier network. The
generator then learns to avoid these areas of the feature space and thus
creates better features. We apply our ADR approach to the problem of
unsupervised domain adaptation for image classification and semantic
segmentation tasks, and demonstrate significant improvement over the state of
the art. We also show that our approach can be used to train Generative
Adversarial Networks for semi-supervised learning.


CyCADA: Cycle-Consistent Adversarial Domain Adaptation

  Domain adaptation is critical for success in new, unseen environments.
Adversarial adaptation models applied in feature spaces discover domain
invariant representations, but are difficult to visualize and sometimes fail to
capture pixel-level and low-level domain shifts. Recent work has shown that
generative adversarial networks combined with cycle-consistency constraints are
surprisingly effective at mapping images between domains, even without the use
of aligned image pairs. We propose a novel discriminatively-trained
Cycle-Consistent Adversarial Domain Adaptation model. CyCADA adapts
representations at both the pixel-level and feature-level, enforces
cycle-consistency while leveraging a task loss, and does not require aligned
pairs. Our model can be applied in a variety of visual recognition and
prediction settings. We show new state-of-the-art results across multiple
adaptation tasks, including digit classification and semantic segmentation of
road scenes demonstrating transfer from synthetic to real world domains.


Contextual Multi-Scale Region Convolutional 3D Network for Activity
  Detection

  Activity detection is a fundamental problem in computer vision. Detecting
activities of different temporal scales is particularly challenging. In this
paper, we propose the contextual multi-scale region convolutional 3D network
(CMS-RC3D) for activity detection. To deal with the inherent temporal scale
variability of activity instances, the temporal feature pyramid is used to
represent activities of different temporal scales. On each level of the
temporal feature pyramid, an activity proposal detector and an activity
classifier are learned to detect activities of specific temporal scales.
Temporal contextual information is fused into activity classifiers for better
recognition. More importantly, the entire model at all levels can be trained
end-to-end. Our CMS-RC3D detector can deal with activities at all temporal
scale ranges with only a single pass through the backbone network. We test our
detector on two public activity detection benchmarks, THUMOS14 and ActivityNet.
Extensive experiments show that the proposed CMS-RC3D detector outperforms
state-of-the-art methods on THUMOS14 by a substantial margin and achieves
comparable results on ActivityNet despite using a shallow feature extractor.


Joint Event Detection and Description in Continuous Video Streams

  Dense video captioning is a fine-grained video understanding task that
involves two sub-problems: localizing distinct events in a long video stream,
and generating captions for the localized events. We propose the Joint Event
Detection and Description Network (JEDDi-Net), which solves the dense video
captioning task in an end-to-end fashion. Our model continuously encodes the
input video stream with three-dimensional convolutional layers, proposes
variable-length temporal events based on pooled features, and generates their
captions. Proposal features are extracted within each proposal segment through
3D Segment-of-Interest pooling from shared video feature encoding. In order to
explicitly model temporal relationships between visual events and their
captions in a single video, we also propose a two-level hierarchical captioning
module that keeps track of context. On the large-scale ActivityNet Captions
dataset, JEDDi-Net demonstrates improved results as measured by standard
metrics. We also present the first dense captioning results on the
TACoS-MultiLevel dataset.


Multilevel Language and Vision Integration for Text-to-Clip Retrieval

  We address the problem of text-based activity retrieval in video. Given a
sentence describing an activity, our task is to retrieve matching clips from an
untrimmed video. To capture the inherent structures present in both text and
video, we introduce a multilevel model that integrates vision and language
features earlier and more tightly than prior work. First, we inject text
features early on when generating clip proposals, to help eliminate unlikely
clips and thus speed up processing and boost performance. Second, to learn a
fine-grained similarity metric for retrieval, we use visual features to
modulate the processing of query sentences at the word level in a recurrent
neural network. A multi-task loss is also employed by adding query
re-generation as an auxiliary task. Our approach significantly outperforms
prior work on two challenging benchmarks: Charades-STA and ActivityNet
Captions.


Unsupervised Video-to-Video Translation

  Unsupervised image-to-image translation is a recently proposed task of
translating an image to a different style or domain given only unpaired image
examples at training time. In this paper, we formulate a new task of
unsupervised video-to-video translation, which poses its own unique challenges.
Translating video implies learning not only the appearance of objects and
scenes but also realistic motion and transitions between consecutive frames.We
investigate the performance of per-frame video-to-video translation using
existing image-to-image translation networks, and propose a spatio-temporal 3D
translator as an alternative solution to this problem. We evaluate our 3D
method on multiple synthetic datasets, such as moving colorized digits, as well
as the realistic segmentation-to-video GTA dataset and a new CT-to-MRI
volumetric images translation dataset. Our results show that frame-wise
translation produces realistic results on a single frame level but
underperforms significantly on the scale of the whole video compared to our
three-dimensional translation approach, which is better able to learn the
complex structure of video and motion and continuity of object appearance.


RISE: Randomized Input Sampling for Explanation of Black-box Models

  Deep neural networks are being used increasingly to automate data analysis
and decision making, yet their decision-making process is largely unclear and
is difficult to explain to the end users. In this paper, we address the problem
of Explainable AI for deep neural networks that take images as input and output
a class probability. We propose an approach called RISE that generates an
importance map indicating how salient each pixel is for the model's prediction.
In contrast to white-box approaches that estimate pixel importance using
gradients or other internal network state, RISE works on black-box models. It
estimates importance empirically by probing the model with randomly masked
versions of the input image and obtaining the corresponding outputs. We compare
our approach to state-of-the-art importance extraction methods using both an
automatic deletion/insertion metric and a pointing metric based on
human-annotated object segments. Extensive experiments on several benchmark
datasets show that our approach matches or exceeds the performance of other
methods, including white-box approaches.
  Project page: http://cs-people.bu.edu/vpetsiuk/rise/


Syn2Real: A New Benchmark forSynthetic-to-Real Visual Domain Adaptation

  Unsupervised transfer of object recognition models from synthetic to real
data is an important problem with many potential applications. The challenge is
how to "adapt" a model trained on simulated images so that it performs well on
real-world data without any additional supervision. Unfortunately, current
benchmarks for this problem are limited in size and task diversity. In this
paper, we present a new large-scale benchmark called Syn2Real, which consists
of a synthetic domain rendered from 3D object models and two real-image domains
containing the same object categories. We define three related tasks on this
benchmark: closed-set object classification, open-set object classification,
and object detection. Our evaluation of multiple state-of-the-art methods
reveals a large gap in adaptation performance between the easier closed-set
classification task and the more difficult open-set and detection tasks. We
conclude that developing adaptation methods that work well across all three
tasks presents a significant future challenge for syn2real domain transfer.


Explainable Neural Computation via Stack Neural Module Networks

  In complex inferential tasks like question answering, machine learning models
must confront two challenges: the need to implement a compositional reasoning
process, and, in many applications, the need for this reasoning process to be
interpretable to assist users in both development and prediction. Existing
models designed to produce interpretable traces of their decision-making
process typically require these traces to be supervised at training time. In
this paper, we present a novel neural modular approach that performs
compositional reasoning by automatically inducing a desired sub-task
decomposition without relying on strong supervision. Our model allows linking
different reasoning tasks though shared modules that handle common routines
across tasks. Experiments show that the model is more interpretable to human
evaluators compared to other state-of-the-art models: users can better
understand the model's underlying reasoning procedure and predict when it will
succeed or fail based on observing its intermediate outputs.


Object Hallucination in Image Captioning

  Despite continuously improving performance, contemporary image captioning
models are prone to "hallucinating" objects that are not actually in a scene.
One problem is that standard metrics only measure similarity to ground truth
captions and may not fully capture image relevance. In this work, we propose a
new image relevance metric to evaluate current models with veridical visual
labels and assess their rate of object hallucination. We analyze how captioning
model architectures and learning objectives contribute to object hallucination,
explore when hallucination is likely due to image misclassification or language
priors, and assess how well current sentence metrics capture object
hallucination. We investigate these questions on the standard image captioning
benchmark, MSCOCO, using a diverse set of models. Our analysis yields several
interesting findings, including that models which score best on standard
sentence metrics do not always have lower hallucination and that models which
hallucinate more tend to make errors driven by language priors.


Toward Driving Scene Understanding: A Dataset for Learning Driver
  Behavior and Causal Reasoning

  Driving Scene understanding is a key ingredient for intelligent
transportation systems. To achieve systems that can operate in a complex
physical and social environment, they need to understand and learn how humans
drive and interact with traffic scenes. We present the Honda Research Institute
Driving Dataset (HDD), a challenging dataset to enable research on learning
driver behavior in real-life environments. The dataset includes 104 hours of
real human driving in the San Francisco Bay Area collected using an
instrumented vehicle equipped with different sensors. We provide a detailed
analysis of HDD with a comparison to other driving datasets. A novel annotation
methodology is introduced to enable research on driver behavior understanding
from untrimmed data sequences. As the first step, baseline algorithms for
driver behavior detection are trained and tested to demonstrate the feasibility
of the proposed task.


Open-vocabulary Phrase Detection

  Most existing work that grounds natural language phrases in images starts
with the assumption that the phrase in question is relevant to the image. In
this paper we address a more realistic version of the natural language
grounding task where we must both identify whether the phrase is relevant to an
image and localize the phrase. This can also be viewed as a generalization of
object detection to an open-ended vocabulary, essentially introducing elements
of few- and zero-shot detection. We propose a Phrase R-CNN network for this
task that extends Faster R-CNN to relate image regions and phrases. By
carefully initializing the classification layers of our network using canonical
correlation analysis (CCA), we encourage a solution that is more discerning
when reasoning between similar phrases, resulting in over double the
performance compared to a naive adaptation on two popular phrase grounding
datasets, Flickr30K Entities and ReferIt Game, with test-time phrase vocabulary
sizes of 5K and 39K, respectively.


A Two-Stream Variational Adversarial Network for Video Generation

  Video generation is an inherently challenging task, as it requires the model
to generate realistic content and motion simultaneously. Existing methods
generate both motion and content together using a single generator network, but
this approach may fail on complex videos. In this paper, we propose a
two-stream video generation model that separates content and motion generation
into two parallel generators, called Two-Stream Variational Adversarial Network
(TwoStreamVAN). Our model outputs a realistic video given an input action label
by progressively generating and fusing motion and content features at multiple
scales using adaptive motion kernels. In addition, to better evaluate video
generation models, we design a new synthetic human action dataset to bridge the
difficulty gap between over-complicated human action datasets and simple toy
datasets. Our model significantly outperforms existing methods on the standard
Weizmann Human Action and MUG Facial Expression datasets, as well as our new
dataset.


Moment Matching for Multi-Source Domain Adaptation

  Conventional unsupervised domain adaptation (UDA) assumes that training data
are sampled from a single domain. This neglects the more practical scenario
where training data are collected from multiple sources, requiring multi-source
domain adaptation. We make three major contributions towards addressing this
problem. First, we collect and annotate by far the largest UDA dataset with six
domains and about 0.6 million images distributed among 345 categories,
addressing the gap in data availability for multi-source UDA research. Second,
we propose a new deep learning approach, Moment Matching for Multi-Source
Domain Adaptation M3SDA, which aims to transfer knowledge learned from multiple
labeled source domains to an unlabeled target domain by dynamically aligning
moments of their feature distributions. Third, we provide new theoretical
insight specifically for moment matching approaches in both single and multiple
source domain adaptation. Extensive experiments are conducted to demonstrate
the power of our new dataset in benchmarking state-of-the-art multi-source
domain adaptation methods, as well as the advantage of our proposed model.


Guided Zoom: Questioning Network Evidence for Fine-grained
  Classification

  We propose Guided Zoom, an approach that utilizes spatial grounding to make
more informed predictions. It does so by making sure the model has "the right
reasons" for a prediction, being defined as reasons that are coherent with
those used to make similar correct decisions at training time. The
reason/evidence upon which a deep neural network makes a prediction is defined
to be the spatial grounding, in the pixel space, for a specific class
conditional probability in the model output. Guided Zoom questions how
reasonable the evidence used to make a prediction is. In state-of-the-art deep
single-label classification models, the top-k (k = 2, 3, 4, ...) accuracy is
usually significantly higher than the top-1 accuracy. This is more evident in
fine-grained datasets, where differences between classes are quite subtle. We
show that Guided Zoom results in the refinement of a model's classification
accuracy on three finegrained classification datasets. We also explore the
complementarity of different grounding techniques, by comparing their ensemble
to an adversarial erasing approach that iteratively reveals the next most
discriminative evidence.


Similarity R-C3D for Few-shot Temporal Activity Detection

  Many activities of interest are rare events, with only a few labeled examples
available. Therefore models for temporal activity detection which are able to
learn from a few examples are desirable. In this paper, we present a
conceptually simple and general yet novel framework for few-shot temporal
activity detection which detects the start and end time of the few-shot input
activities in an untrimmed video. Our model is end-to-end trainable and can
benefit from more few-shot examples. At test time, each proposal is assigned
the label of the few-shot activity class corresponding to the maximum
similarity score. Our Similarity R-C3D method outperforms previous work on
three large-scale benchmarks for temporal activity detection (THUMOS14,
ActivityNet1.2, and ActivityNet1.3 datasets) in the few-shot setting. Our code
will be made available.


Cross-Domain Image Manipulation by Demonstration

  In this work we propose a model that can manipulate individual visual
attributes of objects in a real scene using examples of how respective
attribute manipulations affect the output of a simulation. As an example, we
train our model to manipulate the expression of a human face using
nonphotorealistic 3D renders of a face with varied expression. Our model
manages to preserve all other visual attributes of a real face, such as head
orientation, even though this and other attributes are not labeled in either
real or synthetic domain. Since our model learns to manipulate a specific
property in isolation using only "synthetic demonstrations" of such
manipulations without explicitly provided labels, it can be applied to shape,
texture, lighting, and other properties that are difficult to measure or
represent as real-valued vectors. We measure the degree to which our model
preserves other attributes of a real image when a single specific attribute is
manipulated. We use digit datasets to analyze how discrepancy in attribute
distributions affects the performance of our model, and demonstrate results in
a far more difficult setting: learning to manipulate real human faces using
nonphotorealistic 3D renders.


Learning Deep Object Detectors from 3D Models

  Crowdsourced 3D CAD models are becoming easily accessible online, and can
potentially generate an infinite number of training images for almost any
object category.We show that augmenting the training data of contemporary Deep
Convolutional Neural Net (DCNN) models with such synthetic data can be
effective, especially when real training data is limited or not well matched to
the target domain. Most freely available CAD models capture 3D shape but are
often missing other low level cues, such as realistic object texture, pose, or
background. In a detailed analysis, we use synthetic CAD-rendered images to
probe the ability of DCNN to learn without these cues, with surprising
findings. In particular, we show that when the DCNN is fine-tuned on the target
detection task, it exhibits a large degree of invariance to missing low-level
cues, but, when pretrained on generic ImageNet classification, it learns better
when the low-level cues are simulated. We show that our synthetic DCNN training
approach significantly outperforms previous methods on the PASCAL VOC2007
dataset when learning in the few-shot scenario and improves performance in a
domain shift scenario on the Office benchmark.


A Multi-scale Multiple Instance Video Description Network

  Generating natural language descriptions for in-the-wild videos is a
challenging task. Most state-of-the-art methods for solving this problem borrow
existing deep convolutional neural network (CNN) architectures (AlexNet,
GoogLeNet) to extract a visual representation of the input video. However,
these deep CNN architectures are designed for single-label centered-positioned
object classification. While they generate strong semantic features, they have
no inherent structure allowing them to detect multiple objects of different
sizes and locations in the frame. Our paper tries to solve this problem by
integrating the base CNN into several fully convolutional neural networks
(FCNs) to form a multi-scale network that handles multiple receptive field
sizes in the original image. FCNs, previously applied to image segmentation,
can generate class heat-maps efficiently compared to sliding window mechanisms,
and can easily handle multiple scales. To further handle the ambiguity over
multiple objects and locations, we incorporate the Multiple Instance Learning
mechanism (MIL) to consider objects in different positions and at different
scales simultaneously. We integrate our multi-scale multi-instance architecture
with a sequence-to-sequence recurrent neural network to generate sentence
descriptions based on the visual representation. Ours is the first end-to-end
trainable architecture that is capable of multi-scale region processing.
Evaluation on a Youtube video dataset shows the advantage of our approach
compared to the original single-scale whole frame CNN model. Our flexible and
efficient architecture can potentially be extended to support other video
processing tasks.


Captioning Images with Diverse Objects

  Recent captioning models are limited in their ability to scale and describe
concepts unseen in paired image-text corpora. We propose the Novel Object
Captioner (NOC), a deep visual semantic captioning model that can describe a
large number of object categories not present in existing image-caption
datasets. Our model takes advantage of external sources -- labeled images from
object recognition datasets, and semantic knowledge extracted from unannotated
text. We propose minimizing a joint objective which can learn from these
diverse data sources and leverage distributional semantic embeddings, enabling
the model to generalize and describe novel objects outside of image-caption
datasets. We demonstrate that our model exploits semantic information to
generate captions for hundreds of object categories in the ImageNet object
recognition dataset that are not observed in MSCOCO image-caption training
data, as well as many categories that are observed very rarely. Both automatic
evaluations and human judgements show that our model considerably outperforms
prior work in being able to describe many more categories of objects.


Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for
  Visual Question Answering

  We address the problem of Visual Question Answering (VQA), which requires
joint image and language understanding to answer a question about a given
photograph. Recent approaches have applied deep image captioning methods based
on convolutional-recurrent networks to this problem, but have failed to model
spatial inference. To remedy this, we propose a model we call the Spatial
Memory Network and apply it to the VQA task. Memory networks are recurrent
neural networks with an explicit attention mechanism that selects certain parts
of the information stored in memory. Our Spatial Memory Network stores neuron
activations from different spatial regions of the image in its memory, and uses
the question to choose relevant regions for computing the answer, a process of
which constitutes a single "hop" in the network. We propose a novel spatial
attention architecture that aligns words with image patches in the first hop,
and obtain improved results by adding a second attention hop which considers
the whole question to choose visual evidence based on the results of the first
hop. To better understand the inference process learned by the network, we
design synthetic questions that specifically require spatial inference and
visualize the attention weights. We evaluate our model on two published visual
question answering datasets, DAQUAR [1] and VQA [2], and obtain improved
results compared to a strong deep baseline model (iBOWIMG) which concatenates
image and question features to predict the answer [3].


Deep Compositional Captioning: Describing Novel Object Categories
  without Paired Training Data

  While recent deep neural network models have achieved promising results on
the image captioning task, they rely largely on the availability of corpora
with paired image and sentence captions to describe objects in context. In this
work, we propose the Deep Compositional Captioner (DCC) to address the task of
generating descriptions of novel objects which are not present in paired
image-sentence datasets. Our method achieves this by leveraging large object
recognition datasets and external text corpora and by transferring knowledge
between semantically similar concepts. Current deep caption models can only
describe objects contained in paired image-sentence corpora, despite the fact
that they are pre-trained with large object recognition datasets, namely
ImageNet. In contrast, our model can compose sentences that describe novel
objects and their interactions with other objects. We demonstrate our model's
ability to describe novel concepts by empirically evaluating its performance on
MSCOCO and show qualitative results on ImageNet images of objects for which no
paired image-caption data exist. Further, we extend our approach to generate
descriptions of objects in video clips. Our results show that DCC has distinct
advantages over existing image and video captioning approaches for generating
descriptions of new objects in context.


Adapting Deep Visuomotor Representations with Weak Pairwise Constraints

  Real-world robotics problems often occur in domains that differ significantly
from the robot's prior training environment. For many robotic control tasks,
real world experience is expensive to obtain, but data is easy to collect in
either an instrumented environment or in simulation. We propose a novel domain
adaptation approach for robot perception that adapts visual representations
learned on a large easy-to-obtain source dataset (e.g. synthetic images) to a
target real-world domain, without requiring expensive manual data annotation of
real world data before policy search. Supervised domain adaptation methods
minimize cross-domain differences using pairs of aligned images that contain
the same object or scene in both the source and target domains, thus learning a
domain-invariant representation. However, they require manual alignment of such
image pairs. Fully unsupervised adaptation methods rely on minimizing the
discrepancy between the feature distributions across domains. We propose a
novel, more powerful combination of both distribution and pairwise image
alignment, and remove the requirement for expensive annotation by using weakly
aligned pairs of images in the source and target domains. Focusing on adapting
from simulation to real world data using a PR2 robot, we evaluate our approach
on a manipulation task and show that by using weakly paired images, our method
compensates for domain shift more effectively than previous techniques,
enabling better robot performance in the real world.


Learning to Reason: End-to-End Module Networks for Visual Question
  Answering

  Natural language questions are inherently compositional, and many are most
easily answered by reasoning about their decomposition into modular
sub-problems. For example, to answer "is there an equal number of balls and
boxes?" we can look for balls, look for boxes, count them, and compare the
results. The recently proposed Neural Module Network (NMN) architecture
implements this approach to question answering by parsing questions into
linguistic substructures and assembling question-specific deep networks from
smaller modules that each solve one subtask. However, existing NMN
implementations rely on brittle off-the-shelf parsers, and are restricted to
the module configurations proposed by these parsers rather than learning them
from data. In this paper, we propose End-to-End Module Networks (N2NMNs), which
learn to reason by directly predicting instance-specific network layouts
without the aid of a parser. Our model learns to generate network structures
(by imitating expert demonstrations) while simultaneously learning network
parameters (using the downstream task loss). Experimental results on the new
CLEVR dataset targeted at compositional question answering show that N2NMNs
achieve an error reduction of nearly 50% relative to state-of-the-art
attentional approaches, while discovering interpretable network architectures
specialized for each question.


