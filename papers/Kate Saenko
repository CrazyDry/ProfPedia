What Do Deep CNNs Learn About Objects?

  Deep convolutional neural networks learn extremely powerful imagerepresentations, yet most of that power is hidden in the millions of deep-layerparameters. What exactly do these parameters represent? Recent work has startedto analyse CNN representations, finding that, e.g., they are invariant to some2D transformations Fischer et al. (2014), but are confused by particular typesof image noise Nguyen et al. (2014). In this work, we delve deeper and ask: howinvariant are CNNs to object-class variations caused by 3D shape, pose, andphotorealism?

Simultaneous Deep Transfer Across Domains and Tasks

  Recent reports suggest that a generic supervised deep CNN model trained on alarge-scale dataset reduces, but does not remove, dataset bias. Fine-tuningdeep models in a new domain can require a significant amount of labeled data,which for many applications is simply not available. We propose a new CNNarchitecture to exploit unlabeled and sparsely labeled target domain data. Ourapproach simultaneously optimizes for domain invariance to facilitate domaintransfer and uses a soft label distribution matching loss to transferinformation between tasks. Our proposed adaptation method offers empiricalperformance which exceeds previously published results on two standardbenchmark visual domain adaptation tasks, evaluated across supervised andsemi-supervised adaptation settings.

Improving LSTM-based Video Description with Linguistic Knowledge Mined  from Text

  This paper investigates how linguistic knowledge mined from large textcorpora can aid the generation of natural language descriptions of videos.Specifically, we integrate both a neural language model and distributionalsemantics trained on large text corpora into a recent LSTM-based architecturefor video description. We evaluate our approach on a collection of Youtubevideos as well as two large movie description datasets showing significantimprovements in grammaticality while modestly improving descriptive quality.

Hierarchical Reinforcement Learning with Hindsight

  Reinforcement Learning (RL) algorithms can suffer from poor sample efficiencywhen rewards are delayed and sparse. We introduce a solution that enablesagents to learn temporally extended actions at multiple levels of abstractionin a sample efficient and automated fashion. Our approach combines universalvalue functions and hindsight learning, allowing agents to learn policiesbelonging to different time scales in parallel. We show that our methodsignificantly accelerates learning in a variety of discrete and continuoustasks.

Adapting control policies from simulation to reality using a pairwise  loss

  This paper proposes an approach to domain transfer based on a pairwise lossfunction that helps transfer control policies learned in simulation onto a realrobot. We explore the idea in the context of a 'category level' manipulationtask where a control policy is learned that enables a robot to perform a matingtask involving novel objects. We explore the case where depth images are usedas the main form of sensor input. Our experimental results demonstrate thatproposed method consistently outperforms baseline methods that train only insimulation or that combine real and simulated data in a naive way.

Efficient Learning of Domain-invariant Image Representations

  We present an algorithm that learns representations which explicitlycompensate for domain mismatch and which can be efficiently realized as linearclassifiers. Specifically, we form a linear transformation that maps featuresfrom the target (test) domain to the source (training) domain as part oftraining the classifier. We optimize both the transformation and classifierparameters jointly, and introduce an efficient cost function based onmisclassification loss. Our method combines several features previouslyunavailable in a single algorithm: multi-class adaptation throughrepresentation learning, ability to map across heterogeneous feature spaces,and scalability to large datasets. We present experiments on several imagedatasets that demonstrate improved accuracy and computational advantagescompared to previous approaches.

Towards Adapting ImageNet to Reality: Scalable Domain Adaptation with  Implicit Low-rank Transformations

  Images seen during test time are often not from the same distribution asimages used for learning. This problem, known as domain shift, occurs whentraining classifiers from object-centric internet image databases and trying toapply them directly to scene understanding tasks. The consequence is oftensevere performance degradation and is one of the major barriers for theapplication of classifiers in real-world systems. In this paper, we show how tolearn transform-based domain adaptation classifiers in a scalable manner. Thekey idea is to exploit an implicit rank constraint, originated from amax-margin domain adaptation formulation, to make optimization tractable.Experiments show that the transformation between domains can be veryefficiently learned from data and easily applied to new categories. This beginsto bridge the gap between large-scale internet image collections and objectimages captured in everyday life environments.

Modeling Radiometric Uncertainty for Vision with Tone-mapped Color  Images

  To produce images that are suitable for display, tone-mapping is widely usedin digital cameras to map linear color measurements into narrow gamuts withlimited dynamic range. This introduces non-linear distortion that must beundone, through a radiometric calibration process, before computer visionsystems can analyze such photographs radiometrically. This paper considers theinherent uncertainty of undoing the effects of tone-mapping. We observe thatthis uncertainty varies substantially across color space, making some pixelsmore reliable than others. We introduce a model for this uncertainty and amethod for fitting it to a given camera or imaging pipeline. Once fit, themodel provides for each pixel in a tone-mapped digital photograph a probabilitydistribution over linear scene colors that could have induced it. Wedemonstrate how these distributions can be useful for visual inference byincorporating them into estimation algorithms for a representative set ofvision tasks.

Detector Discovery in the Wild: Joint Multiple Instance and  Representation Learning

  We develop methods for detector learning which exploit joint training overboth weak and strong labels and which transfer learned perceptualrepresentations from strongly-labeled auxiliary tasks. Previous methods forweak-label learning often learn detector models independently using latentvariable optimization, but fail to share deep representation knowledge acrossclasses and usually require strong initialization. Other previous methodstransfer deep representations from domains with strong labels to those withonly weak labels, but do not optimize over individual latent boxes, and thusmay miss specific salient structures for a particular category. We propose amodel that subsumes these previous approaches, and simultaneously trains arepresentation and detectors for categories with either weak or strong labelspresent. We provide a novel formulation of a joint multiple instance learningmethod that includes examples from classification-style data when available,and also performs domain transfer learning to improve the underlying detectorrepresentation. Our model outperforms known methods on ImageNet-200 detectionwith weak labels.

Deep Domain Confusion: Maximizing for Domain Invariance

  Recent reports suggest that a generic supervised deep CNN model trained on alarge-scale dataset reduces, but does not remove, dataset bias on a standardbenchmark. Fine-tuning deep models in a new domain can require a significantamount of data, which for many applications is simply not available. We proposea new CNN architecture which introduces an adaptation layer and an additionaldomain confusion loss, to learn a representation that is both semanticallymeaningful and domain invariant. We additionally show that a domain confusionmetric can be used for model selection to determine the dimension of anadaptation layer and the best position for the layer in the CNN architecture.Our proposed adaptation method offers empirical performance which exceedspreviously published results on a standard benchmark visual domain adaptationtask.

Translating Videos to Natural Language Using Deep Recurrent Neural  Networks

  Solving the visual symbol grounding problem has long been a goal ofartificial intelligence. The field appears to be advancing closer to this goalwith recent breakthroughs in deep learning for natural language grounding instatic images. In this paper, we propose to translate videos directly tosentences using a unified deep neural network with both convolutional andrecurrent structure. Described video datasets are scarce, and most existingmethods have been applied to toy domains with a small vocabulary of possiblewords. By transferring knowledge from 1.2M+ images with category labels and100,000+ images with captions, our method is able to create sentencedescriptions of open-domain videos with large vocabularies. We compare ourapproach with recent work using language generation metrics, subject, verb, andobject prediction accuracy, and a human evaluation.

Sequence to Sequence -- Video to Text

  Real-world videos often have complex dynamics; and methods for generatingopen-domain video descriptions should be sensitive to temporal structure andallow both input (sequence of frames) and output (sequence of words) ofvariable length. To approach this problem, we propose a novel end-to-endsequence-to-sequence model to generate captions for videos. For this we exploitrecurrent neural networks, specifically LSTMs, which have demonstratedstate-of-the-art performance in image caption generation. Our LSTM model istrained on video-sentence pairs and learns to associate a sequence of videoframes to a sequence of words in order to generate a description of the eventin the video clip. Our model naturally is able to learn the temporal structureof the sequence of frames as well as the sequence model of the generatedsentences, i.e. a language model. We evaluate several variants of our modelthat exploit different visual features on a standard set of YouTube videos andtwo movie description datasets (M-VAD and MPII-MD).

Spatial Semantic Regularisation for Large Scale Object Detection

  Large scale object detection with thousands of classes introduces the problemof many contradicting false positive detections, which have to be suppressed.Class-independent non-maximum suppression has traditionally been used for thisstep, but it does not scale well as the number of classes grows. Traditionalnon-maximum suppression does not consider label- and instance-levelrelationships nor does it allow an exploitation of the spatial layout ofdetection proposals. We propose a new multi-class spatial semanticregularisation method based on affinity propagation clustering, whichsimultaneously optimises across all categories and all proposed locations inthe image, to improve both the localisation and categorisation of selecteddetection proposals. Constraints are shared across the labels through thesemantic WordNet hierarchy. Our approach proves to be especially useful inlarge scale settings with thousands of classes, where spatial and semanticinteractions are very frequent and only weakly supervised detectors can bebuilt due to a lack of bounding box annotations. Detection experiments areconducted on the ImageNet and COCO dataset, and in settings with thousands ofdetected categories. Our method provides a significant precision improvement byreducing false positives, while simultaneously improving the recall.

Natural Language Object Retrieval

  In this paper, we address the task of natural language object retrieval, tolocalize a target object within a given image based on a natural language queryof the object. Natural language object retrieval differs from text-based imageretrieval task as it involves spatial information about objects within thescene and global scene context. To address this issue, we propose a novelSpatial Context Recurrent ConvNet (SCRC) model as scoring function on candidateboxes for object retrieval, integrating spatial configurations and globalscene-level contextual information into the network. Our model processes querytext, local image descriptors, spatial configurations and global contextfeatures through a recurrent network, outputs the probability of the query textconditioned on each candidate box as a score for the box, and can transfervisual-linguistic knowledge from image captioning domain to our task.Experimental results demonstrate that our method effectively utilizes bothlocal and global information, outperforming previous baseline methodssignificantly on different datasets and scenarios, and can exploit large scalevision and language datasets for knowledge transfer.

Return of Frustratingly Easy Domain Adaptation

  Unlike human learning, machine learning often fails to handle changes betweentraining (source) and test (target) input distributions. Such domain shifts,common in practical scenarios, severely damage the performance of conventionalmachine learning methods. Supervised domain adaptation methods have beenproposed for the case when the target data have labels, including some thatperform very well despite being "frustratingly easy" to implement. However, inpractice, the target domain is often unlabeled, requiring unsupervisedadaptation. We propose a simple, effective, and efficient method forunsupervised domain adaptation called CORrelation ALignment (CORAL). CORALminimizes domain shift by aligning the second-order statistics of source andtarget distributions, without requiring any target labels. Even though it isextraordinarily simple--it can be implemented in four lines of Matlabcode--CORAL performs remarkably well in extensive evaluations on standardbenchmark datasets.

High precision grasp pose detection in dense clutter

  This paper considers the problem of grasp pose detection in point clouds. Wefollow a general algorithmic structure that first generates a large set of6-DOF grasp candidates and then classifies each of them as a good or a badgrasp. Our focus in this paper is on improving the second step by using depthsensor scans from large online datasets to train a convolutional neuralnetwork. We propose two new representations of grasp candidates, and wequantify the effect of using prior knowledge of two forms: instance or categoryknowledge of the object to be grasped, and pretraining the network on simulateddepth data obtained from idealized CAD models. Our analysis shows that a moreinformative grasp candidate representation as well as pretraining and priorknowledge significantly improve grasp detection. We evaluate our approach on aBaxter Research Robot and demonstrate an average grasp success rate of 93% indense clutter. This is a 20% improvement compared to our prior work.

Fine-to-coarse Knowledge Transfer For Low-Res Image Classification

  We address the difficult problem of distinguishing fine-grained objectcategories in low resolution images. Wepropose a simple an effective deeplearning approach that transfers fine-grained knowledge gained from highresolution training data to the coarse low-resolution test scenario. Suchfine-to-coarse knowledge transfer has many real world applications, such asidentifying objects in surveillance photos or satellite images where the imageresolution at the test time is very low but plenty of high resolution photos ofsimilar objects are available. Our extensive experiments on two standardbenchmark datasets containing fine-grained car models and bird speciesdemonstrate that our approach can effectively transfer fine-detail knowledge tocoarse-detail imagery.

Deep CORAL: Correlation Alignment for Deep Domain Adaptation

  Deep neural networks are able to learn powerful representations from largequantities of labeled input data, however they cannot always generalize wellacross changes in input distributions. Domain adaptation algorithms have beenproposed to compensate for the degradation in performance due to domain shift.In this paper, we address the case when the target domain is unlabeled,requiring unsupervised adaptation. CORAL is a "frustratingly easy" unsuperviseddomain adaptation method that aligns the second-order statistics of the sourceand target distributions with a linear transformation. Here, we extend CORAL tolearn a nonlinear transformation that aligns correlations of layer activationsin deep neural networks (Deep CORAL). Experiments on standard benchmarkdatasets show state-of-the-art performance.

Combining Texture and Shape Cues for Object Recognition With Minimal  Supervision

  We present a novel approach to object classification and detection whichrequires minimal supervision and which combines visual texture cues and shapeinformation learned from freely available unlabeled web search results. Theexplosion of visual data on the web can potentially make visual examples ofalmost any object easily accessible via web search. Previous unsupervisedmethods have utilized either large scale sources of texture cues from the web,or shape information from data such as crowdsourced CAD models. We propose atwo-stream deep learning framework that combines these cues, with one streamlearning visual texture cues from image search data, and the other streamlearning rich shape information from 3D CAD models. To perform classificationor detection for a novel image, the predictions of the two streams are combinedusing a late fusion scheme. We present experiments and visualizations for bothtasks on the standard benchmark PASCAL VOC 2007 to demonstrate that texture andshape provide complementary information in our model. Our method outperformsprevious web image based models, 3D CAD model based approaches, and weaklysupervised models.

Modeling Relationships in Referential Expressions with Compositional  Modular Networks

  People often refer to entities in an image in terms of their relationshipswith other entities. For example, "the black cat sitting under the table"refers to both a "black cat" entity and its relationship with another "table"entity. Understanding these relationships is essential for interpreting andgrounding such natural language expressions. Most prior work focuses on eithergrounding entire referential expressions holistically to one region, orlocalizing relationships based on a fixed set of categories. In this paper weinstead present a modular deep architecture capable of analyzing referentialexpressions into their component parts, identifying entities and relationshipsmentioned in the input expression and grounding them all in the scene. We callthis approach Compositional Modular Networks (CMNs): a novel architecture thatlearns linguistic analysis and visual inference end-to-end. Our approach isbuilt around two types of neural modules that inspect local regions andpairwise interactions between regions. We evaluate CMNs on multiple referentialexpression datasets, outperforming state-of-the-art approaches on all tasks.

Top-down Visual Saliency Guided by Captions

  Neural image/video captioning models can generate accurate descriptions, buttheir internal process of mapping regions to words is a black box and thereforedifficult to explain. Top-down neural saliency methods can find importantregions given a high-level semantic task such as object classification, butcannot use a natural language sentence as the top-down input for the task. Inthis paper, we propose Caption-Guided Visual Saliency to expose theregion-to-word mapping in modern encoder-decoder networks and demonstrate thatit is learned implicitly from caption training data, without any pixel-levelannotations. Our approach can produce spatial or spatiotemporal heatmaps forboth predicted captions, and for arbitrary query sentences. It recoverssaliency without the overhead of introducing explicit attention layers, and canbe used to analyze a variety of existing model architectures and improve theirdesign. Evaluation on large-scale video and image datasets demonstrates thatour approach achieves comparable captioning performance with existing methodswhile providing more accurate saliency heatmaps. Our code is available atvisionlearninggroup.github.io/caption-guided-saliency/.

Synthetic to Real Adaptation with Generative Correlation Alignment  Networks

  Synthetic images rendered from 3D CAD models are useful for augmentingtraining data for object recognition algorithms. However, the generated imagesare non-photorealistic and do not match real image statistics. This leads to alarge domain discrepancy, causing models trained on synthetic data to performpoorly on real domains. Recent work has shown the great potential of deepconvolutional neural networks to generate realistic images, but has notutilized generative models to address synthetic-to-real domain adaptation. Inthis work, we propose a Deep Generative Correlation Alignment Network (DGCAN)to synthesize images using a novel domain adaption algorithm. DGCAN leverages ashape preserving loss and a low level statistic matching loss to minimize thedomain discrepancy between synthetic and real images in deep feature space.Experimentally, we show training off-the-shelf classifiers on the newlygenerated data can significantly boost performance when testing on the realimage domains (PASCAL VOC 2007 benchmark and Office dataset), improving uponseveral existing methods.

Adversarial Discriminative Domain Adaptation

  Adversarial learning methods are a promising approach to training robust deepnetworks, and can generate complex samples across diverse domains. They alsocan improve recognition despite the presence of domain shift or dataset bias:several adversarial approaches to unsupervised domain adaptation have recentlybeen introduced, which reduce the difference between the training and testdomain distributions and thus improve generalization performance. Priorgenerative approaches show compelling visualizations, but are not optimal ondiscriminative tasks and can be limited to smaller shifts. Prior discriminativeapproaches could handle larger domain shifts, but imposed tied weights on themodel and did not exploit a GAN-based loss. We first outline a novelgeneralized framework for adversarial adaptation, which subsumes recentstate-of-the-art approaches as special cases, and we use this generalized viewto better relate the prior approaches. We propose a previously unexploredinstance of our general framework which combines discriminative modeling,untied weight sharing, and a GAN loss, which we call Adversarial DiscriminativeDomain Adaptation (ADDA). We show that ADDA is more effective yet considerablysimpler than competing domain-adversarial methods, and demonstrate the promiseof our approach by exceeding state-of-the-art unsupervised adaptation resultson standard cross-domain digit classification tasks and a new more difficultcross-modality object classification task.

R-C3D: Region Convolutional 3D Network for Temporal Activity Detection

  We address the problem of activity detection in continuous, untrimmed videostreams. This is a difficult task that requires extracting meaningfulspatio-temporal features to capture activities, accurately localizing the startand end times of each activity. We introduce a new model, Region Convolutional3D Network (R-C3D), which encodes the video streams using a three-dimensionalfully convolutional network, then generates candidate temporal regionscontaining activities, and finally classifies selected regions into specificactivities. Computation is saved due to the sharing of convolutional featuresbetween the proposal and the classification pipelines. The entire model istrained end-to-end with jointly optimized localization and classificationlosses. R-C3D is faster than existing methods (569 frames per second on asingle Titan X Maxwell GPU) and achieves state-of-the-art results on THUMOS'14.We further demonstrate that our model is a general activity detection frameworkthat does not rely on assumptions about particular dataset properties byevaluating our approach on ActivityNet and Charades. Our code is available athttp://ai.bu.edu/r-c3d/.

Stable Distribution Alignment Using the Dual of the Adversarial Distance

  Methods that align distributions by minimizing an adversarial distancebetween them have recently achieved impressive results. However, theseapproaches are difficult to optimize with gradient descent and they often donot converge well without careful hyperparameter tuning and properinitialization. We investigate whether turning the adversarial min-max probleminto an optimization problem by replacing the maximization part with its dualimproves the quality of the resulting alignment and explore its connections toMaximum Mean Discrepancy. Our empirical results suggest that using the dualformulation for the restricted family of linear discriminators results in amore stable convergence to a desirable solution when compared with theperformance of a primal min-max GAN-like objective and an MMD objective underthe same restrictions. We test our hypothesis on the problem of aligning twosynthetic point clouds on a plane and on a real-image domain adaptation problemon digits. In both cases, the dual formulation yields an iterative procedurethat gives more stable and monotonic improvement over time.

VisDA: The Visual Domain Adaptation Challenge

  We present the 2017 Visual Domain Adaptation (VisDA) dataset and challenge, alarge-scale testbed for unsupervised domain adaptation across visual domains.Unsupervised domain adaptation aims to solve the real-world problem of domainshift, where machine learning models trained on one domain must be transferredand adapted to a novel visual domain without additional supervision. TheVisDA2017 challenge is focused on the simulation-to-reality shift and has twoassociated tasks: image classification and image segmentation. The goal in bothtracks is to first train a model on simulated, synthetic data in the sourcedomain and then adapt it to perform well on real image data in the unlabeledtest domain. Our dataset is the largest one to date for cross-domain objectclassification, with over 280K images across 12 categories in the combinedtraining, validation and testing domains. The image segmentation dataset isalso large-scale with over 30K images across 18 categories in the threedomains. We compare VisDA to existing cross-domain adaptation datasets andprovide a baseline performance analysis using various domain adaptation modelsthat are currently popular in the field.

Adversarial Dropout Regularization

  We present a method for transferring neural representations from label-richsource domains to unlabeled target domains. Recent adversarial methods proposedfor this task learn to align features across domains by fooling a specialdomain critic network. However, a drawback of this approach is that the criticsimply labels the generated features as in-domain or not, without consideringthe boundaries between classes. This can lead to ambiguous features beinggenerated near class boundaries, reducing target classification accuracy. Wepropose a novel approach, Adversarial Dropout Regularization (ADR), toencourage the generator to output more discriminative features for the targetdomain. Our key idea is to replace the critic with one that detectsnon-discriminative features, using dropout on the classifier network. Thegenerator then learns to avoid these areas of the feature space and thuscreates better features. We apply our ADR approach to the problem ofunsupervised domain adaptation for image classification and semanticsegmentation tasks, and demonstrate significant improvement over the state ofthe art. We also show that our approach can be used to train GenerativeAdversarial Networks for semi-supervised learning.

CyCADA: Cycle-Consistent Adversarial Domain Adaptation

  Domain adaptation is critical for success in new, unseen environments.Adversarial adaptation models applied in feature spaces discover domaininvariant representations, but are difficult to visualize and sometimes fail tocapture pixel-level and low-level domain shifts. Recent work has shown thatgenerative adversarial networks combined with cycle-consistency constraints aresurprisingly effective at mapping images between domains, even without the useof aligned image pairs. We propose a novel discriminatively-trainedCycle-Consistent Adversarial Domain Adaptation model. CyCADA adaptsrepresentations at both the pixel-level and feature-level, enforcescycle-consistency while leveraging a task loss, and does not require alignedpairs. Our model can be applied in a variety of visual recognition andprediction settings. We show new state-of-the-art results across multipleadaptation tasks, including digit classification and semantic segmentation ofroad scenes demonstrating transfer from synthetic to real world domains.

Contextual Multi-Scale Region Convolutional 3D Network for Activity  Detection

  Activity detection is a fundamental problem in computer vision. Detectingactivities of different temporal scales is particularly challenging. In thispaper, we propose the contextual multi-scale region convolutional 3D network(CMS-RC3D) for activity detection. To deal with the inherent temporal scalevariability of activity instances, the temporal feature pyramid is used torepresent activities of different temporal scales. On each level of thetemporal feature pyramid, an activity proposal detector and an activityclassifier are learned to detect activities of specific temporal scales.Temporal contextual information is fused into activity classifiers for betterrecognition. More importantly, the entire model at all levels can be trainedend-to-end. Our CMS-RC3D detector can deal with activities at all temporalscale ranges with only a single pass through the backbone network. We test ourdetector on two public activity detection benchmarks, THUMOS14 and ActivityNet.Extensive experiments show that the proposed CMS-RC3D detector outperformsstate-of-the-art methods on THUMOS14 by a substantial margin and achievescomparable results on ActivityNet despite using a shallow feature extractor.

Joint Event Detection and Description in Continuous Video Streams

  Dense video captioning is a fine-grained video understanding task thatinvolves two sub-problems: localizing distinct events in a long video stream,and generating captions for the localized events. We propose the Joint EventDetection and Description Network (JEDDi-Net), which solves the dense videocaptioning task in an end-to-end fashion. Our model continuously encodes theinput video stream with three-dimensional convolutional layers, proposesvariable-length temporal events based on pooled features, and generates theircaptions. Proposal features are extracted within each proposal segment through3D Segment-of-Interest pooling from shared video feature encoding. In order toexplicitly model temporal relationships between visual events and theircaptions in a single video, we also propose a two-level hierarchical captioningmodule that keeps track of context. On the large-scale ActivityNet Captionsdataset, JEDDi-Net demonstrates improved results as measured by standardmetrics. We also present the first dense captioning results on theTACoS-MultiLevel dataset.

Multilevel Language and Vision Integration for Text-to-Clip Retrieval

  We address the problem of text-based activity retrieval in video. Given asentence describing an activity, our task is to retrieve matching clips from anuntrimmed video. To capture the inherent structures present in both text andvideo, we introduce a multilevel model that integrates vision and languagefeatures earlier and more tightly than prior work. First, we inject textfeatures early on when generating clip proposals, to help eliminate unlikelyclips and thus speed up processing and boost performance. Second, to learn afine-grained similarity metric for retrieval, we use visual features tomodulate the processing of query sentences at the word level in a recurrentneural network. A multi-task loss is also employed by adding queryre-generation as an auxiliary task. Our approach significantly outperformsprior work on two challenging benchmarks: Charades-STA and ActivityNetCaptions.

Unsupervised Video-to-Video Translation

  Unsupervised image-to-image translation is a recently proposed task oftranslating an image to a different style or domain given only unpaired imageexamples at training time. In this paper, we formulate a new task ofunsupervised video-to-video translation, which poses its own unique challenges.Translating video implies learning not only the appearance of objects andscenes but also realistic motion and transitions between consecutive frames.Weinvestigate the performance of per-frame video-to-video translation usingexisting image-to-image translation networks, and propose a spatio-temporal 3Dtranslator as an alternative solution to this problem. We evaluate our 3Dmethod on multiple synthetic datasets, such as moving colorized digits, as wellas the realistic segmentation-to-video GTA dataset and a new CT-to-MRIvolumetric images translation dataset. Our results show that frame-wisetranslation produces realistic results on a single frame level butunderperforms significantly on the scale of the whole video compared to ourthree-dimensional translation approach, which is better able to learn thecomplex structure of video and motion and continuity of object appearance.

RISE: Randomized Input Sampling for Explanation of Black-box Models

  Deep neural networks are being used increasingly to automate data analysisand decision making, yet their decision-making process is largely unclear andis difficult to explain to the end users. In this paper, we address the problemof Explainable AI for deep neural networks that take images as input and outputa class probability. We propose an approach called RISE that generates animportance map indicating how salient each pixel is for the model's prediction.In contrast to white-box approaches that estimate pixel importance usinggradients or other internal network state, RISE works on black-box models. Itestimates importance empirically by probing the model with randomly maskedversions of the input image and obtaining the corresponding outputs. We compareour approach to state-of-the-art importance extraction methods using both anautomatic deletion/insertion metric and a pointing metric based onhuman-annotated object segments. Extensive experiments on several benchmarkdatasets show that our approach matches or exceeds the performance of othermethods, including white-box approaches.  Project page: http://cs-people.bu.edu/vpetsiuk/rise/

Syn2Real: A New Benchmark forSynthetic-to-Real Visual Domain Adaptation

  Unsupervised transfer of object recognition models from synthetic to realdata is an important problem with many potential applications. The challenge ishow to "adapt" a model trained on simulated images so that it performs well onreal-world data without any additional supervision. Unfortunately, currentbenchmarks for this problem are limited in size and task diversity. In thispaper, we present a new large-scale benchmark called Syn2Real, which consistsof a synthetic domain rendered from 3D object models and two real-image domainscontaining the same object categories. We define three related tasks on thisbenchmark: closed-set object classification, open-set object classification,and object detection. Our evaluation of multiple state-of-the-art methodsreveals a large gap in adaptation performance between the easier closed-setclassification task and the more difficult open-set and detection tasks. Weconclude that developing adaptation methods that work well across all threetasks presents a significant future challenge for syn2real domain transfer.

Explainable Neural Computation via Stack Neural Module Networks

  In complex inferential tasks like question answering, machine learning modelsmust confront two challenges: the need to implement a compositional reasoningprocess, and, in many applications, the need for this reasoning process to beinterpretable to assist users in both development and prediction. Existingmodels designed to produce interpretable traces of their decision-makingprocess typically require these traces to be supervised at training time. Inthis paper, we present a novel neural modular approach that performscompositional reasoning by automatically inducing a desired sub-taskdecomposition without relying on strong supervision. Our model allows linkingdifferent reasoning tasks though shared modules that handle common routinesacross tasks. Experiments show that the model is more interpretable to humanevaluators compared to other state-of-the-art models: users can betterunderstand the model's underlying reasoning procedure and predict when it willsucceed or fail based on observing its intermediate outputs.

Object Hallucination in Image Captioning

  Despite continuously improving performance, contemporary image captioningmodels are prone to "hallucinating" objects that are not actually in a scene.One problem is that standard metrics only measure similarity to ground truthcaptions and may not fully capture image relevance. In this work, we propose anew image relevance metric to evaluate current models with veridical visuallabels and assess their rate of object hallucination. We analyze how captioningmodel architectures and learning objectives contribute to object hallucination,explore when hallucination is likely due to image misclassification or languagepriors, and assess how well current sentence metrics capture objecthallucination. We investigate these questions on the standard image captioningbenchmark, MSCOCO, using a diverse set of models. Our analysis yields severalinteresting findings, including that models which score best on standardsentence metrics do not always have lower hallucination and that models whichhallucinate more tend to make errors driven by language priors.

Toward Driving Scene Understanding: A Dataset for Learning Driver  Behavior and Causal Reasoning

  Driving Scene understanding is a key ingredient for intelligenttransportation systems. To achieve systems that can operate in a complexphysical and social environment, they need to understand and learn how humansdrive and interact with traffic scenes. We present the Honda Research InstituteDriving Dataset (HDD), a challenging dataset to enable research on learningdriver behavior in real-life environments. The dataset includes 104 hours ofreal human driving in the San Francisco Bay Area collected using aninstrumented vehicle equipped with different sensors. We provide a detailedanalysis of HDD with a comparison to other driving datasets. A novel annotationmethodology is introduced to enable research on driver behavior understandingfrom untrimmed data sequences. As the first step, baseline algorithms fordriver behavior detection are trained and tested to demonstrate the feasibilityof the proposed task.

Open-vocabulary Phrase Detection

  Most existing work that grounds natural language phrases in images startswith the assumption that the phrase in question is relevant to the image. Inthis paper we address a more realistic version of the natural languagegrounding task where we must both identify whether the phrase is relevant to animage and localize the phrase. This can also be viewed as a generalization ofobject detection to an open-ended vocabulary, essentially introducing elementsof few- and zero-shot detection. We propose a Phrase R-CNN network for thistask that extends Faster R-CNN to relate image regions and phrases. Bycarefully initializing the classification layers of our network using canonicalcorrelation analysis (CCA), we encourage a solution that is more discerningwhen reasoning between similar phrases, resulting in over double theperformance compared to a naive adaptation on two popular phrase groundingdatasets, Flickr30K Entities and ReferIt Game, with test-time phrase vocabularysizes of 5K and 39K, respectively.

A Two-Stream Variational Adversarial Network for Video Generation

  Video generation is an inherently challenging task, as it requires the modelto generate realistic content and motion simultaneously. Existing methodsgenerate both motion and content together using a single generator network, butthis approach may fail on complex videos. In this paper, we propose atwo-stream video generation model that separates content and motion generationinto two parallel generators, called Two-Stream Variational Adversarial Network(TwoStreamVAN). Our model outputs a realistic video given an input action labelby progressively generating and fusing motion and content features at multiplescales using adaptive motion kernels. In addition, to better evaluate videogeneration models, we design a new synthetic human action dataset to bridge thedifficulty gap between over-complicated human action datasets and simple toydatasets. Our model significantly outperforms existing methods on the standardWeizmann Human Action and MUG Facial Expression datasets, as well as our newdataset.

Moment Matching for Multi-Source Domain Adaptation

  Conventional unsupervised domain adaptation (UDA) assumes that training dataare sampled from a single domain. This neglects the more practical scenariowhere training data are collected from multiple sources, requiring multi-sourcedomain adaptation. We make three major contributions towards addressing thisproblem. First, we collect and annotate by far the largest UDA dataset with sixdomains and about 0.6 million images distributed among 345 categories,addressing the gap in data availability for multi-source UDA research. Second,we propose a new deep learning approach, Moment Matching for Multi-SourceDomain Adaptation M3SDA, which aims to transfer knowledge learned from multiplelabeled source domains to an unlabeled target domain by dynamically aligningmoments of their feature distributions. Third, we provide new theoreticalinsight specifically for moment matching approaches in both single and multiplesource domain adaptation. Extensive experiments are conducted to demonstratethe power of our new dataset in benchmarking state-of-the-art multi-sourcedomain adaptation methods, as well as the advantage of our proposed model.

Guided Zoom: Questioning Network Evidence for Fine-grained  Classification

  We propose Guided Zoom, an approach that utilizes spatial grounding to makemore informed predictions. It does so by making sure the model has "the rightreasons" for a prediction, being defined as reasons that are coherent withthose used to make similar correct decisions at training time. Thereason/evidence upon which a deep neural network makes a prediction is definedto be the spatial grounding, in the pixel space, for a specific classconditional probability in the model output. Guided Zoom questions howreasonable the evidence used to make a prediction is. In state-of-the-art deepsingle-label classification models, the top-k (k = 2, 3, 4, ...) accuracy isusually significantly higher than the top-1 accuracy. This is more evident infine-grained datasets, where differences between classes are quite subtle. Weshow that Guided Zoom results in the refinement of a model's classificationaccuracy on three finegrained classification datasets. We also explore thecomplementarity of different grounding techniques, by comparing their ensembleto an adversarial erasing approach that iteratively reveals the next mostdiscriminative evidence.

Similarity R-C3D for Few-shot Temporal Activity Detection

  Many activities of interest are rare events, with only a few labeled examplesavailable. Therefore models for temporal activity detection which are able tolearn from a few examples are desirable. In this paper, we present aconceptually simple and general yet novel framework for few-shot temporalactivity detection which detects the start and end time of the few-shot inputactivities in an untrimmed video. Our model is end-to-end trainable and canbenefit from more few-shot examples. At test time, each proposal is assignedthe label of the few-shot activity class corresponding to the maximumsimilarity score. Our Similarity R-C3D method outperforms previous work onthree large-scale benchmarks for temporal activity detection (THUMOS14,ActivityNet1.2, and ActivityNet1.3 datasets) in the few-shot setting. Our codewill be made available.

Cross-Domain Image Manipulation by Demonstration

  In this work we propose a model that can manipulate individual visualattributes of objects in a real scene using examples of how respectiveattribute manipulations affect the output of a simulation. As an example, wetrain our model to manipulate the expression of a human face usingnonphotorealistic 3D renders of a face with varied expression. Our modelmanages to preserve all other visual attributes of a real face, such as headorientation, even though this and other attributes are not labeled in eitherreal or synthetic domain. Since our model learns to manipulate a specificproperty in isolation using only "synthetic demonstrations" of suchmanipulations without explicitly provided labels, it can be applied to shape,texture, lighting, and other properties that are difficult to measure orrepresent as real-valued vectors. We measure the degree to which our modelpreserves other attributes of a real image when a single specific attribute ismanipulated. We use digit datasets to analyze how discrepancy in attributedistributions affects the performance of our model, and demonstrate results ina far more difficult setting: learning to manipulate real human faces usingnonphotorealistic 3D renders.

One-Shot Adaptation of Supervised Deep Convolutional Models

  Dataset bias remains a significant barrier towards solving real worldcomputer vision tasks. Though deep convolutional networks have proven to be acompetitive approach for image classification, a question remains: have thesemodels have solved the dataset bias problem? In general, training orfine-tuning a state-of-the-art deep model on a new domain requires asignificant amount of data, which for many applications is simply notavailable. Transfer of models directly to new domains without adaptation hashistorically led to poor recognition performance. In this paper, we pose thefollowing question: is a single image dataset, much larger than previouslyexplored for adaptation, comprehensive enough to learn general deep models thatmay be effectively applied to new image domains? In other words, are deep CNNstrained on large amounts of labeled data as susceptible to dataset bias asprevious methods have been shown to be? We show that a generic supervised deepCNN model trained on a large dataset reduces, but does not remove, datasetbias. Furthermore, we propose several methods for adaptation with deep modelsthat are able to operate with little (one example per category) or no labeleddomain specific data. Our experiments show that adaptation of deep models onbenchmark visual domain adaptation datasets can provide a significantperformance boost.

LSDA: Large Scale Detection Through Adaptation

  A major challenge in scaling object detection is the difficulty of obtaininglabeled images for large numbers of categories. Recently, deep convolutionalneural networks (CNNs) have emerged as clear winners on object classificationbenchmarks, in part due to training with 1.2M+ labeled classification images.Unfortunately, only a small fraction of those labels are available for thedetection task. It is much cheaper and easier to collect large quantities ofimage-level labels from search engines than it is to collect detection data andlabel it with precise bounding boxes. In this paper, we propose Large ScaleDetection through Adaptation (LSDA), an algorithm which learns the differencebetween the two tasks and transfers this knowledge to classifiers forcategories without bounding box annotated data, turning them into detectors.Our method has the potential to enable detection for the tens of thousands ofcategories that lack bounding box annotations, yet have plenty ofclassification data. Evaluation on the ImageNet LSVRC-2013 detection challengedemonstrates the efficacy of our approach. This algorithm enables us to producea >7.6K detector by using available classification data from leaf nodes in theImageNet tree. We additionally demonstrate how to modify our architecture toproduce a fast detector (running at 2fps for the 7.6K detector). Models andsoftware are available at

Learning Deep Object Detectors from 3D Models

  Crowdsourced 3D CAD models are becoming easily accessible online, and canpotentially generate an infinite number of training images for almost anyobject category.We show that augmenting the training data of contemporary DeepConvolutional Neural Net (DCNN) models with such synthetic data can beeffective, especially when real training data is limited or not well matched tothe target domain. Most freely available CAD models capture 3D shape but areoften missing other low level cues, such as realistic object texture, pose, orbackground. In a detailed analysis, we use synthetic CAD-rendered images toprobe the ability of DCNN to learn without these cues, with surprisingfindings. In particular, we show that when the DCNN is fine-tuned on the targetdetection task, it exhibits a large degree of invariance to missing low-levelcues, but, when pretrained on generic ImageNet classification, it learns betterwhen the low-level cues are simulated. We show that our synthetic DCNN trainingapproach significantly outperforms previous methods on the PASCAL VOC2007dataset when learning in the few-shot scenario and improves performance in adomain shift scenario on the Office benchmark.

A Multi-scale Multiple Instance Video Description Network

  Generating natural language descriptions for in-the-wild videos is achallenging task. Most state-of-the-art methods for solving this problem borrowexisting deep convolutional neural network (CNN) architectures (AlexNet,GoogLeNet) to extract a visual representation of the input video. However,these deep CNN architectures are designed for single-label centered-positionedobject classification. While they generate strong semantic features, they haveno inherent structure allowing them to detect multiple objects of differentsizes and locations in the frame. Our paper tries to solve this problem byintegrating the base CNN into several fully convolutional neural networks(FCNs) to form a multi-scale network that handles multiple receptive fieldsizes in the original image. FCNs, previously applied to image segmentation,can generate class heat-maps efficiently compared to sliding window mechanisms,and can easily handle multiple scales. To further handle the ambiguity overmultiple objects and locations, we incorporate the Multiple Instance Learningmechanism (MIL) to consider objects in different positions and at differentscales simultaneously. We integrate our multi-scale multi-instance architecturewith a sequence-to-sequence recurrent neural network to generate sentencedescriptions based on the visual representation. Ours is the first end-to-endtrainable architecture that is capable of multi-scale region processing.Evaluation on a Youtube video dataset shows the advantage of our approachcompared to the original single-scale whole frame CNN model. Our flexible andefficient architecture can potentially be extended to support other videoprocessing tasks.

Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for  Visual Question Answering

  We address the problem of Visual Question Answering (VQA), which requiresjoint image and language understanding to answer a question about a givenphotograph. Recent approaches have applied deep image captioning methods basedon convolutional-recurrent networks to this problem, but have failed to modelspatial inference. To remedy this, we propose a model we call the SpatialMemory Network and apply it to the VQA task. Memory networks are recurrentneural networks with an explicit attention mechanism that selects certain partsof the information stored in memory. Our Spatial Memory Network stores neuronactivations from different spatial regions of the image in its memory, and usesthe question to choose relevant regions for computing the answer, a process ofwhich constitutes a single "hop" in the network. We propose a novel spatialattention architecture that aligns words with image patches in the first hop,and obtain improved results by adding a second attention hop which considersthe whole question to choose visual evidence based on the results of the firsthop. To better understand the inference process learned by the network, wedesign synthetic questions that specifically require spatial inference andvisualize the attention weights. We evaluate our model on two published visualquestion answering datasets, DAQUAR [1] and VQA [2], and obtain improvedresults compared to a strong deep baseline model (iBOWIMG) which concatenatesimage and question features to predict the answer [3].

Deep Compositional Captioning: Describing Novel Object Categories  without Paired Training Data

  While recent deep neural network models have achieved promising results onthe image captioning task, they rely largely on the availability of corporawith paired image and sentence captions to describe objects in context. In thiswork, we propose the Deep Compositional Captioner (DCC) to address the task ofgenerating descriptions of novel objects which are not present in pairedimage-sentence datasets. Our method achieves this by leveraging large objectrecognition datasets and external text corpora and by transferring knowledgebetween semantically similar concepts. Current deep caption models can onlydescribe objects contained in paired image-sentence corpora, despite the factthat they are pre-trained with large object recognition datasets, namelyImageNet. In contrast, our model can compose sentences that describe novelobjects and their interactions with other objects. We demonstrate our model'sability to describe novel concepts by empirically evaluating its performance onMSCOCO and show qualitative results on ImageNet images of objects for which nopaired image-caption data exist. Further, we extend our approach to generatedescriptions of objects in video clips. Our results show that DCC has distinctadvantages over existing image and video captioning approaches for generatingdescriptions of new objects in context.

Adapting Deep Visuomotor Representations with Weak Pairwise Constraints

  Real-world robotics problems often occur in domains that differ significantlyfrom the robot's prior training environment. For many robotic control tasks,real world experience is expensive to obtain, but data is easy to collect ineither an instrumented environment or in simulation. We propose a novel domainadaptation approach for robot perception that adapts visual representationslearned on a large easy-to-obtain source dataset (e.g. synthetic images) to atarget real-world domain, without requiring expensive manual data annotation ofreal world data before policy search. Supervised domain adaptation methodsminimize cross-domain differences using pairs of aligned images that containthe same object or scene in both the source and target domains, thus learning adomain-invariant representation. However, they require manual alignment of suchimage pairs. Fully unsupervised adaptation methods rely on minimizing thediscrepancy between the feature distributions across domains. We propose anovel, more powerful combination of both distribution and pairwise imagealignment, and remove the requirement for expensive annotation by using weaklyaligned pairs of images in the source and target domains. Focusing on adaptingfrom simulation to real world data using a PR2 robot, we evaluate our approachon a manipulation task and show that by using weakly paired images, our methodcompensates for domain shift more effectively than previous techniques,enabling better robot performance in the real world.

