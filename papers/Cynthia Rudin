Direct Learning to Rank and Rerank

  Learning-to-rank techniques have proven to be extremely useful for
prioritization problems, where we rank items in order of their estimated
probabilities, and dedicate our limited resources to the top-ranked items. This
work exposes a serious problem with the state of learning-to-rank algorithms,
which is that they are based on convex proxies that lead to poor
approximations. We then discuss the possibility of "exact" reranking algorithms
based on mathematical programming. We prove that a relaxed version of the
"exact" problem has the same optimal solution, and provide an empirical
analysis.


Please Stop Explaining Black Box Models for High Stakes Decisions

  Black box machine learning models are currently being used for high stakes
decision-making throughout society, causing problems throughout healthcare,
criminal justice, and in other domains. People have hoped that creating methods
for explaining these black box models will alleviate some of these problems,
but trying to explain black box models, rather than creating models that are
interpretable in the first place, is likely to perpetuate bad practices and can
potentially cause catastrophic harm to society. There is a way forward - it is
to design models that are inherently interpretable.


Supersparse Linear Integer Models for Predictive Scoring Systems

  We introduce Supersparse Linear Integer Models (SLIM) as a tool to create
scoring systems for binary classification. We derive theoretical bounds on the
true risk of SLIM scoring systems, and present experimental results to show
that SLIM scoring systems are accurate, sparse, and interpretable
classification models.


Proceedings of the 2018 ICML Workshop on Human Interpretability in
  Machine Learning (WHI 2018)

  This is the Proceedings of the 2018 ICML Workshop on Human Interpretability
in Machine Learning (WHI 2018), which was held in Stockholm, Sweden, July 14,
2018. Invited speakers were Barbara Engelhardt, Cynthia Rudin, Fernanda
Vi\'egas, and Martin Wattenberg.


Shall I Compare Thee to a Machine-Written Sonnet? An Approach to
  Algorithmic Sonnet Generation

  We provide code that produces beautiful poetry. Our sonnet-generation
algorithm includes several novel elements that improve over the
state-of-the-art, leading to rhythmic and inspiring poems. The work discussed
here is the winner of the 2018 PoetiX Literary Turing Test Award for
computer-generated poetry.


Stability Analysis for Regularized Least Squares Regression

  We discuss stability for a class of learning algorithms with respect to noisy
labels. The algorithms we consider are for regression, and they involve the
minimization of regularized risk functionals, such as L(f) := 1/N sum_i
(f(x_i)-y_i)^2+ lambda ||f||_H^2. We shall call the algorithm `stable' if, when
y_i is a noisy version of f*(x_i) for some function f* in H, the output of the
algorithm converges to f* as the regularization term and noise simultaneously
vanish. We consider two flavors of this problem, one where a data set of N
points remains fixed, and the other where N -> infinity. For the case where N
-> infinity, we give conditions for convergence to f_E (the function which is
the expectation of y(x) for each x), as lambda -> 0. For the fixed N case, we
describe the limiting 'non-noisy', 'non-regularized' function f*, and give
conditions for convergence. In the process, we develop a set of tools for
dealing with functionals such as L(f), which are applicable to many other
problems in learning theory.


Analysis of boosting algorithms using the smooth margin function

  We introduce a useful tool for analyzing boosting algorithms called the
``smooth margin function,'' a differentiable approximation of the usual margin
for boosting algorithms. We present two boosting algorithms based on this
smooth margin, ``coordinate ascent boosting'' and ``approximate coordinate
ascent boosting,'' which are similar to Freund and Schapire's AdaBoost
algorithm and Breiman's arc-gv algorithm. We give convergence rates to the
maximum margin solution for both of our algorithms and for arc-gv. We then
study AdaBoost's convergence properties using the smooth margin function. We
precisely bound the margin attained by AdaBoost when the edges of the weak
classifiers fall within a specified range. This shows that a previous bound
proved by R\"{a}tsch and Warmuth is exactly tight. Furthermore, we use the
smooth margin to capture explicit properties of AdaBoost in cases where cyclic
behavior occurs.


Online Coordinate Boosting

  We present a new online boosting algorithm for adapting the weights of a
boosted classifier, which yields a closer approximation to Freund and
Schapire's AdaBoost algorithm than previous online boosting algorithms. We also
contribute a new way of deriving the online algorithm that ties together
previous online boosting work. We assume that the weak hypotheses were selected
beforehand, and only their weights are updated during online boosting. The
update rule is derived by minimizing AdaBoost's loss when viewed in an
incremental form. The equations show that optimization is computationally
expensive. However, a fast online approximation is possible. We compare
approximation error to batch AdaBoost on synthetic datasets and generalization
error on face datasets and the MNIST dataset.


Learning to Predict the Wisdom of Crowds

  The problem of "approximating the crowd" is that of estimating the crowd's
majority opinion by querying only a subset of it. Algorithms that approximate
the crowd can intelligently stretch a limited budget for a crowdsourcing task.
We present an algorithm, "CrowdSense," that works in an online fashion to
dynamically sample subsets of labelers based on an exploration/exploitation
criterion. The algorithm produces a weighted combination of a subset of the
labelers' votes that approximates the crowd's opinion.


Falling Rule Lists

  Falling rule lists are classification models consisting of an ordered list of
if-then rules, where (i) the order of rules determines which example should be
classified by each rule, and (ii) the estimated probability of success
decreases monotonically down the list. These kinds of rule lists are inspired
by healthcare applications where patients would be stratified into risk sets
and the highest at-risk patients should be considered first. We provide a
Bayesian framework for learning falling rule lists that does not rely on
traditional greedy decision tree learning methods.


CRAFT: ClusteR-specific Assorted Feature selecTion

  We present a framework for clustering with cluster-specific feature
selection. The framework, CRAFT, is derived from asymptotic log posterior
formulations of nonparametric MAP-based clustering models. CRAFT handles
assorted data, i.e., both numeric and categorical data, and the underlying
objective functions are intuitively appealing. The resulting algorithm is
simple to implement and scales nicely, requires minimal parameter tuning,
obviates the need to specify the number of clusters a priori, and compares
favorably with other methods on real datasets.


The age of secrecy and unfairness in recidivism prediction

  In our current society, secret algorithms make important decisions about
individuals. There has been substantial discussion about whether these
algorithms are unfair to groups of individuals. While noble, this pursuit is
complex and ultimately stagnating because there is no clear definition of
fairness and competing definitions are largely incompatible. We argue that the
focus on the question of fairness is misplaced, as these algorithms fail to
meet a more important and yet readily obtainable goal: transparency. As a
result, creators of secret algorithms can provide incomplete or misleading
descriptions about how their models work, and various other kinds of errors can
easily go unnoticed. By partially reverse engineering the COMPAS algorithm -- a
recidivism-risk scoring algorithm used throughout the criminal justice system
-- we show that it does not seem to depend linearly on the defendant's age,
despite statements to the contrary by the algorithm's creator. Furthermore, by
subtracting from COMPAS its (hypothesized) nonlinear age component, we show
that COMPAS does not necessarily depend on race, contradicting ProPublica's
analysis, which assumed linearity in age. In other words, faulty assumptions
about a proprietary algorithm lead to faulty conclusions that go unchecked
without careful reverse engineering. Were the algorithm transparent in the
first place, this would likely not have occurred. The most important result in
this work is that we find that there are many defendants with low risk score
but long criminal histories, suggesting that data inconsistencies occur
frequently in criminal justice databases. We argue that transparency satisfies
a different notion of procedural fairness by providing both the defendants and
the public with the opportunity to scrutinize the methodology and calculations
behind risk scores for recidivism.


On Combining Machine Learning with Decision Making

  We present a new application and covering number bound for the framework of
"Machine Learning with Operational Costs (MLOC)," which is an exploratory form
of decision theory. The MLOC framework incorporates knowledge about how a
predictive model will be used for a subsequent task, thus combining machine
learning with the decision that is made afterwards. In this work, we use the
MLOC framework to study a problem that has implications for power grid
reliability and maintenance, called the Machine Learning and Traveling
Repairman Problem ML&TRP. The goal of the ML&TRP is to determine a route for a
"repair crew," which repairs nodes on a graph. The repair crew aims to minimize
the cost of failures at the nodes, but as in many real situations, the failure
probabilities are not known and must be estimated. The MLOC framework allows us
to understand how this uncertainty influences the repair route. We also present
new covering number generalization bounds for the MLOC framework.


Learning About Meetings

  Most people participate in meetings almost every day, multiple times a day.
The study of meetings is important, but also challenging, as it requires an
understanding of social signals and complex interpersonal dynamics. Our aim
this work is to use a data-driven approach to the science of meetings. We
provide tentative evidence that: i) it is possible to automatically detect when
during the meeting a key decision is taking place, from analyzing only the
local dialogue acts, ii) there are common patterns in the way social dialogue
acts are interspersed throughout a meeting, iii) at the time key decisions are
made, the amount of time left in the meeting can be predicted from the amount
of time that has passed, iv) it is often possible to predict whether a proposal
during a meeting will be accepted or rejected based entirely on the language
(the set of persuasive words) used by the speaker.


Supersparse Linear Integer Models for Interpretable Classification

  Scoring systems are classification models that only require users to add,
subtract and multiply a few meaningful numbers to make a prediction. These
models are often used because they are practical and interpretable. In this
paper, we introduce an off-the-shelf tool to create scoring systems that both
accurate and interpretable, known as a Supersparse Linear Integer Model (SLIM).
SLIM is a discrete optimization problem that minimizes the 0-1 loss to
encourage a high level of accuracy, regularizes the L0-norm to encourage a high
level of sparsity, and constrains coefficients to a set of interpretable
values. We illustrate the practical and interpretable nature of SLIM scoring
systems through applications in medicine and criminology, and show that they
are are accurate and sparse in comparison to state-of-the-art classification
models using numerical experiments.


A Statistical Learning Theory Framework for Supervised Pattern Discovery

  This paper formalizes a latent variable inference problem we call {\em
supervised pattern discovery}, the goal of which is to find sets of
observations that belong to a single ``pattern.'' We discuss two versions of
the problem and prove uniform risk bounds for both. In the first version,
collections of patterns can be generated in an arbitrary manner and the data
consist of multiple labeled collections. In the second version, the patterns
are assumed to be generated independently by identically distributed processes.
These processes are allowed to take an arbitrary form, so observations within a
pattern are not in general independent of each other. The bounds for the second
version of the problem are stated in terms of a new complexity measure, the
quasi-Rademacher complexity.


Methods and Models for Interpretable Linear Classification

  We present an integer programming framework to build accurate and
interpretable discrete linear classification models. Unlike existing
approaches, our framework is designed to provide practitioners with the control
and flexibility they need to tailor accurate and interpretable models for a
domain of choice. To this end, our framework can produce models that are fully
optimized for accuracy, by minimizing the 0--1 classification loss, and that
address multiple aspects of interpretability, by incorporating a range of
discrete constraints and penalty functions. We use our framework to produce
models that are difficult to create with existing methods, such as scoring
systems and M-of-N rule tables. In addition, we propose specially designed
optimization methods to improve the scalability of our framework through
decomposition and data reduction. We show that discrete linear classifiers can
attain the training accuracy of any other linear classifier, and provide an
Occam's Razor type argument as to why the use of small discrete coefficients
can provide better generalization. We demonstrate the performance and
flexibility of our framework through numerical experiments and a case study in
which we construct a highly tailored clinical tool for sleep apnea diagnosis.


Generalization Bounds for Learning with Linear, Polygonal, Quadratic and
  Conic Side Knowledge

  In this paper, we consider a supervised learning setting where side knowledge
is provided about the labels of unlabeled examples. The side knowledge has the
effect of reducing the hypothesis space, leading to tighter generalization
bounds, and thus possibly better generalization. We consider several types of
side knowledge, the first leading to linear and polygonal constraints on the
hypothesis space, the second leading to quadratic constraints, and the last
leading to conic constraints. We show how different types of domain knowledge
can lead directly to these kinds of side knowledge. We prove bounds on
complexity measures of the hypothesis space for quadratic and conic side
knowledge, and show that these bounds are tight in a specific sense for the
quadratic case.


Modeling Recovery Curves With Application to Prostatectomy

  We propose a Bayesian model that predicts recovery curves based on
information available before the disruptive event. A recovery curve of interest
is the quantified sexual function of prostate cancer patients after
prostatectomy surgery. We illustrate the utility of our model as a
pre-treatment medical decision aid, producing personalized predictions that are
both interpretable and accurate. We uncover covariate relationships that agree
with and supplement that in existing medical literature.


Scalable Bayesian Rule Lists

  We present an algorithm for building probabilistic rule lists that is two
orders of magnitude faster than previous work. Rule list algorithms are
competitors for decision tree algorithms. They are associative classifiers, in
that they are built from pre-mined association rules. They have a logical
structure that is a sequence of IF-THEN rules, identical to a decision list or
one-sided decision tree. Instead of using greedy splitting and pruning like
decision tree algorithms, we fully optimize over rule lists, striking a
practical balance between accuracy, interpretability, and computational speed.
The algorithm presented here uses a mixture of theoretical bounds (tight enough
to have practical implications as a screening or bounding procedure),
computational reuse, and highly tuned language libraries to achieve
computational efficiency. Currently, for many practical problems, this method
achieves better accuracy and sparsity than decision trees; further, in many
cases, the computational time is practical and often less than that of decision
trees. The result is a probabilistic classifier (which estimates P(y = 1|x) for
each x) that optimizes the posterior of a Bayesian hierarchical model over rule
lists.


Interpretable Machine Learning Models for the Digital Clock Drawing Test

  The Clock Drawing Test (CDT) is a rapid, inexpensive, and popular
neuropsychological screening tool for cognitive conditions. The Digital Clock
Drawing Test (dCDT) uses novel software to analyze data from a digitizing
ballpoint pen that reports its position with considerable spatial and temporal
precision, making possible the analysis of both the drawing process and final
product. We developed methodology to analyze pen stroke data from these
drawings, and computed a large collection of features which were then analyzed
with a variety of machine learning techniques. The resulting scoring systems
were designed to be more accurate than the systems currently used by
clinicians, but just as interpretable and easy to use. The systems also allow
us to quantify the tradeoff between accuracy and interpretability. We created
automated versions of the CDT scoring systems currently used by clinicians,
allowing us to benchmark our models, which indicated that our machine learning
models substantially outperformed the existing scoring systems.


Bayesian hierarchical rule modeling for predicting medical conditions

  We propose a statistical modeling technique, called the Hierarchical
Association Rule Model (HARM), that predicts a patient's possible future
medical conditions given the patient's current and past history of reported
conditions. The core of our technique is a Bayesian hierarchical model for
selecting predictive association rules (such as "condition 1 and condition 2
$\rightarrow$ condition 3") from a large set of candidate rules. Because this
method "borrows strength" using the conditions of many similar patients, it is
able to provide predictions specialized to any given patient, even when little
information about the patient's history of conditions is available.


Box Drawings for Learning with Imbalanced Data

  The vast majority of real world classification problems are imbalanced,
meaning there are far fewer data from the class of interest (the positive
class) than from other classes. We propose two machine learning algorithms to
handle highly imbalanced classification problems. The classifiers constructed
by both methods are created as unions of parallel axis rectangles around the
positive examples, and thus have the benefit of being interpretable. The first
algorithm uses mixed integer programming to optimize a weighted balance between
positive and negative class accuracies. Regularization is introduced to improve
generalization performance. The second method uses an approximation in order to
assist with scalability. Specifically, it follows a \textit{characterize then
discriminate} approach, where the positive class is characterized first by
boxes, and then each box boundary becomes a separate discriminative classifier.
This method has the computational advantages that it can be easily
parallelized, and considers only the relevant regions of feature space.


The Bayesian Case Model: A Generative Approach for Case-Based Reasoning
  and Prototype Classification

  We present the Bayesian Case Model (BCM), a general framework for Bayesian
case-based reasoning (CBR) and prototype classification and clustering. BCM
brings the intuitive power of CBR to a Bayesian generative framework. The BCM
learns prototypes, the "quintessential" observations that best represent
clusters in a dataset, by performing joint inference on cluster labels,
prototypes and important features. Simultaneously, BCM pursues sparsity by
learning subspaces, the sets of features that play important roles in the
characterization of the prototypes. The prototype and subspace representation
provides quantitative benefits in interpretability while preserving
classification accuracy. Human subject experiments verify statistically
significant improvements to participants' understanding when using explanations
produced by BCM, compared to those given by prior art.


Learning Optimized Or's of And's

  Or's of And's (OA) models are comprised of a small number of disjunctions of
conjunctions, also called disjunctive normal form. An example of an OA model is
as follows: If ($x_1 = $ `blue' AND $x_2=$ `middle') OR ($x_1 = $ `yellow'),
then predict $Y=1$, else predict $Y=0$. Or's of And's models have the advantage
of being interpretable to human experts, since they are a set of conditions
that concisely capture the characteristics of a specific subset of data. We
present two optimization-based machine learning frameworks for constructing OA
models, Optimized OA (OOA) and its faster version, Optimized OA with
Approximations (OOAx). We prove theoretical bounds on the properties of
patterns in an OA model. We build OA models as a diagnostic screening tool for
obstructive sleep apnea, that achieves high accuracy with a substantial gain in
interpretability over other methods.


The latent state hazard model, with application to wind turbine
  reliability

  We present a new model for reliability analysis that is able to distinguish
the latent internal vulnerability state of the equipment from the vulnerability
caused by temporary external sources. Consider a wind farm where each turbine
is running under the external effects of temperature, wind speed and direction,
etc. The turbine might fail because of the external effects of a spike in
temperature. If it does not fail during the temperature spike, it could still
fail due to internal degradation, and the spike could cause (or be an
indication of) this degradation. The ability to identify the underlying latent
state can help better understand the effects of external sources and thus lead
to more robust decision-making. We present an experimental study using SCADA
sensor measurements from wind turbines in Italy.


Learning Certifiably Optimal Rule Lists for Categorical Data

  We present the design and implementation of a custom discrete optimization
technique for building rule lists over a categorical feature space. Our
algorithm produces rule lists with optimal training performance, according to
the regularized empirical risk, with a certificate of optimality. By leveraging
algorithmic bounds, efficient data structures, and computational reuse, we
achieve several orders of magnitude speedup in time and a massive reduction of
memory consumption. We demonstrate that our approach produces optimal rule
lists on practical problems in seconds. Our results indicate that it is
possible to construct optimal sparse rule lists that are approximately as
accurate as the COMPAS proprietary risk prediction tool on data from Broward
County, Florida, but that are completely interpretable. This framework is a
novel alternative to CART and other decision tree methods for interpretable
modeling.


Robust Optimization using Machine Learning for Uncertainty Sets

  Our goal is to build robust optimization problems for making decisions based
on complex data from the past. In robust optimization (RO) generally, the goal
is to create a policy for decision-making that is robust to our uncertainty
about the future. In particular, we want our policy to best handle the the
worst possible situation that could arise, out of an uncertainty set of
possible situations. Classically, the uncertainty set is simply chosen by the
user, or it might be estimated in overly simplistic ways with strong
assumptions; whereas in this work, we learn the uncertainty set from data
collected in the past. The past data are drawn randomly from an (unknown)
possibly complicated high-dimensional distribution. We propose a new
uncertainty set design and show how tools from statistical learning theory can
be employed to provide probabilistic guarantees on the robustness of the
policy.


Bayesian Inference of Arrival Rate and Substitution Behavior from Sales
  Transaction Data with Stockouts

  When an item goes out of stock, sales transaction data no longer reflect the
original customer demand, since some customers leave with no purchase while
others substitute alternative products for the one that was out of stock. Here
we develop a Bayesian hierarchical model for inferring the underlying customer
arrival rate and choice model from sales transaction data and the corresponding
stock levels. The model uses a nonhomogeneous Poisson process to allow the
arrival rate to vary throughout the day, and allows for a variety of choice
models. Model parameters are inferred using a stochastic gradient MCMC
algorithm that can scale to large transaction databases. We fit the model to
data from a local bakery and show that it is able to make accurate
out-of-sample predictions, and to provide actionable insight into lost cookie
sales.


Causal Falling Rule Lists

  A causal falling rule list (CFRL) is a sequence of if-then rules that
specifies heterogeneous treatment effects, where (i) the order of rules
determines the treatment effect subgroup a subject belongs to, and (ii) the
treatment effect decreases monotonically down the list. A given CFRL
parameterizes a hierarchical bayesian regression model in which the treatment
effects are incorporated as parameters, and assumed constant within
model-specific subgroups. We formulate the search for the CFRL best supported
by the data as a Bayesian model selection problem, where we perform a search
over the space of CFRL models, and approximate the evidence for a given CFRL
model using standard variational techniques. We apply CFRL to a census wage
dataset to identify subgroups of differing wage inequalities between men and
women.


Cascaded High Dimensional Histograms: A Generative Approach to Density
  Estimation

  We present tree- and list- structured density estimation methods for high
dimensional binary/categorical data. Our density estimation models are high
dimensional analogies to variable bin width histograms. In each leaf of the
tree (or list), the density is constant, similar to the flat density within the
bin of a histogram. Histograms, however, cannot easily be visualized in higher
dimensions, whereas our models can. The accuracy of histograms fades as
dimensions increase, whereas our models have priors that help with
generalization. Our models are sparse, unlike high-dimensional histograms. We
present three generative models, where the first one allows the user to specify
the number of desired leaves in the tree within a Bayesian prior. The second
model allows the user to specify the desired number of branches within the
prior. The third model returns lists (rather than trees) and allows the user to
specify the desired number of rules and the length of rules within the prior.
Our results indicate that the new approaches yield a better balance between
sparsity and accuracy of density estimates than other methods for this task.


Learning Cost-Effective Treatment Regimes using Markov Decision
  Processes

  Decision makers, such as doctors and judges, make crucial decisions such as
recommending treatments to patients, and granting bails to defendants on a
daily basis. Such decisions typically involve weighting the potential benefits
of taking an action against the costs involved. In this work, we aim to
automate this task of learning \emph{cost-effective, interpretable and
actionable treatment regimes}. We formulate this as a problem of learning a
decision list -- a sequence of if-then-else rules -- which maps characteristics
of subjects (eg., diagnostic test results of patients) to treatments. We
propose a novel objective to construct a decision list which maximizes outcomes
for the population, and minimizes overall costs. We model the problem of
learning such a list as a Markov Decision Process (MDP) and employ a variant of
the Upper Confidence Bound for Trees (UCT) strategy which leverages customized
checks for pruning the search space effectively. Experimental results on real
world observational data capturing judicial bail decisions and treatment
recommendations for asthma patients demonstrate the effectiveness of our
approach.


Learning Cost-Effective and Interpretable Regimes for Treatment
  Recommendation

  Decision makers, such as doctors and judges, make crucial decisions such as
recommending treatments to patients, and granting bails to defendants on a
daily basis. Such decisions typically involve weighting the potential benefits
of taking an action against the costs involved. In this work, we aim to
automate this task of learning {cost-effective, interpretable and actionable
treatment regimes. We formulate this as a problem of learning a decision list
-- a sequence of if-then-else rules -- which maps characteristics of subjects
(eg., diagnostic test results of patients) to treatments. We propose a novel
objective to construct a decision list which maximizes outcomes for the
population, and minimizes overall costs. We model the problem of learning such
a list as a Markov Decision Process (MDP) and employ a variant of the Upper
Confidence Bound for Trees (UCT) strategy which leverages customized checks for
pruning the search space effectively. Experimental results on real world
observational data capturing treatment recommendations for asthma patients
demonstrate the effectiveness of our approach.


FLAME: A Fast Large-scale Almost Matching Exactly Approach to Causal
  Inference

  A classical problem in causal inference is that of matching, where treatment
units need to be matched to control units. Some of the main challenges in
developing matching methods arise from the tension among (i) inclusion of as
many covariates as possible in defining the matched groups, (ii) having matched
groups with enough treated and control units for a valid estimate of Average
Treatment Effect (ATE) in each group, and (iii) computing the matched pairs
efficiently for large datasets. In this paper we propose a fast method for
approximate and exact matching in causal analysis called FLAME (Fast
Large-scale Almost Matching Exactly). We define an optimization objective for
match quality, which gives preferences to matching on covariates that can be
useful for predicting the outcome while encouraging as many matches as
possible. FLAME aims to optimize our match quality measure, leveraging
techniques that are natural for query processing in the area of database
management. We provide two implementations of FLAME using SQL queries and
bit-vector techniques.


An Optimization Approach to Learning Falling Rule Lists

  A falling rule list is a probabilistic decision list for binary
classification, consisting of a series of if-then rules with antecedents in the
if clauses and probabilities of the desired outcome ("1") in the then clauses.
Just as in a regular decision list, the order of rules in a falling rule list
is important -- each example is classified by the first rule whose antecedent
it satisfies. Unlike a regular decision list, a falling rule list requires the
probabilities of the desired outcome ("1") to be monotonically decreasing down
the list. We propose an optimization approach to learning falling rule lists
and "softly" falling rule lists, along with Monte-Carlo search algorithms that
use bounds on the optimal solution to prune the search space.


Causal Rule Sets for Identifying Subgroups with Enhanced Treatment
  Effect

  We introduce a novel generative model for interpretable subgroup analysis for
causal inference applications, Causal Rule Sets (CRS). A CRS model uses a small
set of short rules to capture a subgroup where the average treatment effect is
elevated compared to the entire population. We present a Bayesian framework for
learning a causal rule set. The Bayesian framework consists of a prior that
favors simpler models and a Bayesian logistic regression that characterizes the
relation between outcomes, attributes and subgroup membership. We find maximum
a posteriori models using discrete Monte Carlo steps in the joint solution
space of rules sets and parameters. We provide theoretically grounded
heuristics and bounding strategies to improve search efficiency. Experiments
show that the search algorithm can efficiently recover a true underlying
subgroup and CRS shows consistently competitive performance compared to other
state-of-the-art baseline methods.


Extreme Dimension Reduction for Handling Covariate Shift

  In the covariate shift learning scenario, the training and test covariate
distributions differ, so that a predictor's average loss over the training and
test distributions also differ. In this work, we explore the potential of
extreme dimension reduction, i.e. to very low dimensions, in improving the
performance of importance weighting methods for handling covariate shift, which
fail in high dimensions due to potentially high train/test covariate divergence
and the inability to accurately estimate the requisite density ratios. We first
formulate and solve a problem optimizing over linear subspaces a combination of
their predictive utility and train/test divergence within. Applying it to
simulated and real data, we show extreme dimension reduction helps sometimes
but not always, due to a bias introduced by dimension reduction.


A Minimax Surrogate Loss Approach to Conditional Difference Estimation

  We present a new machine learning approach to estimate personalized treatment
effects in the classical potential outcomes framework with binary outcomes. To
overcome the problem that both treatment and control outcomes for the same unit
are required for supervised learning, we propose surrogate loss functions that
incorporate both treatment and control data. The new surrogates yield tighter
bounds than the sum of losses for treatment and control groups. A specific
choice of loss function, namely a type of hinge loss, yields a minimax support
vector machine formulation. The resulting optimization problem requires the
solution to only a single convex optimization problem, incorporating both
treatment and control units, and it enables the kernel trick to be used to
handle nonlinear (also non-parametric) estimation. Statistical learning bounds
are also presented for the framework, and experimental results.


New Techniques for Preserving Global Structure and Denoising with Low
  Information Loss in Single-Image Super-Resolution

  This work identifies and addresses two important technical challenges in
single-image super-resolution: (1) how to upsample an image without magnifying
noise and (2) how to preserve large scale structure when upsampling. We
summarize the techniques we developed for our second place entry in Track 1
(Bicubic Downsampling), seventh place entry in Track 2 (Realistic Adverse
Conditions), and seventh place entry in Track 3 (Realistic difficult) in the
2018 NTIRE Super-Resolution Challenge. Furthermore, we present new neural
network architectures that specifically address the two challenges listed
above: denoising and preservation of large-scale structure.


Bayesian Patchworks: An Approach to Case-Based Reasoning

  Doctors often rely on their past experience in order to diagnose patients.
For a doctor with enough experience, almost every patient would have
similarities to key cases seen in the past, and each new patient could be
viewed as a mixture of these key past cases. Because doctors often tend to
reason this way, an efficient computationally aided diagnostic tool that thinks
in the same way might be helpful in locating key past cases of interest that
could assist with diagnosis. This article develops a novel mathematical model
to mimic the type of logical thinking that physicians use when considering past
cases. The proposed model can also provide physicians with explanations that
would be similar to the way they would naturally reason about cases. The
proposed method is designed to yield predictive accuracy, computational
efficiency, and insight into medical data; the key element is the insight into
medical data, in some sense we are automating a complicated process that
physicians might perform manually. We finally implemented the result of this
work on two publicly available healthcare datasets, for heart disease
prediction and breast cancer prediction.


MALTS: Matching After Learning to Stretch

  We introduce a flexible framework for matching in causal inference that
produces high quality almost-exact matches. Most prior work in matching uses ad
hoc distance metrics, often leading to poor quality matches, particularly when
there are irrelevant covariates that degrade the distance metric. In this work,
we learn an interpretable distance metric used for matching, which leads to
substantially higher quality matches. The distance metric can stretch
continuous covariates and matches exactly on categorical covariates. The
framework is flexible in that the user can choose the form of distance metric,
the type of optimization algorithm, and the type of relaxation for matching.
Our ability to learn flexible distance metrics leads to matches that are
interpretable and useful for estimation of conditional average treatment
effects.


An Interpretable Model with Globally Consistent Explanations for Credit
  Risk

  We propose a possible solution to a public challenge posed by the Fair Isaac
Corporation (FICO), which is to provide an explainable model for credit risk
assessment. Rather than present a black box model and explain it afterwards, we
provide a globally interpretable model that is as accurate as other neural
networks. Our "two-layer additive risk model" is decomposable into subscales,
where each node in the second layer represents a meaningful subscale, and all
of the nonlinearities are transparent. We provide three types of explanations
that are simpler than, but consistent with, the global model. One of these
explanation methods involves solving a minimum set cover problem to find
high-support globally-consistent explanations. We present a new online
visualization tool to allow users to explore the global model and its
explanations.


Variable Importance Clouds: A Way to Explore Variable Importance for the
  Set of Good Models

  Variable importance is central to scientific studies, including the social
sciences and causal inference, healthcare, and in other domains. However,
current notions of variable importance are often tied to a specific predictive
model. This is problematic: what if there were multiple well-performing
predictive models, and a specific variable is important to some of them and not
to others? In that case, we may not be able to tell from a single
well-performing model whether a variable is always important in predicting the
outcome. Rather than depending on variable importance for a single predictive
model, we would like to explore variable importance for all
approximately-equally-accurate predictive models. This work introduces the
concept of a variable importance cloud, which maps every variable to its
importance for every good predictive model. We show properties of the variable
importance cloud and draw connections other areas of statistics. We introduce
variable importance diagrams as a projection of the variable importance cloud
into two dimensions for visualization purposes. Experiments with criminal
justice and marketing data illustrate how variables can change dramatically in
importance for approximately-equally-accurate predictive models.


A Practical Bandit Method with Advantages in Neural Network Tuning

  Stochastic bandit algorithms can be used for challenging non-convex
optimization problems. Hyperparameter tuning of neural networks is particularly
challenging, necessitating new approaches. To this end, we present a method
that adaptively partitions the combined space of hyperparameters, context, and
training resources (e.g., total number of training iterations). By adaptively
partitioning the space, the algorithm is able to focus on the portions of the
hyperparameter search space that are most relevant in a practical way. By
including the resources in the combined space, the method tends to use fewer
training resources overall. Our experiments show that this method can surpass
state-of-the-art methods in tuning neural networks on benchmark datasets. In
some cases, our implementations can achieve the same levels of accuracy on
benchmark datasets as existing state-of-the-art approaches while saving over
50% of our computational resources (e.g. time, training iterations).


Or's of And's for Interpretable Classification, with Application to
  Context-Aware Recommender Systems

  We present a machine learning algorithm for building classifiers that are
comprised of a small number of disjunctions of conjunctions (or's of and's). An
example of a classifier of this form is as follows: If X satisfies (x1 = 'blue'
AND x3 = 'middle') OR (x1 = 'blue' AND x2 = '<15') OR (x1 = 'yellow'), then we
predict that Y=1, ELSE predict Y=0. An attribute-value pair is called a literal
and a conjunction of literals is called a pattern. Models of this form have the
advantage of being interpretable to human experts, since they produce a set of
conditions that concisely describe a specific class. We present two
probabilistic models for forming a pattern set, one with a Beta-Binomial prior,
and the other with Poisson priors. In both cases, there are prior parameters
that the user can set to encourage the model to have a desired size and shape,
to conform with a domain-specific definition of interpretability. We provide
two scalable MAP inference approaches: a pattern level search, which involves
association rule mining, and a literal level search. We show stronger priors
reduce computation. We apply the Bayesian Or's of And's (BOA) model to predict
user behavior with respect to in-vehicle context-aware personalized recommender
systems.


Regulating Greed Over Time

  In retail, there are predictable yet dramatic time-dependent patterns in
customer behavior, such as periodic changes in the number of visitors, or
increases in visitors just before major holidays. The current paradigm of
multi-armed bandit analysis does not take these known patterns into account.
This means that for applications in retail, where prices are fixed for periods
of time, current bandit algorithms will not suffice. This work provides a
remedy that takes the time-dependent patterns into account, and we show how
this remedy is implemented in the UCB and {\epsilon}-greedy methods and we
introduce a new policy called the variable arm pool method. In the corrected
methods, exploitation (greed) is regulated over time, so that more exploitation
occurs during higher reward periods, and more exploration occurs in periods of
low reward. In order to understand why regret is reduced with the corrected
methods, we present a set of bounds that provide insight into why we would want
to exploit during periods of high reward, and discuss the impact on regret. Our
proposed methods perform well in experiments, and were inspired by a
high-scoring entry in the Exploration and Exploitation 3 contest using data
from Yahoo! Front Page. That entry heavily used time-series methods to regulate
greed over time, which was substantially more effective than other contextual
bandit methods.


Reactive point processes: A new approach to predicting power failures in
  underground electrical systems

  Reactive point processes (RPPs) are a new statistical model designed for
predicting discrete events in time based on past history. RPPs were developed
to handle an important problem within the domain of electrical grid
reliability: short-term prediction of electrical grid failures ("manhole
events"), including outages, fires, explosions and smoking manholes, which can
cause threats to public safety and reliability of electrical service in cities.
RPPs incorporate self-exciting, self-regulating and saturating components. The
self-excitement occurs as a result of a past event, which causes a temporary
rise in vulner ability to future events. The self-regulation occurs as a result
of an external inspection which temporarily lowers vulnerability to future
events. RPPs can saturate when too many events or inspections occur close
together, which ensures that the probability of an event stays within a
realistic range. Two of the operational challenges for power companies are (i)
making continuous-time failure predictions, and (ii) cost/benefit analysis for
decision making and proactive maintenance. RPPs are naturally suited for
handling both of these challenges. We use the model to predict power-grid
failures in Manhattan over a short-term horizon, and to provide a cost/benefit
analysis of different proactive maintenance programs.


The Rate of Convergence of AdaBoost

  The AdaBoost algorithm was designed to combine many "weak" hypotheses that
perform slightly better than random guessing into a "strong" hypothesis that
has very low error. We study the rate at which AdaBoost iteratively converges
to the minimum of the "exponential loss." Unlike previous work, our proofs do
not require a weak-learning assumption, nor do they require that minimizers of
the exponential loss are finite. Our first result shows that at iteration $t$,
the exponential loss of AdaBoost's computed parameter vector will be at most
$\epsilon$ more than that of any parameter vector of $\ell_1$-norm bounded by
$B$ in a number of rounds that is at most a polynomial in $B$ and $1/\epsilon$.
We also provide lower bounds showing that a polynomial dependence on these
parameters is necessary. Our second result is that within $C/\epsilon$
iterations, AdaBoost achieves a value of the exponential loss that is at most
$\epsilon$ more than the best possible value, where $C$ depends on the dataset.
We show that this dependence of the rate on $\epsilon$ is optimal up to
constant factors, i.e., at least $\Omega(1/\epsilon)$ rounds are necessary to
achieve within $\epsilon$ of the optimal exponential loss.


Interpretable Classification Models for Recidivism Prediction

  We investigate a long-debated question, which is how to create predictive
models of recidivism that are sufficiently accurate, transparent, and
interpretable to use for decision-making. This question is complicated as these
models are used to support different decisions, from sentencing, to determining
release on probation, to allocating preventative social services. Each use case
might have an objective other than classification accuracy, such as a desired
true positive rate (TPR) or false positive rate (FPR). Each (TPR, FPR) pair is
a point on the receiver operator characteristic (ROC) curve. We use popular
machine learning methods to create models along the full ROC curve on a wide
range of recidivism prediction problems. We show that many methods (SVM, Ridge
Regression) produce equally accurate models along the full ROC curve. However,
methods that designed for interpretability (CART, C5.0) cannot be tuned to
produce models that are accurate and/or interpretable. To handle this
shortcoming, we use a new method known as SLIM (Supersparse Linear Integer
Models) to produce accurate, transparent, and interpretable models along the
full ROC curve. These models can be used for decision-making for many different
use cases, since they are just as accurate as the most powerful black-box
machine learning models, but completely transparent, and highly interpretable.


Interpretable classifiers using rules and Bayesian analysis: Building a
  better stroke prediction model

  We aim to produce predictive models that are not only accurate, but are also
interpretable to human experts. Our models are decision lists, which consist of
a series of if...then... statements (e.g., if high blood pressure, then stroke)
that discretize a high-dimensional, multivariate feature space into a series of
simple, readily interpretable decision statements. We introduce a generative
model called Bayesian Rule Lists that yields a posterior distribution over
possible decision lists. It employs a novel prior structure to encourage
sparsity. Our experiments show that Bayesian Rule Lists has predictive accuracy
on par with the current top algorithms for prediction in machine learning. Our
method is motivated by recent developments in personalized medicine, and can be
used to produce highly accurate and interpretable medical scoring systems. We
demonstrate this by producing an alternative to the CHADS$_2$ score, actively
used in clinical practice for estimating the risk of stroke in patients that
have atrial fibrillation. Our model is as interpretable as CHADS$_2$, but more
accurate.


