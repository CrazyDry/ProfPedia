How Many Vote Operations Are Needed to Manipulate A Voting System?

  In this paper, we propose a framework to study a general class of strategicbehavior in voting, which we call vote operations. We prove the followingtheorem: if we fix the number of alternatives, generate $n$ votes i.i.d.according to a distribution $\pi$, and let $n$ go to infinity, then for any$\epsilon >0$, with probability at least $1-\epsilon$, the minimum number ofoperations that are needed for the strategic individual to achieve her goalfalls into one of the following four categories: (1) 0, (2) $\Theta(\sqrt n)$,(3) $\Theta(n)$, and (4) $\infty$. This theorem holds for any set of voteoperations, any individual vote distribution $\pi$, and any integer generalizedscoring rule, which includes (but is not limited to) almost all commonlystudied voting rules, e.g., approval voting, all positional scoring rules(including Borda, plurality, and veto), plurality with runoff, Bucklin,Copeland, maximin, STV, and ranked pairs.  We also show that many well-studied types of strategic behavior fall underour framework, including (but not limited to) constructive/destructivemanipulation, bribery, and control by adding/deleting votes, margin of victory,and minimum manipulation coalition size. Therefore, our main theorem naturallyapplies to these problems.

Determining Possible and Necessary Winners Given Partial Orders

  Usually a voting rule requires agents to give their preferences as linearorders. However, in some cases it is impractical for an agent to give a linearorder over all the alternatives. It has been suggested to let agents submitpartial orders instead. Then, given a voting rule, a profile of partial orders,and an alternative (candidate) c, two important questions arise: first, is itstill possible for c to win, and second, is c guaranteed to win? These are thepossible winner and necessary winner problems, respectively. Each of these twoproblems is further divided into two sub-problems: determining whether c is aunique winner (that is, c is the only winner), or determining whether c is aco-winner (that is, c is in the set of winners). We consider the setting wherethe number of alternatives is unbounded and the votes are unweighted. Wecompletely characterize the complexity of possible/necessary winner problemsfor the following common voting rules: a class of positional scoring rules(including Borda), Copeland, maximin, Bucklin, ranked pairs, voting trees, andplurality with runoff.

A Statistical Decision-Theoretic Framework for Social Choice

  In this paper, we take a statistical decision-theoretic viewpoint on socialchoice, putting a focus on the decision to be made on behalf of a system ofagents. In our framework, we are given a statistical ranking model, a decisionspace, and a loss function defined on (parameter, decision) pairs, andformulate social choice mechanisms as decision rules that minimize expectedloss. This suggests a general framework for the design and analysis of newsocial choice mechanisms. We compare Bayesian estimators, which minimizeBayesian expected loss, for the Mallows model and the Condorcet modelrespectively, and the Kemeny rule. We consider various normative properties, inaddition to computational complexity and asymptotic behavior. In particular, weshow that the Bayesian estimator for the Condorcet model satisfies some desiredproperties such as anonymity, neutrality, and monotonicity, can be computed inpolynomial time, and is asymptotically different from the other two rules whenthe data are generated from the Condorcet model for some ground truthparameter.

Allocating Indivisible Items in Categorized Domains

  We formulate a general class of allocation problems called categorized domainallocation problems (CDAPs), where indivisible items from multiple categoriesare allocated to agents without monetary transfer and each agent gets at leastone item per category.  We focus on basic CDAPs, where the number of items in each category is equalto the number of agents. We characterize serial dictatorships for basic CDAPsby a minimal set of three axiomatic properties: strategy-proofness,non-bossiness, and category-wise neutrality. Then, we propose a naturalextension of serial dictatorships called categorial sequential allocationmechanisms (CSAMs), which allocate the items in multiple rounds: in each round,the active agent chooses an item from a designated category. We fullycharacterize the worst-case rank efficiency of CSAMs for optimistic andpessimistic agents, and provide a bound for strategic agents. We also conductexperiments to compare expected rank efficiency of various CSAMs w.r.t. randomgenerated data.

Mechanism Design for Multi-Type Housing Markets

  We study multi-type housing markets, where there are $p\ge 2$ types of items,each agent is initially endowed one item of each type, and the goal is todesign mechanisms without monetary transfer to (re)allocate items to the agentsbased on their preferences over bundles of items, such that each agent gets oneitem of each type. In sharp contrast to classical housing markets, previousstudies in multi-type housing markets have been hindered by the lack of naturalsolution concepts, because the strict core might be empty.  We break the barrier in the literature by leveraging AI techniques and makingnatural assumptions on agents' preferences. We show that when agents'preferences are lexicographic, even with different importance orders, theclassical top-trading-cycles mechanism can be extended while preserving most ofits nice properties. We also investigate computational complexity of checkingwhether an allocation is in the strict core and checking whether the strictcore is empty. Our results convey an encouragingly positive message: it ispossible to design good mechanisms for multi-type housing markets under naturalassumptions on preferences.

New Candidates Welcome! Possible Winners with respect to the Addition of  New Candidates

  In voting contexts, some new candidates may show up in the course of theprocess. In this case, we may want to determine which of the initial candidatesare possible winners, given that a fixed number $k$ of new candidates will beadded. We give a computational study of this problem, focusing on scoringrules, and we provide a formal comparison with related problems such as controlvia adding candidates or cloning.

Combining Voting Rules Together

  We propose a simple method for combining together voting rules that performsa run-off between the different winners of each voting rule. We prove that thiscombinator has several good properties. For instance, even if just one of thebase voting rules has a desirable property like Condorcet consistency, thecombination inherits this property. In addition, we prove that combining votingrules together in this way can make finding a manipulation more computationallydifficult. Finally, we study the impact of this combinator on approximationmethods that find close to optimal manipulations.

Welfare of Sequential Allocation Mechanisms for Indivisible Goods

  Sequential allocation is a simple and attractive mechanism for the allocationof indivisible goods. Agents take turns, according to a policy, to pick items.Sequential allocation is guaranteed to return an allocation which is efficientbut may not have an optimal social welfare. We consider therefore the relationbetween welfare and efficiency. We study the (computational) questions of whatwelfare is possible or necessary depending on the choice of policy. We alsoconsider a novel control problem in which the chair chooses a policy to improvesocial welfare.

Composite Marginal Likelihood Methods for Random Utility Models

  We propose a novel and flexiblerank-breaking-then-composite-marginal-likelihood (RBCML) framework for learningrandom utility models (RUMs), which include the Plackett-Luce model. Wecharacterize conditions for the objective function of RBCML to be strictlylog-concave by proving that strict log-concavity is preserved under convolutionand marginalization. We characterize necessary and sufficient conditions forRBCML to satisfy consistency and asymptotic normality. Experiments on syntheticdata show that RBCML for Gaussian RUMs achieves better statistical efficiencyand computational efficiency than the state-of-the-art algorithm and our RBCMLfor the Plackett-Luce model provides flexible tradeoffs between running timeand statistical efficiency.

Dominating Manipulations in Voting with Partial Information

  We consider manipulation problems when the manipulator only has partialinformation about the votes of the nonmanipulators. Such partial information isdescribed by an information set, which is the set of profiles of thenonmanipulators that are indistinguishable to the manipulator. Given such aninformation set, a dominating manipulation is a non-truthful vote that themanipulator can cast which makes the winner at least as preferable (andsometimes more preferable) as the winner when the manipulator votes truthfully.When the manipulator has full information, computing whether or not thereexists a dominating manipulation is in P for many common voting rules (by knownresults). We show that when the manipulator has no information, there is nodominating manipulation for many common voting rules. When the manipulator'sinformation is represented by partial orders and only a small portion of thepreferences are unknown, computing a dominating manipulation is NP-hard formany common voting rules. Our results thus throw light on whether we canprevent strategic behavior by limiting information about the votes of othervoters.

A Mathematical Model for Optimal Decisions in a Representative Democracy

  Direct democracy is a special case of an ensemble of classifiers, where everyperson (classifier) votes on every issue. This fails when the average votercompetence (classifier accuracy) falls below 50%, which can happen in noisysettings where voters have only limited information, or when there are multipletopics and the average voter competence may not be high enough for some topics.Representative democracy, where voters choose representatives to vote, can bean elixir in both these situations. Representative democracy is a specific wayto improve the ensemble of classifiers. We introduce a mathematical model forstudying representative democracy, in particular understanding the parametersof a representative democracy that gives maximum decision making capability.Our main result states that under general and natural conditions,  1. Representative democracy can make the correct decisions simultaneously formultiple noisy issues.  2. When the cost of voting is fixed, the optimal representative democracyrequires that representatives are elected from constant sized groups: thenumber of representatives should be linear in the number of voters.  3. When the cost and benefit of voting are both polynomial, the optimal groupsize is close to linear in the number of voters. This work sets themathematical foundation for studying the quality-quantity tradeoff in arepresentative democracy-type ensemble (fewer highly qualified representativesversus more less qualified representatives).

Probabilistic Automata for Computing with Words

  Usually, probabilistic automata and probabilistic grammars have crisp symbolsas inputs, which can be viewed as the formal models of computing with values.In this paper, we first introduce probabilistic automata and probabilisticgrammars for computing with (some special) words in a probabilistic framework,where the words are interpreted as probabilistic distributions or possibilitydistributions over a set of crisp symbols. By probabilistic conditioning, wethen establish a retraction principle from computing with words to computingwith values for handling crisp inputs and a generalized extension principlefrom computing with words to computing with all words for handling arbitraryinputs. These principles show that computing with values and computing with allwords can be respectively implemented by computing with some special words. Tocompare the transition probabilities of two near inputs, we also examine someanalytical properties of the transition probability functions of generalizedextensions. Moreover, the retractions and the generalized extensions are shownto be equivalence-preserving. Finally, we clarify some relationships among theretractions, the generalized extensions, and the extensions studied recently byQiu and Wang.

Testing and Data Reduction of the Chinese Small Telescope Array (CSTAR)  for Dome A, Antarctica

  The Chinese Small Telescope ARray (hereinafter CSTAR) is the first Chineseastronomical instrument on the Antarctic ice cap. The low temperature and lowpressure testing of the data acquisition system was carried out in a laboratoryrefrigerator and on the 4500m Pamirs high plateau, respectively. The resultsfrom the final four nights of test observations demonstrated that CSTAR wasready for operation at Dome A, Antarctica. In this paper we present adescription of CSTAR and the performance derived from the test observations.

Manipulation of Nanson's and Baldwin's Rules

  Nanson's and Baldwin's voting rules select a winner by successivelyeliminating candidates with low Borda scores. We show that these rules have anumber of desirable computational properties. In particular, with unweightedvotes, it is NP-hard to manipulate either rule with one manipulator, whilstwith weighted votes, it is NP-hard to manipulate either rule with a smallnumber of candidates and a coalition of manipulators. As only a couple of othervoting rules are known to be NP-hard to manipulate with a single manipulator,Nanson's and Baldwin's rules appear to be particularly resistant tomanipulation from a theoretical perspective. We also propose a number ofapproximation methods for manipulating these two rules. Experiments demonstratethat both rules are often difficult to manipulate in practice. These resultssuggest that elimination style voting rules deserve further study.

Price Updating in Combinatorial Prediction Markets with Bayesian  Networks

  To overcome the #P-hardness of computing/updating prices in logarithm marketscoring rule-based (LMSR-based) combinatorial prediction markets, Chen et al.[5] recently used a simple Bayesian network to represent the prices ofsecurities in combinatorial predictionmarkets for tournaments, and showed thattwo types of popular securities are structure preserving. In this paper, wesignificantly extend this idea by employing Bayesian networks in generalcombinatorial prediction markets. We reveal a very natural connection betweenLMSR-based combinatorial prediction markets and probabilistic beliefaggregation,which leads to a complete characterization of all structurepreserving securities for decomposable network structures. Notably, the mainresults by Chen et al. [5] are corollaries of our characterization. We thenprove that in order for a very basic set of securities to be structurepreserving, the graph of the Bayesian network must be decomposable. We alsodiscuss some approximation techniques for securities that are not structurepreserving.

Random Utility Theory for Social Choice

  Random utility theory models an agent's preferences on alternatives bydrawing a real-valued score on each alternative (typically independently) froma parameterized distribution, and then ranking the alternatives according toscores. A special case that has received significant attention is thePlackett-Luce model, for which fast inference methods for maximum likelihoodestimators are available. This paper develops conditions on general randomutility models that enable fast inference within a Bayesian framework throughMC-EM, providing concave loglikelihood functions and bounded sets of globalmaxima solutions. Results on both real-world and simulated data provide supportfor the scalability of the approach and capability for model selection amonggeneral random utility models including Plackett-Luce.

Preference Elicitation For General Random Utility Models

  This paper discusses {General Random Utility Models (GRUMs)}. These are aclass of parametric models that generate partial ranks over alternatives givenattributes of agents and alternatives. We propose two preference elicitationscheme for GRUMs developed from principles in Bayesian experimental design, onefor social choice and the other for personalized choice. We couple this with ageneral Monte-Carlo-Expectation-Maximization (MC-EM) based algorithm for MAPinference under GRUMs. We also prove uni-modality of the likelihood functionsfor a class of GRUMs. We examine the performance of various criteria byexperimental studies, which show that the proposed elicitation scheme increasesthe precision of estimation.

Structure and complexity of ex post efficient random assignments

  In the random assignment problem, objects are randomly assigned to agentskeeping in view the agents' preferences over objects. A random assignmentspecifies the probability of an agent getting an object. We examine thestructural and computational aspects of ex post efficiency of randomassignments. We first show that whereas an ex post efficient assignment can becomputed easily, checking whether a given random assignment is ex postefficient is NP-complete. Hence implementing a given random assignment viadeterministic Pareto optimal assignments is NP-hard. We then formalize anotherconcept of efficiency called robust ex post efficiency that is weaker thanstochastic dominance efficiency but stronger than ex post efficiency. Wepresent a characterization of robust ex post efficiency and show that it can betested in polynomial time if there are a constant number of agent types. It isshown that the well-known random serial dictatorship rule is not robust ex postefficient. Finally, we show that whereas robust ex post efficiency dependssolely on which entries of the assignment matrix are zero/non-zero, ex postefficiency of an assignment depends on the actual values.

Possible and Necessary Allocations via Sequential Mechanisms

  A simple mechanism for allocating indivisible resources is sequentialallocation in which agents take turns to pick items. We focus on possible andnecessary allocation problems, checking whether allocations of a given formoccur in some or all mechanisms for several commonly used classes of sequentialallocation mechanisms. In particular, we consider whether a given agentreceives a given item, a set of items, or a subset of items for five naturalclasses of sequential allocation mechanisms: balanced, recursively balanced,balanced alternating, strictly alternating and all policies. We identifycharacterizations of allocations produced balanced, recursively balanced,balanced alternating policies and strictly alternating policies respectively,which extend the well-known characterization by Brams and King [2005] forpolicies without restrictions. In addition, we examine the computationalcomplexity of possible and necessary allocation problems for these classes.

Learning Mixtures of Plackett-Luce Models

  In this paper we address the identifiability and efficient learning problemsof finite mixtures of Plackett-Luce models for rank data. We prove that for any$k\geq 2$, the mixture of $k$ Plackett-Luce models for no more than $2k-1$alternatives is non-identifiable and this bound is tight for $k=2$. For genericidentifiability, we prove that the mixture of $k$ Plackett-Luce models over $m$alternatives is generically identifiable if $k\leq\lfloor\frac {m-2}2\rfloor!$. We also propose an efficient generalized method of moments (GMM)algorithm to learn the mixture of two Plackett-Luce models and show that thealgorithm is consistent. Our experiments show that our GMM algorithm issignificantly faster than the EMM algorithm by Gormley and Murphy (2008), whileachieving competitive statistical efficiency.

A Cost-Effective Framework for Preference Elicitation and Aggregation

  We propose a cost-effective framework for preference elicitation andaggregation under the Plackett-Luce model with features. Given a budget, ourframework iteratively computes the most cost-effective elicitation questions inorder to help the agents make a better group decision.  We illustrate the viability of the framework with experiments on AmazonMechanical Turk, which we use to estimate the cost of answering different typesof elicitation questions. We compare the prediction accuracy of our frameworkwhen adopting various information criteria that evaluate the expectedinformation gain from a question. Our experiments show carefully designedinformation criteria are much more efficient, i.e., they arrive at the correctanswer using fewer queries, than randomly asking questions given the budgetconstraint.

Practical Algorithms for STV and Ranked Pairs with Parallel Universes  Tiebreaking

  STV and ranked pairs (RP) are two well-studied voting rules for groupdecision-making. They proceed in multiple rounds, and are affected by how tiesare broken in each round. However, the literature is surprisingly vague abouthow ties should be broken. We propose the first algorithms for computing theset of alternatives that are winners under some tiebreaking mechanism under STVand RP, which is also known as parallel-universes tiebreaking (PUT).Unfortunately, PUT-winners are NP-complete to compute under STV and RP, andstandard search algorithms from AI do not apply. We propose multiple DFS-basedalgorithms along with pruning strategies and heuristics to prioritize searchdirection to significantly improve the performance using machine learning. Wealso propose novel ILP formulations for PUT-winners under STV and RP,respectively. Experiments on synthetic and real-world data show that ouralgorithms are overall significantly faster than ILP, while there are a fewcases where ILP is significantly faster for RP.

Towards Non-Parametric Learning to Rank

  This paper studies a stylized, yet natural, learning-to-rank problem andpoints out the critical incorrectness of a widely used nearest neighboralgorithm. We consider a model with $n$ agents (users) $\{x_i\}_{i \in [n]}$and $m$ alternatives (items) $\{y_j\}_{j \in [m]}$, each of which is associatedwith a latent feature vector. Agents rank items nondeterministically accordingto the Plackett-Luce model, where the higher the utility of an item to theagent, the more likely this item will be ranked high by the agent. Our goal isto find neighbors of an arbitrary agent or alternative in the latent space.  We first show that the Kendall-tau distance based kNN produces incorrectresults in our model. Next, we fix the problem by introducing a new algorithmwith features constructed from "global information" of the data matrix. Ourapproach is in sharp contrast to most existing feature engineering methods.Finally, we design another new algorithm identifying similar alternatives. Theconstruction of alternative features can be done using "local information,"highlighting the algorithmic difference between finding similar agents andsimilar alternatives.

Practical Algorithms for Multi-Stage Voting Rules with Parallel  Universes Tiebreaking

  STV and ranked pairs (RP) are two well-studied voting rules for groupdecision-making. They proceed in multiple rounds, and are affected by how tiesare broken in each round. However, the literature is surprisingly vague abouthow ties should be broken. We propose the first algorithms for computing theset of alternatives that are winners under some tiebreaking mechanism under STVand RP, which is also known as parallel-universes tiebreaking (PUT).Unfortunately, PUT-winners are NP-complete to compute under STV and RP, andstandard search algorithms from AI do not apply. We propose multiple DFS-basedalgorithms along with pruning strategies, heuristics, sampling and machinelearning to prioritize search direction to significantly improve theperformance. We also propose novel ILP formulations for PUT-winners under STVand RP, respectively. Experiments on synthetic and real-world data show thatour algorithms are overall faster than ILP.

Incentive Compatible Budget Elicitation in Multi-unit Auctions

  In this paper, we consider the problem of designing incentive compatibleauctions for multiple (homogeneous) units of a good, when bidders have privatevaluations and private budget constraints. When only the valuations are privateand the budgets are public, Dobzinski {\em et al} show that the {\em adaptiveclinching} auction is the unique incentive-compatible auction achievingPareto-optimality. They further show thatthere is no deterministicPareto-optimal auction with private budgets. Our main contribution is to showthe following Budget Monotonicity property of this auction: When there is onlyone infinitely divisible good, a bidder cannot improve her utility by reportinga budget smaller than the truth. This implies that a randomized modification tothe adaptive clinching auction is incentive compatible and Pareto-optimal withprivate budgets.  The Budget Monotonicity property also implies other improved results in thiscontext. For revenue maximization, the same auction improves the best-knowncompetitive ratio due to Abrams by a factor of 4, and asymptotically approachesthe performance of the optimal single-price auction.  Finally, we consider the problem of revenue maximization (or social welfare)in a Bayesian setting. We allow the bidders have public size constraints (onthe amount of good they are willing to buy) in addition to private budgetconstraints. We show a simple poly-time computable 5.83-approximation to theoptimal Bayesian incentive compatible mechanism, that is implementable indominant strategies. Our technique again crucially needs the ability to preventbidders from over-reporting budgets via randomization.

How Private Is Your Voting? A Framework for Comparing the Privacy of  Voting Mechanisms

  Voting privacy has received a lot of attention across several researchcommunities. Traditionally, cryptographic literature has focused on how toprivately implement a voting mechanism. Yet, a number of recent works attemptto minimize the amount of information one can infer from the output (ratherthan the implementation) of the voting mechanism. These works applydifferential privacy (DP) techniques which noise the outcome to achieveprivacy. This approach intrinsically compromises accuracy, rendering such avoting mechanism unsuitable for most realistic scenarios.  In this work we investigate the inherent "noiseless" privacy that differentvoting rules achieve. To this end we utilize the well-accepted notion ofDistributional Differential Privacy (DDP). We prove that under standardassumptions in voting literature about the distribution of votes, most naturalmechanisms achieve a satisfactory level of DDP, indicating that noising--andits negative side-effects for voting--is unnecessary in most cases.  We then put forth a systematic study of noiseless privacy of commonly studiedof voting rules, and compare these rules with respect to their privacy. Notethat both DP and DDP induce (possibly loose) upper bounds on informationleakage, which makes them insufficient for such a task. To circumvent this, weextend the definitions to require the bound to be exact (i.e. optimal) in awell defined manner. Although motivated by voting, our definitions andtechniques can be generically applied to address the optimality (with respectto privacy) of general mechanisms for privacy-preserving data release.

The First Release of the CSTAR Point Source Catalog from Dome A,  Antarctica

  In 2008 January the 24th Chinese expedition team successfully deployed theChinese Small Telescope ARray (CSTAR) to DomeA, the highest point on theAntarctic plateau. CSTAR consists of four 14.5cm optical telescopes, each witha different filter (g, r, i and open) and has a 4.5degree x 4.5degree field ofview (FOV). It operates robotically as part of the Plateau Observatory, PLATO,with each telescope taking an image every 30 seconds throughout the yearwhenever it is dark. During 2008, CSTAR #1 performed almost flawlessly,acquiring more than 0.3 million i-band images for a total integration time of1728 hours during 158 days of observations. For each image taken under good skyconditions, more than 10,000 sources down to 16 mag could be detected. Weperformed aperture photometry on all the sources in the field to create thecatalog described herein. Since CSTAR has a fixed pointing centered on theSouth Celestial Pole (Dec =-90 degree), all the sources within the FOV of CSTARwere monitored continuously for several months. The photometric catalog can beused for studying any variability in these sources, and for the discovery oftransient sources such as supernovae, gamma-ray bursts and minor planets.

The sky brightness and transparency in i-band at Dome A, Antarctica

  The i-band observing conditions at Dome A on the Antarctic plateau have beeninvestigated using data acquired during 2008 with the Chinese Small TelescopeARray. The sky brightness, variations in atmospheric transparency, cloud cover,and the presence of aurorae are obtained from these images. The median skybrightness of moonless clear nights is 20.5 mag arcsec^{-2} in the SDSS $i$band at the South Celestial Pole (which includes a contribution of about 0.06mag from diffuse Galactic light). The median over all Moon phases in theAntarctic winter is about 19.8 mag arcsec^{-2}. There were no thick clouds in2008. We model contributions of the Sun and the Moon to the sky background toobtain the relationship between the sky brightness and transparency. Auroraeare identified by comparing the observed sky brightness to the sky brightnessexpected from this model. About 2% of the images are affected by relativelystrong aurorae.

