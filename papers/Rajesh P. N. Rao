Towards Neural Co-Processors for the Brain: Combining Decoding and
  Encoding in Brain-Computer Interfaces

  The field of brain-computer interfaces is poised to advance from the
traditional goal of controlling prosthetic devices using brain signals to
combining neural decoding and encoding within a single neuroprosthetic device.
Such a device acts as a "co-processor" for the brain, with applications ranging
from inducing Hebbian plasticity for rehabilitation after brain injury to
reanimating paralyzed limbs and enhancing memory. We review recent progress in
simultaneous decoding and encoding for closed-loop control and plasticity
induction. To address the challenge of multi-channel decoding and encoding, we
introduce a unifying framework for developing brain co-processors based on
artificial neural networks and deep learning. These "neural co-processors" can
be used to jointly optimize cost functions with the nervous system to achieve
desired behaviors ranging from targeted neuro-rehabilitation to augmentation of
brain function.


The Indus Script and Economics. A Role for Indus Seals and Tablets in
  Rationing and Administration of Labor

  The Indus script remains one of the last major undeciphered scripts of the
ancient world. We focus here on Indus inscriptions on a group of miniature
tablets discovered by Meadow and Kenoyer in Harappa in 1997. By drawing
parallels with proto-Elamite and proto-Cuneiform inscriptions, we explore how
these miniature tablets may have been used to record rations allocated to
porters or laborers. We then show that similar inscriptions are found on stamp
seals, leading to the potentially provocative conclusion that rather than
simply indicating ownership of property, Indus seals may have been used for
generating tokens, tablets and sealings for repetitive economic transactions
such as rations and exchange of canonical amounts of goods, grains, animals,
and labor in a barter-based economy.


Large Margin Boltzmann Machines and Large Margin Sigmoid Belief Networks

  Current statistical models for structured prediction make simplifying
assumptions about the underlying output graph structure, such as assuming a
low-order Markov chain, because exact inference becomes intractable as the
tree-width of the underlying graph increases. Approximate inference algorithms,
on the other hand, force one to trade off representational power with
computational efficiency. In this paper, we propose two new types of
probabilistic graphical models, large margin Boltzmann machines (LMBMs) and
large margin sigmoid belief networks (LMSBNs), for structured prediction.
LMSBNs in particular allow a very fast inference algorithm for arbitrary graph
structures that runs in polynomial time with a high probability. This
probability is data-distribution dependent and is maximized in learning. The
new approach overcomes the representation-efficiency trade-off in previous
models and allows fast structured prediction with complicated graph structures.
We present results from applying a fully connected model to multi-label scene
classification and demonstrate that the proposed approach can yield significant
performance gains over current state-of-the-art methods.


Learning Deep Generative Spatial Models for Mobile Robots

  We propose a new probabilistic framework that allows mobile robots to
autonomously learn deep, generative models of their environments that span
multiple levels of abstraction. Unlike traditional approaches that combine
engineered models for low-level features, geometry, and semantics, our approach
leverages recent advances in Sum-Product Networks (SPNs) and deep learning to
learn a single, universal model of the robot's spatial environment. Our model
is fully probabilistic and generative, and represents a joint distribution over
spatial information ranging from low-level geometry to semantic
interpretations. Once learned, it is capable of solving a wide range of tasks:
from semantic classification of places, uncertainty estimation, and novelty
detection, to generation of place appearances based on semantic information and
prediction of missing data in partial observations. Experiments on laser-range
data from a mobile robot show that the proposed universal model obtains
performance superior to state-of-the-art models fine-tuned to one specific
task, such as Generative Adversarial Networks (GANs) or SVMs.


Electrocorticographic Dynamics Predict Visually Guided Motor Imagery of
  Grasp Shaping

  Identification of intended movement type and movement phase of hand grasp
shaping are critical features for the control of volitional neuroprosthetics.
We demonstrate that neural dynamics during visually-guided imagined grasp
shaping can encode intended movement. We apply Procrustes analysis and LASSO
regression to achieve 72% accuracy (chance = 25%) in distinguishing between
visually-guided imagined grasp trajectories. Further, we can predict the stage
of grasp shaping in the form of elapsed time from start of trial (R2=0.4). Our
approach contributes to more accurate single-trial decoding of higher-level
movement goals and the phase of grasping movements in individuals not trained
with brain-computer interfaces. We also find that the overall time-varying
trajectory structure of imagined movements tend to be consistent within
individuals, and that transient trajectory deviations within trials return to
the task-dependent trajectory mean. These overall findings may contribute to
the further understanding of the cortical dynamics of human motor imagery.


Learning Graph-Structured Sum-Product Networks for Probabilistic
  Semantic Maps

  We introduce Graph-Structured Sum-Product Networks (GraphSPNs), a
probabilistic approach to structured prediction for problems where dependencies
between latent variables are expressed in terms of arbitrary, dynamic graphs.
While many approaches to structured prediction place strict constraints on the
interactions between inferred variables, many real-world problems can be only
characterized using complex graph structures of varying size, often
contaminated with noise when obtained from real data. Here, we focus on one
such problem in the domain of robotics. We demonstrate how GraphSPNs can be
used to bolster inference about semantic, conceptual place descriptions using
noisy topological relations discovered by a robot exploring large-scale office
spaces. Through experiments, we show that GraphSPNs consistently outperform the
traditional approach based on undirected graphical models, successfully
disambiguating information in global semantic maps built from uncertain, noisy
local evidence. We further exploit the probabilistic nature of the model to
infer marginal distributions over semantic descriptions of as yet unexplored
places and detect spatial environment configurations that are novel and
incongruent with the known evidence.


Transformational Sparse Coding

  A fundamental problem faced by object recognition systems is that objects and
their features can appear in different locations, scales and orientations.
Current deep learning methods attempt to achieve invariance to local
translations via pooling, discarding the locations of features in the process.
Other approaches explicitly learn transformed versions of the same feature,
leading to representations that quickly explode in size. Instead of discarding
the rich and useful information about feature transformations to achieve
invariance, we argue that models should learn object features conjointly with
their transformations to achieve equivariance. We propose a new model of
unsupervised learning based on sparse coding that can learn object features
jointly with their affine transformations directly from images. Results based
on learning from natural images indicate that our approach matches the
reconstruction quality of traditional sparse coding but with significantly
fewer degrees of freedom while simultaneously learning transformations from
data. These results open the door to scaling up unsupervised learning to allow
deep feature+transformation learning in a manner consistent with the
ventral+dorsal stream architecture of the primate visual cortex.


Statistical analysis of the Indus script using $n$-grams

  The Indus script is one of the major undeciphered scripts of the ancient
world. The small size of the corpus, the absence of bilingual texts, and the
lack of definite knowledge of the underlying language has frustrated efforts at
decipherment since the discovery of the remains of the Indus civilisation.
Recently, some researchers have questioned the premise that the Indus script
encodes spoken language. Building on previous statistical approaches, we apply
the tools of statistical language processing, specifically $n$-gram Markov
chains, to analyse the Indus script for syntax. Our main results are that the
script has well-defined signs which begin and end texts, that there is
directionality and strong correlations in the sign order, and that there are
groups of signs which appear to have identical syntactic function. All these
require no {\it a priori} suppositions regarding the syntactic or semantic
content of the signs, but follow directly from the statistical analysis. Using
information theoretic measures, we find the information in the script to be
intermediate between that of a completely random and a completely fixed
ordering of signs. Our study reveals that the Indus script is a structured sign
system showing features of a formal language, but, at present, cannot
conclusively establish that it encodes {\it natural} language. Our $n$-gram
Markov model is useful for predicting signs which are missing or illegible in a
corpus of Indus texts. This work forms the basis for the development of a
stochastic grammar which can be used to explore the syntax of the Indus script
in greater detail.


Unsupervised decoding of long-term, naturalistic human neural recordings
  with automated video and audio annotations

  Fully automated decoding of human activities and intentions from direct
neural recordings is a tantalizing challenge in brain-computer interfacing.
Most ongoing efforts have focused on training decoders on specific, stereotyped
tasks in laboratory settings. Implementing brain-computer interfaces (BCIs) in
natural settings requires adaptive strategies and scalable algorithms that
require minimal supervision. Here we propose an unsupervised approach to
decoding neural states from human brain recordings acquired in a naturalistic
context. We demonstrate our approach on continuous long-term
electrocorticographic (ECoG) data recorded over many days from the brain
surface of subjects in a hospital room, with simultaneous audio and video
recordings. We first discovered clusters in high-dimensional ECoG recordings
and then annotated coherent clusters using speech and movement labels extracted
automatically from audio and video recordings. To our knowledge, this
represents the first time techniques from computer vision and speech processing
have been used for natural ECoG decoding. Our results show that our
unsupervised approach can discover distinct behaviors from ECoG data, including
moving, speaking and resting. We verify the accuracy of our approach by
comparing to manual annotations. Projecting the discovered cluster centers back
onto the brain, this technique opens the door to automated functional brain
mapping in natural settings.


Multistep Model for Predicting Upper-Limb 3D Isometric Force Application
  from Pre-Movement Electrocorticographic Features

  Neural correlates of movement planning onset and direction may be present in
human electrocorticography in the signal dynamics of both motor and non-motor
cortical regions. We use a three-stage model of jPCA reduced-rank hidden Markov
model (jPCA-RR-HMM), regularized shrunken-centroid discriminant analysis (RDA),
and LASSO regression to extract direction-sensitive planning information and
movement onset in an upper-limb 3D isometric force task in a human subject.
This mode achieves a relatively high true positive force-onset prediction rate
of 60% within 250ms, and an above-chance 36% accuracy (17% chance) in
predicting one of six planned 3D directions of isometric force using
pre-movement signals. We also find direction-distinguishing information up to
400ms before force onset in the pre-movement signals, captured by electrodes
placed over the limb-ipsilateral dorsal premotor regions. This approach can
contribute to more accurate decoding of higher-level movement goals, at earlier
timescales, and inform sensor placement. Our results also contribute to further
understanding of the spatiotemporal features of human motor planning.


Interactive Web Application for Exploring Matrices of Neural
  Connectivity

  We present here a browser-based application for visualizing patterns of
connectivity in 3D stacked data matrices with large numbers of pairwise
relations. Visualizing a connectivity matrix, looking for trends and patterns,
and dynamically manipulating these values is a challenge for scientists from
diverse fields, including neuroscience and genomics. In particular,
high-dimensional neural data include those acquired via electroencephalography
(EEG), electrocorticography (ECoG), magnetoencephalography (MEG), and
functional MRI. Neural connectivity data contains multivariate attributes for
each edge between different brain regions, which motivated our lightweight,
open source, easy-to-use visualization tool for the exploration of these
connectivity matrices to highlight connections of interest. Here we present a
client-side, mobile-compatible visualization tool written entirely in
HTML5/JavaScript that allows in-browser manipulation of user-defined files for
exploration of brain connectivity. Visualizations can highlight different
aspects of the data simultaneously across different dimensions. Input files are
in JSON format, and custom Python scripts have been written to parse MATLAB or
Python data files into JSON-loadable format. We demonstrate the analysis of
connectivity data acquired via human ECoG recordings as a domain-specific
implementation of our application. We envision applications for this
interactive tool in fields seeking to visualize pairwise connectivity.


BrainNet: A Multi-Person Brain-to-Brain Interface for Direct
  Collaboration Between Brains

  We present BrainNet which, to our knowledge, is the first multi-person
non-invasive direct brain-to-brain interface for collaborative problem solving.
The interface combines electroencephalography (EEG) to record brain signals and
transcranial magnetic stimulation (TMS) to deliver information noninvasively to
the brain. The interface allows three human subjects to collaborate and solve a
task using direct brain-to-brain communication. Two of the three subjects are
"Senders" whose brain signals are decoded using real-time EEG data analysis to
extract decisions about whether to rotate a block in a Tetris-like game before
it is dropped to fill a line. The Senders' decisions are transmitted via the
Internet to the brain of a third subject, the "Receiver," who cannot see the
game screen. The decisions are delivered to the Receiver's brain via magnetic
stimulation of the occipital cortex. The Receiver integrates the information
received and makes a decision using an EEG interface about either turning the
block or keeping it in the same position. A second round of the game gives the
Senders one more chance to validate and provide feedback to the Receiver's
action. We evaluated the performance of BrainNet in terms of (1) Group-level
performance during the game; (2) True/False positive rates of subjects'
decisions; (3) Mutual information between subjects. Five groups of three
subjects successfully used BrainNet to perform the Tetris task, with an average
accuracy of 0.813. Furthermore, by varying the information reliability of the
Senders by artificially injecting noise into one Sender's signal, we found that
Receivers are able to learn which Sender is more reliable based solely on the
information transmitted to their brains. Our results raise the possibility of
future brain-to-brain interfaces that enable cooperative problem solving by
humans using a "social network" of connected brains.


