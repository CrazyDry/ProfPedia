Natural Language Inference from Multiple Premises

  We define a novel textual entailment task that requires inference overmultiple premise sentences. We present a new dataset for this task thatminimizes trivial lexical inferences, emphasizes knowledge of everyday events,and presents a more challenging setting for textual entailment. We evaluateseveral strong neural baselines and analyze how the multiple premise taskdiffers from standard textual entailment.

Reasoning about RoboCup Soccer Narratives

  This paper presents an approach for learning to translate simple narratives,i.e., texts (sequences of sentences) describing dynamic systems, into coherentsequences of events without the need for labeled training data. Our approachincorporates domain knowledge in the form of preconditions and effects ofevents, and we show that it outperforms state-of-the-art supervised learningsystems on the task of reconstructing RoboCup soccer games from theircommentaries.

Flickr30k Entities: Collecting Region-to-Phrase Correspondences for  Richer Image-to-Sentence Models

  The Flickr30k dataset has become a standard benchmark for sentence-basedimage description. This paper presents Flickr30k Entities, which augments the158k captions from Flickr30k with 244k coreference chains, linking mentions ofthe same entities across different captions for the same image, and associatingthem with 276k manually annotated bounding boxes. Such annotations areessential for continued progress in automatic image description and groundedlanguage understanding. They enable us to define a new benchmark forlocalization of textual entity mentions in an image. We present a strongbaseline for this task that combines an image-text embedding, detectors forcommon objects, a color classifier, and a bias towards selecting largerobjects. While our baseline rivals in accuracy more complex state-of-the-artmodels, we show that its gains cannot be easily parlayed into improvements onsuch tasks as image-sentence retrieval, thus underlining the limitations ofcurrent methods and the need for further research.

Evaluating Induced CCG Parsers on Grounded Semantic Parsing

  We compare the effectiveness of four different syntactic CCG parsers for asemantic slot-filling task to explore how much syntactic supervision isrequired for downstream semantic analysis. This extrinsic, task-basedevaluation provides a unique window to explore the strengths and weaknesses ofsemantics captured by unsupervised grammar induction systems. We release a newFreebase semantic parsing dataset called SPADES (Semantic PArsing ofDEclarative Sentences) containing 93K cloze-style questions paired withanswers. We evaluate all our models on this dataset. Our code and data areavailable at https://github.com/sivareddyg/graph-parser.

Phrase Localization and Visual Relationship Detection with Comprehensive  Image-Language Cues

  This paper presents a framework for localization or grounding of phrases inimages using a large collection of linguistic and visual cues. We model theappearance, size, and position of entity bounding boxes, adjectives thatcontain attribute information, and spatial relationships between pairs ofentities connected by verbs or prepositions. Special attention is given torelationships between people and clothing or body part mentions, as they areuseful for distinguishing individuals. We automatically learn weights forcombining these cues and at test time, perform joint inference over all phrasesin a caption. The resulting system produces state of the art performance onphrase localization on the Flickr30k Entities dataset and visual relationshipdetection on the Stanford VRD dataset.

