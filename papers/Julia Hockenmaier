Natural Language Inference from Multiple Premises

  We define a novel textual entailment task that requires inference over
multiple premise sentences. We present a new dataset for this task that
minimizes trivial lexical inferences, emphasizes knowledge of everyday events,
and presents a more challenging setting for textual entailment. We evaluate
several strong neural baselines and analyze how the multiple premise task
differs from standard textual entailment.


Reasoning about RoboCup Soccer Narratives

  This paper presents an approach for learning to translate simple narratives,
i.e., texts (sequences of sentences) describing dynamic systems, into coherent
sequences of events without the need for labeled training data. Our approach
incorporates domain knowledge in the form of preconditions and effects of
events, and we show that it outperforms state-of-the-art supervised learning
systems on the task of reconstructing RoboCup soccer games from their
commentaries.


Flickr30k Entities: Collecting Region-to-Phrase Correspondences for
  Richer Image-to-Sentence Models

  The Flickr30k dataset has become a standard benchmark for sentence-based
image description. This paper presents Flickr30k Entities, which augments the
158k captions from Flickr30k with 244k coreference chains, linking mentions of
the same entities across different captions for the same image, and associating
them with 276k manually annotated bounding boxes. Such annotations are
essential for continued progress in automatic image description and grounded
language understanding. They enable us to define a new benchmark for
localization of textual entity mentions in an image. We present a strong
baseline for this task that combines an image-text embedding, detectors for
common objects, a color classifier, and a bias towards selecting larger
objects. While our baseline rivals in accuracy more complex state-of-the-art
models, we show that its gains cannot be easily parlayed into improvements on
such tasks as image-sentence retrieval, thus underlining the limitations of
current methods and the need for further research.


Evaluating Induced CCG Parsers on Grounded Semantic Parsing

  We compare the effectiveness of four different syntactic CCG parsers for a
semantic slot-filling task to explore how much syntactic supervision is
required for downstream semantic analysis. This extrinsic, task-based
evaluation provides a unique window to explore the strengths and weaknesses of
semantics captured by unsupervised grammar induction systems. We release a new
Freebase semantic parsing dataset called SPADES (Semantic PArsing of
DEclarative Sentences) containing 93K cloze-style questions paired with
answers. We evaluate all our models on this dataset. Our code and data are
available at https://github.com/sivareddyg/graph-parser.


Phrase Localization and Visual Relationship Detection with Comprehensive
  Image-Language Cues

  This paper presents a framework for localization or grounding of phrases in
images using a large collection of linguistic and visual cues. We model the
appearance, size, and position of entity bounding boxes, adjectives that
contain attribute information, and spatial relationships between pairs of
entities connected by verbs or prepositions. Special attention is given to
relationships between people and clothing or body part mentions, as they are
useful for distinguishing individuals. We automatically learn weights for
combining these cues and at test time, perform joint inference over all phrases
in a caption. The resulting system produces state of the art performance on
phrase localization on the Flickr30k Entities dataset and visual relationship
detection on the Stanford VRD dataset.


