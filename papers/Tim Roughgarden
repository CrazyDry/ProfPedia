Communication Complexity (for Algorithm Designers)

  This document collects the lecture notes from my course "CommunicationComplexity (for Algorithm Designers),'' taught at Stanford in the winterquarter of 2015. The two primary goals of the course are: 1. Learn severalcanonical problems that have proved the most useful for proving lower bounds(Disjointness, Index, Gap-Hamming, etc.). 2. Learn how to reduce lower boundsfor fundamental algorithmic problems to communication complexity lower bounds.Along the way, we'll also: 3. Get exposure to lots of cool computational modelsand some famous results about them --- data streams and linear sketches,compressive sensing, space-query time trade-offs in data structures,sublinear-time algorithms, and the extension complexity of linear programs. 4.Scratch the surface of techniques for proving communication complexity lowerbounds (fooling sets, corruption bounds, etc.).

Beyond Worst-Case Analysis

  In the worst-case analysis of algorithms, the overall performance of analgorithm is summarized by its worst performance on any input. This approachhas countless success stories, but there are also important computationalproblems --- like linear programming, clustering, online caching, and neuralnetwork training --- where the worst-case analysis framework does not provideany helpful advice on how to solve the problem. This article covers a number ofmodeling methods for going beyond worst-case analysis and articulating whichinputs are the most relevant.

An approximately truthful-in-expectation mechanism for combinatorial  auctions using value queries

  This manuscript presents an alternative implementation of thetruthful-in-expectation mechanism of Dughmi, Roughgarden and Yan forcombinatorial auctions with weighted-matroid-rank-sum valuations. The newimplementation uses only value queries and is approximatelytruthful-in-expectation, in the sense that by reporting truthfully each agentmaximizes his utility within a multiplicative 1-o(1) factor. It still providesan optimal (1-1/e-o(1))-approximation in social welfare. We achieve this byfirst presenting an approximately maximal-in-distributional-range allocationrule and then showing a black-box transformation to an approximatelytruthful-in-expectation mechanism.

On the Computational Power of Online Gradient Descent

  We prove that the evolution of weight vectors in online gradient descent canencode arbitrary polynomial-space computations, even in very simple learningsettings. Our results imply that, under weak complexity-theoretic assumptions,it is impossible to reason efficiently about the fine-grained behavior ofonline gradient descent.

Optimal Mechansim Design and Money Burning

  Mechanism design is now a standard tool in computer science for aligning theincentives of self-interested agents with the objectives of a system designer.There is, however, a fundamental disconnect between the traditional applicationdomains of mechanism design (such as auctions) and those arising in computerscience (such as networks): while monetary transfers (i.e., payments) areessential for most of the known positive results in mechanism design, they areundesirable or even technologically infeasible in many computer systems.Classical impossibility results imply that the reach of mechanisms withouttransfers is severely limited.  Computer systems typically do have the ability to reduce servicequality--routing systems can drop or delay traffic, scheduling protocols candelay the release of jobs, and computational payment schemes can requirecomputational payments from users (e.g., in spam-fighting systems). Servicedegradation is tantamount to requiring that users burn money}, and such``payments'' can be used to influence the preferences of the agents at a costof degrading the social surplus.  We develop a framework for the design and analysis of money-burningmechanisms to maximize the residual surplus--the total value of the chosenoutcome minus the payments required.

Optimal Platform Design

  An auction house cannot generally provide the optimal auction technology toevery client. Instead it provides one or several auction technologies, andclients select the most appropriate one. For example, eBay provides ascendingauctions and "buy-it-now" pricing. For each client the offered technology maynot be optimal, but it would be too costly for clients to create their own. Wecall these mechanisms, which emphasize generality rather than optimality,platform mechanisms. A platform mechanism will be adopted by a client if itsperformance exceeds that of the client's outside option, e.g., hiring (at acost) a consultant to design the optimal mechanism. We ask two relatedquestions. First, for what costs of the outside option will the platform beuniversally adopted? Second, what is the structure of good platform mechanisms?We answer these questions using a novel prior-free analysis framework in whichwe seek mechanisms that are approximately optimal for every prior.

Complexity Theory, Game Theory, and Economics

  This document collects the lecture notes from my mini-course "ComplexityTheory, Game Theory, and Economics," taught at the Bellairs Research Instituteof McGill University, Holetown, Barbados, February 19--23, 2017, as the 29thMcGill Invitational Workshop on Computational Complexity.  The goal of this mini-course is twofold: (i) to explain how complexity theoryhas helped illuminate several barriers in economics and game theory; and (ii)to illustrate how game-theoretic questions have led to new and interestingcomplexity theory, including recent several breakthroughs. It consists of twofive-lecture sequences: the Solar Lectures, focusing on the communication andcomputational complexity of computing equilibria; and the Lunar Lectures,focusing on applications of complexity theory in game theory and economics. Nobackground in game theory is assumed.

Approximately Optimal Mechanism Design

  Optimal mechanism design enjoys a beautiful and well-developed theory, andalso a number of killer applications. Rules of thumb produced by the fieldinfluence everything from how governments sell wireless spectrum licenses tohow the major search engines auction off online advertising. There are,however, some basic problems for which the traditional optimal mechanism designapproach is ill-suited---either because it makes overly strong assumptions, orbecause it advocates overly complex designs. This survey reviews several commonissues with optimal mechanisms, including exorbitant communication,computation, and informational requirements; and it presents several examplesdemonstrating that passing to the relaxed goal of an approximately optimalmechanism allows us to reason about fundamental questions that seem out ofreach of the traditional theory.

Approximately Efficient Cost-Sharing Mechanisms

  We make three different types of contributions to cost-sharing: First, weidentify several new classes of combinatorial cost functions that admitincentive-compatible mechanisms achieving both a constant-factor approximationof budget-balance and a polylogarithmic approximation of the social costformulation of efficiency. Second, we prove a new, optimal lower bound on theapproximate efficiency of every budget-balanced Moulin mechanism for Steinertree or SSRoB cost functions. This lower bound exposes a latent approximationhierarchy among different cost-sharing problems. Third, we show that weakeningthe definition of incentive-compatibility to strategyproofness can permitexponentially more efficient approximately budget-balanced mechanisms, inparticular for set cover cost-sharing problems.

Near-Optimal Multi-Unit Auctions with Ordered Bidders

  We construct prior-free auctions with constant-factor approximationguarantees with ordered bidders, in both unlimited and limited supply settings.We compare the expected revenue of our auctions on a bid vector to the monotoneprice benchmark, the maximum revenue that can be obtained from a bid vectorusing supply-respecting prices that are nonincreasing in the bidder orderingand bounded above by the second-highest bid. As a consequence, our auctions aresimultaneously near-optimal in a wide range of Bayesian multi-unitenvironments.

Privately Solving Linear Programs

  In this paper, we initiate the systematic study of solving linear programsunder differential privacy. The first step is simply to define the problem: tothis end, we introduce several natural classes of private linear programs thatcapture different ways sensitive data can be incorporated into a linearprogram. For each class of linear programs we give an efficient, differentiallyprivate solver based on the multiplicative weights framework, or we give animpossibility result.

The Price of Anarchy in Large Games

  Game-theoretic models relevant for computer science applications usuallyfeature a large number of players. The goal of this paper is to develop ananalytical framework for bounding the price of anarchy in such models. Wedemonstrate the wide applicability of our framework through instantiations forseveral well-studied models, including simultaneous single-item auctions,greedy combinatorial auctions, and routing games. In all cases, we identifyconditions under which the POA of large games is much better than that ofworst-case instances. Our results also give new senses in which simple auctionscan perform almost as well as optimal ones in realistic settings.

Public projects, Boolean functions and the borders of Border's theorem

  Border's theorem gives an intuitive linear characterization of the feasibleinterim allocation rules of a Bayesian single-item environment, and it hasseveral applications in economic and algorithmic mechanism design. All knowngeneralizations of Border's theorem either restrict attention to relativelysimple settings, or resort to approximation. This paper identifies acomplexity-theoretic barrier that indicates, assuming standard complexity classseparations, that Border's theorem cannot be extended significantly beyond thestate-of-the-art. We also identify a surprisingly tight connection betweenMyerson's optimal auction theory, when applied to public project settings, andsome fundamental results in the analysis of Boolean functions.

Ironing in the Dark

  This paper presents the first polynomial-time algorithm for position andmatroid auction environments that learns, from samples from an unknown boundedvaluation distribution, an auction with expected revenue arbitrarily close tothe maximum possible. In contrast to most previous work, our results apply toarbitrary (not necessarily regular) distributions and the strongest possiblebenchmark, the Myerson-optimal auction. Learning a near-optimal auction for anirregular distribution is technically challenging because it requires learningthe appropriate "ironed intervals," a delicate global property of thedistribution.

Pricing Multi-Unit Markets

  We study the power and limitations of posted prices in multi-unit markets,where agents arrive sequentially in an arbitrary order. We prove upper andlower bounds on the largest fraction of the optimal social welfare that can beguaranteed with posted prices, under a range of assumptions about thedesigner's information and agents' valuations. Our results provide insightsabout the relative power of uniform and non-uniform prices, the relativedifficulty of different valuation classes, and the implications of differentinformational assumptions. Among other results, we prove constant-factorguarantees for agents with (symmetric) subadditive valuations, even in anincomplete-information setting and with uniform prices.

Communication Complexity of Discrete Fair Division

  We initiate the study of the communication complexity of fair division withindivisible goods. We focus on some of the most well-studied fairness notions(envy-freeness, proportionality, and approximations thereof) and valuationclasses (submodular, subadditive and unrestricted). Within these parameters,our results completely resolve whether the communication complexity ofcomputing a fair allocation (or determining that none exist) is polynomial orexponential (in the number of goods), for every combination of fairness notion,valuation class, and number of players, for both deterministic and randomizedprotocols.

Approximately Optimal Mechanism Design: Motivation, Examples, and  Lessons Learned

  Optimal mechanism design enjoys a beautiful and well-developed theory, andalso a number of killer applications. Rules of thumb produced by the fieldinfluence everything from how governments sell wireless spectrum licenses tohow the major search engines auction off online advertising. There are,however, some basic problems for which the traditional optimal mechanism designapproach is ill-suited --- either because it makes overly strong assumptions,or because it advocates overly complex designs. The thesis of this paper isthat approximately optimal mechanisms allow us to reason about fundamentalquestions that seem out of reach of the traditional theory.  This survey has three main parts. The first part describes the approximatelyoptimal mechanism design paradigm --- how it works, and what we aim to learn byapplying it. The second and third parts of the survey cover two case studies,where we instantiate the general design paradigm to investigate two basicquestions. In the first example, we consider revenue maximization in asingle-item auction with heterogeneous bidders. Our goal is to understand ifcomplexity --- in the sense of detailed distributional knowledge --- is anessential feature of good auctions for this problem, or alternatively if thereare simpler auctions that are near-optimal. The second example considerswelfare maximization with multiple items. Our goal here is similar in spirit:when is complexity --- in the form of high-dimensional bid spaces --- anessential feature of every auction that guarantees reasonable welfare? Arethere interesting cases where low-dimensional bid spaces suffice?

Tight Error Bounds for Structured Prediction

  Structured prediction tasks in machine learning involve the simultaneousprediction of multiple labels. This is typically done by maximizing a scorefunction on the space of labels, which decomposes as a sum of pairwiseelements, each depending on two specific labels. Intuitively, the more pairwiseterms are used, the better the expected accuracy. However, there is currentlyno theoretical account of this intuition. This paper takes a significant stepin this direction.  We formulate the problem as classifying the vertices of a known graph$G=(V,E)$, where the vertices and edges of the graph are labelled and correlatesemi-randomly with the ground truth. We show that the prospects for achievinglow expected Hamming error depend on the structure of the graph $G$ ininteresting ways. For example, if $G$ is a very poor expander, like a path,then large expected Hamming error is inevitable. Our main positive result showsthat, for a wide class of graphs including 2D grid graphs common in machinevision applications, there is a polynomial-time algorithm with small andinformation-theoretically near-optimal expected error. Our results provide afirst step toward a theoretical justification for the empirical success of theefficient approximate inference algorithms that are used for structuredprediction in models where exact inference is intractable.

Interactive Privacy via the Median Mechanism

  We define a new interactive differentially private mechanism -- the medianmechanism -- for answering arbitrary predicate queries that arrive online.Relative to fixed accuracy and privacy constraints, this mechanism can answerexponentially more queries than the previously best known interactive privacymechanism (the Laplace mechanism, which independently perturbs each queryresult). Our guarantee is almost the best possible, even for non-interactiveprivacy mechanisms. Conceptually, the median mechanism is the first privacymechanism capable of identifying and exploiting correlations among queries inan interactive setting.  We also give an efficient implementation of the median mechanism, withrunning time polynomial in the number of queries, the database size, and thedomain size. This efficient implementation guarantees privacy for all inputdatabases, and accurate query results for almost all input databases. Thedependence of the privacy on the number of queries in this mechanism improvesover that of the best previously known efficient mechanism by asuper-polynomial factor, even in the non-interactive setting.

Decompositions of Triangle-Dense Graphs

  High triangle density -- the graph property stating that a constant fractionof two-hop paths belong to a triangle -- is a common signature of socialnetworks. This paper studies triangle-dense graphs from a structuralperspective. We prove constructively that significant portions of atriangle-dense graph are contained in a disjoint union of dense, radius 2subgraphs. This result quantifies the extent to which triangle-dense graphsresemble unions of cliques. We also show that our algorithm recovers plantedclusterings in approximation-stable k-median instances.

The Pseudo-Dimension of Near-Optimal Auctions

  This paper develops a general approach, rooted in statistical learningtheory, to learning an approximately revenue-maximizing auction from data. Weintroduce $t$-level auctions to interpolate between simple auctions, such aswelfare maximization with reserve prices, and optimal auctions, therebybalancing the competing demands of expressivity and simplicity. We prove thatsuch auctions have small representation error, in the sense that for everyproduct distribution $F$ over bidders' valuations, there exists a $t$-levelauction with small $t$ and expected revenue close to optimal. We show that theset of $t$-level auctions has modest pseudo-dimension (for polynomial $t$) andtherefore leads to small learning error. One consequence of our results isthat, in arbitrary single-parameter settings, one can learn a mechanism withexpected revenue arbitrarily close to optimal from a polynomial number ofsamples.

A PAC Approach to Application-Specific Algorithm Selection

  The best algorithm for a computational problem generally depends on the"relevant inputs," a concept that depends on the application domain and oftendefies formal articulation. While there is a large literature on empiricalapproaches to selecting the best algorithm for a given application domain,there has been surprisingly little theoretical analysis of the problem.  This paper adapts concepts from statistical and online learning theory toreason about application-specific algorithm selection. Our models captureseveral state-of-the-art empirical and theoretical approaches to the problem,ranging from self-improving algorithms to empirical performance models, and ourresults identify conditions under which these approaches are guaranteed toperform well. We present one framework that models algorithm selection as astatistical learning problem, and our work here shows that dimension notionsfrom statistical learning theory, historically used to measure the complexityof classes of binary- and real-valued functions, are relevant in a much broaderalgorithmic context. We also study the online version of the algorithmselection problem, and give possibility and impossibility results for theexistence of no-regret learning algorithms.

Learning Simple Auctions

  We present a general framework for proving polynomial sample complexitybounds for the problem of learning from samples the best auction in a class of"simple" auctions. Our framework captures all of the most prominent examples of"simple" auctions, including anonymous and non-anonymous item and bundlepricings, with either a single or multiple buyers. The technique we propose isto break the analysis of auctions into two natural pieces. First, one showsthat the set of allocation rules have large amounts of structure; second,fixing an allocation on a sample, one shows that the set of auctions agreeingwith this allocation on that sample have revenue functions with lowdimensionality. Our results effectively imply that whenever it's possible tocompute a near-optimal simple auction with a known prior, it is also possibleto compute such an auction with an unknown prior (given a polynomial number ofsamples).

The Price of Anarchy in Auctions

  This survey outlines a general and modular theory for proving approximationguarantees for equilibria of auctions in complex settings. This theorycomplements traditional economic techniques, which generally focus on exact andoptimal solutions and are accordingly limited to relatively stylized settings.  We highlight three user-friendly analytical tools: smoothness-typeinequalities, which immediately yield approximation guarantees for many auctionformats of interest in the special case of complete information anddeterministic strategies; extension theorems, which extend such guarantees torandomized strategies, no-regret learning outcomes, and incomplete-informationsettings; and composition theorems, which extend such guarantees from simplerto more complex auctions. Combining these tools yields tight worst-caseapproximation guarantees for the equilibria of many widely-used auctionformats.

When Are Welfare Guarantees Robust?

  Computational and economic results suggest that social welfare maximizationand combinatorial auction design are much easier when bidders' valuationssatisfy the "gross substitutes" condition. The goal of this paper is toevaluate rigorously the folklore belief that the main take-aways from theseresults remain valid in settings where the gross substitutes condition holdsonly approximately. We show that for valuations that pointwise approximate agross substitutes valuation (in fact even a linear valuation), optimal socialwelfare cannot be approximated to within a subpolynomial factor and demandoracles cannot be simulated using a subexponential number of value queries. Wethen provide several positive results by imposing additional structure on thevaluations (beyond gross substitutes), using a more stringent notion ofapproximation, and/or using more powerful oracle access to the valuations. Forexample, we prove that the performance of the greedy algorithm degradesgracefully for near-linear valuations with approximately decreasing marginalvalues, that with demand queries, approximate welfare guarantees for XOSvaluations degrade gracefully for valuations that are pointwise close to XOS,and that the performance of the Kelso-Crawford auction degrades gracefully forvaluations that are close to various subclasses of gross substitutesvaluations.

Online Prediction with Selfish Experts

  We consider the problem of binary prediction with expert advice in settingswhere experts have agency and seek to maximize their credibility. This papermakes three main contributions. First, it defines a model to reason formallyabout settings with selfish experts, and demonstrates that "incentivecompatible" (IC) algorithms are closely related to the design of proper scoringrules. Designing a good IC algorithm is easy if the designer's loss function isquadratic, but for other loss functions, novel techniques are required. Second,we design IC algorithms with good performance guarantees for the absolute lossfunction. Third, we give a formal separation between the power of onlineprediction with selfish experts and online prediction with honest experts byproving lower bounds for both IC and non-IC algorithms. In particular, withselfish experts and the absolute loss function, there is no (randomized)algorithm for online prediction-IC or otherwise-with asymptotically vanishingregret.

Stability and Recovery for Independence Systems

  Two genres of heuristics that are frequently reported to perform much betteron "real-world" instances than in the worst case are greedy algorithms andlocal search algorithms. In this paper, we systematically study these two typesof algorithms for the problem of maximizing a monotone submodular set functionsubject to downward-closed feasibility constraints. We considerperturbation-stable instances, in the sense of Bilu and Linial, and preciselyidentify the stability threshold beyond which these algorithms are guaranteedto recover the optimal solution. Byproducts of our work include the firstdefinition of perturbation-stability for non-additive objective functions, anda resolution of the worst-case approximation guarantee of local search inp-extendible systems.

Finding Cliques in Social Networks: A New Distribution-Free Model

  We propose a new distribution-free model of social networks. Our definitionsare motivated by one of the most universal signatures of social networks,triadic closure---the property that pairs of vertices with common neighborstend to be adjacent. Our most basic definition is that of a "$c$-closed" graph,where for every pair of vertices $u,v$ with at least $c$ common neighbors, $u$and $v$ are adjacent. We study the classic problem of enumerating all maximalcliques, an important task in social network analysis. We prove that thisproblem is fixed-parameter tractable with respect to $c$ on $c$-closed graphs.Our results carry over to "weakly $c$-closed graphs", which only require avertex deletion ordering that avoids pairs of non-adjacent vertices with $c$common neighbors. Numerical experiments show that well-studied social networkstend to be weakly $c$-closed for modest values of $c$.

Simple versus Optimal Contracts

  We consider the classic principal-agent model of contract theory, in which aprincipal designs an outcome-dependent compensation scheme to incentivize anagent to take a costly and unobservable action. When all of the modelparameters---including the full distribution over principal rewards resultingfrom each agent action---are known to the designer, an optimal contract can inprinciple be computed by linear programming. In addition to their demandinginformational requirements, such optimal contracts are often complex andunintuitive, and do not resemble contracts used in practice.  This paper examines contract theory through the theoretical computer sciencelens, with the goal of developing novel theory to explain and justify theprevalence of relatively simple contracts, such as linear (pure commission)contracts. First, we consider the case where the principal knows only the firstmoment of each action's reward distribution, and we prove that linear contractsare guaranteed to be worst-case optimal, ranging over all reward distributionsconsistent with the given moments. Second, we study linear contracts from aworst-case approximation perspective, and prove several tight parameterizedapproximation bounds.

Universally Utility-Maximizing Privacy Mechanisms

  A mechanism for releasing information about a statistical database withsensitive data must resolve a trade-off between utility and privacy. Privacycan be rigorously quantified using the framework of {\em differential privacy},which requires that a mechanism's output distribution is nearly the samewhether or not a given database row is included or excluded. The goal of thispaper is strong and general utility guarantees, subject to differentialprivacy.  We pursue mechanisms that guarantee near-optimal utility to every potentialuser, independent of its side information (modeled as a prior distribution overquery results) and preferences (modeled via a loss function).  Our main result is: for each fixed count query and differential privacylevel, there is a {\em geometric mechanism} $M^*$ -- a discrete variant of thesimple and well-studied Laplace mechanism -- that is {\em simultaneouslyexpected loss-minimizing} for every possible user, subject to the differentialprivacy constraint. This is an extremely strong utility guarantee: {\em every}potential user $u$, no matter what its side information and preferences,derives as much utility from $M^*$ as from interacting with a differentiallyprivate mechanism $M_u$ that is optimally tailored to $u$.

From Convex Optimization to Randomized Mechanisms: Toward Optimal  Combinatorial Auctions

  We design an expected polynomial-time, truthful-in-expectation,(1-1/e)-approximation mechanism for welfare maximization in a fundamental classof combinatorial auctions. Our results apply to bidders with valuations thatare m matroid rank sums (MRS), which encompass most concrete examples ofsubmodular functions studied in this context, including coverage functions,matroid weighted-rank functions, and convex combinations thereof. Ourapproximation factor is the best possible, even for known and explicitly givencoverage valuations, assuming P != NP. Ours is the firsttruthful-in-expectation and polynomial-time mechanism to achieve aconstant-factor approximation for an NP-hard welfare maximization problem incombinatorial auctions with heterogeneous goods and restricted valuations.  Our mechanism is an instantiation of a new framework for designingapproximation mechanisms based on randomized rounding algorithms. A typicalsuch algorithm first optimizes over a fractional relaxation of the originalproblem, and then randomly rounds the fractional solution to an integral one.With rare exceptions, such algorithms cannot be converted into truthfulmechanisms. The high-level idea of our mechanism design framework is tooptimize directly over the (random) output of the rounding algorithm, ratherthan over the input to the rounding algorithm. This approach leads totruthful-in-expectation mechanisms, and these mechanisms can be implementedefficiently when the corresponding objective function is concave. For bidderswith MRS valuations, we give a novel randomized rounding algorithm that leadsto both a concave objective function and a (1-1/e)-approximation of the optimalwelfare.

Making the Most of Your Samples

  We study the problem of setting a price for a potential buyer with avaluation drawn from an unknown distribution $D$. The seller has "data"' about$D$ in the form of $m \ge 1$ i.i.d. samples, and the algorithmic challenge isto use these samples to obtain expected revenue as close as possible to whatcould be achieved with advance knowledge of $D$.  Our first set of results quantifies the number of samples $m$ that arenecessary and sufficient to obtain a $(1-\epsilon)$-approximation. For example,for an unknown distribution that satisfies the monotone hazard rate (MHR)condition, we prove that $\tilde{\Theta}(\epsilon^{-3/2})$ samples arenecessary and sufficient. Remarkably, this is fewer samples than is necessaryto accurately estimate the expected revenue obtained by even a single reserveprice. We also prove essentially tight sample complexity bounds for regulardistributions, bounded-support distributions, and a wide class of irregulardistributions. Our lower bound approach borrows tools from differential privacyand information theory, and we believe it could find further applications inauction theory.  Our second set of results considers the single-sample case. For regulardistributions, we prove that no pricing strategy is better than$\tfrac{1}{2}$-approximate, and this is optimal by the Bulow-Klemperer theorem.For MHR distributions, we show how to do better: we give a simple pricingstrategy that guarantees expected revenue at least $0.589$ times the maximumpossible. We also prove that no pricing strategy achieves an approximationguarantee better than $\frac{e}{4} \approx .68$.

Almost Envy-Freeness with General Valuations

  The goal of fair division is to distribute resources among competing playersin a "fair" way. Envy-freeness is the most extensively studied fairness notionin fair division. Envy-free allocations do not always exist with indivisiblegoods, motivating the study of relaxed versions of envy-freeness. We study theenvy-freeness up to any good (EFX) property, which states that no playerprefers the bundle of another player following the removal of any single good,and prove the first general results about this property. We use the leximinsolution to show existence of EFX allocations in several contexts, sometimes inconjunction with Pareto optimality. For two players with valuations obeying amild assumption, one of these results provides stronger guarantees than thecurrently deployed algorithm on Spliddit, a popular fair division website.Unfortunately, finding the leximin solution can require exponential time. Weshow that this is necessary by proving an exponential lower bound on the numberof value queries needed to identify an EFX allocation, even for two playerswith identical valuations. We consider both additive and more generalvaluations, and our work suggests that there is a rich landscape of problems toexplore in the fair division of indivisible goods with different classes ofplayer valuations.

Optimal Algorithms for Continuous Non-monotone Submodular and  DR-Submodular Maximization

  In this paper we study the fundamental problems of maximizing a continuousnon-monotone submodular function over the hypercube, both with and withoutcoordinate-wise concavity. This family of optimization problems has severalapplications in machine learning, economics, and communication systems. Ourmain result is the first $\frac{1}{2}$-approximation algorithm for continuoussubmodular function maximization; this approximation factor of $\frac{1}{2}$ isthe best possible for algorithms that only query the objective function atpolynomially many points. For the special case of DR-submodular maximization,i.e. when the submodular functions is also coordinate wise concave along allcoordinates, we provide a different $\frac{1}{2}$-approximation algorithm thatruns in quasilinear time. Both of these results improve upon prior work [Bianet al, 2017, Soma and Yoshida, 2017].  Our first algorithm uses novel ideas such as reducing the guaranteedapproximation problem to analyzing a zero-sum game for each coordinate, andincorporates the geometry of this zero-sum game to fix the value at thiscoordinate. Our second algorithm exploits coordinate-wise concavity to identifya monotone equilibrium condition sufficient for getting the requiredapproximation guarantee, and hunts for the equilibrium point using binarysearch. We further run experiments to verify the performance of our proposedalgorithms in related machine learning applications.

An Optimal Algorithm for Online Unconstrained Submodular Maximization

  We consider a basic problem at the interface of two fundamental fields:submodular optimization and online learning. In the online unconstrainedsubmodular maximization (online USM) problem, there is a universe$[n]=\{1,2,...,n\}$ and a sequence of $T$ nonnegative (not necessarilymonotone) submodular functions arrive over time. The goal is to design acomputationally efficient online algorithm, which chooses a subset of $[n]$ ateach time step as a function only of the past, such that the accumulated valueof the chosen subsets is as close as possible to the maximum total value of afixed subset in hindsight. Our main result is a polynomial-time no-$1/2$-regretalgorithm for this problem, meaning that for every sequence of nonnegativesubmodular functions, the algorithm's expected total value is at least $1/2$times that of the best subset in hindsight, up to an error term sublinear in$T$. The factor of $1/2$ cannot be improved upon by any polynomial-time onlinealgorithm when the submodular functions are presented as value oracles.Previous work on the offline problem implies that picking a subset uniformly atrandom in each time step achieves zero $1/4$-regret.  A byproduct of our techniques is an explicit subroutine for the two-expertsproblem that has an unusually strong regret guarantee: the total value of itschoices is comparable to twice the total value of either expert on rounds itdid not pick that expert. This subroutine may be of independent interest.

Combinatorial Auctions with Restricted Complements

  Complements between goods - where one good takes on added value in thepresence of another - have been a thorn in the side of algorithmic mechanismdesigners. On the one hand, complements are common in the standard motivatingapplications for combinatorial auctions, like spectrum license auctions. On theother, welfare maximization in the presence of complements is notoriouslydifficult, and this intractability has stymied theoretical progress in thearea. For example, there are no known positive results for combinatorialauctions in which bidder valuations are multi-parameter andnon-complement-free, other than the relatively weak results known for generalvaluations.  To make inroads on the problem of combinatorial auction design in thepresence of complements, we propose a model for valuations with complementsthat is parameterized by the "size" of the complements. A valuation in ourmodel is represented succinctly by a weighted hypergraph, where the size of thehyper-edges corresponds to degree of complementarity. Our model permits avariety of computationally efficient queries, and non-trivialwelfare-maximization algorithms and mechanisms.  We design the following polynomial-time approximation algorithms and truthfulmechanisms for welfare maximization with bidders with hypergraph valuations.  1- For bidders whose valuations correspond to subgraphs of a known graph thatis planar (or more generally, excludes a fixed minor), we give a truthful and(1+epsilon)-approximate mechanism.  2- We give a polynomial-time, r-approximation algorithm for welfaremaximization with hypergraph-r valuations. Our algorithm randomly rounds acompact linear programming relaxation of the problem.  3- We design a different approximation algorithm and use it to give apolynomial-time, truthful-in-expectation mechanism that has an approximationfactor of O(log^r m).

Private Matchings and Allocations

  We consider a private variant of the classical allocation problem: given kgoods and n agents with individual, private valuation functions over bundles ofgoods, how can we partition the goods amongst the agents to maximize socialwelfare? An important special case is when each agent desires at most one good,and specifies her (private) value for each good: in this case, the problem isexactly the maximum-weight matching problem in a bipartite graph.  Private matching and allocation problems have not been considered in thedifferential privacy literature, and for good reason: they are plainlyimpossible to solve under differential privacy. Informally, the allocation mustmatch agents to their preferred goods in order to maximize social welfare, butthis preference is exactly what agents wish to hide. Therefore, we consider theproblem under the relaxed constraint of joint differential privacy: for anyagent i, no coalition of agents excluding i should be able to learn about thevaluation function of agent i. In this setting, the full allocation is nolonger published---instead, each agent is told what good to get. We first showthat with a small number of identical copies of each good, it is possible toefficiently and accurately solve the maximum weight matching problem whileguaranteeing joint differential privacy. We then consider the more generalallocation problem, when bidder valuations satisfy the gross substitutescondition. Finally, we prove that the allocation problem cannot be solved tonon-trivial accuracy under joint differential privacy without requiringmultiple copies of each type of good.

The Sample Complexity of Revenue Maximization

  In the design and analysis of revenue-maximizing auctions, auctionperformance is typically measured with respect to a prior distribution overinputs. The most obvious source for such a distribution is past data. The goalis to understand how much data is necessary and sufficient to guaranteenear-optimal expected revenue.  Our basic model is a single-item auction in which bidders' valuations aredrawn independently from unknown and non-identical distributions. The seller isgiven $m$ samples from each of these distributions "for free" and chooses anauction to run on a fresh sample. How large does m need to be, as a function ofthe number k of bidders and eps > 0, so that a (1 - eps)-approximation of theoptimal revenue is achievable?  We prove that, under standard tail conditions on the underlyingdistributions, m = poly(k, 1/eps) samples are necessary and sufficient. Ourlower bound stands in contrast to many recent results on simple andprior-independent auctions and fundamentally involves the interplay betweenbidder competition, non-identical distributions, and a very close (but stillconstant) approximation of the optimal revenue. It effectively shows that theonly way to achieve a sufficiently good constant approximation of the optimalrevenue is through a detailed understanding of bidders' valuationdistributions. Our upper bound is constructive and applies in particular to avariant of the empirical Myerson auction, the natural auction that runs therevenue-maximizing auction with respect to the empirical distributions of thesamples.  Our sample complexity lower bound depends on the set of allowabledistributions, and to capture this we introduce alpha-strongly regulardistributions, which interpolate between the well-studied classes of regular(alpha = 0) and MHR (alpha = 1) distributions. We give evidence that thisdefinition is of independent interest.

Approximately Efficient Two-Sided Combinatorial Auctions

  Mechanism design for one-sided markets has been investigated for severaldecades in economics and in computer science. More recently, there has been anincreased attention on mechanisms for two-sided markets, in which buyers andsellers act strategically. For two-sided markets, an impossibility result ofMyerson and Satterthwaite states that no mechanism can simultaneously satisfyindividual rationality (IR), incentive compatibility (IC), strongbudget-balance (SBB), and be efficient. On the other hand, importantapplications to web advertisement, stock exchange, and frequency spectrumallocation, require us to consider two-sided combinatorial auctions in whichbuyers have preferences on subsets of items, and sellers may offer multipleheterogeneous items. No efficient mechanism was known so far for such two-sidedcombinatorial markets. This work provides the first IR, IC and SBB mechanismsthat provides an O(1)-approximation to the optimal social welfare for two-sidedmarkets. An initial construction yields such a mechanism, but exposes aconceptual problem in the traditional SBB notion. This leads us to define thestronger notion of direct trade strong budget balance (DSBB). We then proceedto design mechanisms that are IR, IC, DSBB, and again provide anO(1)-approximation to the optimal social welfare. Our mechanisms work for anynumber of buyers with XOS valuations - a class in between submodular andsubadditive functions - and any number of sellers. We provide a mechanism thatis dominant strategy incentive compatible (DSIC) if the sellers each have oneitem for sale, and one that is bayesian incentive compatible (BIC) if sellershold multiple items and have additive valuations over them. Finally, we presenta DSIC mechanism for the case that the valuation functions of all buyers andsellers are additive.

The idemetric property: when most distances are (almost) the same

  We introduce the \emph{idemetric} property, which formalises the idea thatmost nodes in a graph have similar distances between them, and which turns outto be quite standard amongst small-world network models. Modulo reasonablesparsity assumptions, we are then able to show that a strong form ofidemetricity is actually equivalent to a very weak expander condition (PUMP).This provides a direct way of providing short proofs that small-world networkmodels such as the Watts-Strogatz model are strongly idemetric (for a widerange of parameters), and also provides further evidence that being idemetricis a common property.  We then consider how satisfaction of the idemetric property is relevant toalgorithm design. For idemetric graphs we observe, for example, that a singlebreadth-first search provides a solution to the all-pairs shortest pathsproblem, so long as one is prepared to accept paths which are of stretch closeto 2 with high probability. Since we are able to show that Kleinberg's model isidemetric, these results contrast nicely with the well known negative resultsof Kleinberg concerning efficient decentralised algorithms for finding shortpaths: for precisely the same model as Kleinberg's negative results hold, weare able to show that very efficient (and decentralised) algorithms exist ifone allows for reasonable preprocessing. For deterministic distributed routingalgorithms we are also able to obtain results proving that less routinginformation is required for idemetric graphs than the worst case in order toachieve stretch less than 3 with high probability: while $\Omega(n^2)$ routinginformation is required in the worst case for stretch strictly less than 3 onalmost all pairs, for idemetric graphs the total routing information requiredis $O(nlog(n))$.

