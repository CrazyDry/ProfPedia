Rating Effects on Social News Posts and Comments

  At a time when information seekers first turn to digital sources for news and
opinion, it is critical that we understand the role that social media plays in
human behavior. This is especially true when information consumers also act as
information producers and editors through their online activity. In order to
better understand the effects that editorial ratings have on online human
behavior, we report the results of a two large-scale in-vivo experiments in
social media. We find that small, random rating manipulations on social media
posts and comments created significant changes in downstream ratings resulting
in significantly different final outcomes. We found positive herding effects
for positive treatments on posts, increasing the final rating by 11.02% on
average, but not for positive treatments on comments. Contrary to the results
of related work, we found negative herding effects for negative treatments on
posts and comments, decreasing the final ratings on average, of posts by 5.15%
and of comments by 37.4%. Compared to the control group, the probability of
reaching a high rating (>=2000) for posts is increased by 24.6% when posts
receive the positive treatment and for comments is decreased by 46.6% when
comments receive the negative treatment.


Scalable Models for Computing Hierarchies in Information Networks

  Information hierarchies are organizational structures that often used to
organize and present large and complex information as well as provide a
mechanism for effective human navigation. Fortunately, many statistical and
computational models exist that automatically generate hierarchies; however,
the existing approaches do not consider linkages in information {\em networks}
that are increasingly common in real-world scenarios. Current approaches also
tend to present topics as an abstract probably distribution over words, etc
rather than as tangible nodes from the original network. Furthermore, the
statistical techniques present in many previous works are not yet capable of
processing data at Web-scale. In this paper we present the Hierarchical
Document Topic Model (HDTM), which uses a distributed vertex-programming
process to calculate a nonparametric Bayesian generative model. Experiments on
three medium size data sets and the entire Wikipedia dataset show that HDTM can
infer accurate hierarchies even over large information networks.


Random Voting Effects in Social-Digital Spaces: A case study of Reddit
  Post Submissions

  At a time when information seekers first turn to digital sources for news and
opinion, it is critical that we understand the role that social media plays in
human behavior. This is especially true when information consumers also act as
information producers and editors by their online activity. In order to better
understand the effects that editorial ratings have on online human behavior, we
report the results of a large-scale in-vivo experiment in social media. We find
that small, random rating manipulations on social media submissions created
significant changes in downstream ratings resulting in significantly different
final outcomes. Positive treatment resulted in a positive effect that increased
the final rating by 11.02% on average. Compared to the control group, positive
treatment also increased the probability of reaching a high rating (>=2000) by
24.6%. Contrary to the results of related work we also find that negative
treatment resulted in a negative effect that decreased the final rating by
5.15% on average.


Web Content Extraction - a Meta-Analysis of its Past and Thoughts on its
  Future

  In this paper, we present a meta-analysis of several Web content extraction
algorithms, and make recommendations for the future of content extraction on
the Web. First, we find that nearly all Web content extractors do not consider
a very large, and growing, portion of modern Web pages. Second, it is well
understood that wrapper induction extractors tend to break as the Web changes;
heuristic/feature engineering extractors were thought to be immune to a Web
site's evolution, but we find that this is not the case: heuristic content
extractor performance also tends to degrade over time due to the evolution of
Web site forms and practices. We conclude with recommendations for future work
that address these and other findings.


Discriminative Predicate Path Mining for Fact Checking in Knowledge
  Graphs

  Traditional fact checking by experts and analysts cannot keep pace with the
volume of newly created information. It is important and necessary, therefore,
to enhance our ability to computationally determine whether some statement of
fact is true or false. We view this problem as a link-prediction task in a
knowledge graph, and present a discriminative path-based method for fact
checking in knowledge graphs that incorporates connectivity, type information,
and predicate interactions. Given a statement S of the form (subject,
predicate, object), for example, (Chicago, capitalOf, Illinois), our approach
mines discriminative paths that alternatively define the generalized statement
(U.S. city, predicate, U.S. state) and uses the mined rules to evaluate the
veracity of statement S. We evaluate our approach by examining thousands of
claims related to history, geography, biology, and politics using a public,
million node knowledge graph extracted from Wikipedia and PubMedDB. Not only
does our approach significantly outperform related models, we also find that
the discriminative predicate path model is easily interpretable and provides
sensible reasons for the final determination.


Ozy: A General Orchestration Container

  Service-Oriented Computing is a paradigm that uses services as building
blocks for building distributed applications. The primary motivation for
orchestrating services in the cloud used to be distributed business processes,
which drove the standardization of the Business Process Execution Language
(BPEL) and its central notion that a service is a business process. In recent
years, there has been a transition towards other motivations for orchestrating
services in the cloud, {\em e.g.}, XaaS, RMAD. Although it is theoretically
possible to make all of those services into WSDL/SOAP services, it would be too
complicated and costly for industry adoption. Therefore, the central notion
that a service is a business process is too restrictive. Instead, we view a
service as a technology neutral, loosely coupled, location transparent
procedure. With these ideas in mind, we introduce a new approach to services
orchestration: Ozy, a general orchestration container. We define this new
approach in terms of existing technology, and we show that the Ozy container
relaxes many traditional constraints and allows for simpler, more feature-rich
applications.


Striations in PageRank-Ordered Matrices

  Patterns often appear in a variety of large, real-world networks, and
interesting physical phenomena are often explained by network topology as in
the case of the bow-tie structure of the World Wide Web, or the small world
phenomenon in social networks. The discovery and modelling of such regular
patterns has a wide application from disease propagation to financial markets.
In this work we describe a newly discovered regularly occurring striation
pattern found in the PageRank ordering of adjacency matrices that encode
real-world networks. We demonstrate that these striations are the result of
well-known graph generation processes resulting in regularities that are
manifest in the typical neighborhood distribution. The spectral view explored
in this paper encodes a tremendous amount about the explicit and implicit
topology of a given network, so we also discuss the interesting network
properties, outliers and anomalies that a viewer can determine from a brief
look at the re-ordered matrix.


Growing Graphs with Hyperedge Replacement Graph Grammars

  Discovering the underlying structures present in large real world graphs is a
fundamental scientific problem. In this paper we show that a graph's clique
tree can be used to extract a hyperedge replacement grammar. If we store an
ordering from the extraction process, the extracted graph grammar is guaranteed
to generate an isomorphic copy of the original graph. Or, a stochastic
application of the graph grammar rules can be used to quickly create random
graphs. In experiments on large real world networks, we show that random
graphs, generated from extracted graph grammars, exhibit a wide range of
properties that are very similar to the original graphs. In addition to graph
properties like degree or eigenvector centrality, what a graph "looks like"
ultimately depends on small details in local graph substructures that are
difficult to define at a global level. We show that our generative graph model
is able to preserve these local substructures when generating new graphs and
performs well on new and difficult tests of model robustness.


ProjE: Embedding Projection for Knowledge Graph Completion

  With the large volume of new information created every day, determining the
validity of information in a knowledge graph and filling in its missing parts
are crucial tasks for many researchers and practitioners. To address this
challenge, a number of knowledge graph completion methods have been developed
using low-dimensional graph embeddings. Although researchers continue to
improve these models using an increasingly complex feature space, we show that
simple changes in the architecture of the underlying model can outperform
state-of-the-art models without the need for complex feature engineering. In
this work, we present a shared variable neural network model called ProjE that
fills-in missing information in a knowledge graph by learning joint embeddings
of the knowledge graph's entities and edges, and through subtle, but important,
changes to the standard loss function. In doing so, ProjE has a parameter size
that is smaller than 11 out of 15 existing methods while performing $37\%$
better than the current-best method on standard datasets. We also show, via a
new fact checking task, that ProjE is capable of accurately determining the
veracity of many declarative statements.


Forward Backward Similarity Search in Knowledge Networks

  Similarity search is a fundamental problem in social and knowledge networks
like GitHub, DBLP, Wikipedia, etc. Existing network similarity measures are
limited because they only consider similarity from the perspective of the query
node. However, due to the complicated topology of real-world networks, ignoring
the preferences of target nodes often results in odd or unintuitive
performance. In this work, we propose a dual perspective similarity metric
called Forward Backward Similarity (FBS) that efficiently computes topological
similarity from the perspective of both the query node and the perspective of
candidate nodes. The effectiveness of our method is evaluated by traditional
quantitative ranking metrics and large-scale human judgement on four large real
world networks. The proposed method matches human preference and outperforms
other similarity search algorithms on community overlap and link prediction.
Finally, we demonstrate top-5 rankings for five famous researchers on an
academic collaboration network to illustrate how our approach captures
semantics more intuitively than other approaches.


Consumers and Curators: Browsing and Voting Patterns on Reddit

  As crowd-sourced curation of news and information become the norm, it is
important to understand not only how individuals consume information through
social news Web sites, but also how they contribute to their ranking systems.
In the present work, we introduce and make available a new dataset containing
the activity logs that recorded all activity for 309 Reddit users for one year.
Using this newly collected data, we present findings that highlight the
browsing and voting behavior of the study's participants. We find that most
users do not read the article that they vote on, and that, in total, 73% of
posts were rated (ie, upvoted or downvoted) without first viewing the content.
We also show evidence of cognitive fatigue in the browsing sessions of users
that are most likely to vote.


A Temporal Tree Decomposition for Generating Temporal Graphs

  Discovering the underlying structures present in large real world graphs is a
fundamental scientific problem. Recent work at the intersection of formal
language theory and graph theory has found that a Hyperedge Replacement Grammar
(HRG) can be extracted from a tree decomposition of any graph. This HRG can be
used to generate new graphs that share properties that are similar to the
original graph. Because the extracted HRG is directly dependent on the shape
and contents of the of tree decomposition, it is unlikely that informative
graph-processes are actually being captured with the extraction algorithm. To
address this problem, the current work presents a new extraction algorithm
called temporal HRG (tHRG) that learns HRG production rules from a temporal
tree decomposition of the graph. We observe problems with the assumptions that
are made in a temporal HRG model. In experiments on large real world networks,
we show and provide reasoning as to why tHRG does not perform as well as HRG
and other graph generators.


Predicting User-Interactions on Reddit

  In order to keep up with the demand of curating the deluge of crowd-sourced
content, social media platforms leverage user interaction feedback to make
decisions about which content to display, highlight, and hide. User
interactions such as likes, votes, clicks, and views are assumed to be a proxy
of a content's quality, popularity, or news-worthiness. In this paper we ask:
how predictable are the interactions of a user on social media? To answer this
question we recorded the clicking, browsing, and voting behavior of 186 Reddit
users over a year. We present interesting descriptive statistics about their
combined 339,270 interactions, and we find that relatively simple models are
able to predict users' individual browse- or vote-interactions with reasonable
accuracy.


Learning Hyperedge Replacement Grammars for Graph Generation

  The discovery and analysis of network patterns are central to the scientific
enterprise. In the present work, we developed and evaluated a new approach that
learns the building blocks of graphs that can be used to understand and
generate new realistic graphs. Our key insight is that a graph's clique tree
encodes robust and precise information. We show that a Hyperedge Replacement
Grammar (HRG) can be extracted from the clique tree, and we develop a
fixed-size graph generation algorithm that can be used to produce new graphs of
a specified size. In experiments on large real-world graphs, we show that
graphs generated from the HRG approach exhibit a diverse range of properties
that are similar to those found in the original networks. In addition to graph
properties like degree or eigenvector centrality, what a graph "looks like"
ultimately depends on small details in local graph substructures that are
difficult to define at a global level. We show that the HRG model can also
preserve these local substructures when generating new graphs.


The Infinity Mirror Test for Analyzing the Robustness of Graph
  Generators

  Graph generators learn a model from a source graph in order to generate a new
graph that has many of the same properties. The learned models each have
implicit and explicit biases built in, and its important to understand the
assumptions that are made when generating a new graph. Of course, the
differences between the new graph and the original graph, as compared by any
number of graph properties, are important indicators of the biases inherent in
any modelling task. But these critical differences are subtle and not
immediately apparent using standard performance metrics. Therefore, we
introduce the infinity mirror test for the analysis of graph generator
performance and robustness. This stress test operates by repeatedly,
recursively fitting a model to itself. A perfect graph generator would have no
deviation from the original or ideal graph, however the implicit biases and
assumptions that are cooked into the various models are exaggerated by the
infinity mirror test allowing for new insights that were not available before.
We show, via hundreds of experiments on 6 real world graphs, that several
common graph generators do degenerate in interesting and informative ways. We
believe that the observed degenerative patterns are clues to future development
of better graph models.


Mining Flipping Correlations from Large Datasets with Taxonomies

  In this paper we introduce a new type of pattern -- a flipping correlation
pattern. The flipping patterns are obtained from contrasting the correlations
between items at different levels of abstraction. They represent surprising
correlations, both positive and negative, which are specific for a given
abstraction level, and which "flip" from positive to negative and vice versa
when items are generalized to a higher level of abstraction. We design an
efficient algorithm for finding flipping correlations, the Flipper algorithm,
which outperforms naive pattern mining methods by several orders of magnitude.
We apply Flipper to real-life datasets and show that the discovered patterns
are non-redundant, surprising and actionable. Flipper finds strong contrasting
correlations in itemsets with low-to-medium support, while existing techniques
cannot handle the pattern discovery in this frequency range.


Thinking Like a Vertex: a Survey of Vertex-Centric Frameworks for
  Distributed Graph Processing

  The vertex-centric programming model is an established computational paradigm
recently incorporated into distributed processing frameworks to address
challenges in large-scale graph processing. Billion-node graphs that exceed the
memory capacity of standard machines are not well-supported by popular Big Data
tools like MapReduce, which are notoriously poor-performing for iterative graph
algorithms such as PageRank. In response, a new type of framework challenges
one to Think Like A Vertex (TLAV) and implements user-defined programs from the
perspective of a vertex rather than a graph. Such an approach improves
locality, demonstrates linear scalability, and provides a natural way to
express and compute many iterative graph algorithms. These frameworks are
simple to program and widely applicable, but, like an operating system, are
composed of several intricate, interdependent components, of which a thorough
understanding is necessary in order to elicit top performance at scale. To this
end, the first comprehensive survey of TLAV frameworks is presented. In this
survey, the vertex-centric approach to graph processing is overviewed, TLAV
frameworks are deconstructed into four main components and respectively
analyzed, and TLAV implementations are reviewed and categorized.


Open-World Knowledge Graph Completion

  Knowledge Graphs (KGs) have been applied to many tasks including Web search,
link prediction, recommendation, natural language processing, and entity
linking. However, most KGs are far from complete and are growing at a rapid
pace. To address these problems, Knowledge Graph Completion (KGC) has been
proposed to improve KGs by filling in its missing connections. Unlike existing
methods which hold a closed-world assumption, i.e., where KGs are fixed and new
entities cannot be easily added, in the present work we relax this assumption
and propose a new open-world KGC task. As a first attempt to solve this task we
introduce an open-world KGC model called ConMask. This model learns embeddings
of the entity's name and parts of its text-description to connect unseen
entities to the KG. To mitigate the presence of noisy text descriptions,
ConMask uses a relationship-dependent content masking to extract relevant
snippets and then trains a fully convolutional neural network to fuse the
extracted snippets with entities in the KG. Experiments on large data sets,
both old and new, show that ConMask performs well in the open-world KGC task
and even outperforms existing KGC models on the standard closed-world KGC task.


Visualizing the Flow of Discourse with a Concept Ontology

  Understanding and visualizing human discourse has long being a challenging
task. Although recent work on argument mining have shown success in classifying
the role of various sentences, the task of recognizing concepts and
understanding the ways in which they are discussed remains challenging. Given
an email thread or a transcript of a group discussion, our task is to extract
the relevant concepts and understand how they are referenced and re-referenced
throughout the discussion. In the present work, we present a preliminary
approach for extracting and visualizing group discourse by adapting Wikipedia's
category hierarchy to be an external concept ontology. From a user study, we
found that our method achieved better results than 4 strong alternative
approaches, and we illustrate our visualization method based on the extracted
discourse flows.


Identifying and Understanding User Reactions to Deceptive and Trusted
  Social News Sources

  In the age of social news, it is important to understand the types of
reactions that are evoked from news sources with various levels of credibility.
In the present work we seek to better understand how users react to trusted and
deceptive news sources across two popular, and very different, social media
platforms. To that end, (1) we develop a model to classify user reactions into
one of nine types, such as answer, elaboration, and question, etc, and (2) we
measure the speed and the type of reaction for trusted and deceptive news
sources for 10.8M Twitter posts and 6.2M Reddit comments. We show that there
are significant differences in the speed and the type of reactions between
trusted and deceptive news sources on Twitter, but far smaller differences on
Reddit.


Growing Better Graphs With Latent-Variable Probabilistic Graph Grammars

  Recent work in graph models has found that probabilistic hyperedge
replacement grammars (HRGs) can be extracted from graphs and used to generate
new random graphs with graph properties and substructures close to the
original. In this paper, we show how to add latent variables to the model,
trained using Expectation-Maximization, to generate still better graphs, that
is, ones that generalize better to the test data. We evaluate the new method by
separating training and test graphs, building the model on the former and
measuring the likelihood of the latter, as a more stringent test of how well
the model can generalize to new graphs. On this metric, we find that our
latent-variable HRGs consistently outperform several existing graph models and
provide interesting insights into the building blocks of real world networks.


How Humans versus Bots React to Deceptive and Trusted News Sources: A
  Case Study of Active Users

  Society's reliance on social media as a primary source of news has spawned a
renewed focus on the spread of misinformation. In this work, we identify the
differences in how social media accounts identified as bots react to news
sources of varying credibility, regardless of the veracity of the content those
sources have shared. We analyze bot and human responses annotated using a
fine-grained model that labels responses as being an answer, appreciation,
agreement, disagreement, an elaboration, humor, or a negative reaction. We
present key findings of our analysis into the prevalence of bots, the variety
and speed of bot and human reactions, and the disparity in authorship of
reaction tweets between these two sub-populations. We observe that bots are
responsible for 9-15% of the reactions to sources of any given type but
comprise only 7-10% of accounts responsible for reaction-tweets; trusted news
sources have the highest proportion of humans who reacted; bots respond with
significantly shorter delays than humans when posting answer-reactions in
response to sources identified as propaganda. Finally, we report significantly
different inequality levels in reaction rates for accounts identified as bots
vs not.


GuessTheKarma: A Game to Assess Social Rating Systems

  Popularity systems, like Twitter retweets, Reddit upvotes, and Pinterest pins
have the potential to guide people toward posts that others liked. That,
however, creates a feedback loop that reduces their informativeness: items
marked as more popular get more attention, so that additional upvotes and
retweets may simply reflect the increased attention and not independent
information about the fraction of people that like the items. How much
information remains? For example, how confident can we be that more people
prefer item A to item B if item A had hundreds of upvotes on Reddit and item B
had only a few? We investigate using an Internet game called GuessTheKarma that
collects independent preference judgments (N=20,674) for 400 pairs of images,
approximately 50 per pair. Unlike the rating systems that dominate social media
services, GuessTheKarma is devoid of social and ranking effects that influence
ratings. Overall, Reddit scores were not very good predictors of the true
population preferences for items as measured by GuessTheKarma: the image with
higher score was preferred by a majority of independent raters only 68% of the
time. However, when one image had a low score and the other was one of the
highest scoring in its subreddit, the higher scoring image was preferred nearly
90% of the time by the majority of independent raters. Similarly, Imgur view
counts for the images were poor predictors except when there were orders of
magnitude differences between the pairs. We conclude that popularity systems
marked by feedback loops may convey a strong signal about population
preferences, but only when comparing items that received vastly different
popularity scores.


Propagation from Deceptive News Sources: Who Shares, How Much, How
  Evenly, and How Quickly?

  As people rely on social media as their primary sources of news, the spread
of misinformation has become a significant concern. In this large-scale study
of news in social media we analyze eleven million posts and investigate
propagation behavior of users that directly interact with news accounts
identified as spreading trusted versus malicious content. Unlike previous work,
which looks at specific rumors, topics, or events, we consider all content
propagated by various news sources. Moreover, we analyze and contrast
population versus sub-population behaviour (by demographics) when spreading
misinformation, and distinguish between two types of propagation, i.e., direct
retweets and mentions. Our evaluation examines how evenly, how many, how
quickly, and which users propagate content from various types of news sources
on Twitter.
  Our analysis has identified several key differences in propagation behavior
from trusted versus suspicious news sources. These include high inequity in the
diffusion rate based on the source of disinformation, with a small group of
highly active users responsible for the majority of disinformation spread
overall and within each demographic. Analysis by demographics showed that users
with lower annual income and education share more from disinformation sources
compared to their counterparts. News content is shared significantly more
quickly from trusted, conspiracy, and disinformation sources compared to
clickbait and propaganda. Older users propagate news from trusted sources more
quickly than younger users, but they share from suspicious sources after longer
delays. Finally, users who interact with clickbait and conspiracy sources are
likely to share from propaganda accounts, but not the other way around.


