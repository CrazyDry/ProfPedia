Random Voting Effects in Social-Digital Spaces: A case study of Reddit  Post Submissions

  At a time when information seekers first turn to digital sources for news andopinion, it is critical that we understand the role that social media plays inhuman behavior. This is especially true when information consumers also act asinformation producers and editors by their online activity. In order to betterunderstand the effects that editorial ratings have on online human behavior, wereport the results of a large-scale in-vivo experiment in social media. We findthat small, random rating manipulations on social media submissions createdsignificant changes in downstream ratings resulting in significantly differentfinal outcomes. Positive treatment resulted in a positive effect that increasedthe final rating by 11.02% on average. Compared to the control group, positivetreatment also increased the probability of reaching a high rating (>=2000) by24.6%. Contrary to the results of related work we also find that negativetreatment resulted in a negative effect that decreased the final rating by5.15% on average.

Web Content Extraction - a Meta-Analysis of its Past and Thoughts on its  Future

  In this paper, we present a meta-analysis of several Web content extractionalgorithms, and make recommendations for the future of content extraction onthe Web. First, we find that nearly all Web content extractors do not considera very large, and growing, portion of modern Web pages. Second, it is wellunderstood that wrapper induction extractors tend to break as the Web changes;heuristic/feature engineering extractors were thought to be immune to a Website's evolution, but we find that this is not the case: heuristic contentextractor performance also tends to degrade over time due to the evolution ofWeb site forms and practices. We conclude with recommendations for future workthat address these and other findings.

Discriminative Predicate Path Mining for Fact Checking in Knowledge  Graphs

  Traditional fact checking by experts and analysts cannot keep pace with thevolume of newly created information. It is important and necessary, therefore,to enhance our ability to computationally determine whether some statement offact is true or false. We view this problem as a link-prediction task in aknowledge graph, and present a discriminative path-based method for factchecking in knowledge graphs that incorporates connectivity, type information,and predicate interactions. Given a statement S of the form (subject,predicate, object), for example, (Chicago, capitalOf, Illinois), our approachmines discriminative paths that alternatively define the generalized statement(U.S. city, predicate, U.S. state) and uses the mined rules to evaluate theveracity of statement S. We evaluate our approach by examining thousands ofclaims related to history, geography, biology, and politics using a public,million node knowledge graph extracted from Wikipedia and PubMedDB. Not onlydoes our approach significantly outperform related models, we also find thatthe discriminative predicate path model is easily interpretable and providessensible reasons for the final determination.

Scalable Models for Computing Hierarchies in Information Networks

  Information hierarchies are organizational structures that often used toorganize and present large and complex information as well as provide amechanism for effective human navigation. Fortunately, many statistical andcomputational models exist that automatically generate hierarchies; however,the existing approaches do not consider linkages in information {\em networks}that are increasingly common in real-world scenarios. Current approaches alsotend to present topics as an abstract probably distribution over words, etcrather than as tangible nodes from the original network. Furthermore, thestatistical techniques present in many previous works are not yet capable ofprocessing data at Web-scale. In this paper we present the HierarchicalDocument Topic Model (HDTM), which uses a distributed vertex-programmingprocess to calculate a nonparametric Bayesian generative model. Experiments onthree medium size data sets and the entire Wikipedia dataset show that HDTM caninfer accurate hierarchies even over large information networks.

Ozy: A General Orchestration Container

  Service-Oriented Computing is a paradigm that uses services as buildingblocks for building distributed applications. The primary motivation fororchestrating services in the cloud used to be distributed business processes,which drove the standardization of the Business Process Execution Language(BPEL) and its central notion that a service is a business process. In recentyears, there has been a transition towards other motivations for orchestratingservices in the cloud, {\em e.g.}, XaaS, RMAD. Although it is theoreticallypossible to make all of those services into WSDL/SOAP services, it would be toocomplicated and costly for industry adoption. Therefore, the central notionthat a service is a business process is too restrictive. Instead, we view aservice as a technology neutral, loosely coupled, location transparentprocedure. With these ideas in mind, we introduce a new approach to servicesorchestration: Ozy, a general orchestration container. We define this newapproach in terms of existing technology, and we show that the Ozy containerrelaxes many traditional constraints and allows for simpler, more feature-richapplications.

Striations in PageRank-Ordered Matrices

  Patterns often appear in a variety of large, real-world networks, andinteresting physical phenomena are often explained by network topology as inthe case of the bow-tie structure of the World Wide Web, or the small worldphenomenon in social networks. The discovery and modelling of such regularpatterns has a wide application from disease propagation to financial markets.In this work we describe a newly discovered regularly occurring striationpattern found in the PageRank ordering of adjacency matrices that encodereal-world networks. We demonstrate that these striations are the result ofwell-known graph generation processes resulting in regularities that aremanifest in the typical neighborhood distribution. The spectral view exploredin this paper encodes a tremendous amount about the explicit and implicittopology of a given network, so we also discuss the interesting networkproperties, outliers and anomalies that a viewer can determine from a brieflook at the re-ordered matrix.

Rating Effects on Social News Posts and Comments

  At a time when information seekers first turn to digital sources for news andopinion, it is critical that we understand the role that social media plays inhuman behavior. This is especially true when information consumers also act asinformation producers and editors through their online activity. In order tobetter understand the effects that editorial ratings have on online humanbehavior, we report the results of a two large-scale in-vivo experiments insocial media. We find that small, random rating manipulations on social mediaposts and comments created significant changes in downstream ratings resultingin significantly different final outcomes. We found positive herding effectsfor positive treatments on posts, increasing the final rating by 11.02% onaverage, but not for positive treatments on comments. Contrary to the resultsof related work, we found negative herding effects for negative treatments onposts and comments, decreasing the final ratings on average, of posts by 5.15%and of comments by 37.4%. Compared to the control group, the probability ofreaching a high rating (>=2000) for posts is increased by 24.6% when postsreceive the positive treatment and for comments is decreased by 46.6% whencomments receive the negative treatment.

Growing Graphs with Hyperedge Replacement Graph Grammars

  Discovering the underlying structures present in large real world graphs is afundamental scientific problem. In this paper we show that a graph's cliquetree can be used to extract a hyperedge replacement grammar. If we store anordering from the extraction process, the extracted graph grammar is guaranteedto generate an isomorphic copy of the original graph. Or, a stochasticapplication of the graph grammar rules can be used to quickly create randomgraphs. In experiments on large real world networks, we show that randomgraphs, generated from extracted graph grammars, exhibit a wide range ofproperties that are very similar to the original graphs. In addition to graphproperties like degree or eigenvector centrality, what a graph "looks like"ultimately depends on small details in local graph substructures that aredifficult to define at a global level. We show that our generative graph modelis able to preserve these local substructures when generating new graphs andperforms well on new and difficult tests of model robustness.

ProjE: Embedding Projection for Knowledge Graph Completion

  With the large volume of new information created every day, determining thevalidity of information in a knowledge graph and filling in its missing partsare crucial tasks for many researchers and practitioners. To address thischallenge, a number of knowledge graph completion methods have been developedusing low-dimensional graph embeddings. Although researchers continue toimprove these models using an increasingly complex feature space, we show thatsimple changes in the architecture of the underlying model can outperformstate-of-the-art models without the need for complex feature engineering. Inthis work, we present a shared variable neural network model called ProjE thatfills-in missing information in a knowledge graph by learning joint embeddingsof the knowledge graph's entities and edges, and through subtle, but important,changes to the standard loss function. In doing so, ProjE has a parameter sizethat is smaller than 11 out of 15 existing methods while performing $37\%$better than the current-best method on standard datasets. We also show, via anew fact checking task, that ProjE is capable of accurately determining theveracity of many declarative statements.

Forward Backward Similarity Search in Knowledge Networks

  Similarity search is a fundamental problem in social and knowledge networkslike GitHub, DBLP, Wikipedia, etc. Existing network similarity measures arelimited because they only consider similarity from the perspective of the querynode. However, due to the complicated topology of real-world networks, ignoringthe preferences of target nodes often results in odd or unintuitiveperformance. In this work, we propose a dual perspective similarity metriccalled Forward Backward Similarity (FBS) that efficiently computes topologicalsimilarity from the perspective of both the query node and the perspective ofcandidate nodes. The effectiveness of our method is evaluated by traditionalquantitative ranking metrics and large-scale human judgement on four large realworld networks. The proposed method matches human preference and outperformsother similarity search algorithms on community overlap and link prediction.Finally, we demonstrate top-5 rankings for five famous researchers on anacademic collaboration network to illustrate how our approach capturessemantics more intuitively than other approaches.

Consumers and Curators: Browsing and Voting Patterns on Reddit

  As crowd-sourced curation of news and information become the norm, it isimportant to understand not only how individuals consume information throughsocial news Web sites, but also how they contribute to their ranking systems.In the present work, we introduce and make available a new dataset containingthe activity logs that recorded all activity for 309 Reddit users for one year.Using this newly collected data, we present findings that highlight thebrowsing and voting behavior of the study's participants. We find that mostusers do not read the article that they vote on, and that, in total, 73% ofposts were rated (ie, upvoted or downvoted) without first viewing the content.We also show evidence of cognitive fatigue in the browsing sessions of usersthat are most likely to vote.

A Temporal Tree Decomposition for Generating Temporal Graphs

  Discovering the underlying structures present in large real world graphs is afundamental scientific problem. Recent work at the intersection of formallanguage theory and graph theory has found that a Hyperedge Replacement Grammar(HRG) can be extracted from a tree decomposition of any graph. This HRG can beused to generate new graphs that share properties that are similar to theoriginal graph. Because the extracted HRG is directly dependent on the shapeand contents of the of tree decomposition, it is unlikely that informativegraph-processes are actually being captured with the extraction algorithm. Toaddress this problem, the current work presents a new extraction algorithmcalled temporal HRG (tHRG) that learns HRG production rules from a temporaltree decomposition of the graph. We observe problems with the assumptions thatare made in a temporal HRG model. In experiments on large real world networks,we show and provide reasoning as to why tHRG does not perform as well as HRGand other graph generators.

Predicting User-Interactions on Reddit

  In order to keep up with the demand of curating the deluge of crowd-sourcedcontent, social media platforms leverage user interaction feedback to makedecisions about which content to display, highlight, and hide. Userinteractions such as likes, votes, clicks, and views are assumed to be a proxyof a content's quality, popularity, or news-worthiness. In this paper we ask:how predictable are the interactions of a user on social media? To answer thisquestion we recorded the clicking, browsing, and voting behavior of 186 Redditusers over a year. We present interesting descriptive statistics about theircombined 339,270 interactions, and we find that relatively simple models areable to predict users' individual browse- or vote-interactions with reasonableaccuracy.

Learning Hyperedge Replacement Grammars for Graph Generation

  The discovery and analysis of network patterns are central to the scientificenterprise. In the present work, we developed and evaluated a new approach thatlearns the building blocks of graphs that can be used to understand andgenerate new realistic graphs. Our key insight is that a graph's clique treeencodes robust and precise information. We show that a Hyperedge ReplacementGrammar (HRG) can be extracted from the clique tree, and we develop afixed-size graph generation algorithm that can be used to produce new graphs ofa specified size. In experiments on large real-world graphs, we show thatgraphs generated from the HRG approach exhibit a diverse range of propertiesthat are similar to those found in the original networks. In addition to graphproperties like degree or eigenvector centrality, what a graph "looks like"ultimately depends on small details in local graph substructures that aredifficult to define at a global level. We show that the HRG model can alsopreserve these local substructures when generating new graphs.

The Infinity Mirror Test for Analyzing the Robustness of Graph  Generators

  Graph generators learn a model from a source graph in order to generate a newgraph that has many of the same properties. The learned models each haveimplicit and explicit biases built in, and its important to understand theassumptions that are made when generating a new graph. Of course, thedifferences between the new graph and the original graph, as compared by anynumber of graph properties, are important indicators of the biases inherent inany modelling task. But these critical differences are subtle and notimmediately apparent using standard performance metrics. Therefore, weintroduce the infinity mirror test for the analysis of graph generatorperformance and robustness. This stress test operates by repeatedly,recursively fitting a model to itself. A perfect graph generator would have nodeviation from the original or ideal graph, however the implicit biases andassumptions that are cooked into the various models are exaggerated by theinfinity mirror test allowing for new insights that were not available before.We show, via hundreds of experiments on 6 real world graphs, that severalcommon graph generators do degenerate in interesting and informative ways. Webelieve that the observed degenerative patterns are clues to future developmentof better graph models.

Mining Flipping Correlations from Large Datasets with Taxonomies

  In this paper we introduce a new type of pattern -- a flipping correlationpattern. The flipping patterns are obtained from contrasting the correlationsbetween items at different levels of abstraction. They represent surprisingcorrelations, both positive and negative, which are specific for a givenabstraction level, and which "flip" from positive to negative and vice versawhen items are generalized to a higher level of abstraction. We design anefficient algorithm for finding flipping correlations, the Flipper algorithm,which outperforms naive pattern mining methods by several orders of magnitude.We apply Flipper to real-life datasets and show that the discovered patternsare non-redundant, surprising and actionable. Flipper finds strong contrastingcorrelations in itemsets with low-to-medium support, while existing techniquescannot handle the pattern discovery in this frequency range.

Thinking Like a Vertex: a Survey of Vertex-Centric Frameworks for  Distributed Graph Processing

  The vertex-centric programming model is an established computational paradigmrecently incorporated into distributed processing frameworks to addresschallenges in large-scale graph processing. Billion-node graphs that exceed thememory capacity of standard machines are not well-supported by popular Big Datatools like MapReduce, which are notoriously poor-performing for iterative graphalgorithms such as PageRank. In response, a new type of framework challengesone to Think Like A Vertex (TLAV) and implements user-defined programs from theperspective of a vertex rather than a graph. Such an approach improveslocality, demonstrates linear scalability, and provides a natural way toexpress and compute many iterative graph algorithms. These frameworks aresimple to program and widely applicable, but, like an operating system, arecomposed of several intricate, interdependent components, of which a thoroughunderstanding is necessary in order to elicit top performance at scale. To thisend, the first comprehensive survey of TLAV frameworks is presented. In thissurvey, the vertex-centric approach to graph processing is overviewed, TLAVframeworks are deconstructed into four main components and respectivelyanalyzed, and TLAV implementations are reviewed and categorized.

Open-World Knowledge Graph Completion

  Knowledge Graphs (KGs) have been applied to many tasks including Web search,link prediction, recommendation, natural language processing, and entitylinking. However, most KGs are far from complete and are growing at a rapidpace. To address these problems, Knowledge Graph Completion (KGC) has beenproposed to improve KGs by filling in its missing connections. Unlike existingmethods which hold a closed-world assumption, i.e., where KGs are fixed and newentities cannot be easily added, in the present work we relax this assumptionand propose a new open-world KGC task. As a first attempt to solve this task weintroduce an open-world KGC model called ConMask. This model learns embeddingsof the entity's name and parts of its text-description to connect unseenentities to the KG. To mitigate the presence of noisy text descriptions,ConMask uses a relationship-dependent content masking to extract relevantsnippets and then trains a fully convolutional neural network to fuse theextracted snippets with entities in the KG. Experiments on large data sets,both old and new, show that ConMask performs well in the open-world KGC taskand even outperforms existing KGC models on the standard closed-world KGC task.

Visualizing the Flow of Discourse with a Concept Ontology

  Understanding and visualizing human discourse has long being a challengingtask. Although recent work on argument mining have shown success in classifyingthe role of various sentences, the task of recognizing concepts andunderstanding the ways in which they are discussed remains challenging. Givenan email thread or a transcript of a group discussion, our task is to extractthe relevant concepts and understand how they are referenced and re-referencedthroughout the discussion. In the present work, we present a preliminaryapproach for extracting and visualizing group discourse by adapting Wikipedia'scategory hierarchy to be an external concept ontology. From a user study, wefound that our method achieved better results than 4 strong alternativeapproaches, and we illustrate our visualization method based on the extracteddiscourse flows.

Identifying and Understanding User Reactions to Deceptive and Trusted  Social News Sources

  In the age of social news, it is important to understand the types ofreactions that are evoked from news sources with various levels of credibility.In the present work we seek to better understand how users react to trusted anddeceptive news sources across two popular, and very different, social mediaplatforms. To that end, (1) we develop a model to classify user reactions intoone of nine types, such as answer, elaboration, and question, etc, and (2) wemeasure the speed and the type of reaction for trusted and deceptive newssources for 10.8M Twitter posts and 6.2M Reddit comments. We show that thereare significant differences in the speed and the type of reactions betweentrusted and deceptive news sources on Twitter, but far smaller differences onReddit.

Growing Better Graphs With Latent-Variable Probabilistic Graph Grammars

  Recent work in graph models has found that probabilistic hyperedgereplacement grammars (HRGs) can be extracted from graphs and used to generatenew random graphs with graph properties and substructures close to theoriginal. In this paper, we show how to add latent variables to the model,trained using Expectation-Maximization, to generate still better graphs, thatis, ones that generalize better to the test data. We evaluate the new method byseparating training and test graphs, building the model on the former andmeasuring the likelihood of the latter, as a more stringent test of how wellthe model can generalize to new graphs. On this metric, we find that ourlatent-variable HRGs consistently outperform several existing graph models andprovide interesting insights into the building blocks of real world networks.

How Humans versus Bots React to Deceptive and Trusted News Sources: A  Case Study of Active Users

  Society's reliance on social media as a primary source of news has spawned arenewed focus on the spread of misinformation. In this work, we identify thedifferences in how social media accounts identified as bots react to newssources of varying credibility, regardless of the veracity of the content thosesources have shared. We analyze bot and human responses annotated using afine-grained model that labels responses as being an answer, appreciation,agreement, disagreement, an elaboration, humor, or a negative reaction. Wepresent key findings of our analysis into the prevalence of bots, the varietyand speed of bot and human reactions, and the disparity in authorship ofreaction tweets between these two sub-populations. We observe that bots areresponsible for 9-15% of the reactions to sources of any given type butcomprise only 7-10% of accounts responsible for reaction-tweets; trusted newssources have the highest proportion of humans who reacted; bots respond withsignificantly shorter delays than humans when posting answer-reactions inresponse to sources identified as propaganda. Finally, we report significantlydifferent inequality levels in reaction rates for accounts identified as botsvs not.

GuessTheKarma: A Game to Assess Social Rating Systems

  Popularity systems, like Twitter retweets, Reddit upvotes, and Pinterest pinshave the potential to guide people toward posts that others liked. That,however, creates a feedback loop that reduces their informativeness: itemsmarked as more popular get more attention, so that additional upvotes andretweets may simply reflect the increased attention and not independentinformation about the fraction of people that like the items. How muchinformation remains? For example, how confident can we be that more peopleprefer item A to item B if item A had hundreds of upvotes on Reddit and item Bhad only a few? We investigate using an Internet game called GuessTheKarma thatcollects independent preference judgments (N=20,674) for 400 pairs of images,approximately 50 per pair. Unlike the rating systems that dominate social mediaservices, GuessTheKarma is devoid of social and ranking effects that influenceratings. Overall, Reddit scores were not very good predictors of the truepopulation preferences for items as measured by GuessTheKarma: the image withhigher score was preferred by a majority of independent raters only 68% of thetime. However, when one image had a low score and the other was one of thehighest scoring in its subreddit, the higher scoring image was preferred nearly90% of the time by the majority of independent raters. Similarly, Imgur viewcounts for the images were poor predictors except when there were orders ofmagnitude differences between the pairs. We conclude that popularity systemsmarked by feedback loops may convey a strong signal about populationpreferences, but only when comparing items that received vastly differentpopularity scores.

Propagation from Deceptive News Sources: Who Shares, How Much, How  Evenly, and How Quickly?

  As people rely on social media as their primary sources of news, the spreadof misinformation has become a significant concern. In this large-scale studyof news in social media we analyze eleven million posts and investigatepropagation behavior of users that directly interact with news accountsidentified as spreading trusted versus malicious content. Unlike previous work,which looks at specific rumors, topics, or events, we consider all contentpropagated by various news sources. Moreover, we analyze and contrastpopulation versus sub-population behaviour (by demographics) when spreadingmisinformation, and distinguish between two types of propagation, i.e., directretweets and mentions. Our evaluation examines how evenly, how many, howquickly, and which users propagate content from various types of news sourceson Twitter.  Our analysis has identified several key differences in propagation behaviorfrom trusted versus suspicious news sources. These include high inequity in thediffusion rate based on the source of disinformation, with a small group ofhighly active users responsible for the majority of disinformation spreadoverall and within each demographic. Analysis by demographics showed that userswith lower annual income and education share more from disinformation sourcescompared to their counterparts. News content is shared significantly morequickly from trusted, conspiracy, and disinformation sources compared toclickbait and propaganda. Older users propagate news from trusted sources morequickly than younger users, but they share from suspicious sources after longerdelays. Finally, users who interact with clickbait and conspiracy sources arelikely to share from propaganda accounts, but not the other way around.

