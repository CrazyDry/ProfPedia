Some new solutions to the Klein-Gordon equation in 1-1D space-time in  the presence of time dependent potentials

  We find three exact solutions to the Klein-Gordon equation in 1-1 dimensionalspace-time for different time dependent potentials. In two cases we consider atime dependent scalar potential and in one case a time dependent electricpotential.

Indirect Searches for Kaluza-Klein Dark Matter

  In this talk, we discuss the potential for the indirect detection ofKaluza-Klein dark matter using neutrino telescopes and cosmic positronexperiments. We find that future kilometer-scale neutrino telescopes, such asIceCube, as well as future experiments capable of measuring the cosmic positronspectrum, such as PAMELA and AMS-02, will be quite sensitive to this scenario.Current data from the HEAT experiment can also be explained by the presence ofKaluza-Klein dark matter in the Galactic halo.

Kaluza-Klein Dark Matter and the Positron Excess

  The excess of cosmic positrons observed by the HEAT experiment may be theresult of Kaluza-Klein dark matter annihilating in the galactic halo.Kaluza-Klein dark matter annihilates dominantly into charged leptons that yielda large number and hard spectrum of positrons per annihilation. Given aKaluza-Klein dark matter particle with a mass in the range of 300-400 GeV, noexceptional substructure or clumping is needed in the local distribution ofdark matter to generate a positron flux that explains the HEAT observations.This is in contrast to supersymmetric dark matter that requires unnaturallylarge amounts of dark substructure to produce the observed positron excess.Future astrophysical and collider tests are outlined that will confirm or ruleout this explanation of the HEAT data.

Interplay of the Aharonov-Bohm effect and Klein tunneling in graphene

  We numerically investigate the effect of Klein tunneling on the Aharonov-Bohmoscillations in graphene rings using a tight-binding model withnearest-neighbor couplings. In order to introduce Klein tunneling into thesystem, we apply an electrostatic potential to one of the arms of the ring,such that this arm together with the two adjacent leads form either a $nn'n$-or $npn$-junction ($n,n'$: conduction band transport, $p$: valence bandtransport). The former case corresponds to normal tunneling and the latter caseto Klein tunneling. We find that the transmission properties strongly depend onthe smoothness of the $pn$-interfaces. In particular, for sharp junctions theamplitude profile is symmetric around the charge neutrality point in the gatedarm, whereas for smooth junctions the Aharonov-Bohm oscillations are stronglysuppressed in the Klein tunneling as compared to the normal tunneling regime.

Kaluza-Klein Dark Matter, Electrons and Gamma Ray Telescopes

  Kaluza-Klein dark matter particles can annihilate efficiently intoelectron-positron pairs, providing a discrete feature (a sharp edge) in thecosmic $e^+ e^-$ spectrum at an energy equal to the particle's mass (typicallyseveral hundred GeV to one TeV). Although this feature is probably beyond thereach of satellite or balloon-based cosmic ray experiments (those thatdistinguish the charge and mass of the primary particle), gamma ray telescopesmay provide an alternative detection method. Designed to observe veryhigh-energy gamma-rays, ACTs also observe the diffuse flux of electron-inducedelectromagnetic showers. The GLAST satellite, designed for gamma ray astronomy,will also observe any high energy showers (several hundred GeV and above) inits calorimeter. We show that high-significance detections of anelectron-positron feature from Kaluza-Klein dark matter annihilations arepossible with GLAST, and also with ACTs such as HESS, VERITAS or MAGIC.

Particle Dark Matter: Evidence, Candidates and Constraints

  In this review article, we discuss the current status of particle darkmatter, including experimental evidence and theoretical motivations. We discussa wide array of candidates for particle dark matter, but focus on neutralinosin models of supersymmetry and Kaluza-Klein dark matter in models of universalextra dimensions. We devote much of our attention to direct and indirectdetection techniques, the constraints placed by these experiments and the reachof future experimental efforts.

Searching for Dark Matter with Future Cosmic Positron Experiments

  Dark matter particles annihilating in the Galactic halo can provide a flux ofpositrons potentially observable in upcoming experiments, such as PAMELA andAMS-02. We discuss the spectral features which may be associated with darkmatter annihilation in the positron spectrum and assess the prospects forobserving such features in future experiments. Although we focus on somespecific dark matter candidates, neutralinos and Kaluza-Klein states, we carryout our study in a model independent fashion. We also revisit the positronspectrum observed by HEAT.

Capturing Semantic Similarity for Entity Linking with Convolutional  Neural Networks

  A key challenge in entity linking is making effective use of contextualinformation to disambiguate mentions that might refer to different entities indifferent contexts. We present a model that uses convolutional neural networksto capture semantic correspondence between a mention's context and a proposedtarget entity. These convolutional networks operate at multiple granularitiesto exploit various kinds of topic information, and their rich parameterizationgives them the capacity to learn which n-grams characterize different topics.We combine these networks with a sparse linear model to achievestate-of-the-art performance on multiple entity linking datasets, outperformingthe prior systems of Durrett and Klein (2014) and Nguyen et al. (2014).

Multilingual Constituency Parsing with Self-Attention and Pre-Training

  We extend our previous work on constituency parsing (Kitaev and Klein, 2018)by incorporating pre-training for ten additional languages, and compare thebenefits of no pre-training, ELMo (Peters et al., 2018), and BERT (Devlin etal., 2018). Pre-training is effective across all languages evaluated, and BERToutperforms ELMo in large part due to the benefits of increased model capacity.Our parser obtains new state-of-the-art results for 11 languages, includingEnglish (95.8 F1) and Chinese (91.8 F1).

Indirect Detection of Dirac Right-Handed Neutrino Dark Matter

  We present the signatures and prospects for the indirect detection of a Diracright-handed neutrino dark matter candidate in neutrino telescopes, cosmicpositron experiments and gamma-ray telescopes. An example of such a dark mattercandidate can be found in extra-dimensional models. In some constructions,Kaluza--Klein states with the gauge quantum numbers of a right-handed neutrinocan have sizable gauge interactions with Standard Model particles. Forinstance, in 5D warped Grand Unified Theories, it has been shown that aKaluza--Klein right-handed neutrino may be stable and otherwise aphenomenologically viable dark matter candidate. We find that the prospects forthe indirect detection of such a WIMP are encouraging, particularly forneutrino telescopes and cosmic positron experiments.

The PAMELA and ATIC Signals From Kaluza-Klein Dark Matter

  In this letter, we study the possibility that Kaluza-Klein dark matter in amodel with one universal extra dimension is responsible for the recentobservations of the PAMELA and ATIC experiments. In this model, the dark matterparticles annihilate largely to charged leptons, which enables them to producea spectrum of cosmic ray electrons and positrons consistent with the PAMELA andATIC measurements. To normalize to the observed signal, however, large boostfactors (~10^3) are required. Despite these large boost factors and significantannihilation to hadronic modes (35%), we find that the constraints from cosmicray antiproton measurements can be satisfied. Relic abundance considerations inthis model force us to consider a rather specific range of masses(approximately 600-900 GeV) which is very similar to the range required togenerate the ATIC spectral feature. The results presented here can also be usedas a benchmark for model-independent constraints on dark matter annihilation tohadronic modes.

Kaluza-Klein Dark Matter And Neutrinos From Annihilation In The Sun

  In models with one universal extra dimension (UED), the first Kaluza-Kleinexcitations of the hypercharge gauge boson, B^(1), and the neutral component ofisospin gauge boson, W^3(1), are each viable dark matter candidates. In eithercase, such particles are predicted to accumulate in the core of the Sun, wherethey annihilate to generate a potentially observable flux of high energyneutrinos. In this article, we calculate the flux of neutrinos produced in thismodel and determine the constraints that can be placed on the UED parameterspace from current IceCube data. For the case of B^(1) dark matter, we findthat the present limits from IceCube are stronger than those from direct darkmatter detection experiments such as CDMS and XENON10. For W^3(1) dark matter,the present IceCube data provides a constraint slightly weaker than directdetection experiments. In addition, we also present the projected regions ofUED parameter space that can be probed by IceCube/DeepCore in the near futureand compare them to the prospects for future direct detection experiments.

Spinless photon dark matter from two universal extra dimensions

  We explore the properties of dark matter in theories with two universal extradimensions, where the lightest Kaluza-Klein state is a spin-0 neutral particle,representing a six-dimensional photon polarized along the extra dimensions.Annihilation of this 'spinless photon' proceeds predominantly through Higgsboson exchange, and is largely independent of other Kaluza-Klein particles. Themeasured relic abundance sets an upper limit on the spinless photon mass of 500GeV, which decreases to almost 200 GeV if the Higgs boson is light. Thephenomenology of this dark matter candidate is strikingly different fromKaluza-Klein dark matter in theories with one universal extra dimension.Elastic scattering of the spinless photon with quarks is helicity suppressed,making its direct detection challenging, although possible at upcomingexperiments. The prospects for indirect detection with gamma rays andantimatter are similar to those of neutralinos. The rates predicted at neutrinotelescopes are below the sensitivity of next-generation experiments.

Global well-posedness for the massive Maxwell-Klein-Gordon equation with  small critical Sobolev data

  In this paper we prove global well-posedness and modified scattering for themassive Maxwell-Klein-Gordon equation in the Coulomb gauge on$\mathbb{R}^{1+d}$ $(d \geq 4)$ for data with small critical Sobolev norm. Thisextends to the general case $ m^2 > 0 $ the results of Krieger-Sterbenz-Tataru($d=4,5 $) and Rodnianski-Tao ($ d \geq 6 $), who considered the case $ m=0$.  We proceed by generalizing the global parametrix construction for thecovariant wave operator and the functional framework from the massless case tothe Klein-Gordon setting. The equation exhibits a trilinear cancelationstructure identified by Machedon-Sterbenz. To treat it one needs sharp $ L^2 $null form bounds, which we prove by estimating renormalized solutions in nullframes spaces similar to the ones considered by Bejenaru-Herr. To overcomelogarithmic divergences we rely on an embedding property of $ \Box^{-1} $ inconjunction with endpoint Strichartz estimates in Lorentz spaces.

Long-ranged attraction between disordered heterogeneous surfaces

  Long-ranged attractions across water between two surfaces that are randomlycovered with (mobile) positive and negative charge domains have been attributedto induced correlation of the charges (positive lining up with negative) as thesurfaces approach. Here we show, by directly measuring normal forces under arapid shear field, that these attractions may not in fact be due to suchcorrelations. It is rather the inherent interaction-asymmetry between equally-and between oppositely-charged domains that results in the long-rangedattraction even in the complete absence of any charge correlation.

Alignment-based compositional semantics for instruction following

  This paper describes an alignment-based model for interpreting naturallanguage instructions in context. We approach instruction following as a searchover plans, scoring sequences of actions conditioned on structured observationsof text and the environment. By explicitly modeling both the low-levelcompositional structure of individual actions and the high-level structure offull plans, we are able to learn both grounded representations of sentencemeaning and pragmatic constraints on interpretation. To demonstrate the model'sflexibility, we apply it to a diverse set of benchmark tasks. On every task, weoutperform strong task-specific baselines, and achieve several newstate-of-the-art results.

Learning to Compose Neural Networks for Question Answering

  We describe a question answering model that applies to both images andstructured knowledge bases. The model uses natural language strings toautomatically assemble neural networks from a collection of composable modules.Parameters for these modules are learned jointly with network-assemblyparameters via reinforcement learning, with only (world, question, answer)triples as supervision. Our approach, which we term a dynamic neural modelnetwork, achieves state-of-the-art results on benchmark datasets in both visualand structured domains.

Reasoning About Pragmatics with Neural Listeners and Speakers

  We present a model for pragmatically describing scenes, in which contrastivebehavior results from a combination of inference-driven pragmatics and learnedsemantics. Like previous learned approaches to language generation, our modeluses a simple feature-driven architecture (here a pair of neural "listener" and"speaker" models) to ground language in the world. Like inference-drivenapproaches to pragmatics, our model actively reasons about listener behaviorwhen selecting utterances. For training, our approach requires only ordinarycaptions, annotated _without_ demonstration of the pragmatic behavior the modelultimately exhibits. In human evaluations on a referring expression game, ourapproach succeeds 81% of the time, compared to a 69% success rate usingexisting techniques.

Abstract Syntax Networks for Code Generation and Semantic Parsing

  Tasks like code generation and semantic parsing require mapping unstructured(or partially structured) inputs to well-formed, executable outputs. Weintroduce abstract syntax networks, a modeling framework for these problems.The outputs are represented as abstract syntax trees (ASTs) and constructed bya decoder with a dynamically-determined modular structure paralleling thestructure of the output tree. On the benchmark Hearthstone dataset for codegeneration, our model obtains 79.2 BLEU and 22.7% exact match accuracy,compared to previous state-of-the-art values of 67.1 and 6.1%. Furthermore, weperform competitively on the Atis, Jobs, and Geo semantic parsing datasets withno task-specific engineering.

Fine-Grained Entity Typing with High-Multiplicity Assignments

  As entity type systems become richer and more fine-grained, we expect thenumber of types assigned to a given entity to increase. However, mostfine-grained typing work has focused on datasets that exhibit a low degree oftype multiplicity. In this paper, we consider the high-multiplicity regimeinherent in data sources such as Wikipedia that have semi-open type systems. Weintroduce a set-prediction approach to this problem and show that our modeloutperforms unstructured baselines on a new Wikipedia-based fine-grained typingcorpus.

A Minimal Span-Based Neural Constituency Parser

  In this work, we present a minimal neural model for constituency parsingbased on independent scoring of labels and spans. We show that this model isnot only compatible with classical dynamic programming techniques, but alsoadmits a novel greedy top-down inference algorithm based on recursivepartitioning of the input. We demonstrate empirically that both predictionschemes are competitive with recent work, and when combined with basicextensions to the scoring model are capable of achieving state-of-the-artsingle-model performance on the Penn Treebank (91.79 F1) and strong performanceon the French Treebank (82.23 F1).

Analogs of Linguistic Structure in Deep Representations

  We investigate the compositional structure of message vectors computed by adeep network trained on a communication game. By comparing truth-conditionalrepresentations of encoder-produced message vectors to human-produced referringexpressions, we are able to identify aligned (vector, utterance) pairs with thesame meaning. We then search for structured relationships among these alignedpairs to discover simple vector space transformations corresponding tonegation, conjunction, and disjunction. Our results suggest that neuralrepresentations are capable of spontaneously developing a "syntax" withfunctional analogues to qualitative properties of natural language.

Searching For Dark Matter with Neutrino Telescopes

  One of the most interesting mysteries of astrophysics is the puzzle of darkmatter. Although numerous techniques have been explored and developed to detectthis elusive substance, its nature remains unknown. One such method uses largehigh-energy neutrino telescopes to look for the annihilation products of darkmatter annihilations. In this summary article, we briefly review thistechnique. We describe the calculations used to find the rate of capture ofWIMPs in the Sun or Earth and the spectrum of neutrinos produced in theresulting dark matter annihilations. We will discuss these calculations withinthe context of supersymmetry and models with universal extra dimensions, thelightest supersymmetric particle and lightest Kaluza-Klein particle providingthe WIMP candidate in these cases, respectively. We will also discuss thestatus of some of the experiments relevant to these searches: AMANDA, IceCubeand ANTARES.

Improved Bounds on Universal Extra Dimensions and Consequences for LKP  Dark Matter

  We study constraints on models with a flat "Universal'' Extra Dimension inwhich all Standard Model fields propagate in the bulk. A significantly improvedconstraint on the compactification scale is obtained from the extended set ofelectroweak precision observables accurately measured at LEP1 and LEP2. We finda lower bound of M_c = R^{-1} > 700 (800) GeV at the 99% (95%) confidencelevel. We also discuss the implications of this constraint on the prospects forthe direct and indirect detection of Kaluza-Klein dark matter in this model.

Distinguishing Supersymmetry From Universal Extra Dimensions or Little  Higgs Models With Dark Matter Experiments

  There are compelling reasons to think that new physics will appear at orbelow the TeV-scale. It is not known what form this new physics will take,however. Although The Large Hadron collider is very likely to discover newparticles associated with the TeV-scale, it may be difficult for it todetermine the nature of those particles, whether superpartners, Kaluza-Kleinmodes or other states. In this article, we consider how direct and indirectdark matter detection experiments may provide information complementary tohadron colliders, which can be used to discriminate between supersymmetry,models with universal extra dimensions, and Little Higgs theories. We findthat, in many scenarios, dark matter experiments can be effectively used todistinguish between these possibilities.

Quantization of heterotic strings in a Goedel/Anti de Sitter spacetime  and chronology protection

  We show that a Goedel-like deformation of AdS3 in heterotic string theory canbe realized as an exact string background. Indeed this class of solutions isobtained as an exactly marginal deformation of the conformal field theorydescribing the NS5/F1 heterotic background. It can also be embedded in type IIsuperstrings as a Kaluza-Klein reduction. We compute the spectrum of this modelas well as the genus one modular invariant partition function. We discuss theissue of closed timelike curves and the propagation of long strings. Theydestabilize completely the background, although we construct another exactstring background that may describe the result of the condensation of theselong strings. Closed timelike curves are avoided in that case.

Probing Kaluza-Klein Dark Matter with Neutrino Telescopes

  In models in which all of the Standard Model fields live in extra universaldimensions, the lightest Kaluza-Klein (KK) particle can be stable. Calculationsof the one-loop radiative corrections to the masses of the KK modes suggestthat the identity of the lightest KK particle (LKP) is mostly the first KKexcitation of the hypercharge gauge boson. This LKP is a viable dark mattercandidate with an ideal present-day relic abundance if its mass is moderatelylarge, between 600 to 1200 GeV. Such weakly interacting dark matter particlesare expected to become gravitationally trapped in large bodies, such as theSun, and annihilate into neutrinos or other particles that decay intoneutrinos. We calculate the annihilation rate, neutrino flux and the resultingevent rate in present and future neutrino telescopes. The relatively large massimplies that the neutrino energy spectrum is expected to be well above theenergy threshold of AMANDA and IceCube. We find that the event rate in IceCubeis between a few to tens of events per year.

Global well-posedness of high dimensional Maxwell-Dirac for small  critical data

  In this paper, we prove global well-posedness of the massless Maxwell-Diracequation in Coulomb gauge on $\mathbb{R}^{1+d}$ $(d \geq 4)$ for data withsmall scale-critical Sobolev norm, as well as modified scattering of thesolutions. Main components of our proof are A) uncovering null structure ofMaxwell-Dirac in the Coulomb gauge, and B) proving solvability of theunderlying covariant Dirac equation. A key step for achieving both is toexploit (and justify) a deep analogy between Maxwell-Dirac andMaxwell-Klein-Gordon (for which an analogous result was proved earlier byKrieger-Sterbenz-Tataru), which says that the most difficult part ofMaxwell-Dirac takes essentially the same form as Maxwell-Klein-Gordon.

Exotic Neutrino Interactions at the Pierre Auger Observatory

  The Pierre Auger Observatory for cosmic rays provides a laboratory forstudying fundamental interactions at energies well beyond those available atcolliders. In addition to hadrons or photons, Auger is sensitive to ultra-highenergy neutrinos in the cosmic radiation and models for new physics can beexplored by observing neutrino interactions at center-of-mass energies beyondthe TeV scale. By comparing the rate for quasi-horizontal, deeply penetratingair showers triggered by all types of neutrinos with the rate for slightlyupgoing showers generated by Earth-skimming tau neutrinos, any deviation of theneutrino-nucleon cross-section from the Standard Model expectation can beconstrained. We show that this can test models of low-scale quantum gravity(including processes such as Kaluza-Klein graviton exchange, microscopic blackhole production and string resonances), as well as non-perturbative electroweakinstanton mediated processes. Moreover, the observed ratios of neutrino flavorswould severely constrain the possibility of neutrino decay.

Learning Dependency-Based Compositional Semantics

  Suppose we want to build a system that answers a natural language question byrepresenting its semantics as a logical form and computing the answer given astructured database of facts. The core part of such a system is the semanticparser that maps questions to logical forms. Semantic parsers are typicallytrained from examples of questions annotated with their target logical forms,but this type of annotation is expensive.  Our goal is to learn a semantic parser from question-answer pairs instead,where the logical form is modeled as a latent variable. Motivated by thischallenging learning problem, we develop a new semantic formalism,dependency-based compositional semantics (DCS), which has favorable linguistic,statistical, and computational properties. We define a log-linear distributionover DCS logical forms and estimate the parameters using a simple procedurethat alternates between beam search and numerical optimization. On two standardsemantic parsing benchmarks, our system outperforms all existingstate-of-the-art systems, despite using no annotated logical forms.

Mixture-of-Parents Maximum Entropy Markov Models

  We present the mixture-of-parents maximum entropy Markov model (MoP-MEMM), aclass of directed graphical models extending MEMMs. The MoP-MEMM allowstractable incorporation of long-range dependencies between nodes by restrictingthe conditional distribution of each node to be a mixture of distributionsgiven the parents. We show how to efficiently compute the exact marginalposterior node distributions, regardless of the range of the dependencies. Thisenables us to model non-sequential correlations present within text documents,as well as between interconnected documents, such as hyperlinked web pages. Weapply the MoP-MEMM to a named entity recognition task and a web pageclassification task. In each, our model shows significant improvement over thebasic MEMM, and is competitive with other long-range sequence models that useapproximate inference.

Thermodynamics of five-dimensional static three-charge STU black holes  with squashed horizons

  We present a new expression for the five-dimensional static Kaluza-Kleinblack hole solution with squashed $S^3$ horizons and three different chargeparameters. This black hole solution belongs to $D = 5$ $N = 2$ supergravitytheory, its spacetime is locally asymptotically flat and has a spatial infinity$R \times S^1 \hookrightarrow S^2$. The form of the solution is extraordinarysimple and permits us very conveniently to calculate its conserved charges byusing the counterterm method. It is further shown that our thermodynamicalquantities perfectly obey both the differential and the integral first laws ofblack hole thermodynamics if the length of the compact extra-dimension can beviewed as a thermodynamical variable.

On the accuracy of self-normalized log-linear models

  Calculation of the log-normalizer is a major computational obstacle inapplications of log-linear models with large output spaces. The problem of fastnormalizer computation has therefore attracted significant attention in thetheoretical and applied machine learning literature. In this paper, we analyzea recently proposed technique known as "self-normalization", which introduces aregularization term in training to penalize log normalizers for deviating fromzero. This makes it possible to use unnormalized model scores as approximateprobabilities. Empirical evidence suggests that self-normalization is extremelyeffective, but a theoretical understanding of why it should work, and howgenerally it can be applied, is largely lacking. We prove generalization boundson the estimated variance of normalizers and upper bounds on the loss inaccuracy due to self-normalization, describe classes of input distributionsthat self-normalize easily, and construct explicit examples of high-varianceinput distributions. Our theoretical results make predictions about thedifficulty of fitting self-normalized models to several classes ofdistributions, and we conclude with empirical validation of these predictions.

Neural CRF Parsing

  This paper describes a parsing model that combines the exact dynamicprogramming of CRF parsing with the rich nonlinear featurization of neural netapproaches. Our model is structurally a CRF that factors over anchored ruleproductions, but instead of linear potential functions based on sparsefeatures, we use nonlinear potentials computed via a feedforward neuralnetwork. Because potentials are still local to anchored rules, structuredinference (CKY) is unchanged from the sparse case. Computing gradients duringlearning involves backpropagating an error signal formed from standard CRFsufficient statistics (expected rule counts). Using only dense features, ourneural CRF already exceeds a strong baseline CRF model (Hall et al., 2014). Incombination with sparse features, our system achieves 91.1 F1 on section 23 ofthe Penn Treebank, and more generally outperforms the best prior single parserresults on a range of languages.

Neural Module Networks

  Visual question answering is fundamentally compositional in nature---aquestion like "where is the dog?" shares substructure with questions like "whatcolor is the dog?" and "where is the cat?" This paper seeks to simultaneouslyexploit the representational capacity of deep networks and the compositionallinguistic structure of questions. We describe a procedure for constructing andlearning *neural module networks*, which compose collections of jointly-trainedneural "modules" into deep networks for question answering. Our approachdecomposes questions into their linguistic substructures, and uses thesestructures to dynamically instantiate modular networks (with reusablecomponents for recognizing dogs, classifying colors, etc.). The resultingcompound networks are jointly trained. We evaluate our approach on twochallenging datasets for visual question answering, achieving state-of-the-artresults on both the VQA natural image dataset and a new dataset of complexquestions about abstract shapes.

Learning-Based Single-Document Summarization with Compression and  Anaphoricity Constraints

  We present a discriminative model for single-document summarization thatintegrally combines compression and anaphoricity constraints. Our model selectstextual units to include in the summary based on a rich set of sparse featureswhose weights are learned on a large corpus. We allow for the deletion ofcontent within a sentence when that deletion is licensed by compression rules;in our framework, these are implemented as dependencies between subsententialunits of text. Anaphoricity constraints then improve cross-sentence coherenceby guaranteeing that, for each pronoun included in the summary, the pronoun'santecedent is included as well or the pronoun is rewritten as a full mention.When trained end-to-end, our final system outperforms prior work on both ROUGEas well as on human judgments of linguistic quality.

Wave-particle duality coming from a bead oscillator in an elastic  medium, theoretical study and quantum similarities

  We introduce a dual wave-particle macroscopic system, where a bead oscillatoroscillates in an elastic medium which obeys the Klein-Gordon equation. Thistheoretical system is mostly inspired by bouncing droplets experiments and beadsliding on a vibrating string experiments. This system is studied using acommon and simple mathematical formalism. We compute the motion equation of thebead as well as the wave equation of the system. We introduce the effectivevelocity of the bead with respect to the elastic medium and the wave $\psi$,created by the bead, which modulates the natural wave of the medium. Providedsome conditions, $\psi$ obeys an equation analogous to the free Schr\"odingerequation. In the case of linear and spherical cavities, the particle-likecharacteristics of the bead, expressed with its effective velocity, areproportional to the corresponding wave-like characteristics of the system.  This paper is a translation of Dualit\'e onde-corpuscule form\'ee par unemasselotte oscillante dans un milieu \'elastique : \'etude th\'eorique etsimilitudes quantiques.

Modular Multitask Reinforcement Learning with Policy Sketches

  We describe a framework for multitask deep reinforcement learning guided bypolicy sketches. Sketches annotate tasks with sequences of named subtasks,providing information about high-level structural relationships among tasks butnot how to implement them---specifically not providing the detailed guidanceused by much previous work on learning policy abstractions for RL (e.g.intermediate rewards, subtask completion signals, or intrinsic motivations). Tolearn from sketches, we present a model that associates every subtask with amodular subpolicy, and jointly maximizes reward over full task-specificpolicies by tying parameters across shared subpolicies. Optimization isaccomplished via a decoupled actor--critic training objective that facilitateslearning common behaviors from multiple dissimilar reward functions. Weevaluate the effectiveness of our approach in three environments featuring bothdiscrete and continuous control, and with sparse rewards that can be obtainedonly after completing a number of high-level subgoals. Experiments show thatusing our approach to learn policies guided by sketches gives betterperformance than existing techniques for learning task-specific or sharedpolicies, while naturally inducing a library of interpretable primitivebehaviors that can be recombined to rapidly adapt to new tasks.

Translating Neuralese

  Several approaches have recently been proposed for learning decentralizeddeep multiagent policies that coordinate via a differentiable communicationchannel. While these policies are effective for many tasks, interpretation oftheir induced communication strategies has remained a challenge. Here wepropose to interpret agents' messages by translating them. Unlike in typicalmachine translation problems, we have no parallel data to learn from. Insteadwe develop a translation model based on the insight that agent messages andnatural language strings mean the same thing if they induce the same beliefabout the world in a listener. We present theoretical guarantees and empiricalevidence that our approach preserves both the semantics and pragmatics ofmessages by ensuring that players communicating through a translation layer donot suffer a substantial loss in reward relative to players with a commonlanguage.

Improving Neural Parsing by Disentangling Model Combination and  Reranking Effects

  Recent work has proposed several generative neural models for constituencyparsing that achieve state-of-the-art results. Since direct search in thesegenerative models is difficult, they have primarily been used to rescorecandidate outputs from base parsers in which decoding is more straightforward.We first present an algorithm for direct search in these generative models. Wethen demonstrate that the rescoring results are at least partly due to implicitmodel combination rather than reranking effects. Finally, we show that explicitmodel combination can improve performance even further, resulting in newstate-of-the-art numbers on the PTB of 94.25 F1 when training only on gold dataand 94.66 F1 when using external data.

Parsing with Traces: An $O(n^4)$ Algorithm and a Structural  Representation

  General treebank analyses are graph structured, but parsers are typicallyrestricted to tree structures for efficiency and modeling reasons. We propose anew representation and algorithm for a class of graph structures that isflexible enough to cover almost all treebank structures, while still admittingefficient learning and inference. In particular, we consider directed, acyclic,one-endpoint-crossing graph structures, which cover most long-distancedislocation, shared argumentation, and similar tree-violating linguisticphenomena. We describe how to convert phrase structure parses, includingtraces, to our new representation in a reversible manner. Our dynamic programuniquely decomposes structures, is sound and complete, and covers 97.3% of thePenn English Treebank. We also implement a proof-of-concept parser thatrecovers a range of null elements and trace types.

Effective Inference for Generative Neural Parsing

  Generative neural models have recently achieved state-of-the-art results forconstituency parsing. However, without a feasible search procedure, their usehas so far been limited to reranking the output of external parsers in whichdecoding is more tractable. We describe an alternative to the conventionalaction-level beam search used for discriminative neural models that enables usto decode directly in these generative models. We then show that by improvingour basic candidate selection strategy and using a coarse pruning function, wecan improve accuracy while exploring significantly less of the search space.Applied to the model of Choe and Charniak (2016), our inference procedureobtains 92.56 F1 on section 23 of the Penn Treebank, surpassing priorstate-of-the-art results for single-model systems.

Learning with Latent Language

  The named concepts and compositional operators present in natural languageprovide a rich source of information about the kinds of abstractions humans useto navigate the world. Can this linguistic background knowledge improve thegenerality and efficiency of learned classifiers and control policies? Thispaper aims to show that using the space of natural language strings as aparameter space is an effective way to capture natural task structure. In apretraining phase, we learn a language interpretation model that transformsinputs (e.g. images) into outputs (e.g. labels) given natural languagedescriptions. To learn a new concept (e.g. a classifier), we search directly inthe space of descriptions to minimize the interpreter's loss on trainingexamples. Crucially, our models do not require language data to learn theseconcepts: language is used only in pretraining to impose structure onsubsequent learning. Results on image classification, text editing, andreinforcement learning show that, in all settings, models with a linguisticparameterization outperform those without.

Unified Pragmatic Models for Generating and Following Instructions

  We show that explicit pragmatic inference aids in correctly generating andfollowing natural language instructions for complex, sequential tasks. Ourpragmatics-enabled models reason about why speakers produce certaininstructions, and about how listeners will react upon hearing them. Likeprevious pragmatic models, we use learned base listener and speaker models tobuild a pragmatic speaker that uses the base listener to simulate theinterpretation of candidate descriptions, and a pragmatic listener that reasonscounterfactually about alternative descriptions. We extend these models totasks with sequential structure. Evaluation of language generation andinterpretation shows that pragmatic inference improves state-of-the-artlistener models (at correctly interpreting human instructions) and speakermodels (at producing instructions correctly interpreted by humans) in diversesettings.

What's Going On in Neural Constituency Parsers? An Analysis

  A number of differences have emerged between modern and classic approaches toconstituency parsing in recent years, with structural components like grammarsand feature-rich lexicons becoming less central while recurrent neural networkrepresentations rise in popularity. The goal of this work is to analyze theextent to which information provided directly by the model structure inclassical systems is still being captured by neural methods. To this end, wepropose a high-performance neural model (92.08 F1 on PTB) that isrepresentative of recent work and perform a series of investigativeexperiments. We find that our model implicitly learns to encode much of thesame information that was explicitly provided by grammars and lexicons in thepast, indicating that this scaffolding can largely be subsumed by powerfulgeneral-purpose neural machinery.

Constituency Parsing with a Self-Attentive Encoder

  We demonstrate that replacing an LSTM encoder with a self-attentivearchitecture can lead to improvements to a state-of-the-art discriminativeconstituency parser. The use of attention makes explicit the manner in whichinformation is propagated between different locations in the sentence, which weuse to both analyze our model and propose potential improvements. For example,we find that separating positional and content information in the encoder canlead to improved parsing accuracy. Additionally, we evaluate differentapproaches for lexical representation. Our parser achieves new state-of-the-artresults for single models trained on the Penn Treebank: 93.55 F1 without theuse of any external data, and 95.13 F1 when using pre-trained wordrepresentations. Our parser also outperforms the previous best-publishedaccuracy figures on 8 of the 9 languages in the SPMRL dataset.

Policy Gradient as a Proxy for Dynamic Oracles in Constituency Parsing

  Dynamic oracles provide strong supervision for training constituency parserswith exploration, but must be custom defined for a given parser's transitionsystem. We explore using a policy gradient method as a parser-agnosticalternative. In addition to directly optimizing for a tree-level metric such asF1, policy gradient has the potential to reduce exposure bias by allowingexploration during training; moreover, it does not require a dynamic oracle forsupervision. On four constituency parsers in three languages, the methodsubstantially outperforms static oracle likelihood training in almost allsettings. For parsers where a dynamic oracle is available (including a noveloracle which we define for the transition system of Dyer et al. 2016), policygradient typically recaptures a substantial fraction of the performance gainafforded by the dynamic oracle.

Triplet Lifetime in Gaseous Argon

  MiniCLEAN is a single-phase liquid argon dark matter experiment. During theinitial cooling phase, impurities within the cold gas ($<$140 K) were monitoredby measuring the scintillation light triplet lifetime, and ultimately a tripletlifetime of 3.480 $\pm$ 0.001 (stat.) $\pm$ 0.064 (sys.) $\mu$s was obtained,indicating ultra-pure argon. This is the longest argon triplet time constantever reported. The effect of quenching of separate components of thescintillation light is also investigated.

Pragmatically Informative Text Generation

  We improve the informativeness of models for conditional text generationusing techniques from computational pragmatics. These techniques formulatelanguage production as a game between speakers and listeners, in which aspeaker should generate output text that a listener can use to correctlyidentify the original input that the text describes. While such approaches arewidely used in cognitive science and grounded language learning, they havereceived less attention for more standard language generation tasks. Weconsider two pragmatic modeling methods for text generation: one wherepragmatics is imposed by information preservation, and another where pragmaticsis imposed by explicit modeling of distractors. We find that these methodsimprove the performance of strong existing systems for abstractivesummarization and generation from structured meaning representations.

Prospects For Detecting Dark Matter With GLAST In Light Of The WMAP Haze

  Observations by the WMAP experiment have identified an excess of microwaveemission from the center of the Milky Way. It has previously been shown thatthis "WMAP Haze" could be synchrotron emission from relativistic electrons andpositrons produced in the annihilations of dark matter particles. Inparticular, the intensity, spectrum and angular distribution of the WMAP Hazeis consistent with an electroweak scale dark matter particle (such as asupersymmetric neutralino or Kaluza-Klein dark matter in models with universalextra dimensions) annihilating with a cross section on the order of sigmav~3x10^-26 cm^3/s and distributed with a cusped halo profile. No further exoticastrophysical or annihilation boost factors are required. If dark matterannihilations are in fact responsible for the observed Haze, then otherannihilation products will also be produced, including gamma rays. In thisarticle, we study the prospects for the GLAST satellite to detect gamma raysfrom dark matter annihilations in the Galactic Center region in this scenario.We find that by studying only the inner 0.1 degrees around the Galactic Center,GLAST will be able to detect dark matter annihilating to heavy quarks or gaugebosons over astrophysical backgrounds with 5sigma (3sigma) significance if theyare lighter than approximately 320-500 GeV (500-750 GeV). If the angular windowis broadened to study the dark matter halo profile's angular extension (whilesimultaneously reducing the astrophysical backgrounds), WIMPs as heavy asseveral TeV can be identified by GLAST with high significance. Only if the darkmatter particles annihilate mostly to electrons or muons will GLAST be unableto identify the gamma ray spectrum associated with the WMAP Haze.

