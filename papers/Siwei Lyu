Interpretation and Generalization of Score Matching

  Score matching is a recently developed parameter learning method that isparticularly effective to complicated high dimensional density models withintractable partition functions. In this paper, we study two issues that havenot been completely resolved for score matching. First, we provide a formallink between maximum likelihood and score matching. Our analysis shows thatscore matching finds model parameters that are more robust with noisy trainingdata. Second, we develop a generalization of score matching. Based on thisgeneralization, we further demonstrate an extension of score matching to modelsof discrete data.

In Ictu Oculi: Exposing AI Generated Fake Face Videos by Detecting Eye  Blinking

  The new developments in deep generative networks have significantly improvethe quality and efficiency in generating realistically-looking fake facevideos. In this work, we describe a new method to expose fake face videosgenerated with neural networks. Our method is based on detection of eyeblinking in the videos, which is a physiological signal that is not wellpresented in the synthesized fake videos. Our method is tested over benchmarksof eye-blinking detection datasets and also show promising performance ondetecting videos generated with DeepFake.

Contrast Enhancement Estimation for Digital Image Forensics

  Inconsistency in contrast enhancement can be used to expose image forgeries.In this work, we describe a new method to estimate contrast enhancement from asingle image. Our method takes advantage of the nature of contrast enhancementas a mapping between pixel values, and the distinct characteristics itintroduces to the image pixel histogram. Our method recovers the original pixelhistogram and the contrast enhancement simultaneously from a single image withan iterative algorithm. Unlike previous methods, our method is robust in thepresence of additive noise perturbations that are used to hide the traces ofcontrast enhancement. Furthermore, we also develop an e effective method to todetect image regions undergone contrast enhancement transformations that aredifferent from the rest of the image, and use this method to detect compositeimages. We perform extensive experimental evaluations to demonstrate theefficacy and efficiency of our method method.

A Univariate Bound of Area Under ROC

  Area under ROC (AUC) is an important metric for binary classification andbipartite ranking problems. However, it is difficult to directly optimizing AUCas a learning objective, so most existing algorithms are based on optimizing asurrogate loss to AUC. One significant drawback of these surrogate losses isthat they require pairwise comparisons among training data, which leads to slowrunning time and increasing local storage for online learning. In this work, wedescribe a new surrogate loss based on a reformulation of the AUC risk, whichdoes not require pairwise comparison but rankings of the predictions. Wefurther show that the ranking operation can be avoided, and the learningobjective obtained based on this surrogate enjoys linear complexity in time andstorage. We perform experiments to demonstrate the effectiveness of the onlineand batch algorithms for AUC optimization based on the proposed surrogate loss.

De-identification without losing faces

  Training of deep learning models for computer vision requires large image orvideo datasets from real world. Often, in collecting such datasets, we need toprotect the privacy of the people captured in the images or videos, while stillpreserve the useful attributes such as facial expressions. In this work, wedescribe a new face de-identification method that can preserve essential facialattributes in the faces while concealing the identities. Our method takesadvantage of the recent advances in face attribute transfer models, whilemaintaining a high visual quality. Instead of changing factors of the originalfaces or synthesizing faces completely, our method use a trained facialattribute transfer model to map non-identity related facial attributes to theface of donors, who are a small number (usually 2 to 3) of consented subjects.Using the donors' faces ensures that the natural appearance of the synthesizedfaces, while ensuring the identity of the synthesized faces are changed. On theother hand, the FATM blends the donors' facial attributes to those of theoriginal faces to diversify the appearance of the synthesized faces.Experimental results on several sets of images and videos demonstrate theeffectiveness of our face de-ID algorithm.

LSTM with Working Memory

  Previous RNN architectures have largely been superseded by LSTM, or "LongShort-Term Memory". Since its introduction, there have been many variations onthis simple design. However, it is still widely used and we are not aware of agated-RNN architecture that outperforms LSTM in a broad sense while still beingas simple and efficient. In this paper we propose a modified LSTM-likearchitecture. Our architecture is still simple and achieves better performanceon the tasks that we tested on. We also introduce a new RNN performancebenchmark that uses the handwritten digits and stresses several importantnetwork capabilities.

Exposing Deep Fakes Using Inconsistent Head Poses

  In this paper, we propose a new method to expose AI-generated fake faceimages or videos (commonly known as the Deep Fakes). Our method is based on theobservations that Deep Fakes are created by splicing synthesized face regioninto the original image, and in doing so, introducing errors that can berevealed when 3D head poses are estimated from the face images. We performexperiments to demonstrate this phenomenon and further develop a classificationmethod based on this cue. Using features based on this cue, an SVM classifieris evaluated using a set of real face images and Deep Fakes.

Exposing GAN-synthesized Faces Using Landmark Locations

  Generative adversary networks (GANs) have recently led to highly realisticimage synthesis results. In this work, we describe a new method to exposeGAN-synthesized images using the locations of the facial landmark points. Ourmethod is based on the observations that the facial parts configurationgenerated by GAN models are different from those of the real faces, due to thelack of global constraints. We perform experiments demonstrating thisphenomenon, and show that an SVM classifier trained using the locations offacial landmark points is sufficient to achieve good classification performancefor GAN-synthesized faces.

UA-DETRAC: A New Benchmark and Protocol for Multi-Object Detection and  Tracking

  In recent years, numerous effective multi-object tracking (MOT) methods aredeveloped because of the wide range of applications. Existing performanceevaluations of MOT methods usually separate the object tracking step from theobject detection step by using the same fixed object detection results forcomparisons. In this work, we perform a comprehensive quantitative study on theeffects of object detection accuracy to the overall MOT performance, using thenew large-scale University at Albany DETection and tRACking (UA-DETRAC)benchmark dataset. The UA-DETRAC benchmark dataset consists of 100 challengingvideo sequences captured from real-world traffic scenes (over 140,000 frameswith rich annotations, including occlusion, weather, vehicle category,truncation, and vehicle bounding boxes) for object detection, object trackingand MOT system. We evaluate complete MOT systems constructed from combinationsof state-of-the-art object detection and object tracking methods. Our analysisshows the complex effects of object detection accuracy on MOT systemperformance. Based on these observations, we propose new evaluation tools andmetrics for MOT systems that consider both object detection and object trackingfor comprehensive analysis.

Improving Image Restoration with Soft-Rounding

  Several important classes of images such as text, barcode and pattern imageshave the property that pixels can only take a distinct subset of values. Thisknowledge can benefit the restoration of such images, but it has not beenwidely considered in current restoration methods. In this work, we describe aneffective and efficient approach to incorporate the knowledge of distinct pixelvalues of the pristine images into the general regularized least squaresrestoration framework. We introduce a new regularizer that attains zero at thedesignated pixel values and becomes a quadratic penalty function in theintervals between them. When incorporated into the regularized least squaresrestoration framework, this regularizer leads to a simple and efficient stepthat resembles and extends the rounding operation, which we term assoft-rounding. We apply the soft-rounding enhanced solution to the restorationof binary text/barcode images and pattern images with multiple distinct pixelvalues. Experimental results show that soft-rounding enhanced restorationmethods achieve significant improvement in both visual quality and quantitativemeasures (PSNR and SSIM). Furthermore, we show that this regularizer can alsobenefit the restoration of general natural images.

Geometric Hypergraph Learning for Visual Tracking

  Graph based representation is widely used in visual tracking field by findingcorrect correspondences between target parts in consecutive frames. However,most graph based trackers consider pairwise geometric relations between localparts. They do not make full use of the target's intrinsic structure, therebymaking the representation easily disturbed by errors in pairwise affinitieswhen large deformation and occlusion occur. In this paper, we propose ageometric hypergraph learning based tracking method, which fully exploitshigh-order geometric relations among multiple correspondences of parts inconsecutive frames. Then visual tracking is formulated as the mode-seekingproblem on the hypergraph in which vertices represent correspondence hypothesesand hyperedges describe high-order geometric relations. Besides, aconfidence-aware sampling method is developed to select representative verticesand hyperedges to construct the geometric hypergraph for more robustness andscalability. The experiments are carried out on two challenging datasets(VOT2014 and Deform-SOT) to demonstrate that the proposed method performsfavorable against other existing trackers.

Robust Localized Multi-view Subspace Clustering

  In multi-view clustering, different views may have different confidencelevels when learning a consensus representation. Existing methods usuallyaddress this by assigning distinctive weights to different views. However, dueto noisy nature of real-world applications, the confidence levels of samples inthe same view may also vary. Thus considering a unified weight for a view maylead to suboptimal solutions. In this paper, we propose a novel localizedmulti-view subspace clustering model that considers the confidence levels ofboth views and samples. By assigning weight to each sample under each viewproperly, we can obtain a robust consensus representation via fusing thenoiseless structures among views and samples. We further develop a regularizeron weight parameters based on the convex conjugacy theory, and samples weightsare determined in an adaptive manner. An efficient iterative algorithm isdeveloped with a convergence guarantee. Experimental results on four benchmarksdemonstrate the correctness and effectiveness of the proposed model.

Learning with Average Top-k Loss

  In this work, we introduce the {\em average top-$k$} (\atk) loss as a newaggregate loss for supervised learning, which is the average over the $k$largest individual losses over a training dataset. We show that the \atk lossis a natural generalization of the two widely used aggregate losses, namely theaverage loss and the maximum loss, but can combine their advantages andmitigate their drawbacks to better adapt to different data distributions.Furthermore, it remains a convex function over all individual losses, which canlead to convex optimization problems that can be solved effectively withconventional gradient-based methods. We provide an intuitive interpretation ofthe \atk loss based on its equivalent effect on the continuous individual lossfunctions, suggesting that it can reduce the penalty on correctly classifieddata. We further give a learning theory analysis of \matk learning on theclassification calibration of the \atk loss and the error bounds of \atk-SVM.We demonstrate the applicability of minimum average top-$k$ learning for binaryclassification and regression using synthetic and real datasets.

Tagging like Humans: Diverse and Distinct Image Annotation

  In this work we propose a new automatic image annotation model, dubbed {\bfdiverse and distinct image annotation} (D2IA). The generative model D2IA isinspired by the ensemble of human annotations, which create semanticallyrelevant, yet distinct and diverse tags. In D2IA, we generate a relevant anddistinct tag subset, in which the tags are relevant to the image contents andsemantically distinct to each other, using sequential sampling from adeterminantal point process (DPP) model. Multiple such tag subsets that coverdiverse semantic aspects or diverse semantic levels of the image contents aregenerated by randomly perturbing the DPP sampling process. We leverage agenerative adversarial network (GAN) model to train D2IA. Extensive experimentsincluding quantitative and qualitative comparisons, as well as human subjectstudies, on two benchmark datasets demonstrate that the proposed model canproduce more diverse and distinct tags than the state-of-the-arts.

STS Classification with Dual-stream CNN

  The structured time series (STS) classification problem requires the modelingof interweaved spatiotemporal dependency. most previous STS classificationmethods model the spatial and temporal dependencies independently. Due to thecomplexity of the STS data, we argue that a desirable STS classification methodshould be a holistic framework that can be made as adaptive and flexible aspossible. This motivates us to design a deep neural network with such merits.Inspired by the dual-stream hypothesis in neural science, we propose a noveldual-stream framework for modeling the interweaved spatiotemporal dependency,and develop a convolutional neural network within this framework that aims toachieve high adaptability and flexibility in STS configurations from variousdiagonals, i.e., sequential order, dependency range and features. The proposedarchitecture is highly modularized and scalable, making it easy to be adaptedto specific tasks. The effectiveness of our model is demonstrated throughexperiments on synthetic data as well as benchmark datasets for skeleton basedactivity recognition.

Who did What at Where and When: Simultaneous Multi-Person Tracking and  Activity Recognition

  We present a bootstrapping framework to simultaneously improve multi-persontracking and activity recognition at individual, interaction and social groupactivity levels. The inference consists of identifying trajectories of allpedestrian actors, individual activities, pairwise interactions, and collectiveactivities, given the observed pedestrian detections. Our method uses agraphical model to represent and solve the joint tracking and recognitionproblems via multi-stages: (1) activity-aware tracking, (2) joint interactionrecognition and occlusion recovery, and (3) collective activity recognition. Wesolve the where and when problem with visual tracking, as well as the who andwhat problem with recognition. High-order correlations among the visible andoccluded individuals, pairwise interactions, groups, and activities are thensolved using a hypergraph formulation within the Bayesian framework.Experiments on several benchmarks show the advantages of our approach overstate-of-art methods.

Multi-Scale Supervised Network for Human Pose Estimation

  Human pose estimation is an important topic in computer vision with manyapplications including gesture and activity recognition. However, poseestimation from image is challenging due to appearance variations, occlusions,clutter background, and complex activities. To alleviate these problems, wedevelop a robust pose estimation method based on the recent deep conv-deconvmodules with two improvements: (1) multi-scale supervision of body keypoints,and (2) a global regression to improve structural consistency of keypoints. Werefine keypoint detection heatmaps using layer-wise multi-scale supervision tobetter capture local contexts. Pose inference via keypoint association isoptimized globally using a regression network at the end. Our method caneffectively disambiguate keypoint matches in close proximity including themismatch of left-right body parts, and better infer occluded parts.Experimental results show that our method achieves competitive performanceamong state-of-the-art methods on the MPII and FLIC datasets.

Robust Adversarial Perturbation on Deep Proposal-based Models

  Adversarial noises are useful tools to probe the weakness of deep learningbased computer vision algorithms. In this paper, we describe a robustadversarial perturbation (R-AP) method to attack deep proposal-based objectdetectors and instance segmentation algorithms. Our method focuses on attackingthe common component in these algorithms, namely Region Proposal Network (RPN),to universally degrade their performance in a black-box fashion. To do so, wedesign a loss function that combines a label loss and a novel shape loss, andoptimize it with respect to image using a gradient based iterative algorithm.Evaluations are performed on the MS COCO 2014 dataset for the adversarialattacking of 6 state-of-the-art object detectors and 2 instance segmentationalgorithms. Experimental results demonstrate the efficacy of the proposedmethod.

Evolvement Constrained Adversarial Learning for Video Style Transfer

  Video style transfer is a useful component for applications such as augmentedreality, non-photorealistic rendering, and interactive games. Many existingmethods use optical flow to preserve the temporal smoothness of the synthesizedvideo. However, the estimation of optical flow is sensitive to occlusions andrapid motions. Thus, in this work, we introduce a novel evolve-sync losscomputed by evolvements to replace optical flow. Using this evolve-sync loss,we build an adversarial learning framework, termed as Video Style TransferGenerative Adversarial Network (VST-GAN), which improves upon the MGAN methodfor image style transfer for more efficient video style transfer. We performextensive experimental evaluations of our method and show quantitative andqualitative improvements over the state-of-the-art methods.

Learning Non-Uniform Hypergraph for Multi-Object Tracking

  The majority of Multi-Object Tracking (MOT) algorithms based on thetracking-by-detection scheme do not use higher order dependencies among objectsor tracklets, which makes them less effective in handling complex scenarios. Inthis work, we present a new near-online MOT algorithm based on non-uniformhypergraph, which can model different degrees of dependencies among trackletsin a unified objective. The nodes in the hypergraph correspond to the trackletsand the hyperedges with different degrees encode various kinds of dependenciesamong them. Specifically, instead of setting the weights of hyperedges withdifferent degrees empirically, they are learned automatically using thestructural support vector machine algorithm (SSVM). Several experiments arecarried out on various challenging datasets (i.e., PETS09, ParkingLot sequence,SubwayFace, and MOT16 benchmark), to demonstrate that our method achievesfavorable performance against the state-of-the-art MOT methods.

Residual Attention based Network for Hand Bone Age Assessment

  Computerized automatic methods have been employed to boost the productivityas well as objectiveness of hand bone age assessment. These approaches makepredictions according to the whole X-ray images, which include other objectsthat may introduce distractions. Instead, our framework is inspired by theclinical workflow (Tanner-Whitehouse) of hand bone age assessment, whichfocuses on the key components of the hand. The proposed framework is composedof two components: a Mask R-CNN subnet of pixelwise hand segmentation and aresidual attention network for hand bone age assessment. The Mask R-CNN subnetsegments the hands from X-ray images to avoid the distractions of other objects(e.g., X-ray tags). The hierarchical attention components of the residualattention subnet force our network to focus on the key components of the X-rayimages and generate the final predictions as well as the associated visualsupports, which is similar to the assessment procedure of clinicians. Weevaluate the performance of the proposed pipeline on the RSNA pediatric boneage dataset and the results demonstrate its superiority over the previousmethods.

Attention-driven Tree-structured Convolutional LSTM for High Dimensional  Data Understanding

  Modeling the sequential information of image sequences has been a vital stepof various vision tasks and convolutional long short-term memory (ConvLSTM) hasdemonstrated its superb performance in such spatiotemporal problems.Nevertheless, the hierarchical data structures in a significant amount of tasks(e.g., human body parts and vessel/airway tree in biomedical images) cannot beproperly modeled by sequential models. Thus, ConvLSTM is not suitable fortree-structured image data analysis. In order to address these limitations, wepresent tree-structured ConvLSTM models for tree-structured image analysistasks which can be trained end-to-end. To demonstrate the effectiveness of theproposed tree-structured ConvLSTM model, we present a tree-structuredsegmentation framework which consists of a tree-structured ConvLSTM and anattention fully convolutional network (FCN) model. The proposed framework isextensively validated on four large-scale coronary artery datasets. The resultsdemonstrate the effectiveness and efficiency of the proposed method.

Object-driven Text-to-Image Synthesis via Adversarial Training

  In this paper, we propose Object-driven Attentive Generative AdversarialNewtorks (Obj-GANs) that allow object-centered text-to-image synthesis forcomplex scenes. Following the two-step (layout-image) generation process, anovel object-driven attentive image generator is proposed to synthesize salientobjects by paying attention to the most relevant words in the text descriptionand the pre-generated semantic layout. In addition, a new Fast R-CNN basedobject-wise discriminator is proposed to provide rich object-wisediscrimination signals on whether the synthesized object matches the textdescription and the pre-generated layout. The proposed Obj-GAN significantlyoutperforms the previous state of the art in various metrics on the large-scaleCOCO benchmark, increasing the Inception score by 27% and decreasing the FIDscore by 11%. A thorough comparison between the traditional grid attention andthe new object-driven attention is provided through analyzing their mechanismsand visualizing their attention layers, showing insights of how the proposedmodel generates complex scenes in high quality.

Multi-Scale Structure-Aware Network for Human Pose Estimation

  We develop a robust multi-scale structure-aware neural network for human poseestimation. This method improves the recent deep conv-deconv hourglass modelswith four key improvements: (1) multi-scale supervision to strengthencontextual feature learning in matching body keypoints by combining featureheatmaps across scales, (2) multi-scale regression network at the end toglobally optimize the structural matching of the multi-scale features, (3)structure-aware loss used in the intermediate supervision and at the regressionto improve the matching of keypoints and respective neighbors to infer ahigher-order matching configurations, and (4) a keypoint masking trainingscheme that can effectively fine-tune our network to robustly localize occludedkeypoints via adjacent matches. Our method can effectively improvestate-of-the-art pose estimation methods that suffer from difficulties in scalevarieties, occlusions, and complex multi-person scenarios. This multi-scalesupervision tightly integrates with the regression network to effectively (i)localize keypoints using the ensemble of multi-scale features, and (ii) inferglobal pose configuration by maximizing structural consistencies acrossmultiple keypoints and scales. The keypoint masking training enhances theseadvantages to focus learning on hard occlusion samples. Our method achieves theleading position in the MPII challenge leaderboard among the state-of-the-artmethods.

Exploring the Vulnerability of Single Shot Module in Object Detectors  via Imperceptible Background Patches

  Recent works succeeded to generate adversarial perturbations on the entireimage or the object of interests to corrupt CNN based object detectors. In thispaper, we focus on exploring the vulnerability of the Single Shot Module (SSM)commonly used in recent object detectors, by adding small perturbations topatches in the background outside the object. The SSM is referred to the RegionProposal Network used in a two-stage object detector or the single-stage objectdetector itself. The SSM is typically a fully convolutional neural networkwhich generates output in a single forward pass. Due to the excessiveconvolutions used in SSM, the actual receptive field is larger than the objectitself. As such, we propose a novel method to corrupt object detectors bygenerating imperceptible patches only in the background. Our method can find afew background patches for perturbation, which can effectively decrease truepositives and dramatically increase false positives. Efficacy is demonstratedon 5 two-stage object detectors and 8 single-stage object detectors on the MSCOCO 2014 dataset. Results indicate that perturbations with small distortionsoutside the bounding box of object region can still severely damage thedetection performance.

Exposing DeepFake Videos By Detecting Face Warping Artifacts

  In this work, we describe a new deep learning based method that caneffectively distinguish AI-generated fake videos (referred to as {\em DeepFake}videos hereafter) from real videos. Our method is based on the observationsthat current DeepFake algorithm can only generate images of limitedresolutions, which need to be further warped to match the original faces in thesource video. Such transforms leave distinctive artifacts in the resultingDeepFake videos, and we show that they can be effectively captured byconvolutional neural networks (CNNs). Compared to previous methods which use alarge amount of real and DeepFake generated images to train CNN classifier, ourmethod does not need DeepFake generated images as negative training examplessince we target the artifacts in affine face warping as the distinctive featureto distinguish real and fake images. The advantages of our method are two-fold:(1) Such artifacts can be simulated directly using simple image processingoperations on a image to make it as negative example. Since training a DeepFakemodel to generate negative examples is time-consuming and resource-demanding,our method saves a plenty of time and resources in training data collection;(2) Since such artifacts are general existed in DeepFake videos from differentsources, our method is more robust compared to others. Our method is evaluatedon two sets of DeepFake video datasets for its effectiveness in practice.

Data Priming Network for Automatic Check-Out

  Automatic Check-Out (ACO) receives increased interests in recent years. Animportant component of the ACO system is the visual item counting, whichrecognize the categories and counts of the items chosen by the customers.However, the training of such a system is challenged by the domain adaptationproblem, in which the training data are images from isolated items while thetesting images are for collections of items. Existing methods solve thisproblem with data augmentation using synthesized images, but the imagesynthesis leads to unreal images that affect the training process. In thispaper, we propose a new data priming method to solve the domain adaptationproblem. Specifically, we first use pre-augmentation data priming, in which weremove distracting background from the training images and select images withrealistic view angles by the pose pruning method. In the post-augmentationstep, we train a data priming network using detection and countingcollaborative learning, and select more reliable images from testing data totrain the final visual item tallying network. Experiments on the large scaleRetail Product Checkout (RPC) dataset demonstrate the superiority of theproposed method, i.e., we achieve 80.51% checkout accuracy compared with 56.68%of the baseline methods.

Multi-label Learning with Missing Labels using Mixed Dependency Graphs

  This work focuses on the problem of multi-label learning with missing labels(MLML), which aims to label each test instance with multiple class labels giventraining instances that have an incomplete/partial set of these labels. The keypoint to handle missing labels is propagating the label information fromprovided labels to missing labels, through a dependency graph that each labelof each instance is treated as a node. We build this graph by utilizingdifferent types of label dependencies. Specifically, the instance-levelsimilarity is served as undirected edges to connect the label nodes acrossdifferent instances and the semantic label hierarchy is used as directed edgesto connect different classes. This base graph is referred to as the mixeddependency graph, as it includes both undirected and directed edges.Furthermore, we present another two types of label dependencies to connect thelabel nodes across different classes. One is the class co-occurrence, which isalso encoded as undirected edges. Combining with the base graph, we obtain anew mixed graph, called MG-CO (mixed graph with co-occurrence). The other isthe sparse and low rank decomposition of the whole label matrix, to embedhigh-order dependencies over all labels. Combining with the base graph, the newmixed graph is called as MG-SL (mixed graph with sparse and low rankdecomposition). Based on MG-CO and MG-SL, we propose two convex transductiveformulations of the MLML problem, denoted as MLMG-CO and MLMG-SL, respectively.Two important applications, including image annotation and tag based imageretrieval, can be jointly handled using our proposed methods. Experiments onbenchmark datasets show that our methods give significant improvements inperformance and robustness to missing labels over the state-of-the-art methods.

