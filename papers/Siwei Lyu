Interpretation and Generalization of Score Matching

  Score matching is a recently developed parameter learning method that is
particularly effective to complicated high dimensional density models with
intractable partition functions. In this paper, we study two issues that have
not been completely resolved for score matching. First, we provide a formal
link between maximum likelihood and score matching. Our analysis shows that
score matching finds model parameters that are more robust with noisy training
data. Second, we develop a generalization of score matching. Based on this
generalization, we further demonstrate an extension of score matching to models
of discrete data.


In Ictu Oculi: Exposing AI Generated Fake Face Videos by Detecting Eye
  Blinking

  The new developments in deep generative networks have significantly improve
the quality and efficiency in generating realistically-looking fake face
videos. In this work, we describe a new method to expose fake face videos
generated with neural networks. Our method is based on detection of eye
blinking in the videos, which is a physiological signal that is not well
presented in the synthesized fake videos. Our method is tested over benchmarks
of eye-blinking detection datasets and also show promising performance on
detecting videos generated with DeepFake.


Contrast Enhancement Estimation for Digital Image Forensics

  Inconsistency in contrast enhancement can be used to expose image forgeries.
In this work, we describe a new method to estimate contrast enhancement from a
single image. Our method takes advantage of the nature of contrast enhancement
as a mapping between pixel values, and the distinct characteristics it
introduces to the image pixel histogram. Our method recovers the original pixel
histogram and the contrast enhancement simultaneously from a single image with
an iterative algorithm. Unlike previous methods, our method is robust in the
presence of additive noise perturbations that are used to hide the traces of
contrast enhancement. Furthermore, we also develop an e effective method to to
detect image regions undergone contrast enhancement transformations that are
different from the rest of the image, and use this method to detect composite
images. We perform extensive experimental evaluations to demonstrate the
efficacy and efficiency of our method method.


A Univariate Bound of Area Under ROC

  Area under ROC (AUC) is an important metric for binary classification and
bipartite ranking problems. However, it is difficult to directly optimizing AUC
as a learning objective, so most existing algorithms are based on optimizing a
surrogate loss to AUC. One significant drawback of these surrogate losses is
that they require pairwise comparisons among training data, which leads to slow
running time and increasing local storage for online learning. In this work, we
describe a new surrogate loss based on a reformulation of the AUC risk, which
does not require pairwise comparison but rankings of the predictions. We
further show that the ranking operation can be avoided, and the learning
objective obtained based on this surrogate enjoys linear complexity in time and
storage. We perform experiments to demonstrate the effectiveness of the online
and batch algorithms for AUC optimization based on the proposed surrogate loss.


De-identification without losing faces

  Training of deep learning models for computer vision requires large image or
video datasets from real world. Often, in collecting such datasets, we need to
protect the privacy of the people captured in the images or videos, while still
preserve the useful attributes such as facial expressions. In this work, we
describe a new face de-identification method that can preserve essential facial
attributes in the faces while concealing the identities. Our method takes
advantage of the recent advances in face attribute transfer models, while
maintaining a high visual quality. Instead of changing factors of the original
faces or synthesizing faces completely, our method use a trained facial
attribute transfer model to map non-identity related facial attributes to the
face of donors, who are a small number (usually 2 to 3) of consented subjects.
Using the donors' faces ensures that the natural appearance of the synthesized
faces, while ensuring the identity of the synthesized faces are changed. On the
other hand, the FATM blends the donors' facial attributes to those of the
original faces to diversify the appearance of the synthesized faces.
Experimental results on several sets of images and videos demonstrate the
effectiveness of our face de-ID algorithm.


LSTM with Working Memory

  Previous RNN architectures have largely been superseded by LSTM, or "Long
Short-Term Memory". Since its introduction, there have been many variations on
this simple design. However, it is still widely used and we are not aware of a
gated-RNN architecture that outperforms LSTM in a broad sense while still being
as simple and efficient. In this paper we propose a modified LSTM-like
architecture. Our architecture is still simple and achieves better performance
on the tasks that we tested on. We also introduce a new RNN performance
benchmark that uses the handwritten digits and stresses several important
network capabilities.


Exposing Deep Fakes Using Inconsistent Head Poses

  In this paper, we propose a new method to expose AI-generated fake face
images or videos (commonly known as the Deep Fakes). Our method is based on the
observations that Deep Fakes are created by splicing synthesized face region
into the original image, and in doing so, introducing errors that can be
revealed when 3D head poses are estimated from the face images. We perform
experiments to demonstrate this phenomenon and further develop a classification
method based on this cue. Using features based on this cue, an SVM classifier
is evaluated using a set of real face images and Deep Fakes.


Exposing GAN-synthesized Faces Using Landmark Locations

  Generative adversary networks (GANs) have recently led to highly realistic
image synthesis results. In this work, we describe a new method to expose
GAN-synthesized images using the locations of the facial landmark points. Our
method is based on the observations that the facial parts configuration
generated by GAN models are different from those of the real faces, due to the
lack of global constraints. We perform experiments demonstrating this
phenomenon, and show that an SVM classifier trained using the locations of
facial landmark points is sufficient to achieve good classification performance
for GAN-synthesized faces.


UA-DETRAC: A New Benchmark and Protocol for Multi-Object Detection and
  Tracking

  In recent years, numerous effective multi-object tracking (MOT) methods are
developed because of the wide range of applications. Existing performance
evaluations of MOT methods usually separate the object tracking step from the
object detection step by using the same fixed object detection results for
comparisons. In this work, we perform a comprehensive quantitative study on the
effects of object detection accuracy to the overall MOT performance, using the
new large-scale University at Albany DETection and tRACking (UA-DETRAC)
benchmark dataset. The UA-DETRAC benchmark dataset consists of 100 challenging
video sequences captured from real-world traffic scenes (over 140,000 frames
with rich annotations, including occlusion, weather, vehicle category,
truncation, and vehicle bounding boxes) for object detection, object tracking
and MOT system. We evaluate complete MOT systems constructed from combinations
of state-of-the-art object detection and object tracking methods. Our analysis
shows the complex effects of object detection accuracy on MOT system
performance. Based on these observations, we propose new evaluation tools and
metrics for MOT systems that consider both object detection and object tracking
for comprehensive analysis.


Geometric Hypergraph Learning for Visual Tracking

  Graph based representation is widely used in visual tracking field by finding
correct correspondences between target parts in consecutive frames. However,
most graph based trackers consider pairwise geometric relations between local
parts. They do not make full use of the target's intrinsic structure, thereby
making the representation easily disturbed by errors in pairwise affinities
when large deformation and occlusion occur. In this paper, we propose a
geometric hypergraph learning based tracking method, which fully exploits
high-order geometric relations among multiple correspondences of parts in
consecutive frames. Then visual tracking is formulated as the mode-seeking
problem on the hypergraph in which vertices represent correspondence hypotheses
and hyperedges describe high-order geometric relations. Besides, a
confidence-aware sampling method is developed to select representative vertices
and hyperedges to construct the geometric hypergraph for more robustness and
scalability. The experiments are carried out on two challenging datasets
(VOT2014 and Deform-SOT) to demonstrate that the proposed method performs
favorable against other existing trackers.


Improving Image Restoration with Soft-Rounding

  Several important classes of images such as text, barcode and pattern images
have the property that pixels can only take a distinct subset of values. This
knowledge can benefit the restoration of such images, but it has not been
widely considered in current restoration methods. In this work, we describe an
effective and efficient approach to incorporate the knowledge of distinct pixel
values of the pristine images into the general regularized least squares
restoration framework. We introduce a new regularizer that attains zero at the
designated pixel values and becomes a quadratic penalty function in the
intervals between them. When incorporated into the regularized least squares
restoration framework, this regularizer leads to a simple and efficient step
that resembles and extends the rounding operation, which we term as
soft-rounding. We apply the soft-rounding enhanced solution to the restoration
of binary text/barcode images and pattern images with multiple distinct pixel
values. Experimental results show that soft-rounding enhanced restoration
methods achieve significant improvement in both visual quality and quantitative
measures (PSNR and SSIM). Furthermore, we show that this regularizer can also
benefit the restoration of general natural images.


Robust Localized Multi-view Subspace Clustering

  In multi-view clustering, different views may have different confidence
levels when learning a consensus representation. Existing methods usually
address this by assigning distinctive weights to different views. However, due
to noisy nature of real-world applications, the confidence levels of samples in
the same view may also vary. Thus considering a unified weight for a view may
lead to suboptimal solutions. In this paper, we propose a novel localized
multi-view subspace clustering model that considers the confidence levels of
both views and samples. By assigning weight to each sample under each view
properly, we can obtain a robust consensus representation via fusing the
noiseless structures among views and samples. We further develop a regularizer
on weight parameters based on the convex conjugacy theory, and samples weights
are determined in an adaptive manner. An efficient iterative algorithm is
developed with a convergence guarantee. Experimental results on four benchmarks
demonstrate the correctness and effectiveness of the proposed model.


Learning with Average Top-k Loss

  In this work, we introduce the {\em average top-$k$} (\atk) loss as a new
aggregate loss for supervised learning, which is the average over the $k$
largest individual losses over a training dataset. We show that the \atk loss
is a natural generalization of the two widely used aggregate losses, namely the
average loss and the maximum loss, but can combine their advantages and
mitigate their drawbacks to better adapt to different data distributions.
Furthermore, it remains a convex function over all individual losses, which can
lead to convex optimization problems that can be solved effectively with
conventional gradient-based methods. We provide an intuitive interpretation of
the \atk loss based on its equivalent effect on the continuous individual loss
functions, suggesting that it can reduce the penalty on correctly classified
data. We further give a learning theory analysis of \matk learning on the
classification calibration of the \atk loss and the error bounds of \atk-SVM.
We demonstrate the applicability of minimum average top-$k$ learning for binary
classification and regression using synthetic and real datasets.


Tagging like Humans: Diverse and Distinct Image Annotation

  In this work we propose a new automatic image annotation model, dubbed {\bf
diverse and distinct image annotation} (D2IA). The generative model D2IA is
inspired by the ensemble of human annotations, which create semantically
relevant, yet distinct and diverse tags. In D2IA, we generate a relevant and
distinct tag subset, in which the tags are relevant to the image contents and
semantically distinct to each other, using sequential sampling from a
determinantal point process (DPP) model. Multiple such tag subsets that cover
diverse semantic aspects or diverse semantic levels of the image contents are
generated by randomly perturbing the DPP sampling process. We leverage a
generative adversarial network (GAN) model to train D2IA. Extensive experiments
including quantitative and qualitative comparisons, as well as human subject
studies, on two benchmark datasets demonstrate that the proposed model can
produce more diverse and distinct tags than the state-of-the-arts.


STS Classification with Dual-stream CNN

  The structured time series (STS) classification problem requires the modeling
of interweaved spatiotemporal dependency. most previous STS classification
methods model the spatial and temporal dependencies independently. Due to the
complexity of the STS data, we argue that a desirable STS classification method
should be a holistic framework that can be made as adaptive and flexible as
possible. This motivates us to design a deep neural network with such merits.
Inspired by the dual-stream hypothesis in neural science, we propose a novel
dual-stream framework for modeling the interweaved spatiotemporal dependency,
and develop a convolutional neural network within this framework that aims to
achieve high adaptability and flexibility in STS configurations from various
diagonals, i.e., sequential order, dependency range and features. The proposed
architecture is highly modularized and scalable, making it easy to be adapted
to specific tasks. The effectiveness of our model is demonstrated through
experiments on synthetic data as well as benchmark datasets for skeleton based
activity recognition.


Who did What at Where and When: Simultaneous Multi-Person Tracking and
  Activity Recognition

  We present a bootstrapping framework to simultaneously improve multi-person
tracking and activity recognition at individual, interaction and social group
activity levels. The inference consists of identifying trajectories of all
pedestrian actors, individual activities, pairwise interactions, and collective
activities, given the observed pedestrian detections. Our method uses a
graphical model to represent and solve the joint tracking and recognition
problems via multi-stages: (1) activity-aware tracking, (2) joint interaction
recognition and occlusion recovery, and (3) collective activity recognition. We
solve the where and when problem with visual tracking, as well as the who and
what problem with recognition. High-order correlations among the visible and
occluded individuals, pairwise interactions, groups, and activities are then
solved using a hypergraph formulation within the Bayesian framework.
Experiments on several benchmarks show the advantages of our approach over
state-of-art methods.


Multi-Scale Supervised Network for Human Pose Estimation

  Human pose estimation is an important topic in computer vision with many
applications including gesture and activity recognition. However, pose
estimation from image is challenging due to appearance variations, occlusions,
clutter background, and complex activities. To alleviate these problems, we
develop a robust pose estimation method based on the recent deep conv-deconv
modules with two improvements: (1) multi-scale supervision of body keypoints,
and (2) a global regression to improve structural consistency of keypoints. We
refine keypoint detection heatmaps using layer-wise multi-scale supervision to
better capture local contexts. Pose inference via keypoint association is
optimized globally using a regression network at the end. Our method can
effectively disambiguate keypoint matches in close proximity including the
mismatch of left-right body parts, and better infer occluded parts.
Experimental results show that our method achieves competitive performance
among state-of-the-art methods on the MPII and FLIC datasets.


Robust Adversarial Perturbation on Deep Proposal-based Models

  Adversarial noises are useful tools to probe the weakness of deep learning
based computer vision algorithms. In this paper, we describe a robust
adversarial perturbation (R-AP) method to attack deep proposal-based object
detectors and instance segmentation algorithms. Our method focuses on attacking
the common component in these algorithms, namely Region Proposal Network (RPN),
to universally degrade their performance in a black-box fashion. To do so, we
design a loss function that combines a label loss and a novel shape loss, and
optimize it with respect to image using a gradient based iterative algorithm.
Evaluations are performed on the MS COCO 2014 dataset for the adversarial
attacking of 6 state-of-the-art object detectors and 2 instance segmentation
algorithms. Experimental results demonstrate the efficacy of the proposed
method.


Evolvement Constrained Adversarial Learning for Video Style Transfer

  Video style transfer is a useful component for applications such as augmented
reality, non-photorealistic rendering, and interactive games. Many existing
methods use optical flow to preserve the temporal smoothness of the synthesized
video. However, the estimation of optical flow is sensitive to occlusions and
rapid motions. Thus, in this work, we introduce a novel evolve-sync loss
computed by evolvements to replace optical flow. Using this evolve-sync loss,
we build an adversarial learning framework, termed as Video Style Transfer
Generative Adversarial Network (VST-GAN), which improves upon the MGAN method
for image style transfer for more efficient video style transfer. We perform
extensive experimental evaluations of our method and show quantitative and
qualitative improvements over the state-of-the-art methods.


Learning Non-Uniform Hypergraph for Multi-Object Tracking

  The majority of Multi-Object Tracking (MOT) algorithms based on the
tracking-by-detection scheme do not use higher order dependencies among objects
or tracklets, which makes them less effective in handling complex scenarios. In
this work, we present a new near-online MOT algorithm based on non-uniform
hypergraph, which can model different degrees of dependencies among tracklets
in a unified objective. The nodes in the hypergraph correspond to the tracklets
and the hyperedges with different degrees encode various kinds of dependencies
among them. Specifically, instead of setting the weights of hyperedges with
different degrees empirically, they are learned automatically using the
structural support vector machine algorithm (SSVM). Several experiments are
carried out on various challenging datasets (i.e., PETS09, ParkingLot sequence,
SubwayFace, and MOT16 benchmark), to demonstrate that our method achieves
favorable performance against the state-of-the-art MOT methods.


Residual Attention based Network for Hand Bone Age Assessment

  Computerized automatic methods have been employed to boost the productivity
as well as objectiveness of hand bone age assessment. These approaches make
predictions according to the whole X-ray images, which include other objects
that may introduce distractions. Instead, our framework is inspired by the
clinical workflow (Tanner-Whitehouse) of hand bone age assessment, which
focuses on the key components of the hand. The proposed framework is composed
of two components: a Mask R-CNN subnet of pixelwise hand segmentation and a
residual attention network for hand bone age assessment. The Mask R-CNN subnet
segments the hands from X-ray images to avoid the distractions of other objects
(e.g., X-ray tags). The hierarchical attention components of the residual
attention subnet force our network to focus on the key components of the X-ray
images and generate the final predictions as well as the associated visual
supports, which is similar to the assessment procedure of clinicians. We
evaluate the performance of the proposed pipeline on the RSNA pediatric bone
age dataset and the results demonstrate its superiority over the previous
methods.


Attention-driven Tree-structured Convolutional LSTM for High Dimensional
  Data Understanding

  Modeling the sequential information of image sequences has been a vital step
of various vision tasks and convolutional long short-term memory (ConvLSTM) has
demonstrated its superb performance in such spatiotemporal problems.
Nevertheless, the hierarchical data structures in a significant amount of tasks
(e.g., human body parts and vessel/airway tree in biomedical images) cannot be
properly modeled by sequential models. Thus, ConvLSTM is not suitable for
tree-structured image data analysis. In order to address these limitations, we
present tree-structured ConvLSTM models for tree-structured image analysis
tasks which can be trained end-to-end. To demonstrate the effectiveness of the
proposed tree-structured ConvLSTM model, we present a tree-structured
segmentation framework which consists of a tree-structured ConvLSTM and an
attention fully convolutional network (FCN) model. The proposed framework is
extensively validated on four large-scale coronary artery datasets. The results
demonstrate the effectiveness and efficiency of the proposed method.


Object-driven Text-to-Image Synthesis via Adversarial Training

  In this paper, we propose Object-driven Attentive Generative Adversarial
Newtorks (Obj-GANs) that allow object-centered text-to-image synthesis for
complex scenes. Following the two-step (layout-image) generation process, a
novel object-driven attentive image generator is proposed to synthesize salient
objects by paying attention to the most relevant words in the text description
and the pre-generated semantic layout. In addition, a new Fast R-CNN based
object-wise discriminator is proposed to provide rich object-wise
discrimination signals on whether the synthesized object matches the text
description and the pre-generated layout. The proposed Obj-GAN significantly
outperforms the previous state of the art in various metrics on the large-scale
COCO benchmark, increasing the Inception score by 27% and decreasing the FID
score by 11%. A thorough comparison between the traditional grid attention and
the new object-driven attention is provided through analyzing their mechanisms
and visualizing their attention layers, showing insights of how the proposed
model generates complex scenes in high quality.


Multi-Scale Structure-Aware Network for Human Pose Estimation

  We develop a robust multi-scale structure-aware neural network for human pose
estimation. This method improves the recent deep conv-deconv hourglass models
with four key improvements: (1) multi-scale supervision to strengthen
contextual feature learning in matching body keypoints by combining feature
heatmaps across scales, (2) multi-scale regression network at the end to
globally optimize the structural matching of the multi-scale features, (3)
structure-aware loss used in the intermediate supervision and at the regression
to improve the matching of keypoints and respective neighbors to infer a
higher-order matching configurations, and (4) a keypoint masking training
scheme that can effectively fine-tune our network to robustly localize occluded
keypoints via adjacent matches. Our method can effectively improve
state-of-the-art pose estimation methods that suffer from difficulties in scale
varieties, occlusions, and complex multi-person scenarios. This multi-scale
supervision tightly integrates with the regression network to effectively (i)
localize keypoints using the ensemble of multi-scale features, and (ii) infer
global pose configuration by maximizing structural consistencies across
multiple keypoints and scales. The keypoint masking training enhances these
advantages to focus learning on hard occlusion samples. Our method achieves the
leading position in the MPII challenge leaderboard among the state-of-the-art
methods.


Exploring the Vulnerability of Single Shot Module in Object Detectors
  via Imperceptible Background Patches

  Recent works succeeded to generate adversarial perturbations on the entire
image or the object of interests to corrupt CNN based object detectors. In this
paper, we focus on exploring the vulnerability of the Single Shot Module (SSM)
commonly used in recent object detectors, by adding small perturbations to
patches in the background outside the object. The SSM is referred to the Region
Proposal Network used in a two-stage object detector or the single-stage object
detector itself. The SSM is typically a fully convolutional neural network
which generates output in a single forward pass. Due to the excessive
convolutions used in SSM, the actual receptive field is larger than the object
itself. As such, we propose a novel method to corrupt object detectors by
generating imperceptible patches only in the background. Our method can find a
few background patches for perturbation, which can effectively decrease true
positives and dramatically increase false positives. Efficacy is demonstrated
on 5 two-stage object detectors and 8 single-stage object detectors on the MS
COCO 2014 dataset. Results indicate that perturbations with small distortions
outside the bounding box of object region can still severely damage the
detection performance.


Exposing DeepFake Videos By Detecting Face Warping Artifacts

  In this work, we describe a new deep learning based method that can
effectively distinguish AI-generated fake videos (referred to as {\em DeepFake}
videos hereafter) from real videos. Our method is based on the observations
that current DeepFake algorithm can only generate images of limited
resolutions, which need to be further warped to match the original faces in the
source video. Such transforms leave distinctive artifacts in the resulting
DeepFake videos, and we show that they can be effectively captured by
convolutional neural networks (CNNs). Compared to previous methods which use a
large amount of real and DeepFake generated images to train CNN classifier, our
method does not need DeepFake generated images as negative training examples
since we target the artifacts in affine face warping as the distinctive feature
to distinguish real and fake images. The advantages of our method are two-fold:
(1) Such artifacts can be simulated directly using simple image processing
operations on a image to make it as negative example. Since training a DeepFake
model to generate negative examples is time-consuming and resource-demanding,
our method saves a plenty of time and resources in training data collection;
(2) Since such artifacts are general existed in DeepFake videos from different
sources, our method is more robust compared to others. Our method is evaluated
on two sets of DeepFake video datasets for its effectiveness in practice.


Data Priming Network for Automatic Check-Out

  Automatic Check-Out (ACO) receives increased interests in recent years. An
important component of the ACO system is the visual item counting, which
recognize the categories and counts of the items chosen by the customers.
However, the training of such a system is challenged by the domain adaptation
problem, in which the training data are images from isolated items while the
testing images are for collections of items. Existing methods solve this
problem with data augmentation using synthesized images, but the image
synthesis leads to unreal images that affect the training process. In this
paper, we propose a new data priming method to solve the domain adaptation
problem. Specifically, we first use pre-augmentation data priming, in which we
remove distracting background from the training images and select images with
realistic view angles by the pose pruning method. In the post-augmentation
step, we train a data priming network using detection and counting
collaborative learning, and select more reliable images from testing data to
train the final visual item tallying network. Experiments on the large scale
Retail Product Checkout (RPC) dataset demonstrate the superiority of the
proposed method, i.e., we achieve 80.51% checkout accuracy compared with 56.68%
of the baseline methods.


Multi-label Learning with Missing Labels using Mixed Dependency Graphs

  This work focuses on the problem of multi-label learning with missing labels
(MLML), which aims to label each test instance with multiple class labels given
training instances that have an incomplete/partial set of these labels. The key
point to handle missing labels is propagating the label information from
provided labels to missing labels, through a dependency graph that each label
of each instance is treated as a node. We build this graph by utilizing
different types of label dependencies. Specifically, the instance-level
similarity is served as undirected edges to connect the label nodes across
different instances and the semantic label hierarchy is used as directed edges
to connect different classes. This base graph is referred to as the mixed
dependency graph, as it includes both undirected and directed edges.
Furthermore, we present another two types of label dependencies to connect the
label nodes across different classes. One is the class co-occurrence, which is
also encoded as undirected edges. Combining with the base graph, we obtain a
new mixed graph, called MG-CO (mixed graph with co-occurrence). The other is
the sparse and low rank decomposition of the whole label matrix, to embed
high-order dependencies over all labels. Combining with the base graph, the new
mixed graph is called as MG-SL (mixed graph with sparse and low rank
decomposition). Based on MG-CO and MG-SL, we propose two convex transductive
formulations of the MLML problem, denoted as MLMG-CO and MLMG-SL, respectively.
Two important applications, including image annotation and tag based image
retrieval, can be jointly handled using our proposed methods. Experiments on
benchmark datasets show that our methods give significant improvements in
performance and robustness to missing labels over the state-of-the-art methods.


