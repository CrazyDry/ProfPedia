Vanishing point detection with convolutional neural networks

  Inspired by the finding that vanishing point (road tangent) guides driver's
gaze, in our previous work we showed that vanishing point attracts gaze during
free viewing of natural scenes as well as in visual search (Borji et al.,
Journal of Vision 2016). We have also introduced improved saliency models using
vanishing point detectors (Feng et al., WACV 2016). Here, we aim to predict
vanishing points in naturalistic environments by training convolutional neural
networks in an end-to-end manner over a large set of road images downloaded
from Youtube with vanishing points annotated. Results demonstrate effectiveness
of our approach compared to classic approaches of vanishing point detection in
the literature.


Vanishing Point Attracts Eye Movements in Scene Free-viewing

  Eye movements are crucial in understanding complex scenes. By predicting
where humans look in natural scenes, we can understand how they percieve scenes
and priotriaze information for further high-level processing. Here, we study
the effect of a particular type of scene structural information known as
vanishing point and show that human gaze is attracted to vanishing point
regions. We then build a combined model of traditional saliency and vanishing
point channel that outperforms state of the art saliency models.


Computational models of attention

  This chapter reviews recent computational models of visual attention. We
begin with models for the bottom-up or stimulus-driven guidance of attention to
salient visual items, which we examine in seven different broad categories. We
then examine more complex models which address the top-down or goal-oriented
guidance of attention towards items that are more relevant to the task at hand.


Bottom-up Attention, Models of

  In this review, we examine the recent progress in saliency prediction and
proposed several avenues for future research. In spite of tremendous efforts
and huge progress, there is still room for improvement in terms finer-grained
analysis of deep saliency models, evaluation measures, datasets, annotation
methods, cognitive studies, and new applications. This chapter will appear in
Encyclopedia of Computational Neuroscience.


CAT2000: A Large Scale Fixation Dataset for Boosting Saliency Research

  Saliency modeling has been an active research area in computer vision for
about two decades. Existing state of the art models perform very well in
predicting where people look in natural scenes. There is, however, the risk
that these models may have been overfitting themselves to available small scale
biased datasets, thus trapping the progress in a local minimum. To gain a
deeper insight regarding current issues in saliency modeling and to better
gauge progress, we recorded eye movements of 120 observers while they freely
viewed a large number of naturalistic and artificial images. Our stimuli
includes 4000 images; 200 from each of 20 categories covering different types
of scenes such as Cartoons, Art, Objects, Low resolution images, Indoor,
Outdoor, Jumbled, Random, and Line drawings. We analyze some basic properties
of this dataset and compare some successful models. We believe that our dataset
opens new challenges for the next generation of saliency models and helps
conduct behavioral studies on bottom-up visual attention.


Fixation prediction with a combined model of bottom-up saliency and
  vanishing point

  By predicting where humans look in natural scenes, we can understand how they
perceive complex natural scenes and prioritize information for further
high-level visual processing. Several models have been proposed for this
purpose, yet there is a gap between best existing saliency models and human
performance. While many researchers have developed purely computational models
for fixation prediction, less attempts have been made to discover cognitive
factors that guide gaze. Here, we study the effect of a particular type of
scene structural information, known as the vanishing point, and show that human
gaze is attracted to the vanishing point regions. We record eye movements of 10
observers over 532 images, out of which 319 have vanishing points. We then
construct a combined model of traditional saliency and a vanishing point
channel and show that our model outperforms state of the art saliency models
using three scores on our dataset.


Reconciling saliency and object center-bias hypotheses in explaining
  free-viewing fixations

  Predicting where people look in natural scenes has attracted a lot of
interest in computer vision and computational neuroscience over the past two
decades. Two seemingly contrasting categories of cues have been proposed to
influence where people look: \textit{low-level image saliency} and
\textit{high-level semantic information}. Our first contribution is to take a
detailed look at these cues to confirm the hypothesis proposed by
Henderson~\cite{henderson1993eye} and Nuthmann \&
Henderson~\cite{nuthmann2010object} that observers tend to look at the center
of objects. We analyzed fixation data for scene free-viewing over 17 observers
on 60 fully annotated images with various types of objects. Images contained
different types of scenes, such as natural scenes, line drawings, and 3D
rendered scenes. Our second contribution is to propose a simple combined model
of low-level saliency and object center-bias that outperforms each individual
component significantly over our data, as well as on the OSIE dataset by Xu et
al.~\cite{xu2014predicting}. The results reconcile saliency with object
center-bias hypotheses and highlight that both types of cues are important in
guiding fixations. Our work opens new directions to understand strategies that
humans use in observing scenes and objects, and demonstrates the construction
of combined models of low-level saliency and high-level object-based
information.


Salient Object Detection: A Survey

  Detecting and segmenting salient objects in natural scenes, often referred to
as salient object detection, has attracted a lot of interest in computer
vision. While many models have been proposed and several applications have
emerged, yet a deep understanding of achievements and issues is lacking. We aim
to provide a comprehensive review of the recent progress in salient object
detection and situate this field among other closely related areas such as
generic scene segmentation, object proposal generation, and saliency for
fixation prediction. Covering 228 publications, we survey i) roots, key
concepts, and tasks, ii) core techniques and main modeling trends, and iii)
datasets and evaluation metrics in salient object detection. We also discuss
open problems such as evaluation metrics and dataset bias in model performance
and suggest future research directions.


Computational models: Bottom-up and top-down aspects

  Computational models of visual attention have become popular over the past
decade, we believe primarily for two reasons: First, models make testable
predictions that can be explored by experimentalists as well as theoreticians,
second, models have practical and technological applications of interest to the
applied science and engineering communities. In this chapter, we take a
critical look at recent attention modeling efforts. We focus on {\em
computational models of attention} as defined by Tsotsos \& Rothenstein
\shortcite{Tsotsos_Rothenstein11}: Models which can process any visual stimulus
(typically, an image or video clip), which can possibly also be given some task
definition, and which make predictions that can be compared to human or animal
behavioral or physiological responses elicited by the same stimulus and task.
Thus, we here place less emphasis on abstract models, phenomenological models,
purely data-driven fitting or extrapolation models, or models specifically
designed for a single task or for a restricted class of stimuli. For
theoretical models, we refer the reader to a number of previous reviews that
address attention theories and models more generally
\cite{Itti_Koch01nrn,Paletta_etal05,Frintrop_etal10,Rothenstein_Tsotsos08,Gottlieb_Balan10,Toet11,Borji_Itti12pami}.


Negative Results in Computer Vision: A Perspective

  A negative result is when the outcome of an experiment or a model is not what
is expected or when a hypothesis does not hold. Despite being often overlooked
in the scientific community, negative results are results and they carry value.
While this topic has been extensively discussed in other fields such as social
sciences and biosciences, less attention has been paid to it in the computer
vision community. The unique characteristics of computer vision, particularly
its experimental aspect, call for a special treatment of this matter. In this
paper, I will address what makes negative results important, how they should be
disseminated and incentivized, and what lessons can be learned from cognitive
vision research in this regard. Further, I will discuss issues such as computer
vision and human vision interaction, experimental design and statistical
hypothesis testing, explanatory versus predictive modeling, performance
evaluation, model comparison, as well as computer vision research culture.


Pros and Cons of GAN Evaluation Measures

  Generative models, in particular generative adversarial networks (GANs), have
received significant attention recently. A number of GAN variants have been
proposed and have been utilized in many applications. Despite large strides in
terms of theoretical progress, evaluating and comparing GANs remains a daunting
task. While several measures have been introduced, as of yet, there is no
consensus as to which measure best captures strengths and limitations of models
and should be used for fair model comparison. As in other areas of computer
vision and machine learning, it is critical to settle on one or few good
measures to steer the progress in this field. In this paper, I review and
critically discuss more than 24 quantitative and 5 qualitative measures for
evaluating generative models with a particular emphasis on GAN-derived models.
I also provide a set of 7 desiderata followed by an evaluation of whether a
given measure or a family of measures is compatible with them.


Saliency Prediction in the Deep Learning Era: An Empirical Investigation

  Visual saliency models have enjoyed a big leap in performance in recent
years, thanks to advances in deep learning and large scale annotated data.
Despite enormous effort and huge breakthroughs, however, models still fall
short in reaching human-level accuracy. In this work, I explore the landscape
of the field emphasizing on new deep saliency models, benchmarks, and datasets.
A large number of image and video saliency models are reviewed and compared
over two image benchmarks and two large scale video datasets. Further, I
identify factors that contribute to the gap between models and humans and
discuss remaining issues that need to be addressed to build the next generation
of more powerful saliency models. Some specific questions that are addressed
include: in what ways current models fail, how to remedy them, what can be
learned from cognitive studies of attention, how explicit saliency judgments
relate to fixations, how to conduct fair model comparison, and what are the
emerging applications of saliency models.


What are the Receptive, Effective Receptive, and Projective Fields of
  Neurons in Convolutional Neural Networks?

  In this work, we explain in detail how receptive fields, effective receptive
fields, and projective fields of neurons in different layers, convolution or
pooling, of a Convolutional Neural Network (CNN) are calculated. While our
focus here is on CNNs, the same operations, but in the reverse order, can be
used to calculate these quantities for deconvolutional neural networks. These
are important concepts, not only for better understanding and analyzing
convolutional and deconvolutional networks, but also for optimizing their
performance in real-world applications.


Vanishing point attracts gaze in free-viewing and visual search tasks

  To investigate whether the vanishing point (VP) plays a significant role in
gaze guidance, we ran two experiments. In the first one, we recorded fixations
of 10 observers (4 female; mean age 22; SD=0.84) freely viewing 532 images, out
of which 319 had VP (shuffled presentation; each image for 4 secs). We found
that the average number of fixations at a local region (80x80 pixels) centered
at the VP is significantly higher than the average fixations at random
locations (t-test; n=319; p=1.8e-35). To address the confounding factor of
saliency, we learned a combined model of bottom-up saliency and VP. AUC score
of our model (0.85; SD=0.01) is significantly higher than the original saliency
model (e.g., 0.8 using AIM model by Bruce & Tsotsos (2009), t-test; p=
3.14e-16) and the VP-only model (0.64, t-test; p= 4.02e-22). In the second
experiment, we asked 14 subjects (4 female, mean age 23.07, SD=1.26) to search
for a target character (T or L) placed randomly on a 3x3 imaginary grid
overlaid on top of an image. Subjects reported their answers by pressing one of
two keys. Stimuli consisted of 270 color images (180 with a single VP, 90
without). The target happened with equal probability inside each cell (15 times
L, 15 times T). We found that subjects were significantly faster (and more
accurate) when target happened inside the cell containing the VP compared to
cells without VP (median across 14 subjects 1.34 sec vs. 1.96; Wilcoxon
rank-sum test; p = 0.0014). Response time at VP cells were also significantly
lower than response time on images without VP (median 2.37; p= 4.77e-05). These
findings support the hypothesis that vanishing point, similar to face and text
(Cerf et al., 2009) as well as gaze direction (Borji et al., 2014) attracts
attention in free-viewing and visual search.


What can we learn about CNNs from a large scale controlled object
  dataset?

  Tolerance to image variations (e.g. translation, scale, pose, illumination)
is an important desired property of any object recognition system, be it human
or machine. Moving towards increasingly bigger datasets has been trending in
computer vision specially with the emergence of highly popular deep learning
models. While being very useful for learning invariance to object inter- and
intra-class shape variability, these large-scale wild datasets are not very
useful for learning invariance to other parameters forcing researchers to
resort to other tricks for training a model. In this work, we introduce a
large-scale synthetic dataset, which is freely and publicly available, and use
it to answer several fundamental questions regarding invariance and selectivity
properties of convolutional neural networks. Our dataset contains two parts: a)
objects shot on a turntable: 16 categories, 8 rotation angles, 11 cameras on a
semicircular arch, 5 lighting conditions, 3 focus levels, variety of
backgrounds (23.4 per instance) generating 1320 images per instance (over 20
million images in total), and b) scenes: in which a robot arm takes pictures of
objects on a 1:160 scale scene. We study: 1) invariance and selectivity of
different CNN layers, 2) knowledge transfer from one object category to
another, 3) systematic or random sampling of images to build a train set, 4)
domain adaptation from synthetic to natural scenes, and 5) order of knowledge
delivery to CNNs. We also explore how our analyses can lead the field to
develop more efficient CNNs.


Human-like Clustering with Deep Convolutional Neural Networks

  Classification and clustering have been studied separately in machine
learning and computer vision. Inspired by the recent success of deep learning
models in solving various vision problems (e.g., object recognition, semantic
segmentation) and the fact that humans serve as the gold standard in assessing
clustering algorithms, here, we advocate for a unified treatment of the two
problems and suggest that hierarchical frameworks that progressively build
complex patterns on top of the simpler ones (e.g., convolutional neural
networks) offer a promising solution. We do not dwell much on the learning
mechanisms in these frameworks as they are still a matter of debate, with
respect to biological constraints. Instead, we emphasize on the
compositionality of the real world structures and objects. In particular, we
show that CNNs, trained end to end using back propagation with noisy labels,
are able to cluster data points belonging to several overlapping shapes, and do
so much better than the state of the art algorithms. The main takeaway lesson
from our study is that mechanisms of human vision, particularly the hierarchal
organization of the visual ventral stream should be taken into account in
clustering algorithms (e.g., for learning representations in an unsupervised
manner or with minimum supervision) to reach human level clustering
performance. This, by no means, suggests that other methods do not hold merits.
For example, methods relying on pairwise affinities (e.g., spectral clustering)
have been very successful in many scenarios but still fail in some cases (e.g.,
overlapping clusters).


Deeply supervised salient object detection with short connections

  Recent progress on saliency detection is substantial, benefiting mostly from
the explosive development of Convolutional Neural Networks (CNNs). Semantic
segmentation and saliency detection algorithms developed lately have been
mostly based on Fully Convolutional Neural Networks (FCNs). There is still a
large room for improvement over the generic FCN models that do not explicitly
deal with the scale-space problem. Holistically-Nested Edge Detector (HED)
provides a skip-layer structure with deep supervision for edge and boundary
detection, but the performance gain of HED on salience detection is not
obvious. In this paper, we propose a new method for saliency detection by
introducing short connections to the skip-layer structures within the HED
architecture. Our framework provides rich multi-scale feature maps at each
layer, a property that is critically needed to perform segment detection. Our
method produces state-of-the-art results on 5 widely tested salient object
detection benchmarks, with advantages in terms of efficiency (0.15 seconds per
image), effectiveness, and simplicity over the existing algorithms.


Protecting entanglement by adjusting the velocities of moving qubits
  inside non-Markovian environments

  Efficient entanglement preservation in open quantum systems is a crucial
scope towards a reliable exploitation of quantum resources. We address this
issue by studying how two-qubit entanglement dynamically behaves when two atom
qubits move inside two separated identical cavities. The moving qubits
independently interact with their respective cavity. As a main general result,
we find that under resonant qubit-cavity interaction the initial entanglement
between two moving qubits remains closer to its initial value as time passes
compared to the case of stationary qubits. In particular, we show that the
initial entanglement can be strongly protected from decay by suitably adjusting
the velocities of the qubits according to the non-Markovian features of the
cavities. Our results supply a further way of preserving quantum correlations
against noise with a natural implementation in cavity-QED scenarios and are
straightforwardly extendable to many qubits for scalability.


What is a salient object? A dataset and a baseline model for salient
  object detection

  Salient object detection or salient region detection models, diverging from
fixation prediction models, have traditionally been dealing with locating and
segmenting the most salient object or region in a scene. While the notion of
most salient object is sensible when multiple objects exist in a scene, current
datasets for evaluation of saliency detection approaches often have scenes with
only one single object. We introduce three main contributions in this paper:
First, we take an indepth look at the problem of salient object detection by
studying the relationship between where people look in scenes and what they
choose as the most salient object when they are explicitly asked. Based on the
agreement between fixations and saliency judgments, we then suggest that the
most salient object is the one that attracts the highest fraction of fixations.
Second, we provide two new less biased benchmark datasets containing scenes
with multiple objects that challenge existing saliency models. Indeed, we
observed a severe drop in performance of 8 state-of-the-art models on our
datasets (40% to 70%). Third, we propose a very simple yet powerful model based
on superpixels to be used as a baseline for model evaluation and comparison.
While on par with the best models on MSRA-5K dataset, our model wins over other
models on our data highlighting a serious drawback of existing models, which is
convoluting the processes of locating the most salient object and its
segmentation. We also provide a review and statistical analysis of some labeled
scene datasets that can be used for evaluating salient object detection models.
We believe that our work can greatly help remedy the over-fitting of models to
existing biased datasets and opens new venues for future research in this
fast-evolving field.


Paying Attention to Descriptions Generated by Image Captioning Models

  To bridge the gap between humans and machines in image understanding and
describing, we need further insight into how people describe a perceived scene.
In this paper, we study the agreement between bottom-up saliency-based visual
attention and object referrals in scene description constructs. We investigate
the properties of human-written descriptions and machine-generated ones. We
then propose a saliency-boosted image captioning model in order to investigate
benefits from low-level cues in language models. We learn that (1) humans
mention more salient objects earlier than less salient ones in their
descriptions, (2) the better a captioning model performs, the better attention
agreement it has with human descriptions, (3) the proposed saliency-boosted
model, compared to its baseline form, does not improve significantly on the MS
COCO database, indicating explicit bottom-up boosting does not help when the
task is well learnt and tuned on a data, (4) a better generalization is,
however, observed for the saliency-boosted model on unseen data.


Exploiting inter-image similarity and ensemble of extreme learners for
  fixation prediction using deep features

  This paper presents a novel fixation prediction and saliency modeling
framework based on inter-image similarities and ensemble of Extreme Learning
Machines (ELM). The proposed framework is inspired by two observations, 1) the
contextual information of a scene along with low-level visual cues modulates
attention, 2) the influence of scene memorability on eye movement patterns
caused by the resemblance of a scene to a former visual experience. Motivated
by such observations, we develop a framework that estimates the saliency of a
given image using an ensemble of extreme learners, each trained on an image
similar to the input image. That is, after retrieving a set of similar images
for a given image, a saliency predictor is learnt from each of the images in
the retrieved image set using an ELM, resulting in an ensemble. The saliency of
the given image is then measured in terms of the mean of predicted saliency
value by the ensemble's members.


Learning to predict where to look in interactive environments using deep
  recurrent q-learning

  Bottom-Up (BU) saliency models do not perform well in complex interactive
environments where humans are actively engaged in tasks (e.g., sandwich making
and playing the video games). In this paper, we leverage Reinforcement Learning
(RL) to highlight task-relevant locations of input frames. We propose a soft
attention mechanism combined with the Deep Q-Network (DQN) model to teach an RL
agent how to play a game and where to look by focusing on the most pertinent
parts of its visual input. Our evaluations on several Atari 2600 games show
that the soft attention based model could predict fixation locations
significantly better than bottom-up models such as Itti-Kochs saliency and
Graph-Based Visual Saliency (GBVS) models.


Saliency Revisited: Analysis of Mouse Movements versus Fixations

  This paper revisits visual saliency prediction by evaluating the recent
advancements in this field such as crowd-sourced mouse tracking-based databases
and contextual annotations. We pursue a critical and quantitative approach
towards some of the new challenges including the quality of mouse tracking
versus eye tracking for model training and evaluation. We extend quantitative
evaluation of models in order to incorporate contextual information by
proposing an evaluation methodology that allows accounting for contextual
factors such as text, faces, and object attributes. The proposed contextual
evaluation scheme facilitates detailed analysis of models and helps identify
their pros and cons. Through several experiments, we find that (1) mouse
tracking data has lower inter-participant visual congruency and higher
dispersion, compared to the eye tracking data, (2) mouse tracking data does not
totally agree with eye tracking in general and in terms of different contextual
regions in specific, and (3) mouse tracking data leads to acceptable results in
training current existing models, and (4) mouse tracking data is less reliable
for model selection and evaluation. The contextual evaluation also reveals
that, among the studied models, there is no single model that performs best on
all the tested annotations.


Structure-measure: A New Way to Evaluate Foreground Maps

  Foreground map evaluation is crucial for gauging the progress of object
segmentation algorithms, in particular in the filed of salient object detection
where the purpose is to accurately detect and segment the most salient object
in a scene. Several widely-used measures such as Area Under the Curve (AUC),
Average Precision (AP) and the recently proposed Fbw have been utilized to
evaluate the similarity between a non-binary saliency map (SM) and a
ground-truth (GT) map. These measures are based on pixel-wise errors and often
ignore the structural similarities. Behavioral vision studies, however, have
shown that the human visual system is highly sensitive to structures in scenes.
Here, we propose a novel, efficient, and easy to calculate measure known an
structural similarity measure (Structure-measure) to evaluate non-binary
foreground maps. Our new measure simultaneously evaluates region-aware and
object-aware structural similarity between a SM and a GT map. We demonstrate
superiority of our measure over existing ones using 5 meta-measures on 5
benchmark datasets.


An Unsupervised Game-Theoretic Approach to Saliency Detection

  We propose a novel unsupervised game-theoretic salient object detection
algorithm that does not require labeled training data. First, saliency
detection problem is formulated as a non-cooperative game, hereinafter referred
to as Saliency Game, in which image regions are players who choose to be
"background" or "foreground" as their pure strategies. A payoff function is
constructed by exploiting multiple cues and combining complementary features.
Saliency maps are generated according to each region's strategy in the Nash
equilibrium of the proposed Saliency Game. Second, we explore the complementary
relationship between color and deep features and propose an Iterative Random
Walk algorithm to combine saliency maps produced by the Saliency Game using
different features. Iterative random walk allows sharing information across
feature spaces, and detecting objects that are otherwise very hard to detect.
Extensive experiments over 6 challenging datasets demonstrate the superiority
of our proposed unsupervised algorithm compared to several state of the art
supervised algorithms.


Segmenting Sky Pixels in Images

  Outdoor scene parsing models are often trained on ideal datasets and produce
quality results. However, this leads to a discrepancy when applied to the real
world. The quality of scene parsing, particularly sky classification, decreases
in night time images, images involving varying weather conditions, and scene
changes due to seasonal weather. This project focuses on approaching these
challenges by using a state-of-the-art model in conjunction with a non-ideal
dataset: SkyFinder and a subset from SUN database with Sky object. We focus
specifically on sky segmentation, the task of determining sky and not-sky
pixels, and improving upon an existing state-of-the-art model: RefineNet. As a
result of our efforts, we have seen an improvement of 10-15% in the average MCR
compared to the prior methods on SkyFinder dataset. We have also improved from
an off-the shelf-model in terms of average mIOU by nearly 35%. Further, we
analyze our trained models on images w.r.t two aspects: times of day and
weather, and find that, in spite of facing same challenges as prior methods,
our trained models significantly outperform them.


Visual Weather Temperature Prediction

  In this paper, we attempt to employ convolutional recurrent neural networks
for weather temperature estimation using only image data. We study ambient
temperature estimation based on deep neural networks in two scenarios a)
estimating temperature of a single outdoor image, and b) predicting temperature
of the last image in an image sequence. In the first scenario, visual features
are extracted by a convolutional neural network trained on a large-scale image
dataset. We demonstrate that promising performance can be obtained, and analyze
how volume of training data influences performance. In the second scenario, we
consider the temporal evolution of visual appearance, and construct a recurrent
neural network to predict the temperature of the last image in a given image
sequence. We obtain better prediction accuracy compared to the state-of-the-art
models. Further, we investigate how performance varies when information is
extracted from different scene regions, and when images are captured in
different daytime hours. Our approach further reinforces the idea of using only
visual information for cost efficient weather prediction in the future.


What Catches the Eye? Visualizing and Understanding Deep Saliency Models

  Deep convolutional neural networks have demonstrated high performances for
fixation prediction in recent years. How they achieve this, however, is less
explored and they remain to be black box models. Here, we attempt to shed light
on the internal structure of deep saliency models and study what features they
extract for fixation prediction. Specifically, we use a simple yet powerful
architecture, consisting of only one CNN and a single resolution input,
combined with a new loss function for pixel-wise fixation prediction during
free viewing of natural scenes. We show that our simple method is on par or
better than state-of-the-art complicated saliency models. Furthermore, we
propose a method, related to saliency model evaluation metrics, to visualize
deep models for fixation prediction. Our method reveals the inner
representations of deep models for fixation prediction and provides evidence
that saliency, as experienced by humans, is likely to involve high-level
semantic knowledge in addition to low-level perceptual cues. Our results can be
useful to measure the gap between current saliency models and the human
inter-observer model and to build new models to close this gap.


Salient Objects in Clutter: Bringing Salient Object Detection to the
  Foreground

  We provide a comprehensive evaluation of salient object detection (SOD)
models. Our analysis identifies a serious design bias of existing SOD datasets
which assumes that each image contains at least one clearly outstanding salient
object in low clutter. The design bias has led to a saturated high performance
for state-of-the-art SOD models when evaluated on existing datasets. The
models, however, still perform far from being satisfactory when applied to
real-world daily scenes. Based on our analyses, we first identify 7 crucial
aspects that a comprehensive and balanced dataset should fulfill. Then, we
propose a new high quality dataset and update the previous saliency benchmark.
Specifically, our SOC (Salient Objects in Clutter) dataset, includes images
with salient and non-salient objects from daily object categories. Beyond
object category annotations, each salient image is accompanied by attributes
that reflect common challenges in real-world scenes. Finally, we report
attribute-based performance assessment on our dataset.


Three Birds One Stone: A General Architecture for Salient Object
  Segmentation, Edge Detection and Skeleton Extraction

  In this paper, we aim at solving pixel-wise binary problems, including
salient object segmentation, skeleton extraction, and edge detection, by
introducing a unified architecture. Previous works have proposed tailored
methods for solving each of the three tasks independently. Here, we show that
these tasks share some similarities that can be exploited for developing a
unified framework. In particular, we introduce a horizontal cascade, each
component of which is densely connected to the outputs of previous component.
Stringing these components together allows us to effectively exploit features
across different levels hierarchically to effectively address the multiple
pixel-wise binary regression tasks. To assess the performance of our proposed
network on these tasks, we carry out exhaustive evaluations on multiple
representative datasets. Although these tasks are inherently very different, we
show that our unified approach performs very well on all of them and works far
better than current single-purpose state-of-the-art methods. All the code in
this paper will be publicly available.


Enhanced-alignment Measure for Binary Foreground Map Evaluation

  The existing binary foreground map (FM) measures to address various types of
errors in either pixel-wise or structural ways. These measures consider
pixel-level match or image-level information independently, while cognitive
vision studies have shown that human vision is highly sensitive to both global
information and local details in scenes. In this paper, we take a detailed look
at current binary FM evaluation measures and propose a novel and effective
E-measure (Enhanced-alignment measure). Our measure combines local pixel values
with the image-level mean value in one term, jointly capturing image-level
statistics and local pixel matching information. We demonstrate the superiority
of our measure over the available measures on 4 popular datasets via 5
meta-measures, including ranking models for applications, demoting generic,
random Gaussian noise maps, ground-truth switch, as well as human judgments. We
find large improvements in almost all the meta-measures. For instance, in terms
of application ranking, we observe improvementrangingfrom9.08% to 19.65%
compared with other popular measures.


Improving Sequential Determinantal Point Processes for Supervised Video
  Summarization

  It is now much easier than ever before to produce videos. While the
ubiquitous video data is a great source for information discovery and
extraction, the computational challenges are unparalleled. Automatically
summarizing the videos has become a substantial need for browsing, searching,
and indexing visual content. This paper is in the vein of supervised video
summarization using sequential determinantal point process (SeqDPP), which
models diversity by a probabilistic distribution. We improve this model in two
folds. In terms of learning, we propose a large-margin algorithm to address the
exposure bias problem in SeqDPP. In terms of modeling, we design a new
probabilistic distribution such that, when it is integrated into SeqDPP, the
resulting model accepts user input about the expected length of the summary.
Moreover, we also significantly extend a popular video summarization dataset by
1) more egocentric videos, 2) dense user annotations, and 3) a refined
evaluation scheme. We conduct extensive experiments on this dataset (about 60
hours of videos in total) and compare our approach to several competitive
baselines.


Cross-view image synthesis using geometry-guided conditional GANs

  We address the problem of generating images across two drastically different
views, namely ground (street) and aerial (overhead) views. Image synthesis by
itself is a very challenging computer vision task and is even more so when
generation is conditioned on an image in another view. Due the difference in
viewpoints, there is small overlapping field of view and little common content
between these two views. Here, we try to preserve the pixel information between
the views so that the generated image is a realistic representation of cross
view input image. For this, we propose to use homography as a guide to map the
images between the views based on the common field of view to preserve the
details in the input image. We then use generative adversarial networks to
inpaint the missing regions in the transformed image and add realism to it. Our
exhaustive evaluation and model comparison demonstrate that utilizing geometry
constraints adds fine details to the generated images and can be a better
approach for cross view image synthesis than purely pixel based synthesis
methods.


Invariance Analysis of Saliency Models versus Human Gaze During Scene
  Free Viewing

  Most of current studies on human gaze and saliency modeling have used
high-quality stimuli. In real world, however, captured images undergo various
types of distortions during the whole acquisition, transmission, and displaying
chain. Some distortion types include motion blur, lighting variations and
rotation. Despite few efforts, influences of ubiquitous distortions on visual
attention and saliency models have not been systematically investigated. In
this paper, we first create a large-scale database including eye movements of
10 observers over 1900 images degraded by 19 types of distortions. Second, by
analyzing eye movements and saliency models, we find that: a) observers look at
different locations over distorted versus original images, and b) performances
of saliency models are drastically hindered over distorted images, with the
maximum performance drop belonging to Rotation and Shearing distortions.
Finally, we investigate the effectiveness of different distortions when serving
as data augmentation transformations. Experimental results verify that some
useful data augmentation transformations which preserve human gaze of reference
images can improve deep saliency models against distortions, while some invalid
transformations which severely change human gaze will degrade the performance.


Adversarial Attacks against Deep Saliency Models

  Currently, a plethora of saliency models based on deep neural networks have
led great breakthroughs in many complex high-level vision tasks (e.g. scene
description, object detection). The robustness of these models, however, has
not yet been studied. In this paper, we propose a sparse feature-space
adversarial attack method against deep saliency models for the first time. The
proposed attack only requires a part of the model information, and is able to
generate a sparser and more insidious adversarial perturbation, compared to
traditional image-space attacks. These adversarial perturbations are so subtle
that a human observer cannot notice their presences, but the model outputs will
be revolutionized. This phenomenon raises security threats to deep saliency
models in practical applications. We also explore some intriguing properties of
the feature-space attack, e.g. 1) the hidden layers with bigger receptive
fields generate sparser perturbations, 2) the deeper hidden layers achieve
higher attack success rates, and 3) different loss functions and different
attacked layers will result in diverse perturbations. Experiments indicate that
the proposed method is able to successfully attack different model
architectures across various image scenes.


Non-Markovianity and coherence of a moving qubit inside a leaky cavity

  Non-Markovian features of a system evolution, stemming from memory effects,
may be utilized to transfer, storage, and revive basic quantum properties of
the system states. It is well known that an atom qubit undergoes non-Markovian
dynamics in high quality cavities. We here consider the qubit-cavity
interaction in the case when the qubit is in motion inside a leaky cavity. We
show that, owing to the inhibition of the decay rate, the coherence of the
traveling qubit remains closer to its initial value as time goes by compared to
that of a qubit at rest. We also demonstrate that quantum coherence is
preserved more efficiently for larger qubit velocities. This is true
independently of the evolution being Markovian or non-Markovian, albeit the
latter condition is more effective at a given value of velocity. We however
find that the degree of non-Markovianity is eventually weakened as the qubit
velocity increases, despite a better coherence maintenance.


Salient Object Detection: A Benchmark

  We extensively compare, qualitatively and quantitatively, 40 state-of-the-art
models (28 salient object detection, 10 fixation prediction, 1 objectness, and
1 baseline) over 6 challenging datasets for the purpose of benchmarking salient
object detection and segmentation methods. From the results obtained so far,
our evaluation shows a consistent rapid progress over the last few years in
terms of both accuracy and running time. The top contenders in this benchmark
significantly outperform the models identified as the best in the previous
benchmark conducted just two years ago. We find that the models designed
specifically for salient object detection generally work better than models in
closely related areas, which in turn provides a precise definition and suggests
an appropriate treatment of this problem that distinguishes it from other
problems. In particular, we analyze the influences of center bias and scene
complexity in model performance, which, along with the hard cases for
state-of-the-art models, provide useful hints towards constructing more
challenging large scale datasets and better saliency models. Finally, we
propose probable solutions for tackling several open problems such as
evaluation scores and dataset bias, which also suggest future research
directions in the rapidly-growing field of salient object detection.


A Review of Co-saliency Detection Technique: Fundamentals, Applications,
  and Challenges

  Co-saliency detection is a newly emerging and rapidly growing research area
in computer vision community. As a novel branch of visual saliency, co-saliency
detection refers to the discovery of common and salient foregrounds from two or
more relevant images, and can be widely used in many computer vision tasks. The
existing co-saliency detection algorithms mainly consist of three components:
extracting effective features to represent the image regions, exploring the
informative cues or factors to characterize co-saliency, and designing
effective computational frameworks to formulate co-saliency. Although numerous
methods have been developed, the literature is still lacking a deep review and
evaluation of co-saliency detection techniques. In this paper, we aim at
providing a comprehensive review of the fundamentals, challenges, and
applications of co-saliency detection. Specifically, we provide an overview of
some related computer vision works, review the history of co-saliency
detection, summarize and categorize the major algorithms in this research area,
discuss some open issues in this area, present the potential applications of
co-saliency detection, and finally point out some unsolved challenges and
promising future works. We expect this review to be beneficial to both fresh
and senior researchers in this field, and give insights to researchers in other
related areas regarding the utility of co-saliency detection algorithms.


Ego2Top: Matching Viewers in Egocentric and Top-view Videos

  Egocentric cameras are becoming increasingly popular and provide us with
large amounts of videos, captured from the first person perspective. At the
same time, surveillance cameras and drones offer an abundance of visual
information, often captured from top-view. Although these two sources of
information have been separately studied in the past, they have not been
collectively studied and related. Having a set of egocentric cameras and a
top-view camera capturing the same area, we propose a framework to identify the
egocentric viewers in the top-view video. We utilize two types of features for
our assignment procedure. Unary features encode what a viewer (seen from
top-view or recording an egocentric video) visually experiences over time.
Pairwise features encode the relationship between the visual content of a pair
of viewers. Modeling each view (egocentric or top) by a graph, the assignment
process is formulated as spectral graph matching. Evaluating our method over a
dataset of 50 top-view and 188 egocentric videos taken in different scenarios
demonstrates the efficiency of the proposed approach in assigning egocentric
viewers to identities present in top-view camera. We also study the effect of
different parameters such as the number of egocentric viewers and visual
features.


Egocentric Meets Top-view

  Thanks to the availability and increasing popularity of Egocentric cameras
such as GoPro cameras, glasses, and etc. we have been provided with a plethora
of videos captured from the first person perspective. Surveillance cameras and
Unmanned Aerial Vehicles(also known as drones) also offer tremendous amount of
videos, mostly with top-down or oblique view-point. Egocentric vision and
top-view surveillance videos have been studied extensively in the past in the
computer vision community. However, the relationship between the two has yet to
be explored thoroughly. In this effort, we attempt to explore this relationship
by approaching two questions. First, having a set of egocentric videos and a
top-view video, can we verify if the top-view video contains all, or some of
the egocentric viewers present in the egocentric set? And second, can we
identify the egocentric viewers in the content of the top-view video? In other
words, can we find the cameramen in the surveillance videos? These problems can
become more challenging when the videos are not time-synchronous. Thus we
formalize the problem in a way which handles and also estimates the unknown
relative time-delays between the egocentric videos and the top-view video. We
formulate the problem as a spectral graph matching instance, and jointly seek
the optimal assignments and relative time-delays of the videos. As a result, we
spatiotemporally localize the egocentric observers in the top-view video. We
model each view (egocentric or top) using a graph, and compute the assignment
and time-delays in an iterative-alternative fashion.


Egocentric Height Estimation

  Egocentric, or first-person vision which became popular in recent years with
an emerge in wearable technology, is different than exocentric (third-person)
vision in some distinguishable ways, one of which being that the camera wearer
is generally not visible in the video frames. Recent work has been done on
action and object recognition in egocentric videos, as well as work on
biometric extraction from first-person videos. Height estimation can be a
useful feature for both soft-biometrics and object tracking. Here, we propose a
method of estimating the height of an egocentric camera without any calibration
or reference points. We used both traditional computer vision approaches and
deep learning in order to determine the visual cues that results in best height
estimation. Here, we introduce a framework inspired by two stream networks
comprising of two Convolutional Neural Networks, one based on spatial
information, and one based on information given by optical flow in a frame.
Given an egocentric video as an input to the framework, our model yields a
height estimate as an output. We also incorporate late fusion to learn a
combination of temporal and spatial cues. Comparing our model with other
methods we used as baselines, we achieve height estimates for videos with a
Mean Average Error of 14.04 cm over a range of 103 cm of data, and
classification accuracy for relative height (tall, medium or short) up to
93.75% where chance level is 33%.


EgoTransfer: Transferring Motion Across Egocentric and Exocentric
  Domains using Deep Neural Networks

  Mirror neurons have been observed in the primary motor cortex of primate
species, in particular in humans and monkeys. A mirror neuron fires when a
person performs a certain action, and also when he observes the same action
being performed by another person. A crucial step towards building fully
autonomous intelligent systems with human-like learning abilities is the
capability in modeling the mirror neuron. On one hand, the abundance of
egocentric cameras in the past few years has offered the opportunity to study a
lot of vision problems from the first-person perspective. A great deal of
interesting research has been done during the past few years, trying to explore
various computer vision tasks from the perspective of the self. On the other
hand, videos recorded by traditional static cameras, capture humans performing
different actions from an exocentric third-person perspective. In this work, we
take the first step towards relating motion information across these two
perspectives. We train models that predict motion in an egocentric view, by
observing it from an exocentric view, and vice versa. This allows models to
predict how an egocentric motion would look like from outside. To do so, we
train linear and nonlinear models and evaluate their performance in terms of
retrieving the egocentric (exocentric) motion features, while having access to
an exocentric (egocentric) motion feature. Our experimental results demonstrate
that motion information can be successfully transferred across the two views.


Revisiting Video Saliency: A Large-scale Benchmark and a New Model

  In this work, we contribute to video saliency research in two ways. First, we
introduce a new benchmark for predicting human eye movements during dynamic
scene free-viewing, which is long-time urged in this field. Our dataset, named
DHF1K (Dynamic Human Fixation), consists of 1K high-quality, elaborately
selected video sequences spanning a large range of scenes, motions, object
types and background complexity. Existing video saliency datasets lack variety
and generality of common dynamic scenes and fall short in covering challenging
situations in unconstrained environments. In contrast, DHF1K makes a
significant leap in terms of scalability, diversity and difficulty, and is
expected to boost video saliency modeling. Second, we propose a novel video
saliency model that augments the CNN-LSTM network architecture with an
attention mechanism to enable fast, end-to-end saliency learning. The attention
mechanism explicitly encodes static saliency information, thus allowing LSTM to
focus on learning more flexible temporal saliency representation across
successive frames. Such a design fully leverages existing large-scale static
fixation datasets, avoids overfitting, and significantly improves training
efficiency and testing performance. We thoroughly examine the performance of
our model, with respect to state-of-the-art saliency models, on three
large-scale datasets (i.e., DHF1K, Hollywood2, UCF sports). Experimental
results over more than 1.2K testing videos containing 400K frames demonstrate
that our model outperforms other competitors.


Analysis of Hand Segmentation in the Wild

  A large number of works in egocentric vision have concentrated on action and
object recognition. Detection and segmentation of hands in first-person videos,
however, has less been explored. For many applications in this domain, it is
necessary to accurately segment not only hands of the camera wearer but also
the hands of others with whom he is interacting. Here, we take an in-depth look
at the hand segmentation problem. In the quest for robust hand segmentation
methods, we evaluated the performance of the state of the art semantic
segmentation methods, off the shelf and fine-tuned, on existing datasets. We
fine-tune RefineNet, a leading semantic segmentation method, for hand
segmentation and find that it does much better than the best contenders.
Existing hand segmentation datasets are collected in the laboratory settings.
To overcome this limitation, we contribute by collecting two new datasets: a)
EgoYouTubeHands including egocentric videos containing hands in the wild, and
b) HandOverFace to analyze the performance of our models in presence of similar
appearance occlusions. We further explore whether conditional random fields can
help refine generated hand segmentations. To demonstrate the benefit of
accurate hand maps, we train a CNN for hand-based activity recognition and
achieve higher accuracy when a CNN was trained using hand maps produced by the
fine-tuned RefineNet. Finally, we annotate a subset of the EgoHands dataset for
fine-grained action recognition and show that an accuracy of 58.6% can be
achieved by just looking at a single hand pose which is much better than the
chance level (12.5%).


Cross-View Image Synthesis using Conditional GANs

  Learning to generate natural scenes has always been a challenging task in
computer vision. It is even more painstaking when the generation is conditioned
on images with drastically different views. This is mainly because
understanding, corresponding, and transforming appearance and semantic
information across the views is not trivial. In this paper, we attempt to solve
the novel problem of cross-view image synthesis, aerial to street-view and vice
versa, using conditional generative adversarial networks (cGAN). Two new
architectures called Crossview Fork (X-Fork) and Crossview Sequential (X-Seq)
are proposed to generate scenes with resolutions of 64x64 and 256x256 pixels.
X-Fork architecture has a single discriminator and a single generator. The
generator hallucinates both the image and its semantic segmentation in the
target view. X-Seq architecture utilizes two cGANs. The first one generates the
target image which is subsequently fed to the second cGAN for generating its
corresponding semantic segmentation map. The feedback from the second cGAN
helps the first cGAN generate sharper images. Both of our proposed
architectures learn to generate natural images as well as their semantic
segmentation maps. The proposed methods show that they are able to capture and
maintain the true semantics of objects in source and target views better than
the traditional image-to-image translation method which considers only the
visual appearance of the scene. Extensive qualitative and quantitative
evaluations support the effectiveness of our frameworks, compared to two state
of the art methods, for natural scene generation across drastically different
views.


Learning a Saliency Evaluation Metric Using Crowdsourced Perceptual
  Judgments

  In the area of human fixation prediction, dozens of computational saliency
models are proposed to reveal certain saliency characteristics under different
assumptions and definitions. As a result, saliency model benchmarking often
requires several evaluation metrics to simultaneously assess saliency models
from multiple perspectives. However, most computational metrics are not
designed to directly measure the perceptual similarity of saliency maps so that
the evaluation results may be sometimes inconsistent with the subjective
impression. To address this problem, this paper first conducts extensive
subjective tests to find out how the visual similarities between saliency maps
are perceived by humans. Based on the crowdsourced data collected in these
tests, we conclude several key factors in assessing saliency maps and quantize
the performance of existing metrics. Inspired by these factors, we propose to
learn a saliency evaluation metric based on a two-stream convolutional neural
network using crowdsourced perceptual judgements. Specifically, the relative
saliency score of each pair from the crowdsourced data is utilized to
regularize the network during the training process. By capturing the key
factors shared by various subjects in comparing saliency maps, the learned
metric better aligns with human perception of saliency maps, making it a good
complement to the existing metrics. Experimental results validate that the
learned metric can be generalized to the comparisons of saliency maps from new
images, new datasets, new models and synthetic data. Due to the effectiveness
of the learned metric, it also can be used to facilitate the development of new
models for fixation prediction.


From Third Person to First Person: Dataset and Baselines for Synthesis
  and Retrieval

  First-person (egocentric) and third person (exocentric) videos are
drastically different in nature. The relationship between these two views have
been studied in recent years, however, it has yet to be fully explored. In this
work, we introduce two datasets (synthetic and natural/real) containing
simultaneously recorded egocentric and exocentric videos. We also explore
relating the two domains (egocentric and exocentric) in two aspects. First, we
synthesize images in the egocentric domain from the exocentric domain using a
conditional generative adversarial network (cGAN). We show that with enough
training data, our network is capable of hallucinating how the world would look
like from an egocentric perspective, given an exocentric video. Second, we
address the cross-view retrieval problem across the two views. Given an
egocentric query frame (or its momentary optical flow), we retrieve its
corresponding exocentric frame (or optical flow) from a gallery set. We show
that using synthetic data could be beneficial in retrieving real data. We show
that performing domain adaptation from the synthetic domain to the natural/real
domain, is helpful in tasks such as retrieval. We believe that the presented
datasets and the proposed baselines offer new opportunities for further
research in this direction. The code and dataset are publicly available.


Multi-View Egocentric Video Summarization

  With vast amounts of video content being uploaded to the Internet every
minute, video summarization becomes critical for efficient browsing, searching,
and indexing of visual content. Nonetheless, the spread of social and
egocentric cameras tends to create an abundance of sparse scenarios captured by
several devices, and ultimately required to be jointly summarized. In this
paper, we propose the problem of summarizing videos recorded simultaneously by
several egocentric cameras that intermittently share the field of view. We
present a supervised-learning framework that (a) identifies a diverse set of
important events among dynamically moving cameras that often are not capturing
the same scene, and (b) selects the most representative view(s) at each event
to be included in the universal summary. A key contribution of our work is
collecting a new multi-view egocentric dataset, Multi-Ego, due to the lack of
an applicable and relevant alternative. Our dataset consists of 41 sequences,
each recorded simultaneously by 3 cameras and covering a wide variety of
real-life scenarios. The footage is annotated comprehensively by multiple
individuals under various summarization settings: (a) single view, (b) two
view, and (c) three view, with a consensus analysis ensuring a reliable ground
truth. We conduct extensive experiments on the compiled dataset to show the
effectiveness of our approach over several state-of-the-art baselines. We also
show that it can learn from data of varied number-of-views, deeming it a
scalable and a generic summarization approach. Our dataset and materials are
publicly available.


Video Summarization via Actionness Ranking

  To automatically produce a brief yet expressive summary of a long video, an
automatic algorithm should start by resembling the human process of summary
generation. Prior work proposed supervised and unsupervised algorithms to train
models for learning the underlying behavior of humans by increasing modeling
complexity or craft-designing better heuristics to simulate human summary
generation process. In this work, we take a different approach by analyzing a
major cue that humans exploit for the summary generation; the nature and
intensity of actions.
  We empirically observed that a frame is more likely to be included in
human-generated summaries if it contains a substantial amount of deliberate
motion performed by an agent, which is referred to as actionness. Therefore, we
hypothesize that learning to automatically generate summaries involves an
implicit knowledge of actionness estimation and ranking. We validate our
hypothesis by running a user study that explores the correlation between
human-generated summaries and actionness ranks. We also run a consensus and
behavioral analysis between human subjects to ensure reliable and consistent
results. The analysis exhibits a considerable degree of agreement among
subjects within obtained data and verifying our initial hypothesis.
  Based on the study findings, we develop a method to incorporate actionness
data to explicitly regulate a learning algorithm that is trained for summary
generation. We assess the performance of our approach to four summarization
benchmark datasets and demonstrate an evident advantage compared to
state-of-the-art summarization methods.


A Synchronized Multi-Modal Attention-Caption Dataset and Analysis

  In this work, we present a novel multi-modal dataset consisting of eye
movements and verbal descriptions recorded synchronously over images. Using
this data, we study the differences between human attention in free-viewing and
image captioning tasks. We look into the relationship between human attention
and language constructs during perception and sentence articulation. We also
compare human and machine attention, in particular the top-down soft attention
approach that is argued to mimick human attention, in captioning tasks. Our
study reveals that, (1) human attention behaviour in free-viewing is different
than image description as humans tend to fixate on a greater variety of regions
under the latter task; (2) there is a strong relationship between the described
objects and the objects attended by subjects ($97\%$ of described objects are
being attended); (3) a convolutional neural network as feature encoder captures
regions that human attend under image captioning to a great extent (around
$78\%$); (4) the soft-attention as the top-down mechanism does not agree with
human attention behaviour neither spatially nor temporally; and (5)
soft-attention does not add strong beneficial human-like attention behaviour
for the task of captioning as it has low correlation between caption scores and
attention consistency scores, indicating a large gap between human and machine
in regard to top-down attention.


