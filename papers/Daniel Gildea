Synchronous Context-Free Grammars and Optimal Linear Parsing Strategies

  Synchronous Context-Free Grammars (SCFGs), also known as syntax-directed
translation schemata, are unlike context-free grammars in that they do not have
a binary normal form. In general, parsing with SCFGs takes space and time
polynomial in the length of the input strings, but with the degree of the
polynomial depending on the permutations of the SCFG rules. We consider linear
parsing strategies, which add one nonterminal at a time. We show that for a
given input permutation, the problems of finding the linear parsing strategy
with the minimum space and time complexity are both NP-hard.


Human languages order information efficiently

  Most languages use the relative order between words to encode meaning
relations. Languages differ, however, in what orders they use and how these
orders are mapped onto different meanings. We test the hypothesis that, despite
these differences, human languages might constitute different `solutions' to
common pressures of language use. Using Monte Carlo simulations over data from
five languages, we find that their word orders are efficient for processing in
terms of both dependency length and local lexical probability. This suggests
that biases originating in how the brain understands language strongly
constrain how human languages change over generations.


Feature-based Decipherment for Large Vocabulary Machine Translation

  Orthographic similarities across languages provide a strong signal for
probabilistic decipherment, especially for closely related language pairs. The
existing decipherment models, however, are not well-suited for exploiting these
orthographic similarities. We propose a log-linear model with latent variables
that incorporates orthographic similarity features. Maximum likelihood training
is computationally expensive for the proposed log-linear model. To address this
challenge, we perform approximate inference via MCMC sampling and contrastive
divergence. Our results show that the proposed log-linear model with
contrastive divergence scales to large vocabularies and outperforms the
existing generative decipherment models by exploiting the orthographic
features.


Exploring phrase-compositionality in skip-gram models

  In this paper, we introduce a variation of the skip-gram model which jointly
learns distributed word vector representations and their way of composing to
form phrase embeddings. In particular, we propose a learning procedure that
incorporates a phrase-compositionality function which can capture how we want
to compose phrases vectors from their component word vectors. Our experiments
show improvement in word and phrase similarity tasks as well as syntactic tasks
like dependency parsing using the proposed joint models.


AMR-to-text generation as a Traveling Salesman Problem

  The task of AMR-to-text generation is to generate grammatical text that
sustains the semantic meaning for a given AMR graph. We at- tack the task by
first partitioning the AMR graph into smaller fragments, and then generating
the translation for each fragment, before finally deciding the order by solving
an asymmetric generalized traveling salesman problem (AGTSP). A Maximum Entropy
classifier is trained to estimate the traveling costs, and a TSP solver is used
to find the optimized solution. The final model reports a BLEU score of 22.44
on the SemEval-2016 Task8 dataset.


AMR-to-text Generation with Synchronous Node Replacement Grammar

  This paper addresses the task of AMR-to-text generation by leveraging
synchronous node replacement grammar. During training, graph-to-string rules
are learned using a heuristic extraction algorithm. At test time, a graph
transducer is applied to collapse input AMRs and generate output sentences.
Evaluated on SemEval-2016 Task 8, our method gives a BLEU score of 25.62, which
is the best reported so far.


Addressing the Data Sparsity Issue in Neural AMR Parsing

  Neural attention models have achieved great success in different NLP tasks.
How- ever, they have not fulfilled their promise on the AMR parsing task due to
the data sparsity issue. In this paper, we de- scribe a sequence-to-sequence
model for AMR parsing and present different ways to tackle the data sparsity
problem. We show that our methods achieve significant improvement over a
baseline neural atten- tion model and our results are also compet- itive
against state-of-the-art systems that do not use extra linguistic resources.


Neural Transition-based Syntactic Linearization

  The task of linearization is to find a grammatical order given a set of
words. Traditional models use statistical methods. Syntactic linearization
systems, which generate a sentence along with its syntactic tree, have shown
state-of-the-art performance. Recent work shows that a multi-layer LSTM
language model outperforms competitive statistical syntactic linearization
systems without using syntax. In this paper, we study neural syntactic
linearization, building a transition-based syntactic linearizer leveraging a
feed-forward neural network, observing significantly better results compared to
LSTM language models on this task.


Semantic Neural Machine Translation using AMR

  It is intuitive that semantic representations can be useful for machine
translation, mainly because they can help in enforcing meaning preservation and
handling data sparsity (many sentences correspond to one meaning) of machine
translation models. On the other hand, little work has been done on leveraging
semantics for neural machine translation (NMT). In this work, we study the
usefulness of AMR (short for abstract meaning representation) on NMT.
Experiments on a standard English-to-German dataset show that incorporating AMR
as additional knowledge can significantly improve a strong attention-based
sequence-to-sequence neural translation model.


Parsing Linear Context-Free Rewriting Systems with Fast Matrix
  Multiplication

  We describe a matrix multiplication recognition algorithm for a subset of
binary linear context-free rewriting systems (LCFRS) with running time
$O(n^{\omega d})$ where $M(m) = O(m^{\omega})$ is the running time for $m
\times m$ matrix multiplication and $d$ is the "contact rank" of the LCFRS --
the maximal number of combination and non-combination points that appear in the
grammar rules. We also show that this algorithm can be used as a subroutine to
get a recognition algorithm for general binary LCFRS with running time
$O(n^{\omega d + 1})$. The currently best known $\omega$ is smaller than
$2.38$. Our result provides another proof for the best known result for parsing
mildly context sensitive formalisms such as combinatory categorial grammars,
head grammars, linear indexed grammars, and tree adjoining grammars, which can
be parsed in time $O(n^{4.76})$. It also shows that inversion transduction
grammars can be parsed in time $O(n^{5.76})$. In addition, binary LCFRS
subsumes many other formalisms and types of grammars, for some of which we also
improve the asymptotic complexity of parsing.


Sense Embedding Learning for Word Sense Induction

  Conventional word sense induction (WSI) methods usually represent each
instance with discrete linguistic features or cooccurrence features, and train
a model for each polysemous word individually. In this work, we propose to
learn sense embeddings for the WSI task. In the training stage, our method
induces several sense centroids (embedding) for each polysemous word. In the
testing stage, our method represents each instance as a contextual vector, and
induces its sense by finding the nearest sense centroid in the embedding space.
The advantages of our method are (1) distributed sense vectors are taken as the
knowledge representations which are trained discriminatively, and usually have
better performance than traditional count-based distributional models, and (2)
a general model for the whole vocabulary is jointly trained to induce sense
centroids under the mutlitask learning framework. Evaluated on SemEval-2010 WSI
dataset, our method outperforms all participants and most of the recent
state-of-the-art methods. We further verify the two advantages by comparing
with carefully designed baselines.


Convergence of the EM Algorithm for Gaussian Mixtures with Unbalanced
  Mixing Coefficients

  The speed of convergence of the Expectation Maximization (EM) algorithm for
Gaussian mixture model fitting is known to be dependent on the amount of
overlap among the mixture components. In this paper, we study the impact of
mixing coefficients on the convergence of EM. We show that when the mixture
components exhibit some overlap, the convergence of EM becomes slower as the
dynamic range among the mixing coefficients increases. We propose a
deterministic anti-annealing algorithm, that significantly improves the speed
of convergence of EM for such mixtures with unbalanced mixing coefficients. The
proposed algorithm is compared against other standard optimization techniques
like BFGS, Conjugate Gradient, and the traditional EM algorithm. Finally, we
propose a similar deterministic anti-annealing based algorithm for the
Dirichlet process mixture model and demonstrate its advantages over the
conventional variational Bayesian approach.


A Graph-to-Sequence Model for AMR-to-Text Generation

  The problem of AMR-to-text generation is to recover a text representing the
same meaning as an input AMR graph. The current state-of-the-art method uses a
sequence-to-sequence model, leveraging LSTM for encoding a linearized AMR
structure. Although being able to model non-local semantic information, a
sequence LSTM can lose information from the AMR graph structure, and thus faces
challenges with large graphs, which result in long sequences. We introduce a
neural graph-to-sequence model, using a novel LSTM structure for directly
encoding graph-level semantics. On a standard benchmark, our model shows
superior results to existing methods in the literature.


N-ary Relation Extraction using Graph State LSTM

  Cross-sentence $n$-ary relation extraction detects relations among $n$
entities across multiple sentences. Typical methods formulate an input as a
\textit{document graph}, integrating various intra-sentential and
inter-sentential dependencies. The current state-of-the-art method splits the
input graph into two DAGs, adopting a DAG-structured LSTM for each. Though
being able to model rich linguistic knowledge by leveraging graph edges,
important information can be lost in the splitting procedure. We propose a
graph-state LSTM model, which uses a parallel state to model each word,
recurrently enriching state values via message passing. Compared with DAG
LSTMs, our graph LSTM keeps the original graph structure, and speeds up
computation by allowing more parallelization. On a standard benchmark, our
model shows the best result in the literature.


Exploring Graph-structured Passage Representation for Multi-hop Reading
  Comprehension with Graph Neural Networks

  Multi-hop reading comprehension focuses on one type of factoid question,
where a system needs to properly integrate multiple pieces of evidence to
correctly answer a question. Previous work approximates global evidence with
local coreference information, encoding coreference chains with DAG-styled GRU
layers within a gated-attention reader. However, coreference is limited in
providing information for rich inference. We introduce a new method for better
connecting global evidence, which forms more complex graphs compared to DAGs.
To perform evidence integration on our graphs, we investigate two recent graph
neural networks, namely graph convolutional network (GCN) and graph recurrent
network (GRN). Experiments on two standard datasets show that richer global
information leads to better answers. Our method performs better than all
published results on these datasets.


Automated Analysis and Prediction of Job Interview Performance

  We present a computational framework for automatically quantifying verbal and
nonverbal behaviors in the context of job interviews. The proposed framework is
trained by analyzing the videos of 138 interview sessions with 69
internship-seeking undergraduates at the Massachusetts Institute of Technology
(MIT). Our automated analysis includes facial expressions (e.g., smiles, head
gestures, facial tracking points), language (e.g., word counts, topic
modeling), and prosodic information (e.g., pitch, intonation, and pauses) of
the interviewees. The ground truth labels are derived by taking a weighted
average over the ratings of 9 independent judges. Our framework can
automatically predict the ratings for interview traits such as excitement,
friendliness, and engagement with correlation coefficients of 0.75 or higher,
and can quantify the relative importance of prosody, language, and facial
expressions. By analyzing the relative feature weights learned by the
regression models, our framework recommends to speak more fluently, use less
filler words, speak as "we" (vs. "I"), use more unique words, and smile more.
We also find that the students who were rated highly while answering the first
interview question were also rated highly overall (i.e., first impression
matters). Finally, our MIT Interview dataset will be made available to other
researchers to further validate and expand our findings.


