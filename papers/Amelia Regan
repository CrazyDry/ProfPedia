Thompson Sampling in Dynamic Systems for Contextual Bandit Problems

  We consider the multiarm bandit problems in the timevarying dynamic system
for rich structural features. For the nonlinear dynamic model, we propose the
approximate inference for the posterior distributions based on Laplace
Approximation. For the context bandit problems, Thompson Sampling is adopted
based on the underlying posterior distributions of the parameters. More
specifically, we introduce the discount decays on the previous samples impact
and analyze the different decay rates with the underlying sample dynamics.
Consequently, the exploration and exploitation is adaptively tradeoff according
to the dynamics in the system.


Sorting with GPUs: A Survey

  Sorting is a fundamental operation in computer science and is a bottleneck in
many important fields. Sorting is critical to database applications, online
search and indexing,biomedical computing, and many other applications. The
explosive growth in computational power and availability of GPU coprocessors
has allowed sort operations on GPUs to be done much faster than any
equivalently priced CPU. Current trends in GPU computing shows that this
explosive growth in GPU capabilities is likely to continue for some time. As
such, there is a need to develop algorithms to effectively harness the power of
GPUs for crucial applications such as sorting.


Online Classification Using a Voted RDA Method

  We propose a voted dual averaging method for online classification problems
with explicit regularization. This method employs the update rule of the
regularized dual averaging (RDA) method, but only on the subsequence of
training examples where a classification error is made. We derive a bound on
the number of mistakes made by this method on the training set, as well as its
generalization error rate. We also introduce the concept of relative strength
of regularization, and show how it affects the mistake bound and generalization
performance. We experimented with the method using $\ell_1$ regularization on a
large-scale natural language processing task, and obtained state-of-the-art
classification performance with fairly sparse models.


A Spatial-Temporal Decomposition Based Deep Neural Network for Time
  Series Forecasting

  Spatial time series forecasting problems arise in a broad range of
applications, such as environmental and transportation problems. These problems
are challenging because of the existence of specific spatial, short-term and
long-term patterns, and the curse of dimensionality. In this paper, we propose
a deep neural network framework for large-scale spatial time series forecasting
problems. We explicitly designed the neural network architecture for capturing
various types of patterns. In preprocessing, a time series decomposition method
is applied to separately feed short-term, long-term and spatial patterns into
different components of a neural network. A fuzzy clustering method finds
cluster of neighboring time series based on similarity of time series
residuals; as they can be meaningful short-term patterns for spatial time
series. In neural network architecture, each kernel of a multi-kernel
convolution layer is applied to a cluster of time series to extract short-term
features in neighboring areas. The output of convolution layer is concatenated
by trends and followed by convolution-LSTM layer to capture long-term patterns
in larger regional areas. To make a robust prediction when faced with missing
data, an unsupervised pretrained denoising autoencoder reconstructs the output
of the model in a fine-tuning step. The experimental results illustrate the
model outperforms baseline and state of the art models in a traffic flow
prediction dataset.


