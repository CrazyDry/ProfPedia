Representing and Reasoning With Probabilistic Knowledge: A Bayesian  Approach

  PAGODA (Probabilistic Autonomous Goal-Directed Agent) is a model forautonomous learning in probabilistic domains [desJardins, 1992] thatincorporates innovative techniques for using the agent's existing knowledge toguide and constrain the learning process and for representing, reasoning with,and learning probabilistic knowledge. This paper describes the probabilisticrepresentation and inference mechanism used in PAGODA. PAGODA forms theoriesabout the effects of its actions and the world state on the environment overtime. These theories are represented as conditional probability distributions.A restriction is imposed on the structure of the theories that allows theinference mechanism to find a unique predicted distribution for any action andworld state description. These restricted theories are called uniquelypredictive theories. The inference mechanism, Probability Combination usingIndependence (PCI), uses minimal independence assumptions to combine theprobabilities in a theory to make probabilistic predictions.

More-or-Less CP-Networks

  Preferences play an important role in our everyday lives. CP-networks, orCP-nets in short, are graphical models for representing conditional qualitativepreferences under ceteris paribus ("all else being equal") assumptions. Despitetheir intuitive nature and rich representation, dominance testing with CP-netsis computationally complex, even when the CP-nets are restricted tobinary-valued preferences. Tractable algorithms exist for binary CP-nets, butthese algorithms are incomplete for multi-valued CPnets. In this paper, weidentify a class of multivalued CP-nets, which we call more-or-less CPnets,that have the same computational complexity as binary CP-nets. More-or-lessCP-nets exploit the monotonicity of the attribute values and use intervals toaggregate values that induce similar preferences. We then present a searchcontrol rule for dominance testing that effectively prunes the search spacewhile preserving completeness.

Multi-view constrained clustering with an incomplete mapping between  views

  Multi-view learning algorithms typically assume a complete bipartite mappingbetween the different views in order to exchange information during thelearning process. However, many applications provide only a partial mappingbetween the views, creating a challenge for current methods. To address thisproblem, we propose a multi-view algorithm based on constrained clustering thatcan operate with an incomplete mapping. Given a set of pairwise constraints ineach view, our approach propagates these constraints using a local similaritymeasure to those instances that can be mapped to the other views, allowing thepropagated constraints to be transferred across views via the partial mapping.It uses co-EM to iteratively estimate the propagation within each view based onthe current clustering model, transfer the constraints across views, and thenupdate the clustering model. By alternating the learning process between views,this approach produces a unified clustering model that is consistent with allviews. We show that this approach significantly improves clustering performanceover several other methods for transferring constraints and allows multi-viewclustering to be reliably applied when given a limited mapping between theviews. Our evaluation reveals that the propagated constraints have highprecision with respect to the true clusters in the data, explaining theirbenefit to clustering performance in both single- and multi-view learningscenarios.

