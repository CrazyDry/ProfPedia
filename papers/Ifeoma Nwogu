A Concept Learning Approach to Multisensory Object Perception

  This paper presents a computational model of concept learning using Bayesianinference for a grammatically structured hypothesis space, and test the modelon multisensory (visual and haptics) recognition of 3D objects. The study isperformed on a set of artificially generated 3D objects known as fribbles,which are complex, multipart objects with categorical structures. The goal ofthis work is to develop a working multisensory representational model thatintegrates major themes on concepts and concepts learning from the cognitivescience literature. The model combines the representational power of aprobabilistic generative grammar with the inferential power of Bayesianinduction.

Computational Social Dynamics: Analyzing the Face-level Interactions in  a Group

  Interactional synchrony refers to how the speech or behavior of two or morepeople involved in a conversation become more finely synchronized with eachother, and they can appear to behave almost in direct response to one another.Studies have shown that interactional synchrony is a hallmark of relationships,and is produced as a result of rapport. %Research has also shown that up totwo-thirds of human communication occurs via nonverbal channels such asgestures (or body movements), facial expressions, \etc.  In this work, we use computer vision based methods to extract nonverbal cues,specifically from the face, and develop a model to measure interactionalsynchrony based on those cues. This paper illustrates a novel method ofconstructing a dynamic deep neural architecture, specifically made up ofintermediary long short-term memory networks (LSTMs), useful for learning andpredicting the extent of synchrony between two or more processes, by emulatingthe nonlinear dependencies between them. On a synthetic dataset, where pairs ofsequences were generated from a Gaussian process with known covariates, thearchitecture could successfully determine the covariance values of thegenerating process within an error of 0.5% when tested on 100 pairs ofinteracting signals. On a real-life dataset involving groups of three people,the model successfully estimated the extent of synchrony of each group on ascale of 1 to 5, with an overall prediction mean of $2.96%$ error whenperforming 5-fold validation, as compared to 26.1% on the random permutationsserving as the control baseline.

An Analysis of Random Projections in Cancelable Biometrics

  With increasing concerns about security, the need for highly secure physicalbiometrics-based authentication systems utilizing \emph{cancelable biometric}technologies is on the rise. Because the problem of cancelable templategeneration deals with the trade-off between template security and matchingperformance, many state-of-the-art algorithms successful in generating highquality cancelable biometrics all have random projection as one of their earlyprocessing steps. This paper therefore presents a formal analysis of why randomprojections is an essential step in cancelable biometrics. By formally definingthe notion of an \textit{Independent Subspace Structure} for datasets, it canbe shown that random projection preserves the subspace structure of datavectors generated from a union of independent linear subspaces. The bound onthe minimum number of random vectors required for this to hold is also derivedand is shown to depend logarithmically on the number of data samples, not onlyin independent subspaces but in disjoint subspace settings as well. Thetheoretical analysis presented is supported in detail with empirical results onreal-world face recognition datasets.

Dimensionality Reduction with Subspace Structure Preservation

  Modeling data as being sampled from a union of independent subspaces has beenwidely applied to a number of real world applications. However, dimensionalityreduction approaches that theoretically preserve this independence assumptionhave not been well studied. Our key contribution is to show that $2K$projection vectors are sufficient for the independence preservation of any $K$class data sampled from a union of independent subspaces. It is thisnon-trivial observation that we use for designing our dimensionality reductiontechnique. In this paper, we propose a novel dimensionality reduction algorithmthat theoretically preserves this structure for a given dataset. We support ourtheoretical analysis with empirical results on both synthetic and real worlddata achieving \textit{state-of-the-art} results compared to populardimensionality reduction techniques.

Is Joint Training Better for Deep Auto-Encoders?

  Traditionally, when generative models of data are developed via deeparchitectures, greedy layer-wise pre-training is employed. In a well-trainedmodel, the lower layer of the architecture models the data distributionconditional upon the hidden variables, while the higher layers model the hiddendistribution prior. But due to the greedy scheme of the layerwise trainingtechnique, the parameters of lower layers are fixed when training higherlayers. This makes it extremely challenging for the model to learn the hiddendistribution prior, which in turn leads to a suboptimal model for the datadistribution. We therefore investigate joint training of deep autoencoders,where the architecture is viewed as one stack of two or more single-layerautoencoders. A single global reconstruction objective is jointly optimized,such that the objective for the single autoencoders at each layer acts as alocal, layer-level regularizer. We empirically evaluate the performance of thisjoint training scheme and observe that it not only learns a better data model,but also learns better higher layer representations, which highlights itspotential for unsupervised feature learning. In addition, we find that theusage of regularizations in the joint training scheme is crucial in achievinggood performance. In the supervised setting, joint training also shows superiorperformance when training deeper models. The joint training framework can thusprovide a platform for investigating more efficient usage of different types ofregularizers, especially in light of the growing volumes of available unlabeleddata.

