A Concept Learning Approach to Multisensory Object Perception

  This paper presents a computational model of concept learning using Bayesian
inference for a grammatically structured hypothesis space, and test the model
on multisensory (visual and haptics) recognition of 3D objects. The study is
performed on a set of artificially generated 3D objects known as fribbles,
which are complex, multipart objects with categorical structures. The goal of
this work is to develop a working multisensory representational model that
integrates major themes on concepts and concepts learning from the cognitive
science literature. The model combines the representational power of a
probabilistic generative grammar with the inferential power of Bayesian
induction.


Computational Social Dynamics: Analyzing the Face-level Interactions in
  a Group

  Interactional synchrony refers to how the speech or behavior of two or more
people involved in a conversation become more finely synchronized with each
other, and they can appear to behave almost in direct response to one another.
Studies have shown that interactional synchrony is a hallmark of relationships,
and is produced as a result of rapport. %Research has also shown that up to
two-thirds of human communication occurs via nonverbal channels such as
gestures (or body movements), facial expressions, \etc.
  In this work, we use computer vision based methods to extract nonverbal cues,
specifically from the face, and develop a model to measure interactional
synchrony based on those cues. This paper illustrates a novel method of
constructing a dynamic deep neural architecture, specifically made up of
intermediary long short-term memory networks (LSTMs), useful for learning and
predicting the extent of synchrony between two or more processes, by emulating
the nonlinear dependencies between them. On a synthetic dataset, where pairs of
sequences were generated from a Gaussian process with known covariates, the
architecture could successfully determine the covariance values of the
generating process within an error of 0.5% when tested on 100 pairs of
interacting signals. On a real-life dataset involving groups of three people,
the model successfully estimated the extent of synchrony of each group on a
scale of 1 to 5, with an overall prediction mean of $2.96%$ error when
performing 5-fold validation, as compared to 26.1% on the random permutations
serving as the control baseline.


An Analysis of Random Projections in Cancelable Biometrics

  With increasing concerns about security, the need for highly secure physical
biometrics-based authentication systems utilizing \emph{cancelable biometric}
technologies is on the rise. Because the problem of cancelable template
generation deals with the trade-off between template security and matching
performance, many state-of-the-art algorithms successful in generating high
quality cancelable biometrics all have random projection as one of their early
processing steps. This paper therefore presents a formal analysis of why random
projections is an essential step in cancelable biometrics. By formally defining
the notion of an \textit{Independent Subspace Structure} for datasets, it can
be shown that random projection preserves the subspace structure of data
vectors generated from a union of independent linear subspaces. The bound on
the minimum number of random vectors required for this to hold is also derived
and is shown to depend logarithmically on the number of data samples, not only
in independent subspaces but in disjoint subspace settings as well. The
theoretical analysis presented is supported in detail with empirical results on
real-world face recognition datasets.


Dimensionality Reduction with Subspace Structure Preservation

  Modeling data as being sampled from a union of independent subspaces has been
widely applied to a number of real world applications. However, dimensionality
reduction approaches that theoretically preserve this independence assumption
have not been well studied. Our key contribution is to show that $2K$
projection vectors are sufficient for the independence preservation of any $K$
class data sampled from a union of independent subspaces. It is this
non-trivial observation that we use for designing our dimensionality reduction
technique. In this paper, we propose a novel dimensionality reduction algorithm
that theoretically preserves this structure for a given dataset. We support our
theoretical analysis with empirical results on both synthetic and real world
data achieving \textit{state-of-the-art} results compared to popular
dimensionality reduction techniques.


Is Joint Training Better for Deep Auto-Encoders?

  Traditionally, when generative models of data are developed via deep
architectures, greedy layer-wise pre-training is employed. In a well-trained
model, the lower layer of the architecture models the data distribution
conditional upon the hidden variables, while the higher layers model the hidden
distribution prior. But due to the greedy scheme of the layerwise training
technique, the parameters of lower layers are fixed when training higher
layers. This makes it extremely challenging for the model to learn the hidden
distribution prior, which in turn leads to a suboptimal model for the data
distribution. We therefore investigate joint training of deep autoencoders,
where the architecture is viewed as one stack of two or more single-layer
autoencoders. A single global reconstruction objective is jointly optimized,
such that the objective for the single autoencoders at each layer acts as a
local, layer-level regularizer. We empirically evaluate the performance of this
joint training scheme and observe that it not only learns a better data model,
but also learns better higher layer representations, which highlights its
potential for unsupervised feature learning. In addition, we find that the
usage of regularizations in the joint training scheme is crucial in achieving
good performance. In the supervised setting, joint training also shows superior
performance when training deeper models. The joint training framework can thus
provide a platform for investigating more efficient usage of different types of
regularizers, especially in light of the growing volumes of available unlabeled
data.


