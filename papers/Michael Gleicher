Inferring geometric constraints in human demonstrations

  This paper presents an approach for inferring geometric constraints in human
demonstrations. In our method, geometric constraint models are built to create
representations of kinematic constraints such as fixed point, axial rotation,
prismatic motion, planar motion and others across multiple degrees of freedom.
Our method infers geometric constraints using both kinematic and force/torque
information. The approach first fits all the constraint models using kinematic
information and evaluates them individually using position, force and moment
criteria. Our approach does not require information about the constraint type
or contact geometry; it can determine both simultaneously. We present
experimental evaluations using instrumented tongs that show how constraints can
be robustly inferred in recordings of human demonstrations.


Visual Designs for Binned Aggregation of Multi-Class Scatterplots

  Point sets in 2D with multiple classes are a common type of data. A canonical
visualization design for them are scatterplots, which do not scale to large
collections of points. For these larger data sets, binned aggregation (or
binning) is often used to summarize the data, with many possible design
alternatives for creating effective visual representations of these summaries.
There are a wide range of designs to show summaries of 2D multi-class point
data, each capable of supporting different analysis tasks. In this paper, we
explore the space of visual designs for such data, and provide design
guidelines for different analysis scenarios. To support these guidelines, we
compile a set of abstract tasks and ground them in concrete examples using
multiple sample datasets. We then assess designs, and survey a range of design
decisions, considering their appropriateness to the tasks. In addition, we
provide a web-based implementation to experiment with design choices,
supporting the validation of designs based on task needs.


Characterizing Input Methods for Human-to-robot Demonstrations

  Human demonstrations are important in a range of robotics applications, and
are created with a variety of input methods. However, the design space for
these input methods has not been extensively studied. In this paper, focusing
on demonstrations of hand-scale object manipulation tasks to robot arms with
two-finger grippers, we identify distinct usage paradigms in robotics that
utilize human-to-robot demonstrations, extract abstract features that form a
design space for input methods, and characterize existing input methods as well
as a novel input method that we introduce, the instrumented tongs. We detail
the design specifications for our method and present a user study that compares
it against three common input methods: free-hand manipulation, kinesthetic
guidance, and teleoperation. Study results show that instrumented tongs provide
high quality demonstrations and a positive experience for the demonstrator
while offering good correspondence to the target robot.


Visual Analytics for Automated Model Discovery

  A recent advancement in the machine learning community is the development of
automated machine learning (autoML) systems, such as autoWeka or Google's Cloud
AutoML, which automate the model selection and tuning process. However, while
autoML tools give users access to arbitrarily complex models, they typically
return those models with little context or explanation. Visual analytics can be
helpful in giving a user of autoML insight into their data, and a more complete
understanding of the models discovered by autoML, including differences between
multiple models. In this work, we describe how visual analytics for automated
model discovery differs from traditional visual analytics for machine learning.
First, we propose an architecture based on an extension of existing visual
analytics frameworks. Then we describe a prototype system Snowcat, developed
according to the presented framework and architecture, that aids users in
generating models for a diverse set of data and modeling tasks.


