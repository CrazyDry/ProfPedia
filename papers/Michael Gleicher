Inferring geometric constraints in human demonstrations

  This paper presents an approach for inferring geometric constraints in humandemonstrations. In our method, geometric constraint models are built to createrepresentations of kinematic constraints such as fixed point, axial rotation,prismatic motion, planar motion and others across multiple degrees of freedom.Our method infers geometric constraints using both kinematic and force/torqueinformation. The approach first fits all the constraint models using kinematicinformation and evaluates them individually using position, force and momentcriteria. Our approach does not require information about the constraint typeor contact geometry; it can determine both simultaneously. We presentexperimental evaluations using instrumented tongs that show how constraints canbe robustly inferred in recordings of human demonstrations.

Visual Designs for Binned Aggregation of Multi-Class Scatterplots

  Point sets in 2D with multiple classes are a common type of data. A canonicalvisualization design for them are scatterplots, which do not scale to largecollections of points. For these larger data sets, binned aggregation (orbinning) is often used to summarize the data, with many possible designalternatives for creating effective visual representations of these summaries.There are a wide range of designs to show summaries of 2D multi-class pointdata, each capable of supporting different analysis tasks. In this paper, weexplore the space of visual designs for such data, and provide designguidelines for different analysis scenarios. To support these guidelines, wecompile a set of abstract tasks and ground them in concrete examples usingmultiple sample datasets. We then assess designs, and survey a range of designdecisions, considering their appropriateness to the tasks. In addition, weprovide a web-based implementation to experiment with design choices,supporting the validation of designs based on task needs.

Characterizing Input Methods for Human-to-robot Demonstrations

  Human demonstrations are important in a range of robotics applications, andare created with a variety of input methods. However, the design space forthese input methods has not been extensively studied. In this paper, focusingon demonstrations of hand-scale object manipulation tasks to robot arms withtwo-finger grippers, we identify distinct usage paradigms in robotics thatutilize human-to-robot demonstrations, extract abstract features that form adesign space for input methods, and characterize existing input methods as wellas a novel input method that we introduce, the instrumented tongs. We detailthe design specifications for our method and present a user study that comparesit against three common input methods: free-hand manipulation, kinestheticguidance, and teleoperation. Study results show that instrumented tongs providehigh quality demonstrations and a positive experience for the demonstratorwhile offering good correspondence to the target robot.

Visual Analytics for Automated Model Discovery

  A recent advancement in the machine learning community is the development ofautomated machine learning (autoML) systems, such as autoWeka or Google's CloudAutoML, which automate the model selection and tuning process. However, whileautoML tools give users access to arbitrarily complex models, they typicallyreturn those models with little context or explanation. Visual analytics can behelpful in giving a user of autoML insight into their data, and a more completeunderstanding of the models discovered by autoML, including differences betweenmultiple models. In this work, we describe how visual analytics for automatedmodel discovery differs from traditional visual analytics for machine learning.First, we propose an architecture based on an extension of existing visualanalytics frameworks. Then we describe a prototype system Snowcat, developedaccording to the presented framework and architecture, that aids users ingenerating models for a diverse set of data and modeling tasks.

