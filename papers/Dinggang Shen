Population-Guided Large Margin Classifier for High-Dimension Low
  -Sample-Size Problems

  Various applications in different fields, such as gene expression analysis or
computer vision, suffer from data sets with high-dimensional low-sample-size
(HDLSS), which has posed significant challenges for standard statistical and
modern machine learning methods. In this paper, we propose a novel linear
binary classifier, denoted by population-guided large margin classifier
(PGLMC), which is applicable to any sorts of data, including HDLSS. PGLMC is
conceived with a projecting direction w given by the comprehensive
consideration of local structural information of the hyperplane and the
statistics of the training samples. Our proposed model has several advantages
compared to those widely used approaches. First, it is not sensitive to the
intercept term b. Second, it operates well with imbalanced data. Third, it is
relatively simple to be implemented based on Quadratic Programming. Fourth, it
is robust to the model specification for various real applications. The
theoretical properties of PGLMC are proven. We conduct a series of evaluations
on two simulated and six real-world benchmark data sets, including DNA
classification, digit recognition, medical image analysis, and face
recognition. PGLMC outperforms the state-of-the-art classification methods in
most cases, or at least obtains comparable results.


Tensor Generalized Estimating Equations for Longitudinal Imaging
  Analysis

  In an increasing number of neuroimaging studies, brain images, which are in
the form of multidimensional arrays (tensors), have been collected on multiple
subjects at multiple time points. Of scientific interest is to analyze such
massive and complex longitudinal images to diagnose neurodegenerative disorders
and to identify disease relevant brain regions. In this article, we treat those
problems in a unifying regression framework with image predictors, and propose
tensor generalized estimating equations (GEE) for longitudinal imaging
analysis. The GEE approach takes into account intra-subject correlation of
responses, whereas a low rank tensor decomposition of the coefficient array
enables effective estimation and prediction with limited sample size. We
propose an efficient estimation algorithm, study the asymptotics in both fixed
$p$ and diverging $p$ regimes, and also investigate tensor GEE with
regularization that is particularly useful for region selection. The efficacy
of the proposed tensor GEE is demonstrated on both simulated data and a real
data set from the Alzheimer's Disease Neuroimaging Initiative (ADNI).


Medical Image Synthesis with Context-Aware Generative Adversarial
  Networks

  Computed tomography (CT) is critical for various clinical applications, e.g.,
radiotherapy treatment planning and also PET attenuation correction. However,
CT exposes radiation during acquisition, which may cause side effects to
patients. Compared to CT, magnetic resonance imaging (MRI) is much safer and
does not involve any radiations. Therefore, recently, researchers are greatly
motivated to estimate CT image from its corresponding MR image of the same
subject for the case of radiotherapy planning. In this paper, we propose a
data-driven approach to address this challenging problem. Specifically, we
train a fully convolutional network to generate CT given an MR image. To better
model the nonlinear relationship from MRI to CT and to produce more realistic
images, we propose to use the adversarial training strategy and an image
gradient difference loss function. We further apply AutoContext Model to
implement a context-aware generative adversarial network. Experimental results
show that our method is accurate and robust for predicting CT images from MRI
images, and also outperforms three state-of-the-art methods under comparison.


BIRNet: Brain Image Registration Using Dual-Supervised Fully
  Convolutional Networks

  In this paper, we propose a deep learning approach for image registration by
predicting deformation from image appearance. Since obtaining ground-truth
deformation fields for training can be challenging, we design a fully
convolutional network that is subject to dual-guidance: (1) Coarse guidance
using deformation fields obtained by an existing registration method; and (2)
Fine guidance using image similarity. The latter guidance helps avoid overly
relying on the supervision from the training deformation fields, which could be
inaccurate. For effective training, we further improve the deep convolutional
network with gap filling, hierarchical loss, and multi-source strategies.
Experiments on a variety of datasets show promising registration accuracy and
efficiency compared with state-of-the-art methods.


Robust Group Comparison Using Non-Parametric Block-Based Statistics

  Voxel-based analysis methods localize brain structural differences by
performing voxel-wise statistical comparisons on two groups of images aligned
to a common space. This procedure requires highly accurate registration as well
as a sufficiently large dataset. However, in practice, the registration
algorithms are not perfect due to noise, artifacts, and complex structural
variations. The sample size is also limited due to low disease prevalence,
recruitment difficulties, and demographic matching issues. To address these
issues, in this paper, we propose a method, called block-based statistic (BBS),
for robust group comparison. BBS consists of two major components: Block
matching and permutation test. Specifically, based on two group of images
aligned to a common space, we first perform block matching so that structural
misalignments can be corrected. Then, based on results given by block matching,
we conduct robust non-parametric statistical inference based on permutation
test. Extensive experiments were performed on synthetic data and the real
diffusion MR data of mild cognitive impairment patients. The experimental
results indicate that BBS significantly improves statistical power,
notwithstanding the small sample size.


Global Deep Learning Methods for Multimodality Isointense Infant Brain
  Image Segmentation

  An important step in early brain development study is to perform automatic
segmentation of infant brain magnetic resonance (MR) images into cerebrospinal
fluid (CSF), gray matter (GM) and white matter (WM) regions. This task is
especially challenging in the isointense stage (approximately 6-8 months of
age) when GM and WM exhibit similar levels of intensities in MR images. Deep
learning has shown its great promise in various image segmentation tasks.
However, existing models do not have an efficient and effective way to
aggregate global information. They also suffer from information loss during
up-sampling operations. In this work, we address these problems by proposing a
global aggregation block, which can be flexibly used for global information
fusion. We build a novel model based on 3D U-Net to make fast and accurate
voxel-wise dense prediction. We perform thorough experiments, and results
indicate that our model outperforms previous best models significantly on 3D
multimodality isointense infant brain MR image segmentation.


Mitigating Gyral Bias in Cortical Tractography via Asymmetric Fiber
  Orientation Distributions

  Diffusion tractography in brain connectomics often involves tracing axonal
trajectories across gray-white matter boundaries in gyral blades of complex
cortical convolutions. To date, gyral bias is observed in most tractography
algorithms with streamlines predominantly terminating at gyral crowns instead
of sulcal banks. This work demonstrates that asymmetric fiber orientation
distribution functions (AFODFs), computed via a multi-tissue global estimation
framework, can mitigate the effects of gyral bias, enabling fiber streamlines
at gyral blades to make sharper turns into the cortical gray matter. We use
ex-vivo data of an adult rhesus macaque and in-vivo data from the Human
Connectome Project (HCP) to show that the fiber streamlines given by AFODFs
bend more naturally into the cortex than the conventional symmetric FODFs in
typical gyral blades. We demonstrate that AFODF tractography improves
cortico-cortical connectivity and provides highly consistent outcomes between
two different field strengths (3T and 7T).


Real-Time Quality Assessment of Pediatric MRI via Semi-Supervised Deep
  Nonlocal Residual Neural Networks

  In this paper, we introduce an image quality assessment (IQA) method for
pediatric T1- and T2-weighted MR images. IQA is first performed slice-wise
using a nonlocal residual neural network (NR-Net) and then volume-wise by
agglomerating the slice QA results using random forest. Our method requires
only a small amount of quality-annotated images for training and is designed to
be robust to annotation noise that might occur due to rater errors and the
inevitable mix of good and bad slices in an image volume. Using a small set of
quality-assessed images, we pre-train NR-Net to annotate each image slice with
an initial quality rating (i.e., pass, questionable, fail), which we then
refine by semi-supervised learning and iterative self-training. Experimental
results demonstrate that our method, trained using only samples of modest size,
exhibit great generalizability, capable of real-time (milliseconds per volume)
large-scale IQA with near-perfect accuracy.


FRNET: Flattened Residual Network for Infant MRI Skull Stripping

  Skull stripping for brain MR images is a basic segmentation task. Although
many methods have been proposed, most of them focused mainly on the adult MR
images. Skull stripping for infant MR images is more challenging due to the
small size and dynamic intensity changes of brain tissues during the early
ages. In this paper, we propose a novel CNN based framework to robustly extract
brain region from infant MR image without any human assistance. Specifically,
we propose a simplified but more robust flattened residual network architecture
(FRnet). We also introduce a new boundary loss function to highlight ambiguous
and low contrast regions between brain and non-brain regions. To make the whole
framework more robust to MR images with different imaging quality, we further
introduce an artifact simulator for data augmentation. We have trained and
tested our proposed framework on a large dataset (N=343), covering newborns to
48-month-olds, and obtained performance better than the state-of-the-art
methods in all age groups.


Learning Discriminative Bayesian Networks from High-dimensional
  Continuous Neuroimaging Data

  Due to its causal semantics, Bayesian networks (BN) have been widely employed
to discover the underlying data relationship in exploratory studies, such as
brain research. Despite its success in modeling the probability distribution of
variables, BN is naturally a generative model, which is not necessarily
discriminative. This may cause the ignorance of subtle but critical network
changes that are of investigation values across populations. In this paper, we
propose to improve the discriminative power of BN models for continuous
variables from two different perspectives. This brings two general
discriminative learning frameworks for Gaussian Bayesian networks (GBN). In the
first framework, we employ Fisher kernel to bridge the generative models of GBN
and the discriminative classifiers of SVMs, and convert the GBN parameter
learning to Fisher kernel learning via minimizing a generalization error bound
of SVMs. In the second framework, we employ the max-margin criterion and build
it directly upon GBN models to explicitly optimize the classification
performance of the GBNs. The advantages and disadvantages of the two frameworks
are discussed and experimentally compared. Both of them demonstrate strong
power in learning discriminative parameters of GBNs for neuroimaging based
brain network analysis, as well as maintaining reasonable representation
capacity. The contributions of this paper also include a new Directed Acyclic
Graph (DAG) constraint with theoretical guarantee to ensure the graph validity
of GBN.


Deep Embedding Convolutional Neural Network for Synthesizing CT Image
  from T1-Weighted MR Image

  Recently, more and more attention is drawn to the field of medical image
synthesis across modalities. Among them, the synthesis of computed tomography
(CT) image from T1-weighted magnetic resonance (MR) image is of great
importance, although the mapping between them is highly complex due to large
gaps of appearances of the two modalities. In this work, we aim to tackle this
MR-to-CT synthesis by a novel deep embedding convolutional neural network
(DECNN). Specifically, we generate the feature maps from MR images, and then
transform these feature maps forward through convolutional layers in the
network. We can further compute a tentative CT synthesis from the midway of the
flow of feature maps, and then embed this tentative CT synthesis back to the
feature maps. This embedding operation results in better feature maps, which
are further transformed forward in DECNN. After repeat-ing this embedding
procedure for several times in the network, we can eventually synthesize a
final CT image in the end of the DECNN. We have validated our proposed method
on both brain and prostate datasets, by also compar-ing with the
state-of-the-art methods. Experimental results suggest that our DECNN (with
repeated embedding op-erations) demonstrates its superior performances, in
terms of both the perceptive quality of the synthesized CT image and the
run-time cost for synthesizing a CT image.


Interactive Medical Image Segmentation via Point-Based Interaction and
  Sequential Patch Learning

  Due to low tissue contrast, irregular object appearance, and unpredictable
location variation, segmenting the objects from different medical imaging
modalities (e.g., CT, MR) is considered as an important yet challenging task.
In this paper, we present a novel method for interactive medical image
segmentation with the following merits. (1) Our design is fundamentally
different from previous pure patch-based and image-based segmentation methods.
We observe that during delineation, the physician repeatedly check the
inside-outside intensity changing to determine the boundary, which indicates
that comparison in an inside-outside manner is extremely important. Thus, we
innovatively model our segmentation task as learning the representation of the
bi-directional sequential patches, starting from (or ending in) the given
central point of the object. This can be realized by our proposed ConvRNN
network embedded with a gated memory propagation unit. (2) Unlike previous
interactive methods (requiring bounding box or seed points), we only ask the
physician to merely click on the rough central point of the object before
segmentation, which could simultaneously enhance the performance and reduce the
segmentation time. (3) We utilize our method in a multi-level framework for
better performance. We systematically evaluate our method in three different
segmentation tasks including CT kidney tumor, MR prostate, and PROMISE12
challenge, showing promising results compared with state-of-the-art methods.
The code is available here:
\href{https://github.com/sunalbert/Sequential-patch-based-segmentation}{Sequential-patch-based-segmentation}.


Deep Learning based Inter-Modality Image Registration Supervised by
  Intra-Modality Similarity

  Non-rigid inter-modality registration can facilitate accurate information
fusion from different modalities, but it is challenging due to the very
different image appearances across modalities. In this paper, we propose to
train a non-rigid inter-modality image registration network, which can directly
predict the transformation field from the input multimodal images, such as CT
and MR images. In particular, the training of our inter-modality registration
network is supervised by intra-modality similarity metric based on the
available paired data, which is derived from a pre-aligned CT and MR dataset.
Specifically, in the training stage, to register the input CT and MR images,
their similarity is evaluated on the warped MR image and the MR image that is
paired with the input CT. So that, the intra-modality similarity metric can be
directly applied to measure whether the input CT and MR images are well
registered. Moreover, we use the idea of dual-modality fashion, in which we
measure the similarity on both CT modality and MR modality. In this way, the
complementary anatomies in both modalities can be jointly considered to more
accurately train the inter-modality registration network. In the testing stage,
the trained inter-modality registration network can be directly applied to
register the new multimodal images without any paired data. Experimental
results have shown that, the proposed method can achieve promising accuracy and
efficiency for the challenging non-rigid inter-modality registration task and
also outperforms the state-of-the-art approaches.


Angular Upsampling in Infant Diffusion MRI Using Neighborhood Matching
  in x-q Space

  Diffusion MRI requires sufficient coverage of the diffusion wavevector space,
also known as the q-space, to adequately capture the pattern of water diffusion
in various directions and scales. As a result, the acquisition time can be
prohibitive for individuals who are unable to stay still in the scanner for an
extensive period of time, such as infants. To address this problem, in this
paper we harness non-local self-similar information in the x-q space of
diffusion MRI data for q-space upsampling. Specifically, we first perform
neighborhood matching to establish the relationships of signals in x-q space.
The signal relationships are then used to regularize an ill-posed inverse
problem related to the estimation of high angular resolution diffusion MRI data
from its low-resolution counterpart. Our framework allows information from
curved white matter structures to be used for effective regularization of the
otherwise ill-posed problem. Extensive evaluations using synthetic and infant
diffusion MRI data demonstrate the effectiveness of our method. Compared with
the widely adopted interpolation methods using spherical radial basis functions
and spherical harmonics, our method is able to produce high angular resolution
diffusion MRI data with greater quality, both qualitatively and quantitatively.


Noise Reduction in Diffusion MRI Using Non-Local Self-Similar
  Information in Joint x-q Space

  Diffusion MRI affords valuable insights into white matter microstructures,
but suffers from low signal-to-noise ratio (SNR), especially at high diffusion
weighting (i.e., b-value). To avoid time-intensive repeated acquisition,
post-processing algorithms are often used to reduce noise. Among existing
methods, non-local means (NLM) has been shown to be particularly effective.
However, most NLM algorithms for diffusion MRI focus on patch matching in the
spatial domain (i.e., x-space) and disregard the fact that the data live in a
combined 6D space covering both spatial domain and diffusion wavevector domain
(i.e., q-space). This drawback leads to inaccurate patch matching in curved
white matter structures and hence the inability to effectively use recurrent
information for noise reduction. The goal of this paper is to overcome this
limitation by extending NLM to the joint x-q space. Specifically, we define for
each point in the x-q space a spherical patch from which we extract
rotation-invariant features for patch matching. The ability to perform patch
matching across q-samples allows patches from differentially orientated
structures to be used for effective noise removal. Extensive experiments on
synthetic, repeated acquisition, and real data demonstrate that our method
outperforms state-of-the-art methods, both qualitatively and quantitatively.


Deep Chronnectome Learning via Full Bidirectional Long Short-Term Memory
  Networks for MCI Diagnosis

  Brain functional connectivity (FC) extracted from resting-state fMRI
(RS-fMRI) has become a popular approach for disease diagnosis, where
discriminating subjects with mild cognitive impairment (MCI) from normal
controls (NC) is still one of the most challenging problems. Dynamic functional
connectivity (dFC), consisting of time-varying spatiotemporal dynamics, may
characterize "chronnectome" diagnostic information for improving MCI
classification. However, most of the current dFC studies are based on detecting
discrete major brain status via spatial clustering, which ignores rich
spatiotemporal dynamics contained in such chronnectome. We propose Deep
Chronnectome Learning for exhaustively mining the comprehensive information,
especially the hidden higher-level features, i.e., the dFC time series that may
add critical diagnostic power for MCI classification. To this end, we devise a
new Fully-connected Bidirectional Long Short-Term Memory Network (Full-BiLSTM)
to effectively learn the periodic brain status changes using both past and
future information for each brief time segment and then fuse them to form the
final output. We have applied our method to a rigorously built large-scale
multi-site database (i.e., with 164 data from NCs and 330 from MCIs, which can
be further augmented by 25 folds). Our method outperforms other
state-of-the-art approaches with an accuracy of 73.6% under solid
cross-validations. We also made extensive comparisons among multiple variants
of LSTM models. The results suggest high feasibility of our method with
promising value also for other brain disorder diagnoses.


Deep Morphological Simplification Network (MS-Net) for Guided
  Registration of Brain Magnetic Resonance Images

  Objective: Deformable brain MR image registration is challenging due to large
inter-subject anatomical variation. For example, the highly complex cortical
folding pattern makes it hard to accurately align corresponding cortical
structures of individual images. In this paper, we propose a novel deep
learning way to simplify the difficult registration problem of brain MR images.
Methods: We train a morphological simplification network (MS-Net), which can
generate a "simple" image with less anatomical details based on the "complex"
input. With MS-Net, the complexity of the fixed image or the moving image under
registration can be reduced gradually, thus building an individual
(simplification) trajectory represented by MS-Net outputs. Since the generated
images at the ends of the two trajectories (of the fixed and moving images) are
so simple and very similar in appearance, they are easy to register. Thus, the
two trajectories can act as a bridge to link the fixed and the moving images,
and guide their registration. Results: Our experiments show that the proposed
method can achieve highly accurate registration performance on different
datasets (i.e., NIREP, LPBA, IBSR, CUMC, and MGH). Moreover, the method can be
also easily transferred across diverse image datasets and obtain superior
accuracy on surface alignment. Conclusion and Significance: We propose MS-Net
as a powerful and flexible tool to simplify brain MR images and their
registration. To our knowledge, this is the first work to simplify brain MR
image registration by deep learning, instead of estimating deformation field
directly.


Spherical U-Net on Cortical Surfaces: Methods and Applications

  Convolutional Neural Networks (CNNs) have been providing the state-of-the-art
performance for learning-related problems involving 2D/3D images in Euclidean
space. However, unlike in the Euclidean space, the shapes of many structures in
medical imaging have a spherical topology in a manifold space, e.g., brain
cortical or subcortical surfaces represented by triangular meshes, with large
inter-subject and intrasubject variations in vertex number and local
connectivity. Hence, there is no consistent neighborhood definition and thus no
straightforward convolution/transposed convolution operations for
cortical/subcortical surface data. In this paper, by leveraging the regular and
consistent geometric structure of the resampled cortical surface mapped onto
the spherical space, we propose a novel convolution filter analogous to the
standard convolution on the image grid. Accordingly, we develop corresponding
operations for convolution, pooling, and transposed convolution for spherical
surface data and thus construct spherical CNNs. Specifically, we propose the
Spherical U-Net architecture by replacing all operations in the standard U-Net
with their spherical operation counterparts. We then apply the Spherical U-Net
to two challenging and neuroscientifically important tasks in infant brains:
cortical surface parcellation and cortical attribute map development
prediction. Both applications demonstrate the competitive performance in the
accuracy, computational efficiency, and effectiveness of our proposed Spherical
U-Net, in comparison with the state-of-the-art methods.


Regularized Spherical Polar Fourier Diffusion MRI with Optimal
  Dictionary Learning

  Compressed Sensing (CS) takes advantage of signal sparsity or compressibility
and allows superb signal reconstruction from relatively few measurements. Based
on CS theory, a suitable dictionary for sparse representation of the signal is
required. In diffusion MRI (dMRI), CS methods were proposed to reconstruct
diffusion-weighted signal and the Ensemble Average Propagator (EAP), and there
are two kinds of Dictionary Learning (DL) methods: 1) Discrete Representation
DL (DR-DL), and 2) Continuous Representation DL (CR-DL). DR-DL is susceptible
to numerical inaccuracy owing to interpolation and regridding errors in a
discretized q-space. In this paper, we propose a novel CR-DL approach, called
Dictionary Learning - Spherical Polar Fourier Imaging (DL-SPFI) for effective
compressed-sensing reconstruction of the q-space diffusion-weighted signal and
the EAP. In DL-SPFI, an dictionary that sparsifies the signal is learned from
the space of continuous Gaussian diffusion signals. The learned dictionary is
then adaptively applied to different voxels using a weighted LASSO framework
for robust signal reconstruction. The adaptive dictionary is proved to be
optimal. Compared with the start-of-the-art CR-DL and DR-DL methods proposed by
Merlet et al. and Bilgic et al., espectively, our work offers the following
advantages. First, the learned dictionary is proved to be optimal for Gaussian
diffusion signals. Second, to our knowledge, this is the first work to learn a
voxel-adaptive dictionary. The importance of the adaptive dictionary in EAP
reconstruction will be demonstrated theoretically and empirically. Third,
optimization in DL-SPFI is only performed in a small subspace resided by the
SPF coefficients, as opposed to the q-space approach utilized by Merlet et al.
The experiment results demonstrate the advantages of DL-SPFI over the original
SPF basis and Bilgic et al.'s method.


Single- and Multiple-Shell Uniform Sampling Schemes for Diffusion MRI
  Using Spherical Codes

  In diffusion MRI (dMRI), a good sampling scheme is important for efficient
acquisition and robust reconstruction. Diffusion weighted signal is normally
acquired on single or multiple shells in q-space. Signal samples are typically
distributed uniformly on different shells to make them invariant to the
orientation of structures within tissue, or the laboratory coordinate frame.
The Electrostatic Energy Minimization (EEM) method, originally proposed for
single shell sampling scheme in dMRI, was recently generalized to multi-shell
schemes, called Generalized EEM (GEEM). GEEM has been successfully used in the
Human Connectome Project (HCP). However, EEM does not directly address the goal
of optimal sampling, i.e., achieving large angular separation between sampling
points. In this paper, we propose a more natural formulation, called Spherical
Code (SC), to directly maximize the minimal angle between different samples in
single or multiple shells. We consider not only continuous problems to design
single or multiple shell sampling schemes, but also discrete problems to
uniformly extract sub-sampled schemes from an existing single or multiple shell
scheme, and to order samples in an existing scheme. We propose five algorithms
to solve the above problems, including an incremental SC (ISC), a sophisticated
greedy algorithm called Iterative Maximum Overlap Construction (IMOC), an 1-Opt
greedy method, a Mixed Integer Linear Programming (MILP) method, and a
Constrained Non-Linear Optimization (CNLO) method. To our knowledge, this is
the first work to use the SC formulation for single or multiple shell sampling
schemes in dMRI. Experimental results indicate that SC methods obtain larger
angular separation and better rotational invariance than the state-of-the-art
EEM and GEEM. The related codes and a tutorial have been released in DMRITool.


Deep CNN ensembles and suggestive annotations for infant brain MRI
  segmentation

  Precise 3D segmentation of infant brain tissues is an essential step towards
comprehensive volumetric studies and quantitative analysis of early brain
developement. However, computing such segmentations is very challenging,
especially for 6-month infant brain, due to the poor image quality, among other
difficulties inherent to infant brain MRI, e.g., the isointense contrast
between white and gray matter and the severe partial volume effect due to small
brain sizes. This study investigates the problem with an ensemble of semi-dense
fully convolutional neural networks (CNNs), which employs T1-weighted and
T2-weighted MR images as input. We demonstrate that the ensemble agreement is
highly correlated with the segmentation errors. Therefore, our method provides
measures that can guide local user corrections. To the best of our knowledge,
this work is the first ensemble of 3D CNNs for suggesting annotations within
images. Furthermore, inspired by the very recent success of dense networks, we
propose a novel architecture, SemiDenseNet, which connects all convolutional
layers directly to the end of the network. Our architecture allows the
efficient propagation of gradients during training, while limiting the number
of parameters, requiring one order of magnitude less parameters than popular
medical image segmentation networks such as 3D U-Net. Another contribution of
our work is the study of the impact that early or late fusions of multiple
image modalities might have on the performances of deep architectures. We
report evaluations of our method on the public data of the MICCAI iSEG-2017
Challenge on 6-month infant brain MRI segmentation, and show very competitive
results among 21 teams, ranking first or second in most metrics.


Identifying the Best Machine Learning Algorithms for Brain Tumor
  Segmentation, Progression Assessment, and Overall Survival Prediction in the
  BRATS Challenge

  Gliomas are the most common primary brain malignancies, with different
degrees of aggressiveness, variable prognosis and various heterogeneous
histologic sub-regions, i.e., peritumoral edematous/invaded tissue, necrotic
core, active and non-enhancing core. This intrinsic heterogeneity is also
portrayed in their radio-phenotype, as their sub-regions are depicted by
varying intensity profiles disseminated across multi-parametric magnetic
resonance imaging (mpMRI) scans, reflecting varying biological properties.
Their heterogeneous shape, extent, and location are some of the factors that
make these tumors difficult to resect, and in some cases inoperable. The amount
of resected tumor is a factor also considered in longitudinal scans, when
evaluating the apparent tumor for potential diagnosis of progression.
Furthermore, there is mounting evidence that accurate segmentation of the
various tumor sub-regions can offer the basis for quantitative image analysis
towards prediction of patient overall survival. This study assesses the
state-of-the-art machine learning (ML) methods used for brain tumor image
analysis in mpMRI scans, during the last seven instances of the International
Brain Tumor Segmentation (BraTS) challenge, i.e., 2012-2018. Specifically, we
focus on i) evaluating segmentations of the various glioma sub-regions in
pre-operative mpMRI scans, ii) assessing potential tumor progression by virtue
of longitudinal growth of tumor sub-regions, beyond use of the RECIST/RANO
criteria, and iii) predicting the overall survival from pre-operative mpMRI
scans of patients that underwent gross total resection. Finally, we investigate
the challenge of identifying the best ML algorithms for each of these tasks,
considering that apart from being diverse on each instance of the challenge,
the multi-institutional mpMRI BraTS dataset has also been a continuously
evolving/growing dataset.


