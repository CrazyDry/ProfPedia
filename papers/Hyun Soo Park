Collective Political Opinion Formation in Nonlinear Social Interaction

  We have presented a numerical model of a collective opinion formation
procedure to explain political phenomena such as two-party and multi-party
systems in politics, political unrest, military coup d'etats and netizen
revolutions. Nonlinear interaction with binary and independent decision making
processes can yield various collective behaviors or collective political
opinions. Statistical physics and nonlinear dynamics may provide useful tools
to study various socio-political dynamics.


Future Localization from an Egocentric Depth Image

  This paper presents a method for future localization: to predict a set of
plausible trajectories of ego-motion given a depth image. We predict paths
avoiding obstacles, between objects, even paths turning around a corner into
space behind objects. As a byproduct of the predicted trajectories of
ego-motion, we discover in the image the empty space occluded by foreground
objects. We use no image based features such as semantic labeling/segmentation
or object detection/recognition for this algorithm. Inspired by proxemics, we
represent the space around a person using an EgoSpace map, akin to an
illustrated tourist map, that measures a likelihood of occlusion at the
egocentric coordinate system. A future trajectory of ego-motion is modeled by a
linear combination of compact trajectory bases allowing us to constrain the
predicted trajectory. We learn the relationship between the EgoSpace map and
trajectory from the EgoMotion dataset providing in-situ measurements of the
future trajectory. A cost function that takes into account partial occlusion
due to foreground objects is minimized to predict a trajectory. This cost
function generates a trajectory that passes through the occluded space, which
allows us to discover the empty space behind the foreground objects. We
quantitatively evaluate our method to show predictive validity and apply to
various real world scenes including walking, shopping, and social interactions.


Polarization-Selective Control of Nonlinear Optomechanical Interactions
  in Subwavelength Elliptical Waveguides

  Photonic devices exhibiting all-optically reconfigurable polarization
dependence with a large dynamic range would be highly attractive for active
polarization control. Here, we report that strongly polarization-selective
nonlinear optomechanical interactions emerge in subwavelength waveguides. By
using full-vectorial finite element analysis, we find that at certain core
ellipticities (aspect ratios) the forward simulated light scattering mediated
by a specific acoustic mode is eliminated for one polarization mode, whereas
that for the other polarization mode is rather enhanced. This intriguing
phenomenon can be explained by the interplay between the electrostrictive force
and radiation pressure and turns out to be tailorable by choice of waveguide
materials.


Enhanced high-temperature performance of GaN light-emitting diodes grown
  on silicon substrates

  We compare the temperature dependence of optical and electrical
characteristics of commercially available GaN light-emitting diodes (LEDs)
grown on silicon and sapphire substrates. Contrary to conventional
expectations, LEDs grown on silicon substrates, commonly referred to as
GaN-on-Si LEDs, show less efficiency droop at higher temperatures even with
more threading dislocations. Analysis of the junction temperature reveals that
GaN-on-Si LEDs have a cooler junction despite sharing identical epitaxial
structures and packaging compared to LEDs grown on sapphire substrates. We also
observe a decrease in ideality factor with increase in ambient temperature for
GaN-on-Si LEDs, indicating an increase in ideal diode current with temperature.
Analysis of the strain and temperature coefficient measurements suggests that
there is an increase in hole transport efficiency within the active region for
GaN-on-Si LEDs compared to the LEDs grown on sapphire, which accounts for the
less temperature-dependent efficiency droop.


HUMBI 1.0: HUman Multiview Behavioral Imaging Dataset

  This paper presents a new dataset called HUMBI - a large corpus of high
fidelity models of behavioral signals in 3D from a diverse population measured
by a massive multi-camera system. With our novel design of a portable imaging
system (consists of 107 HD cameras), we collect human behaviors from 164
subjects across gender, ethnicity, age, and physical condition at a public
venue. Using the multiview image streams, we reconstruct high fidelity models
of five elementary parts: gaze, face, hands, body, and cloth. As a byproduct,
the 3D model provides geometrically consistent image annotation via 2D
projection, e.g., body part segmentation. This dataset is a significant
departure from the existing human datasets that suffers from subject diversity.
We hope the HUMBI opens up a new opportunity for the development for behavioral
imaging.


Precision improvement of MEMS gyros for indoor mobile robots with
  horizontal motion inspired by methods of TRIZ

  In the paper, the problem of precision improvement for the MEMS gyrosensors
on indoor robots with horizontal motion is solved by methods of TRIZ ("the
theory of inventive problem solving").


Customizing First Person Image Through Desired Actions

  This paper studies a problem of inverse visual path planning: creating a
visual scene from a first person action. Our conjecture is that the spatial
arrangement of a first person visual scene is deployed to afford an action, and
therefore, the action can be inversely used to synthesize a new scene such that
the action is feasible. As a proof-of-concept, we focus on linking visual
experiences induced by walking.
  A key innovation of this paper is a concept of ActionTunnel---a 3D virtual
tunnel along the future trajectory encoding what the wearer will visually
experience as moving into the scene. This connects two distinctive first person
images through similar walking paths. Our method takes a first person image
with a user defined future trajectory and outputs a new image that can afford
the future motion. The image is created by combining present and future
ActionTunnels in 3D where the missing pixels in adjoining area are computed by
a generative adversarial network. Our work can provide a travel across
different first person experiences in diverse real world scenes.


Social Behavior Prediction from First Person Videos

  This paper presents a method to predict the future movements (location and
gaze direction) of basketball players as a whole from their first person
videos. The predicted behaviors reflect an individual physical space that
affords to take the next actions while conforming to social behaviors by
engaging to joint attention. Our key innovation is to use the 3D reconstruction
of multiple first person cameras to automatically annotate each other's the
visual semantics of social configurations.
  We leverage two learning signals uniquely embedded in first person videos.
Individually, a first person video records the visual semantics of a spatial
and social layout around a person that allows associating with past similar
situations. Collectively, first person videos follow joint attention that can
link the individuals to a group. We learn the egocentric visual semantics of
group movements using a Siamese neural network to retrieve future trajectories.
We consolidate the retrieved trajectories from all players by maximizing a
measure of social compatibility---the gaze alignment towards joint attention
predicted by their social formation, where the dynamics of joint attention is
learned by a long-term recurrent convolutional network. This allows us to
characterize which social configuration is more plausible and predict future
group trajectories.


3D Semantic Trajectory Reconstruction from 3D Pixel Continuum

  This paper presents a method to reconstruct dense semantic trajectory stream
of human interactions in 3D from synchronized multiple videos. The interactions
inherently introduce self-occlusion and illumination/appearance/shape changes,
resulting in highly fragmented trajectory reconstruction with noisy and coarse
semantic labels. Our conjecture is that among many views, there exists a set of
views that can confidently recognize the visual semantic label of a 3D
trajectory. We introduce a new representation called 3D semantic map---a
probability distribution over the semantic labels per trajectory. We construct
the 3D semantic map by reasoning about visibility and 2D recognition confidence
based on view-pooling, i.e., finding the view that best represents the
semantics of the trajectory. Using the 3D semantic map, we precisely infer all
trajectory labels jointly by considering the affinity between long range
trajectories via estimating their local rigid transformations. This inference
quantitatively outperforms the baseline approaches in terms of predictive
validity, representation robustness, and affinity effectiveness. We demonstrate
that our algorithm can robustly compute the semantic labels of a large scale
trajectory set involving real-world human interactions with object, scenes, and
people.


MONET: Multiview Semi-supervised Keypoint via Epipolar Divergence

  This paper presents MONET---an end-to-end semi-supervised learning framework
for a pose detector using multiview image streams. What differentiates MONET
from existing models is its capability of detecting general subjects including
non-human species without a pre-trained model. A key challenge of such subjects
lies in the limited availability of expert manual annotations, which often
leads to a large bias in the detection model. We address this challenge by
using the epipolar constraint embedded in the unlabeled data in two ways.
First, given a set of the labeled data, the keypoint trajectories can be
reliably reconstructed in 3D using multiview optical flows, resulting in
considerable data augmentation in space and time from nearly exhaustive views.
Second, the detection across views must geometrically agree with each other. We
introduce a new measure of geometric consistency in keypoint distributions
called epipolar divergence---a generalized distance from the epipolar lines to
the corresponding keypoint distribution. Epipolar divergence characterizes when
two view keypoint distributions produces zero reprojection error. We design a
twin network that minimizes the epipolar divergence through stereo
rectification that can significantly alleviate computational complexity and
sampling aliasing in training. We demonstrate that our framework can localize
customized keypoints of diverse species, e.g., humans, dogs, and monkeys.


Multiview Supervision By Registration

  This paper presents a semi-supervised learning framework to train a keypoint
detector using multiview image streams given the limited labeled data
(typically $<$4\%). We leverage the complementary relationship between
multiview geometry and visual tracking to provide three types of supervisionary
signals to utilize the unlabeled data: (1) keypoint detection in one view can
be supervised by other views via the epipolar geometry; (2) a keypoint moves
smoothly over time where its optical flow can be used to temporally supervise
consecutive image frames to each other; (3) visible keypoint in one view is
likely to be visible in the adjacent view. We integrate these three signals in
a differentiable fashion to design a new end-to-end neural network composed of
three pathways. This design allows us to extensively use the unlabeled data to
train the keypoint detector. We show that our approach outperforms existing
detectors including DeepLabCut tailored to the keypoint detection of non-human
species such as monkeys, dogs, and mice.


ECO: Egocentric Cognitive Mapping

  We present a new method to localize a camera within a previously unseen
environment perceived from an egocentric point of view. Although this is, in
general, an ill-posed problem, humans can effortlessly and efficiently
determine their relative location and orientation and navigate into a
previously unseen environments, e.g., finding a specific item in a new grocery
store. To enable such a capability, we design a new egocentric representation,
which we call ECO (Egocentric COgnitive map). ECO is biologically inspired, by
the cognitive map that allows human navigation, and it encodes the surrounding
visual semantics with respect to both distance and orientation. ECO possesses
three main properties: (1) reconfigurability: complex semantics and geometry is
captured via the synthesis of atomic visual representations (e.g., image
patch); (2) robustness: the visual semantics are registered in a geometrically
consistent way (e.g., aligning with respect to the gravity vector,
frontalizing, and rescaling to canonical depth), thus enabling us to learn
meaningful atomic representations; (3) adaptability: a domain adaptation
framework is designed to generalize the learned representation without manual
calibration. As a proof-of-concept, we use ECO to localize a camera within
real-world scenes---various grocery stores---and demonstrate performance
improvements when compared to existing semantic localization approaches.


Multiview Cross-supervision for Semantic Segmentation

  This paper presents a semi-supervised learning framework for a customized
semantic segmentation task using multiview image streams. A key challenge of
the customized task lies in the limited accessibility of the labeled data due
to the requirement of prohibitive manual annotation effort. We hypothesize that
it is possible to leverage multiview image streams that are linked through the
underlying 3D geometry, which can provide an additional supervisionary signal
to train a segmentation model. We formulate a new cross-supervision method
using a shape belief transfer---the segmentation belief in one image is used to
predict that of the other image through epipolar geometry analogous to
shape-from-silhouette. The shape belief transfer provides the upper and lower
bounds of the segmentation for the unlabeled data where its gap approaches
asymptotically to zero as the number of the labeled views increases. We
integrate this theory to design a novel network that is agnostic to camera
calibration, network model, and semantic category and bypasses the intermediate
process of suboptimal 3D reconstruction. We validate this network by
recognizing a customized semantic category per pixel from realworld visual data
including non-human species and a subject of interest in social videos where
attaining large-scale annotation data is infeasible.


BOAO Photometric Survey of Galactic Open Clusters. III. Czernik 24 and
  Czernik 27

  We present BV CCD photometry for the open clusters Czernik 24 and Czernik 27.
These clusters have never been studied before, and we provide, for the first
time, the cluster parameters; reddening, distance, metallicity and age. Czernik
24 is an old open cluster with age 1.8 +/- 0.2 Gyr, metallicity [Fe/H]=-0.41
+/- 0.15 dex, distance modulus (m-M)_0 = 13.1 +/- 0.3 mag (d=4.1 +/- 0.5 kpc),
and reddening E(B-V) = 0.54 +/- 0.12 mag. The parameters for Czernik 27 are
estimated to be age = 0.63 +/- 0.07 Gyr, [Fe/H]= -0.02 +/- 0.10 dex, (m-M)_0 =
13.8 +/- 0.2 mag (d=5.8 +/- 0.5 kpc), and E(B-V) = 0.15 +/- 0.05 mag. The
metallicity and distance values for Czernik 24 are consistent with the relation
between the metallicity and the Galactocentric distance of other old open
clusters. We find the metallicity gradient of 51 old open clusters including
Czernik 24 to be Delta [Fe/H]/Delta R_gc= -0.064 +/- 0.009 dex/kpc.


Optical Characterization of PtSi/Si by Spectroscopic Ellipsometry

  We report optical characterization of PtSi films for thermoelectric device
applications by nondestructive spectroscopic ellipsometry (SE). Pt monolayer
and Pt-Si multilayer which consists of 3 pairs of Pt and Si layers were
deposited on p-doped-silicon substrates by sputtering method and then rapid
annealing process was done to form PtSi films through intermixing of Pt and Si
atoms at the interface. Pseudodielectric function data <{\epsilon}> =
<{\epsilon}1> + i<{\epsilon}2> of the PtSi/Si samples were obtained from 1.12
to 6.52 eV by using spectroscopic ellipsometry. Employing Tauc-Lorentz and
Drude models, the dielectric function ({\epsilon}) of PtSi films were
determined. We found that the composition ratio of Pt:Si is nearly 1:1 for PtSi
monolayer and we observed transitions between occupied and unoccupied states in
Pt 5d states. We also observed formation of PtSi layers in Pt-Si multilayer
sample. The SE results were confirmed by the transmission electron microscopy
and energy dispersive X-ray spectroscopy.


Exploiting Egocentric Object Prior for 3D Saliency Detection

  On a minute-to-minute basis people undergo numerous fluid interactions with
objects that barely register on a conscious level. Recent neuroscientific
research demonstrates that humans have a fixed size prior for salient objects.
This suggests that a salient object in 3D undergoes a consistent transformation
such that people's visual system perceives it with an approximately fixed size.
This finding indicates that there exists a consistent egocentric object prior
that can be characterized by shape, size, depth, and location in the first
person view.
  In this paper, we develop an EgoObject Representation, which encodes these
characteristics by incorporating shape, location, size and depth features from
an egocentric RGBD image. We empirically show that this representation can
accurately characterize the egocentric object prior by testing it on an
egocentric RGBD dataset for three tasks: the 3D saliency detection, future
saliency prediction, and interaction classification. This representation is
evaluated on our new Egocentric RGBD Saliency dataset that includes various
activities such as cooking, dining, and shopping. By using our EgoObject
representation, we outperform previously proposed models for saliency detection
(relative 30% improvement for 3D saliency detection task) on our dataset.
Additionally, we demonstrate that this representation allows us to predict
future salient objects based on the gaze cue and classify people's interactions
with objects.


First Person Action-Object Detection with EgoNet

  Unlike traditional third-person cameras mounted on robots, a first-person
camera, captures a person's visual sensorimotor object interactions from up
close. In this paper, we study the tight interplay between our momentary visual
attention and motor action with objects from a first-person camera. We propose
a concept of action-objects---the objects that capture person's conscious
visual (watching a TV) or tactile (taking a cup) interactions. Action-objects
may be task-dependent but since many tasks share common person-object spatial
configurations, action-objects exhibit a characteristic 3D spatial distance and
orientation with respect to the person.
  We design a predictive model that detects action-objects using EgoNet, a
joint two-stream network that holistically integrates visual appearance (RGB)
and 3D spatial layout (depth and height) cues to predict per-pixel likelihood
of action-objects. Our network also incorporates a first-person coordinate
embedding, which is designed to learn a spatial distribution of the
action-objects in the first-person data. We demonstrate EgoNet's predictive
power, by showing that it consistently outperforms previous baseline
approaches. Furthermore, EgoNet also exhibits a strong generalization ability,
i.e., it predicts semantically meaningful objects in novel first-person
datasets. Our method's ability to effectively detect action-objects could be
used to improve robots' understanding of human-object interactions.


Am I a Baller? Basketball Performance Assessment from First-Person
  Videos

  This paper presents a method to assess a basketball player's performance from
his/her first-person video. A key challenge lies in the fact that the
evaluation metric is highly subjective and specific to a particular evaluator.
We leverage the first-person camera to address this challenge. The
spatiotemporal visual semantics provided by a first-person view allows us to
reason about the camera wearer's actions while he/she is participating in an
unscripted basketball game. Our method takes a player's first-person video and
provides a player's performance measure that is specific to an evaluator's
preference.
  To achieve this goal, we first use a convolutional LSTM network to detect
atomic basketball events from first-person videos. Our network's ability to
zoom-in to the salient regions addresses the issue of a severe camera wearer's
head movement in first-person videos. The detected atomic events are then
passed through the Gaussian mixtures to construct a highly non-linear visual
spatiotemporal basketball assessment feature. Finally, we use this feature to
learn a basketball assessment model from pairs of labeled first-person
basketball videos, for which a basketball expert indicates, which of the two
players is better.
  We demonstrate that despite not knowing the basketball evaluator's criterion,
our model learns to accurately assess the players in real-world games.
Furthermore, our model can also discover basketball events that contribute
positively and negatively to a player's performance.


Unsupervised Learning of Important Objects from First-Person Videos

  A first-person camera, placed at a person's head, captures, which objects are
important to the camera wearer. Most prior methods for this task learn to
detect such important objects from the manually labeled first-person data in a
supervised fashion. However, important objects are strongly related to the
camera wearer's internal state such as his intentions and attention, and thus,
only the person wearing the camera can provide the importance labels. Such a
constraint makes the annotation process costly and limited in scalability.
  In this work, we show that we can detect important objects in first-person
images without the supervision by the camera wearer or even third-person
labelers. We formulate an important detection problem as an interplay between
the 1) segmentation and 2) recognition agents. The segmentation agent first
proposes a possible important object segmentation mask for each image, and then
feeds it to the recognition agent, which learns to predict an important object
mask using visual semantics and spatial features.
  We implement such an interplay between both agents via an alternating
cross-pathway supervision scheme inside our proposed Visual-Spatial Network
(VSN). Our VSN consists of spatial ("where") and visual ("what") pathways, one
of which learns common visual semantics while the other focuses on the spatial
location cues. Our unsupervised learning is accomplished via a cross-pathway
supervision, where one pathway feeds its predictions to a segmentation agent,
which proposes a candidate important object segmentation mask that is then used
by the other pathway as a supervisory signal. We show our method's success on
two different important object datasets, where our method achieves similar or
better results as the supervised methods.


