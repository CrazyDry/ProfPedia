Collective Political Opinion Formation in Nonlinear Social Interaction

  We have presented a numerical model of a collective opinion formationprocedure to explain political phenomena such as two-party and multi-partysystems in politics, political unrest, military coup d'etats and netizenrevolutions. Nonlinear interaction with binary and independent decision makingprocesses can yield various collective behaviors or collective politicalopinions. Statistical physics and nonlinear dynamics may provide useful toolsto study various socio-political dynamics.

Future Localization from an Egocentric Depth Image

  This paper presents a method for future localization: to predict a set ofplausible trajectories of ego-motion given a depth image. We predict pathsavoiding obstacles, between objects, even paths turning around a corner intospace behind objects. As a byproduct of the predicted trajectories ofego-motion, we discover in the image the empty space occluded by foregroundobjects. We use no image based features such as semantic labeling/segmentationor object detection/recognition for this algorithm. Inspired by proxemics, werepresent the space around a person using an EgoSpace map, akin to anillustrated tourist map, that measures a likelihood of occlusion at theegocentric coordinate system. A future trajectory of ego-motion is modeled by alinear combination of compact trajectory bases allowing us to constrain thepredicted trajectory. We learn the relationship between the EgoSpace map andtrajectory from the EgoMotion dataset providing in-situ measurements of thefuture trajectory. A cost function that takes into account partial occlusiondue to foreground objects is minimized to predict a trajectory. This costfunction generates a trajectory that passes through the occluded space, whichallows us to discover the empty space behind the foreground objects. Wequantitatively evaluate our method to show predictive validity and apply tovarious real world scenes including walking, shopping, and social interactions.

Polarization-Selective Control of Nonlinear Optomechanical Interactions  in Subwavelength Elliptical Waveguides

  Photonic devices exhibiting all-optically reconfigurable polarizationdependence with a large dynamic range would be highly attractive for activepolarization control. Here, we report that strongly polarization-selectivenonlinear optomechanical interactions emerge in subwavelength waveguides. Byusing full-vectorial finite element analysis, we find that at certain coreellipticities (aspect ratios) the forward simulated light scattering mediatedby a specific acoustic mode is eliminated for one polarization mode, whereasthat for the other polarization mode is rather enhanced. This intriguingphenomenon can be explained by the interplay between the electrostrictive forceand radiation pressure and turns out to be tailorable by choice of waveguidematerials.

Enhanced high-temperature performance of GaN light-emitting diodes grown  on silicon substrates

  We compare the temperature dependence of optical and electricalcharacteristics of commercially available GaN light-emitting diodes (LEDs)grown on silicon and sapphire substrates. Contrary to conventionalexpectations, LEDs grown on silicon substrates, commonly referred to asGaN-on-Si LEDs, show less efficiency droop at higher temperatures even withmore threading dislocations. Analysis of the junction temperature reveals thatGaN-on-Si LEDs have a cooler junction despite sharing identical epitaxialstructures and packaging compared to LEDs grown on sapphire substrates. We alsoobserve a decrease in ideality factor with increase in ambient temperature forGaN-on-Si LEDs, indicating an increase in ideal diode current with temperature.Analysis of the strain and temperature coefficient measurements suggests thatthere is an increase in hole transport efficiency within the active region forGaN-on-Si LEDs compared to the LEDs grown on sapphire, which accounts for theless temperature-dependent efficiency droop.

HUMBI 1.0: HUman Multiview Behavioral Imaging Dataset

  This paper presents a new dataset called HUMBI - a large corpus of highfidelity models of behavioral signals in 3D from a diverse population measuredby a massive multi-camera system. With our novel design of a portable imagingsystem (consists of 107 HD cameras), we collect human behaviors from 164subjects across gender, ethnicity, age, and physical condition at a publicvenue. Using the multiview image streams, we reconstruct high fidelity modelsof five elementary parts: gaze, face, hands, body, and cloth. As a byproduct,the 3D model provides geometrically consistent image annotation via 2Dprojection, e.g., body part segmentation. This dataset is a significantdeparture from the existing human datasets that suffers from subject diversity.We hope the HUMBI opens up a new opportunity for the development for behavioralimaging.

Precision improvement of MEMS gyros for indoor mobile robots with  horizontal motion inspired by methods of TRIZ

  In the paper, the problem of precision improvement for the MEMS gyrosensorson indoor robots with horizontal motion is solved by methods of TRIZ ("thetheory of inventive problem solving").

Social Behavior Prediction from First Person Videos

  This paper presents a method to predict the future movements (location andgaze direction) of basketball players as a whole from their first personvideos. The predicted behaviors reflect an individual physical space thataffords to take the next actions while conforming to social behaviors byengaging to joint attention. Our key innovation is to use the 3D reconstructionof multiple first person cameras to automatically annotate each other's thevisual semantics of social configurations.  We leverage two learning signals uniquely embedded in first person videos.Individually, a first person video records the visual semantics of a spatialand social layout around a person that allows associating with past similarsituations. Collectively, first person videos follow joint attention that canlink the individuals to a group. We learn the egocentric visual semantics ofgroup movements using a Siamese neural network to retrieve future trajectories.We consolidate the retrieved trajectories from all players by maximizing ameasure of social compatibility---the gaze alignment towards joint attentionpredicted by their social formation, where the dynamics of joint attention islearned by a long-term recurrent convolutional network. This allows us tocharacterize which social configuration is more plausible and predict futuregroup trajectories.

Customizing First Person Image Through Desired Actions

  This paper studies a problem of inverse visual path planning: creating avisual scene from a first person action. Our conjecture is that the spatialarrangement of a first person visual scene is deployed to afford an action, andtherefore, the action can be inversely used to synthesize a new scene such thatthe action is feasible. As a proof-of-concept, we focus on linking visualexperiences induced by walking.  A key innovation of this paper is a concept of ActionTunnel---a 3D virtualtunnel along the future trajectory encoding what the wearer will visuallyexperience as moving into the scene. This connects two distinctive first personimages through similar walking paths. Our method takes a first person imagewith a user defined future trajectory and outputs a new image that can affordthe future motion. The image is created by combining present and futureActionTunnels in 3D where the missing pixels in adjoining area are computed bya generative adversarial network. Our work can provide a travel acrossdifferent first person experiences in diverse real world scenes.

3D Semantic Trajectory Reconstruction from 3D Pixel Continuum

  This paper presents a method to reconstruct dense semantic trajectory streamof human interactions in 3D from synchronized multiple videos. The interactionsinherently introduce self-occlusion and illumination/appearance/shape changes,resulting in highly fragmented trajectory reconstruction with noisy and coarsesemantic labels. Our conjecture is that among many views, there exists a set ofviews that can confidently recognize the visual semantic label of a 3Dtrajectory. We introduce a new representation called 3D semantic map---aprobability distribution over the semantic labels per trajectory. We constructthe 3D semantic map by reasoning about visibility and 2D recognition confidencebased on view-pooling, i.e., finding the view that best represents thesemantics of the trajectory. Using the 3D semantic map, we precisely infer alltrajectory labels jointly by considering the affinity between long rangetrajectories via estimating their local rigid transformations. This inferencequantitatively outperforms the baseline approaches in terms of predictivevalidity, representation robustness, and affinity effectiveness. We demonstratethat our algorithm can robustly compute the semantic labels of a large scaletrajectory set involving real-world human interactions with object, scenes, andpeople.

MONET: Multiview Semi-supervised Keypoint via Epipolar Divergence

  This paper presents MONET---an end-to-end semi-supervised learning frameworkfor a pose detector using multiview image streams. What differentiates MONETfrom existing models is its capability of detecting general subjects includingnon-human species without a pre-trained model. A key challenge of such subjectslies in the limited availability of expert manual annotations, which oftenleads to a large bias in the detection model. We address this challenge byusing the epipolar constraint embedded in the unlabeled data in two ways.First, given a set of the labeled data, the keypoint trajectories can bereliably reconstructed in 3D using multiview optical flows, resulting inconsiderable data augmentation in space and time from nearly exhaustive views.Second, the detection across views must geometrically agree with each other. Weintroduce a new measure of geometric consistency in keypoint distributionscalled epipolar divergence---a generalized distance from the epipolar lines tothe corresponding keypoint distribution. Epipolar divergence characterizes whentwo view keypoint distributions produces zero reprojection error. We design atwin network that minimizes the epipolar divergence through stereorectification that can significantly alleviate computational complexity andsampling aliasing in training. We demonstrate that our framework can localizecustomized keypoints of diverse species, e.g., humans, dogs, and monkeys.

Multiview Supervision By Registration

  This paper presents a semi-supervised learning framework to train a keypointdetector using multiview image streams given the limited labeled data(typically $<$4\%). We leverage the complementary relationship betweenmultiview geometry and visual tracking to provide three types of supervisionarysignals to utilize the unlabeled data: (1) keypoint detection in one view canbe supervised by other views via the epipolar geometry; (2) a keypoint movessmoothly over time where its optical flow can be used to temporally superviseconsecutive image frames to each other; (3) visible keypoint in one view islikely to be visible in the adjacent view. We integrate these three signals ina differentiable fashion to design a new end-to-end neural network composed ofthree pathways. This design allows us to extensively use the unlabeled data totrain the keypoint detector. We show that our approach outperforms existingdetectors including DeepLabCut tailored to the keypoint detection of non-humanspecies such as monkeys, dogs, and mice.

ECO: Egocentric Cognitive Mapping

  We present a new method to localize a camera within a previously unseenenvironment perceived from an egocentric point of view. Although this is, ingeneral, an ill-posed problem, humans can effortlessly and efficientlydetermine their relative location and orientation and navigate into apreviously unseen environments, e.g., finding a specific item in a new grocerystore. To enable such a capability, we design a new egocentric representation,which we call ECO (Egocentric COgnitive map). ECO is biologically inspired, bythe cognitive map that allows human navigation, and it encodes the surroundingvisual semantics with respect to both distance and orientation. ECO possessesthree main properties: (1) reconfigurability: complex semantics and geometry iscaptured via the synthesis of atomic visual representations (e.g., imagepatch); (2) robustness: the visual semantics are registered in a geometricallyconsistent way (e.g., aligning with respect to the gravity vector,frontalizing, and rescaling to canonical depth), thus enabling us to learnmeaningful atomic representations; (3) adaptability: a domain adaptationframework is designed to generalize the learned representation without manualcalibration. As a proof-of-concept, we use ECO to localize a camera withinreal-world scenes---various grocery stores---and demonstrate performanceimprovements when compared to existing semantic localization approaches.

Multiview Cross-supervision for Semantic Segmentation

  This paper presents a semi-supervised learning framework for a customizedsemantic segmentation task using multiview image streams. A key challenge ofthe customized task lies in the limited accessibility of the labeled data dueto the requirement of prohibitive manual annotation effort. We hypothesize thatit is possible to leverage multiview image streams that are linked through theunderlying 3D geometry, which can provide an additional supervisionary signalto train a segmentation model. We formulate a new cross-supervision methodusing a shape belief transfer---the segmentation belief in one image is used topredict that of the other image through epipolar geometry analogous toshape-from-silhouette. The shape belief transfer provides the upper and lowerbounds of the segmentation for the unlabeled data where its gap approachesasymptotically to zero as the number of the labeled views increases. Weintegrate this theory to design a novel network that is agnostic to cameracalibration, network model, and semantic category and bypasses the intermediateprocess of suboptimal 3D reconstruction. We validate this network byrecognizing a customized semantic category per pixel from realworld visual dataincluding non-human species and a subject of interest in social videos whereattaining large-scale annotation data is infeasible.

BOAO Photometric Survey of Galactic Open Clusters. III. Czernik 24 and  Czernik 27

  We present BV CCD photometry for the open clusters Czernik 24 and Czernik 27.These clusters have never been studied before, and we provide, for the firsttime, the cluster parameters; reddening, distance, metallicity and age. Czernik24 is an old open cluster with age 1.8 +/- 0.2 Gyr, metallicity [Fe/H]=-0.41+/- 0.15 dex, distance modulus (m-M)_0 = 13.1 +/- 0.3 mag (d=4.1 +/- 0.5 kpc),and reddening E(B-V) = 0.54 +/- 0.12 mag. The parameters for Czernik 27 areestimated to be age = 0.63 +/- 0.07 Gyr, [Fe/H]= -0.02 +/- 0.10 dex, (m-M)_0 =13.8 +/- 0.2 mag (d=5.8 +/- 0.5 kpc), and E(B-V) = 0.15 +/- 0.05 mag. Themetallicity and distance values for Czernik 24 are consistent with the relationbetween the metallicity and the Galactocentric distance of other old openclusters. We find the metallicity gradient of 51 old open clusters includingCzernik 24 to be Delta [Fe/H]/Delta R_gc= -0.064 +/- 0.009 dex/kpc.

Exploiting Egocentric Object Prior for 3D Saliency Detection

  On a minute-to-minute basis people undergo numerous fluid interactions withobjects that barely register on a conscious level. Recent neuroscientificresearch demonstrates that humans have a fixed size prior for salient objects.This suggests that a salient object in 3D undergoes a consistent transformationsuch that people's visual system perceives it with an approximately fixed size.This finding indicates that there exists a consistent egocentric object priorthat can be characterized by shape, size, depth, and location in the firstperson view.  In this paper, we develop an EgoObject Representation, which encodes thesecharacteristics by incorporating shape, location, size and depth features froman egocentric RGBD image. We empirically show that this representation canaccurately characterize the egocentric object prior by testing it on anegocentric RGBD dataset for three tasks: the 3D saliency detection, futuresaliency prediction, and interaction classification. This representation isevaluated on our new Egocentric RGBD Saliency dataset that includes variousactivities such as cooking, dining, and shopping. By using our EgoObjectrepresentation, we outperform previously proposed models for saliency detection(relative 30% improvement for 3D saliency detection task) on our dataset.Additionally, we demonstrate that this representation allows us to predictfuture salient objects based on the gaze cue and classify people's interactionswith objects.

First Person Action-Object Detection with EgoNet

  Unlike traditional third-person cameras mounted on robots, a first-personcamera, captures a person's visual sensorimotor object interactions from upclose. In this paper, we study the tight interplay between our momentary visualattention and motor action with objects from a first-person camera. We proposea concept of action-objects---the objects that capture person's consciousvisual (watching a TV) or tactile (taking a cup) interactions. Action-objectsmay be task-dependent but since many tasks share common person-object spatialconfigurations, action-objects exhibit a characteristic 3D spatial distance andorientation with respect to the person.  We design a predictive model that detects action-objects using EgoNet, ajoint two-stream network that holistically integrates visual appearance (RGB)and 3D spatial layout (depth and height) cues to predict per-pixel likelihoodof action-objects. Our network also incorporates a first-person coordinateembedding, which is designed to learn a spatial distribution of theaction-objects in the first-person data. We demonstrate EgoNet's predictivepower, by showing that it consistently outperforms previous baselineapproaches. Furthermore, EgoNet also exhibits a strong generalization ability,i.e., it predicts semantically meaningful objects in novel first-persondatasets. Our method's ability to effectively detect action-objects could beused to improve robots' understanding of human-object interactions.

Optical Characterization of PtSi/Si by Spectroscopic Ellipsometry

  We report optical characterization of PtSi films for thermoelectric deviceapplications by nondestructive spectroscopic ellipsometry (SE). Pt monolayerand Pt-Si multilayer which consists of 3 pairs of Pt and Si layers weredeposited on p-doped-silicon substrates by sputtering method and then rapidannealing process was done to form PtSi films through intermixing of Pt and Siatoms at the interface. Pseudodielectric function data <{\epsilon}> =<{\epsilon}1> + i<{\epsilon}2> of the PtSi/Si samples were obtained from 1.12to 6.52 eV by using spectroscopic ellipsometry. Employing Tauc-Lorentz andDrude models, the dielectric function ({\epsilon}) of PtSi films weredetermined. We found that the composition ratio of Pt:Si is nearly 1:1 for PtSimonolayer and we observed transitions between occupied and unoccupied states inPt 5d states. We also observed formation of PtSi layers in Pt-Si multilayersample. The SE results were confirmed by the transmission electron microscopyand energy dispersive X-ray spectroscopy.

Am I a Baller? Basketball Performance Assessment from First-Person  Videos

  This paper presents a method to assess a basketball player's performance fromhis/her first-person video. A key challenge lies in the fact that theevaluation metric is highly subjective and specific to a particular evaluator.We leverage the first-person camera to address this challenge. Thespatiotemporal visual semantics provided by a first-person view allows us toreason about the camera wearer's actions while he/she is participating in anunscripted basketball game. Our method takes a player's first-person video andprovides a player's performance measure that is specific to an evaluator'spreference.  To achieve this goal, we first use a convolutional LSTM network to detectatomic basketball events from first-person videos. Our network's ability tozoom-in to the salient regions addresses the issue of a severe camera wearer'shead movement in first-person videos. The detected atomic events are thenpassed through the Gaussian mixtures to construct a highly non-linear visualspatiotemporal basketball assessment feature. Finally, we use this feature tolearn a basketball assessment model from pairs of labeled first-personbasketball videos, for which a basketball expert indicates, which of the twoplayers is better.  We demonstrate that despite not knowing the basketball evaluator's criterion,our model learns to accurately assess the players in real-world games.Furthermore, our model can also discover basketball events that contributepositively and negatively to a player's performance.

Unsupervised Learning of Important Objects from First-Person Videos

  A first-person camera, placed at a person's head, captures, which objects areimportant to the camera wearer. Most prior methods for this task learn todetect such important objects from the manually labeled first-person data in asupervised fashion. However, important objects are strongly related to thecamera wearer's internal state such as his intentions and attention, and thus,only the person wearing the camera can provide the importance labels. Such aconstraint makes the annotation process costly and limited in scalability.  In this work, we show that we can detect important objects in first-personimages without the supervision by the camera wearer or even third-personlabelers. We formulate an important detection problem as an interplay betweenthe 1) segmentation and 2) recognition agents. The segmentation agent firstproposes a possible important object segmentation mask for each image, and thenfeeds it to the recognition agent, which learns to predict an important objectmask using visual semantics and spatial features.  We implement such an interplay between both agents via an alternatingcross-pathway supervision scheme inside our proposed Visual-Spatial Network(VSN). Our VSN consists of spatial ("where") and visual ("what") pathways, oneof which learns common visual semantics while the other focuses on the spatiallocation cues. Our unsupervised learning is accomplished via a cross-pathwaysupervision, where one pathway feeds its predictions to a segmentation agent,which proposes a candidate important object segmentation mask that is then usedby the other pathway as a supervisory signal. We show our method's success ontwo different important object datasets, where our method achieves similar orbetter results as the supervised methods.

