Information-theoretically Secure Regenerating Codes for Distributed  Storage

  Regenerating codes are a class of codes for distributed storage networks thatprovide reliability and availability of data, and also perform efficient noderepair. Another important aspect of a distributed storage network is itssecurity. In this paper, we consider a threat model where an eavesdropper maygain access to the data stored in a subset of the storage nodes, and possiblyalso, to the data downloaded during repair of some nodes. We provide explicitconstructions of regenerating codes that achieve information-theoretic secrecycapacity in this setting.

Regenerating Codes for Errors and Erasures in Distributed Storage

  Regenerating codes are a class of codes proposed for providing reliability ofdata and efficient repair of failed nodes in distributed storage systems. Inthis paper, we address the fundamental problem of handling errors and erasuresduring the data-reconstruction and node-repair operations. We provide explicitregenerating codes that are resilient to errors and erasures, and show thatthese codes are optimal with respect to storage and bandwidth requirements. Asa special case, we also establish the capacity of a class of distributedstorage systems in the presence of malicious adversaries. While our codeconstructions are based on previously constructed Product-Matrix codes, we alsoprovide necessary and sufficient conditions for introducing resilience in anyregenerating code.

When is it Better to Compare than to Score?

  When eliciting judgements from humans for an unknown quantity, one often hasthe choice of making direct-scoring (cardinal) or comparative (ordinal)measurements. In this paper we study the relative merits of either choice,providing empirical and theoretical guidelines for the selection of ameasurement scheme. We provide empirical evidence based on experiments onAmazon Mechanical Turk that in a variety of tasks, (pairwise-comparative)ordinal measurements have lower per sample noise and are typically faster toelicit than cardinal ones. Ordinal measurements however typically provide lessinformation. We then consider the popular Thurstone and Bradley-Terry-Luce(BTL) models for ordinal measurements and characterize the minimax error ratesfor estimating the unknown quantity. We compare these minimax error rates tothose under cardinal measurement models and quantify for what noise levelsordinal measurements are better. Finally, we revisit the data collected fromour experiments and show that fitting these models confirms this prediction:for tasks where the noise in ordinal measurements is sufficiently low, theordinal approach results in smaller errors in the estimation.

Double or Nothing: Multiplicative Incentive Mechanisms for Crowdsourcing

  Crowdsourcing has gained immense popularity in machine learning applicationsfor obtaining large amounts of labeled data. Crowdsourcing is cheap and fast,but suffers from the problem of low-quality data. To address this fundamentalchallenge in crowdsourcing, we propose a simple payment mechanism toincentivize workers to answer only the questions that they are sure of and skipthe rest. We show that surprisingly, under a mild and natural "no-free-lunch"requirement, this mechanism is the one and only incentive-compatible paymentmechanism possible. We also show that among all possible incentive-compatiblemechanisms (that may or may not satisfy no-free-lunch), our mechanism makes thesmallest possible payment to spammers. We further extend our results to a moregeneral setting in which workers are required to provide a quantized confidencefor each question. Interestingly, this unique mechanism takes a"multiplicative" form. The simplicity of the mechanism is an added benefit. Inpreliminary experiments involving over 900 worker-task pairs, we observe asignificant drop in the error rates under this unique mechanism for the same orlower monetary expenditure.

On the Impossibility of Convex Inference in Human Computation

  Human computation or crowdsourcing involves joint inference of theground-truth-answers and the worker-abilities by optimizing an objectivefunction, for instance, by maximizing the data likelihood based on an assumedunderlying model. A variety of methods have been proposed in the literature toaddress this inference problem. As far as we know, none of the objectivefunctions in existing methods is convex. In machine learning and appliedstatistics, a convex function such as the objective function of support vectormachines (SVMs) is generally preferred, since it can leverage thehigh-performance algorithms and rigorous guarantees established in theextensive literature on convex optimization. One may thus wonder if thereexists a meaningful convex objective function for the inference problem inhuman computation. In this paper, we investigate this convexity issue for humancomputation. We take an axiomatic approach by formulating a set of axioms thatimpose two mild and natural assumptions on the objective function for theinference. Under these axioms, we show that it is unfortunately impossible toensure convexity of the inference problem. On the other hand, we show thatinterestingly, in the absence of a requirement to model "spammers", one canconstruct reasonable objective functions for crowdsourcing that guaranteeconvex inference.

Approval Voting and Incentives in Crowdsourcing

  The growing need for labeled training data has made crowdsourcing animportant part of machine learning. The quality of crowdsourced labels is,however, adversely affected by three factors: (1) the workers are not experts;(2) the incentives of the workers are not aligned with those of the requesters;and (3) the interface does not allow workers to convey their knowledgeaccurately, by forcing them to make a single choice among a set of options. Inthis paper, we address these issues by introducing approval voting to utilizethe expertise of workers who have partial knowledge of the true answer, andcoupling it with a ("strictly proper") incentive-compatible compensationmechanism. We show rigorous theoretical guarantees of optimality of ourmechanism together with a simple axiomatic characterization. We also conductpreliminary empirical studies on Amazon Mechanical Turk which validate ourapproach.

Estimation from Pairwise Comparisons: Sharp Minimax Bounds with Topology  Dependence

  Data in the form of pairwise comparisons arises in many domains, includingpreference elicitation, sporting competitions, and peer grading among others.We consider parametric ordinal models for such pairwise comparison datainvolving a latent vector $w^* \in \mathbb{R}^d$ that represents the"qualities" of the $d$ items being compared; this class of models includes thetwo most widely used parametric models--the Bradley-Terry-Luce (BTL) and theThurstone models. Working within a standard minimax framework, we provide tightupper and lower bounds on the optimal error in estimating the quality scorevector $w^*$ under this class of models. The bounds depend on the topology ofthe comparison graph induced by the subset of pairs being compared via itsLaplacian spectrum. Thus, in settings where the subset of pairs may be chosen,our results provide principled guidelines for making this choice. Finally, wecompare these error rates to those under cardinal measurement models and showthat the error rates in the ordinal and cardinal settings have identicalscalings apart from constant pre-factors.

A Permutation-based Model for Crowd Labeling: Optimal Estimation and  Robustness

  The aggregation and denoising of crowd labeled data is a task that has gainedincreased significance with the advent of crowdsourcing platforms and massivedatasets. In this paper, we propose a permutation-based model for crowd labeleddata that is a significant generalization of the common Dawid-Skene model, andintroduce a new error metric by which to compare different estimators. Workingin a high-dimensional non-asymptotic framework that allows both the number ofworkers and tasks to scale, we derive optimal rates of convergence for thepermutation-based model. We show that the permutation-based model offerssignificant robustness in estimation due to its richness, while surprisinglyincurring only a small additional statistical penalty as compared to theDawid-Skene model. Finally, we propose a computationally-efficient method,called the OBI-WAN estimator, that is uniformly optimal over a classintermediate between the permutation-based and the Dawid-Skene models, and isuniformly consistent over the entire permutation-based model class. Incontrast, the guarantees for estimators available in prior literature aresub-optimal over the original Dawid-Skene model.

Design and Analysis of the NIPS 2016 Review Process

  Neural Information Processing Systems (NIPS) is a top-tier annual conferencein machine learning. The 2016 edition of the conference comprised more than2,400 paper submissions, 3,000 reviewers, and 8,000 attendees. This representsa growth of nearly 40% in terms of submissions, 96% in terms of reviewers, andover 100% in terms of attendees as compared to the previous year. The massivescale as well as rapid growth of the conference calls for a thorough qualityassessment of the peer-review process and novel means of improvement. In thispaper, we analyze several aspects of the data collected during the reviewprocess, including an experiment investigating the efficacy of collectingordinal rankings from reviewers. Our goal is to check the soundness of thereview process, and provide insights that may be useful in the design of thereview process of subsequent conferences.

Low Permutation-rank Matrices: Structural Properties and Noisy  Completion

  We consider the problem of noisy matrix completion, in which the goal is toreconstruct a structured matrix whose entries are partially observed in noise.Standard approaches to this underdetermined inverse problem are based onassuming that the underlying matrix has low rank, or is well-approximated by alow rank matrix. In this paper, we propose a richer model based on what we termthe "permutation-rank" of a matrix. We first describe how the classicalnon-negative rank model enforces restrictions that may be undesirable inpractice, and how and these restrictions can be avoided by using the richerpermutation-rank model. Second, we establish the minimax rates of estimationunder the new permutation-based model, and prove that surprisingly, the minimaxrates are equivalent up to logarithmic factors to those for estimation underthe typical low rank model. Third, we analyze a computationally efficientsingular-value-thresholding algorithm, known to be optimal for the low-ranksetting, and show that it also simultaneously yields a consistent estimator forthe low-permutation rank setting. Finally, we present various structuralresults characterizing the uniqueness of the permutation-rank decomposition,and characterizing convex approximations of the permutation-rank polytope.

Explicit Construction of Optimal Exact Regenerating Codes for  Distributed Storage

  Erasure coding techniques are used to increase the reliability of distributedstorage systems while minimizing storage overhead. Also of interest isminimization of the bandwidth required to repair the system following a nodefailure. In a recent paper, Wu et al. characterize the tradeoff between therepair bandwidth and the amount of data stored per node. They also prove theexistence of regenerating codes that achieve this tradeoff.  In this paper, we introduce Exact Regenerating Codes, which are regeneratingcodes possessing the additional property of being able to duplicate the datastored at a failed node. Such codes require low processing and communicationoverheads, making the system practical and easy to maintain. Explicitconstruction of exact regenerating codes is provided for the minimum bandwidthpoint on the storage-repair bandwidth tradeoff, relevant todistributed-mail-server applications. A subspace based approach is provided andshown to yield necessary and sufficient conditions on a linear code to possessthe exact regeneration property as well as prove the uniqueness of ourconstruction.  Also included in the paper, is an explicit construction of regenerating codesfor the minimum storage point for parameters relevant to storage inpeer-to-peer systems. This construction supports a variable number of nodes andcan handle multiple, simultaneous node failures. All constructions given in thepaper are of low complexity, requiring low field size in particular.

Simple, Robust and Optimal Ranking from Pairwise Comparisons

  We consider data in the form of pairwise comparisons of n items, with thegoal of precisely identifying the top k items for some value of k < n, oralternatively, recovering a ranking of all the items. We analyze the Copelandcounting algorithm that ranks the items in order of the number of pairwisecomparisons won, and show it has three attractive features: (a) itscomputational efficiency leads to speed-ups of several orders of magnitude incomputation time as compared to prior work; (b) it is robust in thattheoretical guarantees impose no conditions on the underlying matrix ofpairwise-comparison probabilities, in contrast to some prior work that appliesonly to the BTL parametric model; and (c) it is an optimal method up toconstant factors, meaning that it achieves the information-theoretic limits forrecovering the top k-subset. We extend our results to obtain sharp guaranteesfor approximate recovery under the Hamming distortion metric, and moregenerally, to any arbitrary error requirement that satisfies a simple andnatural monotonicity condition.

Enabling Node Repair in Any Erasure Code for Distributed Storage

  Erasure codes are an efficient means of storing data across a network incomparison to data replication, as they tend to reduce the amount of datastored in the network and offer increased resilience in the presence of nodefailures. The codes perform poorly though, when repair of a failed node iscalled for, as they typically require the entire file to be downloaded torepair a failed node. A new class of erasure codes, termed as regeneratingcodes were recently introduced, that do much better in this respect. However,given the variety of efficient erasure codes available in the literature, thereis considerable interest in the construction of coding schemes that wouldenable traditional erasure codes to be used, while retaining the feature thatonly a fraction of the data need be downloaded for node repair. In this paper,we present a simple, yet powerful, framework that does precisely this. Underthis framework, the nodes are partitioned into two 'types' and encoded usingtwo codes in a manner that reduces the problem of node-repair to that oferasure-decoding of the constituent codes. Depending upon the choice of the twocodes, the framework can be used to avail one or more of the followingadvantages: simultaneous minimization of storage space and repair-bandwidth,low complexity of operation, fewer disk reads at helper nodes during repair,and error detection and correction.

Distributed Secret Dissemination Across a Network

  Shamir's (n, k) threshold secret sharing is an important component of severalcryptographic protocols, such as those for secure multiparty-computation andkey management. These protocols typically assume the presence of directcommunication links from the dealer to all participants, in which case thedealer can directly pass the shares of the secret to each participant. In thispaper, we consider the problem of secret sharing when the dealer does not havedirect communication links to all the participants, and instead, the dealer andthe participants form a general network. Existing methods are based on securemessage transmissions from the dealer to each participant requiringconsiderable coordination in the network. In this paper, we present adistributed algorithm for disseminating shares over a network, which we callthe SNEAK algorithm, requiring each node to know only the identities of itsone-hop neighbours. While SNEAK imposes a stronger condition on the network byrequiring the dealer to be what we call k-propagating rather than k-connectedas required by the existing solutions, we show that in addition to beingdistributed, SNEAK achieves significant reduction in the communication cost andthe amount of randomness required.

The MDS Queue: Analysing the Latency Performance of Erasure Codes

  In order to scale economically, data centers are increasingly evolving theirdata storage methods from the use of simple data replication to the use of morepowerful erasure codes, which provide the same level of reliability asreplication but at a significantly lower storage cost. In particular, it iswell known that Maximum-Distance-Separable (MDS) codes, such as Reed-Solomoncodes, provide the maximum storage efficiency. While the use of codes forproviding improved reliability in archival storage systems, where the data isless frequently accessed (or so-called "cold data"), is well understood, therole of codes in the storage of more frequently accessed and active "hot data",where latency is the key metric, is less clear.  In this paper, we study data storage systems based on MDS codes through thelens of queueing theory, and term this the "MDS queue." We analyticallycharacterize the (average) latency performance of MDS queues, for which wepresent insightful scheduling policies that form upper and lower bounds toperformance, and are observed to be quite tight. Extensive simulations are alsoprovided and used to validate our theoretical analysis. We also employ theframework of the MDS queue to analyse different methods of performing so-calleddegraded reads (reading of partial data) in distributed data storage.

On Minimizing Data-read and Download for Storage-Node Recovery

  We consider the problem of efficient recovery of the data stored in anyindividual node of a distributed storage system, from the rest of the nodes.Applications include handling failures and degraded reads. We measureefficiency in terms of the amount of data-read and the download required. Tominimize the download, we focus on the minimum bandwidth setting of the'regenerating codes' model for distributed storage. Under this model, thesystem has a total of n nodes, and the data stored in any node must be(efficiently) recoverable from any d of the other (n-1) nodes. Lower bounds onthe two metrics under this model were derived previously; it has also beenshown that these bounds are achievable for the amount of data-read and downloadwhen d=n-1, and for the amount of download alone when d<n-1.  In this paper, we complete this picture by proving the converse result, thatwhen d<n-1, these lower bounds are strictly loose with respect to the amount ofread required. The proof is information-theoretic, and hence applies tonon-linear codes as well. We also show that under two (practical) relaxationsof the problem setting, these lower bounds can be met for both read anddownload simultaneously.

Fundamental Limits on Communication for Oblivious Updates in Storage  Networks

  In distributed storage systems, storage nodes intermittently go offline fornumerous reasons. On coming back online, nodes need to update their contents toreflect any modifications to the data in the interim. In this paper, weconsider a setting where no information regarding modified data needs to belogged in the system. In such a setting, a 'stale' node needs to update itscontents by downloading data from already updated nodes, while neither thestale node nor the updated nodes have any knowledge as to which data symbolsare modified and what their value is. We investigate the fundamental limits onthe amount of communication necessary for such an "oblivious" update process.  We first present a generic lower bound on the amount of communication that isnecessary under any storage code with a linear encoding (while allowingnon-linear update protocols). This lower bound is derived under a set ofextremely weak conditions, giving all updated nodes access to the entiremodified data and the stale node access to the entire stale data as sideinformation. We then present codes and update algorithms that are optimal inthat they meet this lower bound. Next, we present a lower bound for animportant subclass of codes, that of linear Maximum-Distance-Separable (MDS)codes. We then present an MDS code construction and an associated updatealgorithm that meets this lower bound. These results thus establish thecapacity of oblivious updates in terms of the communication requirements underthese settings.

Stochastically Transitive Models for Pairwise Comparisons: Statistical  and Computational Issues

  There are various parametric models for analyzing pairwise comparison data,including the Bradley-Terry-Luce (BTL) and Thurstone models, but their relianceon strong parametric assumptions is limiting. In this work, we study a flexiblemodel for pairwise comparisons, under which the probabilities of outcomes arerequired only to satisfy a natural form of stochastic transitivity. This classincludes parametric models including the BTL and Thurstone models as specialcases, but is considerably more general. We provide various examples of modelsin this broader stochastically transitive class for which classical parametricmodels provide poor fits. Despite this greater flexibility, we show that thematrix of probabilities can be estimated at the same rate as in standardparametric models. On the other hand, unlike in the BTL and Thurstone models,computing the minimax-optimal estimator in the stochastically transitive modelis non-trivial, and we explore various computationally tractable alternatives.We show that a simple singular value thresholding algorithm is statisticallyconsistent but does not achieve the minimax rate. We then propose and studyalgorithms that achieve the minimax rate over interesting sub-classes of thefull stochastically transitive class. We complement our theoretical resultswith thorough numerical simulations.

Feeling the Bern: Adaptive Estimators for Bernoulli Probabilities of  Pairwise Comparisons

  We study methods for aggregating pairwise comparison data in order toestimate outcome probabilities for future comparisons among a collection of nitems. Working within a flexible framework that imposes only a form of strongstochastic transitivity (SST), we introduce an adaptivity index defined by theindifference sets of the pairwise comparison probabilities. In addition tomeasuring the usual worst-case risk of an estimator, this adaptivity index alsocaptures the extent to which the estimator adapts to instance-specificdifficulty relative to an oracle estimator. We prove three main results thatinvolve this adaptivity index and different algorithms. First, we propose athree-step estimator termed Count-Randomize-Least squares (CRL), and show thatit has adaptivity index upper bounded as $\sqrt{n}$ up to logarithmic factors.We then show that that conditional on the hardness of planted clique, nocomputationally efficient estimator can achieve an adaptivity index smallerthan $\sqrt{n}$. Second, we show that a regularized least squares estimator canachieve a poly-logarithmic adaptivity index, thereby demonstrating a$\sqrt{n}$-gap between optimal and computationally achievable adaptivity.Finally, we prove that the standard least squares estimator, which is known tobe optimally adaptive in several closely related problems, fails to adapt inthe context of estimating pairwise probabilities.

Explicit Codes Minimizing Repair Bandwidth for Distributed Storage

  We consider the setting of data storage across n nodes in a distributedmanner. A data collector (DC) should be able to reconstruct the entire data byconnecting to any k out of the n nodes and downloading all the data stored inthem. When a node fails, it has to be regenerated back using the existingnodes. In a recent paper, Wu et al. have obtained an information theoreticlower bound for the repair bandwidth. Also, there has been additional interestin storing data in systematic form as no post processing is required when DCconnects to k systematic nodes. Because of their preferred status there is aneed to regenerate back any systematic node quickly and exactly. Replacement ofa failed node by an exact replica is termed Exact Regeneration.In this paper,we consider the problem of minimizing the repair bandwidth for exactregeneration of the systematic nodes. The file to be stored is of size B andeach node can store alpha = B/k units of data. A failed systematic node isregenerated by downloading beta units of data each from d existing nodes. Wegive a lower bound for the repair bandwidth for exact regeneration of thesystematic nodes which matches with the bound given by Wu et al. For d >= 2k-1we give an explicit code construction which minimizes the repair bandwidth whenthe existing k-1 systematic nodes participate in the regeneration. We show theexistence and construction of codes that achieve the bound for d >= 2k-3. Herewe also establish the necessity of interference alignment. We prove that thebound is not achievable for d <= 2k-4 when beta=1. We also give a coding schemewhich can be used for any d and k, which is optimal for d >= 2k-1.

Distributed Storage Codes with Repair-by-Transfer and Non-achievability  of Interior Points on the Storage-Bandwidth Tradeoff

  Regenerating codes are a class of recently developed codes for distributedstorage that, like Reed-Solomon codes, permit data recovery from any subset ofk nodes within the n-node network. However, regenerating codes possess inaddition, the ability to repair a failed node by connecting to an arbitrarysubset of d nodes. It has been shown that for the case of functional-repair,there is a tradeoff between the amount of data stored per node and thebandwidth required to repair a failed node. A special case of functional-repairis exact-repair where the replacement node is required to store data identicalto that in the failed node. Exact-repair is of interest as it greatlysimplifies system implementation. The first result of the paper is an explicit,exact-repair code for the point on the storage-bandwidth tradeoff correspondingto the minimum possible repair bandwidth, for the case when d=n-1. This codehas a particularly simple graphical description and most interestingly, has theability to carry out exact-repair through mere transfer of data and without anyneed to perform arithmetic operations. Hence the term `repair-by-transfer'. Thesecond result of this paper shows that the interior points on thestorage-bandwidth tradeoff cannot be achieved under exact-repair, thus pointingto the existence of a separate tradeoff under exact-repair. Specifically, weidentify a set of scenarios, termed `helper node pooling', and show that it isthe necessity to satisfy such scenarios that over-constrains the system.

Optimal Exact-Regenerating Codes for Distributed Storage at the MSR and  MBR Points via a Product-Matrix Construction

  Regenerating codes are a class of distributed storage codes that optimallytrade the bandwidth needed for repair of a failed node with the amount of datastored per node of the network. Minimum Storage Regenerating (MSR) codesminimize first, the amount of data stored per node, and then the repairbandwidth, while Minimum Bandwidth Regenerating (MBR) codes carry out theminimization in the reverse order. An [n, k, d] regenerating code permits thedata to be recovered by connecting to any k of the n nodes in the network,while requiring that repair of a failed node be made possible by connecting(using links of lesser capacity) to any d nodes. Previous, explicit and generalconstructions of exact-regenerating codes have been confined to the case n=d+1.In this paper, we present optimal, explicit constructions of MBR codes for allfeasible values of [n, k, d] and MSR codes for all [n, k, d >= 2k-2], using aproduct-matrix framework. The particular product-matrix nature of theconstructions is shown to significantly simplify system operation. To the bestof our knowledge, these are the first constructions of exact-regenerating codesthat allow the number n of nodes in the distributed storage network, to bechosen independent of the other parameters. The paper also contains a simplerdescription, in the product-matrix framework, of a previously constructed MSRcode in which the parameter d satisfies [n=d+1, k, d >= 2k-1].

When Do Redundant Requests Reduce Latency ?

  Several systems possess the flexibility to serve requests in more than oneway. For instance, a distributed storage system storing multiple replicas ofthe data can serve a request from any of the multiple servers that store therequested data, or a computational task may be performed in a compute-clusterby any one of multiple processors. In such systems, the latency of serving therequests may potentially be reduced by sending "redundant requests": a requestmay be sent to more servers than needed, and it is deemed served when therequisite number of servers complete service. Such a mechanism trades off thepossibility of faster execution of at least one copy of the request with theincrease in the delay due to an increased load on the system. Due to thistradeoff, it is unclear when redundant requests may actually help. Severalrecent works empirically evaluate the latency performance of redundant requestsin diverse settings.  This work aims at an analytical study of the latency performance of redundantrequests, with the primary goals of characterizing under what scenarios sendingredundant requests will help (and under what scenarios they will not help), aswell as designing optimal redundant-requesting policies. We first present amodel that captures the key features of such systems. We show that when servicetimes are i.i.d. memoryless or "heavier", and when the additional copies ofalready-completed jobs can be removed instantly, redundant requests reduce theaverage latency. On the other hand, when service times are "lighter" or whenservice times are memoryless and removal of jobs is not instantaneous, then nothaving any redundancy in the requests is optimal under high loads. Our resultshold for arbitrary arrival processes.

A Piggybacking Design Framework for Read-and Download-efficient  Distributed Storage Codes

  We present a new 'piggybacking' framework for designing distributed storagecodes that are efficient in data-read and download required during node-repair.We illustrate the power of this framework by constructing classes of explicitcodes that entail the smallest data-read and download for repair among allexisting solutions for three important settings: (a) codes meeting theconstraints of being Maximum-Distance-Separable (MDS), high-rate and having asmall number of substripes, arising out of practical considerations forimplementation in data centers, (b) binary MDS codes for all parameters wherebinary MDS codes exist, (c) MDS codes with the smallest repair-locality. Inaddition, we employ this framework to enable efficient repair of parity nodesin existing codes that were originally constructed to address the repair ofonly the systematic nodes. The basic idea behind our framework is to takemultiple instances of existing codes and add carefully designed functions ofthe data of one instance to the other. Typical savings in data-read duringrepair is 25% to 50% depending on the choice of the code parameters.

Regularized Minimax Conditional Entropy for Crowdsourcing

  There is a rapidly increasing interest in crowdsourcing for data labeling. Bycrowdsourcing, a large number of labels can be often quickly gathered at lowcost. However, the labels provided by the crowdsourcing workers are usually notof high quality. In this paper, we propose a minimax conditional entropyprinciple to infer ground truth from noisy crowdsourced labels. Under thisprinciple, we derive a unique probabilistic labeling model jointlyparameterized by worker ability and item difficulty. We also propose anobjective measurement principle, and show that our method is the only methodwhich satisfies this objective measurement principle. We validate our methodthrough a variety of real crowdsourcing datasets with binary, multiclass orordinal labels.

Information-theoretically Secure Erasure Codes for Distributed Storage

  Repair operations in distributed storage systems potentially expose the datato malicious acts of passive eavesdroppers or active adversaries, which can bedetrimental to the security of the system. This paper presents erasure codesand repair algorithms that ensure security of the data in the presence ofpassive eavesdroppers and active adversaries, while maintaining highavailability, reliability and efficiency in the system. Our codes are optimalin that they meet previously proposed lower bounds on the storage,network-bandwidth, and reliability requirements for a wide range of systemparameters. Our results thus establish the capacity of such systems. Our codesfor security from active adversaries provide an additional appealing feature of`on-demand security' where the desired level of security can be chosenseparately for each instance of repair, and our algorithms remain optimalsimultaneously for all possible levels. The paper also provides necessary andsufficient conditions governing the transformation of any (non-secure) codeinto one providing on-demand security.

Parametric Prediction from Parametric Agents

  We consider a problem of prediction based on opinions elicited fromheterogeneous rational agents with private information. Making an accurateprediction with a minimal cost requires a joint design of the incentivemechanism and the prediction algorithm. Such a problem lies at the nexus ofstatistical learning theory and game theory, and arises in many domains such asconsumer surveys and mobile crowdsourcing. In order to elicit heterogeneousagents' private information and incentivize agents with different capabilitiesto act in the principal's best interest, we design an optimal joint incentivemechanism and prediction algorithm called COPE (COst and PredictionElicitation), the analysis of which offers several valuable engineeringinsights. First, when the costs incurred by the agents are linear in theexerted effort, COPE corresponds to a "crowd contending" mechanism, where theprincipal only employs the agent with the highest capability. Second, when thecosts are quadratic, COPE corresponds to a "crowd-sourcing" mechanism thatemploys multiple agents with different capabilities at the same time. Numericalsimulations show that COPE improves the principal's profit and the networkprofit significantly (larger than 30% in our simulations), comparing to thosemechanisms that assume all agents have equal capabilities.

PeerReview4All: Fair and Accurate Reviewer Assignment in Peer Review

  We consider the problem of automated assignment of papers to reviewers inconference peer review, with a focus on fairness and statistical accuracy. Ourfairness objective is to maximize the review quality of the most disadvantagedpaper, in contrast to the commonly used objective of maximizing the totalquality over all papers. We design an assignment algorithm based on anincremental max-flow procedure that we prove is near-optimally fair. Ourstatistical accuracy objective is to ensure correct recovery of the papers thatshould be accepted. We provide a sharp minimax analysis of the accuracy of thepeer-review process for a popular objective-score model as well as for a novelsubjective-score model that we propose in the paper. Our analysis proves thatour proposed assignment algorithm also leads to a near-optimal statisticalaccuracy. Finally, we design a novel experiment that allows for an objectivecomparison of various assignment algorithms, and overcomes the inherentdifficulty posed by the absence of a ground truth in experiments onpeer-review. The results of this experiment corroborate the theoreticalguarantees of our algorithm.

Choosing How to Choose Papers

  It is common to see a handful of reviewers reject a highly novel paper,because they view, say, extensive experiments as far more important thannovelty, whereas the community as a whole would have embraced the paper. Moregenerally, the disparate mapping of criteria scores to final recommendations bydifferent reviewers is a major source of inconsistency in peer review. In thispaper we present a framework --- based on $L(p,q)$-norm empirical riskminimization --- for learning the community's aggregate mapping. We draw oncomputational social choice to identify desirable values of $p$ and $q$;specifically, we characterize $p=q=1$ as the only choice that satisfies threenatural axiomatic properties. Finally, we implement and apply our approach toreviews from IJCAI 2017.

An Incentive Mechanism for Crowd Sensing with Colluding Agents

  Vehicular mobile crowd sensing is a fast-emerging paradigm to collect dataabout the environment by mounting sensors on vehicles such as taxis. Animportant problem in vehicular crowd sensing is to design payment mechanisms toincentivize drivers (agents) to collect data, with the overall goal ofobtaining the maximum amount of data (across multiple vehicles) for a givenbudget. Past works on this problem consider a setting where each agent operatesin isolation---an assumption which is frequently violated in practice. In thispaper, we design an incentive mechanism to incentivize agents who can engage inarbitrary collusions. We then show that in a "homogeneous" setting, ourmechanism is optimal, and can do as well as any mechanism which knows theagents' preferences a priori. Moreover, if the agents are non-colluding, thenour mechanism automatically does as well as any other non-colluding mechanism.We also show that our proposed mechanism has strong (and asymptoticallyoptimal) guarantees for a more general "heterogeneous" setting. Experimentsbased on synthesized data and real-world data reveal gains of over 30\%attained by our mechanism compared to past literature.

Your 2 is My 1, Your 3 is My 9: Handling Arbitrary Miscalibrations in  Ratings

  Cardinal scores (numeric ratings) collected from people are well known tosuffer from miscalibrations. A popular approach to address this issue is toassume simplistic models of miscalibration (such as linear biases) to de-biasthe scores. This approach, however, often fares poorly because people'smiscalibrations are typically far more complex and not well understood. In theabsence of simplifying assumptions on the miscalibration, it is widely believedby the crowdsourcing community that the only useful information in the cardinalscores is the induced ranking. In this paper, inspired by the framework ofStein's shrinkage, empirical Bayes, and the classic two-envelope problem, wecontest this widespread belief. Specifically, we consider cardinal scores witharbitrary (or even adversarially chosen) miscalibrations which are onlyrequired to be consistent with the induced ranking. We design estimators whichdespite making no assumptions on the miscalibration, strictly and uniformlyoutperform all possible estimators that rely on only the ranking. Ourestimators are flexible in that they can be used as a plug-in for a variety ofapplications, and we provide a proof-of-concept for A/B testing and ranking.Our results thus provide novel insights in the eternal debate between cardinaland ordinal data.

A Solution to the Network Challenges of Data Recovery in Erasure-coded  Distributed Storage Systems: A Study on the Facebook Warehouse Cluster

  Erasure codes, such as Reed-Solomon (RS) codes, are being increasinglyemployed in data centers to combat the cost of reliably storing large amountsof data. Although these codes provide optimal storage efficiency, they requiresignificantly high network and disk usage during recovery of missing data. Inthis paper, we first present a study on the impact of recovery operations oferasure-coded data on the data-center network, based on measurements fromFacebook's warehouse cluster in production. To the best of our knowledge, thisis the first study of its kind available in the literature. Our study revealsthat recovery of RS-coded data results in a significant increase in networktraffic, more than a hundred terabytes per day, in a cluster storing multiplepetabytes of RS-coded data.  To address this issue, we present a new storage code using our recentlyproposed "Piggybacking" framework, that reduces the network and disk usageduring recovery by 30% in theory, while also being storage optimal andsupporting arbitrary design parameters. The implementation of the proposed codein the Hadoop Distributed File System (HDFS) is underway. We use themeasurements from the warehouse cluster to show that the proposed code wouldlead to a reduction of close to fifty terabytes of cross-rack traffic per day.

On Strategyproof Conference Peer Review

  We consider peer review in a conference setting where there is typically anoverlap between the set of reviewers and the set of authors. This overlap canincentivize strategic reviews to influence the final ranking of one's ownpapers. In this work, we address this problem through the lens of socialchoice, and present a theoretical framework for strategyproof and efficientpeer review. We first present and analyze an algorithm for reviewer-assignmentand aggregation that guarantees strategyproofness and a natural efficiencyproperty called unanimity, when the authorship graph satisfies a simpleproperty. Our algorithm is based on the so-called partitioning method, and canbe thought as a generalization of this method to conference peer reviewsettings. We then empirically show that the requisite property on theauthorship graph is indeed satisfied in the ICLR-17 submission data, andfurther demonstrate a simple trick to make the partitioning method morepractically appealing for conference peer review. Finally, we complement ourpositive results with negative theoretical results where we prove that undervarious ways of strengthening the requirements, it is impossible for anyalgorithm to be strategyproof and efficient.

Interference Alignment in Regenerating Codes for Distributed Storage:  Necessity and Code Constructions

  Regenerating codes are a class of recently developed codes for distributedstorage that, like Reed-Solomon codes, permit data recovery from any arbitraryk of n nodes. However regenerating codes possess in addition, the ability torepair a failed node by connecting to any arbitrary d nodes and downloading anamount of data that is typically far less than the size of the data file. Thisamount of download is termed the repair bandwidth. Minimum storage regenerating(MSR) codes are a subclass of regenerating codes that require the least amountof network storage; every such code is a maximum distance separable (MDS) code.Further, when a replacement node stores data identical to that in the failednode, the repair is termed as exact.  The four principal results of the paper are (a) the explicit construction ofa class of MDS codes for d = n-1 >= 2k-1 termed the MISER code, that achievesthe cut-set bound on the repair bandwidth for the exact-repair of systematicnodes, (b) proof of the necessity of interference alignment in exact-repair MSRcodes, (c) a proof showing the impossibility of constructing linear,exact-repair MSR codes for d < 2k-3 in the absence of symbol extension, and (d)the construction, also explicit, of MSR codes for d = k+1. Interferencealignment (IA) is a theme that runs throughout the paper: the MISER code isbuilt on the principles of IA and IA is also a crucial component to thenon-existence proof for d < 2k-3. To the best of our knowledge, theconstructions presented in this paper are the first, explicit constructions ofregenerating codes that achieve the cut-set bound.

Active Ranking from Pairwise Comparisons and when Parametric Assumptions  Don't Help

  We consider sequential or active ranking of a set of n items based on noisypairwise comparisons. Items are ranked according to the probability that agiven item beats a randomly chosen item, and ranking refers to partitioning theitems into sets of pre-specified sizes according to their scores. This notionof ranking includes as special cases the identification of the top-k items andthe total ordering of the items. We first analyze a sequential rankingalgorithm that counts the number of comparisons won, and uses these counts todecide whether to stop, or to compare another pair of items, chosen based onconfidence intervals specified by the data collected up to that point. We provethat this algorithm succeeds in recovering the ranking using a number ofcomparisons that is optimal up to logarithmic factors. This guarantee does notrequire any structural properties of the underlying pairwise probabilitymatrix, unlike a significant body of past work on pairwise ranking based onparametric models such as the Thurstone or Bradley-Terry-Luce models. It hasbeen a long-standing open question as to whether or not imposing theseparametric assumptions allows for improved ranking algorithms. For stochasticcomparison models, in which the pairwise probabilities are bounded away fromzero, our second contribution is to resolve this issue by proving a lower boundfor parametric models. This shows, perhaps surprisingly, that these popularparametric modeling choices offer at most logarithmic gains for stochasticcomparisons.

