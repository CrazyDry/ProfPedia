Algorithm Portfolio Design: Theory vs. Practice

  Stochastic algorithms are among the best for solving computationally hardsearch and reasoning problems. The runtime of such procedures is characterizedby a random variable. Different algorithms give rise to different probabilitydistributions. One can take advantage of such differences by combining severalalgorithms into a portfolio, and running them in parallel or interleaving themon a single processor. We provide a detailed evaluation of the portfolioapproach on distributions of hard combinatorial search problems. We show underwhat conditions the protfolio approach can have a dramatic computationaladvantage over the best traditional methods.

Unstructured Human Activity Detection from RGBD Images

  Being able to detect and recognize human activities is essential for severalapplications, including personal assistive robotics. In this paper, we performdetection and recognition of unstructured human activity in unstructuredenvironments. We use a RGBD sensor (Microsoft Kinect) as the input sensor, andcompute a set of features based on human pose and motion, as well as based onimage and pointcloud information. Our algorithm is based on a hierarchicalmaximum entropy Markov model (MEMM), which considers a person's activity ascomposed of a set of sub-activities. We infer the two-layered graph structureusing a dynamic programming approach. We test our algorithm on detecting andrecognizing twelve different activities performed by four people in differentenvironments, such as a kitchen, a living room, an office, etc., and achievegood performance even when the person was not seen before in the training set.

Playing games against nature: optimal policies for renewable resource  allocation

  In this paper we introduce a class of Markov decision processes that arise asa natural model for many renewable resource allocation problems. Upon extendingresults from the inventory control literature, we prove that they admit aclosed form solution and we show how to exploit this structure to speed up itscomputation. We consider the application of the proposed framework to severalproblems arising in very different domains, and as part of the ongoing effortin the emerging field of Computational Sustainability we discuss in detail itsapplication to the Northern Pacific Halibut marine fishery. Our approach isapplied to a model based on real world data, obtaining a policy with aguaranteed lower bound on the utility function that is structurally verydifferent from the one currently employed.

Understanding Sampling Style Adversarial Search Methods

  UCT has recently emerged as an exciting new adversarial reasoning techniquebased on cleverly balancing exploration and exploitation in a Monte-Carlosampling setting. It has been particularly successful in the game of Go but thereasons for its success are not well understood and attempts to replicate itssuccess in other domains such as Chess have failed. We provide an in-depthanalysis of the potential of UCT in domain-independent settings, in cases whereheuristic values are available, and the effect of enhancing random playouts tomore informed playouts between two weak minimax players. To provide furtherinsights, we develop synthetic game tree instances and discuss interestingproperties of UCT, both empirically and analytically.

Survey Propagation Revisited

  Survey propagation (SP) is an exciting new technique that has been remarkablysuccessful at solving very large hard combinatorial problems, such asdetermining the satisfiability of Boolean formulas. In a promising attempt atunderstanding the success of SP, it was recently shown that SP can be viewed asa form of belief propagation, computing marginal probabilities over certainobjects called covers of a formula. This explanation was, however, shortlydismissed by experiments suggesting that non-trivial covers simply do not existfor large formulas. In this paper, we show that these experiments weremisleading: not only do covers exist for large hard random formulas, SP issurprisingly accurate at computing marginals over these covers despite theexistence of many cycles in the formulas. This re-opens a potentially simplerline of reasoning for understanding SP, in contrast to some alternative linesof explanation that have been proposed assuming covers do not exist.

Uniform Solution Sampling Using a Constraint Solver As an Oracle

  We consider the problem of sampling from solutions defined by a set of hardconstraints on a combinatorial space. We propose a new sampling technique that,while enforcing a uniform exploration of the search space, leverages thereasoning power of a systematic constraint solver in a black-box scheme. Wepresent a series of challenging domains, such as energy barriers and highlyasymmetric spaces, that reveal the difficulties introduced by hard constraints.We demonstrate that standard approaches such as Simulated Annealing and GibbsSampling are greatly affected, while our new technique can overcome many ofthese difficulties. Finally, we show that our sampling scheme naturally definesa new approximate model counting technique, which we empirically show to bevery accurate on a range of benchmark problems.

A Bayesian Approach to Tackling Hard Computational Problems

  We are developing a general framework for using learned Bayesian models fordecision-theoretic control of search and reasoningalgorithms. We illustrate theapproach on the specific task of controlling both general and domain-specificsolvers on a hard class of structured constraint satisfaction problems. Asuccessful strategyfor reducing the high (and even infinite) variance inrunning time typically exhibited by backtracking search algorithms is to cutoff and restart the search if a solution is not found within a certainamount oftime. Previous work on restart strategies have employed fixed cut off values.We show how to create a dynamic cut off strategy by learning a Bayesian modelthat predicts the ultimate length of a trial based on observing the earlybehavior of the search algorithm. Furthermore, we describe the generalconditions under which a dynamic restart strategy can outperform thetheoretically optimal fixed strategy.

Taming the Curse of Dimensionality: Discrete Integration by Hashing and  Optimization

  Integration is affected by the curse of dimensionality and quickly becomesintractable as the dimensionality of the problem grows. We propose a randomizedalgorithm that, with high probability, gives a constant-factor approximation ofa general discrete integral defined over an exponentially large set. Thisalgorithm relies on solving only a small number of instances of a discretecombinatorial optimization problem subject to randomly generated parityconstraints used as a hash function. As an application, we demonstrate thatwith a small number of MAP queries we can efficiently approximate the partitionfunction of discrete graphical models, which can in turn be used, for instance,for marginal computation or model selection.

Synthesizing Manipulation Sequences for Under-Specified Tasks using  Unrolled Markov Random Fields

  Many tasks in human environments require performing a sequence of navigationand manipulation steps involving objects. In unstructured human environments,the location and configuration of the objects involved often change inunpredictable ways. This requires a high-level planning strategy that is robustand flexible in an uncertain environment. We propose a novel dynamic planningstrategy, which can be trained from a set of example sequences. High leveltasks are expressed as a sequence of primitive actions or controllers (withappropriate parameters). Our score function, based on Markov Random Field(MRF), captures the relations between environment, controllers, and theirarguments. By expressing the environment using sets of attributes, the approachgeneralizes well to unseen scenarios. We train the parameters of our MRF usinga maximum margin learning method. We provide a detailed empirical validation ofour overall framework demonstrating successful plan strategies for a variety oftasks.

Optimization With Parity Constraints: From Binary Codes to Discrete  Integration

  Many probabilistic inference tasks involve summations over exponentiallylarge sets. Recently, it has been shown that these problems can be reduced tosolving a polynomial number of MAP inference queries for a model augmented withrandomly generated parity constraints. By exploiting a connection withmax-likelihood decoding of binary codes, we show that these optimizations arecomputationally hard. Inspired by iterative message passing decodingalgorithms, we propose an Integer Linear Programming (ILP) formulation for theproblem, enhanced with new sparsification techniques to improve decodingperformance. By solving the ILP through a sequence of LP relaxations, we getboth lower and upper bounds on the partition function, which hold with highprobability and are much tighter than those obtained with variational methods.

Pattern Decomposition with Complex Combinatorial Constraints:  Application to Materials Discovery

  Identifying important components or factors in large amounts of noisy data isa key problem in machine learning and data mining. Motivated by a patterndecomposition problem in materials discovery, aimed at discovering newmaterials for renewable energy, e.g. for fuel and solar cells, we introduceCombiFD, a framework for factor based pattern decomposition that allows theincorporation of a-priori knowledge as constraints, including complexcombinatorial constraints. In addition, we propose a new pattern decompositionalgorithm, called AMIQO, based on solving a sequence of (mixed-integer)quadratic programs. Our approach considerably outperforms the state of the arton the materials discovery problem, scaling to larger datasets and recoveringmore precise and physically meaningful decompositions. We also show theeffectiveness of our approach for enforcing background knowledge on otherapplication domains.

Variable Elimination in the Fourier Domain

  The ability to represent complex high dimensional probability distributionsin a compact form is one of the key insights in the field of graphical models.Factored representations are ubiquitous in machine learning and lead to majorcomputational advantages. We explore a different type of compact representationbased on discrete Fourier representations, complementing the classical approachbased on conditional independencies. We show that a large class ofprobabilistic graphical models have a compact Fourier representation. Thistheoretical result opens up an entirely new way of approximating a probabilitydistribution. We demonstrate the significance of this approach by applying itto the variable elimination algorithm. Compared with the traditional bucketrepresentation and other approximate inference algorithms, we obtainsignificant improvements.

Watch-Bot: Unsupervised Learning for Reminding Humans of Forgotten  Actions

  We present a robotic system that watches a human using a Kinect v2 RGB-Dsensor, detects what he forgot to do while performing an activity, and ifnecessary reminds the person using a laser pointer to point out the relatedobject. Our simple setup can be easily deployed on any assistive robot.  Our approach is based on a learning algorithm trained in a purelyunsupervised setting, which does not require any human annotations. This makesour approach scalable and applicable to variant scenarios. Our model learns theaction/object co-occurrence and action temporal relations in the activity, anduses the learned rich relationships to infer the forgotten action and therelated object. We show that our approach not only improves the unsupervisedaction segmentation and action cluster assignment performance, but alsoeffectively detects the forgotten actions on a challenging human activity RGB-Dvideo dataset. In robotic experiments, we show that our robot is able to remindpeople of forgotten actions successfully.

Solving Marginal MAP Problems with NP Oracles and Parity Constraints

  Arising from many applications at the intersection of decision making andmachine learning, Marginal Maximum A Posteriori (Marginal MAP) Problems unifythe two main classes of inference, namely maximization (optimization) andmarginal inference (counting), and are believed to have higher complexity thanboth of them. We propose XOR_MMAP, a novel approach to solve the Marginal MAPProblem, which represents the intractable counting subproblem with queries toNP oracles, subject to additional parity constraints. XOR_MMAP provides aconstant factor approximation to the Marginal MAP Problem, by encoding it as asingle optimization in polynomial size of the original problem. We evaluate ourapproach in several machine learning and decision making applications, and showthat our approach outperforms several state-of-the-art Marginal MAP solvers.

XOR-Sampling for Network Design with Correlated Stochastic Events

  Many network optimization problems can be formulated as stochastic networkdesign problems in which edges are present or absent stochastically.Furthermore, protective actions can guarantee that edges will remain present.We consider the problem of finding the optimal protection strategy under abudget limit in order to maximize some connectivity measurements of thenetwork. Previous approaches rely on the assumption that edges are independent.In this paper, we consider a more realistic setting where multiple edges arenot independent due to natural disasters or regional events that make thestates of multiple edges stochastically correlated. We use Markov Random Fieldsto model the correlation and define a new stochastic network design framework.We provide a novel algorithm based on Sample Average Approximation (SAA)coupled with a Gibbs or XOR sampler. The experimental results on real roadnetwork data show that the policies produced by SAA with the XOR sampler havehigher quality and lower variance compared to SAA with Gibbs sampler.

From spin glasses to hard satisfiable formulas

  We introduce a highly structured family of hard satisfiable 3-SAT formulascorresponding to an ordered spin-glass model from statistical physics. Thismodel has provably "glassy" behavior; that is, it has many local optima withlarge energy barriers between them, so that local search algorithms get stuckand have difficulty finding the true ground state, i.e., the unique satisfyingassignment. We test the hardness of our formulas with two Davis-Putnam solvers,Satz and zChaff, the recently introduced Survey Propagation (SP), and two localsearch algorithms, Walksat and Record-to-Record Travel (RRT). We compare ourformulas to random 3-XOR-SAT formulas and to two other generators of hardsatisfiable instances, the minimum disagreement parity formulas of Crawford etal., and Hirsch's hgen. For the complete solvers the running time of ourformulas grows exponentially in sqrt(n), and exceeds that of random 3-XOR-SATformulas for small problem sizes. SP is unable to solve our formulas with asfew as 25 variables. For Walksat, our formulas appear to be harder than anyother known generator of satisfiable instances. Finally, our formulas can besolved efficiently by RRT but only if the parameter d is tuned to the height ofthe barriers between local minima, and we use this parameter to measure thebarrier heights in random 3-XOR-SAT formulas as well.

On the Erdos Discrepancy Problem

  According to the Erd\H{o}s discrepancy conjecture, for any infinite $\pm 1$sequence, there exists a homogeneous arithmetic progression of unboundeddiscrepancy. In other words, for any $\pm 1$ sequence $(x_1,x_2,...)$ and adiscrepancy $C$, there exist integers $m$ and $d$ such that $|\sum_{i=1}^m x_{i\cdot d}| > C$. This is an $80$-year-old open problem and recent developmentproved that this conjecture is true for discrepancies up to $2$. Paul Erd\H{o}salso conjectured that this property of unbounded discrepancy even holds for therestricted case of completely multiplicative sequences (CMSs), namely sequences$(x_1,x_2,...)$ where $x_{a \cdot b} = x_{a} \cdot x_{b}$ for any $a,b \geq 1$.The longest CMS with discrepancy $2$ has been proven to be of size $246$. Inthis paper, we prove that any completely multiplicative sequence of size$127,646$ or more has discrepancy at least $4$, proving the Erd\H{o}sdiscrepancy conjecture for CMSs of discrepancies up to $3$. In addition, weprove that this bound is tight and increases the size of the longest knownsequence of discrepancy $3$ from $17,000$ to $127,645$. Finally, we provideinductive construction rules as well as streamlining methods to improve thelower bounds for sequences of higher discrepancies.

Watch-n-Patch: Unsupervised Learning of Actions and Relations

  There is a large variation in the activities that humans perform in theireveryday lives. We consider modeling these composite human activities whichcomprises multiple basic level actions in a completely unsupervised setting.Our model learns high-level co-occurrence and temporal relations between theactions. We consider the video as a sequence of short-term action clips, whichcontains human-words and object-words. An activity is about a set ofaction-topics and object-topics indicating which actions are present and whichobjects are interacting with. We then propose a new probabilistic modelrelating the words and the topics. It allows us to model long-range actionrelations that commonly exist in the composite activities, which is challengingin previous works. We apply our model to the unsupervised action segmentationand clustering, and to a novel application that detects forgotten actions,which we call action patching. For evaluation, we contribute a new challengingRGB-D activity video dataset recorded by the new Kinect v2, which containsseveral human daily activities as compositions of multiple actions interactingwith different objects. Moreover, we develop a robotic system that watchespeople and reminds people by applying our action patching algorithm. Ourrobotic setup can be easily deployed on any assistive robot.

Understanding Batch Normalization

  Batch normalization (BN) is a technique to normalize activations inintermediate layers of deep neural networks. Its tendency to improve accuracyand speed up training have established BN as a favorite technique in deeplearning. Yet, despite its enormous success, there remains little consensus onthe exact reason and mechanism behind these improvements. In this paper we takea step towards a better understanding of BN, following an empirical approach.We conduct several experiments, and show that BN primarily enables trainingwith larger learning rates, which is the cause for faster convergence andbetter generalization. For networks without BN we demonstrate how largegradient updates can result in diverging loss and activations growinguncontrollably with network depth, which limits possible learning rates. BNavoids this problem by constantly correcting activations to be zero-mean and ofunit standard deviation, which enables larger gradient steps, yields fasterconvergence and may help bypass sharp local minima. We further show variousways in which gradients and activations of deep unnormalized networks areill-behaved. We contrast our results against recent findings in random matrixtheory, shedding new light on classical initialization schemes and theirconsequences.

Structure and Problem Hardness: Goal Asymmetry and DPLL Proofs in<br>  SAT-Based Planning

  In Verification and in (optimal) AI Planning, a successful method is toformulate the application as boolean satisfiability (SAT), and solve it withstate-of-the-art DPLL-based procedures. There is a lack of understanding of whythis works so well. Focussing on the Planning context, we identify a form ofproblem structure concerned with the symmetrical or asymmetrical nature of thecost of achieving the individual planning goals. We quantify this sort ofstructure with a simple numeric parameter called AsymRatio, ranging between 0and 1. We run experiments in 10 benchmark domains from the InternationalPlanning Competitions since 2000; we show that AsymRatio is a good indicator ofSAT solver performance in 8 of these domains. We then examine carefully craftedsynthetic planning domains that allow control of the amount of structure, andthat are clean enough for a rigorous analysis of the combinatorial searchspace. The domains are parameterized by size, and by the amount of structure.The CNFs we examine are unsatisfiable, encoding one planning step less than thelength of the optimal plan. We prove upper and lower bounds on the size of thebest possible DPLL refutations, under different settings of the amount ofstructure, as a function of size. We also identify the best possible sets ofbranching variables (backdoors). With minimum AsymRatio, we prove exponentiallower bounds, and identify minimal backdoors of size linear in the number ofvariables. With maximum AsymRatio, we identify logarithmic DPLL refutations(and backdoors), showing a doubly exponential gap between the two structuralextreme cases. The reasons for this behavior -- the proof arguments --illuminate the prototypical patterns of structure causing the empiricalbehavior observed in the competition benchmarks.

