Classification Constrained Dimensionality Reduction

  Dimensionality reduction is a topic of recent interest. In this paper, we
present the classification constrained dimensionality reduction (CCDR)
algorithm to account for label information. The algorithm can account for
multiple classes as well as the semi-supervised setting. We present an
out-of-sample expressions for both labeled and unlabeled data. For unlabeled
data, we introduce a method of embedding a new point as preprocessing to a
classifier. For labeled data, we introduce a method that improves the embedding
during the training phase using the out-of-sample extension. We investigate
classification performance using the CCDR algorithm on hyper-spectral satellite
imagery data. We demonstrate the performance gain for both local and global
classifiers and demonstrate a 10% improvement of the $k$-nearest neighbors
algorithm performance. We present a connection between intrinsic dimension
estimation and the optimal embedding dimension obtained using the CCDR
algorithm.


Information Preserving Component Analysis: Data Projections for Flow
  Cytometry Analysis

  Flow cytometry is often used to characterize the malignant cells in leukemia
and lymphoma patients, traced to the level of the individual cell. Typically,
flow cytometric data analysis is performed through a series of 2-dimensional
projections onto the axes of the data set. Through the years, clinicians have
determined combinations of different fluorescent markers which generate
relatively known expression patterns for specific subtypes of leukemia and
lymphoma -- cancers of the hematopoietic system. By only viewing a series of
2-dimensional projections, the high-dimensional nature of the data is rarely
exploited. In this paper we present a means of determining a low-dimensional
projection which maintains the high-dimensional relationships (i.e.
information) between differing oncological data sets. By using machine learning
techniques, we allow clinicians to visualize data in a low dimension defined by
a linear combination of all of the available markers, rather than just 2 at a
time. This provides an aid in diagnosing similar forms of cancer, as well as a
means for variable selection in exploratory flow cytometric research. We refer
to our method as Information Preserving Component Analysis (IPCA).


An Information Geometric Framework for Dimensionality Reduction

  This report concerns the problem of dimensionality reduction through
information geometric methods on statistical manifolds. While there has been
considerable work recently presented regarding dimensionality reduction for the
purposes of learning tasks such as classification, clustering, and
visualization, these methods have focused primarily on Riemannian manifolds in
Euclidean space. While sufficient for many applications, there are many
high-dimensional signals which have no straightforward and meaningful Euclidean
representation. In these cases, signals may be more appropriately represented
as a realization of some distribution lying on a statistical manifold, or a
manifold of probability density functions (PDFs). We present a framework for
dimensionality reduction that uses information geometry for both statistical
manifold reconstruction as well as dimensionality reduction in the data domain.


Confidence-Constrained Maximum Entropy Framework for Learning from
  Multi-Instance Data

  Multi-instance data, in which each object (bag) contains a collection of
instances, are widespread in machine learning, computer vision, bioinformatics,
signal processing, and social sciences. We present a maximum entropy (ME)
framework for learning from multi-instance data. In this approach each bag is
represented as a distribution using the principle of ME. We introduce the
concept of confidence-constrained ME (CME) to simultaneously learn the
structure of distribution space and infer each distribution. The shared
structure underlying each density is used to learn from instances inside each
bag. The proposed CME is free of tuning parameters. We devise a fast
optimization algorithm capable of handling large scale multi-instance data. In
the experimental section, we evaluate the performance of the proposed approach
in terms of exact rank recovery in the space of distributions and compare it
with the regularized ME approach. Moreover, we compare the performance of CME
with Multi-Instance Learning (MIL) state-of-the-art algorithms and show a
comparable performance in terms of accuracy with reduced computational
complexity.


Dynamic Programming for Instance Annotation in Multi-instance
  Multi-label Learning

  Labeling data for classification requires significant human effort. To reduce
labeling cost, instead of labeling every instance, a group of instances (bag)
is labeled by a single bag label. Computer algorithms are then used to infer
the label for each instance in a bag, a process referred to as instance
annotation. This task is challenging due to the ambiguity regarding the
instance labels. We propose a discriminative probabilistic model for the
instance annotation problem and introduce an expectation maximization framework
for inference, based on the maximum likelihood approach. For many probabilistic
approaches, brute-force computation of the instance label posterior probability
given its bag label is exponential in the number of instances in the bag. Our
key contribution is a dynamic programming method for computing the posterior
that is linear in the number of instances. We evaluate our methods using both
benchmark and real world data sets, in the domain of bird song, image
annotation, and activity recognition. In many cases, the proposed framework
outperforms, sometimes significantly, the current state-of-the-art MIML
learning methods, both in instance label prediction and bag label prediction.


FINE: Fisher Information Non-parametric Embedding

  We consider the problems of clustering, classification, and visualization of
high-dimensional data when no straightforward Euclidean representation exists.
Typically, these tasks are performed by first reducing the high-dimensional
data to some lower dimensional Euclidean space, as many manifold learning
methods have been developed for this task. In many practical problems however,
the assumption of a Euclidean manifold cannot be justified. In these cases, a
more appropriate assumption would be that the data lies on a statistical
manifold, or a manifold of probability density functions (PDFs). In this paper
we propose using the properties of information geometry in order to define
similarities between data sets using the Fisher information metric. We will
show this metric can be approximated using entirely non-parametric methods, as
the parameterization of the manifold is generally unknown. Furthermore, by
using multi-dimensional scaling methods, we are able to embed the corresponding
PDFs into a low-dimensional Euclidean space. This not only allows for
classification of the data, but also visualization of the manifold. As a whole,
we refer to our framework as Fisher Information Non-parametric Embedding
(FINE), and illustrate its uses on a variety of practical problems, including
bio-medical applications and document classification.


Sparse image reconstruction for molecular imaging

  The application that motivates this paper is molecular imaging at the atomic
level. When discretized at sub-atomic distances, the volume is inherently
sparse. Noiseless measurements from an imaging technology can be modeled by
convolution of the image with the system point spread function (psf). Such is
the case with magnetic resonance force microscopy (MRFM), an emerging
technology where imaging of an individual tobacco mosaic virus was recently
demonstrated with nanometer resolution. We also consider additive white
Gaussian noise (AWGN) in the measurements. Many prior works of sparse
estimators have focused on the case when H has low coherence; however, the
system matrix H in our application is the convolution matrix for the system
psf. A typical convolution matrix has high coherence. The paper therefore does
not assume a low coherence H. A discrete-continuous form of the Laplacian and
atom at zero (LAZE) p.d.f. used by Johnstone and Silverman is formulated, and
two sparse estimators derived by maximizing the joint p.d.f. of the observation
and image conditioned on the hyperparameters. A thresholding rule that
generalizes the hard and soft thresholding rule appears in the course of the
derivation. This so-called hybrid thresholding rule, when used in the iterative
thresholding framework, gives rise to the hybrid estimator, a generalization of
the lasso. Unbiased estimates of the hyperparameters for the lasso and hybrid
estimator are obtained via Stein's unbiased risk estimate (SURE). A numerical
study with a Gaussian psf and two sparse images shows that the hybrid estimator
outperforms the lasso.


Novelty Detection Under Multi-Instance Multi-Label Framework

  Novelty detection plays an important role in machine learning and signal
processing. This paper studies novelty detection in a new setting where the
data object is represented as a bag of instances and associated with multiple
class labels, referred to as multi-instance multi-label (MIML) learning.
Contrary to the common assumption in MIML that each instance in a bag belongs
to one of the known classes, in novelty detection, we focus on the scenario
where bags may contain novel-class instances. The goal is to determine, for any
given instance in a new bag, whether it belongs to a known class or a novel
class. Detecting novelty in the MIML setting captures many real-world phenomena
and has many potential applications. For example, in a collection of tagged
images, the tag may only cover a subset of objects existing in the images.
Discovering an object whose class has not been previously tagged can be useful
for the purpose of soliciting a label for the new object class. To address this
novel problem, we present a discriminative framework for detecting new class
instances. Experiments demonstrate the effectiveness of our proposed method,
and reveal that the presence of unlabeled novel instances in training bags is
helpful to the detection of such instances in testing stage.


Weakly-supervised Dictionary Learning

  We present a probabilistic modeling and inference framework for
discriminative analysis dictionary learning under a weak supervision setting.
Dictionary learning approaches have been widely used for tasks such as
low-level signal denoising and restoration as well as high-level classification
tasks, which can be applied to audio and image analysis. Synthesis dictionary
learning aims at jointly learning a dictionary and corresponding sparse
coefficients to provide accurate data representation. This approach is useful
for denoising and signal restoration, but may lead to sub-optimal
classification performance. By contrast, analysis dictionary learning provides
a transform that maps data to a sparse discriminative representation suitable
for classification. We consider the problem of analysis dictionary learning for
time-series data under a weak supervision setting in which signals are assigned
with a global label instead of an instantaneous label signal. We propose a
discriminative probabilistic model that incorporates both label information and
sparsity constraints on the underlying latent instantaneous label signal using
cardinality control. We present the expectation maximization (EM) procedure for
maximum likelihood estimation (MLE) of the proposed model. To facilitate a
computationally efficient E-step, we propose both a chain and a novel tree
graph reformulation of the graphical model. The performance of the proposed
model is demonstrated on both synthetic and real-world data.


Empirical estimation of entropy functionals with confidence

  This paper introduces a class of k-nearest neighbor ($k$-NN) estimators
called bipartite plug-in (BPI) estimators for estimating integrals of
non-linear functions of a probability density, such as Shannon entropy and
R\'enyi entropy. The density is assumed to be smooth, have bounded support, and
be uniformly bounded from below on this set. Unlike previous $k$-NN estimators
of non-linear density functionals, the proposed estimator uses data-splitting
and boundary correction to achieve lower mean square error. Specifically, we
assume that $T$ i.i.d. samples ${X}_i \in \mathbb{R}^d$ from the density are
split into two pieces of cardinality $M$ and $N$ respectively, with $M$ samples
used for computing a k-nearest-neighbor density estimate and the remaining $N$
samples used for empirical estimation of the integral of the density
functional. By studying the statistical properties of k-NN balls, explicit
rates for the bias and variance of the BPI estimator are derived in terms of
the sample size, the dimension of the samples and the underlying probability
distribution. Based on these results, it is possible to specify optimal choice
of tuning parameters $M/T$, $k$ for maximizing the rate of decrease of the mean
square error (MSE). The resultant optimized BPI estimator converges faster and
achieves lower mean squared error than previous $k$-NN entropy estimators. In
addition, a central limit theorem is established for the BPI estimator that
allows us to specify tight asymptotic confidence intervals.


