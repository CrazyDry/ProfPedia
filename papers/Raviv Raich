Classification Constrained Dimensionality Reduction

  Dimensionality reduction is a topic of recent interest. In this paper, wepresent the classification constrained dimensionality reduction (CCDR)algorithm to account for label information. The algorithm can account formultiple classes as well as the semi-supervised setting. We present anout-of-sample expressions for both labeled and unlabeled data. For unlabeleddata, we introduce a method of embedding a new point as preprocessing to aclassifier. For labeled data, we introduce a method that improves the embeddingduring the training phase using the out-of-sample extension. We investigateclassification performance using the CCDR algorithm on hyper-spectral satelliteimagery data. We demonstrate the performance gain for both local and globalclassifiers and demonstrate a 10% improvement of the $k$-nearest neighborsalgorithm performance. We present a connection between intrinsic dimensionestimation and the optimal embedding dimension obtained using the CCDRalgorithm.

Information Preserving Component Analysis: Data Projections for Flow  Cytometry Analysis

  Flow cytometry is often used to characterize the malignant cells in leukemiaand lymphoma patients, traced to the level of the individual cell. Typically,flow cytometric data analysis is performed through a series of 2-dimensionalprojections onto the axes of the data set. Through the years, clinicians havedetermined combinations of different fluorescent markers which generaterelatively known expression patterns for specific subtypes of leukemia andlymphoma -- cancers of the hematopoietic system. By only viewing a series of2-dimensional projections, the high-dimensional nature of the data is rarelyexploited. In this paper we present a means of determining a low-dimensionalprojection which maintains the high-dimensional relationships (i.e.information) between differing oncological data sets. By using machine learningtechniques, we allow clinicians to visualize data in a low dimension defined bya linear combination of all of the available markers, rather than just 2 at atime. This provides an aid in diagnosing similar forms of cancer, as well as ameans for variable selection in exploratory flow cytometric research. We referto our method as Information Preserving Component Analysis (IPCA).

An Information Geometric Framework for Dimensionality Reduction

  This report concerns the problem of dimensionality reduction throughinformation geometric methods on statistical manifolds. While there has beenconsiderable work recently presented regarding dimensionality reduction for thepurposes of learning tasks such as classification, clustering, andvisualization, these methods have focused primarily on Riemannian manifolds inEuclidean space. While sufficient for many applications, there are manyhigh-dimensional signals which have no straightforward and meaningful Euclideanrepresentation. In these cases, signals may be more appropriately representedas a realization of some distribution lying on a statistical manifold, or amanifold of probability density functions (PDFs). We present a framework fordimensionality reduction that uses information geometry for both statisticalmanifold reconstruction as well as dimensionality reduction in the data domain.

Dynamic Programming for Instance Annotation in Multi-instance  Multi-label Learning

  Labeling data for classification requires significant human effort. To reducelabeling cost, instead of labeling every instance, a group of instances (bag)is labeled by a single bag label. Computer algorithms are then used to inferthe label for each instance in a bag, a process referred to as instanceannotation. This task is challenging due to the ambiguity regarding theinstance labels. We propose a discriminative probabilistic model for theinstance annotation problem and introduce an expectation maximization frameworkfor inference, based on the maximum likelihood approach. For many probabilisticapproaches, brute-force computation of the instance label posterior probabilitygiven its bag label is exponential in the number of instances in the bag. Ourkey contribution is a dynamic programming method for computing the posteriorthat is linear in the number of instances. We evaluate our methods using bothbenchmark and real world data sets, in the domain of bird song, imageannotation, and activity recognition. In many cases, the proposed frameworkoutperforms, sometimes significantly, the current state-of-the-art MIMLlearning methods, both in instance label prediction and bag label prediction.

Confidence-Constrained Maximum Entropy Framework for Learning from  Multi-Instance Data

  Multi-instance data, in which each object (bag) contains a collection ofinstances, are widespread in machine learning, computer vision, bioinformatics,signal processing, and social sciences. We present a maximum entropy (ME)framework for learning from multi-instance data. In this approach each bag isrepresented as a distribution using the principle of ME. We introduce theconcept of confidence-constrained ME (CME) to simultaneously learn thestructure of distribution space and infer each distribution. The sharedstructure underlying each density is used to learn from instances inside eachbag. The proposed CME is free of tuning parameters. We devise a fastoptimization algorithm capable of handling large scale multi-instance data. Inthe experimental section, we evaluate the performance of the proposed approachin terms of exact rank recovery in the space of distributions and compare itwith the regularized ME approach. Moreover, we compare the performance of CMEwith Multi-Instance Learning (MIL) state-of-the-art algorithms and show acomparable performance in terms of accuracy with reduced computationalcomplexity.

FINE: Fisher Information Non-parametric Embedding

  We consider the problems of clustering, classification, and visualization ofhigh-dimensional data when no straightforward Euclidean representation exists.Typically, these tasks are performed by first reducing the high-dimensionaldata to some lower dimensional Euclidean space, as many manifold learningmethods have been developed for this task. In many practical problems however,the assumption of a Euclidean manifold cannot be justified. In these cases, amore appropriate assumption would be that the data lies on a statisticalmanifold, or a manifold of probability density functions (PDFs). In this paperwe propose using the properties of information geometry in order to definesimilarities between data sets using the Fisher information metric. We willshow this metric can be approximated using entirely non-parametric methods, asthe parameterization of the manifold is generally unknown. Furthermore, byusing multi-dimensional scaling methods, we are able to embed the correspondingPDFs into a low-dimensional Euclidean space. This not only allows forclassification of the data, but also visualization of the manifold. As a whole,we refer to our framework as Fisher Information Non-parametric Embedding(FINE), and illustrate its uses on a variety of practical problems, includingbio-medical applications and document classification.

Sparse image reconstruction for molecular imaging

  The application that motivates this paper is molecular imaging at the atomiclevel. When discretized at sub-atomic distances, the volume is inherentlysparse. Noiseless measurements from an imaging technology can be modeled byconvolution of the image with the system point spread function (psf). Such isthe case with magnetic resonance force microscopy (MRFM), an emergingtechnology where imaging of an individual tobacco mosaic virus was recentlydemonstrated with nanometer resolution. We also consider additive whiteGaussian noise (AWGN) in the measurements. Many prior works of sparseestimators have focused on the case when H has low coherence; however, thesystem matrix H in our application is the convolution matrix for the systempsf. A typical convolution matrix has high coherence. The paper therefore doesnot assume a low coherence H. A discrete-continuous form of the Laplacian andatom at zero (LAZE) p.d.f. used by Johnstone and Silverman is formulated, andtwo sparse estimators derived by maximizing the joint p.d.f. of the observationand image conditioned on the hyperparameters. A thresholding rule thatgeneralizes the hard and soft thresholding rule appears in the course of thederivation. This so-called hybrid thresholding rule, when used in the iterativethresholding framework, gives rise to the hybrid estimator, a generalization ofthe lasso. Unbiased estimates of the hyperparameters for the lasso and hybridestimator are obtained via Stein's unbiased risk estimate (SURE). A numericalstudy with a Gaussian psf and two sparse images shows that the hybrid estimatoroutperforms the lasso.

Novelty Detection Under Multi-Instance Multi-Label Framework

  Novelty detection plays an important role in machine learning and signalprocessing. This paper studies novelty detection in a new setting where thedata object is represented as a bag of instances and associated with multipleclass labels, referred to as multi-instance multi-label (MIML) learning.Contrary to the common assumption in MIML that each instance in a bag belongsto one of the known classes, in novelty detection, we focus on the scenariowhere bags may contain novel-class instances. The goal is to determine, for anygiven instance in a new bag, whether it belongs to a known class or a novelclass. Detecting novelty in the MIML setting captures many real-world phenomenaand has many potential applications. For example, in a collection of taggedimages, the tag may only cover a subset of objects existing in the images.Discovering an object whose class has not been previously tagged can be usefulfor the purpose of soliciting a label for the new object class. To address thisnovel problem, we present a discriminative framework for detecting new classinstances. Experiments demonstrate the effectiveness of our proposed method,and reveal that the presence of unlabeled novel instances in training bags ishelpful to the detection of such instances in testing stage.

Weakly-supervised Dictionary Learning

  We present a probabilistic modeling and inference framework fordiscriminative analysis dictionary learning under a weak supervision setting.Dictionary learning approaches have been widely used for tasks such aslow-level signal denoising and restoration as well as high-level classificationtasks, which can be applied to audio and image analysis. Synthesis dictionarylearning aims at jointly learning a dictionary and corresponding sparsecoefficients to provide accurate data representation. This approach is usefulfor denoising and signal restoration, but may lead to sub-optimalclassification performance. By contrast, analysis dictionary learning providesa transform that maps data to a sparse discriminative representation suitablefor classification. We consider the problem of analysis dictionary learning fortime-series data under a weak supervision setting in which signals are assignedwith a global label instead of an instantaneous label signal. We propose adiscriminative probabilistic model that incorporates both label information andsparsity constraints on the underlying latent instantaneous label signal usingcardinality control. We present the expectation maximization (EM) procedure formaximum likelihood estimation (MLE) of the proposed model. To facilitate acomputationally efficient E-step, we propose both a chain and a novel treegraph reformulation of the graphical model. The performance of the proposedmodel is demonstrated on both synthetic and real-world data.

Empirical estimation of entropy functionals with confidence

  This paper introduces a class of k-nearest neighbor ($k$-NN) estimatorscalled bipartite plug-in (BPI) estimators for estimating integrals ofnon-linear functions of a probability density, such as Shannon entropy andR\'enyi entropy. The density is assumed to be smooth, have bounded support, andbe uniformly bounded from below on this set. Unlike previous $k$-NN estimatorsof non-linear density functionals, the proposed estimator uses data-splittingand boundary correction to achieve lower mean square error. Specifically, weassume that $T$ i.i.d. samples ${X}_i \in \mathbb{R}^d$ from the density aresplit into two pieces of cardinality $M$ and $N$ respectively, with $M$ samplesused for computing a k-nearest-neighbor density estimate and the remaining $N$samples used for empirical estimation of the integral of the densityfunctional. By studying the statistical properties of k-NN balls, explicitrates for the bias and variance of the BPI estimator are derived in terms ofthe sample size, the dimension of the samples and the underlying probabilitydistribution. Based on these results, it is possible to specify optimal choiceof tuning parameters $M/T$, $k$ for maximizing the rate of decrease of the meansquare error (MSE). The resultant optimized BPI estimator converges faster andachieves lower mean squared error than previous $k$-NN entropy estimators. Inaddition, a central limit theorem is established for the BPI estimator thatallows us to specify tight asymptotic confidence intervals.

