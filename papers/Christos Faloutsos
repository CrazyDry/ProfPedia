GMine: A System for Scalable, Interactive Graph Visualization and Mining

  Several graph visualization tools exist. However, they are not able to handlelarge graphs, and/or they do not allow interaction. We are interested on largegraphs, with hundreds of thousands of nodes. Such graphs bring two challenges:the first one is that any straightforward interactive manipulation will beprohibitively slow. The second one is sensory overload: even if we could plotand replot the graph quickly, the user would be overwhelmed with the vastvolume of information because the screen would be too cluttered as nodes andedges overlap each other. GMine system addresses both these issues, by usingsummarization and multi-resolution. GMine offers multi-resolution graphexploration by partitioning a given graph into a hierarchy ofcom-munities-within-communities and storing it into a novel R-tree-likestructure which we name G-Tree. GMine offers summarization by implementing aninnovative subgraph extraction algorithm and then visualizing its output.

Got the Flu (or Mumps)? Check the Eigenvalue!

  For a given, arbitrary graph, what is the epidemic threshold? That is, underwhat conditions will a virus result in an epidemic? We provide the super-modeltheorem, which generalizes older results in two important, orthogonaldimensions. The theorem shows that (a) for a wide range of virus propagationmodels (VPM) that include all virus propagation models in standard literature(say, [8][5]), and (b) for any contact graph, the answer always depends on thefirst eigenvalue of the connectivity matrix. We give the proof of the theorem,arithmetic examples for popular VPMs, like flu (SIS), mumps (SIR), SIRS andmore. We also show the implications of our discovery: easy (although sometimescounter-intuitive) answers to `what-if' questions; easier design and evaluationof immunization policies, and significantly faster agent-based simulations.

A Linear-Time Approximation of the Earth Mover's Distance

  Color descriptors are one of the important features used in content-basedimage retrieval. The Dominant Color Descriptor (DCD) represents a fewperceptually dominant colors in an image through color quantization. For imageretrieval based on DCD, the earth mover's distance and the optimal colorcomposition distance are proposed to measure the dissimilarity between twoimages. Although providing good retrieval results, both methods are tootime-consuming to be used in a large image database. To solve the problem, wepropose a new distance function that calculates an approximate earth mover'sdistance in linear time. To calculate the dissimilarity in linear time, theproposed approach employs the space-filling curve for multidimensional colorspace. To improve the accuracy, the proposed approach uses multiple curves andadjusts the color positions. As a result, our approach achievesorder-of-magnitude time improvement but incurs small errors. We have performedextensive experiments to show the effectiveness and efficiency of the proposedapproach. The results reveal that our approach achieves almost the same resultswith the EMD in linear time.

V-SMART-Join: A Scalable MapReduce Framework for All-Pair Similarity  Joins of Multisets and Vectors

  This work proposes V-SMART-Join, a scalable MapReduce-based framework fordiscovering all pairs of similar entities. The V-SMART-Join framework isapplicable to sets, multisets, and vectors. V-SMART-Join is motivated by theobserved skew in the underlying distributions of Internet traffic, and is afamily of 2-stage algorithms, where the first stage computes and joins thepartial results, and the second stage computes the similarity exactly for allcandidate pairs. The V-SMART-Join algorithms are very efficient and scalable inthe number of entities, as well as their cardinalities. They were up to 30times faster than the state of the art algorithm, VCL, when compared on a realdataset of a small size. We also established the scalability of the proposedalgorithms by running them on a dataset of a realistic size, on which VCL neversucceeded to finish. Experiments were run using real datasets of IPs andcookies, where each IP is represented as a multiset of cookies, and the goal isto discover similar IPs to identify Internet proxies.

NetSimile: A Scalable Approach to Size-Independent Network Similarity

  Given a set of k networks, possibly with different sizes and no overlaps innodes or edges, how can we quickly assess similarity between them, withoutsolving the node-correspondence problem? Analogously, how can we extract asmall number of descriptive, numerical features from each graph thateffectively serve as the graph's "signature"? Having such features will enablea wealth of graph mining tasks, including clustering, outlier detection,visualization, etc.  We propose NetSimile -- a novel, effective, and scalable method for solvingthe aforementioned problem. NetSimile has the following desirable properties:(a) It gives similarity scores that are size-invariant. (b) It is scalable,being linear on the number of edges for "signature" vector extraction. (c) Itdoes not need to solve the node-correspondence problem. We present extensiveexperiments on numerous synthetic and real graphs from disparate domains, andshow NetSimile's superiority over baseline competitors. We also show howNetSimile enables several mining tasks such as clustering, visualization,discontinuity detection, network transfer learning, and re-identificationacross networks.

DELTACON: A Principled Massive-Graph Similarity Function

  How much did a network change since yesterday? How different is the wiringbetween Bob's brain (a left-handed male) and Alice's brain (a right-handedfemale)? Graph similarity with known node correspondence, i.e. the detection ofchanges in the connectivity of graphs, arises in numerous settings. In thiswork, we formally state the axioms and desired properties of the graphsimilarity functions, and evaluate when state-of-the-art methods fail to detectcrucial connectivity changes in graphs. We propose DeltaCon, a principled,intuitive, and scalable algorithm that assesses the similarity between twographs on the same nodes (e.g. employees of a company, customers of a mobilecarrier). Experiments on various synthetic and real graphs showcase theadvantages of our method over existing similarity measures. Finally, we employDeltaCon to real applications: (a) we classify people to groups of high and lowcreativity based on their brain connectivity graphs, and (b) do temporalanomaly detection in the who-emails-whom Enron graph.

Modeling Website Popularity Competition in the Attention-Activity  Marketplace

  How does a new startup drive the popularity of competing websites intooblivion like Facebook famously did to MySpace? This question is of greatinterest to academics, technologists, and financial investors alike. In thiswork we exploit the singular way in which Facebook wiped out the popularity ofMySpace, Hi5, Friendster, and Multiply to guide the design of a new popularitycompetition model. Our model provides new insights into what Nobel LaureateHerbert A. Simon called the "marketplace of attention," which we recast as theattention-activity marketplace. Our model design is further substantiated byuser-level activity of 250,000 MySpace users obtained between 2004 and 2009.The resulting model not only accurately fits the observed Daily Active Users(DAU) of Facebook and its competitors but also predicts their fate four yearsinto the future.

Revisit Behavior in Social Media: The Phoenix-R Model and Discoveries

  How many listens will an artist receive on a online radio? How about plays ona YouTube video? How many of these visits are new or returning users? Modelingand mining popularity dynamics of social activity has important implicationsfor researchers, content creators and providers. We here investigate the effectof revisits (successive visits from a single user) on content popularity. Usingfour datasets of social activity, with up to tens of millions media objects(e.g., YouTube videos, Twitter hashtags or LastFM artists), we show the effectof revisits in the popularity evolution of such objects. Secondly, we proposethe Phoenix-R model which captures the popularity dynamics of individualobjects. Phoenix-R has the desired properties of being: (1) parsimonious, beingbased on the minimum description length principle, and achieving lower rootmean squared error than state-of-the-art baselines; (2) applicable, the modelis effective for predicting future popularity values of objects.

VoG: Summarizing and Understanding Large Graphs

  How can we succinctly describe a million-node graph with a few simplesentences? How can we measure the "importance" of a set of discovered subgraphsin a large graph? These are exactly the problems we focus on. Our main ideasare to construct a "vocabulary" of subgraph-types that often occur in realgraphs (e.g., stars, cliques, chains), and from a set of subgraphs, find themost succinct description of a graph in terms of this vocabulary. We measuresuccess in a well-founded way by means of the Minimum Description Length (MDL)principle: a subgraph is included in the summary if it decreases the totaldescription length of the graph.  Our contributions are three-fold: (a) formulation: we provide a principledencoding scheme to choose vocabulary subgraphs; (b) algorithm: we develop\method, an efficient method to minimize the description cost, and (c)applicability: we report experimental results on multi-million-edge realgraphs, including Flickr and the Notre Dame web graph.

Standards for Graph Algorithm Primitives

  It is our view that the state of the art in constructing a large collectionof graph algorithms in terms of linear algebraic operations is mature enough tosupport the emergence of a standard set of primitive building blocks. Thispaper is a position paper defining the problem and announcing our intention tolaunch an open effort to define this standard.

SuperGraph Visualization

  Given a large social or computer network, how can we visualize it, findpatterns, outliers, communities? Although several graph visualization toolsexist, they cannot handle large graphs with hundred thousand nodes and possiblymillion edges. Such graphs bring two challenges: interactive visualizationdemands prohibitive processing power and, even if we could interactively updatethe visualization, the user would be overwhelmed by the excessive number ofgraphical items. To cope with this problem, we propose a formal innovation onthe use of graph hierarchies that leads to GMine system. GMine promotesscalability using a hierarchy of graph partitions, promotes concomitantpresentation for the graph hierarchy and for the original graph, and extendsanalytical possibilities with the integration of the graph partitions in aninteractive environment.

TribeFlow: Mining & Predicting User Trajectories

  Which song will Smith listen to next? Which restaurant will Alice go totomorrow? Which product will John click next? These applications have in commonthe prediction of user trajectories that are in a constant state of flux over ahidden network (e.g. website links, geographic location). What users are doingnow may be unrelated to what they will be doing in an hour from now. Mindful ofthese challenges we propose TribeFlow, a method designed to cope with thecomplex challenges of learning personalized predictive models ofnon-stationary, transient, and time-heterogeneous user trajectories. TribeFlowis a general method that can perform next product recommendation, next songrecommendation, next location prediction, and general arbitrary-length usertrajectory prediction without domain-specific knowledge. TribeFlow is moreaccurate and up to 413x faster than top competitors.

M3A: Model, MetaModel, and Anomaly Detection in Web Searches

  'Alice' is submitting one web search per five minutes, for three hours in arow - is it normal? How to detect abnormal search behaviors, among Alice andother users? Is there any distinct pattern in Alice's (or other users') searchbehavior? We studied what is probably the largest, publicly available, querylog that contains more than 30 million queries from 0.6 million users. In thispaper, we present a novel, user-and group-level framework, M3A: Model,MetaModel and Anomaly detection. For each user, we discover and explain asurprising, bi-modal pattern of the inter-arrival time (IAT) of landed queries(queries with user click-through). Specifically, the model Camel-Log isproposed to describe such an IAT distribution; we then notice the correlationsamong its parameters at the group level. Thus, we further propose the metamodelMeta-Click, to capture and explain the two-dimensional, heavy-tail distributionof the parameters. Combining Camel-Log and Meta-Click, the proposed M3A has thefollowing strong points: (1) the accurate modeling of marginal IATdistribution, (2) quantitative interpretations, and (3) anomaly detection.

PNP: Fast Path Ensemble Method for Movie Design

  How can we design a product or movie that will attract, for example, theinterest of Pennsylvania adolescents or liberal newspaper critics? What shouldbe the genre of that movie and who should be in the cast? In this work, we seekto identify how we can design new movies with features tailored to a specificuser population. We formulate the movie design as an optimization problem overthe inference of user-feature scores and selection of the features thatmaximize the number of attracted users. Our approach, PNP, is based on aheterogeneous, tripartite graph of users, movies and features (e.g., actors,directors, genres), where users rate movies and features contribute to movies.We learn the preferences by leveraging user similarities defined throughdifferent types of relations, and show that our method outperformsstate-of-the-art approaches, including matrix factorization and otherheterogeneous graph-based analysis. We evaluate PNP on publicly availablereal-world data and show that it is highly scalable and effectively providesmovie designs oriented towards different groups of users, including men, women,and adolescents.

Fast, Warped Graph Embedding: Unifying Framework and One-Click Algorithm

  What is the best way to describe a user in a social network with just a fewnumbers? Mathematically, this is equivalent to assigning a vectorrepresentation to each node in a graph, a process called graph embedding. Wepropose a novel framework, GEM-D that unifies most of the past algorithms suchas LapEigs, DeepWalk and node2vec. GEM-D achieves its goal by decomposing anygraph embedding algorithm into three building blocks: node proximity function,warping function and loss function. Based on thorough analysis of GEM-D, wepropose a novel algorithm, called UltimateWalk, which outperforms themost-recently proposed state-of-the-art DeepWalk and node2vec. Thecontributions of this work are: (1) The proposed framework, GEM-D unifies thepast graph embedding algorithms and provides a general recipe of how to designa graph embedding; (2) the nonlinearlity in the warping function contributessignificantly to the quality of embedding and the exponential function isempirically optimal; (3) the proposed algorithm, UltimateWalk is one-click (nouser-defined parameters), scalable and has a closed-form solution.

HoloScope: Topology-and-Spike Aware Fraud Detection

  As online fraudsters invest more resources, including purchasing large poolsof fake user accounts and dedicated IPs, fraudulent attacks become less obviousand their detection becomes increasingly challenging. Existing approaches suchas average degree maximization suffer from the bias of including more nodesthan necessary, resulting in lower accuracy and increased need for manualverification. Hence, we propose HoloScope, which uses information from graphtopology and temporal spikes to more accurately detect groups of fraudulentusers. In terms of graph topology, we introduce "contrast suspiciousness," adynamic weighting approach, which allows us to more accurately detectfraudulent blocks, particularly low-density blocks. In terms of temporalspikes, HoloScope takes into account the sudden bursts and drops of fraudsters'attacking patterns. In addition, we provide theoretical bounds for how muchthis increases the time cost needed for fraudsters to conduct adversarialattacks. Additionally, from the perspective of ratings, HoloScope incorporatesthe deviation of rating scores in order to catch fraudsters more accurately.Moreover, HoloScope has a concise framework and sub-quadratic time complexity,making the algorithm reproducible and scalable. Extensive experiments showedthat HoloScope achieved significant accuracy improvements on synthetic and realdata, compared with state-of-the-art fraud detection methods.

DenseAlert: Incremental Dense-Subtensor Detection in Tensor Streams

  Consider a stream of retweet events - how can we spot fraudulent lock-stepbehavior in such multi-aspect data (i.e., tensors) evolving over time? Can wedetect it in real time, with an accuracy guarantee? Past studies have shownthat dense subtensors tend to indicate anomalous or even fraudulent behavior inmany tensor data, including social media, Wikipedia, and TCP dumps. Thus,several algorithms have been proposed for detecting dense subtensors rapidlyand accurately. However, existing algorithms assume that tensors are static,while many real-world tensors, including those mentioned above, evolve overtime.  We propose DenseStream, an incremental algorithm that maintains and updates adense subtensor in a tensor stream (i.e., a sequence of changes in a tensor),and DenseAlert, an incremental algorithm spotting the sudden appearances ofdense subtensors. Our algorithms are: (1) Fast and 'any time': updates by ouralgorithms are up to a million times faster than the fastest batch algorithms,(2) Provably accurate: our algorithms guarantee a lower bound on the density ofthe subtensor they maintain, and (3) Effective: our DenseAlert successfullyspots anomalies in real-world tensors, especially those overlooked by existingalgorithms.

EagleMine: Vision-Guided Mining in Large Graphs

  Given a graph with millions of nodes, what patterns exist in thedistributions of node characteristics, and how can we detect them and separateanomalous nodes in a way similar to human vision? In this paper, we propose avision-guided algorithm, EagleMine, to summarize micro-cluster patterns intwo-dimensional histogram plots constructed from node features in a largegraph. EagleMine utilizes a water-level tree to capture cluster structuresaccording to vision-based intuition at multi-resolutions. EagleMine traversesthe water-level tree from the root and adopts statistical hypothesis tests todetermine the optimal clusters that should be fitted along the path, andsummarizes each cluster with a truncated Gaussian distribution. Experiments onreal data show that our method can find truncated and overlapped ellipticalclusters, even when some baseline methods split one visual cluster into pieceswith Gaussian spheres. To identify potentially anomalous microclusters,EagleMine also a designates score to measure the suspiciousness of outliergroups (i.e. node clusters) and outlier nodes, detecting bots and anomaloususers with high accuracy in the real Microblog data.

DiSLR: Distributed Sampling with Limited Redundancy For Triangle  Counting in Graph Streams

  Given a web-scale graph that grows over time, how should its edges be storedand processed on multiple machines for rapid and accurate estimation of thecount of triangles? The count of triangles (i.e., cliques of size three) hasproven useful in many applications, including anomaly detection, communitydetection, and link recommendation. For triangle counting in large and dynamicgraphs, recent work has focused largely on streaming algorithms and distributedalgorithms. To achieve the advantages of both approaches, we propose DiSLR, adistributed streaming algorithm that estimates the counts of global trianglesand local triangles associated with each node. Making one pass over the inputstream, DiSLR carefully processes and stores the edges across multiple machinesso that the redundant use of computational and storage resources is minimized.Compared to its best competitors, DiSLR is (a) Accurate: giving up to 39Xsmaller estimation error, (b) Fast: up to 10.4X faster, scaling linearly withthe number of edges in the input stream, and (c) Theoretically sound: yieldingunbiased estimates with variances decreasing faster as the number of machinesis scaled up.

LinkNBed: Multi-Graph Representation Learning with Entity Linkage

  Knowledge graphs have emerged as an important model for studying complexmulti-relational data. This has given rise to the construction of numerouslarge scale but incomplete knowledge graphs encoding information extracted fromvarious resources. An effective and scalable approach to jointly learn overmultiple graphs and eventually construct a unified graph is a crucial next stepfor the success of knowledge-based inference for many downstream applications.To this end, we propose LinkNBed, a deep relational learning framework thatlearns entity and relationship representations across multiple graphs. Weidentify entity linkage across graphs as a vital component to achieve our goal.We design a novel objective that leverage entity linkage and build an efficientmulti-task training procedure. Experiments on link prediction and entitylinkage demonstrate substantial improvements over the state-of-the-artrelational learning approaches.

Impact of Load Models on Power Flow Optimization

  Aggregated load models, such as PQ and ZIP, are used to represent theapproximated load demand at specific buses in grid simulation and optimizationproblems. In this paper we examine the impact of model choice on the optimalpower flow solution and demonstrate that it is possible for different loadmodels to represent the same amount of real and reactive power at the optimalsolution yet correspond to completely different grid operating points. Weintroduce the metric derived from the maximum power transfer theorem toidentify the behavior of an aggregated model in the OPF formulation to indicateits possible limitations. A dataset from the Carnegie Mellon campus is used tocharacterize three types of load models using a time-series machine learningalgorithm, from which the optimal power flow results demonstrate that thechoice of load model type has a significant impact on the solution set points.For example, our results show that the PQ load accurately characterizes the CMUdata behavior correctly for only 16.7% of the cases.

Graph Evolution: Densification and Shrinking Diameters

  How do real graphs evolve over time? What are ``normal'' growth patterns insocial, technological, and information networks? Many studies have discoveredpatterns in static graphs, identifying properties in a single snapshot of alarge network, or in a very small number of snapshots; these include heavytails for in- and out-degree distributions, communities, small-world phenomena,and others. However, given the lack of information about network evolution overlong periods, it has been hard to convert these findings into statements abouttrends over time.  Here we study a wide range of real graphs, and we observe some surprisingphenomena. First, most of these graphs densify over time, with the number ofedges growing super-linearly in the number of nodes. Second, the averagedistance between nodes often shrinks over time, in contrast to the conventionalwisdom that such distance parameters should increase slowly as a function ofthe number of nodes (like O(log n) or O(log(log n)).  Existing graph generation models do not exhibit these types of behavior, evenat a qualitative level. We provide a new graph generator, based on a ``forestfire'' spreading process, that has a simple, intuitive justification, requiresvery few parameters (like the ``flammability'' of nodes), and produces graphsexhibiting the full range of properties observed both in prior work and in thepresent study.  We also notice that the ``forest fire'' model exhibits a sharp transitionbetween sparse graphs and graphs that are densifying. Graphs with decreasingdistance between the nodes are generated around this transition point.

Cascading Behavior in Large Blog Graphs

  How do blogs cite and influence each other? How do such links evolve? Doesthe popularity of old blog posts drop exponentially with time? These are someof the questions that we address in this work. Our goal is to build a modelthat generates realistic cascades, so that it can help us with link predictionand outlier detection.  Blogs (weblogs) have become an important medium of information because oftheir timely publication, ease of use, and wide availability. In fact, theyoften make headlines, by discussing and discovering evidence about politicalevents and facts. Often blogs link to one another, creating a publiclyavailable record of how information and influence spreads through an underlyingsocial network. Aggregating links from several blog posts creates a directedgraph which we analyze to discover the patterns of information propagation inblogspace, and thereby understand the underlying social network. Not only areblogs interesting on their own merit, but our analysis also sheds light on howrumors, viruses, and ideas propagate over social and computer networks.  Here we report some surprising findings of the blog linking and informationpropagation structure, after we analyzed one of the largest available datasets,with 45,000 blogs and ~ 2.2 million blog-postings. Our analysis also shedslight on how rumors, viruses, and ideas propagate over social and computernetworks. We also present a simple model that mimics the spread of informationon the blogosphere, and produces information cascades very similar to thosefound in real life.

Kronecker Graphs: An Approach to Modeling Networks

  How can we model networks with a mathematically tractable model that allowsfor rigorous analysis of network properties? Networks exhibit a long list ofsurprising properties: heavy tails for the degree distribution; smalldiameters; and densification and shrinking diameters over time. Most presentnetwork models either fail to match several of the above properties, arecomplicated to analyze mathematically, or both. In this paper we propose agenerative model for networks that is both mathematically tractable and cangenerate networks that have the above mentioned properties. Our main idea is touse the Kronecker product to generate graphs that we refer to as "Kroneckergraphs".  First, we prove that Kronecker graphs naturally obey common networkproperties. We also provide empirical evidence showing that Kronecker graphscan effectively model the structure of real networks.  We then present KronFit, a fast and scalable algorithm for fitting theKronecker graph generation model to large real networks. A naive approach tofitting would take super- exponential time. In contrast, KronFit takes lineartime, by exploiting the structure of Kronecker matrix multiplication and byusing statistical simulation techniques.  Experiments on large real and synthetic networks show that KronFit findsaccurate parameters that indeed very well mimic the properties of targetnetworks. Once fitted, the model parameters can be used to gain insights aboutthe network structure, and the resulting synthetic graphs can be used for null-models, anonymization, extrapolations, and graph summarization.

Scoup-SMT: Scalable Coupled Sparse Matrix-Tensor Factorization

  How can we correlate neural activity in the human brain as it responds towords, with behavioral data expressed as answers to questions about these samewords? In short, we want to find latent variables, that explain both the brainactivity, as well as the behavioral responses. We show that this is an instanceof the Coupled Matrix-Tensor Factorization (CMTF) problem. We proposeScoup-SMT, a novel, fast, and parallel algorithm that solves the CMTF problemand produces a sparse latent low-rank subspace of the data. In our experiments,we find that Scoup-SMT is 50-100 times faster than a state-of-the-art algorithmfor CMTF, along with a 5 fold increase in sparsity. Moreover, we extendScoup-SMT to handle missing data without degradation of performance. We applyScoup-SMT to BrainQ, a dataset consisting of a (nouns, brain voxels, humansubjects) tensor and a (nouns, properties) matrix, with coupling along thenouns dimension. Scoup-SMT is able to find meaningful latent variables, as wellas to predict brain activity with competitive accuracy. Finally, we demonstratethe generality of Scoup-SMT, by applying it on a Facebook dataset (users,friends, wall-postings); there, Scoup-SMT spots spammer-like anomalies.

Universal and Distinct Properties of Communication Dynamics: How to  Generate Realistic Inter-event Times

  With the advancement of information systems, means of communications arebecoming cheaper, faster and more available. Today, millions of people carryingsmart-phones or tablets are able to communicate at practically any time andanywhere they want. Among others, they can access their e-mails, comment onweblogs, watch and post comments on videos, make phone calls or text messagesalmost ubiquitously. Given this scenario, in this paper we tackle a fundamentalaspect of this new era of communication: how the time intervals betweencommunication events behave for different technologies and means ofcommunications? Are there universal patterns for the inter-event timedistribution (IED)? In which ways inter-event times behave differently amongparticular technologies? To answer these questions, we analyze eight differentdatasets from real and modern communication data and we found four well definedpatterns that are seen in all the eight datasets. Moreover, we propose the useof the Self-Feeding Process (SFP) to generate inter-event times betweencommunications. The SFP is extremely parsimonious point process that requiresat most two parameters and is able to generate inter-event times with all theuniversal properties we observed in the data. We show the potential applicationof SFP by proposing a framework to generate a synthetic dataset containingrealistic communication events of any one of the analyzed means ofcommunications (e.g. phone calls, e-mails, comments on blogs) and an algorithmto detect anomalies.

Spotting Suspicious Link Behavior with fBox: An Adversarial Perspective

  How can we detect suspicious users in large online networks? Onlinepopularity of a user or product (via follows, page-likes, etc.) can bemonetized on the premise of higher ad click-through rates or increased sales.Web services and social networks which incentivize popularity thus suffer froma major problem of fake connections from link fraudsters looking to make aquick buck. Typical methods of catching this suspicious behavior use spectraltechniques to spot large groups of often blatantly fraudulent (but sometimeshonest) users. However, small-scale, stealthy attacks may go unnoticed due tothe nature of low-rank eigenanalysis used in practice.  In this work, we take an adversarial approach to find and prove claims aboutthe weaknesses of modern, state-of-the-art spectral methods and propose fBox,an algorithm designed to catch small-scale, stealth attacks that slip below theradar. Our algorithm has the following desirable properties: (a) it hastheoretical underpinnings, (b) it is shown to be highly effective on real dataand (c) it is scalable (linear on the input size). We evaluate fBox on a large,public 41.7 million node, 1.5 billion edge who-follows-whom social graph fromTwitter in 2010 and with high precision identify many suspicious accounts whichhave persisted without suspension even to this day.

Large Graph Analysis in the GMine System

  Current applications have produced graphs on the order of hundreds ofthousands of nodes and millions of edges. To take advantage of such graphs, onemust be able to find patterns, outliers and communities. These tasks are betterperformed in an interactive environment, where human expertise can guide theprocess. For large graphs, though, there are some challenges: the excessiveprocessing requirements are prohibitive, and drawing hundred-thousand nodesresults in cluttered images hard to comprehend. To cope with these problems, wepropose an innovative framework suited for any kind of tree-like graph visualdesign. GMine integrates (a) a representation for graphs organized ashierarchies of partitions - the concepts of SuperGraph and Graph-Tree; and (b)a graph summarization methodology - CEPS. Our graph representation deals withthe problem of tracing the connection aspects of a graph hierarchy with sublinear complexity, allowing one to grasp the neighborhood of a single node orof a group of nodes in a single click. As a proof of concept, the visualenvironment of GMine is instantiated as a system in which large graphs can beinvestigated globally and locally.

EdgeCentric: Anomaly Detection in Edge-Attributed Networks

  Given a network with attributed edges, how can we identify anomalousbehavior? Networks with edge attributes are commonplace in the real world. Forexample, edges in e-commerce networks often indicate how users rated productsand services in terms of number of stars, and edges in online social andphonecall networks contain temporal information about when friendships wereformed and when users communicated with each other -- in such cases, edgeattributes capture information about how the adjacent nodes interact with otherentities in the network. In this paper, we aim to utilize exactly thisinformation to discern suspicious from typical node behavior. Our work has anumber of notable contributions, including (a) formulation: while most othergraph-based anomaly detection works use structural graph connectivity or nodeinformation, we focus on the new problem of leveraging edge information, (b)methodology: we introduce EdgeCentric, an intuitive and scalablecompression-based approach for detecting edge-attributed graph anomalies, and(c) practicality: we show that EdgeCentric successfully spots numerous suchanomalies in several large, edge-attributed real-world graphs, including theFlipkart e-commerce graph with over 3 million product reviews between 1.1million users and 545 thousand products, where it achieved 0.87 precision overthe top 100 results.

BIRDNEST: Bayesian Inference for Ratings-Fraud Detection

  Review fraud is a pervasive problem in online commerce, in which fraudulentsellers write or purchase fake reviews to manipulate perception of theirproducts and services. Fake reviews are often detected based on several signs,including 1) they occur in short bursts of time; 2) fraudulent user accountshave skewed rating distributions. However, these may both be true in any givendataset. Hence, in this paper, we propose an approach for detecting fraudulentreviews which combines these 2 approaches in a principled manner, allowingsuccessful detection even when one of these signs is not present. To combinethese 2 approaches, we formulate our Bayesian Inference for Rating Data (BIRD)model, a flexible Bayesian model of user rating behavior. Based on our model weformulate a likelihood-based suspiciousness metric, Normalized ExpectedSurprise Total (NEST). We propose a linear-time algorithm for performingBayesian inference using our model and computing the metric. Experiments onreal data show that BIRDNEST successfully spots review fraud in large,real-world graphs: the 50 most suspicious users of the Flipkart platformflagged by our algorithm were investigated and all identified as fraudulent bydomain experts at Flipkart.

Tensor Decomposition for Signal Processing and Machine Learning

  Tensors or {\em multi-way arrays} are functions of three or more indices$(i,j,k,\cdots)$ -- similar to matrices (two-way arrays), which are functionsof two indices $(r,c)$ for (row,column). Tensors have a rich history,stretching over almost a century, and touching upon numerous disciplines; butthey have only recently become ubiquitous in signal and data analytics at theconfluence of signal processing, statistics, data mining and machine learning.This overview article aims to provide a good starting point for researchers andpractitioners interested in learning about and working with tensors. As such,it focuses on fundamentals and motivation (using various application examples),aiming to strike an appropriate balance of breadth {\em and depth} that willenable someone having taken first graduate courses in matrix algebra andprobability to get started doing research and/or developing tensor algorithmsand software. Some background in applied optimization is useful but notstrictly required. The material covered includes tensor rank and rankdecomposition; basic tensor factorization models and their relationships andproperties (including fairly good coverage of identifiability); broad coverageof algorithms ranging from alternating optimization to stochastic gradient;statistical performance analysis; and applications ranging from sourceseparation to collaborative filtering, mixture and topic modeling,classification, and multilinear subspace learning.

FairJudge: Trustworthy User Prediction in Rating Platforms

  Rating platforms enable large-scale collection of user opinion about items(products, other users, etc.). However, many untrustworthy users givefraudulent ratings for excessive monetary gains. In the paper, we presentFairJudge, a system to identify such fraudulent users. We propose threemetrics: (i) the fairness of a user that quantifies how trustworthy the user isin rating the products, (ii) the reliability of a rating that measures howreliable the rating is, and (iii) the goodness of a product that measures thequality of the product. Intuitively, a user is fair if it provides reliableratings that are close to the goodness of the product. We formulate a mutuallyrecursive definition of these metrics, and further address cold start problemsand incorporate behavioral properties of users and products in the formulation.We propose an iterative algorithm, FairJudge, to predict the values of thethree metrics. We prove that FairJudge is guaranteed to converge in a boundednumber of iterations, with linear time complexity. By conducting five differentexperiments on five rating platforms, we show that FairJudge significantlyoutperforms nine existing algorithms in predicting fair and unfair users. Wereported the 100 most unfair users in the Flipkart network to their reviewfraud investigators, and 80 users were correctly identified (80% accuracy). TheFairJudge algorithm is already being deployed at Flipkart.

The Many Faces of Link Fraud

  Most past work on social network link fraud detection tries to separategenuine users from fraudsters, implicitly assuming that there is only one typeof fraudulent behavior. But is this assumption true? And, in either case, whatare the characteristics of such fraudulent behaviors? In this work, we set uphoneypots ("dummy" social network accounts), and buy fake followers (aftercareful IRB approval). We report the signs of such behaviors including odditiesin local network connectivity, account attributes, and similarities anddifferences across fraud providers. Most valuably, we discover and characterizeseveral types of fraud behaviors. We discuss how to leverage our insights inpractice by engineering strongly performing entropy-based features anddemonstrating high classification accuracy. Our contributions are (a)instrumentation: we detail our experimental setup and carefully engineered datacollection process to scrape Twitter data while respecting API rate-limits, (b)observations on fraud multimodality: we analyze our honeypot fraudsterecosystem and give surprising insights into the multifaceted behaviors of thesefraudster types, and (c) features: we propose novel features that give strong(>0.95 precision/recall) discriminative power on ground-truth Twitter data.

Structural patterns of information cascades and their implications for  dynamics and semantics

  Information cascades are ubiquitous in both physical society and onlinesocial media, taking on large variations in structures, dynamics and semantics.Although the dynamics and semantics of information cascades have been studied,the structural patterns and their correlations with dynamics and semantics arelargely unknown. Here we explore a large-scale dataset including $432$ millioninformation cascades with explicit records of spreading traces, spreadingbehaviors, information content as well as user profiles. We find that thestructural complexity of information cascades is far beyond the previousconjectures. We first propose a ten-dimensional metric to quantify thestructural characteristics of information cascades, reflecting cascade size,silhouette, direction and activity aspects. We find that bimodal law governsmajority of the metrics, information flows in cascades have four directions,and the self-loop number and average activity of cascades follows power law. Wethen analyze the high-order structural patterns of information cascades.Finally, we evaluate to what extent the structural features of informationcascades can explain its dynamic patterns and semantics, and finally uncoversome notable implications of structural patterns in information cascades. Ourdiscoveries also provide a foundation for the microscopic mechanisms forinformation spreading, potentially leading to implications for cascadeprediction and outlier detection.

LookOut on Time-Evolving Graphs: Succinctly Explaining Anomalies from  Any Detector

  Why is a given node in a time-evolving graph ($t$-graph) marked as an anomalyby an off-the-shelf detection algorithm? Is it because of the number of itsoutgoing or incoming edges, or their timings? How can we best convince a humananalyst that the node is anomalous? Our work aims to provide succinct,interpretable, and simple explanations of anomalous behavior in $t$-graphs(communications, IP-IP interactions, etc.) while respecting the limitedattention of human analysts. Specifically, we extract key features from suchgraphs, and propose to output a few pair (scatter) plots from this featurespace which "best" explain known anomalies. To this end, our work has four maincontributions: (a) problem formulation: we introduce an "analyst-friendly"problem formulation for explaining anomalies via pair plots, (b) explanationalgorithm: we propose a plot-selection objective and the LookOut algorithm toapproximate it with optimality guarantees, (c) generality: our explanationalgorithm is both domain- and detector-agnostic, and (d) scalability: we showthat LookOut scales linearly on the number of edges of the input graph. Ourexperiments show that LookOut performs near-ideally in terms of maximizingexplanation objective on several real datasets including Enron e-mail and DBLPcoauthorship. Furthermore, LookOut produces fast, visually interpretable andintuitive results in explaining "ground-truth" anomalies from Enron, DBLP andLBNL (computer network) data.

Out-of-Core and Distributed Algorithms for Dense Subtensor Mining

  How can we detect fraudulent lockstep behavior in large-scale multi-aspectdata (i.e., tensors)? Can we detect it when data are too large to fit in memoryor even on a disk? Past studies have shown that dense subtensors in real-worldtensors (e.g., social media, Wikipedia, TCP dumps, etc.) signal anomalous orfraudulent behavior such as retweet boosting, bot activities, and networkattacks. Thus, various approaches, including tensor decomposition and search,have been proposed for detecting dense subtensors rapidly and accurately.However, existing methods have low accuracy, or they assume that tensors aresmall enough to fit in main memory, which is unrealistic in many real-worldapplications such as social media and web. To overcome these limitations, wepropose D-CUBE, a disk-based dense-subtensor detection method, which also canrun in a distributed manner across multiple machines. Compared tostate-of-the-art methods, D-CUBE is (1) Memory Efficient: requires up to 1,600Xless memory and handles 1,000X larger data (2.6TB), (2) Fast: up to 7X fasterdue to its near-linear scalability, (3) Provably Accurate: gives a guarantee onthe densities of the detected subtensors, and (4) Effective: spotted networkattacks from TCP dumps and synchronized behavior in rating data mostaccurately.

Did We Get It Right? Predicting Query Performance in E-commerce Search

  In this paper, we address the problem of evaluating whether results served byan e-commerce search engine for a query are good or not. This is a criticalquestion in evaluating any e-commerce search engine. While this question istraditionally answered using simple metrics like query click-through rate(CTR), we observe that in e-commerce search, such metrics can be misleading.Upon inspection, we find cases where CTR is high but the results are poor andvice versa. Similar cases exist for other metrics like time to click which areoften also used for evaluating search engines. We aim to learn the quality ofthe results served by the search engine based on users' interactions with theresults. Although this problem has been studied in the web search context, thisis the first study for e-commerce search, to the best of our knowledge. Despitecertain commonalities with evaluating web search engines, there are severalmajor differences such as underlying reasons for search failure, andavailability of rich user interaction data with products (e.g. adding a productto the cart). We study large-scale user interaction logs from Flipkart's searchengine, analyze behavioral patterns and build models to classify queries basedon user behavior signals. We demonstrate the feasibility and efficacy of suchmodels in accurately predicting query performance. Our classifier is able toachieve an average AUC of 0.75 on a held-out test set.

Linearized and Single-Pass Belief Propagation

  How can we tell when accounts are fake or real in a social network? And howcan we tell which accounts belong to liberal, conservative or centrist users?Often, we can answer such questions and label nodes in a network based on thelabels of their neighbors and appropriate assumptions of homophily ("birds of afeather flock together") or heterophily ("opposites attract"). One of the mostwidely used methods for this kind of inference is Belief Propagation (BP) whichiteratively propagates the information from a few nodes with explicit labelsthroughout a network until convergence. One main problem with BP, however, isthat there are no known exact guarantees of convergence in graphs with loops.  This paper introduces Linearized Belief Propagation (LinBP), a linearizationof BP that allows a closed-form solution via intuitive matrix equations and,thus, comes with convergence guarantees. It handles homophily, heterophily, andmore general cases that arise in multi-class settings. Plus, it allows acompact implementation in SQL. The paper also introduces Single-pass BeliefPropagation (SBP), a "localized" version of LinBP that propagates informationacross every edge at most once and for which the final class assignments dependonly on the nearest labeled neighbors. In addition, SBP allows fast incrementalupdates in dynamic networks. Our runtime experiments show that LinBP and SBPare orders of magnitude faster than standard

