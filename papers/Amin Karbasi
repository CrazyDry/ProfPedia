Comparison-Based Learning with Rank Nets

  We consider the problem of search through comparisons, where a user ispresented with two candidate objects and reveals which is closer to herintended target. We study adaptive strategies for finding the target, thatrequire knowledge of rank relationships but not actual distances betweenobjects. We propose a new strategy based on rank nets, and show that for targetdistributions with a bounded doubling constant, it finds the target in a numberof comparisons close to the entropy of the target distribution and, hence, ofthe optimum. We extend these results to the case of noisy oracles, and comparethis strategy to prior art over multiple datasets.

Projection-Free Bandit Convex Optimization

  In this paper, we propose the first computationally efficient projection-freealgorithm for bandit convex optimization (BCO). We show that our algorithmachieves a sublinear regret of $O(nT^{4/5})$ (where $T$ is the horizon and $n$is the dimension) for any bounded convex functions with uniformly boundedgradients. We also evaluate the performance of our algorithm against baselineson both synthetic and real data sets for quadratic programming, portfolioselection and matrix completion problems.

Coupled Neural Associative Memories

  We propose a novel architecture to design a neural associative memory that iscapable of learning a large number of patterns and recalling them later inpresence of noise. It is based on dividing the neurons into local clusters andparallel plains, very similar to the architecture of the visual cortex ofmacaque brain. The common features of our proposed architecture with those ofspatially-coupled codes enable us to show that the performance of such networksin eliminating noise is drastically better than the previous approaches whilemaintaining the ability of learning an exponentially large number of patterns.Previous work either failed in providing good performance during the recallphase or in offering large pattern retrieval (storage) capacities. We alsopresent computational experiments that lend additional support to thetheoretical analysis.

Neural Networks Built from Unreliable Components

  Recent advances in associative memory design through strutured pattern setsand graph-based inference algorithms have allowed the reliable learning andretrieval of an exponential number of patterns. Both these and classicalassociative memories, however, have assumed internally noiseless computationalnodes. This paper considers the setting when internal computations are alsonoisy. Even if all components are noisy, the final error probability in recallcan often be made exceedingly small, as we characterize. There is a thresholdphenomenon. We also show how to optimize inference algorithm parameters whenknowing statistical properties of internal noise.

Noise Facilitation in Associative Memories of Exponential Capacity

  Recent advances in associative memory design through structured pattern setsand graph-based inference algorithms have allowed reliable learning and recallof an exponential number of patterns. Although these designs correct externalerrors in recall, they assume neurons that compute noiselessly, in contrast tothe highly variable neurons in brain regions thought to operate associativelysuch as hippocampus and olfactory cortex.  Here we consider associative memories with noisy internal computations andanalytically characterize performance. As long as the internal noise level isbelow a specified threshold, the error probability in the recall phase can bemade exceedingly small. More surprisingly, we show that internal noise actuallyimproves the performance of the recall phase while the pattern retrievalcapacity remains intact, i.e., the number of stored patterns does not reducewith noise (up to a threshold). Computational experiments lend additionalsupport to our theoretical analysis. This work suggests a functional benefit tonoisy neurons in biological neuronal networks.

From Small-World Networks to Comparison-Based Search

  The problem of content search through comparisons has recently receivedconsiderable attention. In short, a user searching for a target objectnavigates through a database in the following manner: the user is asked toselect the object most similar to her target from a small list of objects. Anew object list is then presented to the user based on her earlier selection.This process is repeated until the target is included in the list presented, atwhich point the search terminates. This problem is known to be strongly relatedto the small-world network design problem.  However, contrary to prior work, which focuses on cases where objects in thedatabase are equally popular, we consider here the case where the demand forobjects may be heterogeneous. We show that, under heterogeneous demand, thesmall-world network design problem is NP-hard. Given the above negative result,we propose a novel mechanism for small-world design and provide an upper boundon its performance under heterogeneous demand. The above mechanism has anatural equivalent in the context of content search through comparisons, and weestablish both an upper bound and a lower bound for the performance of thismechanism. These bounds are intuitively appealing, as they depend on theentropy of the demand as well as its doubling constant, a quantity capturingthe topology of the set of target objects. They also illustrate interestingconnections between comparison-based search to classic results from informationtheory. Finally, we propose an adaptive learning algorithm for content searchthat meets the performance guarantees achieved by the above mechanisms.

Multi-Level Error-Resilient Neural Networks with Learning

  The problem of neural network association is to retrieve a previouslymemorized pattern from its noisy version using a network of neurons. An idealneural network should include three components simultaneously: a learningalgorithm, a large pattern retrieval capacity and resilience against noise.Prior works in this area usually improve one or two aspects at the cost of thethird.  Our work takes a step forward in closing this gap. More specifically, we showthat by forcing natural constraints on the set of learning patterns, we candrastically improve the retrieval capacity of our neural network. Moreover, wedevise a learning algorithm whose role is to learn those patterns satisfyingthe above mentioned constraints. Finally we show that our neural network cancope with a fair amount of noise.

Near-Optimally Teaching the Crowd to Classify

  How should we present training examples to learners to teach themclassification rules? This is a natural problem when training workers forcrowdsourcing labeling tasks, and is also motivated by challenges indata-driven online education. We propose a natural stochastic model of thelearners, modeling them as randomly switching among hypotheses based onobserved feedback. We then develop STRICT, an efficient algorithm for selectingexamples to teach to workers. Our solution greedily maximizes a submodularsurrogate objective function in order to select examples to show to thelearners. We prove that our strategy is competitive with the optimal teachingpolicy. Moreover, for the special case of linear separators, we prove that anexponential reduction in error probability can be achieved. Our experiments onsimulated workers as well as three real image annotation tasks on AmazonMechanical Turk show the effectiveness of our teaching algorithm.

Near Optimal Bayesian Active Learning for Decision Making

  How should we gather information to make effective decisions? We addressBayesian active learning and experimental design problems, where wesequentially select tests to reduce uncertainty about a set of hypotheses.Instead of minimizing uncertainty per se, we consider a set of overlappingdecision regions of these hypotheses. Our goal is to drive uncertainty into asingle decision region as quickly as possible.  We identify necessary and sufficient conditions for correctly identifying adecision region that contains all hypotheses consistent with observations. Wedevelop a novel Hyperedge Cutting (HEC) algorithm for this problem, and provethat is competitive with the intractable optimal policy. Our efficientimplementation of the algorithm relies on computing subsets of the completehomogeneous symmetric polynomials. Finally, we demonstrate its effectiveness ontwo practical applications: approximate comparison-based learning and activelocalization using a robot manipulator.

Lazier Than Lazy Greedy

  Is it possible to maximize a monotone submodular function faster than thewidely used lazy greedy algorithm (also known as accelerated greedy), both intheory and practice? In this paper, we develop the first linear-time algorithmfor maximizing a general monotone submodular function subject to a cardinalityconstraint. We show that our randomized algorithm, STOCHASTIC-GREEDY, canachieve a $(1-1/e-\varepsilon)$ approximation guarantee, in expectation, to theoptimum solution in time linear in the size of the data and independent of thecardinality constraint. We empirically demonstrate the effectiveness of ouralgorithm on submodular functions arising in data summarization, includingtraining large-scale kernel methods, exemplar-based clustering, and sensorplacement. We observe that STOCHASTIC-GREEDY practically achieves the sameutility value as lazy greedy but runs much faster. More surprisingly, weobserve that in many practical scenarios STOCHASTIC-GREEDY does not evaluatethe whole fraction of data points even once and still achievesindistinguishable results compared to lazy greedy.

Distributed Submodular Maximization

  Many large-scale machine learning problems--clustering, non-parametriclearning, kernel machines, etc.--require selecting a small yet representativesubset from a large dataset. Such problems can often be reduced to maximizing asubmodular set function subject to various constraints. Classical approaches tosubmodular optimization require centralized access to the full dataset, whichis impractical for truly large-scale problems. In this paper, we consider theproblem of submodular function maximization in a distributed fashion. Wedevelop a simple, two-stage protocol GreeDi, that is easily implemented usingMapReduce style computations. We theoretically analyze our approach, and showthat under certain natural conditions, performance close to the centralizedapproach can be achieved. We begin with monotone submodular maximizationsubject to a cardinality constraint, and then extend this approach to obtainapproximation guarantees for (not necessarily monotone) submodular maximizationsubject to more general constraints including matroid or knapsack constraints.In our extensive experiments, we demonstrate the effectiveness of our approachon several applications, including sparse Gaussian process inference andexemplar based clustering on tens of millions of examples using Hadoop.

Fast Mixing for Discrete Point Processes

  We investigate the systematic mechanism for designing fast mixing Markovchain Monte Carlo algorithms to sample from discrete point processes under theDobrushin uniqueness condition for Gibbs measures. Discrete point processes aredefined as probability distributions $\mu(S)\propto \exp(\beta f(S))$ over allsubsets $S\in 2^V$ of a finite set $V$ through a bounded set function$f:2^V\rightarrow \mathbb{R}$ and a parameter $\beta>0$. A subclass of discretepoint processes characterized by submodular functions (which includelog-submodular distributions, submodular point processes, and determinantalpoint processes) has recently gained a lot of interest in machine learning andshown to be effective for modeling diversity and coverage. We show that if theset function (not necessarily submodular) displays a natural notion of decay ofcorrelation, then, for $\beta$ small enough, it is possible to design fastmixing Markov chain Monte Carlo methods that yield error bounds on marginalapproximations that do not depend on the size of the set $V$. The sufficientconditions that we derive involve a control on the (discrete) Hessian of setfunctions, a quantity that has not been previously considered in theliterature. We specialize our results for submodular functions, and we discusscanonical examples where the Hessian can be easily controlled.

Tradeoffs for Space, Time, Data and Risk in Unsupervised Learning

  Faced with massive data, is it possible to trade off (statistical) risk, and(computational) space and time? This challenge lies at the heart of large-scalemachine learning. Using k-means clustering as a prototypical unsupervisedlearning problem, we show how we can strategically summarize the data (controlspace) in order to trade off risk and time when data is generated by aprobabilistic model. Our summarization is based on coreset constructions fromcomputational geometry. We also develop an algorithm, TRAM, to navigate thespace/time/data/risk tradeoff in practice. In particular, we show that for afixed risk (or data size), as the data size increases (resp. risk increases)the running time of TRAM decreases. Our extensive experiments on real data setsdemonstrate the existence and practical utility of such tradeoffs, not only fork-means but also for Gaussian Mixture Models.

Streaming Weak Submodularity: Interpreting Neural Networks on the Fly

  In many machine learning applications, it is important to explain thepredictions of a black-box classifier. For example, why does a deep neuralnetwork assign an image to a particular class? We cast interpretability ofblack-box classifiers as a combinatorial maximization problem and propose anefficient streaming algorithm to solve it subject to cardinality constraints.By extending ideas from Badanidiyuru et al. [2014], we provide a constantfactor approximation guarantee for our algorithm in the case of random streamorder and a weakly submodular objective function. This is the first suchtheoretical guarantee for this general class of functions, and we also showthat no such algorithm exists for a worst case stream order. Our algorithmobtains similar explanations of Inception V3 predictions $10$ times faster thanthe state-of-the-art LIME framework of Ribeiro et al. [2016].

Conditional Gradient Method for Stochastic Submodular Maximization:  Closing the Gap

  In this paper, we study the problem of \textit{constrained} and\textit{stochastic} continuous submodular maximization. Even though theobjective function is not concave (nor convex) and is defined in terms of anexpectation, we develop a variant of the conditional gradient method, called\alg, which achieves a \textit{tight} approximation guarantee. More precisely,for a monotone and continuous DR-submodular function and subject to a\textit{general} convex body constraint, we prove that \alg achieves a$[(1-1/e)\text{OPT} -\eps]$ guarantee (in expectation) with$\mathcal{O}{(1/\eps^3)}$ stochastic gradient computations. This guaranteematches the known hardness results and closes the gap between deterministic andstochastic continuous submodular maximization. By using stochastic continuousoptimization as an interface, we also provide the first $(1-1/e)$ tightapproximation guarantee for maximizing a \textit{monotone but stochastic}submodular \textit{set} function subject to a general matroid constraint.

Deletion-Robust Submodular Maximization at Scale

  Can we efficiently extract useful information from a large user-generateddataset while protecting the privacy of the users and/or ensuring fairness inrepresentation. We cast this problem as an instance of a deletion-robustsubmodular maximization where part of the data may be deleted due to privacyconcerns or fairness criteria. We propose the first memory-efficientcentralized, streaming, and distributed methods with constant-factorapproximation guarantees against any number of adversarial deletions. Weextensively evaluate the performance of our algorithms against priorstate-of-the-art on real-world applications, including (i) Uber-pick uplocations with location privacy constraints; (ii) feature selection withfairness constraints for income prediction and crime rate prediction; and (iii)robust to deletion summarization of census data, consisting of 2,458,285feature vectors.

Decentralized Submodular Maximization: Bridging Discrete and Continuous  Settings

  In this paper, we showcase the interplay between discrete and continuousoptimization in network-structured settings. We propose the first fullydecentralized optimization method for a wide class of non-convex objectivefunctions that possess a diminishing returns property. More specifically, givenan arbitrary connected network and a global continuous submodular function,formed by a sum of local functions, we develop Decentralized Continuous Greedy(DCG), a message passing algorithm that converges to the tight (1-1/e)approximation factor of the optimum global solution using only localcomputation and communication. We also provide strong convergence bounds as afunction of network size and spectral characteristics of the underlyingtopology. Interestingly, DCG readily provides a simple recipe for decentralizeddiscrete submodular maximization through the means of continuous relaxations.Formally, we demonstrate that by lifting the local discrete functions tocontinuous domains and using DCG as an interface we can develop a consensusalgorithm that also achieves the tight (1-1/e) approximation guarantee of theglobal discrete solution once a proper rounding scheme is applied.

Online Continuous Submodular Maximization

  In this paper, we consider an online optimization process, where theobjective functions are not convex (nor concave) but instead belong to a broadclass of continuous submodular functions. We first propose a variant of theFrank-Wolfe algorithm that has access to the full gradient of the objectivefunctions. We show that it achieves a regret bound of $O(\sqrt{T})$ (where $T$is the horizon of the online optimization problem) against a$(1-1/e)$-approximation to the best feasible solution in hindsight. However, inmany scenarios, only an unbiased estimate of the gradients are available. Forsuch settings, we then propose an online stochastic gradient ascent algorithmthat also achieves a regret bound of $O(\sqrt{T})$ regret, albeit against aweaker $1/2$-approximation to the best feasible solution in hindsight. We alsogeneralize our results to $\gamma$-weakly submodular functions and prove thesame sublinear regret bounds. Finally, we demonstrate the efficiency of ouralgorithms on a few problem instances, including non-convex/non-concavequadratic programs, multilinear extensions of submodular set functions, andD-optimal design.

Do Less, Get More: Streaming Submodular Maximization with Subsampling

  In this paper, we develop the first one-pass streaming algorithm forsubmodular maximization that does not evaluate the entire stream even once. Bycarefully subsampling each element of data stream, our algorithm enjoys thetightest approximation guarantees in various settings while having the smallestmemory footprint and requiring the lowest number of function evaluations. Morespecifically, for a monotone submodular function and a $p$-matchoid constraint,our randomized algorithm achieves a $4p$ approximation ratio (in expectation)with $O(k)$ memory and $O(km/p)$ queries per element ($k$ is the size of thelargest feasible solution and $m$ is the number of matroids used to define theconstraint). For the non-monotone case, our approximation ratio increases onlyslightly to $4p+2-o(1)$. To the best or our knowledge, our algorithm is thefirst that combines the benefits of streaming and subsampling in a novel way inorder to truly scale submodular maximization to massive machine learningproblems. To showcase its practicality, we empirically evaluated theperformance of our algorithm on a video summarization application and observedthat it outperforms the state-of-the-art algorithm by up to fifty fold, whilemaintaining practically the same utility.

Submodularity on Hypergraphs: From Sets to Sequences

  In a nutshell, submodular functions encode an intuitive notion of diminishingreturns. As a result, submodularity appears in many important machine learningtasks such as feature selection and data summarization. Although there has beena large volume of work devoted to the study of submodular functions in recentyears, the vast majority of this work has been focused on algorithms thatoutput sets, not sequences. However, in many settings, the order in which weoutput items can be just as important as the items themselves.  To extend the notion of submodularity to sequences, we use a directed graphon the items where the edges encode the additional value of selecting items ina particular order. Existing theory is limited to the case where thisunderlying graph is a directed acyclic graph. In this paper, we introduce twonew algorithms that provably give constant factor approximations for generalgraphs and hypergraphs having bounded in or out degrees. Furthermore, we showthe utility of our new algorithms for real-world applications in movierecommendation, online link prediction, and the design of course sequences forMOOCs.

Data Summarization at Scale: A Two-Stage Submodular Approach

  The sheer scale of modern datasets has resulted in a dire need forsummarization techniques that identify representative elements in a dataset.Fortunately, the vast majority of data summarization tasks satisfy an intuitivediminishing returns condition known as submodularity, which allows us to findnearly-optimal solutions in linear time. We focus on a two-stage submodularframework where the goal is to use some given training functions to reduce theground set so that optimizing new functions (drawn from the same distribution)over the reduced set provides almost as much value as optimizing them over theentire ground set. In this paper, we develop the first streaming anddistributed solutions to this problem. In addition to providing strongtheoretical guarantees, we demonstrate both the utility and efficiency of ouralgorithms on real-world tasks including image summarization and ride-shareoptimization.

Eliminating Latent Discrimination: Train Then Mask

  How can we control for latent discrimination in predictive models? How can weprovably remove it? Such questions are at the heart of algorithmic fairness andits impacts on society. In this paper, we define a new operational fairnesscriteria, inspired by the well-understood notion of omitted variable-bias instatistics and econometrics. Our notion of fairness effectively controls forsensitive features and provides diagnostics for deviations from fair decisionmaking. We then establish analytical and algorithmic results about theexistence of a fair classifier in the context of supervised learning. Ourresults readily imply a simple, but rather counter-intuitive, strategy foreliminating latent discrimination. In order to prevent other features proxyingfor sensitive features, we need to include sensitive features in the trainingphase, but exclude them in the test/evaluation phase while controlling fortheir effects. We evaluate the performance of our algorithm on severalreal-world datasets and show how fairness for these datasets can be improvedwith a very small loss in accuracy.

Unconstrained Submodular Maximization with Constant Adaptive Complexity

  In this paper, we consider the unconstrained submodular maximization problem.We propose the first algorithm for this problem that achieves a tight$(1/2-\varepsilon)$-approximation guarantee using $\tilde{O}(\varepsilon^{-1})$adaptive rounds and a linear number of function evaluations. No previouslyknown algorithm for this problem achieves an approximation ratio better than$1/3$ using less than $\Omega(n)$ rounds of adaptivity, where $n$ is the sizeof the ground set. Moreover, our algorithm easily extends to the maximizationof a non-negative continuous DR-submodular function subject to a box constraintand achieves a tight $(1/2-\varepsilon)$-approximation guarantee for thisproblem while keeping the same adaptive and query complexities.

Black Box Submodular Maximization: Discrete and Continuous Settings

  In this paper, we consider the problem of black box continuous submodularmaximization where we only have access to the function values and noinformation about the derivatives is provided. For a monotone and continuousDR-submodular function, and subject to a bounded convex body constraint, wepropose Black-box Continuous Greedy, a derivative-free algorithm that provablyachieves the tight $[(1-1/e)OPT-\epsilon]$ approximation guarantee with$O(d/\epsilon^3)$ function evaluations. We then extend our result to thestochastic setting where function values are subject to stochastic zero-meannoise. It is through this stochastic generalization that we revisit thediscrete submodular maximization problem and use the multi-linear extension asa bridge between discrete and continuous settings. Finally, we extensivelyevaluate the performance of our algorithm on continuous and discrete submodularobjective functions using both synthetic and real data.

Adaptive Sequence Submodularity

  In many machine learning applications, one needs to interactively select asequence of items (e.g., recommending movies based on a user's feedback) ormake sequential decisions in certain orders (e.g., guiding an agent through aseries of states). Not only do sequences already pose a dauntingly large searchspace, but we must take into account past observations, as well as theuncertainty of future outcomes. Without further structure, finding an optimalsequence is notoriously challenging, if not completely intractable.  In this paper, we introduce adaptive sequence submodularity, a rich frameworkthat generalizes the notion of submodularity to adaptive policies thatexplicitly consider sequential dependencies between items. We show that oncesuch dependencies are encoded by a directed graph, an adaptive greedy policy isguaranteed to achieve a constant factor approximation guarantee, where theconstant naturally depends on the structural properties of the underlyinggraph. Additionally, to demonstrate the practical utility of our results, werun experiments on Amazon product recommendation and Wikipedia link predictiontasks.

Quantized Frank-Wolfe: Communication-Efficient Distributed Optimization

  How can we efficiently mitigate the overhead of gradient communications indistributed optimization? This problem is at the heart of training scalablemachine learning models and has been mainly studied in the unconstrainedsetting. In this paper, we propose Quantized Frank-Wolfe (QFW), the firstprojection-free and communication-efficient algorithm for solving constrainedoptimization problems at scale. We consider both convex and non-convexobjective functions, expressed as a finite-sum or more generally a stochasticoptimization problem, and provide strong theoretical guarantees on theconvergence rate of QFW. This is done by proposing quantization schemes thatefficiently compress gradients while controlling the variance introduced duringthis process. Finally, we empirically validate the efficiency of QFW in termsof communication and the quality of returned solution against naturalbaselines.

Stochastic Conditional Gradient++

  In this paper, we develop Stochastic Continuous Greedy++ (SCG++), the firstefficient variant of a conditional gradient method for maximizing a continuoussubmodular function subject to a convex constraint. Concretely, for a monotoneand continuous DR-submodular function, SCG++ achieves a tight$[(1-1/e)\text{OPT} -\epsilon]$ solution while using $O(1/\epsilon^2)$stochastic oracle queries and $O(1/\epsilon)$ calls to the linear optimizationoracle. The best previously known algorithms either achieve a suboptimal$[(1/2)\text{OPT} -\epsilon]$ solution with $O(1/\epsilon^2)$ stochasticgradients or the tight $[(1-1/e)\text{OPT} -\epsilon]$ solution with suboptimal$O(1/\epsilon^3)$ stochastic gradients. SCG++ enjoys optimality in terms ofboth approximation guarantee and stochastic stochastic oracle queries. Ournovel variance reduction method naturally extends to stochastic convexminimization. More precisely, we develop Stochastic Frank-Wolfe++ (SFW++) thatachieves an $\epsilon$-approximate optimum with only $O(1/\epsilon)$ calls tothe linear optimization oracle while using $O(1/\epsilon^2)$ stochastic oraclequeries in total. Therefore, SFW++ is the first efficient projection-freealgorithm that achieves the optimum complexity $O(1/\epsilon^2)$ in terms ofstochastic oracle queries.

Convolutional Neural Associative Memories: Massive Capacity with Noise  Tolerance

  The task of a neural associative memory is to retrieve a set of previouslymemorized patterns from their noisy versions using a network of neurons. Anideal network should have the ability to 1) learn a set of patterns as theyarrive, 2) retrieve the correct patterns from noisy queries, and 3) maximizethe pattern retrieval capacity while maintaining the reliability in respondingto queries. The majority of work on neural associative memories has focused ondesigning networks capable of memorizing any set of randomly chosen patterns atthe expense of limiting the retrieval capacity. In this paper, we show that ifwe target memorizing only those patterns that have inherent redundancy (i.e.,belong to a subspace), we can obtain all the aforementioned properties. This isin sharp contrast with the previous work that could only improve one or twoaspects at the expense of the third. More specifically, we propose frameworkbased on a convolutional neural network along with an iterative algorithm thatlearns the redundancy among the patterns. The resulting network has a retrievalcapacity that is exponential in the size of the network. Moreover, theasymptotic error correction performance of our network is linear in the size ofthe patterns. We then ex- tend our approach to deal with patterns lieapproximately in a subspace. This extension allows us to memorize datasetscontaining natural patterns (e.g., images). Finally, we report experimentalresults on both synthetic and real datasets to support our claims.

An Estimation Theoretic Approach for Sparsity Pattern Recovery in the  Noisy Setting

  Compressed sensing deals with the reconstruction of sparse signals using asmall number of linear measurements. One of the main challenges in compressedsensing is to find the support of a sparse signal. In the literature, severalbounds on the scaling law of the number of measurements for successful supportrecovery have been derived where the main focus is on random Gaussianmeasurement matrices. In this paper, we investigate the noisy support recoveryproblem from an estimation theoretic point of view, where no specificassumption is made on the underlying measurement matrix. The linearmeasurements are perturbed by additive white Gaussian noise. We define theoutput of a support estimator to be a set of position values in increasingorder. We set the error between the true and estimated supports as the$\ell_2$-norm of their difference. On the one hand, this choice allows us touse the machinery behind the $\ell_2$-norm error metric and on the other hand,converts the support recovery into a more intuitive and geometrical problem.First, by using the Hammersley-Chapman-Robbins (HCR) bound, we derive afundamental lower bound on the performance of any \emph{unbiased} estimator ofthe support set. This lower bound provides us with necessary conditions on thenumber of measurements for reliable $\ell_2$-norm support recovery, which wespecifically evaluate for uniform Gaussian measurement matrices. Then, weanalyze the maximum likelihood estimator and derive conditions under which theHCR bound is achievable. This leads us to the number of measurements for theoptimum decoder which is sufficient for reliable $\ell_2$-norm supportrecovery. Using this framework, we specifically evaluate sufficient conditionsfor uniform Gaussian measurement matrices.

Calibration Using Matrix Completion with Application to Ultrasound  Tomography

  We study the calibration process in circular ultrasound tomography deviceswhere the sensor positions deviate from the circumference of a perfect circle.This problem arises in a variety of applications in signal processing rangingfrom breast imaging to sensor network localization. We introduce a novel methodof calibration/localization based on the time-of-flight (ToF) measurementsbetween sensors when the enclosed medium is homogeneous. In the presence of allthe pairwise ToFs, one can easily estimate the sensor positions usingmulti-dimensional scaling (MDS) method. In practice however, due to thetransitional behaviour of the sensors and the beam form of the transducers, theToF measurements for close-by sensors are unavailable. Further, randommalfunctioning of the sensors leads to random missing ToF measurements. On topof the missing entries, in practice an unknown time delay is also added to themeasurements. In this work, we incorporate the fact that a matrix defined fromall the ToF measurements is of rank at most four. In order to estimate themissing ToFs, we apply a state-of-the-art low-rank matrix completion algorithm,OPTSPACE . To find the correct positions of the sensors (our ultimate goal) wethen apply MDS. We show analytic bounds on the overall error of the wholeprocess in the presence of noise and hence deduce its robustness. Finally, weconfirm the functionality of our method in practice by simulations mimickingthe measurements of a circular ultrasound tomography device.

Robust Localization from Incomplete Local Information

  We consider the problem of localizing wireless devices in an ad-hoc networkembedded in a d-dimensional Euclidean space. Obtaining a good estimation ofwhere wireless devices are located is crucial in wireless network applicationsincluding environment monitoring, geographic routing and topology control. Whenthe positions of the devices are unknown and only local distance information isgiven, we need to infer the positions from these local distance measurements.This problem is particularly challenging when we only have access tomeasurements that have limited accuracy and are incomplete. We consider theextreme case of this limitation on the available information, namely only theconnectivity information is available, i.e., we only know whether a pair ofnodes is within a fixed detection range of each other or not, and noinformation is known about how far apart they are. Further, to account fordetection failures, we assume that even if a pair of devices is within thedetection range, it fails to detect the presence of one another with someprobability and this probability of failure depends on how far apart thosedevices are. Given this limited information, we investigate the performance ofa centralized positioning algorithm MDS-MAP introduced by Shang et al., and adistributed positioning algorithm, introduced by Savarese et al., calledHOP-TERRAIN. In particular, for a network consisting of n devices positionedrandomly, we provide a bound on the resulting error for both algorithms. Weshow that the error is bounded, decreasing at a rate that is proportional toR/Rc, where Rc is the critical detection range when the resulting randomnetwork starts to be connected, and R is the detection range of each device.

Seeing the Unseen Network: Inferring Hidden Social Ties from  Respondent-Driven Sampling

  Learning about the social structure of hidden and hard-to-reach populations--- such as drug users and sex workers --- is a major goal of epidemiologicaland public health research on risk behaviors and disease prevention.Respondent-driven sampling (RDS) is a peer-referral process widely used by manyhealth organizations, where research subjects recruit other subjects from theirsocial network. In such surveys, researchers observe who recruited whom, alongwith the time of recruitment and the total number of acquaintances (networkdegree) of respondents. However, due to privacy concerns, the identities ofacquaintances are not disclosed. In this work, we show how to reconstruct theunderlying network structure through which the subjects are recruited. Weformulate the dynamics of RDS as a continuous-time diffusion process over theunderlying graph and derive the likelihood for the recruitment time seriesunder an arbitrary recruitment time distribution. We develop an efficientstochastic optimization algorithm called RENDER (REspoNdent-Driven nEtworkReconstruction) that finds the network that best explains the collected data.We support our analytical results through an exhaustive set of experiments onboth synthetic and real data.

Near-Optimal Active Learning of Halfspaces via Query Synthesis in the  Noisy Setting

  In this paper, we consider the problem of actively learning a linearclassifier through query synthesis where the learner can construct artificialqueries in order to estimate the true decision boundaries. This problem hasrecently gained a lot of interest in automated science and adversarial reverseengineering for which only heuristic algorithms are known. In suchapplications, queries can be constructed de novo to elicit information (e.g.,automated science) or to evade detection with minimal cost (e.g., adversarialreverse engineering). We develop a general framework, called dimension coupling(DC), that 1) reduces a d-dimensional learning problem to d-1 low dimensionalsub-problems, 2) solves each sub-problem efficiently, 3) appropriatelyaggregates the results and outputs a linear classifier, and 4) provides atheoretical guarantee for all possible schemes of aggregation. The proposedmethod is proved resilient to noise. We show that the DC framework avoids thecurse of dimensionality: its computational complexity scales linearly with thedimension. Moreover, we show that the query complexity of DC is near optimal(within a constant factor of the optimum algorithm). To further support ourtheoretical analysis, we compare the performance of DC with the existing work.We observe that DC consistently outperforms the prior arts in terms of querycomplexity while often running orders of magnitude faster.

Submodular Variational Inference for Network Reconstruction

  In real-world and online social networks, individuals receive and transmitinformation in real time. Cascading information transmissions (e.g. phonecalls, text messages, social media posts) may be understood as a realization ofa diffusion process operating on the network, and its branching path can berepresented by a directed tree. The process only traverses and thus reveals alimited portion of the edges. The network reconstruction/inference problem isto infer the unrevealed connections. Most existing approaches derive alikelihood and attempt to find the network topology maximizing the likelihood,a problem that is highly intractable. In this paper, we focus on the networkreconstruction problem for a broad class of real-world diffusion processes,exemplified by a network diffusion scheme called respondent-driven sampling(RDS). We prove that under realistic and general models of network diffusion,the posterior distribution of an observed RDS realization is a Bayesianlog-submodular model.We then propose VINE (Variational Inference for NetworkrEconstruction), a novel, accurate, and computationally efficient variationalinference algorithm, for the network reconstruction problem under this model.Crucially, we do not assume any particular probabilistic model for theunderlying network. VINE recovers any connected graph with high accuracy asshown by our experimental results on real-life networks.

Estimating the Size of a Large Network and its Communities from a Random  Sample

  Most real-world networks are too large to be measured or studied directly andthere is substantial interest in estimating global network properties fromsmaller sub-samples. One of the most important global properties is the numberof vertices/nodes in the network. Estimating the number of vertices in a largenetwork is a major challenge in computer science, epidemiology, demography, andintelligence analysis. In this paper we consider a population random graph G =(V;E) from the stochastic block model (SBM) with K communities/blocks. A sampleis obtained by randomly choosing a subset W and letting G(W) be the inducedsubgraph in G of the vertices in W. In addition to G(W), we observe the totaldegree of each sampled vertex and its block membership. Given this partialinformation, we propose an efficient PopULation Size Estimation algorithm,called PULSE, that correctly estimates the size of the whole population as wellas the size of each community. To support our theoretical analysis, we performan exhaustive set of experiments to study the effects of sample size, K, andSBM model parameters on the accuracy of the estimates. The experimental resultsalso demonstrate that PULSE significantly outperforms a widely-used methodcalled the network scale-up estimator in a wide variety of scenarios. Weconclude with extensions and directions for future work.

Greed is Good: Near-Optimal Submodular Maximization via Greedy  Optimization

  It is known that greedy methods perform well for maximizing monotonesubmodular functions. At the same time, such methods perform poorly in the faceof non-monotonicity. In this paper, we show - arguably, surprisingly - thatinvoking the classical greedy algorithm $O(\sqrt{k})$-times leads to the(currently) fastest deterministic algorithm, called Repeated Greedy, formaximizing a general submodular function subject to $k$-independent systemconstraints. Repeated Greedy achieves $(1 + O(1/\sqrt{k}))k$ approximationusing $O(nr\sqrt{k})$ function evaluations (here, $n$ and $r$ denote the sizeof the ground set and the maximum size of a feasible solution, respectively).We then show that by a careful sampling procedure, we can run the greedyalgorithm only once and obtain the (currently) fastest randomized algorithm,called Sample Greedy, for maximizing a submodular function subject to$k$-extendible system constraints (a subclass of $k$-independent systemconstrains). Sample Greedy achieves $(k + 3)$-approximation with only $O(nr/k)$function evaluations. Finally, we derive an almost matching lower bound, andshow that no polynomial time algorithm can have an approximation ratio smallerthan $ k + 1/2 - \varepsilon$. To further support our theoretical results, wecompare the performance of Repeated Greedy and Sample Greedy with prior art ina concrete application (movie recommendation). We consistently observe thatwhile Sample Greedy achieves practically the same utility as the best baseline,it performs at least two orders of magnitude faster.

Gradient Methods for Submodular Maximization

  In this paper, we study the problem of maximizing continuous submodularfunctions that naturally arise in many learning applications such as thoseinvolving utility functions in active learning and sensing, matrixapproximations and network inference. Despite the apparent lack of convexity insuch functions, we prove that stochastic projected gradient methods can providestrong approximation guarantees for maximizing continuous submodular functionswith convex constraints. More specifically, we prove that for monotonecontinuous DR-submodular functions, all fixed points of projected gradientascent provide a factor $1/2$ approximation to the global maxima. We also studystochastic gradient and mirror methods and show that after$\mathcal{O}(1/\epsilon^2)$ iterations these methods reach solutions whichachieve in expectation objective values exceeding$(\frac{\text{OPT}}{2}-\epsilon)$. An immediate application of our results isto maximize submodular functions that are defined stochastically, i.e. thesubmodular function is defined as an expectation over a family of submodularfunctions with an unknown distribution. We will show how stochastic gradientmethods are naturally well-suited for this setting, leading to a factor $1/2$approximation when the function is monotone. In particular, it allows us toapproximately maximize discrete, monotone submodular optimization problems viaprojected gradient descent on a continuous relaxation, directly connecting thediscrete and continuous domains. Finally, experiments on real data demonstratethat our projected gradient methods consistently achieve the best utilitycompared to other continuous baselines while remaining competitive in terms ofcomputational effort.

Comparison Based Learning from Weak Oracles

  There is increasing interest in learning algorithms that involve interactionbetween human and machine. Comparison-based queries are among the most naturalways to get feedback from humans. A challenge in designing comparison-basedinteractive learning algorithms is coping with noisy answers. The most commonfix is to submit a query several times, but this is not applicable in manysituations due to its prohibitive cost and due to the unrealistic assumption ofindependent noise in different repetitions of the same query.  In this paper, we introduce a new weak oracle model, where a non-malicioususer responds to a pairwise comparison query only when she is quite sure aboutthe answer. This model is able to mimic the behavior of a human in noise-proneregions. We also consider the application of this weak oracle model to theproblem of content search (a variant of the nearest neighbor search problem)through comparisons. More specifically, we aim at devising efficient algorithmsto locate a target object in a database equipped with a dissimilarity metricvia invocation of the weak comparison oracle. We propose two algorithms termedWORCS-I and WORCS-II (Weak-Oracle Comparison-based Search), which provablylocate the target object in a number of comparisons close to the entropy of thetarget distribution. While WORCS-I provides better theoretical guarantees,WORCS-II is applicable to more technically challenging scenarios where thealgorithm has limited access to the ranking dissimilarity between objects. Aseries of experiments validate the performance of our proposed algorithms.

Projection-Free Online Optimization with Stochastic Gradient: From  Convexity to Submodularity

  Online optimization has been a successful framework for solving large-scaleproblems under computational constraints and partial information. Currentmethods for online convex optimization require either a projection or exactgradient computation at each step, both of which can be prohibitively expensivefor large-scale applications. At the same time, there is a growing trend ofnon-convex optimization in machine learning community and a need for onlinemethods. Continuous DR-submodular functions, which exhibit a naturaldiminishing returns condition, have recently been proposed as a broad class ofnon-convex functions which may be efficiently optimized. Although onlinemethods have been introduced, they suffer from similar problems. In this work,we propose Meta-Frank-Wolfe, the first online projection-free algorithm thatuses stochastic gradient estimates. The algorithm relies on a careful samplingof gradients in each round and achieves the optimal $O( \sqrt{T})$ adversarialregret bounds for convex and continuous submodular optimization. We alsopropose One-Shot Frank-Wolfe, a simpler algorithm which requires only a singlestochastic gradient estimate in each round and achieves an $O(T^{2/3})$stochastic regret bound for convex and continuous submodular optimization. Weapply our methods to develop a novel "lifting" framework for the onlinediscrete submodular maximization and also see that they outperform currentstate-of-the-art techniques on various experiments.

Compressed Sensing with Probabilistic Measurements: A Group Testing  Solution

  Detection of defective members of large populations has been widely studiedin the statistics community under the name "group testing", a problem whichdates back to World War II when it was suggested for syphilis screening. Therethe main interest is to identify a small number of infected people among alarge population using collective samples. In viral epidemics, one way toacquire collective samples is by sending agents inside the population. While inclassical group testing, it is assumed that the sampling procedure is fullyknown to the reconstruction algorithm, in this work we assume that the decoderpossesses only partial knowledge about the sampling process. This assumption isjustified by observing the fact that in a viral sickness, there is a chancethat an agent remains healthy despite having contact with an infected person.Therefore, the reconstruction method has to cope with two different types ofuncertainty; namely, identification of the infected population and thepartially unknown sampling procedure.  In this work, by using a natural probabilistic model for "viral infections",we design non-adaptive sampling procedures that allow successful identificationof the infected population with overwhelming probability 1-o(1). We proposeboth probabilistic and explicit design procedures that require a "small" numberof agents to single out the infected individuals. More precisely, for acontamination probability p, the number of agents required by the probabilisticand explicit designs for identification of up to k infected members is boundedby m = O(k^2 (log n)/p^2) and m = O(k^2 (log n)^2 /p^2), respectively. In bothcases, a simple decoder is able to successfully identify the infectedpopulation in time O(mn).

Graph-Constrained Group Testing

  Non-adaptive group testing involves grouping arbitrary subsets of $n$ itemsinto different pools. Each pool is then tested and defective items areidentified. A fundamental question involves minimizing the number of poolsrequired to identify at most $d$ defective items. Motivated by applications innetwork tomography, sensor networks and infection propagation, a variation ofgroup testing problems on graphs is formulated. Unlike conventional grouptesting problems, each group here must conform to the constraints imposed by agraph. For instance, items can be associated with vertices and each pool is anyset of nodes that must be path connected. In this paper, a test is associatedwith a random walk. In this context, conventional group testing corresponds tothe special case of a complete graph on $n$ vertices.  For interesting classes of graphs a rather surprising result is obtained,namely, that the number of tests required to identify $d$ defective items issubstantially similar to what is required in conventional group testingproblems, where no such constraints on pooling is imposed. Specifically, ifT(n) corresponds to the mixing time of the graph $G$, it is shown that with$m=O(d^2T^2(n)\log(n/d))$ non-adaptive tests, one can identify the defectiveitems. Consequently, for the Erdos-Renyi random graph $G(n,p)$, as well asexpander graphs with constant spectral gap, it follows that $m=O(d^2\log^3n)$non-adaptive tests are sufficient to identify $d$ defective items. Next, aspecific scenario is considered that arises in network tomography, for which itis shown that $m=O(d^3\log^3n)$ non-adaptive tests are sufficient to identify$d$ defective items. Noisy counterparts of the graph constrained group testingproblem are considered, for which parallel results are developed. We alsobriefly discuss extensions to compressive sensing on graphs.

Group Testing with Probabilistic Tests: Theory, Design and Application

  Identification of defective members of large populations has been widelystudied in the statistics community under the name of group testing. Itinvolves grouping subsets of items into different pools and detecting defectivemembers based on the set of test results obtained for each pool.  In a classical noiseless group testing setup, it is assumed that the samplingprocedure is fully known to the reconstruction algorithm, in the sense that theexistence of a defective member in a pool results in the test outcome of thatpool to be positive. However, this may not be always a valid assumption in somecases of interest. In particular, we consider the case where the defectiveitems in a pool can become independently inactive with a certain probability.Hence, one may obtain a negative test result in a pool despite containing somedefective items. As a result, any sampling and reconstruction method should beable to cope with two different types of uncertainty, i.e., the unknown set ofdefective items and the partially unknown, probabilistic testing procedure.  In this work, motivated by the application of detecting infected people inviral epidemics, we design non-adaptive sampling procedures that allowsuccessful identification of the defective items through a set of probabilistictests. Our design requires only a small number of tests to single out thedefective items. In particular, for a population of size $N$ and at most $K$defective items with activation probability $p$, our results show that $M =O(K^2\log{(N/K)}/p^3)$ tests is sufficient if the sampling procedure shouldwork for all possible sets of defective items, while $M = O(K\log{(N)}/p^3)$tests is enough to be successful for any single set of defective items.Moreover, we show that the defective members can be recovered using a simplereconstruction algorithm with complexity of $O(MN)$.

Weakly Submodular Maximization Beyond Cardinality Constraints: Does  Randomization Help Greedy?

  Submodular functions are a broad class of set functions, which naturallyarise in diverse areas. Many algorithms have been suggested for themaximization of these functions. Unfortunately, once the function deviates fromsubmodularity, the known algorithms may perform arbitrarily poorly. Amendingthis issue, by obtaining approximation results for set functions generalizingsubmodular functions, has been the focus of recent works.  One such class, known as weakly submodular functions, has received a lot ofattention. A key result proved by Das and Kempe (2011) showed that theapproximation ratio of the greedy algorithm for weakly submodular maximizationsubject to a cardinality constraint degrades smoothly with the distance fromsubmodularity. However, no results have been obtained for maximization subjectto constraints beyond cardinality. In particular, it is not known whether thegreedy algorithm achieves any non-trivial approximation ratio for suchconstraints.  In this paper, we prove that a randomized version of the greedy algorithm(previously used by Buchbinder et al. (2014) for a different problem) achievesan approximation ratio of $(1 + 1/\gamma)^{-2}$ for the maximization of aweakly submodular function subject to a general matroid constraint, where$\gamma$ is a parameter measuring the distance of the function fromsubmodularity. Moreover, we also experimentally compare the performance of thisversion of the greedy algorithm on real world problems against naturalbenchmarks, and show that the algorithm we study performs well also inpractice. To the best of our knowledge, this is the first algorithm with anon-trivial approximation guarantee for maximizing a weakly submodular functionsubject to a constraint other than the simple cardinality constraint. Inparticular, it is the first algorithm with such a guarantee for the importantand broad class of matroid constraints.

Stochastic Conditional Gradient Methods: From Convex Minimization to  Submodular Maximization

  This paper considers stochastic optimization problems for a large class ofobjective functions, including convex and continuous submodular. Stochasticproximal gradient methods have been widely used to solve such problems;however, their applicability remains limited when the problem dimension islarge and the projection onto a convex set is costly. Instead, stochasticconditional gradient methods are proposed as an alternative solution relying on(i) Approximating gradients via a simple averaging technique requiring a singlestochastic gradient evaluation per iteration; (ii) Solving a linear program tocompute the descent/ascent direction. The averaging technique reduces the noiseof gradient approximations as time progresses, and replacing projection step inproximal methods by a linear program lowers the computational complexity ofeach iteration. We show that under convexity and smoothness assumptions, ourproposed method converges to the optimal objective function value at asublinear rate of $O(1/t^{1/3})$. Further, for a monotone and continuousDR-submodular function and subject to a general convex body constraint, weprove that our proposed method achieves a $((1-1/e)OPT-\eps)$ guarantee with$O(1/\eps^3)$ stochastic gradient computations. This guarantee matches theknown hardness results and closes the gap between deterministic and stochasticcontinuous submodular maximization. Additionally, we obtain $((1/e)OPT -\eps)$guarantee after using $O(1/\eps^3)$ stochastic gradients for the case that theobjective function is continuous DR-submodular but non-monotone and theconstraint set is down-closed. By using stochastic continuous optimization asan interface, we provide the first $(1-1/e)$ tight approximation guarantee formaximizing a monotone but stochastic submodular set function subject to amatroid constraint and $(1/e)$ approximation guarantee for the non-monotonecase.

