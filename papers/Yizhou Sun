Thermal engineering in low-dimensional quantum devices: a tutorial
  review of nonequilibrium Green's function methods

  Thermal engineering of quantum devices has attracted much attention since the
discovery of quantized thermal conductance of phonons. Although easily
submerged in numerous excitations in macro-systems, quantum behaviors of
phonons manifest in nanoscale low-dimensional systems even at room temperature.
Especially in nano transport devices, phonons move quasi-ballistically when the
transport length is smaller than their bulk mean free paths. It has been shown
that phonon nonequilibrium Green's function method (NEGF) is effective for the
investigation of nanoscale quantum transport of phonons. In this tutorial
review two aspects of thermal engineering of quantum devices are discussed
using NEGF methods. One covers transport properties of pure phonons; the other
concerns the caloritronic effects, which manipulate other degrees of freedom,
such as charge, spin, and valley, via the temperature gradient. For each part,
we outline basic theoretical formalisms first, then provide a survey on related
investigations on models or realistic materials. Particular attention is given
to phonon topologies and a generalized phonon NEGF method. Finally, we conclude
our review and summarize with an outlook.


Topological Spin Hall Effect due to Magnetic Skyrmions

  The intrinsic spin Hall effect (SHE) originates from the topology of the
Bloch bands in momentum space. The duality between real space and momentum
space calls for a spin Hall effect induced from a real space topology in
analogy to the topological Hall effect (THE) of skyrmions. We theoretically
demonstrate the topological spin Hall effect (TSHE) in which a pure transverse
spin current is generated from a skyrmion spin texture.


Convolutional Set Matching for Graph Similarity

  We introduce GSimCNN (Graph Similarity Computation via Convolutional Neural
Networks) for predicting the similarity score between two graphs. As the core
operation of graph similarity search, pairwise graph similarity computation is
a challenging problem due to the NP-hard nature of computing many graph
distance/similarity metrics. We demonstrate our model using the Graph Edit
Distance (GED) as the example metric. Experiments on three real graph datasets
demonstrate that our model achieves the state-of-the-art performance on graph
similarity search.


Relation Strength-Aware Clustering of Heterogeneous Information Networks
  with Incomplete Attributes

  With the rapid development of online social media, online shopping sites and
cyber-physical systems, heterogeneous information networks have become
increasingly popular and content-rich over time. In many cases, such networks
contain multiple types of objects and links, as well as different kinds of
attributes. The clustering of these objects can provide useful insights in many
applications. However, the clustering of such networks can be challenging since
(a) the attribute values of objects are often incomplete, which implies that an
object may carry only partial attributes or even no attributes to correctly
label itself; and (b) the links of different types may carry different kinds of
semantic meanings, and it is a difficult task to determine the nature of their
relative importance in helping the clustering for a given purpose. In this
paper, we address these challenges by proposing a model-based clustering
algorithm. We design a probabilistic model which clusters the objects of
different types into a common hidden space, by using a user-specified set of
attributes, as well as the links from different relations. The strengths of
different types of links are automatically learned, and are determined by the
given purpose of clustering. An iterative algorithm is designed for solving the
clustering problem, in which the strengths of different types of links and the
quality of clustering results mutually enhance each other. Our experimental
results on real and synthetic data sets demonstrate the effectiveness and
efficiency of the algorithm.


Enhancement of 3rd-harmonics generation during ultrashort pulse
  diffraction in multi-layer volume-grating

  Successful phase-matching methods for Third Harmonics Generation (THG)
include phase-matching in birefringent crystal and quasi-phase-matching (QPM)
in crystal with periodically poled domains. However, these methods are not
feasible in some isotropic materials (e.g. fused silica and photosensitive
silicate glass). It was known that volume-grating in isotropic materials can
independently generate frequency-converted waves. One of disadvantages of
single-layer volume-grating is that the brightness of harmonic emission can not
be enhanced by increasing the grating thickness. In this paper, a THG device
with stratified sub-gratings was designed to enhance THG in isotropic
materials: several sub-gratings were arranged parallel, and the grating-figures
misalignment between neighboring sub-gratings was pre-fabricated. In terms of
extension of interaction length in THG, our multi-layer sub-grating is formally
equivalent to the multi-layer periodically poled crystal (e.g. lithium niobate)
in conventional QPM approach. According to the calculation results, the N-layer
(N >2) can, in principle, generate TH output intensity of N*N times stronger
than single-layer volume-grating does, also compared to N times stronger than
N-layer without figures-misalignment. The effect of random fabrication error in
grating thickness on normalized conversion efficiency was discussed.


GSplit LBI: Taming the Procedural Bias in Neuroimaging for Disease
  Prediction

  In voxel-based neuroimage analysis, lesion features have been the main focus
in disease prediction due to their interpretability with respect to the related
diseases. However, we observe that there exists another type of features
introduced during the preprocessing steps and we call them "\textbf{Procedural
Bias}". Besides, such bias can be leveraged to improve classification accuracy.
Nevertheless, most existing models suffer from either under-fit without
considering procedural bias or poor interpretability without differentiating
such bias from lesion ones. In this paper, a novel dual-task algorithm namely
\emph{GSplit LBI} is proposed to resolve this problem. By introducing an
augmented variable enforced to be structural sparsity with a variable splitting
term, the estimators for prediction and selecting lesion features can be
optimized separately and mutually monitored by each other following an
iterative scheme. Empirical experiments have been evaluated on the Alzheimer's
Disease Neuroimaging Initiative\thinspace(ADNI) database. The advantage of
proposed model is verified by improved stability of selected lesion features
and better classification results.


Joint Text Embedding for Personalized Content-based Recommendation

  Learning a good representation of text is key to many recommendation
applications. Examples include news recommendation where texts to be
recommended are constantly published everyday. However, most existing
recommendation techniques, such as matrix factorization based methods, mainly
rely on interaction histories to learn representations of items. While latent
factors of items can be learned effectively from user interaction data, in many
cases, such data is not available, especially for newly emerged items.
  In this work, we aim to address the problem of personalized recommendation
for completely new items with text information available. We cast the problem
as a personalized text ranking problem and propose a general framework that
combines text embedding with personalized recommendation. Users and textual
content are embedded into latent feature space. The text embedding function can
be learned end-to-end by predicting user interactions with items. To alleviate
sparsity in interaction data, and leverage large amount of text data with
little or no user interactions, we further propose a joint text embedding model
that incorporates unsupervised text embedding with a combination module.
Experimental results show that our model can significantly improve the
effectiveness of recommendation systems on real-world datasets.


Moir√© Photo Restoration Using Multiresolution Convolutional Neural
  Networks

  Digital cameras and mobile phones enable us to conveniently record precious
moments. While digital image quality is constantly being improved, taking
high-quality photos of digital screens still remains challenging because the
photos are often contaminated with moir\'{e} patterns, a result of the
interference between the pixel grids of the camera sensor and the device
screen. Moir\'{e} patterns can severely damage the visual quality of photos.
However, few studies have aimed to solve this problem. In this paper, we
introduce a novel multiresolution fully convolutional network for automatically
removing moir\'{e} patterns from photos. Since a moir\'{e} pattern spans over a
wide range of frequencies, our proposed network performs a nonlinear
multiresolution analysis of the input image before computing how to cancel
moir\'{e} artefacts within every frequency band. We also create a large-scale
benchmark dataset with $100,000^+$ image pairs for investigating and evaluating
moir\'{e} pattern removal algorithms. Our network achieves state-of-the-art
performance on this dataset in comparison to existing learning architectures
for image restoration problems.


Integrating Feature and Image Pyramid: A Lung Nodule Detector Learned in
  Curriculum Fashion

  Lung nodules suffer large variation in size and appearance in CT images.
Nodules less than 10mm can easily lose information after down-sampling in
convolutional neural networks, which results in low sensitivity. In this paper,
a combination of 3D image and feature pyramid is exploited to integrate
lower-level texture features with high-level semantic features, thus leading to
a higher recall. However, 3D operations are time and memory consuming, which
aggravates the situation with the explosive growth of medical images. To tackle
this problem, we propose a general curriculum training strategy to speed up
training. An dynamic sampling method is designed to pick up partial samples
which give the best contribution to network training, thus leading to much less
time consuming. In experiments, we demonstrate that the proposed network
outperforms previous state-of-the-art methods. Meanwhile, our sampling strategy
halves the training time of the proposal network on LUNA16.


Defect Solitons in Parity-Time Symmetric Optical Lattices with Nonlocal
  Nonlinearity

  The existence and stability of defect solitons in parity-time (PT) symmetric
optical lattices with nonlocal nonlinearity are reported. It is found that
nonlocality can expand the stability region of defect solitons. For positive or
zero defects, fundamental and dipole solitons can exist stably in the
semi-infinite gap and the first gap, respectively. For negative defects,
fundamental solitons can be stable in both the semi-infinite gap and the first
gap, whereas dipole solitons are unstable in the first gap. There exist a
maximum degree of nonlocal nonlinearity, above which the fundamental solitons
in the semi-infinite gap and the dipole solitons in the first gap do not exist
for negative defects. The influence of the imaginary part of the PT-symmetric
potentials on soliton stability is given. When the modulation depth of the
PT-symmetric lattices is small, defect solitons can be stable for positive and
zero defects, even if the PT-symmetric potential is above the phase transition
point.


Medial Meshes for Volume Approximation

  Volume approximation is an important problem found in many applications of
computer graphics, vision, and image processing. The problem is about computing
an accurate and compact approximate representation of 3D volumes using some
simple primitives. In this study, we propose a new volume representation,
called medial meshes, and present an efficient method for its computation.
Specifically, we use the union of a novel type of simple volume primitives,
which are spheres and the convex hulls of two or three spheres, to approximate
a given 3D shape. We compute such a volume approximation based on a new method
for medial axis simplification guided by Hausdorff errors. We further
demonstrate the superior efficiency and accuracy of our method over existing
methods for medial axis simplification.


Model for Topological Phononics and Phonon Diode

  The quantum anomalous Hall effect, an exotic topological state first
theoretically predicted by Haldane and recently experimentally observed, has
attracted enormous interest for low-power-consumption electronics. In this
work, we derived a Schr{\"o}dinger-like equation of phonons, where
topology-related quantities, time reversal symmetry and its breaking can be
naturally introduced similar as for electrons. Furthermore, we proposed a
phononic analog of the Haldane model, which gives the novel quantum (anomalous)
Hall-like phonon states characterized by one-way gapless edge modes immune to
scattering. The topologically nontrivial phonon states are useful not only for
conducting phonons without dissipation but also for designing highly efficient
phononic devices, like an ideal phonon diode, which could find important
applications in future phononics.


A Survey of Heterogeneous Information Network Analysis

  Most real systems consist of a large number of interacting, multi-typed
components, while most contemporary researches model them as homogeneous
networks, without distinguishing different types of objects and links in the
networks. Recently, more and more researchers begin to consider these
interconnected, multi-typed data as heterogeneous information networks, and
develop structural analysis approaches by leveraging the rich semantic meaning
of structural types of objects and links in the networks. Compared to widely
studied homogeneous network, the heterogeneous information network contains
richer structure and semantic information, which provides plenty of
opportunities as well as a lot of challenges for data mining. In this paper, we
provide a survey of heterogeneous information network analysis. We will
introduce basic concepts of heterogeneous information network analysis, examine
its developments on different data mining tasks, discuss some advanced topics,
and point out some future research directions.


Player Skill Decomposition in Multiplayer Online Battle Arenas

  Successful analysis of player skills in video games has important impacts on
the process of enhancing player experience without undermining their continuous
skill development. Moreover, player skill analysis becomes more intriguing in
team-based video games because such form of study can help discover useful
factors in effective team formation. In this paper, we consider the problem of
skill decomposition in MOBA (MultiPlayer Online Battle Arena) games, with the
goal to understand what player skill factors are essential for the outcome of a
game match. To understand the construct of MOBA player skills, we utilize
various skill-based predictive models to decompose player skills into
interpretative parts, the impact of which are assessed in statistical terms. We
apply this analysis approach on two widely known MOBAs, namely League of
Legends (LoL) and Defense of the Ancients 2 (DOTA2). The finding is that base
skills of in-game avatars, base skills of players, and players'
champion-specific skills are three prominent skill components influencing LoL's
match outcomes, while those of DOTA2 are mainly impacted by in-game avatars'
base skills but not much by the other two.


EOMM: An Engagement Optimized Matchmaking Framework

  Matchmaking connects multiple players to participate in online
player-versus-player games. Current matchmaking systems depend on a single core
strategy: create fair games at all times. These systems pair similarly skilled
players on the assumption that a fair game is best player experience. We will
demonstrate, however, that this intuitive assumption sometimes fails and that
matchmaking based on fairness is not optimal for engagement.
  In this paper, we propose an Engagement Optimized Matchmaking (EOMM)
framework that maximizes overall player engagement. We prove that equal-skill
based matchmaking is a special case of EOMM on a highly simplified assumption
that rarely holds in reality. Our simulation on real data from a popular game
made by Electronic Arts, Inc. (EA) supports our theoretical results, showing
significant improvement in enhancing player engagement compared to existing
matchmaking methods.


End-to-end Active Object Tracking via Reinforcement Learning

  We study active object tracking, where a tracker takes as input the visual
observation (i.e., frame sequence) and produces the camera control signal
(e.g., move forward, turn left, etc.). Conventional methods tackle the tracking
and the camera control separately, which is challenging to tune jointly. It
also incurs many human efforts for labeling and many expensive trial-and-errors
in realworld. To address these issues, we propose, in this paper, an end-to-end
solution via deep reinforcement learning, where a ConvNet-LSTM function
approximator is adopted for the direct frame-toaction prediction. We further
propose an environment augmentation technique and a customized reward function,
which are crucial for a successful training. The tracker trained in simulators
(ViZDoom, Unreal Engine) shows good generalization in the case of unseen object
moving path, unseen object appearance, unseen background, and distracting
object. It can restore tracking when occasionally losing the target. With the
experiments over the VOT dataset, we also find that the tracking ability,
obtained solely from simulators, can potentially transfer to real-world
scenarios.


New strategy for black phosphorus crystal growth through ternary
  clathrate

  We are reporting a new synthetic strategy to grow large size black phosphorus
(Black-P) crystals through a ternary clathrate Sn$_{24}$P$_{22-x}$I$_8$, under
lower synthetic temperature and pressure. The Black-P crystals are found grown
in situ at the site where the solid clathrate originally resides, which
suggests chemical vapor mineralizer does not play a critical role for the
Black-P formation. More detailed systematical studies has indicated the P
vacancies in the framework of ternary clathrate Sn$_{24}$P$_{22-x}$I$_8$ is
important for the subsequent Black-P from phosphorus vapors, and a likely
Vapor-Solid-Solid (VSS) model is responsible for the Black-P crystal growth.
The obtained room temperature mobility {\mu} is ~ 350 $cm^2/Vs$ from Hall
measurements at mechanically-cleaved flake, where noticeable micro-cracks are
visible. The obtained high mobility value further suggest the high quality of
the Black-P crystals synthesized through this route.


MSplit LBI: Realizing Feature Selection and Dense Estimation
  Simultaneously in Few-shot and Zero-shot Learning

  It is one typical and general topic of learning a good embedding model to
efficiently learn the representation coefficients between two spaces/subspaces.
To solve this task, $L_{1}$ regularization is widely used for the pursuit of
feature selection and avoiding overfitting, and yet the sparse estimation of
features in $L_{1}$ regularization may cause the underfitting of training data.
$L_{2}$ regularization is also frequently used, but it is a biased estimator.
In this paper, we propose the idea that the features consist of three
orthogonal parts, \emph{namely} sparse strong signals, dense weak signals and
random noise, in which both strong and weak signals contribute to the fitting
of data. To facilitate such novel decomposition, \emph{MSplit} LBI is for the
first time proposed to realize feature selection and dense estimation
simultaneously. We provide theoretical and simulational verification that our
method exceeds $L_{1}$ and $L_{2}$ regularization, and extensive experimental
results show that our method achieves state-of-the-art performance in the
few-shot and zero-shot learning.


Learning K-way D-dimensional Discrete Codes for Compact Embedding
  Representations

  Conventional embedding methods directly associate each symbol with a
continuous embedding vector, which is equivalent to applying a linear
transformation based on a "one-hot" encoding of the discrete symbols. Despite
its simplicity, such approach yields the number of parameters that grows
linearly with the vocabulary size and can lead to overfitting. In this work, we
propose a much more compact K-way D-dimensional discrete encoding scheme to
replace the "one-hot" encoding. In the proposed "KD encoding", each symbol is
represented by a $D$-dimensional code with a cardinality of $K$, and the final
symbol embedding vector is generated by composing the code embedding vectors.
To end-to-end learn semantically meaningful codes, we derive a relaxed discrete
optimization approach based on stochastic gradient descent, which can be
generally applied to any differentiable computational graph with an embedding
layer. In our experiments with various applications from natural language
processing to graph convolutional networks, the total size of the embedding
layer can be reduced up to 98\% while achieving similar or better performance.


Q-DeckRec: A Fast Deck Recommendation System for Collectible Card Games

  Deck building is a crucial component in playing Collectible Card Games
(CCGs). The goal of deck building is to choose a fixed-sized subset of cards
from a large card pool, so that they work well together in-game against
specific opponents. Existing methods either lack flexibility to adapt to
different opponents or require large computational resources, still making them
unsuitable for any real-time or large-scale application. We propose a new deck
recommendation system, named Q-DeckRec, which learns a deck search policy
during a training phase and uses it to solve deck building problem instances.
Our experimental results demonstrate Q-DeckRec requires less computational
resources to build winning-effective decks after a training phase compared to
several baseline methods.


Convolutional Neural Networks for Fast Approximation of Graph Edit
  Distance

  Graph Edit Distance (GED) computation is a core operation of many widely-used
graph applications, such as graph classification, graph matching, and graph
similarity search. However, computing the exact GED between two graphs is
NP-complete. Most current approximate algorithms are based on solving a
combinatorial optimization problem, which involves complicated design and high
time complexity. In this paper, we propose a novel end-to-end neural network
based approach to GED approximation, aiming to alleviate the computational
burden while preserving good performance. The proposed approach, named GSimCNN,
turns GED computation into a learning problem. Each graph is considered as a
set of nodes, represented by learnable embedding vectors. The GED computation
is then considered as a two-set matching problem, where a higher matching score
leads to a lower GED. A Convolutional Neural Network (CNN) based approach is
proposed to tackle the set matching problem. We test our algorithm on three
real graph datasets, and our model achieves significant performance enhancement
against state-of-the-art approximate GED computation algorithms.


Exploit the Connectivity: Multi-Object Tracking with TrackletNet

  Multi-object tracking (MOT) is an important and practical task related to
both surveillance systems and moving camera applications, such as autonomous
driving and robotic vision. However, due to unreliable detection, occlusion and
fast camera motion, tracked targets can be easily lost, which makes MOT very
challenging. Most recent works treat tracking as a re-identification (Re-ID)
task, but how to combine appearance and temporal features is still not well
addressed. In this paper, we propose an innovative and effective tracking
method called TrackletNet Tracker (TNT) that combines temporal and appearance
information together as a unified framework. First, we define a graph model
which treats each tracklet as a vertex. The tracklets are generated by
appearance similarity with CNN features and intersection-over-union (IOU) with
epipolar constraints to compensate camera movement between adjacent frames.
Then, for every pair of two tracklets, the similarity is measured by our
designed multi-scale TrackletNet. Afterwards, the tracklets are clustered into
groups which represent individual object IDs. Our proposed TNT has the ability
to handle most of the challenges in MOT, and achieve promising results on MOT16
and MOT17 benchmark datasets compared with other state-of-the-art methods.


Transductive Zero-Shot Learning with Visual Structure Constraint

  Zero-shot Learning (ZSL) aims to recognize objects of the unseen classes,
whose instances may not have been seen during training. It associates seen and
unseen classes with the common semantic space and provides the visual features
for each data instance. Most existing methods first learn a compatible
projection function between the semantic space and the visual space based on
the data of source seen classes, then directly apply it to target unseen
classes. However, in real scenarios, the data distribution between the source
and target domain might not match well, thus causing the well-known domain
shift problem. Based on the observation that visual features of test instances
can be separated into different clusters, we propose a visual structure
constraint on class centers for transductive ZSL, to improve the generality of
the projection function (i.e. alleviate the above domain shift problem).
Specifically, two different strategies (symmetric Chamfer-distance and
bipartite matching) are adopted to align the projected unseen semantic centers
and visual cluster centers of test instances. Experiments on three widely used
datasets demonstrate that the proposed visual structure constraint can bring
substantial performance gain consistently and achieve state-of-the-art results.


Unsupervised Inductive Whole-Graph Embedding by Preserving Graph
  Proximity

  We introduce a novel approach to graph-level representation learning, which
is to embed an entire graph into a vector space where the embeddings of two
graphs preserve their graph-graph proximity. Our approach, UGRAPHEMB, is a
general framework that provides a novel means to performing graph-level
embedding in a completely unsupervised and inductive manner. The learned neural
network can be considered as a function that receives any graph as input,
either seen or unseen in the training set, and transforms it into an embedding.
A novel graph-level embedding generation mechanism called Multi-Scale Node
Attention (MSNA), is proposed. Experiments on five real graph datasets show
that UGRAPHEMB achieves competitive accuracy in the tasks of graph
classification, similarity ranking, and graph visualization.


Learning K-way D-dimensional Discrete Code For Compact Embedding
  Representations

  Embedding methods such as word embedding have become pillars for many
applications containing discrete structures. Conventional embedding methods
directly associate each symbol with a continuous embedding vector, which is
equivalent to applying linear transformation based on "one-hot" encoding of the
discrete symbols. Despite its simplicity, such approach yields number of
parameters that grows linearly with the vocabulary size and can lead to
overfitting. In this work we propose a much more compact K-way D-dimensional
discrete encoding scheme to replace the "one-hot" encoding. In "KD encoding",
each symbol is represented by a $D$-dimensional code, and each of its dimension
has a cardinality of $K$. The final symbol embedding vector can be generated by
composing the code embedding vectors. To learn the semantically meaningful
code, we derive a relaxed discrete optimization technique based on stochastic
gradient descent. By adopting the new coding system, the efficiency of
parameterization can be significantly improved (from linear to logarithmic),
and this can also mitigate the over-fitting problem. In our experiments with
language modeling, the number of embedding parameters can be reduced by 97\%
while achieving similar or better performance.


Recurrent Meta-Structure for Robust Similarity Measure in Heterogeneous
  Information Networks

  Similarity measure as a fundamental task in heterogeneous information network
analysis has been applied to many areas, e.g., product recommendation,
clustering and Web search. Most of the existing metrics depend on the meta-path
or meta-structure specified by users in advance. These metrics are thus
sensitive to the pre-specified meta-path or meta-structure. In this paper, a
novel similarity measure in heterogeneous information networks, called
Recurrent Meta-Structure-based Similarity (RMSS), is proposed. The recurrent
meta-structure as a schematic structure in heterogeneous information networks
provides a unified framework to integrate all of the meta-paths and
meta-structures. Therefore, RMSS is robust to the meta-paths and
meta-structures. We devise an approach to automatically constructing the
recurrent meta-structure. In order to formalize the semantics, the recurrent
meta-structure is decomposed into several recurrent meta-paths and recurrent
meta-trees, and we then define the commuting matrices of the recurrent
meta-paths and meta-trees. All of the commuting matrices of the recurrent
meta-paths and meta-trees are combined according to different weights. Note
that the weights can be determined by two kinds of weighting strategies: local
weighting strategy and global weighting strategy. As a result, RMSS is defined
by virtue of the final commuting matrix. Experimental evaluations show that the
existing metrics are sensitive to different meta-paths or meta-structures and
that the proposed RMSS outperforms the existing metrics in terms of ranking and
clustering tasks.


HeteroMed: Heterogeneous Information Network for Medical Diagnosis

  With the recent availability of Electronic Health Records (EHR) and great
opportunities they offer for advancing medical informatics, there has been
growing interest in mining EHR for improving quality of care. Disease diagnosis
due to its sensitive nature, huge costs of error, and complexity has become an
increasingly important focus of research in past years. Existing studies model
EHR by capturing co-occurrence of clinical events to learn their latent
embeddings. However, relations among clinical events carry various semantics
and contribute differently to disease diagnosis which gives precedence to a
more advanced modeling of heterogeneous data types and relations in EHR data
than existing solutions. To address these issues, we represent how
high-dimensional EHR data and its rich relationships can be suitably translated
into HeteroMed, a heterogeneous information network for robust medical
diagnosis. Our modeling approach allows for straightforward handling of missing
values and heterogeneity of data. HeteroMed exploits metapaths to capture
higher level and semantically important relations contributing to disease
diagnosis. Furthermore, it employs a joint embedding framework to tailor
clinical event representations to the disease diagnosis goal. To the best of
our knowledge, this is the first study to use Heterogeneous Information Network
for modeling clinical data and disease diagnosis. Experimental results of our
study show superior performance of HeteroMed compared to prior methods in
prediction of exact diagnosis codes and general disease cohorts. Moreover,
HeteroMed outperforms baseline models in capturing similarities of clinical
events which are examined qualitatively through case studies.


FDR-HS: An Empirical Bayesian Identification of Heterogenous Features in
  Neuroimage Analysis

  Recent studies found that in voxel-based neuroimage analysis, detecting and
differentiating "procedural bias" that are introduced during the preprocessing
steps from lesion features, not only can help boost accuracy but also can
improve interpretability. To the best of our knowledge, GSplit LBI is the first
model proposed in the literature to simultaneously capture both procedural bias
and lesion features. Despite the fact that it can improve prediction power by
leveraging the procedural bias, it may select spurious features due to the
multicollinearity in high dimensional space. Moreover, it does not take into
account the heterogeneity of these two types of features. In fact, the
procedural bias and lesion features differ in terms of volumetric change and
spatial correlation pattern. To address these issues, we propose a "two-groups"
Empirical-Bayes method called "FDR-HS" (False-Discovery-Rate Heterogenous
Smoothing). Such method is able to not only avoid multicollinearity, but also
exploit the heterogenous spatial patterns of features. In addition, it enjoys
the simplicity in implementation by introducing hidden variables, which turns
the problem into a convex optimization scheme and can be solved efficiently by
the expectation-maximum (EM) algorithm. Empirical experiments have been
evaluated on the Alzheimer's Disease Neuroimage Initiative (ADNI) database. The
advantage of the proposed model is verified by improved interpretability and
prediction power using selected features by FDR-HS.


Representation Independent Analytics Over Structured Data

  Database analytics algorithms leverage quantifiable structural properties of
the data to predict interesting concepts and relationships. The same
information, however, can be represented using many different structures and
the structural properties observed over particular representations do not
necessarily hold for alternative structures. Thus, there is no guarantee that
current database analytics algorithms will still provide the correct insights,
no matter what structures are chosen to organize the database. Because these
algorithms tend to be highly effective over some choices of structure, such as
that of the databases used to validate them, but not so effective with others,
database analytics has largely remained the province of experts who can find
the desired forms for these algorithms. We argue that in order to make database
analytics usable, we should use or develop algorithms that are effective over a
wide range of choices of structural organizations. We introduce the notion of
representation independence, study its fundamental properties for a wide range
of data analytics algorithms, and empirically analyze the amount of
representation independence of some popular database analytics algorithms. Our
results indicate that most algorithms are not generally representation
independent and find the characteristics of more representation independent
heuristics under certain representational shifts.


Geo-SAGE: A Geographical Sparse Additive Generative Model for Spatial
  Item Recommendation

  With the rapid development of location-based social networks (LBSNs), spatial
item recommendation has become an important means to help people discover
attractive and interesting venues and events, especially when users travel out
of town. However, this recommendation is very challenging compared to the
traditional recommender systems. A user can visit only a limited number of
spatial items, leading to a very sparse user-item matrix. Most of the items
visited by a user are located within a short distance from where he/she lives,
which makes it hard to recommend items when the user travels to a far away
place. Moreover, user interests and behavior patterns may vary dramatically
across different geographical regions. In light of this, we propose Geo-SAGE, a
geographical sparse additive generative model for spatial item recommendation
in this paper. Geo-SAGE considers both user personal interests and the
preference of the crowd in the target region, by exploiting both the
co-occurrence pattern of spatial items and the content of spatial items. To
further alleviate the data sparsity issue, Geo-SAGE exploits the geographical
correlation by smoothing the crowd's preferences over a well-designed spatial
index structure called spatial pyramid. We conduct extensive experiments to
evaluate the performance of our Geo-SAGE model on two real large-scale
datasets. The experimental results clearly demonstrate our Geo-SAGE model
outperforms the state-of-the-art in the two tasks of both out-of-town and
home-town recommendations.


Predictive Encoding of Contextual Relationships for Perceptual
  Inference, Interpolation and Prediction

  We propose a new neurally-inspired model that can learn to encode the global
relationship context of visual events across time and space and to use the
contextual information to modulate the analysis by synthesis process in a
predictive coding framework. The model learns latent contextual representations
by maximizing the predictability of visual events based on local and global
contextual information through both top-down and bottom-up processes. In
contrast to standard predictive coding models, the prediction error in this
model is used to update the contextual representation but does not alter the
feedforward input for the next layer, and is thus more consistent with
neurophysiological observations. We establish the computational feasibility of
this model by demonstrating its ability in several aspects. We show that our
model can outperform state-of-art performances of gated Boltzmann machines
(GBM) in estimation of contextual information. Our model can also interpolate
missing events or predict future events in image sequences while simultaneously
estimating contextual information. We show it achieves state-of-art
performances in terms of prediction accuracy in a variety of tasks and
possesses the ability to interpolate missing frames, a function that is lacking
in GBM.


Reciprocal Recommendation System for Online Dating

  Online dating sites have become popular platforms for people to look for
potential romantic partners. Different from traditional user-item
recommendations where the goal is to match items (e.g., books, videos, etc)
with a user's interests, a recommendation system for online dating aims to
match people who are mutually interested in and likely to communicate with each
other. We introduce similarity measures that capture the unique features and
characteristics of the online dating network, for example, the interest
similarity between two users if they send messages to same users, and
attractiveness similarity if they receive messages from same users. A
reciprocal score that measures the compatibility between a user and each
potential dating candidate is computed and the recommendation list is generated
to include users with top scores. The performance of our proposed
recommendation system is evaluated on a real-world dataset from a major online
dating site in China. The results show that our recommendation algorithms
significantly outperform previously proposed approaches, and the collaborative
filtering-based algorithms achieve much better performance than content-based
algorithms in both precision and recall. Our results also reveal interesting
behavioral difference between male and female users when it comes to looking
for potential dates. In particular, males tend to be focused on their own
interest and oblivious towards their attractiveness to potential dates, while
females are more conscientious to their own attractiveness to the other side of
the line.


Representation Independent Proximity and Similarity Search

  Finding similar or strongly related entities in a graph database is a
fundamental problem in data management and analytics with applications in
similarity query processing, entity resolution, and pattern matching.
Similarity search algorithms usually leverage the structural properties of the
data graph to quantify the degree of similarity or relevance between entities.
Nevertheless, the same information can be represented in many different
structures and the structural properties observed over particular
representations do not necessarily hold for alternative structures. Thus, these
algorithms are effective on some representations and ineffective on others. We
postulate that a similarity search algorithm should return essentially the same
answers over different databases that represent the same information. We
formally define the property of representation independence for similarity
search algorithms as their robustness against transformations that modify the
structure of databases and preserve their information content. We formalize two
widespread groups of such transformations called {\it relationship
reorganizing} and {\it entity rearranging} transformations. We show that
current similarity search algorithms are not representation independent under
these transformations and propose an algorithm called {\bf R-PathSim}, which is
provably robust under these transformations. We perform an extensive empirical
study on the representation independence of current similarity search
algorithms under relationship reorganizing and entity rearranging
transformations. Our empirical results suggest that current similarity search
algorithms except for R-PathSim are highly sensitive to the data
representation. These results also indicate that R-PathSim is as effective or
more effective than other similarity search algorithms.


Entity Embedding-based Anomaly Detection for Heterogeneous Categorical
  Events

  Anomaly detection plays an important role in modern data-driven security
applications, such as detecting suspicious access to a socket from a process.
In many cases, such events can be described as a collection of categorical
values that are considered as entities of different types, which we call
heterogeneous categorical events. Due to the lack of intrinsic distance
measures among entities, and the exponentially large event space, most existing
work relies heavily on heuristics to calculate abnormal scores for events.
Different from previous work, we propose a principled and unified probabilistic
model APE (Anomaly detection via Probabilistic pairwise interaction and Entity
embedding) that directly models the likelihood of events. In this model, we
embed entities into a common latent space using their observed co-occurrence in
different events. More specifically, we first model the compatibility of each
pair of entities according to their embeddings. Then we utilize the weighted
pairwise interactions of different entity types to define the event
probability. Using Noise-Contrastive Estimation with "context-dependent" noise
distribution, our model can be learned efficiently regardless of the large
event space. Experimental results on real enterprise surveillance data show
that our methods can accurately detect abnormal events compared to other
state-of-the-art abnormal detection techniques.


Task-Guided and Path-Augmented Heterogeneous Network Embedding for
  Author Identification

  In this paper, we study the problem of author identification under
double-blind review setting, which is to identify potential authors given
information of an anonymized paper. Different from existing approaches that
rely heavily on feature engineering, we propose to use network embedding
approach to address the problem, which can automatically represent nodes into
lower dimensional feature vectors. However, there are two major limitations in
recent studies on network embedding: (1) they are usually general-purpose
embedding methods, which are independent of the specific tasks; and (2) most of
these approaches can only deal with homogeneous networks, where the
heterogeneity of the network is ignored. Hence, challenges faced here are two
folds: (1) how to embed the network under the guidance of the author
identification task, and (2) how to select the best type of information due to
the heterogeneity of the network.
  To address the challenges, we propose a task-guided and path-augmented
heterogeneous network embedding model. In our model, nodes are first embedded
as vectors in latent feature space. Embeddings are then shared and jointly
trained according to task-specific and network-general objectives. We extend
the existing unsupervised network embedding to incorporate meta paths in
heterogeneous networks, and select paths according to the specific task. The
guidance from author identification task for network embedding is provided both
explicitly in joint training and implicitly during meta path selection. Our
experiments demonstrate that by using path-augmented network embedding with
task guidance, our model can obtain significantly better accuracy at
identifying the true authors comparing to existing methods.


Ideology Detection for Twitter Users with Heterogeneous Types of Links

  The problem of ideology detection is to study the latent (political)
placement for people, which is traditionally studied on politicians according
to their voting behaviors. Recently, more and more studies begin to address the
ideology detection problem for ordinary users based on their online behaviors
that can be captured by social media, e.g., Twitter. As far as we are
concerned, however, the vast majority of the existing methods on ideology
detection on social media have oversimplified the problem as a binary
classification problem (i.e., liberal vs. conservative). Moreover, though
social links can play a critical role in deciding one's ideology, most of the
existing work ignores the heterogeneous types of links in social media. In this
paper we propose to detect \emph{numerical} ideology positions for Twitter
users, according to their \emph{follow}, \emph{mention}, and \emph{retweet}
links to a selected set of politicians. A unified probabilistic model is
proposed that can (1) explain the reasons why links are built among people in
terms of their ideology, (2) integrate heterogeneous types of links together in
determining people's ideology, and (3) automatically learn the quality of each
type of links in deciding one's ideology. Experiments have demonstrated the
advantages of our model in terms of both ranking and political leaning
classification accuracy. It is shown that (1) using multiple types of links is
better than using any single type of links alone to determine one's ideology,
and (2) our model is even more superior than baselines when dealing with people
that are sparsely linked in one type of links. We also show that the detected
ideology for Twitter users aligns with our intuition quite well.


Neural Style Transfer: A Review

  The seminal work of Gatys et al. demonstrated the power of Convolutional
Neural Networks (CNNs) in creating artistic imagery by separating and
recombining image content and style. This process of using CNNs to render a
content image in different styles is referred to as Neural Style Transfer
(NST). Since then, NST has become a trending topic both in academic literature
and industrial applications. It is receiving increasing attention and a variety
of approaches are proposed to either improve or extend the original NST
algorithm. In this paper, we aim to provide a comprehensive overview of the
current progress towards NST. We first propose a taxonomy of current algorithms
in the field of NST. Then, we present several evaluation methods and compare
different NST algorithms both qualitatively and quantitatively. The review
concludes with a discussion of various applications of NST and open problems
for future research. A list of papers discussed in this review, corresponding
codes, pre-trained models and more comparison results are publicly available at
https://github.com/ycjing/Neural-Style-Transfer-Papers.


On Sampling Strategies for Neural Network-based Collaborative Filtering

  Recent advances in neural networks have inspired people to design hybrid
recommendation algorithms that can incorporate both (1) user-item interaction
information and (2) content information including image, audio, and text.
Despite their promising results, neural network-based recommendation algorithms
pose extensive computational costs, making it challenging to scale and improve
upon. In this paper, we propose a general neural network-based recommendation
framework, which subsumes several existing state-of-the-art recommendation
algorithms, and address the efficiency issue by investigating sampling
strategies in the stochastic gradient descent training for the framework. We
tackle this issue by first establishing a connection between the loss functions
and the user-item interaction bipartite graph, where the loss function terms
are defined on links while major computation burdens are located at nodes. We
call this type of loss functions "graph-based" loss functions, for which varied
mini-batch sampling strategies can have different computational costs. Based on
the insight, three novel sampling strategies are proposed, which can
significantly improve the training efficiency of the proposed framework (up to
$\times 30$ times speedup in our experiments), as well as improving the
recommendation performance. Theoretical analysis is also provided for both the
computational cost and the convergence. We believe the study of sampling
strategies have further implications on general graph-based loss functions, and
would also enable more research under the neural network-based recommendation
framework.


Zero-shot Learning via Shared-Reconstruction-Graph Pursuit

  Zero-shot learning (ZSL) aims to recognize objects from novel unseen classes
without any training data. Recently, structure-transfer based methods are
proposed to implement ZSL by transferring structural knowledge from the
semantic embedding space to image feature space to classify testing images.
However, we observe that such a knowledge transfer framework may suffer from
the problem of the geometric inconsistency between the data in the training and
testing spaces. We call this problem as the space shift problem. In this paper,
we propose a novel graph based method to alleviate this space shift problem.
Specifically, a Shared Reconstruction Graph (SRG) is pursued to capture the
common structure of data in the two spaces. With the learned SRG, each unseen
class prototype (cluster center) in the image feature space can be synthesized
by the linear combination of other class prototypes, so that testing instances
can be classified based on the distance to these synthesized prototypes. The
SRG bridges the image feature space and semantic embedding space. By applying
spectral clustering on the learned SRG, many meaningful clusters can be
discovered, which interprets ZSL performance on the datasets. Our method can be
easily extended to the generalized zero-shot learning setting. Experiments on
three popular datasets show that our method outperforms other methods on all
datasets. Even with a small number of training samples, our method can achieve
the state-of-the-art performance.


Piecewise Flat Embedding for Image Segmentation

  We introduce a new multi-dimensional nonlinear embedding -- Piecewise Flat
Embedding (PFE) -- for image segmentation. Based on the theory of sparse signal
recovery, piecewise flat embedding with diverse channels attempts to recover a
piecewise constant image representation with sparse region boundaries and
sparse cluster value scattering. The resultant piecewise flat embedding
exhibits interesting properties such as suppressing slowly varying signals, and
offers an image representation with higher region identifiability which is
desirable for image segmentation or high-level semantic analysis tasks. We
formulate our embedding as a variant of the Laplacian Eigenmap embedding with
an $L_{1,p} (0<p\leq1)$ regularization term to promote sparse solutions. First,
we devise a two-stage numerical algorithm based on Bregman iterations to
compute $L_{1,1}$-regularized piecewise flat embeddings. We further generalize
this algorithm through iterative reweighting to solve the general
$L_{1,p}$-regularized problem. To demonstrate its efficacy, we integrate PFE
into two existing image segmentation frameworks, segmentation based on
clustering and hierarchical segmentation based on contour detection.
Experiments on four major benchmark datasets, BSDS500, MSRC, Stanford
Background Dataset, and PASCAL Context, show that segmentation algorithms
incorporating our embedding achieve significantly improved results.


Modeling Game Avatar Synergy and Opposition through Embedding in
  Multiplayer Online Battle Arena Games

  Multiplayer Online Battle Arena (MOBA) games have received increasing
worldwide popularity recently. In such games, players compete in teams against
each other by controlling selected game avatars, each of which is designed with
different strengths and weaknesses. Intuitively, putting together game avatars
that complement each other (synergy) and suppress those of opponents
(opposition) would result in a stronger team. In-depth understanding of synergy
and opposition relationships among game avatars benefits player in making
decisions in game avatar drafting and gaining better prediction of match
events. However, due to intricate design and complex interactions between game
avatars, thorough understanding of their relationships is not a trivial task.
  In this paper, we propose a latent variable model, namely Game Avatar
Embedding (GAE), to learn avatars' numerical representations which encode
synergy and opposition relationships between pairs of avatars. The merits of
our model are twofold: (1) the captured synergy and opposition relationships
are sensible to experienced human players' perception; (2) the learned
numerical representations of game avatars allow many important downstream
tasks, such as similar avatar search, match outcome prediction, and avatar pick
recommender. To our best knowledge, no previous model is able to simultaneously
support both features. Our quantitative and qualitative evaluations on real
match data from three commercial MOBA games illustrate the benefits of our
model.


The Art of Drafting: A Team-Oriented Hero Recommendation System for
  Multiplayer Online Battle Arena Games

  Multiplayer Online Battle Arena (MOBA) games have received increasing
popularity recently. In a match of such games, players compete in two teams of
five, each controlling an in-game avatars, known as heroes, selected from a
roster of more than 100. The selection of heroes, also known as pick or draft,
takes place before the match starts and alternates between the two teams until
each player has selected one hero. Heroes are designed with different strengths
and weaknesses to promote team cooperation in a game. Intuitively, heroes in a
strong team should complement each other's strengths and suppressing those of
opponents. Hero drafting is therefore a challenging problem due to the complex
hero-to-hero relationships to consider. In this paper, we propose a novel hero
recommendation system that suggests heroes to add to an existing team while
maximizing the team's prospect for victory. To that end, we model the drafting
between two teams as a combinatorial game and use Monte Carlo Tree Search
(MCTS) for estimating the values of hero combinations. Our empirical evaluation
shows that hero teams drafted by our recommendation algorithm have
significantly higher win rate against teams constructed by other baseline and
state-of-the-art strategies.


End-to-end Active Object Tracking and Its Real-world Deployment via
  Reinforcement Learning

  We study active object tracking, where a tracker takes visual observations
(i.e., frame sequences) as input and produces the corresponding camera control
signals as output (e.g., move forward, turn left, etc.). Conventional methods
tackle tracking and camera control tasks separately, and the resulting system
is difficult to tune jointly. These methods also require significant human
efforts for image labeling and expensive trial-and-error system tuning in the
real world. To address these issues, we propose, in this paper, an end-to-end
solution via deep reinforcement learning. A ConvNet-LSTM function approximator
is adopted for the direct frame-to-action prediction. We further propose an
environment augmentation technique and a customized reward function, which are
crucial for successful training. The tracker trained in simulators (ViZDoom and
Unreal Engine) demonstrates good generalization behaviors in the case of unseen
object moving paths, unseen object appearances, unseen backgrounds, and
distracting objects. The system is robust and can restore tracking after
occasional lost of the target being tracked. We also find that the tracking
ability, obtained solely from simulators, can potentially transfer to
real-world scenarios. We demonstrate successful examples of such transfer, via
experiments over the VOT dataset and the deployment of a real-world robot using
the proposed active tracker trained in simulation.


Modeling Personalized Dynamics of Social Network and Opinion at
  Individual Level

  Network dynamics has always been a meaningful topic deserving exploration in
the realm of academy. previous network models contain two parts: (1) generating
structure as per user property; (2) changing property as per network structure.
Properties in these models, however, cannot be interpreted to concept in
prevalent social theories or empirical truth. Also, they usually treat everyone
in an uniform fashion. While such assumption is quite misguiding, and thus
saliently limits their performance. To overcome these flaws, we devise a
personalized evolving model for social network and opinion (PENO), where
citizens' ideology is revealed by variable opinions and four dimensions of
personality are considered for each entity - leadership, openness,
agreeableness, and neuroticism. Opinion propagates via social tie, tie
generates from opinion affinity, and personalities integrally work with opinion
and tie across evolution. To our best knowledge, PENO is the first attempt to
introduce personality impact in network dynamics and verify social science with
reasonable visualization during simulation. We also present its probabilistic
graph and conceive iterative learning algorithm. Experiments show PENO
outperforms several state-of-the-art baselines over two typical prediction
tasks - congress voting prediction for legislative bills and friendship
prediction on a book-commenting website. Finally, we discuss its scalability to
do multi-task learning and transfer learning in daily scenarios.


Embedding Uncertain Knowledge Graphs

  Embedding models for deterministic Knowledge Graphs (KG) have been
extensively studied, with the purpose of capturing latent semantic relations
between entities and incorporating the structured knowledge into machine
learning. However, there are many KGs that model uncertain knowledge, which
typically model the inherent uncertainty of relations facts with a confidence
score, and embedding such uncertain knowledge represents an unresolved
challenge. The capturing of uncertain knowledge will benefit many
knowledge-driven applications such as question answering and semantic search by
providing more natural characterization of the knowledge. In this paper, we
propose a novel uncertain KG embedding model UKGE, which aims to preserve both
structural and uncertainty information of relation facts in the embedding
space. Unlike previous models that characterize relation facts with binary
classification techniques, UKGE learns embeddings according to the confidence
scores of uncertain relation facts. To further enhance the precision of UKGE,
we also introduce probabilistic soft logic to infer confidence scores for
unseen relation facts during training. We propose and evaluate two variants of
UKGE based on different learning objectives. Experiments are conducted on three
real-world uncertain KGs via three tasks, i.e. confidence prediction, relation
fact ranking, and relation fact classification. UKGE shows effectiveness in
capturing uncertain knowledge by achieving promising results on these tasks,
and consistently outperforms baselines on these tasks.


Face Detection with End-to-End Integration of a ConvNet and a 3D Model

  This paper presents a method for face detection in the wild, which integrates
a ConvNet and a 3D mean face model in an end-to-end multi-task discriminative
learning framework. The 3D mean face model is predefined and fixed (e.g., we
used the one provided in the AFLW dataset). The ConvNet consists of two
components: (i) The face pro- posal component computes face bounding box
proposals via estimating facial key-points and the 3D transformation (rotation
and translation) parameters for each predicted key-point w.r.t. the 3D mean
face model. (ii) The face verification component computes detection results by
prun- ing and refining proposals based on facial key-points based configuration
pooling. The proposed method addresses two issues in adapting state- of-the-art
generic object detection ConvNets (e.g., faster R-CNN) for face detection: (i)
One is to eliminate the heuristic design of prede- fined anchor boxes in the
region proposals network (RPN) by exploit- ing a 3D mean face model. (ii) The
other is to replace the generic RoI (Region-of-Interest) pooling layer with a
configuration pooling layer to respect underlying object structures. The
multi-task loss consists of three terms: the classification Softmax loss and
the location smooth l1 -losses [14] of both the facial key-points and the face
bounding boxes. In ex- periments, our ConvNet is trained on the AFLW dataset
only and tested on the FDDB benchmark with fine-tuning and on the AFW benchmark
without fine-tuning. The proposed method obtains very competitive
state-of-the-art performance in the two benchmarks.


Robust Subjective Visual Property Prediction from Crowdsourced Pairwise
  Labels

  The problem of estimating subjective visual properties from image and video
has attracted increasing interest. A subjective visual property is useful
either on its own (e.g. image and video interestingness) or as an intermediate
representation for visual recognition (e.g. a relative attribute). Due to its
ambiguous nature, annotating the value of a subjective visual property for
learning a prediction model is challenging. To make the annotation more
reliable, recent studies employ crowdsourcing tools to collect pairwise
comparison labels because human annotators are much better at ranking two
images/videos (e.g. which one is more interesting) than giving an absolute
value to each of them separately. However, using crowdsourced data also
introduces outliers. Existing methods rely on majority voting to prune the
annotation outliers/errors. They thus require large amount of pairwise labels
to be collected. More importantly as a local outlier detection method, majority
voting is ineffective in identifying outliers that can cause global ranking
inconsistencies. In this paper, we propose a more principled way to identify
annotation outliers by formulating the subjective visual property prediction
task as a unified robust learning to rank problem, tackling both the outlier
detection and learning to rank jointly. Differing from existing methods, the
proposed method integrates local pairwise comparison labels together to
minimise a cost that corresponds to global inconsistency of ranking order. This
not only leads to better detection of annotation outliers but also enables
learning with extremely sparse annotations. Extensive experiments on various
benchmark datasets demonstrate that our new approach significantly outperforms
state-of-the-arts alternatives.


Investigation on the reported superconductivity in intercalated black
  phosphorus

  Superconductivity intrinsic to the intercalated black phosphorus (BP) with a
transition temperature Tc of 3.8 K, independent of the intercalant, whether an
alkali or an alkaline earth element, has been reported recently by R. Zhang et
al. (2017). However, the reported Tc and the field effect on the
superconducting (SC) transition both bear great similarities to those for the
pure Sn, which is commonly used for BP synthesis under the vapor transport
method. We have therefore decided to determine whether a minute amount of Sn is
present in the starting high purity BP crystals and whether it is the culprit
for the small SC signal detected. Energy-dispersive X-ray spectroscopy results
confirmed the existence of Sn in the starting high purity BP crystals purchased
from the same company as in R. Zhang et al. (2017). We have reproduced the SC
transition at 3.8 K in Li- and Na-intercalated BP crystals that contain minute
amounts of Sn when prepared by the vapor transport method but not in BP
crystals that are free of Sn when prepared by the high-pressure method. We have
therefore concluded that the SC transition reported by R. Zhang et al. (2017)
is associated with the Sn but not intrinsic to the intercalated BP crystals.


The Second Data Release of the Beijing-Arizona Sky Survey

  This paper presents the second data release (DR2) of the Beijing-Arizona Sky
Survey (BASS). BASS is an imaging survey of about 5400 deg$^2$ in $g$ and $r$
bands using the 2.3 m Bok telescope. DR2 includes the observations as of July
2017 obtained by BASS and Mayall $z$-band Legacy Survey (MzLS). This is our
first time to include the MzLS data covering the same area as BASS. BASS and
MzLS have respectively completed about 72% and 76% of their observations. The
two surveys will be served for the spectroscopic targeting of the upcoming Dark
Energy Spectroscopic Instrument. Both BASS and MzLS data are reduced by the
same pipeline. We have updated the basic data reduction and photometric methods
in DR2. In particular, source detections are performed on stacked images, and
photometric measurements are co-added from single-epoch images based on these
sources. The median 5$\sigma$ depths with corrections of the Galactic
extinction are 24.05, 23.61, and 23.10 mag for $g$, $r$, and $z$ bands,
respectively. The DR2 data products include stacked images, co-added catalogs,
and single-epoch images and catalogs. The BASS website
(http://batc.bao.ac.cn/BASS/) provides detailed information and links to
download the data.


Graph Edit Distance Computation via Graph Neural Networks

  Graph similarity search is among the most important graph-based applications,
e.g. finding the chemical compounds that are most similar to a query compound.
Graph similarity/distance computation, such as Graph Edit Distance (GED) and
Maximum Common Subgraph (MCS), is the core operation of graph similarity search
and many other applications, but very costly to compute in practice. Inspired
by the recent success of neural network approaches to several graph
applications, such as node or graph classification, we propose a novel neural
network based approach to address this classic yet challenging graph problem,
aiming to alleviate the computational burden while preserving a good
performance.
  The proposed approach, called SimGNN, combines two strategies. First, we
design a learnable embedding function that maps every graph into an embedding
vector, which provides a global summary of a graph. A novel attention mechanism
is proposed to emphasize the important nodes with respect to a specific
similarity metric. Second, we design a pairwise node comparison method to
supplement the graph-level embeddings with fine-grained node-level information.
Our model can be trained in an end-to-end fashion, achieves better
generalization on unseen graphs, and in the worst case runs in quadratic time
with respect to the number of nodes in two graphs. Taking GED computation as an
example, experimental results on three real graph datasets demonstrate the
effectiveness and efficiency of our approach. Specifically, our model achieves
smaller error rate and great time reduction compared against a series of
baselines, including several approximation algorithms on GED computation, and
many existing graph neural network based models. Our study suggests SimGNN
provides a new direction for future research on graph similarity computation
and graph similarity search.


