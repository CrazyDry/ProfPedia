Thermal engineering in low-dimensional quantum devices: a tutorial  review of nonequilibrium Green's function methods

  Thermal engineering of quantum devices has attracted much attention since thediscovery of quantized thermal conductance of phonons. Although easilysubmerged in numerous excitations in macro-systems, quantum behaviors ofphonons manifest in nanoscale low-dimensional systems even at room temperature.Especially in nano transport devices, phonons move quasi-ballistically when thetransport length is smaller than their bulk mean free paths. It has been shownthat phonon nonequilibrium Green's function method (NEGF) is effective for theinvestigation of nanoscale quantum transport of phonons. In this tutorialreview two aspects of thermal engineering of quantum devices are discussedusing NEGF methods. One covers transport properties of pure phonons; the otherconcerns the caloritronic effects, which manipulate other degrees of freedom,such as charge, spin, and valley, via the temperature gradient. For each part,we outline basic theoretical formalisms first, then provide a survey on relatedinvestigations on models or realistic materials. Particular attention is givento phonon topologies and a generalized phonon NEGF method. Finally, we concludeour review and summarize with an outlook.

Topological Spin Hall Effect due to Magnetic Skyrmions

  The intrinsic spin Hall effect (SHE) originates from the topology of theBloch bands in momentum space. The duality between real space and momentumspace calls for a spin Hall effect induced from a real space topology inanalogy to the topological Hall effect (THE) of skyrmions. We theoreticallydemonstrate the topological spin Hall effect (TSHE) in which a pure transversespin current is generated from a skyrmion spin texture.

Convolutional Set Matching for Graph Similarity

  We introduce GSimCNN (Graph Similarity Computation via Convolutional NeuralNetworks) for predicting the similarity score between two graphs. As the coreoperation of graph similarity search, pairwise graph similarity computation isa challenging problem due to the NP-hard nature of computing many graphdistance/similarity metrics. We demonstrate our model using the Graph EditDistance (GED) as the example metric. Experiments on three real graph datasetsdemonstrate that our model achieves the state-of-the-art performance on graphsimilarity search.

Relation Strength-Aware Clustering of Heterogeneous Information Networks  with Incomplete Attributes

  With the rapid development of online social media, online shopping sites andcyber-physical systems, heterogeneous information networks have becomeincreasingly popular and content-rich over time. In many cases, such networkscontain multiple types of objects and links, as well as different kinds ofattributes. The clustering of these objects can provide useful insights in manyapplications. However, the clustering of such networks can be challenging since(a) the attribute values of objects are often incomplete, which implies that anobject may carry only partial attributes or even no attributes to correctlylabel itself; and (b) the links of different types may carry different kinds ofsemantic meanings, and it is a difficult task to determine the nature of theirrelative importance in helping the clustering for a given purpose. In thispaper, we address these challenges by proposing a model-based clusteringalgorithm. We design a probabilistic model which clusters the objects ofdifferent types into a common hidden space, by using a user-specified set ofattributes, as well as the links from different relations. The strengths ofdifferent types of links are automatically learned, and are determined by thegiven purpose of clustering. An iterative algorithm is designed for solving theclustering problem, in which the strengths of different types of links and thequality of clustering results mutually enhance each other. Our experimentalresults on real and synthetic data sets demonstrate the effectiveness andefficiency of the algorithm.

Enhancement of 3rd-harmonics generation during ultrashort pulse  diffraction in multi-layer volume-grating

  Successful phase-matching methods for Third Harmonics Generation (THG)include phase-matching in birefringent crystal and quasi-phase-matching (QPM)in crystal with periodically poled domains. However, these methods are notfeasible in some isotropic materials (e.g. fused silica and photosensitivesilicate glass). It was known that volume-grating in isotropic materials canindependently generate frequency-converted waves. One of disadvantages ofsingle-layer volume-grating is that the brightness of harmonic emission can notbe enhanced by increasing the grating thickness. In this paper, a THG devicewith stratified sub-gratings was designed to enhance THG in isotropicmaterials: several sub-gratings were arranged parallel, and the grating-figuresmisalignment between neighboring sub-gratings was pre-fabricated. In terms ofextension of interaction length in THG, our multi-layer sub-grating is formallyequivalent to the multi-layer periodically poled crystal (e.g. lithium niobate)in conventional QPM approach. According to the calculation results, the N-layer(N >2) can, in principle, generate TH output intensity of N*N times strongerthan single-layer volume-grating does, also compared to N times stronger thanN-layer without figures-misalignment. The effect of random fabrication error ingrating thickness on normalized conversion efficiency was discussed.

GSplit LBI: Taming the Procedural Bias in Neuroimaging for Disease  Prediction

  In voxel-based neuroimage analysis, lesion features have been the main focusin disease prediction due to their interpretability with respect to the relateddiseases. However, we observe that there exists another type of featuresintroduced during the preprocessing steps and we call them "\textbf{ProceduralBias}". Besides, such bias can be leveraged to improve classification accuracy.Nevertheless, most existing models suffer from either under-fit withoutconsidering procedural bias or poor interpretability without differentiatingsuch bias from lesion ones. In this paper, a novel dual-task algorithm namely\emph{GSplit LBI} is proposed to resolve this problem. By introducing anaugmented variable enforced to be structural sparsity with a variable splittingterm, the estimators for prediction and selecting lesion features can beoptimized separately and mutually monitored by each other following aniterative scheme. Empirical experiments have been evaluated on the Alzheimer'sDisease Neuroimaging Initiative\thinspace(ADNI) database. The advantage ofproposed model is verified by improved stability of selected lesion featuresand better classification results.

Joint Text Embedding for Personalized Content-based Recommendation

  Learning a good representation of text is key to many recommendationapplications. Examples include news recommendation where texts to berecommended are constantly published everyday. However, most existingrecommendation techniques, such as matrix factorization based methods, mainlyrely on interaction histories to learn representations of items. While latentfactors of items can be learned effectively from user interaction data, in manycases, such data is not available, especially for newly emerged items.  In this work, we aim to address the problem of personalized recommendationfor completely new items with text information available. We cast the problemas a personalized text ranking problem and propose a general framework thatcombines text embedding with personalized recommendation. Users and textualcontent are embedded into latent feature space. The text embedding function canbe learned end-to-end by predicting user interactions with items. To alleviatesparsity in interaction data, and leverage large amount of text data withlittle or no user interactions, we further propose a joint text embedding modelthat incorporates unsupervised text embedding with a combination module.Experimental results show that our model can significantly improve theeffectiveness of recommendation systems on real-world datasets.

Moir√© Photo Restoration Using Multiresolution Convolutional Neural  Networks

  Digital cameras and mobile phones enable us to conveniently record preciousmoments. While digital image quality is constantly being improved, takinghigh-quality photos of digital screens still remains challenging because thephotos are often contaminated with moir\'{e} patterns, a result of theinterference between the pixel grids of the camera sensor and the devicescreen. Moir\'{e} patterns can severely damage the visual quality of photos.However, few studies have aimed to solve this problem. In this paper, weintroduce a novel multiresolution fully convolutional network for automaticallyremoving moir\'{e} patterns from photos. Since a moir\'{e} pattern spans over awide range of frequencies, our proposed network performs a nonlinearmultiresolution analysis of the input image before computing how to cancelmoir\'{e} artefacts within every frequency band. We also create a large-scalebenchmark dataset with $100,000^+$ image pairs for investigating and evaluatingmoir\'{e} pattern removal algorithms. Our network achieves state-of-the-artperformance on this dataset in comparison to existing learning architecturesfor image restoration problems.

Integrating Feature and Image Pyramid: A Lung Nodule Detector Learned in  Curriculum Fashion

  Lung nodules suffer large variation in size and appearance in CT images.Nodules less than 10mm can easily lose information after down-sampling inconvolutional neural networks, which results in low sensitivity. In this paper,a combination of 3D image and feature pyramid is exploited to integratelower-level texture features with high-level semantic features, thus leading toa higher recall. However, 3D operations are time and memory consuming, whichaggravates the situation with the explosive growth of medical images. To tacklethis problem, we propose a general curriculum training strategy to speed uptraining. An dynamic sampling method is designed to pick up partial sampleswhich give the best contribution to network training, thus leading to much lesstime consuming. In experiments, we demonstrate that the proposed networkoutperforms previous state-of-the-art methods. Meanwhile, our sampling strategyhalves the training time of the proposal network on LUNA16.

Defect Solitons in Parity-Time Symmetric Optical Lattices with Nonlocal  Nonlinearity

  The existence and stability of defect solitons in parity-time (PT) symmetricoptical lattices with nonlocal nonlinearity are reported. It is found thatnonlocality can expand the stability region of defect solitons. For positive orzero defects, fundamental and dipole solitons can exist stably in thesemi-infinite gap and the first gap, respectively. For negative defects,fundamental solitons can be stable in both the semi-infinite gap and the firstgap, whereas dipole solitons are unstable in the first gap. There exist amaximum degree of nonlocal nonlinearity, above which the fundamental solitonsin the semi-infinite gap and the dipole solitons in the first gap do not existfor negative defects. The influence of the imaginary part of the PT-symmetricpotentials on soliton stability is given. When the modulation depth of thePT-symmetric lattices is small, defect solitons can be stable for positive andzero defects, even if the PT-symmetric potential is above the phase transitionpoint.

Medial Meshes for Volume Approximation

  Volume approximation is an important problem found in many applications ofcomputer graphics, vision, and image processing. The problem is about computingan accurate and compact approximate representation of 3D volumes using somesimple primitives. In this study, we propose a new volume representation,called medial meshes, and present an efficient method for its computation.Specifically, we use the union of a novel type of simple volume primitives,which are spheres and the convex hulls of two or three spheres, to approximatea given 3D shape. We compute such a volume approximation based on a new methodfor medial axis simplification guided by Hausdorff errors. We furtherdemonstrate the superior efficiency and accuracy of our method over existingmethods for medial axis simplification.

A Survey of Heterogeneous Information Network Analysis

  Most real systems consist of a large number of interacting, multi-typedcomponents, while most contemporary researches model them as homogeneousnetworks, without distinguishing different types of objects and links in thenetworks. Recently, more and more researchers begin to consider theseinterconnected, multi-typed data as heterogeneous information networks, anddevelop structural analysis approaches by leveraging the rich semantic meaningof structural types of objects and links in the networks. Compared to widelystudied homogeneous network, the heterogeneous information network containsricher structure and semantic information, which provides plenty ofopportunities as well as a lot of challenges for data mining. In this paper, weprovide a survey of heterogeneous information network analysis. We willintroduce basic concepts of heterogeneous information network analysis, examineits developments on different data mining tasks, discuss some advanced topics,and point out some future research directions.

Model for Topological Phononics and Phonon Diode

  The quantum anomalous Hall effect, an exotic topological state firsttheoretically predicted by Haldane and recently experimentally observed, hasattracted enormous interest for low-power-consumption electronics. In thiswork, we derived a Schr{\"o}dinger-like equation of phonons, wheretopology-related quantities, time reversal symmetry and its breaking can benaturally introduced similar as for electrons. Furthermore, we proposed aphononic analog of the Haldane model, which gives the novel quantum (anomalous)Hall-like phonon states characterized by one-way gapless edge modes immune toscattering. The topologically nontrivial phonon states are useful not only forconducting phonons without dissipation but also for designing highly efficientphononic devices, like an ideal phonon diode, which could find importantapplications in future phononics.

Player Skill Decomposition in Multiplayer Online Battle Arenas

  Successful analysis of player skills in video games has important impacts onthe process of enhancing player experience without undermining their continuousskill development. Moreover, player skill analysis becomes more intriguing inteam-based video games because such form of study can help discover usefulfactors in effective team formation. In this paper, we consider the problem ofskill decomposition in MOBA (MultiPlayer Online Battle Arena) games, with thegoal to understand what player skill factors are essential for the outcome of agame match. To understand the construct of MOBA player skills, we utilizevarious skill-based predictive models to decompose player skills intointerpretative parts, the impact of which are assessed in statistical terms. Weapply this analysis approach on two widely known MOBAs, namely League ofLegends (LoL) and Defense of the Ancients 2 (DOTA2). The finding is that baseskills of in-game avatars, base skills of players, and players'champion-specific skills are three prominent skill components influencing LoL'smatch outcomes, while those of DOTA2 are mainly impacted by in-game avatars'base skills but not much by the other two.

EOMM: An Engagement Optimized Matchmaking Framework

  Matchmaking connects multiple players to participate in onlineplayer-versus-player games. Current matchmaking systems depend on a single corestrategy: create fair games at all times. These systems pair similarly skilledplayers on the assumption that a fair game is best player experience. We willdemonstrate, however, that this intuitive assumption sometimes fails and thatmatchmaking based on fairness is not optimal for engagement.  In this paper, we propose an Engagement Optimized Matchmaking (EOMM)framework that maximizes overall player engagement. We prove that equal-skillbased matchmaking is a special case of EOMM on a highly simplified assumptionthat rarely holds in reality. Our simulation on real data from a popular gamemade by Electronic Arts, Inc. (EA) supports our theoretical results, showingsignificant improvement in enhancing player engagement compared to existingmatchmaking methods.

End-to-end Active Object Tracking via Reinforcement Learning

  We study active object tracking, where a tracker takes as input the visualobservation (i.e., frame sequence) and produces the camera control signal(e.g., move forward, turn left, etc.). Conventional methods tackle the trackingand the camera control separately, which is challenging to tune jointly. Italso incurs many human efforts for labeling and many expensive trial-and-errorsin realworld. To address these issues, we propose, in this paper, an end-to-endsolution via deep reinforcement learning, where a ConvNet-LSTM functionapproximator is adopted for the direct frame-toaction prediction. We furtherpropose an environment augmentation technique and a customized reward function,which are crucial for a successful training. The tracker trained in simulators(ViZDoom, Unreal Engine) shows good generalization in the case of unseen objectmoving path, unseen object appearance, unseen background, and distractingobject. It can restore tracking when occasionally losing the target. With theexperiments over the VOT dataset, we also find that the tracking ability,obtained solely from simulators, can potentially transfer to real-worldscenarios.

New strategy for black phosphorus crystal growth through ternary  clathrate

  We are reporting a new synthetic strategy to grow large size black phosphorus(Black-P) crystals through a ternary clathrate Sn$_{24}$P$_{22-x}$I$_8$, underlower synthetic temperature and pressure. The Black-P crystals are found grownin situ at the site where the solid clathrate originally resides, whichsuggests chemical vapor mineralizer does not play a critical role for theBlack-P formation. More detailed systematical studies has indicated the Pvacancies in the framework of ternary clathrate Sn$_{24}$P$_{22-x}$I$_8$ isimportant for the subsequent Black-P from phosphorus vapors, and a likelyVapor-Solid-Solid (VSS) model is responsible for the Black-P crystal growth.The obtained room temperature mobility {\mu} is ~ 350 $cm^2/Vs$ from Hallmeasurements at mechanically-cleaved flake, where noticeable micro-cracks arevisible. The obtained high mobility value further suggest the high quality ofthe Black-P crystals synthesized through this route.

MSplit LBI: Realizing Feature Selection and Dense Estimation  Simultaneously in Few-shot and Zero-shot Learning

  It is one typical and general topic of learning a good embedding model toefficiently learn the representation coefficients between two spaces/subspaces.To solve this task, $L_{1}$ regularization is widely used for the pursuit offeature selection and avoiding overfitting, and yet the sparse estimation offeatures in $L_{1}$ regularization may cause the underfitting of training data.$L_{2}$ regularization is also frequently used, but it is a biased estimator.In this paper, we propose the idea that the features consist of threeorthogonal parts, \emph{namely} sparse strong signals, dense weak signals andrandom noise, in which both strong and weak signals contribute to the fittingof data. To facilitate such novel decomposition, \emph{MSplit} LBI is for thefirst time proposed to realize feature selection and dense estimationsimultaneously. We provide theoretical and simulational verification that ourmethod exceeds $L_{1}$ and $L_{2}$ regularization, and extensive experimentalresults show that our method achieves state-of-the-art performance in thefew-shot and zero-shot learning.

Learning K-way D-dimensional Discrete Codes for Compact Embedding  Representations

  Conventional embedding methods directly associate each symbol with acontinuous embedding vector, which is equivalent to applying a lineartransformation based on a "one-hot" encoding of the discrete symbols. Despiteits simplicity, such approach yields the number of parameters that growslinearly with the vocabulary size and can lead to overfitting. In this work, wepropose a much more compact K-way D-dimensional discrete encoding scheme toreplace the "one-hot" encoding. In the proposed "KD encoding", each symbol isrepresented by a $D$-dimensional code with a cardinality of $K$, and the finalsymbol embedding vector is generated by composing the code embedding vectors.To end-to-end learn semantically meaningful codes, we derive a relaxed discreteoptimization approach based on stochastic gradient descent, which can begenerally applied to any differentiable computational graph with an embeddinglayer. In our experiments with various applications from natural languageprocessing to graph convolutional networks, the total size of the embeddinglayer can be reduced up to 98\% while achieving similar or better performance.

Q-DeckRec: A Fast Deck Recommendation System for Collectible Card Games

  Deck building is a crucial component in playing Collectible Card Games(CCGs). The goal of deck building is to choose a fixed-sized subset of cardsfrom a large card pool, so that they work well together in-game againstspecific opponents. Existing methods either lack flexibility to adapt todifferent opponents or require large computational resources, still making themunsuitable for any real-time or large-scale application. We propose a new deckrecommendation system, named Q-DeckRec, which learns a deck search policyduring a training phase and uses it to solve deck building problem instances.Our experimental results demonstrate Q-DeckRec requires less computationalresources to build winning-effective decks after a training phase compared toseveral baseline methods.

Convolutional Neural Networks for Fast Approximation of Graph Edit  Distance

  Graph Edit Distance (GED) computation is a core operation of many widely-usedgraph applications, such as graph classification, graph matching, and graphsimilarity search. However, computing the exact GED between two graphs isNP-complete. Most current approximate algorithms are based on solving acombinatorial optimization problem, which involves complicated design and hightime complexity. In this paper, we propose a novel end-to-end neural networkbased approach to GED approximation, aiming to alleviate the computationalburden while preserving good performance. The proposed approach, named GSimCNN,turns GED computation into a learning problem. Each graph is considered as aset of nodes, represented by learnable embedding vectors. The GED computationis then considered as a two-set matching problem, where a higher matching scoreleads to a lower GED. A Convolutional Neural Network (CNN) based approach isproposed to tackle the set matching problem. We test our algorithm on threereal graph datasets, and our model achieves significant performance enhancementagainst state-of-the-art approximate GED computation algorithms.

Exploit the Connectivity: Multi-Object Tracking with TrackletNet

  Multi-object tracking (MOT) is an important and practical task related toboth surveillance systems and moving camera applications, such as autonomousdriving and robotic vision. However, due to unreliable detection, occlusion andfast camera motion, tracked targets can be easily lost, which makes MOT verychallenging. Most recent works treat tracking as a re-identification (Re-ID)task, but how to combine appearance and temporal features is still not welladdressed. In this paper, we propose an innovative and effective trackingmethod called TrackletNet Tracker (TNT) that combines temporal and appearanceinformation together as a unified framework. First, we define a graph modelwhich treats each tracklet as a vertex. The tracklets are generated byappearance similarity with CNN features and intersection-over-union (IOU) withepipolar constraints to compensate camera movement between adjacent frames.Then, for every pair of two tracklets, the similarity is measured by ourdesigned multi-scale TrackletNet. Afterwards, the tracklets are clustered intogroups which represent individual object IDs. Our proposed TNT has the abilityto handle most of the challenges in MOT, and achieve promising results on MOT16and MOT17 benchmark datasets compared with other state-of-the-art methods.

Transductive Zero-Shot Learning with Visual Structure Constraint

  Zero-shot Learning (ZSL) aims to recognize objects of the unseen classes,whose instances may not have been seen during training. It associates seen andunseen classes with the common semantic space and provides the visual featuresfor each data instance. Most existing methods first learn a compatibleprojection function between the semantic space and the visual space based onthe data of source seen classes, then directly apply it to target unseenclasses. However, in real scenarios, the data distribution between the sourceand target domain might not match well, thus causing the well-known domainshift problem. Based on the observation that visual features of test instancescan be separated into different clusters, we propose a visual structureconstraint on class centers for transductive ZSL, to improve the generality ofthe projection function (i.e. alleviate the above domain shift problem).Specifically, two different strategies (symmetric Chamfer-distance andbipartite matching) are adopted to align the projected unseen semantic centersand visual cluster centers of test instances. Experiments on three widely useddatasets demonstrate that the proposed visual structure constraint can bringsubstantial performance gain consistently and achieve state-of-the-art results.

Unsupervised Inductive Whole-Graph Embedding by Preserving Graph  Proximity

  We introduce a novel approach to graph-level representation learning, whichis to embed an entire graph into a vector space where the embeddings of twographs preserve their graph-graph proximity. Our approach, UGRAPHEMB, is ageneral framework that provides a novel means to performing graph-levelembedding in a completely unsupervised and inductive manner. The learned neuralnetwork can be considered as a function that receives any graph as input,either seen or unseen in the training set, and transforms it into an embedding.A novel graph-level embedding generation mechanism called Multi-Scale NodeAttention (MSNA), is proposed. Experiments on five real graph datasets showthat UGRAPHEMB achieves competitive accuracy in the tasks of graphclassification, similarity ranking, and graph visualization.

Learning K-way D-dimensional Discrete Code For Compact Embedding  Representations

  Embedding methods such as word embedding have become pillars for manyapplications containing discrete structures. Conventional embedding methodsdirectly associate each symbol with a continuous embedding vector, which isequivalent to applying linear transformation based on "one-hot" encoding of thediscrete symbols. Despite its simplicity, such approach yields number ofparameters that grows linearly with the vocabulary size and can lead tooverfitting. In this work we propose a much more compact K-way D-dimensionaldiscrete encoding scheme to replace the "one-hot" encoding. In "KD encoding",each symbol is represented by a $D$-dimensional code, and each of its dimensionhas a cardinality of $K$. The final symbol embedding vector can be generated bycomposing the code embedding vectors. To learn the semantically meaningfulcode, we derive a relaxed discrete optimization technique based on stochasticgradient descent. By adopting the new coding system, the efficiency ofparameterization can be significantly improved (from linear to logarithmic),and this can also mitigate the over-fitting problem. In our experiments withlanguage modeling, the number of embedding parameters can be reduced by 97\%while achieving similar or better performance.

Recurrent Meta-Structure for Robust Similarity Measure in Heterogeneous  Information Networks

  Similarity measure as a fundamental task in heterogeneous information networkanalysis has been applied to many areas, e.g., product recommendation,clustering and Web search. Most of the existing metrics depend on the meta-pathor meta-structure specified by users in advance. These metrics are thussensitive to the pre-specified meta-path or meta-structure. In this paper, anovel similarity measure in heterogeneous information networks, calledRecurrent Meta-Structure-based Similarity (RMSS), is proposed. The recurrentmeta-structure as a schematic structure in heterogeneous information networksprovides a unified framework to integrate all of the meta-paths andmeta-structures. Therefore, RMSS is robust to the meta-paths andmeta-structures. We devise an approach to automatically constructing therecurrent meta-structure. In order to formalize the semantics, the recurrentmeta-structure is decomposed into several recurrent meta-paths and recurrentmeta-trees, and we then define the commuting matrices of the recurrentmeta-paths and meta-trees. All of the commuting matrices of the recurrentmeta-paths and meta-trees are combined according to different weights. Notethat the weights can be determined by two kinds of weighting strategies: localweighting strategy and global weighting strategy. As a result, RMSS is definedby virtue of the final commuting matrix. Experimental evaluations show that theexisting metrics are sensitive to different meta-paths or meta-structures andthat the proposed RMSS outperforms the existing metrics in terms of ranking andclustering tasks.

HeteroMed: Heterogeneous Information Network for Medical Diagnosis

  With the recent availability of Electronic Health Records (EHR) and greatopportunities they offer for advancing medical informatics, there has beengrowing interest in mining EHR for improving quality of care. Disease diagnosisdue to its sensitive nature, huge costs of error, and complexity has become anincreasingly important focus of research in past years. Existing studies modelEHR by capturing co-occurrence of clinical events to learn their latentembeddings. However, relations among clinical events carry various semanticsand contribute differently to disease diagnosis which gives precedence to amore advanced modeling of heterogeneous data types and relations in EHR datathan existing solutions. To address these issues, we represent howhigh-dimensional EHR data and its rich relationships can be suitably translatedinto HeteroMed, a heterogeneous information network for robust medicaldiagnosis. Our modeling approach allows for straightforward handling of missingvalues and heterogeneity of data. HeteroMed exploits metapaths to capturehigher level and semantically important relations contributing to diseasediagnosis. Furthermore, it employs a joint embedding framework to tailorclinical event representations to the disease diagnosis goal. To the best ofour knowledge, this is the first study to use Heterogeneous Information Networkfor modeling clinical data and disease diagnosis. Experimental results of ourstudy show superior performance of HeteroMed compared to prior methods inprediction of exact diagnosis codes and general disease cohorts. Moreover,HeteroMed outperforms baseline models in capturing similarities of clinicalevents which are examined qualitatively through case studies.

FDR-HS: An Empirical Bayesian Identification of Heterogenous Features in  Neuroimage Analysis

  Recent studies found that in voxel-based neuroimage analysis, detecting anddifferentiating "procedural bias" that are introduced during the preprocessingsteps from lesion features, not only can help boost accuracy but also canimprove interpretability. To the best of our knowledge, GSplit LBI is the firstmodel proposed in the literature to simultaneously capture both procedural biasand lesion features. Despite the fact that it can improve prediction power byleveraging the procedural bias, it may select spurious features due to themulticollinearity in high dimensional space. Moreover, it does not take intoaccount the heterogeneity of these two types of features. In fact, theprocedural bias and lesion features differ in terms of volumetric change andspatial correlation pattern. To address these issues, we propose a "two-groups"Empirical-Bayes method called "FDR-HS" (False-Discovery-Rate HeterogenousSmoothing). Such method is able to not only avoid multicollinearity, but alsoexploit the heterogenous spatial patterns of features. In addition, it enjoysthe simplicity in implementation by introducing hidden variables, which turnsthe problem into a convex optimization scheme and can be solved efficiently bythe expectation-maximum (EM) algorithm. Empirical experiments have beenevaluated on the Alzheimer's Disease Neuroimage Initiative (ADNI) database. Theadvantage of the proposed model is verified by improved interpretability andprediction power using selected features by FDR-HS.

Representation Independent Analytics Over Structured Data

  Database analytics algorithms leverage quantifiable structural properties ofthe data to predict interesting concepts and relationships. The sameinformation, however, can be represented using many different structures andthe structural properties observed over particular representations do notnecessarily hold for alternative structures. Thus, there is no guarantee thatcurrent database analytics algorithms will still provide the correct insights,no matter what structures are chosen to organize the database. Because thesealgorithms tend to be highly effective over some choices of structure, such asthat of the databases used to validate them, but not so effective with others,database analytics has largely remained the province of experts who can findthe desired forms for these algorithms. We argue that in order to make databaseanalytics usable, we should use or develop algorithms that are effective over awide range of choices of structural organizations. We introduce the notion ofrepresentation independence, study its fundamental properties for a wide rangeof data analytics algorithms, and empirically analyze the amount ofrepresentation independence of some popular database analytics algorithms. Ourresults indicate that most algorithms are not generally representationindependent and find the characteristics of more representation independentheuristics under certain representational shifts.

Predictive Encoding of Contextual Relationships for Perceptual  Inference, Interpolation and Prediction

  We propose a new neurally-inspired model that can learn to encode the globalrelationship context of visual events across time and space and to use thecontextual information to modulate the analysis by synthesis process in apredictive coding framework. The model learns latent contextual representationsby maximizing the predictability of visual events based on local and globalcontextual information through both top-down and bottom-up processes. Incontrast to standard predictive coding models, the prediction error in thismodel is used to update the contextual representation but does not alter thefeedforward input for the next layer, and is thus more consistent withneurophysiological observations. We establish the computational feasibility ofthis model by demonstrating its ability in several aspects. We show that ourmodel can outperform state-of-art performances of gated Boltzmann machines(GBM) in estimation of contextual information. Our model can also interpolatemissing events or predict future events in image sequences while simultaneouslyestimating contextual information. We show it achieves state-of-artperformances in terms of prediction accuracy in a variety of tasks andpossesses the ability to interpolate missing frames, a function that is lackingin GBM.

Reciprocal Recommendation System for Online Dating

  Online dating sites have become popular platforms for people to look forpotential romantic partners. Different from traditional user-itemrecommendations where the goal is to match items (e.g., books, videos, etc)with a user's interests, a recommendation system for online dating aims tomatch people who are mutually interested in and likely to communicate with eachother. We introduce similarity measures that capture the unique features andcharacteristics of the online dating network, for example, the interestsimilarity between two users if they send messages to same users, andattractiveness similarity if they receive messages from same users. Areciprocal score that measures the compatibility between a user and eachpotential dating candidate is computed and the recommendation list is generatedto include users with top scores. The performance of our proposedrecommendation system is evaluated on a real-world dataset from a major onlinedating site in China. The results show that our recommendation algorithmssignificantly outperform previously proposed approaches, and the collaborativefiltering-based algorithms achieve much better performance than content-basedalgorithms in both precision and recall. Our results also reveal interestingbehavioral difference between male and female users when it comes to lookingfor potential dates. In particular, males tend to be focused on their owninterest and oblivious towards their attractiveness to potential dates, whilefemales are more conscientious to their own attractiveness to the other side ofthe line.

Geo-SAGE: A Geographical Sparse Additive Generative Model for Spatial  Item Recommendation

  With the rapid development of location-based social networks (LBSNs), spatialitem recommendation has become an important means to help people discoverattractive and interesting venues and events, especially when users travel outof town. However, this recommendation is very challenging compared to thetraditional recommender systems. A user can visit only a limited number ofspatial items, leading to a very sparse user-item matrix. Most of the itemsvisited by a user are located within a short distance from where he/she lives,which makes it hard to recommend items when the user travels to a far awayplace. Moreover, user interests and behavior patterns may vary dramaticallyacross different geographical regions. In light of this, we propose Geo-SAGE, ageographical sparse additive generative model for spatial item recommendationin this paper. Geo-SAGE considers both user personal interests and thepreference of the crowd in the target region, by exploiting both theco-occurrence pattern of spatial items and the content of spatial items. Tofurther alleviate the data sparsity issue, Geo-SAGE exploits the geographicalcorrelation by smoothing the crowd's preferences over a well-designed spatialindex structure called spatial pyramid. We conduct extensive experiments toevaluate the performance of our Geo-SAGE model on two real large-scaledatasets. The experimental results clearly demonstrate our Geo-SAGE modeloutperforms the state-of-the-art in the two tasks of both out-of-town andhome-town recommendations.

Representation Independent Proximity and Similarity Search

  Finding similar or strongly related entities in a graph database is afundamental problem in data management and analytics with applications insimilarity query processing, entity resolution, and pattern matching.Similarity search algorithms usually leverage the structural properties of thedata graph to quantify the degree of similarity or relevance between entities.Nevertheless, the same information can be represented in many differentstructures and the structural properties observed over particularrepresentations do not necessarily hold for alternative structures. Thus, thesealgorithms are effective on some representations and ineffective on others. Wepostulate that a similarity search algorithm should return essentially the sameanswers over different databases that represent the same information. Weformally define the property of representation independence for similaritysearch algorithms as their robustness against transformations that modify thestructure of databases and preserve their information content. We formalize twowidespread groups of such transformations called {\it relationshipreorganizing} and {\it entity rearranging} transformations. We show thatcurrent similarity search algorithms are not representation independent underthese transformations and propose an algorithm called {\bf R-PathSim}, which isprovably robust under these transformations. We perform an extensive empiricalstudy on the representation independence of current similarity searchalgorithms under relationship reorganizing and entity rearrangingtransformations. Our empirical results suggest that current similarity searchalgorithms except for R-PathSim are highly sensitive to the datarepresentation. These results also indicate that R-PathSim is as effective ormore effective than other similarity search algorithms.

Entity Embedding-based Anomaly Detection for Heterogeneous Categorical  Events

  Anomaly detection plays an important role in modern data-driven securityapplications, such as detecting suspicious access to a socket from a process.In many cases, such events can be described as a collection of categoricalvalues that are considered as entities of different types, which we callheterogeneous categorical events. Due to the lack of intrinsic distancemeasures among entities, and the exponentially large event space, most existingwork relies heavily on heuristics to calculate abnormal scores for events.Different from previous work, we propose a principled and unified probabilisticmodel APE (Anomaly detection via Probabilistic pairwise interaction and Entityembedding) that directly models the likelihood of events. In this model, weembed entities into a common latent space using their observed co-occurrence indifferent events. More specifically, we first model the compatibility of eachpair of entities according to their embeddings. Then we utilize the weightedpairwise interactions of different entity types to define the eventprobability. Using Noise-Contrastive Estimation with "context-dependent" noisedistribution, our model can be learned efficiently regardless of the largeevent space. Experimental results on real enterprise surveillance data showthat our methods can accurately detect abnormal events compared to otherstate-of-the-art abnormal detection techniques.

Task-Guided and Path-Augmented Heterogeneous Network Embedding for  Author Identification

  In this paper, we study the problem of author identification underdouble-blind review setting, which is to identify potential authors giveninformation of an anonymized paper. Different from existing approaches thatrely heavily on feature engineering, we propose to use network embeddingapproach to address the problem, which can automatically represent nodes intolower dimensional feature vectors. However, there are two major limitations inrecent studies on network embedding: (1) they are usually general-purposeembedding methods, which are independent of the specific tasks; and (2) most ofthese approaches can only deal with homogeneous networks, where theheterogeneity of the network is ignored. Hence, challenges faced here are twofolds: (1) how to embed the network under the guidance of the authoridentification task, and (2) how to select the best type of information due tothe heterogeneity of the network.  To address the challenges, we propose a task-guided and path-augmentedheterogeneous network embedding model. In our model, nodes are first embeddedas vectors in latent feature space. Embeddings are then shared and jointlytrained according to task-specific and network-general objectives. We extendthe existing unsupervised network embedding to incorporate meta paths inheterogeneous networks, and select paths according to the specific task. Theguidance from author identification task for network embedding is provided bothexplicitly in joint training and implicitly during meta path selection. Ourexperiments demonstrate that by using path-augmented network embedding withtask guidance, our model can obtain significantly better accuracy atidentifying the true authors comparing to existing methods.

Ideology Detection for Twitter Users with Heterogeneous Types of Links

  The problem of ideology detection is to study the latent (political)placement for people, which is traditionally studied on politicians accordingto their voting behaviors. Recently, more and more studies begin to address theideology detection problem for ordinary users based on their online behaviorsthat can be captured by social media, e.g., Twitter. As far as we areconcerned, however, the vast majority of the existing methods on ideologydetection on social media have oversimplified the problem as a binaryclassification problem (i.e., liberal vs. conservative). Moreover, thoughsocial links can play a critical role in deciding one's ideology, most of theexisting work ignores the heterogeneous types of links in social media. In thispaper we propose to detect \emph{numerical} ideology positions for Twitterusers, according to their \emph{follow}, \emph{mention}, and \emph{retweet}links to a selected set of politicians. A unified probabilistic model isproposed that can (1) explain the reasons why links are built among people interms of their ideology, (2) integrate heterogeneous types of links together indetermining people's ideology, and (3) automatically learn the quality of eachtype of links in deciding one's ideology. Experiments have demonstrated theadvantages of our model in terms of both ranking and political leaningclassification accuracy. It is shown that (1) using multiple types of links isbetter than using any single type of links alone to determine one's ideology,and (2) our model is even more superior than baselines when dealing with peoplethat are sparsely linked in one type of links. We also show that the detectedideology for Twitter users aligns with our intuition quite well.

Neural Style Transfer: A Review

  The seminal work of Gatys et al. demonstrated the power of ConvolutionalNeural Networks (CNNs) in creating artistic imagery by separating andrecombining image content and style. This process of using CNNs to render acontent image in different styles is referred to as Neural Style Transfer(NST). Since then, NST has become a trending topic both in academic literatureand industrial applications. It is receiving increasing attention and a varietyof approaches are proposed to either improve or extend the original NSTalgorithm. In this paper, we aim to provide a comprehensive overview of thecurrent progress towards NST. We first propose a taxonomy of current algorithmsin the field of NST. Then, we present several evaluation methods and comparedifferent NST algorithms both qualitatively and quantitatively. The reviewconcludes with a discussion of various applications of NST and open problemsfor future research. A list of papers discussed in this review, correspondingcodes, pre-trained models and more comparison results are publicly available athttps://github.com/ycjing/Neural-Style-Transfer-Papers.

On Sampling Strategies for Neural Network-based Collaborative Filtering

  Recent advances in neural networks have inspired people to design hybridrecommendation algorithms that can incorporate both (1) user-item interactioninformation and (2) content information including image, audio, and text.Despite their promising results, neural network-based recommendation algorithmspose extensive computational costs, making it challenging to scale and improveupon. In this paper, we propose a general neural network-based recommendationframework, which subsumes several existing state-of-the-art recommendationalgorithms, and address the efficiency issue by investigating samplingstrategies in the stochastic gradient descent training for the framework. Wetackle this issue by first establishing a connection between the loss functionsand the user-item interaction bipartite graph, where the loss function termsare defined on links while major computation burdens are located at nodes. Wecall this type of loss functions "graph-based" loss functions, for which variedmini-batch sampling strategies can have different computational costs. Based onthe insight, three novel sampling strategies are proposed, which cansignificantly improve the training efficiency of the proposed framework (up to$\times 30$ times speedup in our experiments), as well as improving therecommendation performance. Theoretical analysis is also provided for both thecomputational cost and the convergence. We believe the study of samplingstrategies have further implications on general graph-based loss functions, andwould also enable more research under the neural network-based recommendationframework.

Zero-shot Learning via Shared-Reconstruction-Graph Pursuit

  Zero-shot learning (ZSL) aims to recognize objects from novel unseen classeswithout any training data. Recently, structure-transfer based methods areproposed to implement ZSL by transferring structural knowledge from thesemantic embedding space to image feature space to classify testing images.However, we observe that such a knowledge transfer framework may suffer fromthe problem of the geometric inconsistency between the data in the training andtesting spaces. We call this problem as the space shift problem. In this paper,we propose a novel graph based method to alleviate this space shift problem.Specifically, a Shared Reconstruction Graph (SRG) is pursued to capture thecommon structure of data in the two spaces. With the learned SRG, each unseenclass prototype (cluster center) in the image feature space can be synthesizedby the linear combination of other class prototypes, so that testing instancescan be classified based on the distance to these synthesized prototypes. TheSRG bridges the image feature space and semantic embedding space. By applyingspectral clustering on the learned SRG, many meaningful clusters can bediscovered, which interprets ZSL performance on the datasets. Our method can beeasily extended to the generalized zero-shot learning setting. Experiments onthree popular datasets show that our method outperforms other methods on alldatasets. Even with a small number of training samples, our method can achievethe state-of-the-art performance.

Piecewise Flat Embedding for Image Segmentation

  We introduce a new multi-dimensional nonlinear embedding -- Piecewise FlatEmbedding (PFE) -- for image segmentation. Based on the theory of sparse signalrecovery, piecewise flat embedding with diverse channels attempts to recover apiecewise constant image representation with sparse region boundaries andsparse cluster value scattering. The resultant piecewise flat embeddingexhibits interesting properties such as suppressing slowly varying signals, andoffers an image representation with higher region identifiability which isdesirable for image segmentation or high-level semantic analysis tasks. Weformulate our embedding as a variant of the Laplacian Eigenmap embedding withan $L_{1,p} (0<p\leq1)$ regularization term to promote sparse solutions. First,we devise a two-stage numerical algorithm based on Bregman iterations tocompute $L_{1,1}$-regularized piecewise flat embeddings. We further generalizethis algorithm through iterative reweighting to solve the general$L_{1,p}$-regularized problem. To demonstrate its efficacy, we integrate PFEinto two existing image segmentation frameworks, segmentation based onclustering and hierarchical segmentation based on contour detection.Experiments on four major benchmark datasets, BSDS500, MSRC, StanfordBackground Dataset, and PASCAL Context, show that segmentation algorithmsincorporating our embedding achieve significantly improved results.

Modeling Game Avatar Synergy and Opposition through Embedding in  Multiplayer Online Battle Arena Games

  Multiplayer Online Battle Arena (MOBA) games have received increasingworldwide popularity recently. In such games, players compete in teams againsteach other by controlling selected game avatars, each of which is designed withdifferent strengths and weaknesses. Intuitively, putting together game avatarsthat complement each other (synergy) and suppress those of opponents(opposition) would result in a stronger team. In-depth understanding of synergyand opposition relationships among game avatars benefits player in makingdecisions in game avatar drafting and gaining better prediction of matchevents. However, due to intricate design and complex interactions between gameavatars, thorough understanding of their relationships is not a trivial task.  In this paper, we propose a latent variable model, namely Game AvatarEmbedding (GAE), to learn avatars' numerical representations which encodesynergy and opposition relationships between pairs of avatars. The merits ofour model are twofold: (1) the captured synergy and opposition relationshipsare sensible to experienced human players' perception; (2) the learnednumerical representations of game avatars allow many important downstreamtasks, such as similar avatar search, match outcome prediction, and avatar pickrecommender. To our best knowledge, no previous model is able to simultaneouslysupport both features. Our quantitative and qualitative evaluations on realmatch data from three commercial MOBA games illustrate the benefits of ourmodel.

The Art of Drafting: A Team-Oriented Hero Recommendation System for  Multiplayer Online Battle Arena Games

  Multiplayer Online Battle Arena (MOBA) games have received increasingpopularity recently. In a match of such games, players compete in two teams offive, each controlling an in-game avatars, known as heroes, selected from aroster of more than 100. The selection of heroes, also known as pick or draft,takes place before the match starts and alternates between the two teams untileach player has selected one hero. Heroes are designed with different strengthsand weaknesses to promote team cooperation in a game. Intuitively, heroes in astrong team should complement each other's strengths and suppressing those ofopponents. Hero drafting is therefore a challenging problem due to the complexhero-to-hero relationships to consider. In this paper, we propose a novel herorecommendation system that suggests heroes to add to an existing team whilemaximizing the team's prospect for victory. To that end, we model the draftingbetween two teams as a combinatorial game and use Monte Carlo Tree Search(MCTS) for estimating the values of hero combinations. Our empirical evaluationshows that hero teams drafted by our recommendation algorithm havesignificantly higher win rate against teams constructed by other baseline andstate-of-the-art strategies.

End-to-end Active Object Tracking and Its Real-world Deployment via  Reinforcement Learning

  We study active object tracking, where a tracker takes visual observations(i.e., frame sequences) as input and produces the corresponding camera controlsignals as output (e.g., move forward, turn left, etc.). Conventional methodstackle tracking and camera control tasks separately, and the resulting systemis difficult to tune jointly. These methods also require significant humanefforts for image labeling and expensive trial-and-error system tuning in thereal world. To address these issues, we propose, in this paper, an end-to-endsolution via deep reinforcement learning. A ConvNet-LSTM function approximatoris adopted for the direct frame-to-action prediction. We further propose anenvironment augmentation technique and a customized reward function, which arecrucial for successful training. The tracker trained in simulators (ViZDoom andUnreal Engine) demonstrates good generalization behaviors in the case of unseenobject moving paths, unseen object appearances, unseen backgrounds, anddistracting objects. The system is robust and can restore tracking afteroccasional lost of the target being tracked. We also find that the trackingability, obtained solely from simulators, can potentially transfer toreal-world scenarios. We demonstrate successful examples of such transfer, viaexperiments over the VOT dataset and the deployment of a real-world robot usingthe proposed active tracker trained in simulation.

Modeling Personalized Dynamics of Social Network and Opinion at  Individual Level

  Network dynamics has always been a meaningful topic deserving exploration inthe realm of academy. previous network models contain two parts: (1) generatingstructure as per user property; (2) changing property as per network structure.Properties in these models, however, cannot be interpreted to concept inprevalent social theories or empirical truth. Also, they usually treat everyonein an uniform fashion. While such assumption is quite misguiding, and thussaliently limits their performance. To overcome these flaws, we devise apersonalized evolving model for social network and opinion (PENO), wherecitizens' ideology is revealed by variable opinions and four dimensions ofpersonality are considered for each entity - leadership, openness,agreeableness, and neuroticism. Opinion propagates via social tie, tiegenerates from opinion affinity, and personalities integrally work with opinionand tie across evolution. To our best knowledge, PENO is the first attempt tointroduce personality impact in network dynamics and verify social science withreasonable visualization during simulation. We also present its probabilisticgraph and conceive iterative learning algorithm. Experiments show PENOoutperforms several state-of-the-art baselines over two typical predictiontasks - congress voting prediction for legislative bills and friendshipprediction on a book-commenting website. Finally, we discuss its scalability todo multi-task learning and transfer learning in daily scenarios.

Embedding Uncertain Knowledge Graphs

  Embedding models for deterministic Knowledge Graphs (KG) have beenextensively studied, with the purpose of capturing latent semantic relationsbetween entities and incorporating the structured knowledge into machinelearning. However, there are many KGs that model uncertain knowledge, whichtypically model the inherent uncertainty of relations facts with a confidencescore, and embedding such uncertain knowledge represents an unresolvedchallenge. The capturing of uncertain knowledge will benefit manyknowledge-driven applications such as question answering and semantic search byproviding more natural characterization of the knowledge. In this paper, wepropose a novel uncertain KG embedding model UKGE, which aims to preserve bothstructural and uncertainty information of relation facts in the embeddingspace. Unlike previous models that characterize relation facts with binaryclassification techniques, UKGE learns embeddings according to the confidencescores of uncertain relation facts. To further enhance the precision of UKGE,we also introduce probabilistic soft logic to infer confidence scores forunseen relation facts during training. We propose and evaluate two variants ofUKGE based on different learning objectives. Experiments are conducted on threereal-world uncertain KGs via three tasks, i.e. confidence prediction, relationfact ranking, and relation fact classification. UKGE shows effectiveness incapturing uncertain knowledge by achieving promising results on these tasks,and consistently outperforms baselines on these tasks.

Face Detection with End-to-End Integration of a ConvNet and a 3D Model

  This paper presents a method for face detection in the wild, which integratesa ConvNet and a 3D mean face model in an end-to-end multi-task discriminativelearning framework. The 3D mean face model is predefined and fixed (e.g., weused the one provided in the AFLW dataset). The ConvNet consists of twocomponents: (i) The face pro- posal component computes face bounding boxproposals via estimating facial key-points and the 3D transformation (rotationand translation) parameters for each predicted key-point w.r.t. the 3D meanface model. (ii) The face verification component computes detection results byprun- ing and refining proposals based on facial key-points based configurationpooling. The proposed method addresses two issues in adapting state- of-the-artgeneric object detection ConvNets (e.g., faster R-CNN) for face detection: (i)One is to eliminate the heuristic design of prede- fined anchor boxes in theregion proposals network (RPN) by exploit- ing a 3D mean face model. (ii) Theother is to replace the generic RoI (Region-of-Interest) pooling layer with aconfiguration pooling layer to respect underlying object structures. Themulti-task loss consists of three terms: the classification Softmax loss andthe location smooth l1 -losses [14] of both the facial key-points and the facebounding boxes. In ex- periments, our ConvNet is trained on the AFLW datasetonly and tested on the FDDB benchmark with fine-tuning and on the AFW benchmarkwithout fine-tuning. The proposed method obtains very competitivestate-of-the-art performance in the two benchmarks.

Robust Subjective Visual Property Prediction from Crowdsourced Pairwise  Labels

  The problem of estimating subjective visual properties from image and videohas attracted increasing interest. A subjective visual property is usefuleither on its own (e.g. image and video interestingness) or as an intermediaterepresentation for visual recognition (e.g. a relative attribute). Due to itsambiguous nature, annotating the value of a subjective visual property forlearning a prediction model is challenging. To make the annotation morereliable, recent studies employ crowdsourcing tools to collect pairwisecomparison labels because human annotators are much better at ranking twoimages/videos (e.g. which one is more interesting) than giving an absolutevalue to each of them separately. However, using crowdsourced data alsointroduces outliers. Existing methods rely on majority voting to prune theannotation outliers/errors. They thus require large amount of pairwise labelsto be collected. More importantly as a local outlier detection method, majorityvoting is ineffective in identifying outliers that can cause global rankinginconsistencies. In this paper, we propose a more principled way to identifyannotation outliers by formulating the subjective visual property predictiontask as a unified robust learning to rank problem, tackling both the outlierdetection and learning to rank jointly. Differing from existing methods, theproposed method integrates local pairwise comparison labels together tominimise a cost that corresponds to global inconsistency of ranking order. Thisnot only leads to better detection of annotation outliers but also enableslearning with extremely sparse annotations. Extensive experiments on variousbenchmark datasets demonstrate that our new approach significantly outperformsstate-of-the-arts alternatives.

Investigation on the reported superconductivity in intercalated black  phosphorus

  Superconductivity intrinsic to the intercalated black phosphorus (BP) with atransition temperature Tc of 3.8 K, independent of the intercalant, whether analkali or an alkaline earth element, has been reported recently by R. Zhang etal. (2017). However, the reported Tc and the field effect on thesuperconducting (SC) transition both bear great similarities to those for thepure Sn, which is commonly used for BP synthesis under the vapor transportmethod. We have therefore decided to determine whether a minute amount of Sn ispresent in the starting high purity BP crystals and whether it is the culpritfor the small SC signal detected. Energy-dispersive X-ray spectroscopy resultsconfirmed the existence of Sn in the starting high purity BP crystals purchasedfrom the same company as in R. Zhang et al. (2017). We have reproduced the SCtransition at 3.8 K in Li- and Na-intercalated BP crystals that contain minuteamounts of Sn when prepared by the vapor transport method but not in BPcrystals that are free of Sn when prepared by the high-pressure method. We havetherefore concluded that the SC transition reported by R. Zhang et al. (2017)is associated with the Sn but not intrinsic to the intercalated BP crystals.

The Second Data Release of the Beijing-Arizona Sky Survey

  This paper presents the second data release (DR2) of the Beijing-Arizona SkySurvey (BASS). BASS is an imaging survey of about 5400 deg$^2$ in $g$ and $r$bands using the 2.3 m Bok telescope. DR2 includes the observations as of July2017 obtained by BASS and Mayall $z$-band Legacy Survey (MzLS). This is ourfirst time to include the MzLS data covering the same area as BASS. BASS andMzLS have respectively completed about 72% and 76% of their observations. Thetwo surveys will be served for the spectroscopic targeting of the upcoming DarkEnergy Spectroscopic Instrument. Both BASS and MzLS data are reduced by thesame pipeline. We have updated the basic data reduction and photometric methodsin DR2. In particular, source detections are performed on stacked images, andphotometric measurements are co-added from single-epoch images based on thesesources. The median 5$\sigma$ depths with corrections of the Galacticextinction are 24.05, 23.61, and 23.10 mag for $g$, $r$, and $z$ bands,respectively. The DR2 data products include stacked images, co-added catalogs,and single-epoch images and catalogs. The BASS website(http://batc.bao.ac.cn/BASS/) provides detailed information and links todownload the data.

Graph Edit Distance Computation via Graph Neural Networks

  Graph similarity search is among the most important graph-based applications,e.g. finding the chemical compounds that are most similar to a query compound.Graph similarity/distance computation, such as Graph Edit Distance (GED) andMaximum Common Subgraph (MCS), is the core operation of graph similarity searchand many other applications, but very costly to compute in practice. Inspiredby the recent success of neural network approaches to several graphapplications, such as node or graph classification, we propose a novel neuralnetwork based approach to address this classic yet challenging graph problem,aiming to alleviate the computational burden while preserving a goodperformance.  The proposed approach, called SimGNN, combines two strategies. First, wedesign a learnable embedding function that maps every graph into an embeddingvector, which provides a global summary of a graph. A novel attention mechanismis proposed to emphasize the important nodes with respect to a specificsimilarity metric. Second, we design a pairwise node comparison method tosupplement the graph-level embeddings with fine-grained node-level information.Our model can be trained in an end-to-end fashion, achieves bettergeneralization on unseen graphs, and in the worst case runs in quadratic timewith respect to the number of nodes in two graphs. Taking GED computation as anexample, experimental results on three real graph datasets demonstrate theeffectiveness and efficiency of our approach. Specifically, our model achievessmaller error rate and great time reduction compared against a series ofbaselines, including several approximation algorithms on GED computation, andmany existing graph neural network based models. Our study suggests SimGNNprovides a new direction for future research on graph similarity computationand graph similarity search.

