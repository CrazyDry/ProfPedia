Response to "Visual Dialogue without Vision or Dialogue" (Massiceti et
  al., 2018)

  In a recent workshop paper, Massiceti et al. presented a baseline model and
subsequent critique of Visual Dialog (Das et al., CVPR 2017) that raises what
we believe to be unfounded concerns about the dataset and evaluation. This
article intends to rebut the critique and clarify potential confusions for
practitioners and future participants in the Visual Dialog challenge.


Lemotif: Abstract Visual Depictions of your Emotional States in Life

  We present Lemotif. Lemotif generates a motif for your emotional life. You
tell Lemotif a little bit about your day -- what were salient events or aspects
and how they made you feel. Lemotif will generate a lemotif -- a creative
abstract visual depiction of your emotions and their sources. Over time,
Lemotif can create visual motifs to capture a summary of your emotional states
over arbitrary periods of time -- making patterns in your emotions and their
sources apparent, presenting opportunities to take actions, and measure their
effectiveness. The underlying principles in Lemotif are that the lemotif should
(1) separate out the sources of the emotions, (2) depict these sources
visually, (3) depict the emotions visually, and (4) have a creative aspect to
them. We verify via human studies that each of these factors contributes to the
proposed lemotifs being favored over corresponding baselines.


Collecting Image Description Datasets using Crowdsourcing

  We describe our two new datasets with images described by humans. Both the
datasets were collected using Amazon Mechanical Turk, a crowdsourcing platform.
The two datasets contain significantly more descriptions per image than other
existing datasets. One is based on a popular image description dataset called
the UIUC Pascal Sentence Dataset, whereas the other is based on the Abstract
Scenes dataset con- taining images made from clipart objects. In this paper we
describe our interfaces, analyze some properties of and show example
descriptions from our two datasets.


Graph R-CNN for Scene Graph Generation

  We propose a novel scene graph generation model called Graph R-CNN, that is
both effective and efficient at detecting objects and their relations in
images. Our model contains a Relation Proposal Network (RePN) that efficiently
deals with the quadratic number of potential relations between objects in an
image. We also propose an attentional Graph Convolutional Network (aGCN) that
effectively captures contextual information between objects and relations.
Finally, we introduce a new evaluation metric that is more holistic and
realistic than existing metrics. We report state-of-the-art performance on
scene graph generation as evaluated using both existing and our proposed
metrics.


Human-Machine CRFs for Identifying Bottlenecks in Holistic Scene
  Understanding

  Recent trends in image understanding have pushed for holistic scene
understanding models that jointly reason about various tasks such as object
detection, scene recognition, shape analysis, contextual reasoning, and local
appearance based classifiers. In this work, we are interested in understanding
the roles of these different tasks in improved scene understanding, in
particular semantic segmentation, object detection and scene recognition.
Towards this goal, we "plug-in" human subjects for each of the various
components in a state-of-the-art conditional random field model. Comparisons
among various hybrid human-machine CRFs give us indications of how much "head
room" there is to improve scene understanding by focusing research efforts on
various individual tasks.


Hierarchical Question-Image Co-Attention for Visual Question Answering

  A number of recent works have proposed attention models for Visual Question
Answering (VQA) that generate spatial maps highlighting image regions relevant
to answering the question. In this paper, we argue that in addition to modeling
"where to look" or visual attention, it is equally important to model "what
words to listen to" or question attention. We present a novel co-attention
model for VQA that jointly reasons about image and question attention. In
addition, our model reasons about the question (and consequently the image via
the co-attention mechanism) in a hierarchical fashion via a novel 1-dimensional
convolution neural networks (CNN). Our model improves the state-of-the-art on
the VQA dataset from 60.3% to 60.5%, and from 61.6% to 63.3% on the COCO-QA
dataset. By using ResNet, the performance is further improved to 62.1% for VQA
and 65.4% for COCO-QA.


Human Attention in Visual Question Answering: Do Humans and Deep
  Networks Look at the Same Regions?

  We conduct large-scale studies on `human attention' in Visual Question
Answering (VQA) to understand where humans choose to look to answer questions
about images. We design and test multiple game-inspired novel
attention-annotation interfaces that require the subject to sharpen regions of
a blurred image to answer a question. Thus, we introduce the VQA-HAT (Human
ATtention) dataset. We evaluate attention maps generated by state-of-the-art
VQA models against human attention both qualitatively (via visualizations) and
quantitatively (via rank-order correlation). Overall, our experiments show that
current attention models in VQA do not seem to be looking at the same regions
as humans.


Human Attention in Visual Question Answering: Do Humans and Deep
  Networks Look at the Same Regions?

  We conduct large-scale studies on `human attention' in Visual Question
Answering (VQA) to understand where humans choose to look to answer questions
about images. We design and test multiple game-inspired novel
attention-annotation interfaces that require the subject to sharpen regions of
a blurred image to answer a question. Thus, we introduce the VQA-HAT (Human
ATtention) dataset. We evaluate attention maps generated by state-of-the-art
VQA models against human attention both qualitatively (via visualizations) and
quantitatively (via rank-order correlation). Overall, our experiments show that
current attention models in VQA do not seem to be looking at the same regions
as humans.


Question Relevance in VQA: Identifying Non-Visual And False-Premise
  Questions

  Visual Question Answering (VQA) is the task of answering natural-language
questions about images. We introduce the novel problem of determining the
relevance of questions to images in VQA. Current VQA models do not reason about
whether a question is even related to the given image (e.g. What is the capital
of Argentina?) or if it requires information from external resources to answer
correctly. This can break the continuity of a dialogue in human-machine
interaction. Our approaches for determining relevance are composed of two
stages. Given an image and a question, (1) we first determine whether the
question is visual or not, (2) if visual, we determine whether the question is
relevant to the given image or not. Our approaches, based on LSTM-RNNs, VQA
model uncertainty, and caption-question similarity, are able to outperform
strong baselines on both relevance tasks. We also present human studies showing
that VQA models augmented with such question relevance reasoning are perceived
as more intelligent, reasonable, and human-like.


Analyzing the Behavior of Visual Question Answering Models

  Recently, a number of deep-learning based models have been proposed for the
task of Visual Question Answering (VQA). The performance of most models is
clustered around 60-70%. In this paper we propose systematic methods to analyze
the behavior of these models as a first step towards recognizing their
strengths and weaknesses, and identifying the most fruitful directions for
progress. We analyze two models, one each from two major classes of VQA models
-- with-attention and without-attention and show the similarities and
differences in the behavior of these models. We also analyze the winning entry
of the VQA Challenge 2016.
  Our behavior analysis reveals that despite recent progress, today's VQA
models are "myopic" (tend to fail on sufficiently novel instances), often "jump
to conclusions" (converge on a predicted answer after 'listening' to just half
the question), and are "stubborn" (do not change their answers across images).


Sort Story: Sorting Jumbled Images and Captions into Stories

  Temporal common sense has applications in AI tasks such as QA, multi-document
summarization, and human-AI communication. We propose the task of sequencing --
given a jumbled set of aligned image-caption pairs that belong to a story, the
task is to sort them such that the output sequence forms a coherent story. We
present multiple approaches, via unary (position) and pairwise (order)
predictions, and their ensemble-based combinations, achieving strong results on
this task. We use both text-based and image-based features, which depict
complementary improvements. Using qualitative examples, we demonstrate that our
models have learnt interesting aspects of temporal common sense.


Visual Word2Vec (vis-w2v): Learning Visually Grounded Word Embeddings
  Using Abstract Scenes

  We propose a model to learn visually grounded word embeddings (vis-w2v) to
capture visual notions of semantic relatedness. While word embeddings trained
using text have been extremely successful, they cannot uncover notions of
semantic relatedness implicit in our visual world. For instance, although
"eats" and "stares at" seem unrelated in text, they share semantics visually.
When people are eating something, they also tend to stare at the food.
Grounding diverse relations like "eats" and "stares at" into vision remains
challenging, despite recent progress in vision. We note that the visual
grounding of words depends on semantics, and not the literal pixels. We thus
use abstract scenes created from clipart to provide the visual grounding. We
find that the embeddings we learn capture fine-grained, visually grounded
notions of semantic relatedness. We show improvements over text-only word
embeddings (word2vec) on three tasks: common-sense assertion classification,
visual paraphrasing and text-based image retrieval. Our code and datasets are
available online.


Punny Captions: Witty Wordplay in Image Descriptions

  Wit is a form of rich interaction that is often grounded in a specific
situation (e.g., a comment in response to an event). In this work, we attempt
to build computational models that can produce witty descriptions for a given
image. Inspired by a cognitive account of humor appreciation, we employ
linguistic wordplay, specifically puns, in image descriptions. We develop two
approaches which involve retrieving witty descriptions for a given image from a
large corpus of sentences, or generating them via an encoder-decoder neural
network architecture. We compare our approach against meaningful baseline
approaches via human studies and show substantial improvements. We find that
when a human is subject to similar constraints as the model regarding word
usage and style, people vote the image descriptions generated by our model to
be slightly wittier than human-written witty descriptions. Unsurprisingly,
humans are almost always wittier than the model when they are free to choose
the vocabulary, style, etc.


C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0
  Dataset

  Visual Question Answering (VQA) has received a lot of attention over the past
couple of years. A number of deep learning models have been proposed for this
task. However, it has been shown that these models are heavily driven by
superficial correlations in the training data and lack compositionality -- the
ability to answer questions about unseen compositions of seen concepts. This
compositionality is desirable and central to intelligence. In this paper, we
propose a new setting for Visual Question Answering where the test
question-answer pairs are compositionally novel compared to training
question-answer pairs. To facilitate developing models under this setting, we
present a new compositional split of the VQA v1.0 dataset, which we call
Compositional VQA (C-VQA). We analyze the distribution of questions and answers
in the C-VQA splits. Finally, we evaluate several existing VQA models under
this new setting and show that the performances of these models degrade by a
significant amount compared to the original VQA setting.


CIDEr: Consensus-based Image Description Evaluation

  Automatically describing an image with a sentence is a long-standing
challenge in computer vision and natural language processing. Due to recent
progress in object detection, attribute classification, action recognition,
etc., there is renewed interest in this area. However, evaluating the quality
of descriptions has proven to be challenging. We propose a novel paradigm for
evaluating image descriptions that uses human consensus. This paradigm consists
of three main parts: a new triplet-based method of collecting human annotations
to measure consensus, a new automated metric (CIDEr) that captures consensus,
and two new datasets: PASCAL-50S and ABSTRACT-50S that contain 50 sentences
describing each image. Our simple metric captures human judgment of consensus
better than existing metrics across sentences generated by various sources. We
also evaluate five state-of-the-art image description approaches using this new
protocol and provide a benchmark for future comparisons. A version of CIDEr
named CIDEr-D is available as a part of MS COCO evaluation server to enable
systematic evaluation and benchmarking.


Don't Just Listen, Use Your Imagination: Leveraging Visual Common Sense
  for Non-Visual Tasks

  Artificial agents today can answer factual questions. But they fall short on
questions that require common sense reasoning. Perhaps this is because most
existing common sense databases rely on text to learn and represent knowledge.
But much of common sense knowledge is unwritten - partly because it tends not
to be interesting enough to talk about, and partly because some common sense is
unnatural to articulate in text. While unwritten, it is not unseen. In this
paper we leverage semantic common sense knowledge learned from images - i.e.
visual common sense - in two textual tasks: fill-in-the-blank and visual
paraphrasing. We propose to "imagine" the scene behind the text, and leverage
visual cues from the "imagined" scenes in addition to textual cues while
answering these questions. We imagine the scenes as a visual abstraction. Our
approach outperforms a strong text-only baseline on these tasks. Our proposed
tasks can serve as benchmarks to quantitatively evaluate progress in solving
tasks that go "beyond recognition". Our code and datasets are publicly
available.


Counting Everyday Objects in Everyday Scenes

  We are interested in counting the number of instances of object classes in
natural, everyday images. Previous counting approaches tackle the problem in
restricted domains such as counting pedestrians in surveillance videos. Counts
can also be estimated from outputs of other vision tasks like object detection.
In this work, we build dedicated models for counting designed to tackle the
large variance in counts, appearances, and scales of objects found in natural
scenes. Our approach is inspired by the phenomenon of subitizing - the ability
of humans to make quick assessments of counts given a perceptual signal, for
small count values. Given a natural scene, we employ a divide and conquer
strategy while incorporating context across the scene to adapt the subitizing
idea to counting. Our approach offers consistent improvements over numerous
baseline approaches for counting on the PASCAL VOC 2007 and COCO datasets.
Subsequently, we study how counting can be used to improve object detection. We
then show a proof of concept application of our counting methods to the task of
Visual Question Answering, by studying the `how many?' questions in the VQA and
COCO-QA datasets.


Joint Unsupervised Learning of Deep Representations and Image Clusters

  In this paper, we propose a recurrent framework for Joint Unsupervised
LEarning (JULE) of deep representations and image clusters. In our framework,
successive operations in a clustering algorithm are expressed as steps in a
recurrent process, stacked on top of representations output by a Convolutional
Neural Network (CNN). During training, image clusters and representations are
updated jointly: image clustering is conducted in the forward pass, while
representation learning in the backward pass. Our key idea behind this
framework is that good representations are beneficial to image clustering and
clustering results provide supervisory signals to representation learning. By
integrating two processes into a single model with a unified weighted triplet
loss and optimizing it end-to-end, we can obtain not only more powerful
representations, but also more precise image clusters. Extensive experiments
show that our method outperforms the state-of-the-art on image clustering
across a variety of image datasets. Moreover, the learned representations
generalize well when transferred to other tasks.


Visual Storytelling

  We introduce the first dataset for sequential vision-to-language, and explore
how this data may be used for the task of visual storytelling. The first
release of this dataset, SIND v.1, includes 81,743 unique photos in 20,211
sequences, aligned to both descriptive (caption) and story language. We
establish several strong baselines for the storytelling task, and motivate an
automatic metric to benchmark progress. Modelling concrete description as well
as figurative and social language, as provided in this dataset and the
storytelling task, has the potential to move artificial intelligence from basic
understandings of typical visual scenes towards more and more human-like
understanding of grounded event structure and subjective expression.


Leveraging Visual Question Answering for Image-Caption Ranking

  Visual Question Answering (VQA) is the task of taking as input an image and a
free-form natural language question about the image, and producing an accurate
answer. In this work we view VQA as a "feature extraction" module to extract
image and caption representations. We employ these representations for the task
of image-caption ranking. Each feature dimension captures (imagines) whether a
fact (question-answer pair) could plausibly be true for the image and caption.
This allows the model to interpret images and captions from a wide variety of
perspectives. We propose score-level and representation-level fusion models to
incorporate VQA knowledge in an existing state-of-the-art VQA-agnostic
image-caption ranking model. We find that incorporating and reasoning about
consistency between images and captions significantly improves performance.
Concretely, our model improves state-of-the-art on caption retrieval by 7.1%
and on image retrieval by 4.4% on the MSCOCO dataset.


Deep Learning the City : Quantifying Urban Perception At A Global Scale

  Computer vision methods that quantify the perception of urban environment are
increasingly being used to study the relationship between a city's physical
appearance and the behavior and health of its residents. Yet, the throughput of
current methods is too limited to quantify the perception of cities across the
world. To tackle this challenge, we introduce a new crowdsourced dataset
containing 110,988 images from 56 cities, and 1,170,000 pairwise comparisons
provided by 81,630 online volunteers along six perceptual attributes: safe,
lively, boring, wealthy, depressing, and beautiful. Using this data, we train a
Siamese-like convolutional neural architecture, which learns from a joint
classification and ranking loss, to predict human judgments of pairwise image
comparisons. Our results show that crowdsourcing combined with neural networks
can produce urban perception data at the global scale.


Measuring Machine Intelligence Through Visual Question Answering

  As machines have become more intelligent, there has been a renewed interest
in methods for measuring their intelligence. A common approach is to propose
tasks for which a human excels, but one which machines find difficult. However,
an ideal task should also be easy to evaluate and not be easily gameable. We
begin with a case study exploring the recently popular task of image captioning
and its limitations as a task for measuring machine intelligence. An
alternative and more promising task is Visual Question Answering that tests a
machine's ability to reason about language and vision. We describe a dataset
unprecedented in size created for the task that contains over 760,000 human
generated questions about images. Using around 10 million human generated
answers, machines may be easily evaluated.


Towards Transparent AI Systems: Interpreting Visual Question Answering
  Models

  Deep neural networks have shown striking progress and obtained
state-of-the-art results in many AI research fields in the recent years.
However, it is often unsatisfying to not know why they predict what they do. In
this paper, we address the problem of interpreting Visual Question Answering
(VQA) models. Specifically, we are interested in finding what part of the input
(pixels in images or words in questions) the VQA model focuses on while
answering the question. To tackle this problem, we use two visualization
techniques -- guided backpropagation and occlusion -- to find important words
in the question and important regions in the image. We then present qualitative
and quantitative analyses of these importance maps. We found that even without
explicit attention mechanisms, VQA models may sometimes be implicitly attending
to relevant regions in the image, and often to appropriate words in the
question.


Grad-CAM: Why did you say that?

  We propose a technique for making Convolutional Neural Network (CNN)-based
models more transparent by visualizing input regions that are 'important' for
predictions -- or visual explanations. Our approach, called Gradient-weighted
Class Activation Mapping (Grad-CAM), uses class-specific gradient information
to localize important regions. These localizations are combined with existing
pixel-space visualizations to create a novel high-resolution and
class-discriminative visualization called Guided Grad-CAM. These methods help
better understand CNN-based models, including image captioning and visual
question answering (VQA) models. We evaluate our visual explanations by
measuring their ability to discriminate between classes, to inspire trust in
humans, and their correlation with occlusion maps. Grad-CAM provides a new way
to understand CNN-based models.
  We have released code, an online demo hosted on CloudCV, and a full version
of this extended abstract.


Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image
  Captioning

  Attention-based neural encoder-decoder frameworks have been widely adopted
for image captioning. Most methods force visual attention to be active for
every generated word. However, the decoder likely requires little to no visual
information from the image to predict non-visual words such as "the" and "of".
Other words that may seem visual can often be predicted reliably just from the
language model e.g., "sign" after "behind a red stop" or "phone" following
"talking on a cell". In this paper, we propose a novel adaptive attention model
with a visual sentinel. At each time step, our model decides whether to attend
to the image (and if so, to which regions) or to the visual sentinel. The model
decides whether to attend to the image and where, in order to extract
meaningful information for sequential word generation. We test our method on
the COCO image captioning 2015 challenge dataset and Flickr30K. Our approach
sets the new state-of-the-art by a significant margin.


Context-aware Captions from Context-agnostic Supervision

  We introduce an inference technique to produce discriminative context-aware
image captions (captions that describe differences between images or visual
concepts) using only generic context-agnostic training data (captions that
describe a concept or an image in isolation). For example, given images and
captions of "siamese cat" and "tiger cat", we generate language that describes
the "siamese cat" in a way that distinguishes it from "tiger cat". Our key
novelty is that we show how to do joint inference over a language model that is
context-agnostic and a listener which distinguishes closely-related concepts.
We first apply our technique to a justification task, namely to describe why an
image contains a particular fine-grained category as opposed to another
closely-related category of the CUB-200-2011 dataset. We then study
discriminative image captioning to generate language that uniquely refers to
one of two semantically-similar images in the COCO dataset. Evaluations with
discriminative ground truth for justification and human studies for
discriminative image captioning reveal that our approach outperforms baseline
generative and speaker-listener approaches for discrimination.


LR-GAN: Layered Recursive Generative Adversarial Networks for Image
  Generation

  We present LR-GAN: an adversarial image generation model which takes scene
structure and context into account. Unlike previous generative adversarial
networks (GANs), the proposed GAN learns to generate image background and
foregrounds separately and recursively, and stitch the foregrounds on the
background in a contextually relevant manner to produce a complete natural
image. For each foreground, the model learns to generate its appearance, shape
and pose. The whole model is unsupervised, and is trained in an end-to-end
manner with gradient descent methods. The experiments demonstrate that LR-GAN
can generate more natural images with objects that are more human recognizable
than DCGAN.


Sound-Word2Vec: Learning Word Representations Grounded in Sounds

  To be able to interact better with humans, it is crucial for machines to
understand sound - a primary modality of human perception. Previous works have
used sound to learn embeddings for improved generic textual similarity
assessment. In this work, we treat sound as a first-class citizen, studying
downstream textual tasks which require aural grounding. To this end, we propose
sound-word2vec - a new embedding scheme that learns specialized word embeddings
grounded in sounds. For example, we learn that two seemingly (semantically)
unrelated concepts, like leaves and paper are similar due to the similar
rustling sounds they make. Our embeddings prove useful in textual tasks
requiring aural reasoning like text-based sound retrieval and discovering foley
sound effects (used in movies). Moreover, our embedding space captures
interesting dependencies between words and onomatopoeia and outperforms prior
work on aurally-relevant word relatedness datasets such as AMEN and ASLex.


Cooperative Learning with Visual Attributes

  Learning paradigms involving varying levels of supervision have received a
lot of interest within the computer vision and machine learning communities.
The supervisory information is typically considered to come from a human
supervisor -- a "teacher" figure. In this paper, we consider an alternate
source of supervision -- a "peer" -- i.e. a different machine. We introduce
cooperative learning, where two agents trying to learn the same visual
concepts, but in potentially different environments using different sources of
data (sensors), communicate their current knowledge of these concepts to each
other. Given the distinct sources of data in both agents, the mode of
communication between the two agents is not obvious. We propose the use of
visual attributes -- semantic mid-level visual properties such as furry,
wooden, etc.-- as the mode of communication between the agents. Our experiments
in three domains -- objects, scenes, and animals -- demonstrate that our
proposed cooperative learning approach improves the performance of both agents
as compared to their performance if they were to learn in isolation. Our
approach is particularly applicable in scenarios where privacy, security and/or
bandwidth constraints restrict the amount and type of information the two
agents can exchange.


ParlAI: A Dialog Research Software Platform

  We introduce ParlAI (pronounced "par-lay"), an open-source software platform
for dialog research implemented in Python, available at http://parl.ai. Its
goal is to provide a unified framework for sharing, training and testing of
dialog models, integration of Amazon Mechanical Turk for data collection, human
evaluation, and online/reinforcement learning; and a repository of machine
learning models for comparing with others' models, and improving upon existing
architectures. Over 20 tasks are supported in the first release, including
popular datasets such as SQuAD, bAbI tasks, MCTest, WikiQA, QACNN, QADailyMail,
CBT, bAbI Dialog, Ubuntu, OpenSubtitles and VQA. Several models are integrated,
including neural models such as memory networks, seq2seq and attentive LSTMs.


Deal or No Deal? End-to-End Learning for Negotiation Dialogues

  Much of human dialogue occurs in semi-cooperative settings, where agents with
different goals attempt to agree on common decisions. Negotiations require
complex communication and reasoning skills, but success is easy to measure,
making this an interesting task for AI. We gather a large dataset of
human-human negotiations on a multi-issue bargaining task, where agents who
cannot observe each other's reward functions must reach an agreement (or a
deal) via natural language dialogue. For the first time, we show it is possible
to train end-to-end models for negotiation, which must learn both linguistic
and reasoning skills with no annotated dialogue states. We also introduce
dialogue rollouts, in which the model plans ahead by simulating possible
complete continuations of the conversation, and find that this technique
dramatically improves performance. Our code and dataset are publicly available
(https://github.com/facebookresearch/end-to-end-negotiator).


Active Learning for Visual Question Answering: An Empirical Study

  We present an empirical study of active learning for Visual Question
Answering, where a deep VQA model selects informative question-image pairs from
a pool and queries an oracle for answers to maximally improve its performance
under a limited query budget. Drawing analogies from human learning, we explore
cramming (entropy), curiosity-driven (expected model change), and goal-driven
(expected error reduction) active learning approaches, and propose a fast and
effective goal-driven active learning scoring function to pick question-image
pairs for deep VQA models under the Bayesian Neural Network framework. We find
that deep VQA models need large amounts of training data before they can start
asking informative questions. But once they do, all three approaches outperform
the random selection baseline and achieve significant query savings. For the
scenario where the model is allowed to ask generic questions about images but
is evaluated only on specific questions (e.g., questions whose answer is either
yes or no), our proposed goal-driven scoring function performs the best.


Embodied Question Answering

  We present a new AI task -- Embodied Question Answering (EmbodiedQA) -- where
an agent is spawned at a random location in a 3D environment and asked a
question ("What color is the car?"). In order to answer, the agent must first
intelligently navigate to explore the environment, gather information through
first-person (egocentric) vision, and then answer the question ("orange").
  This challenging task requires a range of AI skills -- active perception,
language understanding, goal-driven navigation, commonsense reasoning, and
grounding of language into actions. In this work, we develop the environments,
end-to-end-trained reinforcement learning agents, and evaluation protocols for
EmbodiedQA.


Neural Baby Talk

  We introduce a novel framework for image captioning that can produce natural
language explicitly grounded in entities that object detectors find in the
image. Our approach reconciles classical slot filling approaches (that are
generally better grounded in images) with modern neural captioning approaches
(that are generally more natural sounding and accurate). Our approach first
generates a sentence `template' with slot locations explicitly tied to specific
image regions. These slots are then filled in by visual concepts identified in
the regions by object detectors. The entire architecture (sentence template
generation and slot filling with object detectors) is end-to-end
differentiable. We verify the effectiveness of our proposed model on different
image captioning tasks. On standard image captioning and novel object
captioning, our model reaches state-of-the-art on both COCO and Flickr30k
datasets. We also demonstrate that our model has unique advantages when the
train and test distributions of scene compositions -- and hence language priors
of associated captions -- are different. Code has been made available at:
https://github.com/jiasenlu/NeuralBabyTalk


Audio Visual Scene-Aware Dialog (AVSD) Challenge at DSTC7

  Scene-aware dialog systems will be able to have conversations with users
about the objects and events around them. Progress on such systems can be made
by integrating state-of-the-art technologies from multiple research areas
including end-to-end dialog systems visual dialog, and video description. We
introduce the Audio Visual Scene Aware Dialog (AVSD) challenge and dataset. In
this challenge, which is one track of the 7th Dialog System Technology
Challenges (DSTC7) workshop1, the task is to build a system that generates
responses in a dialog about an input video


Talk the Walk: Navigating New York City through Grounded Dialogue

  We introduce "Talk The Walk", the first large-scale dialogue dataset grounded
in action and perception. The task involves two agents (a "guide" and a
"tourist") that communicate via natural language in order to achieve a common
goal: having the tourist navigate to a given target location. The task and
dataset, which are described in detail, are challenging and their full solution
is an open problem that we pose to the community. We (i) focus on the task of
tourist localization and develop the novel Masked Attention for Spatial
Convolutions (MASC) mechanism that allows for grounding tourist utterances into
the guide's map, (ii) show it yields significant improvements for both emergent
and natural language communication, and (iii) using this method, we establish
non-trivial baselines on the full task.


Pythia v0.1: the Winning Entry to the VQA Challenge 2018

  This document describes Pythia v0.1, the winning entry from Facebook AI
Research (FAIR)'s A-STAR team to the VQA Challenge 2018.
  Our starting point is a modular re-implementation of the bottom-up top-down
(up-down) model. We demonstrate that by making subtle but important changes to
the model architecture and the learning rate schedule, fine-tuning image
features, and adding data augmentation, we can significantly improve the
performance of the up-down model on VQA v2.0 dataset -- from 65.67% to 70.22%.
  Furthermore, by using a diverse ensemble of models trained with different
features and on different datasets, we are able to significantly improve over
the 'standard' way of ensembling (i.e. same model with different random seeds)
by 1.31%. Overall, we achieve 72.27% on the test-std split of the VQA v2.0
dataset. Our code in its entirety (training, evaluation, data-augmentation,
ensembling) and pre-trained models are publicly available at:
https://github.com/facebookresearch/pythia


Choose Your Neuron: Incorporating Domain Knowledge through
  Neuron-Importance

  Individual neurons in convolutional neural networks supervised for
image-level classification tasks have been shown to implicitly learn
semantically meaningful concepts ranging from simple textures and shapes to
whole or partial objects - forming a "dictionary" of concepts acquired through
the learning process. In this work we introduce a simple, efficient zero-shot
learning approach based on this observation. Our approach, which we call Neuron
Importance-AwareWeight Transfer (NIWT), learns to map domain knowledge about
novel "unseen" classes onto this dictionary of learned concepts and then
optimizes for network parameters that can effectively combine these concepts -
essentially learning classifiers by discovering and composing learned semantic
concepts in deep networks. Our approach shows improvements over previous
approaches on the CUBirds and AWA2 generalized zero-shot learning benchmarks.
We demonstrate our approach on a diverse set of semantic inputs as external
domain knowledge including attributes and natural language captions. Moreover
by learning inverse mappings, NIWT can provide visual and textual explanations
for the predictions made by the newly learned classifiers and provide neuron
names. Our code is available at
https://github.com/ramprs/neuron-importance-zsl.


Neural Modular Control for Embodied Question Answering

  We present a modular approach for learning policies for navigation over long
planning horizons from language input. Our hierarchical policy operates at
multiple timescales, where the higher-level master policy proposes subgoals to
be executed by specialized sub-policies. Our choice of subgoals is
compositional and semantic, i.e. they can be sequentially combined in arbitrary
orderings, and assume human-interpretable descriptions (e.g. 'exit room', 'find
kitchen', 'find refrigerator', etc.).
  We use imitation learning to warm-start policies at each level of the
hierarchy, dramatically increasing sample efficiency, followed by reinforcement
learning. Independent reinforcement learning at each level of hierarchy enables
sub-policies to adapt to consequences of their actions and recover from errors.
Subsequent joint hierarchical training enables the master policy to adapt to
the sub-policies.


TarMAC: Targeted Multi-Agent Communication

  We explore a collaborative multi-agent reinforcement learning setting where a
team of agents attempts to solve cooperative tasks in partially-observable
environments. In this scenario, learning an effective communication protocol is
key. We propose a communication architecture that allows for targeted
communication, where agents learn both what messages to send and who to send
them to, solely from downstream task-specific reward without any communication
supervision. Additionally, we introduce a multi-stage communication approach
where the agents co-ordinate via multiple rounds of communication before taking
actions in the environment. We evaluate our approach on a diverse set of
cooperative multi-agent tasks, of varying difficulties, with varying number of
agents, in a variety of environments ranging from 2D grid layouts of shapes and
simulated traffic junctions to complex 3D indoor environments. We demonstrate
the benefits of targeted as well as multi-stage communication. Moreover, we
show that the targeted communication strategies learned by agents are both
interpretable and intuitive.


Do Explanations make VQA Models more Predictable to a Human?

  A rich line of research attempts to make deep neural networks more
transparent by generating human-interpretable 'explanations' of their decision
process, especially for interactive tasks like Visual Question Answering (VQA).
In this work, we analyze if existing explanations indeed make a VQA model --
its responses as well as failures -- more predictable to a human. Surprisingly,
we find that they do not. On the other hand, we find that human-in-the-loop
approaches that treat the model as a black-box do.


Dialog System Technology Challenge 7

  This paper introduces the Seventh Dialog System Technology Challenges (DSTC),
which use shared datasets to explore the problem of building dialog systems.
Recently, end-to-end dialog modeling approaches have been applied to various
dialog tasks. The seventh DSTC (DSTC7) focuses on developing technologies
related to end-to-end dialog systems for (1) sentence selection, (2) sentence
generation and (3) audio visual scene aware dialog. This paper summarizes the
overall setup and results of DSTC7, including detailed descriptions of the
different tracks and provided datasets. We also describe overall trends in the
submitted systems and the key results. Each track introduced new datasets and
participants achieved impressive results using state-of-the-art end-to-end
technologies.


Audio-Visual Scene-Aware Dialog

  We introduce the task of scene-aware dialog. Given a follow-up question in an
ongoing dialog about a video, our goal is to generate a complete and natural
response to a question given (a) an input video, and (b) the history of
previous turns in the dialog. To succeed, agents must ground the semantics in
the video and leverage contextual cues from the history of the dialog to answer
the question. To benchmark this task, we introduce the Audio Visual Scene-Aware
Dialog (AVSD) dataset. For each of more than 11,000 videos of human actions for
the Charades dataset. Our dataset contains a dialog about the video, plus a
final summary of the video by one of the dialog participants. We train several
baseline systems for this task and evaluate the performance of the trained
models using several qualitative and quantitative metrics. Our results indicate
that the models must comprehend all the available inputs (video, audio,
question and dialog history) to perform well on this dataset.


Embodied Multimodal Multitask Learning

  Recent efforts on training visual navigation agents conditioned on language
using deep reinforcement learning have been successful in learning policies for
different multimodal tasks, such as semantic goal navigation and embodied
question answering. In this paper, we propose a multitask model capable of
jointly learning these multimodal tasks, and transferring knowledge of words
and their grounding in visual objects across the tasks. The proposed model uses
a novel Dual-Attention unit to disentangle the knowledge of words in the
textual representations and visual concepts in the visual representations, and
align them with each other. This disentangled task-invariant alignment of
representations facilitates grounding and knowledge transfer across both tasks.
We show that the proposed model outperforms a range of baselines on both tasks
in simulated 3D environments. We also show that this disentanglement of
representations makes our model modular, interpretable, and allows for transfer
to instructions containing new words by leveraging object detectors.


Taking a HINT: Leveraging Explanations to Make Vision and Language
  Models More Grounded

  Many vision and language models suffer from poor visual grounding - often
falling back on easy-to-learn language priors rather than associating language
with visual concepts. In this work, we propose a generic framework which we
call Human Importance-aware Network Tuning (HINT) that effectively leverages
human supervision to improve visual grounding. HINT constrains deep networks to
be sensitive to the same input regions as humans. Crucially, our approach
optimizes the alignment between human attention maps and gradient-based network
importances - ensuring that models learn not just to look at but rather rely on
visual concepts that humans found relevant for a task when making predictions.
We demonstrate our approach on Visual Question Answering and Image Captioning
tasks, achieving state of-the-art for the VQA-CP dataset which penalizes
over-reliance on language priors.


Cycle-Consistency for Robust Visual Question Answering

  Despite significant progress in Visual Question Answering over the years,
robustness of today's VQA models leave much to be desired. We introduce a new
evaluation protocol and associated dataset (VQA-Rephrasings) and show that
state-of-the-art VQA models are notoriously brittle to linguistic variations in
questions. VQA-Rephrasings contains 3 human-provided rephrasings for 40k
questions spanning 40k images from the VQA v2.0 validation dataset. As a step
towards improving robustness of VQA models, we propose a model-agnostic
framework that exploits cycle consistency. Specifically, we train a model to
not only answer a question, but also generate a question conditioned on the
answer, such that the answer predicted for the generated question is the same
as the ground truth answer to the original question. Without the use of
additional annotations, we show that our approach is significantly more robust
to linguistic variations than state-of-the-art VQA models, when evaluated on
the VQA-Rephrasings dataset. In addition, our approach outperforms
state-of-the-art approaches on the standard VQA and Visual Question Generation
tasks on the challenging VQA v2.0 dataset.


Probabilistic Neural-symbolic Models for Interpretable Visual Question
  Answering

  We propose a new class of probabilistic neural-symbolic models, that have
symbolic functional programs as a latent, stochastic variable. Instantiated in
the context of visual question answering, our probabilistic formulation offers
two key conceptual advantages over prior neural-symbolic models for VQA.
Firstly, the programs generated by our model are more understandable while
requiring lesser number of teaching examples. Secondly, we show that one can
pose counterfactual scenarios to the model, to probe its beliefs on the
programs that could lead to a specified answer given an image. Our results on
the CLEVR and SHAPES datasets verify our hypotheses, showing that the model
gets better program (and answer) prediction accuracy even in the low data
regime, and allows one to probe the coherence and consistency of reasoning
performed.


Learning Dynamics Model in Reinforcement Learning by Incorporating the
  Long Term Future

  In model-based reinforcement learning, the agent interleaves between model
learning and planning. These two components are inextricably intertwined. If
the model is not able to provide sensible long-term prediction, the executed
planner would exploit model flaws, which can yield catastrophic failures. This
paper focuses on building a model that reasons about the long-term future and
demonstrates how to use this for efficient planning and exploration. To this
end, we build a latent-variable autoregressive model by leveraging recent ideas
in variational inference. We argue that forcing latent variables to carry
future information through an auxiliary task substantially improves long-term
predictions. Moreover, by planning in the latent space, the planner's solution
is ensured to be within regions where the model is valid. An exploration
strategy can be devised by searching for unlikely trajectories under the model.
Our method achieves higher reward faster compared to baselines on a variety of
tasks and environments in both the imitation learning and model-based
reinforcement learning settings.


Trick or TReAT: Thematic Reinforcement for Artistic Typography

  An approach to make text visually appealing and memorable is semantic
reinforcement - the use of visual cues alluding to the context or theme in
which the word is being used to reinforce the message (e.g., Google Doodles).
We present a computational approach for semantic reinforcement called TReAT -
Thematic Reinforcement for Artistic Typography. Given an input word (e.g. exam)
and a theme (e.g. education), the individual letters of the input word are
replaced by cliparts relevant to the theme which visually resemble the letters
- adding creative context to the potentially boring input word. We use an
unsupervised approach to learn a latent space to represent letters and cliparts
and compute similarities between the two. Human studies show that participants
can reliably recognize the word as well as the theme in our outputs (TReATs)
and find them more creative compared to meaningful baselines.


Align2Ground: Weakly Supervised Phrase Grounding Guided by Image-Caption
  Alignment

  We address the problem of grounding free-form textual phrases by using weak
supervision from image-caption pairs. We propose a novel end-to-end model that
uses caption-to-image retrieval as a `downstream' task to guide the process of
phrase localization. Our method, as a first step, infers the latent
correspondences between regions-of-interest (RoIs) and phrases in the caption
and creates a discriminative image representation using these matched RoIs. In
a subsequent step, this (learned) representation is aligned with the caption.
Our key contribution lies in building this `caption-conditioned' image encoding
which tightly couples both the tasks and allows the weak supervision to
effectively guide visual grounding. We provide an extensive empirical and
qualitative analysis to investigate the different components of our proposed
model and compare it with competitive baselines. For phrase localization, we
report an improvement of 4.9% (absolute) over the prior state-of-the-art on the
VisualGenome dataset. We also report results that are at par with the
state-of-the-art on the downstream caption-to-image retrieval task on COCO and
Flickr30k datasets.


