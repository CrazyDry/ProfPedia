Identifiability of two-component skew normal mixtures with one known
  component

  We give sufficient identifiability conditions for estimating mixing
proportions in two-component mixtures of skew normal distributions with one
known component. We consider the univariate case as well as two multivariate
extensions: a multivariate skew normal distribution (MSN) by Azzalini and Dalla
Valle (1996) and the canonical fundamental skew normal distribution (CFUSN) by
Arellano-Valle and Genton (2005). The characteristic function of the CFUSN
distribution is additionally derived.


Estimating the class prior and posterior from noisy positives and
  unlabeled data

  We develop a classification algorithm for estimating posterior distributions
from positive-unlabeled data, that is robust to noise in the positive labels
and effective for high-dimensional data. In recent years, several algorithms
have been proposed to learn from positive-unlabeled data; however, many of
these contributions remain theoretical, performing poorly on real
high-dimensional data that is typically contaminated with noise. We build on
this previous work to develop two practical classification algorithms that
explicitly model the noise in the positive labels and utilize univariate
transforms built on discriminative classifiers. We prove that these univariate
transforms preserve the class prior, enabling estimation in the univariate
space and avoiding kernel density estimation for high-dimensional data. The
theoretical development and both parametric and nonparametric algorithms
proposed here constitutes an important step towards wide-spread use of robust
classification algorithms for positive-unlabeled data.


Community-Wide Evaluation of Computational Function Prediction

  A biological experiment is the most reliable way of assigning function to a
protein. However, in the era of high-throughput sequencing, scientists are
unable to carry out experiments to determine the function of every single gene
product. Therefore, to gain insights into the activity of these molecules and
guide experiments, we must rely on computational means to functionally annotate
the majority of sequence data. To understand how well these algorithms perform,
we have established a challenge involving a broad scientific community in which
we evaluate different annotation methods according to their ability to predict
the associations between previously unannotated protein sequences and Gene
Ontology terms. Here we discuss the rationale, benefits and issues associated
with evaluating computational methods in an ongoing community-wide challenge.


Nonparametric semi-supervised learning of class proportions

  The problem of developing binary classifiers from positive and unlabeled data
is often encountered in machine learning. A common requirement in this setting
is to approximate posterior probabilities of positive and negative classes for
a previously unseen data point. This problem can be decomposed into two steps:
(i) the development of accurate predictors that discriminate between positive
and unlabeled data, and (ii) the accurate estimation of the prior probabilities
of positive and negative examples. In this work we primarily focus on the
latter subproblem. We study nonparametric class prior estimation and formulate
this problem as an estimation of mixing proportions in two-component mixture
models, given a sample from one of the components and another sample from the
mixture itself. We show that estimation of mixing proportions is generally
ill-defined and propose a canonical form to obtain identifiability while
maintaining the flexibility to model any distribution. We use insights from
this theory to elucidate the optimization surface of the class priors and
propose an algorithm for estimating them. To address the problems of
high-dimensional density estimation, we provide practical transformations to
low-dimensional spaces that preserve class priors. Finally, we demonstrate the
efficacy of our method on univariate and multivariate data.


Recovering True Classifier Performance in Positive-Unlabeled Learning

  A common approach in positive-unlabeled learning is to train a classification
model between labeled and unlabeled data. This strategy is in fact known to
give an optimal classifier under mild conditions; however, it results in biased
empirical estimates of the classifier performance. In this work, we show that
the typically used performance measures such as the receiver operating
characteristic curve, or the precision-recall curve obtained on such data can
be corrected with the knowledge of class priors; i.e., the proportions of the
positive and negative examples in the unlabeled data. We extend the results to
a noisy setting where some of the examples labeled positive are in fact
negative and show that the correction also requires the knowledge of the
proportion of noisy examples in the labeled positives. Using state-of-the-art
algorithms to estimate the positive class prior and the proportion of noise, we
experimentally evaluate two correction approaches and demonstrate their
efficacy on real-life data.


Classification in biological networks with hypergraphlet kernels

  Biological and cellular systems are often modeled as graphs in which vertices
represent objects of interest (genes, proteins, drugs) and edges represent
relational ties among these objects (binds-to, interacts-with, regulates). This
approach has been highly successful owing to the theory, methodology and
software that support analysis and learning on graphs. Graphs, however, often
suffer from information loss when modeling physical systems due to their
inability to accurately represent multiobject relationships. Hypergraphs, a
generalization of graphs, provide a framework to mitigate information loss and
unify disparate graph-based methodologies. In this paper, we present a
hypergraph-based approach for modeling physical systems and formulate vertex
classification, edge classification and link prediction problems on
(hyper)graphs as instances of vertex classification on (extended, dual)
hypergraphs in a semi-supervised setting. We introduce a novel kernel method on
vertex- and edge-labeled (colored) hypergraphs for analysis and learning. The
method is based on exact and inexact (via hypergraph edit distances)
enumeration of small simple hypergraphs, referred to as hypergraphlets, rooted
at a vertex of interest. We extensively evaluate this method and show its
potential use in a positive-unlabeled setting to estimate the number of missing
and false positive links in protein-protein interaction networks.


A new class of metrics for learning on real-valued and structured data

  We propose a new class of metrics on sets, vectors, and functions that can be
used in various stages of data mining, including exploratory data analysis,
learning, and result interpretation. These new distance functions unify and
generalize some of the popular metrics, such as the Jaccard and bag distances
on sets, Manhattan distance on vector spaces, and Marczewski-Steinhaus distance
on integrable functions. We prove that the new metrics are complete and show
useful relationships with $f$-divergences for probability distributions. To
further extend our approach to structured objects such as concept hierarchies
and ontologies, we introduce information-theoretic metrics on directed acyclic
graphs drawn according to a fixed probability distribution. We conduct
empirical investigation to demonstrate intuitive interpretation of the new
metrics and their effectiveness on real-valued, high-dimensional, and
structured data. Extensive comparative evaluation demonstrates that the new
metrics outperformed multiple similarity and dissimilarity functions
traditionally used in data mining, including the Minkowski family, the
fractional $L^p$ family, two $f$-divergences, cosine distance, and two
correlation coefficients. Finally, we argue that the new class of metrics is
particularly appropriate for rapid processing of high-dimensional and
structured data in distance-based learning.


Ultra High-Dimensional Nonlinear Feature Selection for Big Biological
  Data

  Machine learning methods are used to discover complex nonlinear relationships
in biological and medical data. However, sophisticated learning models are
computationally unfeasible for data with millions of features. Here we
introduce the first feature selection method for nonlinear learning problems
that can scale up to large, ultra-high dimensional biological data. More
specifically, we scale up the novel Hilbert-Schmidt Independence Criterion
Lasso (HSIC Lasso) to handle millions of features with tens of thousand
samples. The proposed method is guaranteed to find an optimal subset of
maximally predictive features with minimal redundancy, yielding higher
predictive power and improved interpretability. Its effectiveness is
demonstrated through applications to classify phenotypes based on module
expression in human prostate cancer patients and to detect enzymes among
protein structures. We achieve high accuracy with as few as 20 out of one
million features --- a dimensionality reduction of 99.998%. Our algorithm can
be implemented on commodity cloud computing platforms. The dramatic reduction
of features may lead to the ubiquitous deployment of sophisticated prediction
models in mobile health care applications.


Enumerating consistent subgraphs of directed acyclic graphs: an insight
  into biomedical ontologies

  Modern problems of concept annotation associate an object of interest (gene,
individual, text document) with a set of interrelated textual descriptors
(functions, diseases, topics), often organized in concept hierarchies or
ontologies. Most ontologies can be seen as directed acyclic graphs, where nodes
represent concepts and edges represent relational ties between these concepts.
Given an ontology graph, each object can only be annotated by a consistent
subgraph; that is, a subgraph such that if an object is annotated by a
particular concept, it must also be annotated by all other concepts that
generalize it. Ontologies therefore provide a compact representation of a large
space of possible consistent subgraphs; however, until now we have not been
aware of a practical algorithm that can enumerate such annotation spaces for a
given ontology. In this work we propose an algorithm for enumerating consistent
subgraphs of directed acyclic graphs. The algorithm recursively partitions the
graph into strictly smaller graphs until the resulting graph becomes a rooted
tree (forest), for which a linear-time solution is computed. It then combines
the tallies from graphs created in the recursion to obtain the final count. We
prove the correctness of this algorithm and then apply it to characterize four
major biomedical ontologies. We believe this work provides valuable insights
into concept annotation spaces and predictability of ontological annotation.


Uncovering protein interaction in abstracts and text using a novel
  linear model and word proximity networks

  We participated in three of the protein-protein interaction subtasks of the
Second BioCreative Challenge: classification of abstracts relevant for
protein-protein interaction (IAS), discovery of protein pairs (IPS) and text
passages characterizing protein interaction (ISS) in full text documents. We
approached the abstract classification task with a novel, lightweight linear
model inspired by spam-detection techniques, as well as an uncertainty-based
integration scheme. We also used a Support Vector Machine and the Singular
Value Decomposition on the same features for comparison purposes. Our approach
to the full text subtasks (protein pair and passage identification) includes a
feature expansion method based on word-proximity networks. Our approach to the
abstract classification task (IAS) was among the top submissions for this task
in terms of the measures of performance used in the challenge evaluation
(accuracy, F-score and AUC). We also report on a web-tool we produced using our
approach: the Protein Interaction Abstract Relevance Evaluator (PIARE). Our
approach to the full text tasks resulted in one of the highest recall rates as
well as mean reciprocal rank of correct passages. Our approach to abstract
classification shows that a simple linear model, using relatively few features,
is capable of generalizing and uncovering the conceptual nature of
protein-protein interaction from the bibliome. Since the novel approach is
based on a very lightweight linear model, it can be easily ported and applied
to similar problems. In full text problems, the expansion of word features with
word-proximity networks is shown to be useful, though the need for some
improvements is discussed.


The sequencing and interpretation of the genome obtained from a Serbian
  individual

  Recent genetic studies and whole-genome sequencing projects have greatly
improved our understanding of human variation and clinically actionable genetic
information. Smaller ethnic populations, however, remain underrepresented in
both individual and large-scale sequencing efforts and hence present an
opportunity to discover new variants of biomedical and demographic
significance. This report describes the sequencing and analysis of a genome
obtained from an individual of Serbian origin, introducing tens of thousands of
previously unknown variants to the currently available pool. Ancestry analysis
places this individual in close proximity of the Central and Eastern European
populations; i.e., closest to Croatian, Bulgarian and Hungarian individuals
and, in terms of other Europeans, furthest from Ashkenazi Jewish, Spanish,
Sicilian, and Baltic individuals. Our analysis confirmed gene flow between
Neanderthal and ancestral pan-European populations, with similar contributions
to the Serbian genome as those observed in other European groups. Finally, to
assess the burden of potentially disease-causing/clinically relevant variation
in the sequenced genome, we utilized manually curated genotype-phenotype
association databases and variant-effect predictors. We identified several
variants that have previously been associated with severe early-onset disease
that is not evident in the proband, as well as variants that could yet prove to
be clinically relevant to the proband over the next decades. The presence of
numerous private and low-frequency variants along with the observed and
predicted disease-causing mutations in this genome exemplify some of the global
challenges of genome interpretation, especially in the context of understudied
ethnic groups.


An expanded evaluation of protein function prediction methods shows an
  improvement in accuracy

  Background: The increasing volume and variety of genotypic and phenotypic
data is a major defining characteristic of modern biomedical sciences. At the
same time, the limitations in technology for generating data and the inherently
stochastic nature of biomolecular events have led to the discrepancy between
the volume of data and the amount of knowledge gleaned from it. A major
bottleneck in our ability to understand the molecular underpinnings of life is
the assignment of function to biological macromolecules, especially proteins.
While molecular experiments provide the most reliable annotation of proteins,
their relatively low throughput and restricted purview have led to an
increasing role for computational function prediction. However, accurately
assessing methods for protein function prediction and tracking progress in the
field remain challenging. Methodology: We have conducted the second Critical
Assessment of Functional Annotation (CAFA), a timed challenge to assess
computational methods that automatically assign protein function. One hundred
twenty-six methods from 56 research groups were evaluated for their ability to
predict biological functions using the Gene Ontology and gene-disease
associations using the Human Phenotype Ontology on a set of 3,681 proteins from
18 species. CAFA2 featured significantly expanded analysis compared with CAFA1,
with regards to data set size, variety, and assessment metrics. To review
progress in the field, the analysis also compared the best methods
participating in CAFA1 to those of CAFA2. Conclusions: The top performing
methods in CAFA2 outperformed the best methods from CAFA1, demonstrating that
computational function prediction is improving. This increased accuracy can be
attributed to the combined effect of the growing number of experimental
annotations and improved methods for function prediction.


