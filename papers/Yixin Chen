On Feedback Vertex Set: New Measure and New Structures

  We present a new parameterized algorithm for the {feedback vertex set}problem ({\sc fvs}) on undirected graphs. We approach the problem byconsidering a variation of it, the {disjoint feedback vertex set} problem ({\scdisjoint-fvs}), which finds a feedback vertex set of size $k$ that has nooverlap with a given feedback vertex set $F$ of the graph $G$. We develop animproved kernelization algorithm for {\sc disjoint-fvs} and show that {\scdisjoint-fvs} can be solved in polynomial time when all vertices in $G\setminus F$ have degrees upper bounded by three. We then propose a newbranch-and-search process on {\sc disjoint-fvs}, and introduce a newbranch-and-search measure. The process effectively reduces a given graph to agraph on which {\sc disjoint-fvs} becomes polynomial-time solvable, and the newmeasure more accurately evaluates the efficiency of the process. Thesealgorithmic and combinatorial studies enable us to develop an$O^*(3.83^k)$-time parameterized algorithm for the general {\sc fvs} problem,improving all previous algorithms for the problem.

Cluster Editing: Kernelization based on Edge Cuts

  Kernelization algorithms for the {\sc cluster editing} problem have been apopular topic in the recent research in parameterized computation. Thus farmost kernelization algorithms for this problem are based on the concept of {\itcritical cliques}. In this paper, we present new observations and newtechniques for the study of kernelization algorithms for the {\sc clusterediting} problem. Our techniques are based on the study of the relationshipbetween {\sc cluster editing} and graph edge-cuts. As an application, wepresent an ${\cal O}(n^2)$-time algorithm that constructs a $2k$ kernel for the{\it weighted} version of the {\sc cluster editing} problem. Our result meetsthe best kernel size for the unweighted version for the {\sc cluster editing}problem, and significantly improves the previous best kernel of quadratic sizefor the weighted version of the problem.

FAST: Kernelization based on Graph Modular Decomposition

  Kernelization algorithms, usually a preprocessing step before other moretraditional algorithms, are very special in the sense that they return(reduced) instances, instead of final results. This characteristic excludes thefreedom of applying a kernelization algorithm for the weighted version of aproblem to its unweighted instances. Thus with only very few special cases,kernelization algorithms have to be studied separately for weigthed andunweighted versions of a single problem. {\sc feedback arc set on tournament}is currently a very popular problem in recent research of parameterized, aswell as approximation computation, and its wide applications in many areas makeit appear in all top conferences. The theory of graph modular decompositions isa general approach in the study of graph structures, which only had itssurfaces touched in previous work on kernelization algorithms of {\sc feedbackarc set on tournament}. In this paper, we study further properties of graphmodular decompositions and apply them to obtain the first linear kernel for theunweighted {\sc feedback arc set on tournament} problem, which only admitslinear kernel in its weighted version, while quadratic kernel for theunweighted.

A $2k$-Vertex Kernel for Maximum Internal Spanning Tree

  We consider the parameterized version of the maximum internal spanning treeproblem, which, given an $n$-vertex graph and a parameter $k$, asks for aspanning tree with at least $k$ internal vertices. Fomin et al. [J. Comput.System Sci., 79:1-6] crafted a very ingenious reduction rule, and showed that asimple application of this rule is sufficient to yield a $3k$-vertex kernel.Here we propose a novel way to use the same reduction rule, resulting in animproved $2k$-vertex kernel. Our algorithm applies first a greedy procedureconsisting of a sequence of local exchange operations, which ends with alocal-optimal spanning tree, and then uses this special tree to find areducible structure. As a corollary of our kernel, we obtain a deterministicalgorithm for the problem running in time $4^k \cdot n^{O(1)}$.

Joint Representation Learning of Cross-lingual Words and Entities via  Attentive Distant Supervision

  Joint representation learning of words and entities benefits many NLP tasks,but has not been well explored in cross-lingual settings. In this paper, wepropose a novel method for joint representation learning of cross-lingual wordsand entities. It captures mutually complementary knowledge, and enablescross-lingual inferences among knowledge bases and texts. Our method does notrequire parallel corpora, and automatically generates comparable data viadistant supervision using multi-lingual knowledge bases. We utilize two typesof regularizers to align cross-lingual words and entities, and design knowledgeattention and cross-lingual attention to further reduce noises. We conducted aseries of experiments on three tasks: word translation, entity relatedness, andcross-lingual entity linking. The results, both qualitatively andquantitatively, demonstrate the significance of our method.

A new Gini correlation between quantitative and qualitative variables

  We propose a new Gini correlation to measure dependence between a categoricaland numerical variables. Analogous to Pearson $R^2$ in ANOVA model, the Ginicorrelation is interpreted as the ratio of the between-group variation and thetotal variation, but it characterizes independence (zero Gini correlationmutually implies independence). Closely related to the distance correlation,the Gini correlation is of simple formulation by considering the nature ofcategorical variable. As a result, the proposed Gini correlation has a lowercomputational cost than the distance correlation and is more straightforward toperform inference. Simulation and real applications are conducted todemonstrate the advantages.

An $O^*(1.84^k)$ Parameterized Algorithm for the Multiterminal Cut  Problem

  We study the \emph{multiterminal cut} problem, which, given an $n$-vertexgraph whose edges are integer-weighted and a set of terminals, asks for apartition of the vertex set such that each terminal is in a distinct part, andthe total weight of crossing edges is at most $k$. Our weapons shall be twoclassical results known for decades: \emph{maximum volume minimum ($s,t$)-cuts}by [Ford and Fulkerson, \emph{Flows in Networks}, 1962] and \emph{isolatingcuts} by [Dahlhaus et al., \emph{SIAM J. Comp.} 23(4):864-894, 1994]. Wesharpen these old weapons with the help of submodular functions, and apply themto this problem, which enable us to design a more elaborated branching schemeon deciding whether a non-terminal vertex is with a terminal or not. Thisbounded search tree algorithm can be shown to run in $1.84^k\cdot n^{O(1)}$time, thereby breaking the $2^k\cdot n^{O(1)}$ barrier. As a by-product, itgives a $1.36^k\cdot n^{O(1)}$ time algorithm for $3$-terminal cut. Thepreprocessing applied on non-terminal vertices might be of use for study ofthis problem from other aspects.

Combining Fact Extraction and Verification with Neural Semantic Matching  Networks

  The increasing concern with misinformation has stimulated research efforts onautomatic fact checking. The recently-released FEVER dataset introduced abenchmark fact-verification task in which a system is asked to verify a claimusing evidential sentences from Wikipedia documents. In this paper, we presenta connected system consisting of three homogeneous neural semantic matchingmodels that conduct document retrieval, sentence selection, and claimverification jointly for fact extraction and verification. For evidenceretrieval (document retrieval and sentence selection), unlike traditionalvector space IR models in which queries and sources are matched in somepre-designed term vector space, we develop neural models to perform deepsemantic matching from raw textual input, assuming no intermediate termrepresentation and no access to structured external knowledge bases. We alsoshow that Pageview frequency can also help improve the performance of evidenceretrieval results, that later can be matched by using our neural semanticmatching network. For claim verification, unlike previous approaches thatsimply feed upstream retrieved evidence and the claim to a natural languageinference (NLI) model, we further enhance the NLI model by providing it withinternal semantic relatedness scores (hence integrating it with the evidenceretrieval modules) and ontological WordNet features. Experiments on the FEVERdataset indicate that (1) our neural semantic matching method outperformspopular TF-IDF and encoder models, by significant margins on all evidenceretrieval metrics, (2) the additional relatedness score and WordNet featuresimprove the NLI model via better semantic awareness, and (3) by formalizing allthree subtasks as a similar semantic matching problem and improving on allthree stages, the complete model is able to achieve the state-of-the-artresults on the FEVER test set.

Compressing Neural Networks with the Hashing Trick

  As deep nets are increasingly used in applications suited for mobile devices,a fundamental dilemma becomes apparent: the trend in deep learning is to growmodels to absorb ever-increasing data set sizes; however mobile devices aredesigned with very little memory and cannot store such large models. We presenta novel network architecture, HashedNets, that exploits inherent redundancy inneural networks to achieve drastic reductions in model sizes. HashedNets uses alow-cost hash function to randomly group connection weights into hash buckets,and all connections within the same hash bucket share a single parameter value.These parameters are tuned to adjust to the HashedNets weight sharingarchitecture with standard backprop during training. Our hashing procedureintroduces no additional memory overhead, and we demonstrate on severalbenchmark data sets that HashedNets shrink the storage requirements of neuralnetworks substantially while mostly preserving generalization performance.

Compressing Convolutional Neural Networks

  Convolutional neural networks (CNN) are increasingly used in many areas ofcomputer vision. They are particularly attractive because of their ability to"absorb" great quantities of labeled data through millions of parameters.However, as model sizes increase, so do the storage and memory requirements ofthe classifiers. We present a novel network architecture, Frequency-SensitiveHashed Nets (FreshNets), which exploits inherent redundancy in bothconvolutional layers and fully-connected layers of a deep learning model,leading to dramatic savings in memory and storage consumption. Based on the keyobservation that the weights of learned convolutional filters are typicallysmooth and low-frequency, we first convert filter weights to the frequencydomain with a discrete cosine transform (DCT) and use a low-cost hash functionto randomly group frequency parameters into hash buckets. All parametersassigned the same hash bucket share a single value learned with standardback-propagation. To further reduce model size we allocate fewer hash bucketsto high-frequency components, which are generally less important. We evaluateFreshNets on eight data sets, and show that it leads to drastically bettercompressed performance than several relevant baselines.

SAS+ Planning as Satisfiability

  Planning as satisfiability is a principal approach to planning with manyeminent advantages. The existing planning as satisfiability techniques usuallyuse encodings compiled from STRIPS. We introduce a novel SAT encoding scheme(SASE) based on the SAS+ formalism. The new scheme exploits the structuralinformation in SAS+, resulting in an encoding that is both more compact andefficient for planning. We prove the correctness of the new encoding byestablishing an isomorphism between the solution plans of SASE and that ofSTRIPS based encodings. We further analyze the transition variables newlyintroduced in SASE to explain why it accommodates modern SAT solving algorithmsand improves performance. We give empirical statistical results to support ouranalysis. We also develop a number of techniques to further reduce the encodingsize of SASE, and conduct experimental studies to show the strength of eachindividual technique. Finally, we report extensive experimental results todemonstrate significant improvements of SASE over the state-of-the-art STRIPSbased encoding schemes in terms of both time and memory efficiency.

Structural and Topological Nature of Plasticity in Sheared Granular  Materials

  Upon mechanical loading, granular materials yield and undergo plasticdeformation. The nature of plastic deformation is essential for the developmentof the macroscopic constitutive models and the understanding of shear bandformation. However, we still do not fully understand the microscopic nature ofplastic deformation in disordered granular materials. Here we used synchrotronX-ray tomography technique to track the structural evolutions ofthree-dimensional granular materials under shear. We establish that highlydistorted coplanar tetrahedra are the structural defects responsible formicroscopic plasticity in disordered granular packings. The elementary plasticevents occur through flip events which correspond to a neighbor switchingprocess among these coplanar tetrahedra (or equivalently as the rotation motionof 4-ring disclinations). These events are discrete in space and possessspecific orientations with the principal stress direction.

An Efficient L-Shape Fitting Method for Vehicle Pose Detection with 2D  LiDAR

  Detecting vehicles with strong robustness and high efficiency has become oneof the key capabilities of fully autonomous driving cars. This topic hasalready been widely studied by GPU-accelerated deep learning approaches usingimage sensors and 3D LiDAR, however, few studies seek to address it with ahorizontally mounted 2D laser scanner. 2D laser scanner is equipped on almostevery autonomous vehicle for its superiorities in the field of view, lightinginvariance, high accuracy and relatively low price. In this paper, we propose ahighly efficient search-based L-Shape fitting algorithm for detecting positionsand orientations of vehicles with a 2D laser scanner. Differing from theapproach to formulating LShape fitting as a complex optimization problem, ourmethod decomposes the L-Shape fitting into two steps: L-Shape vertexessearching and L-Shape corner localization. Our approach is computationallyefficient due to its minimized complexity. In on-road experiments, our approachis capable of adapting to various circumstances with high efficiency androbustness.

Generalized Second Price Auction with Probabilistic Broad Match

  Generalized Second Price (GSP) auctions are widely used by search enginestoday to sell their ad slots. Most search engines have supported broad matchbetween queries and bid keywords when executing GSP auctions, however, it hasbeen revealed that GSP auction with the standard broad-match mechanism they arecurrently using (denoted as SBM-GSP) has several theoretical drawbacks (e.g.,its theoretical properties are known only for the single-slot case andfull-information setting, and even in this simple setting, the correspondingworst-case social welfare can be rather bad). To address this issue, we proposea novel broad-match mechanism, which we call the Probabilistic Broad-Match(PBM) mechanism. Different from SBM that puts together the ads bidding on allthe keywords matched to a given query for the GSP auction, the GSP with PBM(denoted as PBM-GSP) randomly samples a keyword according to a predefinedprobability distribution and only runs the GSP auction for the ads bidding onthis sampled keyword. We perform a comprehensive study on the theoreticalproperties of the PBM-GSP. Specifically, we study its social welfare in theworst equilibrium, in both full-information and Bayesian settings. The resultsshow that PBM-GSP can generate larger welfare than SBM-GSP under mildconditions. Furthermore, we also study the revenue guarantee for PBM-GSP inBayesian setting. To the best of our knowledge, this is the first work onbroad-match mechanisms for GSP that goes beyond the single-slot case and thefull-information setting.

A Reduction of the Elastic Net to Support Vector Machines with an  Application to GPU Computing

  The past years have witnessed many dedicated open-source projects that builtand maintain implementations of Support Vector Machines (SVM), parallelized forGPU, multi-core CPUs and distributed systems. Up to this point, no comparableeffort has been made to parallelize the Elastic Net, despite its popularity inmany high impact applications, including genetics, neuroscience and systemsbiology. The first contribution in this paper is of theoretical nature. Weestablish a tight link between two seemingly different algorithms and provethat Elastic Net regression can be reduced to SVM with squared hinge lossclassification. Our second contribution is to derive a practical algorithmbased on this reduction. The reduction enables us to utilize prior efforts inspeeding up and parallelizing SVMs to obtain a highly optimized and parallelsolver for the Elastic Net and Lasso. With a simple wrapper, consisting of only11 lines of MATLAB code, we obtain an Elastic Net implementation that naturallyutilizes GPU and multi-core CPUs. We demonstrate on twelve real world datasets, that our algorithm yields identical results as the popular (and highlyoptimized) glmnet implementation but is one or several orders of magnitudefaster.

Multi-Scale Convolutional Neural Networks for Time Series Classification

  Time series classification (TSC), the problem of predicting class labels oftime series, has been around for decades within the community of data miningand machine learning, and found many important applications such as biomedicalengineering and clinical prediction. However, it still remains challenging andfalls short of classification accuracy and efficiency. Traditional approachestypically involve extracting discriminative features from the original timeseries using dynamic time warping (DTW) or shapelet transformation, based onwhich an off-the-shelf classifier can be applied. These methods are ad-hoc andseparate the feature extraction part with the classification part, which limitstheir accuracy performance. Plus, most existing methods fail to take intoaccount the fact that time series often have features at different time scales.To address these problems, we propose a novel end-to-end neural network model,Multi-Scale Convolutional Neural Networks (MCNN), which incorporates featureextraction and classification in a single framework. Leveraging a novelmulti-branch layer and learnable convolutional layers, MCNN automaticallyextracts features at different scales and frequencies, leading to superiorfeature representation. MCNN is also computationally efficient, as it naturallyleverages GPU computing. We conduct comprehensive empirical evaluation withvarious existing methods on a large number of benchmark datasets, and show thatMCNN advances the state-of-the-art by achieving superior accuracy performancethan other leading methods.

Theory and Algorithms for Partial Order Based Reduction in Planning

  Search is a major technique for planning. It amounts to exploring a statespace of planning domains typically modeled as a directed graph. However,prohibitively large sizes of the search space make search expensive. Developingbetter heuristic functions has been the main technique for improving searchefficiency. Nevertheless, recent studies have shown that improving heuristicsalone has certain fundamental limits on improving search efficiency. Recently,a new direction of research called partial order based reduction (POR) has beenproposed as an alternative to improving heuristics. POR has shown promise inspeeding up searches.  POR has been extensively studied in model checking research and is a keyenabling technique for scalability of model checking systems. Although the PORtheory has been extensively studied in model checking, it has never beendeveloped systematically for planning before. In addition, the conditions forPOR in the model checking theory are abstract and not directly applicable inplanning. Previous works on POR algorithms for planning did not establish theconnection between these algorithms and existing theory in model checking.  In this paper, we develop a theory for POR in planning. The new theory wedevelop connects the stubborn set theory in model checking and POR methods inplanning. We show that previous POR algorithms in planning can be explained bythe new theory. Based on the new theory, we propose a new, stronger PORalgorithm. Experimental results on various planning domains show further searchcost reduction using the new algorithm.

Recovering Metabolic Networks using A Novel Hyperlink Prediction Method

  Studying metabolic networks is vital for many areas such as novel drugs andbio-fuels. For biologists, a key challenge is that many reactions areimpractical or expensive to be found through experiments. Our task is torecover the missing reactions. By exploiting the problem structure, we modelreaction recovery as a hyperlink prediction problem, where each reaction isregarded as a hyperlink connecting its participating vertices (metabolites).Different from the traditional link prediction problem where two nodes form alink, a hyperlink can involve an arbitrary number of nodes. Since thecardinality of a hyperlink is variable, existing classifiers based on a fixednumber of input features become infeasible. Traditional methods, such as commonneighbors and Katz index, are not applicable either, since they are restrictedto pairwise similarities. In this paper, we propose a novel hyperlinkprediction algorithm, called Matrix Boosting (MATBoost). MATBoost conductsinference jointly in the incidence space and adjacency space by performing aniterative completion-matching optimization. We carry out extensive experimentsto show that MATBoost achieves state-of-the-art performance. For a metabolicnetwork with 1805 metabolites and 2583 reactions, our algorithm cansuccessfully recover nearly 200 reactions out of 400 missing reactions.

Visually Explainable Recommendation

  Images account for a significant part of user decisions in many applicationscenarios, such as product images in e-commerce, or user image posts in socialnetworks. It is intuitive that user preferences on the visual patterns of image(e.g., hue, texture, color, etc) can be highly personalized, and this providesus with highly discriminative features to make personalized recommendations.  Previous work that takes advantage of images for recommendation usuallytransforms the images into latent representation vectors, which are adopted bya recommendation component to assist personalized user/item profiling andrecommendation. However, such vectors are hardly useful in terms of providingvisual explanations to users about why a particular item is recommended, andthus weakens the explainability of recommendation systems.  As a step towards explainable recommendation models, we propose visuallyexplainable recommendation based on attentive neural networks to model the userattention on images, under the supervision of both implicit feedback andtextual reviews. By this, we can not only provide recommendation results to theusers, but also tell the users why an item is recommended by providingintuitive visual highlights in a personalized manner. Experimental results showthat our models are not only able to improve the recommendation performance,but also can provide persuasive visual explanations for the users to take therecommendations.

Link Prediction Based on Graph Neural Networks

  Link prediction is a key problem for network-structured data. Link predictionheuristics use some score functions, such as common neighbors and Katz index,to measure the likelihood of links. They have obtained wide practical uses dueto their simplicity, interpretability, and for some of them, scalability.However, every heuristic has a strong assumption on when two nodes are likelyto link, which limits their effectiveness on networks where these assumptionsfail. In this regard, a more reasonable way should be learning a suitableheuristic from a given network instead of using predefined ones. By extractinga local subgraph around each target link, we aim to learn a function mappingthe subgraph patterns to link existence, thus automatically learning a`heuristic' that suits the current network. In this paper, we study thisheuristic learning paradigm for link prediction. First, we develop a novel$\gamma$-decaying heuristic theory. The theory unifies a wide range ofheuristics in a single framework, and proves that all these heuristics can bewell approximated from local subgraphs. Our results show that local subgraphsreserve rich information related to link existence. Second, based on the$\gamma$-decaying theory, we propose a new algorithm to learn heuristics fromlocal subgraphs using a graph neural network (GNN). Its experimental resultsshow unprecedented performance, working consistently well on a wide range ofproblems.

Dynamical quantum phase transition for mixed states in open systems

  Based on a kinematic approach in defining a geometric phase for a densitymatrix, we define the generalized Loschmidt overlap amplitude (GLOA) for anopen system for arbitrary quantum evolution. The GLOA reduces to the Loschmidtoverlap amplitude (LOA) with a modified dynamic phase for unitary evolution ofa pure state, with the argument of the GLOA well-defined by the geometricphase, thus possessing similar physical interpretation to that of the LOA. Therate function for the GLOA exhibits non-analyticity at a critical time, whichcorresponds to the dynamical quantum phase transition. We observe that thedynamical quantum phase transition related to GLOA is not destroyed under afinite temperature and weak enough dissipation. In particular, we find that anew type of dynamical quantum phase transition emerges in a dissipation system.The proposed GLOA provides a powerful tool in the investigation of a dynamicalquantum phase transition in an arbitrary quantum system, which not only cancharacterize the robustness of the dynamical quantum phase transition but alsocan be used to search for new transitions.

Segmentation of Levator Hiatus Using Multi-Scale Local Region Active  contours and Boundary Shape Similarity Constraint

  In this paper, a multi-scale framework with local region based active contourand boundary shape similarity constraint is proposed for the segmentation oflevator hiatus in ultrasound images. In this paper, we proposed a multiscaleactive contour framework to segment levator hiatus ultrasound images bycombining the local region information and boundary shape similarityconstraint. In order to get more precisely initializations and reduce thecomputational cost, Gaussian pyramid method is used to decompose the image intocoarse-to-fine scales. A localized region active contour model is firstlyperformed on the coarsest scale image to get a rough contour of the levatorhiatus, then the segmentation result on the coarse scale is interpolated intothe finer scale image as the initialization. The boundary shape similaritybetween different scales is incorporate into the local region based activecontour model so that the result from coarse scale can guide the contourevolution at finer scale. By incorporating the multi-scale and boundary shapesimilarity, the proposed method can precisely locate the levator hiatusboundaries despite various ultrasound image artifacts. With a data set of 90levator hiatus ultrasound images, the efficiency and accuracy of the proposedmethod are validated by quantitative and qualitative evaluations (TP, FP, Js)and comparison with other two state-of-art active contour segmentation methods(C-V model, DRLSE model).

Extracting Actionability from Machine Learning Models by Sub-optimal  Deterministic Planning

  A main focus of machine learning research has been improving thegeneralization accuracy and efficiency of prediction models. Many models suchas SVM, random forest, and deep neural nets have been proposed and achievedgreat success. However, what emerges as missing in many applications isactionability, i.e., the ability to turn prediction results into actions. Forexample, in applications such as customer relationship management, clinicalprediction, and advertisement, the users need not only accurate prediction, butalso actionable instructions which can transfer an input to a desirable goal(e.g., higher profit repays, lower morbidity rates, higher ads hit rates).Existing effort in deriving such actionable knowledge is few and limited tosimple action models which restricted to only change one attribute for eachaction. The dilemma is that in many real applications those action models areoften more complex and harder to extract an optimal solution.  In this paper, we propose a novel approach that achieves actionability bycombining learning with planning, two core areas of AI. In particular, wepropose a framework to extract actionable knowledge from random forest, one ofthe most widely used and best off-the-shelf classifiers. We formulate theactionability problem to a sub-optimal action planning (SOAP) problem, which isto find a plan to alter certain features of a given input so that the randomforest would yield a desirable output, while minimizing the total costs ofactions. Technically, the SOAP problem is formulated in the SAS+ planningformalism, and solved using a Max-SAT based approach. Our experimental resultsdemonstrate the effectiveness and efficiency of the proposed approach on apersonal credit dataset and other benchmarks. Our work represents a newapplication of automated planning on an emerging and challenging machinelearning paradigm.

