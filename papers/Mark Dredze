Twitter as a Source of Global Mobility Patterns for Social Good

  Data on human spatial distribution and movement is essential forunderstanding and analyzing social systems. However existing sources for thisdata are lacking in various ways; difficult to access, biased, have poorgeographical or temporal resolution, or are significantly delayed. In thispaper, we describe how geolocation data from Twitter can be used to estimateglobal mobility patterns and address these shortcomings. These findings willinform how this novel data source can be harnessed to address humanitarian anddevelopment efforts.

Estimating Confusions in the ASR Channel for Improved Topic-based  Language Model Adaptation

  Human language is a combination of elemental languages/domains/styles thatchange across and sometimes within discourses. Language models, which play acrucial role in speech recognizers and machine translation systems, areparticularly sensitive to such changes, unless some form of adaptation takesplace. One approach to speech language model adaptation is self-training, inwhich a language model's parameters are tuned based on automaticallytranscribed audio. However, transcription errors can misguide self-training,particularly in challenging settings such as conversational speech. In thiswork, we propose a model that considers the confusions (errors) of the ASRchannel. By modeling the likely confusions in the ASR output instead of usingjust the 1-best, we improve self-training efficacy by obtaining a more reliablereference transcription estimate. We demonstrate improved topic-based languagemodeling adaptation results over both 1-best and lattice self-training usingour ASR channel confusion estimates on telephone conversations.

Harmonic Grammar, Optimality Theory, and Syntax Learnability: An  Empirical Exploration of Czech Word Order

  This work presents a systematic theoretical and empirical comparison of themajor algorithms that have been proposed for learning Harmonic and OptimalityTheory grammars (HG and OT, respectively). By comparing learning algorithms, weare also able to compare the closely related OT and HG frameworks themselves.Experimental results show that the additional expressivity of the HG frameworkover OT affords performance gains in the task of predicting the surface wordorder of Czech sentences. We compare the perceptron with the classic GradualLearning Algorithm (GLA), which learns OT grammars, as well as the popularMaximum Entropy model. In addition to showing that the perceptron istheoretically appealing, our work shows that the performance of the HG model itlearns approaches that of the upper bound in prediction accuracy on a held outtest set and that it is capable of accurately modeling observed variation.

Interactive Knowledge Base Population

  Most work on building knowledge bases has focused on collecting entities andfacts from as large a collection of documents as possible. We argue for anddescribe a new paradigm where the focus is on a high-recall extraction over asmall collection of documents under the supervision of a human expert, that wecall Interactive Knowledge Base Population (IKBP).

Feature Generation for Robust Semantic Role Labeling

  Hand-engineered feature sets are a well understood method for creating robustNLP models, but they require a lot of expertise and effort to create. In thiswork we describe how to automatically generate rich feature sets from simpleunits called featlets, requiring less engineering. Using information gain toguide the generation process, we train models which rival the state of the arton two standard Semantic Role Labeling datasets with almost no task orlinguistic insight.

Challenges of Using Text Classifiers for Causal Inference

  Causal understanding is essential for many kinds of decision-making, butcausal inference from observational data has typically only been applied tostructured, low-dimensional datasets. While text classifiers producelow-dimensional outputs, their use in causal inference has not previously beenstudied. To facilitate causal analyses based on language data, we consider therole that text classifiers can play in causal inference through establishedmodeling mechanisms from the causality literature on missing data andmeasurement error. We demonstrate how to conduct causal analyses using textclassifiers on simulated and Yelp data, and discuss the opportunities andchallenges of future work that uses text data in causal inference.

Improved Relation Extraction with Feature-Rich Compositional Embedding  Models

  Compositional embedding models build a representation (or embedding) for alinguistic structure based on its component word embeddings. We propose aFeature-rich Compositional Embedding Model (FCM) for relation extraction thatis expressive, generalizes to new domains, and is easy-to-implement. The keyidea is to combine both (unlexicalized) hand-crafted features with learned wordembeddings. The model is able to directly tackle the difficulties met bytraditional compositional embeddings models, such as handling arbitrary typesof sentence annotations and utilizing global information for composition. Wetest the proposed model on two relation extraction tasks, and demonstrate thatour model outperforms both previous compositional models and traditionalfeature rich models on the ACE 2005 relation extraction task, and the SemEval2010 relation classification task. The combination of our model and alog-linear classifier with hand-crafted features gives state-of-the-artresults.

Approximation-Aware Dependency Parsing by Belief Propagation

  We show how to train the fast dependency parser of Smith and Eisner (2008)for improved accuracy. This parser can consider higher-order interactions amongedges while retaining O(n^3) runtime. It outputs the parse with maximumexpected recall -- but for speed, this expectation is taken under a posteriordistribution that is constructed only approximately, using loopy beliefpropagation through structured factors. We show how to adjust the modelparameters to compensate for the errors introduced by this approximation, byfollowing the gradient of the actual loss on training data. We find thisgradient by back-propagation. That is, we treat the entire parser(approximations and all) as a differentiable circuit, as Stoyanov et al. (2011)and Domke (2010) did for loopy CRFs. The resulting trained parser obtainshigher accuracy with fewer iterations of belief propagation than one trained byconditional log-likelihood.

Improving Named Entity Recognition for Chinese Social Media with Word  Segmentation Representation Learning

  Named entity recognition, and other information extraction tasks, frequentlyuse linguistic features such as part of speech tags or chunkings. For languageswhere word boundaries are not readily identified in text, word segmentation isa key first step to generating features for an NER system. While using wordboundary tags as features are helpful, the signals that aid in identifyingthese boundaries may provide richer information for an NER system. Newstate-of-the-art word segmentation systems use neural models to learnrepresentations for predicting word boundaries. We show that these samerepresentations, jointly trained with an NER system, yield significantimprovements in NER for Chinese social media. In our experiments, jointlytraining NER and word segmentation with an LSTM-CRF model yields nearly 5%absolute improvement over previously published results.

Embedding Lexical Features via Low-Rank Tensors

  Modern NLP models rely heavily on engineered features, which often combineword and contextual information into complex lexical features. Such combinationresults in large numbers of features, which can lead to over-fitting. Wepresent a new model that represents complex lexical features---comprised ofparts for words, contextual information and labels---in a tensor that capturesconjunction information among these parts. We apply low-rank tensorapproximations to the corresponding parameter tensors to reduce the parameterspace and improve prediction speed. Furthermore, we investigate two methods forhandling features that include $n$-grams of mixed lengths. Our model achievesstate-of-the-art results on tasks in relation extraction, PP-attachment, andpreposition disambiguation.

Multi-task Domain Adaptation for Sequence Tagging

  Many domain adaptation approaches rely on learning cross domain sharedrepresentations to transfer the knowledge learned in one domain to otherdomains. Traditional domain adaptation only considers adapting for one task. Inthis paper, we explore multi-task representation learning under the domainadaptation scenario. We propose a neural network framework that supports domainadaptation for multiple tasks simultaneously, and learns shared representationsthat better generalize for domain adaptation. We apply the proposed frameworkto domain adaptation for sequence tagging problems considering two tasks:Chinese word segmentation and named entity recognition. Experiments show thatmulti-task domain adaptation works better than disjoint domain adaptation foreach task, and achieves the state-of-the-art results for both tasks in thesocial media domain.

After Sandy Hook Elementary: A Year in the Gun Control Debate on Twitter

  The mass shooting at Sandy Hook elementary school on December 14, 2012catalyzed a year of active debate and legislation on gun control in the UnitedStates. Social media hosted an active public discussion where people expressedtheir support and opposition to a variety of issues surrounding gunlegislation. In this paper, we show how a content-based analysis of Twitterdata can provide insights and understanding into this debate. We estimate therelative support and opposition to gun control measures, along with a topicanalysis of each camp by analyzing over 70 million gun-related tweets from2013. We focus on spikes in conversation surrounding major events related toguns throughout the year. Our general approach can be applied to otherimportant public health and political issues to analyze the prevalence andnature of public opinion.

Can Big Media Data Revolutionarize Gun Violence Prevention?

  The scientific method drives improvements in public health, but a strategy ofobstructionism has impeded scientists from gathering even a minimal amount ofinformation to address America's gun violence epidemic. We argue that in spiteof a lack of federal investment, large amounts of publicly available data offerscientists an opportunity to measure a range of firearm-related behaviors.Given the diversity of available data - including news coverage, social media,web forums, online advertisements, and Internet searches (to name a few) -there are ample opportunities for scientists to study everything from trends inparticular types of gun violence to gun-related behaviors (such as purchasesand safety practices) to public understanding of and sentiment towards variousgun violence reduction measures. Science has been sidelined in the gun violencedebate for too long. Scientists must tap the big media data stream and helpresolve this crisis.

Visual Attention Model for Cross-sectional Stock Return Prediction and  End-to-End Multimodal Market Representation Learning

  Technical and fundamental analysis are traditional tools used to analyzeindividual stocks; however, the finance literature has shown that the pricemovement of each individual stock correlates heavily with other stocks,especially those within the same sector. In this paper we propose a generalpurpose market representation that incorporates fundamental and technicalindicators and relationships between individual stocks. We treat the dailystock market as a "market image" where rows (grouped by market sector)represent individual stocks and columns represent indicators. We apply aconvolutional neural network over this market image to build market features ina hierarchical way. We use a recurrent neural network, with an attentionmechanism over the market feature maps, to model temporal dynamics in themarket. We show that our proposed model outperforms strong baselines in bothshort-term and long-term stock return prediction tasks. We also show anotheruse for our market image: to construct concise and dense market embeddingssuitable for downstream prediction tasks.

Combining Search, Social Media, and Traditional Data Sources to Improve  Influenza Surveillance

  We present a machine learning-based methodology capable of providingreal-time ("nowcast") and forecast estimates of influenza activity in the US byleveraging data from multiple data sources including: Google searches, Twittermicroblogs, nearly real-time hospital visit records, and data from aparticipatory surveillance system. Our main contribution consists of combiningmultiple influenza-like illnesses (ILI) activity estimates, generatedindependently with each data source, into a single prediction of ILI utilizingmachine learning ensemble approaches. Our methodology exploits the informationin each data source and produces accurate weekly ILI predictions for up to fourweeks ahead of the release of CDC's ILI reports. We evaluate the predictiveability of our ensemble approach during the 2013-2014 (retrospective) and2014-2015 (live) flu seasons for each of the four weekly time horizons. Ourensemble approach demonstrates several advantages: (1) our ensemble method'spredictions outperform every prediction using each data source independently,(2) our methodology can produce predictions one week ahead of GFT's real-timeestimates with comparable accuracy, and (3) our two and three week forecastestimates have comparable accuracy to real-time predictions using anautoregressive model. Moreover, our results show that considerable insight isgained from incorporating disparate data streams, in the form of social mediaand crowd sourced data, into influenza predictions in all time horizons

