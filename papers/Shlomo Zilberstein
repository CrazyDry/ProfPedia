Improved Memory-Bounded Dynamic Programming for Decentralized POMDPs

  Memory-Bounded Dynamic Programming (MBDP) has proved extremely effective insolving decentralized POMDPs with large horizons. We generalize the algorithmand improve its scalability by reducing the complexity with respect to thenumber of observations from exponential to polynomial. We derive error boundson solution quality with respect to this new approximation and analyze theconvergence behavior. To evaluate the effectiveness of the improvements, weintroduce a new, larger benchmark problem. Experimental results show thatdespite the high complexity of decentralized POMDPs, scalable solutiontechniques such as MBDP perform surprisingly well.

Region-Based Incremental Pruning for POMDPs

  We present a major improvement to the incremental pruning algorithm forsolving partially observable Markov decision processes. Our technique targetsthe cross-sum step of the dynamic programming (DP) update, a key source ofcomplexity in POMDP algorithms. Instead of reasoning about the whole beliefspace when pruning the cross-sums, our algorithm divides the belief space intosmaller regions and performs independent pruning in each region. We evaluatethe benefits of the new technique both analytically and experimentally, andshow that it produces very significant performance gains. The resultscontribute to the scalability of POMDP algorithms to domains that cannot behandled by the best existing techniques.

Feature Selection Using Regularization in Approximate Linear Programs  for Markov Decision Processes

  Approximate dynamic programming has been used successfully in a large varietyof domains, but it relies on a small set of provided approximation features tocalculate solutions reliably. Large and rich sets of features can causeexisting algorithms to overfit because of a limited number of samples. Weaddress this shortcoming using $L_1$ regularization in approximate linearprogramming. Because the proposed method can automatically select theappropriate richness of features, its performance does not degrade with anincreasing number of features. These results rely on new and stronger samplingbounds for regularized approximate linear programs. We also propose acomputationally efficient homotopy method. The empirical evaluation of theapproach shows that the proposed method performs well on simple MDPs andstandard benchmark problems.

Global Optimization for Value Function Approximation

  Existing value function approximation methods have been successfully used inmany applications, but they often lack useful a priori error bounds. We proposea new approximate bilinear programming formulation of value functionapproximation, which employs global optimization. The formulation providesstrong a priori guarantees on both robust and expected policy loss byminimizing specific norms of the Bellman residual. Solving a bilinear programoptimally is NP-hard, but this is unavoidable because the Bellman-residualminimization itself is NP-hard. We describe and analyze both optimal andapproximate algorithms for solving bilinear programs. The analysis shows thatthis algorithm offers a convergent generalization of approximate policyiteration. We also briefly analyze the behavior of bilinear programmingalgorithms under incomplete samples. Finally, we demonstrate that the proposedapproach can consistently minimize the Bellman residual on simple benchmarkproblems.

Message-Passing Algorithms for Quadratic Programming Formulations of MAP  Estimation

  Computing maximum a posteriori (MAP) estimation in graphical models is animportant inference problem with many applications. We present message-passingalgorithms for quadratic programming (QP) formulations of MAP estimation forpairwise Markov random fields. In particular, we use the concave-convexprocedure (CCCP) to obtain a locally optimal algorithm for the non-convex QPformulation. A similar technique is used to derive a globally convergentalgorithm for the convex QP relaxation of MAP. We also show that a recentlydeveloped expectation-maximization (EM) algorithm for the QP formulation of MAPcan be derived from the CCCP perspective. Experiments on synthetic andreal-world problems confirm that our new approach is competitive withmax-product and its variations. Compared with CPLEX, we achieve more than anorder-of-magnitude speedup in solving optimally the convex QP relaxation.

Anytime Planning for Decentralized POMDPs using Expectation Maximization

  Decentralized POMDPs provide an expressive framework for multi-agentsequential decision making. While fnite-horizon DECPOMDPs have enjoyedsignifcant success, progress remains slow for the infnite-horizon case mainlydue to the inherent complexity of optimizing stochastic controllersrepresenting agent policies. We present a promising new class of algorithms forthe infnite-horizon case, which recasts the optimization problem as inferencein a mixture of DBNs. An attractive feature of this approach is thestraightforward adoption of existing inference techniques in DBNs for solvingDEC-POMDPs and supporting richer representations such as factored or continuousstates and actions. We also derive the Expectation Maximization (EM) algorithmto optimize the joint policy represented as DBNs. Experiments on benchmarkdomains show that EM compares favorably against the state-of-the-art solvers.

Rollout Sampling Policy Iteration for Decentralized POMDPs

  We present decentralized rollout sampling policy iteration (DecRSPI) - a newalgorithm for multi-agent decision problems formalized as DEC-POMDPs. DecRSPIis designed to improve scalability and tackle problems that lack an explicitmodel. The algorithm uses Monte- Carlo methods to generate a sample ofreachable belief states. Then it computes a joint policy for each belief statebased on the rollout estimations. A new policy representation allows us torepresent solutions compactly. The key benefits of the algorithm are its lineartime complexity over the number of agents, its bounded memory usage and goodsolution quality. It can solve larger problems that are intractable forexisting planning algorithms. Experimental results confirm the effectivenessand scalability of the approach.

Optimizing Memory-Bounded Controllers for Decentralized POMDPs

  We present a memory-bounded optimization approach for solvinginfinite-horizon decentralized POMDPs. Policies for each agent are representedby stochastic finite state controllers. We formulate the problem of optimizingthese policies as a nonlinear program, leveraging powerful existing nonlinearoptimization techniques for solving the problem. While existing solvers onlyguarantee locally optimal solutions, we show that our formulation produceshigher quality controllers than the state-of-the-art approach. We alsoincorporate a shared source of randomness in the form of a correlation deviceto further increase solution quality with only a limited increase in space andtime. Our experimental results show that nonlinear optimization can be used toprovide high quality, concise solutions to decentralized decision problemsunder uncertainty.

MAA*: A Heuristic Search Algorithm for Solving Decentralized POMDPs

  We present multi-agent A* (MAA*), the first complete and optimal heuristicsearch algorithm for solving decentralized partially-observable Markov decisionproblems (DEC-POMDPs) with finite horizon. The algorithm is suitable forcomputing optimal plans for a cooperative group of agents that operate in astochastic environment such as multirobot coordination, network trafficcontrol, `or distributed resource allocation. Solving such problems efiectivelyis a major challenge in the area of planning under uncertainty. Our solution isbased on a synthesis of classical heuristic search and decentralized controltheory. Experimental results show that MAA* has significant advantages. Weintroduce an anytime variant of MAA* and conclude with a discussion ofpromising extensions such as an approach to solving infinite horizon problems.

Symbolic Generalization for On-line Planning

  Symbolic representations have been used successfully in off-line planningalgorithms for Markov decision processes. We show that they can also improvethe performance of on-line planners. In addition to reducing computation time,symbolic generalization can reduce the amount of costly real-world interactionsrequired for convergence. We introduce Symbolic Real-Time Dynamic Programming(or sRTDP), an extension of RTDP. After each step of on-line interaction withan environment, sRTDP uses symbolic model-checking techniques to generalizesits experience by updating a group of states rather than a single state. Weexamine two heuristic approaches to dynamic grouping of states and show thatthey accelerate the planning process significantly in terms of both CPU timeand the number of steps of interaction with the environment.

The Complexity of Decentralized Control of Markov Decision Processes

  Planning for distributed agents with partial state information is consideredfrom a decision- theoretic perspective. We describe generalizations of both theMDP and POMDP models that allow for decentralized control. For even a smallnumber of agents, the finite-horizon problems corresponding to both of ourmodels are complete for nondeterministic exponential time. These complexityresults illustrate a fundamental difference between centralized anddecentralized control of Markov processes. In contrast to the MDP and POMDPproblems, the problems we consider provably do not admit polynomial-timealgorithms and most likely require doubly exponential time to solve in theworst case. We have thus provided mathematical evidence corresponding to theintuition that decentralized planning problems cannot easily be reduced tocentralized problems and solved exactly using established techniques.

A Bilinear Programming Approach for Multiagent Planning

  Multiagent planning and coordination problems are common and known to becomputationally hard. We show that a wide range of two-agent problems can beformulated as bilinear programs. We present a successive approximationalgorithm that significantly outperforms the coverage set algorithm, which isthe state-of-the-art method for this class of multiagent problems. Because thealgorithm is formulated for bilinear programs, it is more general and simplerto implement. The new algorithm can be terminated at any time and-unlike thecoverage set algorithm-it facilitates the derivation of a useful onlineperformance bound. It is also much more efficient, on average reducing thecomputation time of the optimal solution by about four orders of magnitude.Finally, we introduce an automatic dimensionality reduction method thatimproves the effectiveness of the algorithm, extending its applicability to newdomains and providing a new way to analyze a subclass of bilinear programs.

Robust Optimization for Tree-Structured Stochastic Network Design

  Stochastic network design is a general framework for optimizing networkconnectivity. It has several applications in computational sustainabilityincluding spatial conservation planning, pre-disaster network preparation, andriver network optimization. A common assumption in previous work has been madethat network parameters (e.g., probability of species colonization) areprecisely known, which is unrealistic in real- world settings. We thereforeaddress the robust river network design problem where the goal is to optimizeriver connectivity for fish movement by removing barriers. We assume that fishpassability probabilities are known only imprecisely, but are within someinterval bounds. We then develop a planning approach that computes the policieswith either high robust ratio or low regret. Empirically, our approach scaleswell to large river networks. We also provide insights into the solutionsgenerated by our robust approach, which has significantly higher robust ratiothan the baseline solution with mean parameter estimates.

Generalizing the Role of Determinization in Probabilistic Planning

  The stochastic shortest path problem (SSP) is a highly expressive model forprobabilistic planning. The computational hardness of SSPs has sparked interestin determinization-based planners that can quickly solve large problems.However, existing methods employ a simplistic approach to determinization. Inparticular, they ignore the possibility of tailoring the determinization to thespecific characteristics of the target domain. In this work we examine thisquestion, by showing that learning a good determinization for a planning domaincan be done efficiently and can improve performance. Moreover, we show how todirectly incorporate probabilistic reasoning into the planning problem when agood determinization is not sufficient by itself. Based on these insights, weintroduce a planner, FF-LAO*, that outperforms state-of-the-art probabilisticplanners on several well-known competition benchmarks.

An Anytime Algorithm for Task and Motion MDPs

  Integrated task and motion planning has emerged as a challenging problem insequential decision making, where a robot needs to compute high-level strategyand low-level motion plans for solving complex tasks. While high-levelstrategies require decision making over longer time-horizons and scales, theirfeasibility depends on low-level constraints based upon the geometries andcontinuous dynamics of the environment. The hybrid nature of this problem makesit difficult to scale; most existing approaches focus on deterministic, fullyobservable scenarios. We present a new approach where the high-level decisionproblem occurs in a stochastic setting and can be modeled as a Markov decisionprocess. In contrast to prior efforts, we show that complete MDP policies, orcontingent behaviors, can be computed effectively in an anytime fashion. Ouralgorithm continuously improves the quality of the solution and is guaranteedto be probabilistically complete. We evaluate the performance of our approachon a challenging, realistic test problem: autonomous aircraft inspection. Ourresults show that we can effectively compute consistent task and motionpolicies for the most likely execution-time outcomes using only a fraction ofthe computation required to develop the complete task and motion policy.

Planning in Stochastic Environments with Goal Uncertainty

  We present the Goal Uncertain Stochastic Shortest Path (GUSSP) problem --- ageneral framework to model stochastic environments with goal uncertainty. Themodel is an extension of the stochastic shortest path (SSP) framework todynamic environments in which it is impossible to determine the exact goalstates ahead of plan execution. GUSSPs introduce flexibility in goalspecification by allowing a belief over possible goal configurations. Thepartial observability is restricted to goals, facilitating the reduction to anSSP. We formally define a GUSSP and discuss its theoretical properties. We thenpropose an admissible heuristic that reduces the planning time of FLARES --- astart-of-the-art probabilistic planner. We also propose a determinizationapproach for solving this class of problems. Finally, we present empiricalresults using a mobile robot and three other problem domains.

Lexicographically Ordered Multi-Objective Clustering

  We introduce a rich model for multi-objective clustering with lexicographicordering over objectives and a slack. The slack denotes the allowedmultiplicative deviation from the optimal objective value of the higherpriority objective to facilitate improvement in lower-priority objectives. Wethen propose an algorithm called Zeus to solve this class of problems, which ischaracterized by a makeshift function. The makeshift fine tunes the clustersformed by the processed objectives so as to improve the clustering with respectto the unprocessed objectives, given the slack. We present makeshift forsolving three different classes of objectives and analyze their solutionguarantees. Finally, we empirically demonstrate the effectiveness of ourapproach on three applications using real-world data.

Communication-Based Decomposition Mechanisms for Decentralized MDPs

  Multi-agent planning in stochastic environments can be framed formally as adecentralized Markov decision problem. Many real-life distributed problems thatarise in manufacturing, multi-robot coordination and information gatheringscenarios can be formalized using this framework. However, finding the optimalsolution in the general case is hard, limiting the applicability of recentlydeveloped algorithms. This paper provides a practical approach for solvingdecentralized control problems when communication among the decision makers ispossible, but costly. We develop the notion of communication-based mechanismthat allows us to decompose a decentralized MDP into multiple single-agentproblems. In this framework, referred to as decentralized semi-Markov decisionprocess with direct communication (Dec-SMDP-Com), agents operate separatelybetween communications. We show that finding an optimal mechanism is equivalentto solving optimally a Dec-SMDP-Com. We also provide a heuristic searchalgorithm that converges on the optimal decomposition. Restricting thedecomposition to some specific types of local behaviors reduces significantlythe complexity of planning. In particular, we present a polynomial-timealgorithm for the case in which individual agents perform goal-orientedbehaviors between communications. The paper concludes with an additionaltractable algorithm that enables the introduction of human knowledge, therebyreducing the overall problem to finding the best time to communicate. Empiricalresults show that these approaches provide good approximate solutions.

Policy Iteration for Decentralized Control of Markov Decision Processes

  Coordination of distributed agents is required for problems arising in manyareas, including multi-robot systems, networking and e-commerce. As a formalframework for such problems, we use the decentralized partially observableMarkov decision process (DEC-POMDP). Though much work has been done on optimaldynamic programming algorithms for the single-agent version of the problem,optimal algorithms for the multiagent case have been elusive. The maincontribution of this paper is an optimal policy iteration algorithm for solvingDEC-POMDPs. The algorithm uses stochastic finite-state controllers to representpolicies. The solution can include a correlation device, which allows agents tocorrelate their actions without communicating. This approach alternates betweenexpanding the controller and performing value-preserving transformations, whichmodify the controller without sacrificing value. We present two efficientvalue-preserving transformations: one can reduce the size of the controller andthe other can improve its value while keeping the size fixed. Empirical resultsdemonstrate the usefulness of value-preserving transformations in increasingvalue while keeping controller size to a minimum. To broaden the applicabilityof the approach, we also present a heuristic version of the policy iterationalgorithm, which sacrifices convergence to optimality. This algorithm furtherreduces the size of the controllers at each step by assuming that probabilitydistributions over the other agents actions are known. While this assumptionmay not hold in general, it helps produce higher quality solutions in our testproblems.

