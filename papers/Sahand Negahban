Restricted strong convexity and weighted matrix completion: Optimal  bounds with noise

  We consider the matrix completion problem under a form of row/column weightedentrywise sampling, including the case of uniform entrywise sampling as aspecial case. We analyze the associated random observation operator, and provethat with high probability, it satisfies a form of restricted strong convexitywith respect to weighted Frobenius norm. Using this property, we obtain ascorollaries a number of error bounds on matrix completion in the weightedFrobenius norm under noisy sampling and for both exact and near low-rankmatrices. Our results are based on measures of the "spikiness" and"low-rankness" of matrices that are less restrictive than the incoherenceconditions imposed in previous work. Our technique involves an $M$-estimatorthat includes controls on both the rank and spikiness of the solution, and weestablish non-asymptotic error bounds in weighted Frobenius norm for recoveringmatrices lying with $\ell_q$-"balls" of bounded spikiness. Usinginformation-theoretic methods, we show that no algorithm can achieve betterestimates (up to a logarithmic factor) over these same sets, showing that ourconditions on matrices and associated rates are essentially optimal.

Individualized Rank Aggregation using Nuclear Norm Regularization

  In recent years rank aggregation has received significant attention from themachine learning community. The goal of such a problem is to combine the(partially revealed) preferences over objects of a large population into asingle, relatively consistent ordering of those objects. However, in manycases, we might not want a single ranking and instead opt for individualrankings. We study a version of the problem known as collaborative ranking. Inthis problem we assume that individual users provide us with pairwisepreferences (for example purchasing one item over another). From thosepreferences we wish to obtain rankings on items that the users have not had anopportunity to explore. The results here have a very interesting connection tothe standard matrix completion problem. We provide a theoretical justificationfor a nuclear norm regularized optimization procedure, and providehigh-dimensional scaling results that show how the error in estimating userpreferences behaves as the number of observations increase.

Minimax Estimation of Bandable Precision Matrices

  The inverse covariance matrix provides considerable insight for understandingstatistical models in the multivariate setting. In particular, when thedistribution over variables is assumed to be multivariate normal, the sparsitypattern in the inverse covariance matrix, commonly referred to as the precisionmatrix, corresponds to the adjacency matrix representation of the Gauss-Markovgraph, which encodes conditional independence statements between variables.Minimax results under the spectral norm have previously been established forcovariance matrices, both sparse and banded, and for sparse precision matrices.We establish minimax estimation bounds for estimating banded precision matricesunder the spectral norm. Our results greatly improve upon the existing bounds;in particular, we find that the minimax rate for estimating banded precisionmatrices matches that of estimating banded covariance matrices. The key insightin our analysis is that we are able to obtain barely-noisy estimates of $k\times k$ subblocks of the precision matrix by inverting slightly wider blocksof the empirical covariance matrix along the diagonal. Our theoretical resultsare complemented by experiments demonstrating the sharpness of our bounds.

On Approximation Guarantees for Greedy Low Rank Optimization

  We provide new approximation guarantees for greedy low rank matrix estimationunder standard assumptions of restricted strong convexity and smoothness. Ournovel analysis also uncovers previously unknown connections between the lowrank estimation and combinatorial optimization, so much so that our bounds arereminiscent of corresponding approximation bounds in submodular maximization.Additionally, we also provide statistical recovery guarantees. Finally, wepresent empirical comparison of greedy estimation with established baselines ontwo important real-world problems.

Warm-starting Contextual Bandits: Robustly Combining Supervised and  Bandit Feedback

  We investigate the feasibility of learning from both fully-labeled superviseddata and contextual bandit data. We specifically consider settings in which theunderlying learning signal may be different between these two data sources.Theoretically, we state and prove no-regret algorithms for learning that isrobust to divergences between the two sources. Empirically, we evaluate some ofthese algorithms on a large selection of datasets, showing that our approachesare feasible, and helpful in practice.

A Unified Framework for High-Dimensional Analysis of M-Estimators with  Decomposable Regularizers

  High-dimensional statistical inference deals with models in which the thenumber of parameters p is comparable to or larger than the sample size n. Sinceit is usually impossible to obtain consistent procedures unless$p/n\rightarrow0$, a line of recent work has studied models with various typesof low-dimensional structure, including sparse vectors, sparse and structuredmatrices, low-rank matrices and combinations thereof. In such settings, ageneral approach to estimation is to solve a regularized optimization problem,which combines a loss function measuring how well the model fits the data withsome regularization function that encourages the assumed structure. This paperprovides a unified framework for establishing consistency and convergence ratesfor such regularized M-estimators under high-dimensional scaling. We state onemain theorem and show how it can be used to re-derive some existing results,and also to obtain a number of new results on consistency and convergencerates, in both $\ell_2$-error and related norms. Our analysis also identifiestwo key properties of loss and regularization functions, referred to asrestricted strong convexity and decomposability, that ensure correspondingregularized M-estimators have fast convergence rates and which are optimal inmany well-studied cases.

Estimation of (near) low-rank matrices with noise and high-dimensional  scaling

  High-dimensional inference refers to problems of statistical estimation inwhich the ambient dimension of the data may be comparable to or possibly evenlarger than the sample size. We study an instance of high-dimensional inferencein which the goal is to estimate a matrix $\Theta^* \in \real^{k \times p}$ onthe basis of $N$ noisy observations, and the unknown matrix $\Theta^*$ isassumed to be either exactly low rank, or ``near'' low-rank, meaning that itcan be well-approximated by a matrix with low rank. We consider an$M$-estimator based on regularization by the trace or nuclear norm overmatrices, and analyze its performance under high-dimensional scaling. Weprovide non-asymptotic bounds on the Frobenius norm error that hold for ageneral class of noisy observation models, and then illustrate theirconsequences for a number of specific matrix models, including low-rankmultivariate or multi-task regression, system identification in vectorautoregressive processes, and recovery of low-rank matrices from randomprojections. Simulation results show excellent agreement with thehigh-dimensional scaling of the error predicted by our theory.

Stochastic optimization and sparse statistical recovery: An optimal  algorithm for high dimensions

  We develop and analyze stochastic optimization algorithms for problems inwhich the expected loss is strongly convex, and the optimum is (approximately)sparse. Previous approaches are able to exploit only one of these twostructures, yielding an $\order(\pdim/T)$ convergence rate for strongly convexobjectives in $\pdim$ dimensions, and an $\order(\sqrt{(\spindex \log\pdim)/T})$ convergence rate when the optimum is $\spindex$-sparse. Ouralgorithm is based on successively solving a series of $\ell_1$-regularizedoptimization problems using Nesterov's dual averaging algorithm. We establishthat the error of our solution after $T$ iterations is at most$\order((\spindex \log\pdim)/T)$, with natural extensions to approximatesparsity. Our results apply to locally Lipschitz losses including the logistic,exponential, hinge and least-squares losses. By recourse to statistical minimaxresults, we show that our convergence rates are optimal up to multiplicativeconstant factors. The effectiveness of our approach is also confirmed innumerical simulations, in which we compare to several baselines on aleast-squares regression problem.

Understanding Adversarial Training: Increasing Local Stability of Neural  Nets through Robust Optimization

  We propose a general framework for increasing local stability of ArtificialNeural Nets (ANNs) using Robust Optimization (RO). We achieve this through analternating minimization-maximization procedure, in which the loss of thenetwork is minimized over perturbed examples that are generated at eachparameter update. We show that adversarial training of ANNs is in factrobustification of the network optimization, and that our proposed frameworkgeneralizes previous approaches for increasing local stability of ANNs.Experimental results reveal that our approach increases the robustness of thenetwork to existing adversarial examples, while making it harder to generatenew ones. Furthermore, our algorithm improves the accuracy of the network alsoon the original test data.

Super-resolution estimation of cyclic arrival rates

  Exploiting the fact that most arrival processes exhibit cyclic behaviour, wepropose a simple procedure for estimating the intensity of a nonhomogeneousPoisson process. The estimator is the super-resolution analogue to Shao 2010and Shao & Lii 2011, which is a sum of $p$ sinusoids where $p$ and thefrequency, amplitude, and phase of each wave are not known and need to beestimated. This results in an interpretable yet flexible specification that issuitable for use in modelling as well as in high resolution simulations.  Our estimation procedure sits in between classic periodogram methods andatomic/total variation norm thresholding. Through a novel use of windowfunctions in the point process domain, our approach attains super-resolutionwithout semidefinite programming. Under suitable conditions, finite sampleguarantees can be derived for our procedure. These resolve some open questionsand expand existing results in spectral estimation literature.

Restricted Strong Convexity Implies Weak Submodularity

  We connect high-dimensional subset selection and submodular maximization. Ourresults extend the work of Das and Kempe (2011) from the setting of linearregression to arbitrary objective functions. For greedy feature selection, thisconnection allows us to obtain strong multiplicative performance bounds onseveral methods without statistical modeling assumptions. We also deriverecovery guarantees of this form under standard assumptions. Our work showsthat greedy algorithms perform within a constant factor from the best possiblesubset-selection solution for a broad class of general objective functions. Ourmethods allow a direct control over the number of obtained features as opposedto regularization parameters that only implicitly control sparsity. Our prooftechnique uses the concept of weak submodularity initially defined by Das andKempe. We draw a connection between convex analysis and submodular set functiontheory which may be of independent interest for other statistical learningapplications that have combinatorial structure.

Feature Selection using Stochastic Gates

  Feature selection problems have been extensively studied for linearestimation, for instance, Lasso, but less emphasis has been placed on featureselection for non-linear functions. In this study, we propose a method forfeature selection in high-dimensional non-linear function estimation problems.The new procedure is based on minimizing the $\ell_0$ norm of the vector ofindicator variables that represent if a feature is selected or not. Ourapproach relies on the continuous relaxation of Bernoulli distributions, whichallows our model to learn the parameters of the approximate Bernoullidistributions via gradient descent. This general framework simultaneouslyminimizes a loss function while selecting relevant features. Furthermore, weprovide an information-theoretic justification of incorporating Bernoullidistribution into our approach and demonstrate the potential of the approach onsynthetic and real-life applications.

Alternating Linear Bandits for Online Matrix-Factorization  Recommendation

  We consider the problem of online collaborative filtering in the onlinesetting, where items are recommended to the users over time. At each time step,the user (selected by the environment) consumes an item (selected by the agent)and provides a rating of the selected item. In this paper, we propose a novelalgorithm for online matrix factorization recommendation that combines linearbandits and alternating least squares. In this formulation, the bandit feedbackis equal to the difference between the ratings of the best and selected items.We evaluate the performance of the proposed algorithm over time using bothcumulative regret and average cumulative NDCG. Simulation results over threesynthetic datasets as well as three real-world datasets for onlinecollaborative filtering indicate the superior performance of the proposedalgorithm over two state-of-the-art online algorithms.

Noisy matrix decomposition via convex relaxation: Optimal rates in high  dimensions

  We analyze a class of estimators based on convex relaxation for solvinghigh-dimensional matrix decomposition problems. The observations are noisyrealizations of a linear transformation $\mathfrak{X}$ of the sum of anapproximately) low rank matrix $\Theta^\star$ with a second matrix$\Gamma^\star$ endowed with a complementary form of low-dimensional structure;this set-up includes many statistical models of interest, including factoranalysis, multi-task regression, and robust covariance estimation. We derive ageneral theorem that bounds the Frobenius norm error for an estimate of thepair $(\Theta^\star, \Gamma^\star)$ obtained by solving a convex optimizationproblem that combines the nuclear norm with a general decomposable regularizer.Our results utilize a "spikiness" condition that is related to but milder thansingular vector incoherence. We specialize our general result to two cases thathave been studied in past work: low rank plus an entrywise sparse matrix, andlow rank plus a columnwise sparse matrix. For both models, our theory yieldsnon-asymptotic Frobenius error bounds for both deterministic and stochasticnoise matrices, and applies to matrices $\Theta^\star$ that can be exactly orapproximately low rank, and matrices $\Gamma^\star$ that can be exactly orapproximately sparse. Moreover, for the case of stochastic noise matrices andthe identity observation operator, we establish matching lower bounds on theminimax error. The sharpness of our predictions is confirmed by numericalsimulations.

Using Machine Learning for Discovery in Synoptic Survey Imaging

  Modern time-domain surveys continuously monitor large swaths of the sky tolook for astronomical variability. Astrophysical discovery in such data sets iscomplicated by the fact that detections of real transient and variable sourcesare highly outnumbered by bogus detections caused by imperfect subtractions,atmospheric effects and detector artefacts. In this work we present a machinelearning (ML) framework for discovery of variability in time-domain imagingsurveys. Our ML methods provide probabilistic statements, in near real time,about the degree to which each newly observed source is astrophysicallyrelevant source of variable brightness. We provide details about each of theanalysis steps involved, including compilation of the training and testingsets, construction of descriptive image-based and contextual features, andoptimization of the feature subset and model tuning parameters. Using avalidation set of nearly 30,000 objects from the Palomar Transient Factory, wedemonstrate a missed detection rate of at most 7.7% at our chosenfalse-positive rate of 1% for an optimized ML classifier of 23 features,selected to avoid feature correlation and over-fitting from an initial libraryof 42 attributes. Importantly, we show that our classification methodology isinsensitive to mis-labelled training data up to a contamination of nearly 10%,making it easier to compile sufficient training sets for accurate performancein future surveys. This ML framework, if so adopted, should enable themaximization of scientific gain from future synoptic survey and enable fastfollow-up decisions on the vast amounts of streaming data produced by suchexperiments.

Scaling Multiple-Source Entity Resolution using Statistically Efficient  Transfer Learning

  We consider a serious, previously-unexplored challenge facing almost allapproaches to scaling up entity resolution (ER) to multiple data sources: theprohibitive cost of labeling training data for supervised learning ofsimilarity scores for each pair of sources. While there exists a richliterature describing almost all aspects of pairwise ER, this new challenge isarising now due to the unprecedented ability to acquire and store data fromonline sources, features driven by ER such as enriched search verticals, andthe uniqueness of noisy and missing data characteristics for each source. Weshow on real-world and synthetic data that for state-of-the-art techniques, thereality of heterogeneous sources means that the number of labeled training datamust scale quadratically in the number of sources, just to maintain constantprecision/recall. We address this challenge with a brand new transfer learningalgorithm which requires far less training data (or equivalently, achievessuperior accuracy with the same data) and is trained using fast convexoptimization. The intuition behind our approach is to adaptively sharestructure learned about one scoring problem with all other scoring problemssharing a data source in common. We demonstrate that our theoreticallymotivated approach incurs no runtime cost while it can maintain constantprecision/recall with the cost of labeling increasing only linearly with thenumber of sources.

Fast global convergence of gradient methods for high-dimensional  statistical recovery

  Many statistical $M$-estimators are based on convex optimization problemsformed by the combination of a data-dependent loss function with a norm-basedregularizer. We analyze the convergence rates of projected gradient andcomposite gradient methods for solving such problems, working within ahigh-dimensional framework that allows the data dimension $\pdim$ to grow with(and possibly exceed) the sample size $\numobs$. This high-dimensionalstructure precludes the usual global assumptions---namely, strong convexity andsmoothness conditions---that underlie much of classical optimization analysis.We define appropriately restricted versions of these conditions, and show thatthey are satisfied with high probability for various statistical models. Underthese conditions, our theory guarantees that projected gradient descent has aglobally geometric rate of convergence up to the \emph{statistical precision}of the model, meaning the typical distance between the true unknown parameter$\theta^*$ and an optimal solution $\hat{\theta}$. This result is substantiallysharper than previous convergence results, which yielded sublinear convergence,or linear convergence only up to the noise level. Our analysis applies to awide range of $M$-estimators and statistical models, including sparse linearregression using Lasso ($\ell_1$-regularized regression); group Lasso for blocksparsity; log-linear models with regularization; low-rank matrix recovery usingnuclear norm regularization; and matrix decomposition. Overall, our analysisreveals interesting connections between statistical precision and computationalefficiency in high-dimensional estimation.

Scalable Greedy Feature Selection via Weak Submodularity

  Greedy algorithms are widely used for problems in machine learning such asfeature selection and set function optimization. Unfortunately, for largedatasets, the running time of even greedy algorithms can be quite high. This isbecause for each greedy step we need to refit a model or calculate a functionusing the previously selected choices and the new candidate.  Two algorithms that are faster approximations to the greedy forward selectionwere introduced recently ([Mirzasoleiman et al. 2013, 2015]). They achievebetter performance by exploiting distributed computation and stochasticevaluation respectively. Both algorithms have provable performance guaranteesfor submodular functions.  In this paper we show that divergent from previously held opinion,submodularity is not required to obtain approximation guarantees for these twoalgorithms. Specifically, we show that a generalized concept of weaksubmodularity suffices to give multiplicative approximation guarantees. Ourresult extends the applicability of these algorithms to a larger class offunctions. Furthermore, we show that a bounded submodularity ratio can be usedto provide data dependent bounds that can sometimes be tighter also forsubmodular functions. We empirically validate our work by showing superiorperformance of fast greedy approximations versus several established baselineson artificial and real datasets.

Rank Centrality: Ranking from Pair-wise Comparisons

  The question of aggregating pair-wise comparisons to obtain a global rankingover a collection of objects has been of interest for a very long time: be itranking of online gamers (e.g. MSR's TrueSkill system) and chess players,aggregating social opinions, or deciding which product to sell based ontransactions. In most settings, in addition to obtaining a ranking, finding`scores' for each object (e.g. player's rating) is of interest forunderstanding the intensity of the preferences.  In this paper, we propose Rank Centrality, an iterative rank aggregationalgorithm for discovering scores for objects (or items) from pair-wisecomparisons. The algorithm has a natural random walk interpretation over thegraph of objects with an edge present between a pair of objects if they arecompared; the score, which we call Rank Centrality, of an object turns out tobe its stationary probability under this random walk. To study the efficacy ofthe algorithm, we consider the popular Bradley-Terry-Luce (BTL) model(equivalent to the Multinomial Logit (MNL) for pair-wise comparisons) in whicheach object has an associated score which determines the probabilistic outcomesof pair-wise comparisons between objects. In terms of the pair-wise marginalprobabilities, which is the main subject of this paper, the MNL model and theBTL model are identical. We bound the finite sample error rates between thescores assumed by the BTL model and those estimated by our algorithm. Inparticular, the number of samples required to learn the score well with highprobability depends on the structure of the comparison graph. When theLaplacian of the comparison graph has a strictly positive spectral gap, e.g.each item is compared to a subset of randomly chosen items, this leads todependence on the number of samples that is nearly order-optimal.

Learning from Comparisons and Choices

  When tracking user-specific online activities, each user's preference isrevealed in the form of choices and comparisons. For example, a user's purchasehistory is a record of her choices, i.e. which item was chosen among a subsetof offerings. A user's preferences can be observed either explicitly as inmovie ratings or implicitly as in viewing times of news articles. Given suchindividualized ordinal data in the form of comparisons and choices, we addressthe problem of collaboratively learning representations of the users and theitems. The learned features can be used to predict a user's preference of anunseen item to be used in recommendation systems. This also allows one tocompute similarities among users and items to be used for categorization andsearch. Motivated by the empirical successes of the MultiNomial Logit (MNL)model in marketing and transportation, and also more recent successes in wordembedding and crowdsourced image embedding, we pose this problem as learningthe MNL model parameters that best explain the data. We propose a convexrelaxation for learning the MNL model, and show that it is minimax optimal upto a logarithmic factor by comparing its performance to a fundamental lowerbound. This characterizes the minimax sample complexity of the problem, andproves that the proposed estimator cannot be improved upon other than by alogarithmic factor. Further, the analysis identifies how the accuracy dependson the topology of sampling via the spectrum of the sampling graph. Thisprovides a guideline for designing surveys when one can choose which items areto be compared. This is accompanied by numerical simulations on synthetic andreal data sets, confirming our theoretical predictions.

