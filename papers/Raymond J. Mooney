Content-Based Book Recommending Using Learning for Text Categorization

  Recommender systems improve access to relevant products and information by
making personalized suggestions based on previous examples of a user's likes
and dislikes. Most existing recommender systems use social filtering methods
that base recommendations on other users' preferences. By contrast,
content-based methods use information about an item itself to make suggestions.
This approach has the advantage of being able to recommended previously unrated
items to users with unique interests and to provide explanations for its
recommendations. We describe a content-based book recommending system that
utilizes information extraction and a machine-learning algorithm for text
categorization. Initial experimental results demonstrate that this approach can
produce accurate recommendations.


Comparative Experiments on Disambiguating Word Senses: An Illustration
  of the Role of Bias in Machine Learning

  This paper describes an experimental comparison of seven different learning
algorithms on the problem of learning to disambiguate the meaning of a word
from context. The algorithms tested include statistical, neural-network,
decision-tree, rule-based, and case-based classification techniques. The
specific problem tested involves disambiguating six senses of the word ``line''
using the words in the current and proceeding sentence as context. The
statistical and neural-network methods perform the best on this particular
problem and we discuss a potential reason for this observed difference. We also
discuss the role of bias in machine learning and its importance in explaining
performance differences observed on specific problems.


Learning Parse and Translation Decisions From Examples With Rich Context

  We present a knowledge and context-based system for parsing and translating
natural language and evaluate it on sentences from the Wall Street Journal.
Applying machine learning techniques, the system uses parse action examples
acquired under supervision to generate a deterministic shift-reduce parser in
the form of a decision structure. It relies heavily on context, as encoded in
features which describe the morphological, syntactic, semantic and other
aspects of a given parse state.


Using Sentence-Level LSTM Language Models for Script Inference

  There is a small but growing body of research on statistical scripts, models
of event sequences that allow probabilistic inference of implicit events from
documents. These systems operate on structured verb-argument events produced by
an NLP pipeline. We compare these systems with recent Recurrent Neural Net
models that directly operate on raw tokens to predict sentences, finding the
latter to be roughly comparable to the former in terms of predicting missing
events in documents.


Supervised and Unsupervised Ensembling for Knowledge Base Population

  We present results on combining supervised and unsupervised methods to
ensemble multiple systems for two popular Knowledge Base Population (KBP)
tasks, Cold Start Slot Filling (CSSF) and Tri-lingual Entity Discovery and
Linking (TEDL). We demonstrate that our combined system along with auxiliary
features outperforms the best performing system for both tasks in the 2015
competition, several ensembling baselines, as well as the state-of-the-art
stacking approach to ensembling KBP systems. The success of our technique on
two different and challenging problems demonstrates the power and generality of
our combined approach to ensembling.


Leveraging Discourse Information Effectively for Authorship Attribution

  We explore techniques to maximize the effectiveness of discourse information
in the task of authorship attribution. We present a novel method to embed
discourse features in a Convolutional Neural Network text classifier, which
achieves a state-of-the-art result by a substantial margin. We empirically
investigate several featurization methods to understand the conditions under
which discourse features contribute non-trivial performance gains, and analyze
discourse embeddings.


Learning a Policy for Opportunistic Active Learning

  Active learning identifies data points to label that are expected to be the
most useful in improving a supervised model. Opportunistic active learning
incorporates active learning into interactive tasks that constrain possible
queries during interactions. Prior work has shown that opportunistic active
learning can be used to improve grounding of natural language descriptions in
an interactive object retrieval task. In this work, we use reinforcement
learning for such an object retrieval task, to learn a policy that effectively
trades off task completion with model improvement that would benefit future
tasks.


Faithful Multimodal Explanation for Visual Question Answering

  AI systems' ability to explain their reasoning is critical to their utility
and trustworthiness. Deep neural networks have enabled significant progress on
many challenging problems such as visual question answering (VQA). However,
most of them are opaque black boxes with limited explanatory capability. This
paper presents a novel approach to developing a high-performing VQA system that
can elucidate its answers with integrated textual and visual explanations that
faithfully reflect important aspects of its underlying reasoning while
capturing the style of comprehensible human explanations. Extensive
experimental evaluation demonstrates the advantages of this approach compared
to competing methods with both automatic evaluation metrics and human
evaluation metrics.


Training a Multilingual Sportscaster: Using Perceptual Context to Learn
  Language

  We present a novel framework for learning to interpret and generate language
using only perceptual context as supervision. We demonstrate its capabilities
by developing a system that learns to sportscast simulated robot soccer games
in both English and Korean without any language-specific prior knowledge.
Training employs only ambiguous supervision consisting of a stream of
descriptive textual comments and a sequence of events extracted from the
simulation trace. The system simultaneously establishes correspondences between
individual comments and the events that they describe while building a
translation model that supports both parsing and generation. We also present a
novel algorithm for learning which events are worth describing. Human
evaluations of the generated commentaries indicate they are of reasonable
quality and in some cases even on par with those produced by humans for our
limited domain.


Stacking With Auxiliary Features

  Ensembling methods are well known for improving prediction accuracy. However,
they are limited in the sense that they cannot discriminate among component
models effectively. In this paper, we propose stacking with auxiliary features
that learns to fuse relevant information from multiple systems to improve
performance. Auxiliary features enable the stacker to rely on systems that not
just agree on an output but also the provenance of the output. We demonstrate
our approach on three very different and difficult problems -- the Cold Start
Slot Filling, the Tri-lingual Entity Discovery and Linking and the ImageNet
object detection tasks. We obtain new state-of-the-art results on the first two
tasks and substantial improvements on the detection task, thus verifying the
power and generality of our approach.


Joint Image Captioning and Question Answering

  Answering visual questions need acquire daily common knowledge and model the
semantic connection among different parts in images, which is too difficult for
VQA systems to learn from images with the only supervision from answers.
Meanwhile, image captioning systems with beam search strategy tend to generate
similar captions and fail to diversely describe images. To address the
aforementioned issues, we present a system to have these two tasks compensate
with each other, which is capable of jointly producing image captions and
answering visual questions. In particular, we utilize question and image
features to generate question-related captions and use the generated captions
as additional features to provide new knowledge to the VQA system. For image
captioning, our system attains more informative results in term of the relative
improvements on VQA tasks as well as competitive results using automated
metrics. Applying our system to the VQA tasks, our results on VQA v2 dataset
achieve 65.8% using generated captions and 69.1% using annotated captions in
validation set and 68.4% in the test-standard set. Further, an ensemble of 10
models results in 69.7% in the test-standard split.


Executable Trigger-Action Comments

  Natural language elements, e.g., todo comments, are frequently used to
communicate among the developers and to describe tasks that need to be
performed (actions) when specific conditions hold in the code repository
(triggers). As projects evolve, development processes change, and development
teams reorganize, these comments, because of their informal nature, frequently
become irrelevant or forgotten.
  We present the first technique, dubbed TrigIt, to specify triggeraction todo
comments as executable statements. Thus, actions are executed automatically
when triggers evaluate to true. TrigIt specifications are written in the host
language (e.g., Java) and are evaluated as part of the build process. The
triggers are specified as query statements over abstract syntax trees and
abstract representation of build configuration scripts, and the actions are
specified as code transformation steps. We implemented TrigIt for the Java
programming language and migrated 20 existing trigger-action comments from 8
popular open-source projects. We evaluate the cost of using TrigIt in terms of
the number of tokens in the executable comments and the time overhead
introduced in the build process.


Improving Grounded Natural Language Understanding through Human-Robot
  Dialog

  Natural language understanding for robotics can require substantial domain-
and platform-specific engineering. For example, for mobile robots to
pick-and-place objects in an environment to satisfy human commands, we can
specify the language humans use to issue such commands, and connect concept
words like red can to physical object properties. One way to alleviate this
engineering for a new domain is to enable robots in human environments to adapt
dynamically---continually learning new language constructions and perceptual
concepts. In this work, we present an end-to-end pipeline for translating
natural language commands to discrete robot actions, and use clarification
dialogs to jointly improve language parsing and concept grounding. We train and
evaluate this agent in a virtual setting on Amazon Mechanical Turk, and we
transfer the learned agent to a physical robot platform to demonstrate it in
the real world.


Using Natural Language for Reward Shaping in Reinforcement Learning

  Recent reinforcement learning (RL) approaches have shown strong performance
in complex domains such as Atari games, but are often highly sample
inefficient. A common approach to reduce interaction time with the environment
is to use reward shaping, which involves carefully designing reward functions
that provide the agent intermediate rewards for progress towards the goal.
However, designing appropriate shaping rewards is known to be difficult as well
as time-consuming. In this work, we address this problem by using natural
language instructions to perform reward shaping. We propose the LanguagE-Action
Reward Network (LEARN), a framework that maps free-form natural language
instructions to intermediate rewards based on actions taken by the agent. These
intermediate language-based rewards can seamlessly be integrated into any
standard reinforcement learning algorithm. We experiment with Montezuma's
Revenge from the Atari Learning Environment, a popular benchmark in RL. Our
experiments on a diverse set of 15 tasks demonstrate that, for the same number
of interactions with the environment, language-based rewards lead to successful
completion of the task 60% more often on average, compared to learning without
language.


Representing Meaning with a Combination of Logical and Distributional
  Models

  NLP tasks differ in the semantic information they require, and at this time
no single se- mantic representation fulfills all requirements. Logic-based
representations characterize sentence structure, but do not capture the graded
aspect of meaning. Distributional models give graded similarity ratings for
words and phrases, but do not capture sentence structure in the same detail as
logic-based approaches. So it has been argued that the two are complementary.
We adopt a hybrid approach that combines logic-based and distributional
semantics through probabilistic logic inference in Markov Logic Networks
(MLNs). In this paper, we focus on the three components of a practical system
integrating logical and distributional models: 1) Parsing and task
representation is the logic-based part where input problems are represented in
probabilistic logic. This is quite different from representing them in standard
first-order logic. 2) For knowledge base construction we form weighted
inference rules. We integrate and compare distributional information with other
sources, notably WordNet and an existing paraphrase collection. In particular,
we use our system to evaluate distributional lexical entailment approaches. We
use a variant of Robinson resolution to determine the necessary inference
rules. More sources can easily be added by mapping them to logical rules; our
system learns a resource-specific weight that corrects for scaling differences
between resources. 3) In discussing probabilistic inference, we show how to
solve the inference problems efficiently. To evaluate our approach, we use the
task of textual entailment (RTE), which can utilize the strengths of both
logic-based and distributional representations. In particular we focus on the
SICK dataset, where we achieve state-of-the-art results.


The SeaQuest Spectrometer at Fermilab

  The SeaQuest spectrometer at Fermilab was designed to detect
oppositely-charged pairs of muons (dimuons) produced by interactions between a
120 GeV proton beam and liquid hydrogen, liquid deuterium and solid nuclear
targets. The primary physics program uses the Drell-Yan process to probe
antiquark distributions in the target nucleon. The spectrometer consists of a
target system, two dipole magnets and four detector stations. The upstream
magnet is a closed-aperture solid iron magnet which also serves as the beam
dump, while the second magnet is an open aperture magnet. Each of the detector
stations consists of scintillator hodoscopes and a high-resolution tracking
device. The FPGA-based trigger compares the hodoscope signals to a set of
pre-programmed roads to determine if the event contains oppositely-signed,
high-mass muon pairs.


Observation of the rare $B^0_s\toμ^+μ^-$ decay from the combined
  analysis of CMS and LHCb data

  A joint measurement is presented of the branching fractions
$B^0_s\to\mu^+\mu^-$ and $B^0\to\mu^+\mu^-$ in proton-proton collisions at the
LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at
a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis
produces the first observation of the $B^0_s\to\mu^+\mu^-$ decay, with a
statistical significance exceeding six standard deviations, and the best
measurement of its branching fraction so far. Furthermore, evidence for the
$B^0\to\mu^+\mu^-$ decay is obtained with a statistical significance of three
standard deviations. The branching fraction measurements are statistically
compatible with SM predictions and impose stringent constraints on several
theories beyond the SM.


