A Single Model Explains both Visual and Auditory Precortical Coding

  Precortical neural systems encode information collected by the senses, but
the driving principles of the encoding used have remained a subject of debate.
We present a model of retinal coding that is based on three constraints:
information preservation, minimization of the neural wiring, and response
equalization. The resulting novel version of sparse principal components
analysis successfully captures a number of known characteristics of the retinal
coding system, such as center-surround receptive fields, color opponency
channels, and spatiotemporal responses that correspond to magnocellular and
parvocellular pathways. Furthermore, when trained on auditory data, the same
model learns receptive fields well fit by gammatone filters, commonly used to
model precortical auditory coding. This suggests that efficient coding may be a
unifying principle of precortical encoding across modalities.


Example Selection For Dictionary Learning

  In unsupervised learning, an unbiased uniform sampling strategy is typically
used, in order that the learned features faithfully encode the statistical
structure of the training data. In this work, we explore whether active example
selection strategies - algorithms that select which examples to use, based on
the current estimate of the features - can accelerate learning. Specifically,
we investigate effects of heuristic and saliency-inspired selection algorithms
on the dictionary learning task with sparse activations. We show that some
selection algorithms do improve the speed of learning, and we speculate on why
they might work.


Skeleton Key: Image Captioning by Skeleton-Attribute Decomposition

  Recently, there has been a lot of interest in automatically generating
descriptions for an image. Most existing language-model based approaches for
this task learn to generate an image description word by word in its original
word order. However, for humans, it is more natural to locate the objects and
their relationships first, and then elaborate on each object, describing
notable attributes. We present a coarse-to-fine method that decomposes the
original image description into a skeleton sentence and its attributes, and
generates the skeleton sentence and attribute phrases separately. By this
decomposition, our method can generate more accurate and novel descriptions
than the previous state-of-the-art. Experimental results on the MS-COCO and a
larger scale Stock3M datasets show that our algorithm yields consistent
improvements across different evaluation metrics, especially on the SPICE
metric, which has much higher correlation with human ratings than the
conventional metrics. Furthermore, our algorithm can generate descriptions with
varied length, benefiting from the separate control of the skeleton and
attributes. This enables image description generation that better accommodates
user preferences.


Recognizing and Curating Photo Albums via Event-Specific Image
  Importance

  Automatic organization of personal photos is a problem with many real world
ap- plications, and can be divided into two main tasks: recognizing the event
type of the photo collection, and selecting interesting images from the
collection. In this paper, we attempt to simultaneously solve both tasks:
album-wise event recognition and image- wise importance prediction. We
collected an album dataset with both event type labels and image importance
labels, refined from an existing CUFED dataset. We propose a hybrid system
consisting of three parts: A siamese network-based event-specific image
importance prediction, a Convolutional Neural Network (CNN) that recognizes the
event type, and a Long Short-Term Memory (LSTM)-based sequence level event
recognizer. We propose an iterative updating procedure for event type and image
importance score prediction. We experimentally verified that image importance
score prediction and event type recognition can each help the performance of
the other.


DeepJ: Style-Specific Music Generation

  Recent advances in deep neural networks have enabled algorithms to compose
music that is comparable to music composed by humans. However, few algorithms
allow the user to generate music with tunable parameters. The ability to tune
properties of generated music will yield more practical benefits for aiding
artists, filmmakers, and composers in their creative tasks. In this paper, we
introduce DeepJ - an end-to-end generative model that is capable of composing
music conditioned on a specific mixture of composer styles. Our innovations
include methods to learn musical style and music dynamics. We use our model to
demonstrate a simple technique for controlling the style of generated music as
a proof of concept. Evaluation of our model using human raters shows that we
have improved over the Biaxial LSTM approach.


Deep Active Object Recognition by Joint Label and Action Prediction

  An active object recognition system has the advantage of being able to act in
the environment to capture images that are more suited for training and that
lead to better performance at test time. In this paper, we propose a deep
convolutional neural network for active object recognition that simultaneously
predicts the object label, and selects the next action to perform on the object
with the aim of improving recognition performance. We treat active object
recognition as a reinforcement learning problem and derive the cost function to
train the network for joint prediction of the object label and the action. A
generative model of object similarities based on the Dirichlet distribution is
proposed and embedded in the network for encoding the state of the system. The
training is carried out by simultaneously minimizing the label and action
prediction errors using gradient descent. We empirically show that the proposed
network is able to predict both the object label and the actions on GERMS, a
dataset for active object recognition. We compare the test label prediction
accuracy of the proposed model with Dirichlet and Naive Bayes state encoding.
The results of experiments suggest that the proposed model equipped with
Dirichlet state encoding is superior in performance, and selects images that
lead to better training and higher accuracy of label prediction at test time.


Basic Level Categorization Facilitates Visual Object Recognition

  Recent advances in deep learning have led to significant progress in the
computer vision field, especially for visual object recognition tasks. The
features useful for object classification are learned by feed-forward deep
convolutional neural networks (CNNs) automatically, and they are shown to be
able to predict and decode neural representations in the ventral visual pathway
of humans and monkeys. However, despite the huge amount of work on optimizing
CNNs, there has not been much research focused on linking CNNs with guiding
principles from the human visual cortex. In this work, we propose a network
optimization strategy inspired by both of the developmental trajectory of
children's visual object recognition capabilities, and Bar (2003), who
hypothesized that basic level information is carried in the fast magnocellular
pathway through the prefrontal cortex (PFC) and then projected back to inferior
temporal cortex (IT), where subordinate level categorization is achieved. We
instantiate this idea by training a deep CNN to perform basic level object
categorization first, and then train it on subordinate level categorization. We
apply this idea to training AlexNet (Krizhevsky et al., 2012) on the ILSVRC
2012 dataset and show that the top-5 accuracy increases from 80.13% to 82.14%,
demonstrating the effectiveness of the method. We also show that subsequent
transfer learning on smaller datasets gives superior results.


Central and peripheral vision for scene recognition: A
  neurocomputational modeling exploration

  What are the roles of central and peripheral vision in human scene
recognition? Larson and Loschky (2009) showed that peripheral vision
contributes more than central vision in obtaining maximum scene recognition
accuracy. However, central vision is more efficient for scene recognition than
peripheral, based on the amount of visual area needed for accurate recognition.
In this study, we model and explain the results of Larson and Loschky (2009)
using a neurocomputational modeling approach. We show that the advantage of
peripheral vision in scene recognition, as well as the efficiency advantage for
central vision, can be replicated using state-of-the-art deep neural network
models. In addition, we propose and provide support for the hypothesis that the
peripheral advantage comes from the inherent usefulness of peripheral features.
This result is consistent with data presented by Thibaut, Tran, Szaffarczyk,
and Boucart (2014), who showed that patients with central vision loss can still
categorize natural scenes efficiently. Furthermore, by using a deep
mixture-of-experts model ("The Deep Model," or TDM) that receives central and
peripheral visual information on separate channels simultaneously, we show that
the peripheral advantage emerges naturally in the learning process: When
trained to categorize scenes, the model weights the peripheral pathway more
than the central pathway. As we have seen in our previous modeling work,
learning creates a transform that spreads different scene categories into
different regions in representational space. Finally, we visualize the features
for the two pathways, and find that different preferences for scene categories
emerge for the two pathways during the training process.


Hierarchical Cellular Automata for Visual Saliency

  Saliency detection, finding the most important parts of an image, has become
increasingly popular in computer vision. In this paper, we introduce
Hierarchical Cellular Automata (HCA) -- a temporally evolving model to
intelligently detect salient objects. HCA consists of two main components:
Single-layer Cellular Automata (SCA) and Cuboid Cellular Automata (CCA). As an
unsupervised propagation mechanism, Single-layer Cellular Automata can exploit
the intrinsic relevance of similar regions through interactions with neighbors.
Low-level image features as well as high-level semantic information extracted
from deep neural networks are incorporated into the SCA to measure the
correlation between different image patches. With these hierarchical deep
features, an impact factor matrix and a coherence matrix are constructed to
balance the influences on each cell's next state. The saliency values of all
cells are iteratively updated according to a well-defined update rule.
Furthermore, we propose CCA to integrate multiple saliency maps generated by
SCA at different scales in a Bayesian framework. Therefore, single-layer
propagation and multi-layer integration are jointly modeled in our unified HCA.
Surprisingly, we find that the SCA can improve all existing methods that we
applied it to, resulting in a similar precision level regardless of the
original results. The CCA can act as an efficient pixel-wise aggregation
algorithm that can integrate state-of-the-art methods, resulting in even better
results. Extensive experiments on four challenging datasets demonstrate that
the proposed algorithm outperforms state-of-the-art conventional methods and is
competitive with deep learning based approaches.


Belief Tree Search for Active Object Recognition

  Active Object Recognition (AOR) has been approached as an unsupervised
learning problem, in which optimal trajectories for object inspection are not
known and are to be discovered by reducing label uncertainty measures or
training with reinforcement learning. Such approaches have no guarantees of the
quality of their solution. In this paper, we treat AOR as a Partially
Observable Markov Decision Process (POMDP) and find near-optimal policies on
training data using Belief Tree Search (BTS) on the corresponding belief Markov
Decision Process (MDP). AOR then reduces to the problem of knowledge transfer
from near-optimal policies on training set to the test set. We train a Long
Short Term Memory (LSTM) network to predict the best next action on the
training set rollouts. We sho that the proposed AOR method generalizes well to
novel views of familiar objects and also to novel objects. We compare this
supervised scheme against guided policy search, and find that the LSTM network
reaches higher recognition accuracy compared to the guided policy method. We
further look into optimizing the observation function to increase the total
collected reward of optimal policy. In AOR, the observation function is known
only approximately. We propose a gradient-based method update to this
approximate observation function to increase the total reward of any policy. We
show that by optimizing the observation function and retraining the supervised
LSTM network, the AOR performance on the test set improves significantly.


Deep-ESN: A Multiple Projection-encoding Hierarchical Reservoir
  Computing Framework

  As an efficient recurrent neural network (RNN) model, reservoir computing
(RC) models, such as Echo State Networks, have attracted widespread attention
in the last decade. However, while they have had great success with time series
data [1], [2], many time series have a multiscale structure, which a
single-hidden-layer RC model may have difficulty capturing. In this paper, we
propose a novel hierarchical reservoir computing framework we call Deep Echo
State Networks (Deep-ESNs). The most distinctive feature of a Deep-ESN is its
ability to deal with time series through hierarchical projections.
Specifically, when an input time series is projected into the high-dimensional
echo-state space of a reservoir, a subsequent encoding layer (e.g., a PCA,
autoencoder, or a random projection) can project the echo-state representations
into a lower-dimensional space. These low-dimensional representations can then
be processed by another ESN. By using projection layers and encoding layers
alternately in the hierarchical framework, a Deep-ESN can not only attenuate
the effects of the collinearity problem in ESNs, but also fully take advantage
of the temporal kernel property of ESNs to explore multiscale dynamics of time
series. To fuse the multiscale representations obtained by each reservoir, we
add connections from each encoding layer to the last output layer. Theoretical
analyses prove that stability of a Deep-ESN is guaranteed by the echo state
property (ESP), and the time complexity is equivalent to a conventional ESN.
Experimental results on some artificial and real world time series demonstrate
that Deep-ESNs can capture multiscale dynamics, and outperform both standard
ESNs and previous hierarchical ESN-based models.


