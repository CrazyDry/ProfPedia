Grasping for a Purpose: Using Task Goals for Efficient Manipulation
  Planning

  In this paper we propose an approach for efficient grasp selection for
manipulation tasks of unknown objects. Even for simple tasks such as
pick-and-place, a unique solution is rare to occur. Rather, multiple candidate
grasps must be considered and (potentially) tested till a successful,
kinematically feasible path is found. To make this process efficient, the
grasps should be ordered such that those more likely to succeed are tested
first. We propose to use grasp manipulability as a metric to prioritize grasps.
We present results of simulation experiments which demonstrate the usefulness
of our metric. Additionally, we present experiments with our physical robot
performing simple manipulation tasks with a small set of different household
objects.


Occlusion-Aware Object Localization, Segmentation and Pose Estimation

  We present a learning approach for localization and segmentation of objects
in an image in a manner that is robust to partial occlusion. Our algorithm
produces a bounding box around the full extent of the object and labels pixels
in the interior that belong to the object. Like existing segmentation aware
detection approaches, we learn an appearance model of the object and consider
regions that do not fit this model as potential occlusions. However, in
addition to the established use of pairwise potentials for encouraging local
consistency, we use higher order potentials which capture information at the
level of im- age segments. We also propose an efficient loss function that
targets both localization and segmentation performance. Our algorithm achieves
13.52% segmentation error and 0.81 area under the false-positive per image vs.
recall curve on average over the challenging CMU Kitchen Occlusion Dataset.
This is a 42.44% decrease in segmentation error and a 16.13% increase in
localization performance compared to the state-of-the-art. Finally, we show
that the visibility labelling produced by our algorithm can make full 3D pose
estimation from a single image robust to occlusion.


From the Lab to the Desert: Fast Prototyping and Learning of Robot
  Locomotion

  We present a methodology for fast prototyping of morphologies and controllers
for robot locomotion. Going beyond simulation-based approaches, we argue that
the form and function of a robot, as well as their interplay with real-world
environmental conditions are critical. Hence, fast design and learning cycles
are necessary to adapt robot shape and behavior to their environment. To this
end, we present a combination of laminate robot manufacturing and
sample-efficient reinforcement learning. We leverage this methodology to
conduct an extensive robot learning experiment. Inspired by locomotion in sea
turtles, we design a low-cost crawling robot with variable, interchangeable
fins. Learning is performed using both bio-inspired and original fin designs in
an artificial indoor environment as well as a natural environment in the
Arizona desert. The findings of this study show that static policies developed
in the laboratory do not translate to effective locomotion strategies in
natural environments. In contrast to that, sample-efficient reinforcement
learning can help to rapidly accommodate changes in the environment or the
robot.


Deep Predictive Models for Collision Risk Assessment in Autonomous
  Driving

  In this paper, we investigate a predictive approach for collision risk
assessment in autonomous and assisted driving. A deep predictive model is
trained to anticipate imminent accidents from traditional video streams. In
particular, the model learns to identify cues in RGB images that are predictive
of hazardous upcoming situations. In contrast to previous work, our approach
incorporates (a) temporal information during decision making, (b) multi-modal
information about the environment, as well as the proprioceptive state and
steering actions of the controlled vehicle, and (c) information about the
uncertainty inherent to the task. To this end, we discuss Deep Predictive
Models and present an implementation using a Bayesian Convolutional LSTM.
Experiments in a simple simulation environment show that the approach can learn
to predict impending accidents with reasonable accuracy, especially when
multiple cameras are used as input sources.


Information Maximizing Exploration with a Latent Dynamics Model

  All reinforcement learning algorithms must handle the trade-off between
exploration and exploitation. Many state-of-the-art deep reinforcement learning
methods use noise in the action selection, such as Gaussian noise in policy
gradient methods or $\epsilon$-greedy in Q-learning. While these methods are
appealing due to their simplicity, they do not explore the state space in a
methodical manner. We present an approach that uses a model to derive reward
bonuses as a means of intrinsic motivation to improve model-free reinforcement
learning. A key insight of our approach is that this dynamics model can be
learned in the latent feature space of a value function, representing the
dynamics of the agent and the environment. This method is both theoretically
grounded and computationally advantageous, permitting the efficient use of
Bayesian information-theoretic methods in high-dimensional state spaces. We
evaluate our method on several continuous control tasks, focusing on improving
exploration.


