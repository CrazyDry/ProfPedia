Shallow Discourse Parsing Using Distributed Argument Representations and
  Bayesian Optimization

  This paper describes the Georgia Tech team's approach to the CoNLL-2016
supplementary evaluation on discourse relation sense classification. We use
long short-term memories (LSTM) to induce distributed representations of each
argument, and then combine these representations with surface features in a
neural network. The architecture of the neural network is determined by
Bayesian hyperparameter search.


Better Document-level Sentiment Analysis from RST Discourse Parsing

  Discourse structure is the hidden link between surface features and
document-level properties, such as sentiment polarity. We show that the
discourse analyses produced by Rhetorical Structure Theory (RST) parsers can
improve document-level sentiment analysis, via composition of local information
up the discourse tree. First, we show that reweighting discourse units
according to their position in a dependency representation of the rhetorical
structure can yield substantial improvements on lexicon-based sentiment
analysis. Next, we present a recursive neural network over the RST structure,
which offers significant improvements over classification-based methods.


TopicViz: Semantic Navigation of Document Collections

  When people explore and manage information, they think in terms of topics and
themes. However, the software that supports information exploration sees text
at only the surface level. In this paper we show how topic modeling -- a
technique for identifying latent themes across large collections of documents
-- can support semantic exploration. We present TopicViz, an interactive
environment for information exploration. TopicViz combines traditional search
and citation-graph functionality with a range of novel interactive
visualizations, centered around a force-directed layout that links documents to
the latent themes discovered by the topic model. We describe several use
scenarios in which TopicViz supports rapid sensemaking on large document
collections.


Nonparametric Bayesian Storyline Detection from Microtexts

  News events and social media are composed of evolving storylines, which
capture public attention for a limited period of time. Identifying storylines
requires integrating temporal and linguistic information, and prior work takes
a largely heuristic approach. We present a novel online non-parametric Bayesian
framework for storyline detection, using the distance-dependent Chinese
Restaurant Process (dd-CRP). To ensure efficient linear-time inference, we
employ a fixed-lag Gibbs sampling procedure, which is novel for the dd-CRP. We
evaluate on the TREC Twitter Timeline Generation (TTG), obtaining encouraging
results: despite using a weak baseline retrieval model, the dd-CRP story
clustering method is competitive with the best entries in the 2014 TTG task.


"You're Mr. Lebowski, I'm the Dude": Inducing Address Term Formality in
  Signed Social Networks

  We present an unsupervised model for inducing signed social networks from the
content exchanged across network edges. Inference in this model solves three
problems simultaneously: (1) identifying the sign of each edge; (2)
characterizing the distribution over content for each edge type; (3) estimating
weights for triadic features that map to theoretical models such as structural
balance. We apply this model to the problem of inducing the social function of
address terms, such as 'Madame', 'comrade', and 'dude'. On a dataset of movie
scripts, our system obtains a coherent clustering of address terms, while at
the same time making intuitively plausible judgments of the formality of social
relations in each film. As an additional contribution, we provide a
bootstrapping technique for identifying and tagging address terms in dialogue.


Confounds and Consequences in Geotagged Twitter Data

  Twitter is often used in quantitative studies that identify
geographically-preferred topics, writing styles, and entities. These studies
rely on either GPS coordinates attached to individual messages, or on the
user-supplied location field in each profile. In this paper, we compare these
data acquisition techniques and quantify the biases that they introduce; we
also measure their effects on linguistic analysis and text-based geolocation.
GPS-tagging and self-reported locations yield measurably different corpora, and
these linguistic differences are partially attributable to differences in
dataset composition by age and gender. Using a latent variable model to induce
age and gender, we show how these demographic variables interact with geography
to affect language use. We also show that the accuracy of text-based
geolocation varies with population demographics, giving the best results for
men above the age of 40.


Morphological Priors for Probabilistic Neural Word Embeddings

  Word embeddings allow natural language processing systems to share
statistical information across related words. These embeddings are typically
based on distributional statistics, making it difficult for them to generalize
to rare or unseen words. We propose to improve word embeddings by incorporating
morphological information, capturing shared sub-word features. Unlike previous
work that constructs word embeddings directly from morphemes, we combine
morphological and distributional information in a unified probabilistic
framework, in which the word embedding is a latent variable. The morphological
information provides a prior distribution on the latent word embeddings, which
in turn condition a likelihood function over an observed corpus. This approach
yields improvements on intrinsic word similarity evaluations, and also in the
downstream task of part-of-speech tagging.


Unsupervised Learning for Lexicon-Based Classification

  In lexicon-based classification, documents are assigned labels by comparing
the number of words that appear from two opposed lexicons, such as positive and
negative sentiment. Creating such words lists is often easier than labeling
instances, and they can be debugged by non-experts if classification
performance is unsatisfactory. However, there is little analysis or
justification of this classification heuristic. This paper describes a set of
assumptions that can be used to derive a probabilistic justification for
lexicon-based classification, as well as an analysis of its expected accuracy.
One key assumption behind lexicon-based classification is that all words in
each lexicon are equally predictive. This is rarely true in practice, which is
why lexicon-based approaches are usually outperformed by supervised classifiers
that learn distinct weights on each word from labeled instances. This paper
shows that it is possible to learn such weights without labeled data, by
leveraging co-occurrence statistics across the lexicons. This offers the best
of both worlds: light supervision in the form of lexicons, and data-driven
classification with higher accuracy than traditional word-counting heuristics.


Mind Your POV: Convergence of Articles and Editors Towards Wikipedia's
  Neutrality Norm

  Wikipedia has a strong norm of writing in a 'neutral point of view' (NPOV).
Articles that violate this norm are tagged, and editors are encouraged to make
corrections. But the impact of this tagging system has not been quantitatively
measured. Does NPOV tagging help articles to converge to the desired style? Do
NPOV corrections encourage editors to adopt this style? We study these
questions using a corpus of NPOV-tagged articles and a set of lexicons
associated with biased language. An interrupted time series analysis shows that
after an article is tagged for NPOV, there is a significant decrease in biased
language in the article, as measured by several lexicons. However, for
individual editors, NPOV corrections and talk page discussions yield no
significant change in the usage of words in most of these lexicons, including
Wikipedia's own list of 'words to watch.' This suggests that NPOV tagging and
discussion does improve content, but has less success enculturating editors to
the site's linguistic norms.


The Referential Reader: A Recurrent Entity Network for Anaphora
  Resolution

  We present a new architecture for storing and accessing entity mentions
during online text processing. While reading the text, entity references are
identified, and may be stored by either updating or overwriting a cell in a
fixed-length memory. The update operation implies coreference with the other
mentions that are stored in the same cell; the overwrite operations causes
these mentions to be forgotten. By encoding the memory operations as
differentiable gates, it is possible to train the model end-to-end, using both
a supervised anaphora resolution objective as well as a supplementary language
modeling objective. Evaluation on a dataset of pronoun-name anaphora
demonstrates that the model achieves state-of-the-art performance with purely
left-to-right processing of the text.


Unsupervised Domain Adaptation of Contextualized Embeddings: A Case
  Study in Early Modern English

  Contextualized word embeddings such as ELMo and BERT provide a foundation for
strong performance across a range of natural language processing tasks, in part
by pretraining on a large and topically-diverse corpus. However, the
applicability of this approach is unknown when the target domain varies
substantially from the text used during pretraining. Specifically, we are
interested the scenario in which labeled data is available in only a canonical
source domain such as newstext, and the target domain is distinct from both the
labeled corpus and the pretraining data. To address this scenario, we propose
domain-adaptive fine-tuning, in which the contextualized embeddings are adapted
by masked language modeling on the target domain. We test this approach on the
challenging domain of Early Modern English, which differs substantially from
existing pretraining corpora. Domain-adaptive fine-tuning yields an improvement
of 4\% in part-of-speech tagging accuracy over a BERT baseline, substantially
improving on prior work on this task.


Unsupervised Domain Adaptation with Feature Embeddings

  Representation learning is the dominant technique for unsupervised domain
adaptation, but existing approaches often require the specification of "pivot
features" that generalize across domains, which are selected by task-specific
heuristics. We show that a novel but simple feature embedding approach provides
better performance, by exploiting the feature template structure common in NLP
problems.


Sí o no, què penses? Catalonian Independence and Linguistic Identity
  on Social Media

  Political identity is often manifested in language variation, but the
relationship between the two is still relatively unexplored from a quantitative
perspective. This study examines the use of Catalan, a language local to the
semi-autonomous region of Catalonia in Spain, on Twitter in discourse related
to the 2017 independence referendum. We corroborate prior findings that
pro-independence tweets are more likely to include the local language than
anti-independence tweets. We also find that Catalan is used more often in
referendum-related discourse than in other contexts, contrary to prior findings
on language variation. This suggests a strong role for the Catalan language in
the expression of Catalonian political identity.


Gender identity and lexical variation in social media

  We present a study of the relationship between gender, linguistic style, and
social networks, using a novel corpus of 14,000 Twitter users. Prior
quantitative work on gender often treats this social variable as a female/male
binary; we argue for a more nuanced approach. By clustering Twitter users, we
find a natural decomposition of the dataset into various styles and topical
interests. Many clusters have strong gender orientations, but their use of
linguistic resources sometimes directly conflicts with the population-level
language statistics. We view these clusters as a more accurate reflection of
the multifaceted nature of gendered language styles. Previous corpus-based work
has also had little to say about individuals whose linguistic styles defy
population-level gender patterns. To identify such individuals, we train a
statistical classifier, and measure the classifier confidence for each
individual in the dataset. Examining individuals whose language does not match
the classifier's model for their gender, we find that they have social networks
that include significantly fewer same-gender social connections and that, in
general, social network homophily is correlated with the use of same-gender
language markers. Pairing computational methods and social theory thus offers a
new perspective on how gender emerges as individuals position themselves
relative to audiences, topics, and mainstream gender norms.


Diffusion of Lexical Change in Social Media

  Computer-mediated communication is driving fundamental changes in the nature
of written language. We investigate these changes by statistical analysis of a
dataset comprising 107 million Twitter messages (authored by 2.7 million unique
user accounts). Using a latent vector autoregressive model to aggregate across
thousands of words, we identify high-level patterns in diffusion of linguistic
change over the United States. Our model is robust to unpredictable changes in
Twitter's sampling rate, and provides a probabilistic characterization of the
relationship of macro-scale linguistic influence to a set of demographic and
geographic predictors. The results of this analysis offer support for prior
arguments that focus on geographical proximity and population size. However,
demographic similarity -- especially with regard to race -- plays an even more
central role, as cities with similar racial demographics are far more likely to
share linguistic influence. Rather than moving towards a single unified
"netspeak" dialect, language evolution in computer-mediated communication
reproduces existing fault lines in spoken American English.


The Social Dynamics of Language Change in Online Networks

  Language change is a complex social phenomenon, revealing pathways of
communication and sociocultural influence. But, while language change has long
been a topic of study in sociolinguistics, traditional linguistic research
methods rely on circumstantial evidence, estimating the direction of change
from differences between older and younger speakers. In this paper, we use a
data set of several million Twitter users to track language changes in
progress. First, we show that language change can be viewed as a form of social
influence: we observe complex contagion for phonetic spellings and "netspeak"
abbreviations (e.g., lol), but not for older dialect markers from spoken
language. Next, we test whether specific types of social network connections
are more influential than others, using a parametric Hawkes process model. We
find that tie strength plays an important role: densely embedded social ties
are significantly better conduits of linguistic influence. Geographic locality
appears to play a more limited role: we find relatively little evidence to
support the hypothesis that individuals are more influenced by geographically
local social ties, even in their usage of geographical dialect markers.


Multilingual Part-of-Speech Tagging: Two Unsupervised Approaches

  We demonstrate the effectiveness of multilingual learning for unsupervised
part-of-speech tagging. The central assumption of our work is that by combining
cues from multiple languages, the structure of each becomes more apparent. We
consider two ways of applying this intuition to the problem of unsupervised
part-of-speech tagging: a model that directly merges tag structures for a pair
of languages into a single sequence and a second model which instead
incorporates multilingual context using latent variables. Both approaches are
formulated as hierarchical Bayesian models, using Markov Chain Monte Carlo
sampling techniques for inference. Our results demonstrate that by
incorporating multilingual evidence we can achieve impressive performance gains
across a range of scenarios. We also found that performance improves steadily
as the number of available languages increases.


Entity-Augmented Distributional Semantics for Discourse Relations

  Discourse relations bind smaller linguistic elements into coherent texts.
However, automatically identifying discourse relations is difficult, because it
requires understanding the semantics of the linked sentences. A more subtle
challenge is that it is not enough to represent the meaning of each sentence of
a discourse relation, because the relation may depend on links between
lower-level elements, such as entity mentions. Our solution computes
distributional meaning representations by composition up the syntactic parse
tree. A key difference from previous work on compositional distributional
semantics is that we also compute representations for entity mentions, using a
novel downward compositional pass. Discourse relations are predicted not only
from the distributional representations of the sentences, but also of their
coreferent entity mentions. The resulting system obtains substantial
improvements over the previous state-of-the-art in predicting implicit
discourse relations in the Penn Discourse Treebank.


Document Context Language Models

  Text documents are structured on multiple levels of detail: individual words
are related by syntax, but larger units of text are related by discourse
structure. Existing language models generally fail to account for discourse
structure, but it is crucial if we are to have language models that reward
coherence and generate coherent texts. We present and empirically evaluate a
set of multi-level recurrent neural network language models, called
Document-Context Language Models (DCLM), which incorporate contextual
information both within and beyond the sentence. In comparison with word-level
recurrent neural network language models, the DCLM models obtain slightly
better predictive likelihoods, and considerably better assessments of document
coherence.


Overcoming Language Variation in Sentiment Analysis with Social
  Attention

  Variation in language is ubiquitous, particularly in newer forms of writing
such as social media. Fortunately, variation is not random, it is often linked
to social properties of the author. In this paper, we show how to exploit
social networks to make sentiment analysis more robust to social language
variation. The key idea is linguistic homophily: the tendency of socially
linked individuals to use language in similar ways. We formalize this idea in a
novel attention-based neural network architecture, in which attention is
divided among several basis models, depending on the author's position in the
social network. This has the effect of smoothing the classification function
across the social network, and makes it possible to induce personalized
classifiers even for authors for whom there is no labeled data or demographic
metadata. This model significantly improves the accuracies of sentiment
analysis on Twitter and on review data.


A Kernel Independence Test for Geographical Language Variation

  Quantifying the degree of spatial dependence for linguistic variables is a
key task for analyzing dialectal variation. However, existing approaches have
important drawbacks. First, they are based on parametric models of dependence,
which limits their power in cases where the underlying parametric assumptions
are violated. Second, they are not applicable to all types of linguistic data:
some approaches apply only to frequencies, others to boolean indicators of
whether a linguistic variable is present. We present a new method for measuring
geographical language variation, which solves both of these problems. Our
approach builds on Reproducing Kernel Hilbert space (RKHS) representations for
nonparametric statistics, and takes the form of a test statistic that is
computed from pairs of individual geotagged observations without aggregation
into predefined geographical bins. We compare this test with prior work using
synthetic data as well as a diverse set of real datasets: a corpus of Dutch
tweets, a Dutch syntactic atlas, and a dataset of letters to the editor in
North American newspapers. Our proposed test is shown to support robust
inferences across a broad range of scenarios and types of data.


A Latent Variable Recurrent Neural Network for Discourse Relation
  Language Models

  This paper presents a novel latent variable recurrent neural network
architecture for jointly modeling sequences of words and (possibly latent)
discourse relations between adjacent sentences. A recurrent neural network
generates individual words, thus reaping the benefits of
discriminatively-trained vector representations. The discourse relations are
represented with a latent variable, which can be predicted or marginalized,
depending on the task. The resulting model can therefore employ a training
objective that includes not only discourse relation classification, but also
word prediction. As a result, it outperforms state-of-the-art alternatives for
two tasks: implicit discourse relation classification in the Penn Discourse
Treebank, and dialog act classification in the Switchboard corpus. Furthermore,
by marginalizing over latent discourse relations at test time, we obtain a
discourse informed language model, which improves over a strong LSTM baseline.


Part-of-Speech Tagging for Historical English

  As more historical texts are digitized, there is interest in applying natural
language processing tools to these archives. However, the performance of these
tools is often unsatisfactory, due to language change and genre differences.
Spelling normalization heuristics are the dominant solution for dealing with
historical texts, but this approach fails to account for changes in usage and
vocabulary. In this empirical paper, we assess the capability of domain
adaptation techniques to cope with historical texts, focusing on the classic
benchmark task of part-of-speech tagging. We evaluate several domain adaptation
methods on the task of tagging Early Modern English and Modern British English
texts in the Penn Corpora of Historical English. We demonstrate that the
Feature Embedding method for unsupervised domain adaptation outperforms word
embeddings and Brown clusters, showing the importance of embedding the entire
feature space, rather than just individual words. Feature Embeddings also give
better performance than spelling normalization, but the combination of the two
methods is better still, yielding a 5% raw improvement in tagging accuracy on
Early Modern English texts.


One Vector is Not Enough: Entity-Augmented Distributional Semantics for
  Discourse Relations

  Discourse relations bind smaller linguistic units into coherent texts.
However, automatically identifying discourse relations is difficult, because it
requires understanding the semantics of the linked arguments. A more subtle
challenge is that it is not enough to represent the meaning of each argument of
a discourse relation, because the relation may depend on links between
lower-level components, such as entity mentions. Our solution computes
distributional meaning representations by composition up the syntactic parse
tree. A key difference from previous work on compositional distributional
semantics is that we also compute representations for entity mentions, using a
novel downward compositional pass. Discourse relations are predicted from the
distributional representations of the arguments, and also of their coreferent
entity mentions. The resulting system obtains substantial improvements over the
previous state-of-the-art in predicting implicit discourse relations in the
Penn Discourse Treebank.


Emoticons vs. Emojis on Twitter: A Causal Inference Approach

  Online writing lacks the non-verbal cues present in face-to-face
communication, which provide additional contextual information about the
utterance, such as the speaker's intention or affective state. To fill this
void, a number of orthographic features, such as emoticons, expressive
lengthening, and non-standard punctuation, have become popular in social media
services including Twitter and Instagram. Recently, emojis have been introduced
to social media, and are increasingly popular. This raises the question of
whether these predefined pictographic characters will come to replace earlier
orthographic methods of paralinguistic communication. In this abstract, we
attempt to shed light on this question, using a matching approach from causal
inference to test whether the adoption of emojis causes individual users to
employ fewer emoticons in their text on Twitter.


Toward Socially-Infused Information Extraction: Embedding Authors,
  Mentions, and Entities

  Entity linking is the task of identifying mentions of entities in text, and
linking them to entries in a knowledge base. This task is especially difficult
in microblogs, as there is little additional text to provide disambiguating
context; rather, authors rely on an implicit common ground of shared knowledge
with their readers. In this paper, we attempt to capture some of this implicit
context by exploiting the social network structure in microblogs. We build on
the theory of homophily, which implies that socially linked individuals share
interests, and are therefore likely to mention the same sorts of entities. We
implement this idea by encoding authors, mentions, and entities in a continuous
vector space, which is constructed so that socially-connected authors have
similar vector representations. These vectors are incorporated into a neural
structured prediction model, which captures structural constraints that are
inherent in the entity linking task. Together, these design decisions yield F1
improvements of 1%-5% on benchmark datasets, as compared to the previous
state-of-the-art.


Mimicking Word Embeddings using Subword RNNs

  Word embeddings improve generalization over lexical features by placing each
word in a lower-dimensional space, using distributional information obtained
from unlabeled data. However, the effectiveness of word embeddings for
downstream NLP tasks is limited by out-of-vocabulary (OOV) words, for which
embeddings do not exist. In this paper, we present MIMICK, an approach to
generating OOV word embeddings compositionally, by learning a function from
spellings to distributional embeddings. Unlike prior work, MIMICK does not
require re-training on the original word embedding corpus; instead, learning is
performed at the type level. Intrinsic and extrinsic evaluations demonstrate
the power of this simple approach. On 23 languages, MIMICK improves performance
over a word-based baseline for tagging part-of-speech and morphosyntactic
attributes. It is competitive with (and complementary to) a supervised
character-based model in low-resource settings.


Making "fetch" happen: The influence of social and linguistic context on
  nonstandard word growth and decline

  In an online community, new words come and go: today's "haha" may be replaced
by tomorrow's "lol." Changes in online writing are usually studied as a social
process, with innovations diffusing through a network of individuals in a
speech community. But unlike other types of innovation, language change is
shaped and constrained by the system in which it takes part. To investigate the
links between social and structural factors in language change, we undertake a
large-scale analysis of nonstandard word growth in the online community Reddit.
We find that dissemination across many linguistic contexts is a sign of growth:
words that appear in more linguistic contexts grow faster and survive longer.
We also find that social dissemination likely plays a less important role in
explaining word growth and decline than previously hypothesized.


Making "fetch" happen: The influence of social and linguistic context on
  nonstandard word growth and decline

  In an online community, new words come and go: today's "haha" may be replaced
by tomorrow's "lol." Changes in online writing are usually studied as a social
process, with innovations diffusing through a network of individuals in a
speech community. But unlike other types of innovation, language change is
shaped and constrained by the system in which it takes part. To investigate the
links between social and structural factors in language change, we undertake a
large-scale analysis of nonstandard word growth in the online community Reddit.
We find that dissemination across many linguistic contexts is a sign of growth:
words that appear in more linguistic contexts grow faster and survive longer.
We also find that social dissemination likely plays a less important role in
explaining word growth and decline than previously hypothesized.


Explainable Prediction of Medical Codes from Clinical Text

  Clinical notes are text documents that are created by clinicians for each
patient encounter. They are typically accompanied by medical codes, which
describe the diagnosis and treatment. Annotating these codes is labor intensive
and error prone; furthermore, the connection between the codes and the text is
not annotated, obscuring the reasons and details behind specific diagnoses and
treatments. We present an attentional convolutional network that predicts
medical codes from clinical text. Our method aggregates information across the
document using a convolutional neural network, and uses an attention mechanism
to select the most relevant segments for each of the thousands of possible
codes. The method is accurate, achieving precision@8 of 0.71 and a Micro-F1 of
0.54, which are both better than the prior state of the art. Furthermore,
through an interpretability evaluation by a physician, we show that the
attention mechanism identifies meaningful explanations for each code assignment


Discriminative Modeling of Social Influence for Prediction and
  Explanation in Event Cascades

  The global dynamics of event cascades are often governed by the local
dynamics of peer influence. However, detecting social influence from
observational data is challenging, due to confounds like homophily and
practical issues like missing data. In this work, we propose a novel
discriminative method to detect influence from observational data. The core of
the approach is to train a ranking algorithm to predict the source of the next
event in a cascade, and compare its out-of-sample accuracy against a
competitive baseline which lacks access to features corresponding to social
influence. Using synthetically generated data, we provide empirical evidence
that this method correctly identifies influence in the presence of confounds,
and is robust to both missing data and misspecification --- unlike popular
alternatives. We also apply the method to two real-world datasets: (1) cascades
of co-sponsorship of legislation in the U.S. House of Representatives, on a
social network of shared campaign donors; (2) rumors about the Higgs boson
discovery, on a follower network of $10^5$ Twitter accounts. Our model
identifies the role of peer influence in these scenarios, and uses it to make
more accurate predictions about the future trajectory of cascades.


Stylistic Variation in Social Media Part-of-Speech Tagging

  Social media features substantial stylistic variation, raising new challenges
for syntactic analysis of online writing. However, this variation is often
aligned with author attributes such as age, gender, and geography, as well as
more readily-available social network metadata. In this paper, we report new
evidence on the link between language and social networks in the task of
part-of-speech tagging. We find that tagger error rates are correlated with
network structure, with high accuracy in some parts of the network, and lower
accuracy elsewhere. As a result, tagger accuracy depends on training from a
balanced sample of the network, rather than training on texts from a narrow
subcommunity. We also describe our attempts to add robustness to stylistic
variation, by building a mixture-of-experts model in which each expert is
associated with a region of the social network. While prior work found that
similar approaches yield performance improvements in sentiment analysis and
entity linking, we were unable to obtain performance improvements in
part-of-speech tagging, despite strong evidence for the link between
part-of-speech error rates and social network structure.


Predicting Semantic Relations using Global Graph Properties

  Semantic graphs, such as WordNet, are resources which curate natural language
on two distinguishable layers. On the local level, individual relations between
synsets (semantic building blocks) such as hypernymy and meronymy enhance our
understanding of the words used to express their meanings. Globally, analysis
of graph-theoretic properties of the entire net sheds light on the structure of
human language as a whole. In this paper, we combine global and local
properties of semantic graphs through the framework of Max-Margin Markov Graph
Models (M3GM), a novel extension of Exponential Random Graph Model (ERGM) that
scales to large multi-relational graphs. We demonstrate how such global
modeling improves performance on the local task of predicting semantic
relations between synsets, yielding new state-of-the-art results on the WN18RR
dataset, a challenging version of WordNet link prediction in which "easy"
reciprocal cases are removed. In addition, the M3GM model identifies
multirelational motifs that are characteristic of well-formed lexical semantic
ontologies.


Training on Synthetic Noise Improves Robustness to Natural Noise in
  Machine Translation

  We consider the problem of making machine translation more robust to
character-level variation at the source side, such as typos. Existing methods
achieve greater coverage by applying subword models such as byte-pair encoding
(BPE) and character-level encoders, but these methods are highly sensitive to
spelling mistakes. We show how training on a mild amount of random synthetic
noise can dramatically improve robustness to these variations, without
diminishing performance on clean text. We focus on translation performance on
natural noise, as captured by frequent corrections in Wikipedia edit logs, and
show that robustness to such noise can be achieved using a balanced diet of
simple synthetic noises at training time, without access to the natural noise
data or distribution.


Character Eyes: Seeing Language through Character-Level Taggers

  Character-level models have been used extensively in recent years in NLP
tasks as both supplements and replacements for closed-vocabulary token-level
word representations. In one popular architecture, character-level LSTMs are
used to feed token representations into a sequence tagger predicting
token-level annotations such as part-of-speech (POS) tags. In this work, we
examine the behavior of POS taggers across languages from the perspective of
individual hidden units within the character LSTM. We aggregate the behavior of
these units into language-level metrics which quantify the challenges that
taggers face on languages with different morphological properties, and identify
links between synthesis and affixation preference and emergent behavior of the
hidden tagger layer. In a comparative experiment, we show how modifying the
balance between forward and backward hidden units affects model arrangement and
performance in these types of languages.


Learning Document-Level Semantic Properties from Free-Text Annotations

  This paper presents a new method for inferring the semantic properties of
documents by leveraging free-text keyphrase annotations. Such annotations are
becoming increasingly abundant due to the recent dramatic growth in
semi-structured, user-generated online content. One especially relevant domain
is product reviews, which are often annotated by their authors with pros/cons
keyphrases such as a real bargain or good value. These annotations are
representative of the underlying semantic properties; however, unlike expert
annotations, they are noisy: lay authors may use different labels to denote the
same property, and some labels may be missing. To learn using such noisy
annotations, we find a hidden paraphrase structure which clusters the
keyphrases. The paraphrase structure is linked with a latent topic model of the
review texts, enabling the system to predict the properties of unannotated
documents and to effectively aggregate the semantic properties of multiple
reviews. Our approach is implemented as a hierarchical Bayesian model with
joint inference. We find that joint inference increases the robustness of the
keyphrase clustering and encourages the latent topics to correlate with
semantically meaningful properties. Multiple evaluations demonstrate that our
model substantially outperforms alternative approaches for summarizing single
and multiple documents into a set of semantically salient keyphrases.


#anorexia, #anarexia, #anarexyia: Characterizing Online Community
  Practices with Orthographic Variation

  Distinctive linguistic practices help communities build solidarity and
differentiate themselves from outsiders. In an online community, one such
practice is variation in orthography, which includes spelling, punctuation, and
capitalization. Using a dataset of over two million Instagram posts, we
investigate orthographic variation in a community that shares pro-eating
disorder (pro-ED) content. We find that not only does orthographic variation
grow more frequent over time, it also becomes more profound or deep, with
variants becoming increasingly distant from the original: as, for example,
#anarexyia is more distant than #anarexia from the original spelling #anorexia.
These changes are driven by newcomers, who adopt the most extreme linguistic
practices as they enter the community. Moreover, this behavior correlates with
engagement: the newcomers who adopt deeper orthographic variants tend to remain
active for longer in the community, and the posts that contain deeper variation
receive more positive feedback in the form of "likes." Previous work has linked
community membership change with language change, and our work casts this
connection in a new light, with newcomers driving an evolving practice, rather
than adapting to it. We also demonstrate the utility of orthographic variation
as a new lens to study sociolinguistic change in online communities,
particularly when the change results from an exogenous force such as a content
ban.


The Spitzer Deep, Wide-Field Survey

  The Spitzer Deep, Wide-Field Survey (SDWFS) is a four-epoch infrared survey
of ten square degrees in the Bootes field of the NOAO Deep Wide-Field Survey
using the IRAC instrument on the Spitzer Space Telescope. SDWFS, a Cycle four
Spitzer Legacy project, occupies a unique position in the area-depth survey
space defined by other Spitzer surveys. The four epochs that make up SDWFS
permit -- for the first time -- the selection of infrared-variable and high
proper motion objects over a wide field on timescales of years. Because of its
large survey volume, SDWFS is sensitive to galaxies out to z~3 with relatively
little impact from cosmic variance for all but the richest systems. The SDWFS
datasets will thus be especially useful for characterizing galaxy evolution
beyond z~1.5. This paper explains the SDWFS observing strategy and data
processing, presents the SDWFS mosaics and source catalogs, and discusses some
early scientific findings. The publicly-released, full-depth catalogs contain
6.78, 5.23, 1.20, and 0.96 x 10e5 distinct sources detected to the average
5-sigma, 4" diameter, aperture-corrected limits of 19.77, 18.83, 16.50, and
15.82 Vega mag at 3.6, 4.5, 5.8, and 8.0 micron, respectively. The SDWFS number
counts and color-color distribution are consistent with other, earlier Spitzer
surveys. At the 6 min integration time of the SDWFS IRAC imaging, more than 50%
of isolated FIRST radio sources and more than 80% of on-axis XBootes sources
are detected out to 8.0 micron. Finally, we present the four highest proper
motion IRAC-selected sources identified from the multi-epoch imaging, two of
which are likely field brown dwarfs of mid-T spectral class.


SDSS-III: Massive Spectroscopic Surveys of the Distant Universe, the
  Milky Way Galaxy, and Extra-Solar Planetary Systems

  Building on the legacy of the Sloan Digital Sky Survey (SDSS-I and II),
SDSS-III is a program of four spectroscopic surveys on three scientific themes:
dark energy and cosmological parameters, the history and structure of the Milky
Way, and the population of giant planets around other stars. In keeping with
SDSS tradition, SDSS-III will provide regular public releases of all its data,
beginning with SDSS DR8 (which occurred in Jan 2011). This paper presents an
overview of the four SDSS-III surveys. BOSS will measure redshifts of 1.5
million massive galaxies and Lya forest spectra of 150,000 quasars, using the
BAO feature of large scale structure to obtain percent-level determinations of
the distance scale and Hubble expansion rate at z<0.7 and at z~2.5. SEGUE-2,
which is now completed, measured medium-resolution (R=1800) optical spectra of
118,000 stars in a variety of target categories, probing chemical evolution,
stellar kinematics and substructure, and the mass profile of the dark matter
halo from the solar neighborhood to distances of 100 kpc. APOGEE will obtain
high-resolution (R~30,000), high signal-to-noise (S/N>100 per resolution
element), H-band (1.51-1.70 micron) spectra of 10^5 evolved, late-type stars,
measuring separate abundances for ~15 elements per star and creating the first
high-precision spectroscopic survey of all Galactic stellar populations (bulge,
bar, disks, halo) with a uniform set of stellar tracers and spectral
diagnostics. MARVELS will monitor radial velocities of more than 8000 FGK stars
with the sensitivity and cadence (10-40 m/s, ~24 visits per star) needed to
detect giant planets with periods up to two years, providing an unprecedented
data set for understanding the formation and dynamical evolution of giant
planet systems. (Abridged)


The DESI Experiment Part I: Science,Targeting, and Survey Design

  DESI (Dark Energy Spectroscopic Instrument) is a Stage IV ground-based dark
energy experiment that will study baryon acoustic oscillations (BAO) and the
growth of structure through redshift-space distortions with a wide-area galaxy
and quasar redshift survey. To trace the underlying dark matter distribution,
spectroscopic targets will be selected in four classes from imaging data. We
will measure luminous red galaxies up to $z=1.0$. To probe the Universe out to
even higher redshift, DESI will target bright [O II] emission line galaxies up
to $z=1.7$. Quasars will be targeted both as direct tracers of the underlying
dark matter distribution and, at higher redshifts ($ 2.1 < z < 3.5$), for the
Ly-$\alpha$ forest absorption features in their spectra, which will be used to
trace the distribution of neutral hydrogen. When moonlight prevents efficient
observations of the faint targets of the baseline survey, DESI will conduct a
magnitude-limited Bright Galaxy Survey comprising approximately 10 million
galaxies with a median $z\approx 0.2$. In total, more than 30 million galaxy
and quasar redshifts will be obtained to measure the BAO feature and determine
the matter power spectrum, including redshift space distortions.


The DESI Experiment Part II: Instrument Design

  DESI (Dark Energy Spectropic Instrument) is a Stage IV ground-based dark
energy experiment that will study baryon acoustic oscillations and the growth
of structure through redshift-space distortions with a wide-area galaxy and
quasar redshift survey. The DESI instrument is a robotically-actuated,
fiber-fed spectrograph capable of taking up to 5,000 simultaneous spectra over
a wavelength range from 360 nm to 980 nm. The fibers feed ten three-arm
spectrographs with resolution $R= \lambda/\Delta\lambda$ between 2000 and 5500,
depending on wavelength. The DESI instrument will be used to conduct a
five-year survey designed to cover 14,000 deg$^2$. This powerful instrument
will be installed at prime focus on the 4-m Mayall telescope in Kitt Peak,
Arizona, along with a new optical corrector, which will provide a three-degree
diameter field of view. The DESI collaboration will also deliver a
spectroscopic pipeline and data management system to reduce and archive all
data for eventual public use.


