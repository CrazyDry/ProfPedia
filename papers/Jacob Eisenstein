Shallow Discourse Parsing Using Distributed Argument Representations and  Bayesian Optimization

  This paper describes the Georgia Tech team's approach to the CoNLL-2016supplementary evaluation on discourse relation sense classification. We uselong short-term memories (LSTM) to induce distributed representations of eachargument, and then combine these representations with surface features in aneural network. The architecture of the neural network is determined byBayesian hyperparameter search.

Better Document-level Sentiment Analysis from RST Discourse Parsing

  Discourse structure is the hidden link between surface features anddocument-level properties, such as sentiment polarity. We show that thediscourse analyses produced by Rhetorical Structure Theory (RST) parsers canimprove document-level sentiment analysis, via composition of local informationup the discourse tree. First, we show that reweighting discourse unitsaccording to their position in a dependency representation of the rhetoricalstructure can yield substantial improvements on lexicon-based sentimentanalysis. Next, we present a recursive neural network over the RST structure,which offers significant improvements over classification-based methods.

TopicViz: Semantic Navigation of Document Collections

  When people explore and manage information, they think in terms of topics andthemes. However, the software that supports information exploration sees textat only the surface level. In this paper we show how topic modeling -- atechnique for identifying latent themes across large collections of documents-- can support semantic exploration. We present TopicViz, an interactiveenvironment for information exploration. TopicViz combines traditional searchand citation-graph functionality with a range of novel interactivevisualizations, centered around a force-directed layout that links documents tothe latent themes discovered by the topic model. We describe several usescenarios in which TopicViz supports rapid sensemaking on large documentcollections.

"You're Mr. Lebowski, I'm the Dude": Inducing Address Term Formality in  Signed Social Networks

  We present an unsupervised model for inducing signed social networks from thecontent exchanged across network edges. Inference in this model solves threeproblems simultaneously: (1) identifying the sign of each edge; (2)characterizing the distribution over content for each edge type; (3) estimatingweights for triadic features that map to theoretical models such as structuralbalance. We apply this model to the problem of inducing the social function ofaddress terms, such as 'Madame', 'comrade', and 'dude'. On a dataset of moviescripts, our system obtains a coherent clustering of address terms, while atthe same time making intuitively plausible judgments of the formality of socialrelations in each film. As an additional contribution, we provide abootstrapping technique for identifying and tagging address terms in dialogue.

Confounds and Consequences in Geotagged Twitter Data

  Twitter is often used in quantitative studies that identifygeographically-preferred topics, writing styles, and entities. These studiesrely on either GPS coordinates attached to individual messages, or on theuser-supplied location field in each profile. In this paper, we compare thesedata acquisition techniques and quantify the biases that they introduce; wealso measure their effects on linguistic analysis and text-based geolocation.GPS-tagging and self-reported locations yield measurably different corpora, andthese linguistic differences are partially attributable to differences indataset composition by age and gender. Using a latent variable model to induceage and gender, we show how these demographic variables interact with geographyto affect language use. We also show that the accuracy of text-basedgeolocation varies with population demographics, giving the best results formen above the age of 40.

Nonparametric Bayesian Storyline Detection from Microtexts

  News events and social media are composed of evolving storylines, whichcapture public attention for a limited period of time. Identifying storylinesrequires integrating temporal and linguistic information, and prior work takesa largely heuristic approach. We present a novel online non-parametric Bayesianframework for storyline detection, using the distance-dependent ChineseRestaurant Process (dd-CRP). To ensure efficient linear-time inference, weemploy a fixed-lag Gibbs sampling procedure, which is novel for the dd-CRP. Weevaluate on the TREC Twitter Timeline Generation (TTG), obtaining encouragingresults: despite using a weak baseline retrieval model, the dd-CRP storyclustering method is competitive with the best entries in the 2014 TTG task.

Morphological Priors for Probabilistic Neural Word Embeddings

  Word embeddings allow natural language processing systems to sharestatistical information across related words. These embeddings are typicallybased on distributional statistics, making it difficult for them to generalizeto rare or unseen words. We propose to improve word embeddings by incorporatingmorphological information, capturing shared sub-word features. Unlike previouswork that constructs word embeddings directly from morphemes, we combinemorphological and distributional information in a unified probabilisticframework, in which the word embedding is a latent variable. The morphologicalinformation provides a prior distribution on the latent word embeddings, whichin turn condition a likelihood function over an observed corpus. This approachyields improvements on intrinsic word similarity evaluations, and also in thedownstream task of part-of-speech tagging.

Unsupervised Learning for Lexicon-Based Classification

  In lexicon-based classification, documents are assigned labels by comparingthe number of words that appear from two opposed lexicons, such as positive andnegative sentiment. Creating such words lists is often easier than labelinginstances, and they can be debugged by non-experts if classificationperformance is unsatisfactory. However, there is little analysis orjustification of this classification heuristic. This paper describes a set ofassumptions that can be used to derive a probabilistic justification forlexicon-based classification, as well as an analysis of its expected accuracy.One key assumption behind lexicon-based classification is that all words ineach lexicon are equally predictive. This is rarely true in practice, which iswhy lexicon-based approaches are usually outperformed by supervised classifiersthat learn distinct weights on each word from labeled instances. This papershows that it is possible to learn such weights without labeled data, byleveraging co-occurrence statistics across the lexicons. This offers the bestof both worlds: light supervision in the form of lexicons, and data-drivenclassification with higher accuracy than traditional word-counting heuristics.

Mind Your POV: Convergence of Articles and Editors Towards Wikipedia's  Neutrality Norm

  Wikipedia has a strong norm of writing in a 'neutral point of view' (NPOV).Articles that violate this norm are tagged, and editors are encouraged to makecorrections. But the impact of this tagging system has not been quantitativelymeasured. Does NPOV tagging help articles to converge to the desired style? DoNPOV corrections encourage editors to adopt this style? We study thesequestions using a corpus of NPOV-tagged articles and a set of lexiconsassociated with biased language. An interrupted time series analysis shows thatafter an article is tagged for NPOV, there is a significant decrease in biasedlanguage in the article, as measured by several lexicons. However, forindividual editors, NPOV corrections and talk page discussions yield nosignificant change in the usage of words in most of these lexicons, includingWikipedia's own list of 'words to watch.' This suggests that NPOV tagging anddiscussion does improve content, but has less success enculturating editors tothe site's linguistic norms.

The Referential Reader: A Recurrent Entity Network for Anaphora  Resolution

  We present a new architecture for storing and accessing entity mentionsduring online text processing. While reading the text, entity references areidentified, and may be stored by either updating or overwriting a cell in afixed-length memory. The update operation implies coreference with the othermentions that are stored in the same cell; the overwrite operations causesthese mentions to be forgotten. By encoding the memory operations asdifferentiable gates, it is possible to train the model end-to-end, using botha supervised anaphora resolution objective as well as a supplementary languagemodeling objective. Evaluation on a dataset of pronoun-name anaphorademonstrates that the model achieves state-of-the-art performance with purelyleft-to-right processing of the text.

Unsupervised Domain Adaptation of Contextualized Embeddings: A Case  Study in Early Modern English

  Contextualized word embeddings such as ELMo and BERT provide a foundation forstrong performance across a range of natural language processing tasks, in partby pretraining on a large and topically-diverse corpus. However, theapplicability of this approach is unknown when the target domain variessubstantially from the text used during pretraining. Specifically, we areinterested the scenario in which labeled data is available in only a canonicalsource domain such as newstext, and the target domain is distinct from both thelabeled corpus and the pretraining data. To address this scenario, we proposedomain-adaptive fine-tuning, in which the contextualized embeddings are adaptedby masked language modeling on the target domain. We test this approach on thechallenging domain of Early Modern English, which differs substantially fromexisting pretraining corpora. Domain-adaptive fine-tuning yields an improvementof 4\% in part-of-speech tagging accuracy over a BERT baseline, substantiallyimproving on prior work on this task.

Unsupervised Domain Adaptation with Feature Embeddings

  Representation learning is the dominant technique for unsupervised domainadaptation, but existing approaches often require the specification of "pivotfeatures" that generalize across domains, which are selected by task-specificheuristics. We show that a novel but simple feature embedding approach providesbetter performance, by exploiting the feature template structure common in NLPproblems.

Sí o no, què penses? Catalonian Independence and Linguistic Identity  on Social Media

  Political identity is often manifested in language variation, but therelationship between the two is still relatively unexplored from a quantitativeperspective. This study examines the use of Catalan, a language local to thesemi-autonomous region of Catalonia in Spain, on Twitter in discourse relatedto the 2017 independence referendum. We corroborate prior findings thatpro-independence tweets are more likely to include the local language thananti-independence tweets. We also find that Catalan is used more often inreferendum-related discourse than in other contexts, contrary to prior findingson language variation. This suggests a strong role for the Catalan language inthe expression of Catalonian political identity.

Gender identity and lexical variation in social media

  We present a study of the relationship between gender, linguistic style, andsocial networks, using a novel corpus of 14,000 Twitter users. Priorquantitative work on gender often treats this social variable as a female/malebinary; we argue for a more nuanced approach. By clustering Twitter users, wefind a natural decomposition of the dataset into various styles and topicalinterests. Many clusters have strong gender orientations, but their use oflinguistic resources sometimes directly conflicts with the population-levellanguage statistics. We view these clusters as a more accurate reflection ofthe multifaceted nature of gendered language styles. Previous corpus-based workhas also had little to say about individuals whose linguistic styles defypopulation-level gender patterns. To identify such individuals, we train astatistical classifier, and measure the classifier confidence for eachindividual in the dataset. Examining individuals whose language does not matchthe classifier's model for their gender, we find that they have social networksthat include significantly fewer same-gender social connections and that, ingeneral, social network homophily is correlated with the use of same-genderlanguage markers. Pairing computational methods and social theory thus offers anew perspective on how gender emerges as individuals position themselvesrelative to audiences, topics, and mainstream gender norms.

Diffusion of Lexical Change in Social Media

  Computer-mediated communication is driving fundamental changes in the natureof written language. We investigate these changes by statistical analysis of adataset comprising 107 million Twitter messages (authored by 2.7 million uniqueuser accounts). Using a latent vector autoregressive model to aggregate acrossthousands of words, we identify high-level patterns in diffusion of linguisticchange over the United States. Our model is robust to unpredictable changes inTwitter's sampling rate, and provides a probabilistic characterization of therelationship of macro-scale linguistic influence to a set of demographic andgeographic predictors. The results of this analysis offer support for priorarguments that focus on geographical proximity and population size. However,demographic similarity -- especially with regard to race -- plays an even morecentral role, as cities with similar racial demographics are far more likely toshare linguistic influence. Rather than moving towards a single unified"netspeak" dialect, language evolution in computer-mediated communicationreproduces existing fault lines in spoken American English.

The Social Dynamics of Language Change in Online Networks

  Language change is a complex social phenomenon, revealing pathways ofcommunication and sociocultural influence. But, while language change has longbeen a topic of study in sociolinguistics, traditional linguistic researchmethods rely on circumstantial evidence, estimating the direction of changefrom differences between older and younger speakers. In this paper, we use adata set of several million Twitter users to track language changes inprogress. First, we show that language change can be viewed as a form of socialinfluence: we observe complex contagion for phonetic spellings and "netspeak"abbreviations (e.g., lol), but not for older dialect markers from spokenlanguage. Next, we test whether specific types of social network connectionsare more influential than others, using a parametric Hawkes process model. Wefind that tie strength plays an important role: densely embedded social tiesare significantly better conduits of linguistic influence. Geographic localityappears to play a more limited role: we find relatively little evidence tosupport the hypothesis that individuals are more influenced by geographicallylocal social ties, even in their usage of geographical dialect markers.

Multilingual Part-of-Speech Tagging: Two Unsupervised Approaches

  We demonstrate the effectiveness of multilingual learning for unsupervisedpart-of-speech tagging. The central assumption of our work is that by combiningcues from multiple languages, the structure of each becomes more apparent. Weconsider two ways of applying this intuition to the problem of unsupervisedpart-of-speech tagging: a model that directly merges tag structures for a pairof languages into a single sequence and a second model which insteadincorporates multilingual context using latent variables. Both approaches areformulated as hierarchical Bayesian models, using Markov Chain Monte Carlosampling techniques for inference. Our results demonstrate that byincorporating multilingual evidence we can achieve impressive performance gainsacross a range of scenarios. We also found that performance improves steadilyas the number of available languages increases.

One Vector is Not Enough: Entity-Augmented Distributional Semantics for  Discourse Relations

  Discourse relations bind smaller linguistic units into coherent texts.However, automatically identifying discourse relations is difficult, because itrequires understanding the semantics of the linked arguments. A more subtlechallenge is that it is not enough to represent the meaning of each argument ofa discourse relation, because the relation may depend on links betweenlower-level components, such as entity mentions. Our solution computesdistributional meaning representations by composition up the syntactic parsetree. A key difference from previous work on compositional distributionalsemantics is that we also compute representations for entity mentions, using anovel downward compositional pass. Discourse relations are predicted from thedistributional representations of the arguments, and also of their coreferententity mentions. The resulting system obtains substantial improvements over theprevious state-of-the-art in predicting implicit discourse relations in thePenn Discourse Treebank.

Entity-Augmented Distributional Semantics for Discourse Relations

  Discourse relations bind smaller linguistic elements into coherent texts.However, automatically identifying discourse relations is difficult, because itrequires understanding the semantics of the linked sentences. A more subtlechallenge is that it is not enough to represent the meaning of each sentence ofa discourse relation, because the relation may depend on links betweenlower-level elements, such as entity mentions. Our solution computesdistributional meaning representations by composition up the syntactic parsetree. A key difference from previous work on compositional distributionalsemantics is that we also compute representations for entity mentions, using anovel downward compositional pass. Discourse relations are predicted not onlyfrom the distributional representations of the sentences, but also of theircoreferent entity mentions. The resulting system obtains substantialimprovements over the previous state-of-the-art in predicting implicitdiscourse relations in the Penn Discourse Treebank.

Emoticons vs. Emojis on Twitter: A Causal Inference Approach

  Online writing lacks the non-verbal cues present in face-to-facecommunication, which provide additional contextual information about theutterance, such as the speaker's intention or affective state. To fill thisvoid, a number of orthographic features, such as emoticons, expressivelengthening, and non-standard punctuation, have become popular in social mediaservices including Twitter and Instagram. Recently, emojis have been introducedto social media, and are increasingly popular. This raises the question ofwhether these predefined pictographic characters will come to replace earlierorthographic methods of paralinguistic communication. In this abstract, weattempt to shed light on this question, using a matching approach from causalinference to test whether the adoption of emojis causes individual users toemploy fewer emoticons in their text on Twitter.

Document Context Language Models

  Text documents are structured on multiple levels of detail: individual wordsare related by syntax, but larger units of text are related by discoursestructure. Existing language models generally fail to account for discoursestructure, but it is crucial if we are to have language models that rewardcoherence and generate coherent texts. We present and empirically evaluate aset of multi-level recurrent neural network language models, calledDocument-Context Language Models (DCLM), which incorporate contextualinformation both within and beyond the sentence. In comparison with word-levelrecurrent neural network language models, the DCLM models obtain slightlybetter predictive likelihoods, and considerably better assessments of documentcoherence.

Overcoming Language Variation in Sentiment Analysis with Social  Attention

  Variation in language is ubiquitous, particularly in newer forms of writingsuch as social media. Fortunately, variation is not random, it is often linkedto social properties of the author. In this paper, we show how to exploitsocial networks to make sentiment analysis more robust to social languagevariation. The key idea is linguistic homophily: the tendency of sociallylinked individuals to use language in similar ways. We formalize this idea in anovel attention-based neural network architecture, in which attention isdivided among several basis models, depending on the author's position in thesocial network. This has the effect of smoothing the classification functionacross the social network, and makes it possible to induce personalizedclassifiers even for authors for whom there is no labeled data or demographicmetadata. This model significantly improves the accuracies of sentimentanalysis on Twitter and on review data.

A Kernel Independence Test for Geographical Language Variation

  Quantifying the degree of spatial dependence for linguistic variables is akey task for analyzing dialectal variation. However, existing approaches haveimportant drawbacks. First, they are based on parametric models of dependence,which limits their power in cases where the underlying parametric assumptionsare violated. Second, they are not applicable to all types of linguistic data:some approaches apply only to frequencies, others to boolean indicators ofwhether a linguistic variable is present. We present a new method for measuringgeographical language variation, which solves both of these problems. Ourapproach builds on Reproducing Kernel Hilbert space (RKHS) representations fornonparametric statistics, and takes the form of a test statistic that iscomputed from pairs of individual geotagged observations without aggregationinto predefined geographical bins. We compare this test with prior work usingsynthetic data as well as a diverse set of real datasets: a corpus of Dutchtweets, a Dutch syntactic atlas, and a dataset of letters to the editor inNorth American newspapers. Our proposed test is shown to support robustinferences across a broad range of scenarios and types of data.

A Latent Variable Recurrent Neural Network for Discourse Relation  Language Models

  This paper presents a novel latent variable recurrent neural networkarchitecture for jointly modeling sequences of words and (possibly latent)discourse relations between adjacent sentences. A recurrent neural networkgenerates individual words, thus reaping the benefits ofdiscriminatively-trained vector representations. The discourse relations arerepresented with a latent variable, which can be predicted or marginalized,depending on the task. The resulting model can therefore employ a trainingobjective that includes not only discourse relation classification, but alsoword prediction. As a result, it outperforms state-of-the-art alternatives fortwo tasks: implicit discourse relation classification in the Penn DiscourseTreebank, and dialog act classification in the Switchboard corpus. Furthermore,by marginalizing over latent discourse relations at test time, we obtain adiscourse informed language model, which improves over a strong LSTM baseline.

Part-of-Speech Tagging for Historical English

  As more historical texts are digitized, there is interest in applying naturallanguage processing tools to these archives. However, the performance of thesetools is often unsatisfactory, due to language change and genre differences.Spelling normalization heuristics are the dominant solution for dealing withhistorical texts, but this approach fails to account for changes in usage andvocabulary. In this empirical paper, we assess the capability of domainadaptation techniques to cope with historical texts, focusing on the classicbenchmark task of part-of-speech tagging. We evaluate several domain adaptationmethods on the task of tagging Early Modern English and Modern British Englishtexts in the Penn Corpora of Historical English. We demonstrate that theFeature Embedding method for unsupervised domain adaptation outperforms wordembeddings and Brown clusters, showing the importance of embedding the entirefeature space, rather than just individual words. Feature Embeddings also givebetter performance than spelling normalization, but the combination of the twomethods is better still, yielding a 5% raw improvement in tagging accuracy onEarly Modern English texts.

Toward Socially-Infused Information Extraction: Embedding Authors,  Mentions, and Entities

  Entity linking is the task of identifying mentions of entities in text, andlinking them to entries in a knowledge base. This task is especially difficultin microblogs, as there is little additional text to provide disambiguatingcontext; rather, authors rely on an implicit common ground of shared knowledgewith their readers. In this paper, we attempt to capture some of this implicitcontext by exploiting the social network structure in microblogs. We build onthe theory of homophily, which implies that socially linked individuals shareinterests, and are therefore likely to mention the same sorts of entities. Weimplement this idea by encoding authors, mentions, and entities in a continuousvector space, which is constructed so that socially-connected authors havesimilar vector representations. These vectors are incorporated into a neuralstructured prediction model, which captures structural constraints that areinherent in the entity linking task. Together, these design decisions yield F1improvements of 1%-5% on benchmark datasets, as compared to the previousstate-of-the-art.

Mimicking Word Embeddings using Subword RNNs

  Word embeddings improve generalization over lexical features by placing eachword in a lower-dimensional space, using distributional information obtainedfrom unlabeled data. However, the effectiveness of word embeddings fordownstream NLP tasks is limited by out-of-vocabulary (OOV) words, for whichembeddings do not exist. In this paper, we present MIMICK, an approach togenerating OOV word embeddings compositionally, by learning a function fromspellings to distributional embeddings. Unlike prior work, MIMICK does notrequire re-training on the original word embedding corpus; instead, learning isperformed at the type level. Intrinsic and extrinsic evaluations demonstratethe power of this simple approach. On 23 languages, MIMICK improves performanceover a word-based baseline for tagging part-of-speech and morphosyntacticattributes. It is competitive with (and complementary to) a supervisedcharacter-based model in low-resource settings.

Making "fetch" happen: The influence of social and linguistic context on  nonstandard word growth and decline

  In an online community, new words come and go: today's "haha" may be replacedby tomorrow's "lol." Changes in online writing are usually studied as a socialprocess, with innovations diffusing through a network of individuals in aspeech community. But unlike other types of innovation, language change isshaped and constrained by the system in which it takes part. To investigate thelinks between social and structural factors in language change, we undertake alarge-scale analysis of nonstandard word growth in the online community Reddit.We find that dissemination across many linguistic contexts is a sign of growth:words that appear in more linguistic contexts grow faster and survive longer.We also find that social dissemination likely plays a less important role inexplaining word growth and decline than previously hypothesized.

Making "fetch" happen: The influence of social and linguistic context on  nonstandard word growth and decline

  In an online community, new words come and go: today's "haha" may be replacedby tomorrow's "lol." Changes in online writing are usually studied as a socialprocess, with innovations diffusing through a network of individuals in aspeech community. But unlike other types of innovation, language change isshaped and constrained by the system in which it takes part. To investigate thelinks between social and structural factors in language change, we undertake alarge-scale analysis of nonstandard word growth in the online community Reddit.We find that dissemination across many linguistic contexts is a sign of growth:words that appear in more linguistic contexts grow faster and survive longer.We also find that social dissemination likely plays a less important role inexplaining word growth and decline than previously hypothesized.

Explainable Prediction of Medical Codes from Clinical Text

  Clinical notes are text documents that are created by clinicians for eachpatient encounter. They are typically accompanied by medical codes, whichdescribe the diagnosis and treatment. Annotating these codes is labor intensiveand error prone; furthermore, the connection between the codes and the text isnot annotated, obscuring the reasons and details behind specific diagnoses andtreatments. We present an attentional convolutional network that predictsmedical codes from clinical text. Our method aggregates information across thedocument using a convolutional neural network, and uses an attention mechanismto select the most relevant segments for each of the thousands of possiblecodes. The method is accurate, achieving precision@8 of 0.71 and a Micro-F1 of0.54, which are both better than the prior state of the art. Furthermore,through an interpretability evaluation by a physician, we show that theattention mechanism identifies meaningful explanations for each code assignment

Discriminative Modeling of Social Influence for Prediction and  Explanation in Event Cascades

  The global dynamics of event cascades are often governed by the localdynamics of peer influence. However, detecting social influence fromobservational data is challenging, due to confounds like homophily andpractical issues like missing data. In this work, we propose a noveldiscriminative method to detect influence from observational data. The core ofthe approach is to train a ranking algorithm to predict the source of the nextevent in a cascade, and compare its out-of-sample accuracy against acompetitive baseline which lacks access to features corresponding to socialinfluence. Using synthetically generated data, we provide empirical evidencethat this method correctly identifies influence in the presence of confounds,and is robust to both missing data and misspecification --- unlike popularalternatives. We also apply the method to two real-world datasets: (1) cascadesof co-sponsorship of legislation in the U.S. House of Representatives, on asocial network of shared campaign donors; (2) rumors about the Higgs bosondiscovery, on a follower network of $10^5$ Twitter accounts. Our modelidentifies the role of peer influence in these scenarios, and uses it to makemore accurate predictions about the future trajectory of cascades.

Stylistic Variation in Social Media Part-of-Speech Tagging

  Social media features substantial stylistic variation, raising new challengesfor syntactic analysis of online writing. However, this variation is oftenaligned with author attributes such as age, gender, and geography, as well asmore readily-available social network metadata. In this paper, we report newevidence on the link between language and social networks in the task ofpart-of-speech tagging. We find that tagger error rates are correlated withnetwork structure, with high accuracy in some parts of the network, and loweraccuracy elsewhere. As a result, tagger accuracy depends on training from abalanced sample of the network, rather than training on texts from a narrowsubcommunity. We also describe our attempts to add robustness to stylisticvariation, by building a mixture-of-experts model in which each expert isassociated with a region of the social network. While prior work found thatsimilar approaches yield performance improvements in sentiment analysis andentity linking, we were unable to obtain performance improvements inpart-of-speech tagging, despite strong evidence for the link betweenpart-of-speech error rates and social network structure.

Predicting Semantic Relations using Global Graph Properties

  Semantic graphs, such as WordNet, are resources which curate natural languageon two distinguishable layers. On the local level, individual relations betweensynsets (semantic building blocks) such as hypernymy and meronymy enhance ourunderstanding of the words used to express their meanings. Globally, analysisof graph-theoretic properties of the entire net sheds light on the structure ofhuman language as a whole. In this paper, we combine global and localproperties of semantic graphs through the framework of Max-Margin Markov GraphModels (M3GM), a novel extension of Exponential Random Graph Model (ERGM) thatscales to large multi-relational graphs. We demonstrate how such globalmodeling improves performance on the local task of predicting semanticrelations between synsets, yielding new state-of-the-art results on the WN18RRdataset, a challenging version of WordNet link prediction in which "easy"reciprocal cases are removed. In addition, the M3GM model identifiesmultirelational motifs that are characteristic of well-formed lexical semanticontologies.

Training on Synthetic Noise Improves Robustness to Natural Noise in  Machine Translation

  We consider the problem of making machine translation more robust tocharacter-level variation at the source side, such as typos. Existing methodsachieve greater coverage by applying subword models such as byte-pair encoding(BPE) and character-level encoders, but these methods are highly sensitive tospelling mistakes. We show how training on a mild amount of random syntheticnoise can dramatically improve robustness to these variations, withoutdiminishing performance on clean text. We focus on translation performance onnatural noise, as captured by frequent corrections in Wikipedia edit logs, andshow that robustness to such noise can be achieved using a balanced diet ofsimple synthetic noises at training time, without access to the natural noisedata or distribution.

Character Eyes: Seeing Language through Character-Level Taggers

  Character-level models have been used extensively in recent years in NLPtasks as both supplements and replacements for closed-vocabulary token-levelword representations. In one popular architecture, character-level LSTMs areused to feed token representations into a sequence tagger predictingtoken-level annotations such as part-of-speech (POS) tags. In this work, weexamine the behavior of POS taggers across languages from the perspective ofindividual hidden units within the character LSTM. We aggregate the behavior ofthese units into language-level metrics which quantify the challenges thattaggers face on languages with different morphological properties, and identifylinks between synthesis and affixation preference and emergent behavior of thehidden tagger layer. In a comparative experiment, we show how modifying thebalance between forward and backward hidden units affects model arrangement andperformance in these types of languages.

Learning Document-Level Semantic Properties from Free-Text Annotations

  This paper presents a new method for inferring the semantic properties ofdocuments by leveraging free-text keyphrase annotations. Such annotations arebecoming increasingly abundant due to the recent dramatic growth insemi-structured, user-generated online content. One especially relevant domainis product reviews, which are often annotated by their authors with pros/conskeyphrases such as a real bargain or good value. These annotations arerepresentative of the underlying semantic properties; however, unlike expertannotations, they are noisy: lay authors may use different labels to denote thesame property, and some labels may be missing. To learn using such noisyannotations, we find a hidden paraphrase structure which clusters thekeyphrases. The paraphrase structure is linked with a latent topic model of thereview texts, enabling the system to predict the properties of unannotateddocuments and to effectively aggregate the semantic properties of multiplereviews. Our approach is implemented as a hierarchical Bayesian model withjoint inference. We find that joint inference increases the robustness of thekeyphrase clustering and encourages the latent topics to correlate withsemantically meaningful properties. Multiple evaluations demonstrate that ourmodel substantially outperforms alternative approaches for summarizing singleand multiple documents into a set of semantically salient keyphrases.

#anorexia, #anarexia, #anarexyia: Characterizing Online Community  Practices with Orthographic Variation

  Distinctive linguistic practices help communities build solidarity anddifferentiate themselves from outsiders. In an online community, one suchpractice is variation in orthography, which includes spelling, punctuation, andcapitalization. Using a dataset of over two million Instagram posts, weinvestigate orthographic variation in a community that shares pro-eatingdisorder (pro-ED) content. We find that not only does orthographic variationgrow more frequent over time, it also becomes more profound or deep, withvariants becoming increasingly distant from the original: as, for example,#anarexyia is more distant than #anarexia from the original spelling #anorexia.These changes are driven by newcomers, who adopt the most extreme linguisticpractices as they enter the community. Moreover, this behavior correlates withengagement: the newcomers who adopt deeper orthographic variants tend to remainactive for longer in the community, and the posts that contain deeper variationreceive more positive feedback in the form of "likes." Previous work has linkedcommunity membership change with language change, and our work casts thisconnection in a new light, with newcomers driving an evolving practice, ratherthan adapting to it. We also demonstrate the utility of orthographic variationas a new lens to study sociolinguistic change in online communities,particularly when the change results from an exogenous force such as a contentban.

The Spitzer Deep, Wide-Field Survey

  The Spitzer Deep, Wide-Field Survey (SDWFS) is a four-epoch infrared surveyof ten square degrees in the Bootes field of the NOAO Deep Wide-Field Surveyusing the IRAC instrument on the Spitzer Space Telescope. SDWFS, a Cycle fourSpitzer Legacy project, occupies a unique position in the area-depth surveyspace defined by other Spitzer surveys. The four epochs that make up SDWFSpermit -- for the first time -- the selection of infrared-variable and highproper motion objects over a wide field on timescales of years. Because of itslarge survey volume, SDWFS is sensitive to galaxies out to z~3 with relativelylittle impact from cosmic variance for all but the richest systems. The SDWFSdatasets will thus be especially useful for characterizing galaxy evolutionbeyond z~1.5. This paper explains the SDWFS observing strategy and dataprocessing, presents the SDWFS mosaics and source catalogs, and discusses someearly scientific findings. The publicly-released, full-depth catalogs contain6.78, 5.23, 1.20, and 0.96 x 10e5 distinct sources detected to the average5-sigma, 4" diameter, aperture-corrected limits of 19.77, 18.83, 16.50, and15.82 Vega mag at 3.6, 4.5, 5.8, and 8.0 micron, respectively. The SDWFS numbercounts and color-color distribution are consistent with other, earlier Spitzersurveys. At the 6 min integration time of the SDWFS IRAC imaging, more than 50%of isolated FIRST radio sources and more than 80% of on-axis XBootes sourcesare detected out to 8.0 micron. Finally, we present the four highest propermotion IRAC-selected sources identified from the multi-epoch imaging, two ofwhich are likely field brown dwarfs of mid-T spectral class.

SDSS-III: Massive Spectroscopic Surveys of the Distant Universe, the  Milky Way Galaxy, and Extra-Solar Planetary Systems

  Building on the legacy of the Sloan Digital Sky Survey (SDSS-I and II),SDSS-III is a program of four spectroscopic surveys on three scientific themes:dark energy and cosmological parameters, the history and structure of the MilkyWay, and the population of giant planets around other stars. In keeping withSDSS tradition, SDSS-III will provide regular public releases of all its data,beginning with SDSS DR8 (which occurred in Jan 2011). This paper presents anoverview of the four SDSS-III surveys. BOSS will measure redshifts of 1.5million massive galaxies and Lya forest spectra of 150,000 quasars, using theBAO feature of large scale structure to obtain percent-level determinations ofthe distance scale and Hubble expansion rate at z<0.7 and at z~2.5. SEGUE-2,which is now completed, measured medium-resolution (R=1800) optical spectra of118,000 stars in a variety of target categories, probing chemical evolution,stellar kinematics and substructure, and the mass profile of the dark matterhalo from the solar neighborhood to distances of 100 kpc. APOGEE will obtainhigh-resolution (R~30,000), high signal-to-noise (S/N>100 per resolutionelement), H-band (1.51-1.70 micron) spectra of 10^5 evolved, late-type stars,measuring separate abundances for ~15 elements per star and creating the firsthigh-precision spectroscopic survey of all Galactic stellar populations (bulge,bar, disks, halo) with a uniform set of stellar tracers and spectraldiagnostics. MARVELS will monitor radial velocities of more than 8000 FGK starswith the sensitivity and cadence (10-40 m/s, ~24 visits per star) needed todetect giant planets with periods up to two years, providing an unprecedenteddata set for understanding the formation and dynamical evolution of giantplanet systems. (Abridged)

The DESI Experiment Part I: Science,Targeting, and Survey Design

  DESI (Dark Energy Spectroscopic Instrument) is a Stage IV ground-based darkenergy experiment that will study baryon acoustic oscillations (BAO) and thegrowth of structure through redshift-space distortions with a wide-area galaxyand quasar redshift survey. To trace the underlying dark matter distribution,spectroscopic targets will be selected in four classes from imaging data. Wewill measure luminous red galaxies up to $z=1.0$. To probe the Universe out toeven higher redshift, DESI will target bright [O II] emission line galaxies upto $z=1.7$. Quasars will be targeted both as direct tracers of the underlyingdark matter distribution and, at higher redshifts ($ 2.1 < z < 3.5$), for theLy-$\alpha$ forest absorption features in their spectra, which will be used totrace the distribution of neutral hydrogen. When moonlight prevents efficientobservations of the faint targets of the baseline survey, DESI will conduct amagnitude-limited Bright Galaxy Survey comprising approximately 10 milliongalaxies with a median $z\approx 0.2$. In total, more than 30 million galaxyand quasar redshifts will be obtained to measure the BAO feature and determinethe matter power spectrum, including redshift space distortions.

The DESI Experiment Part II: Instrument Design

  DESI (Dark Energy Spectropic Instrument) is a Stage IV ground-based darkenergy experiment that will study baryon acoustic oscillations and the growthof structure through redshift-space distortions with a wide-area galaxy andquasar redshift survey. The DESI instrument is a robotically-actuated,fiber-fed spectrograph capable of taking up to 5,000 simultaneous spectra overa wavelength range from 360 nm to 980 nm. The fibers feed ten three-armspectrographs with resolution $R= \lambda/\Delta\lambda$ between 2000 and 5500,depending on wavelength. The DESI instrument will be used to conduct afive-year survey designed to cover 14,000 deg$^2$. This powerful instrumentwill be installed at prime focus on the 4-m Mayall telescope in Kitt Peak,Arizona, along with a new optical corrector, which will provide a three-degreediameter field of view. The DESI collaboration will also deliver aspectroscopic pipeline and data management system to reduce and archive alldata for eventual public use.

