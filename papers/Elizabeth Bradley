Nonlinear time-series analysis revisited

  In 1980 and 1981, two pioneering papers laid the foundation for what became
known as nonlinear time-series analysis: the analysis of observed
data---typically univariate---via dynamical systems theory. Based on the
concept of state-space reconstruction, this set of methods allows us to compute
characteristic quantities such as Lyapunov exponents and fractal dimensions, to
predict the future course of the time series, and even to reconstruct the
equations of motion in some cases. In practice, however, there are a number of
issues that restrict the power of this approach: whether the signal accurately
and thoroughly samples the dynamics, for instance, and whether it contains
noise. Moreover, the numerical algorithms that we use to instantiate these
ideas are not perfect; they involve approximations, scale parameters, and
finite-precision arithmetic, among other things. Even so, nonlinear time-series
analysis has been used to great advantage on thousands of real and synthetic
data sets from a wide variety of systems ranging from roulette wheels to lasers
to the human heart. Even in cases where the data do not meet the mathematical
or algorithmic requirements to assure full topological conjugacy, the results
of nonlinear time-series analysis can be helpful in understanding,
characterizing, and predicting dynamical systems.


Determinism, Complexity, and Predictability in Computer Performance

  Computers are deterministic dynamical systems (CHAOS 19:033124, 2009). Among
other things, that implies that one should be able to use deterministic
forecast rules to predict their behavior. That statement is sometimes-but not
always-true. The memory and processor loads of some simple programs are easy to
predict, for example, but those of more-complex programs like compilers are
not. The goal of this paper is to determine why that is the case. We conjecture
that, in practice, complexity can effectively overwhelm the predictive power of
deterministic forecast models. To explore that, we build models of a number of
performance traces from different programs running on different Intel-based
computers. We then calculate the permutation entropy-a temporal entropy metric
that uses ordinal analysis-of those traces and correlate those values against
the prediction success


Nonlinear dynamics of running: Speed, stability, symmetry and the
  effects of leg amputations

  In this paper, we study dynamic stability during running, focusing on the
effects of speed and the use of a leg prosthesis. We compute and compare the
maximal Lyapunov exponents of kinematic time-series data from subjects with and
without unilateral transtibial amputations running at a wide range of speeds.
We find that the dynamics of the affected leg with the running-specific
prosthesis are less stable than the dynamics of the unaffected leg, and also
less stable than the biological legs of the non-amputee runners. Surprisingly,
we find that the center-of-mass dynamics of runners with two intact biological
legs are slightly less stable than those of runners with amputations. Our
results suggest that while leg asymmetries may be associated with instability,
runners may compensate for this effect by increased control of their
center-of-mass dynamics.


Unix Memory Allocations are Not Poisson

  In multitasking operating systems, requests for free memory are traditionally
modeled as a stochastic counting process with independent,
exponentially-distributed interarrival times because of the analytic simplicity
such Poisson models afford. We analyze the distribution of several million unix
page commits to show that although this approach could be valid over relatively
long timespans, the behavior of the arrival process over shorter periods is
decidedly not Poisson. We find that this result holds regardless of the
originator of the request: unlike network packets, there is little difference
between system- and user-level page-request distributions. We believe this to
be due to the bursty nature of page allocations, which tend to occur in either
small or extremely large increments. Burstiness and persistent variance have
recently been found in self-similar processes in computer networks, but we show
that although page commits are both bursty and possess high variance over long
timescales, they are probably not self-similar. These results suggest that
altogether different models are needed for fine-grained analysis of memory
systems, an important consideration not only for understanding behavior but
also for the design of online control systems.


Advanced Cyberinfrastructure for Science, Engineering, and Public Policy

  Progress in many domains increasingly benefits from our ability to view the
systems through a computational lens, i.e., using computational abstractions of
the domains; and our ability to acquire, share, integrate, and analyze
disparate types of data. These advances would not be possible without the
advanced data and computational cyberinfrastructure and tools for data capture,
integration, analysis, modeling, and simulation. However, despite, and perhaps
because of, advances in "big data" technologies for data acquisition,
management and analytics, the other largely manual, and labor-intensive aspects
of the decision making process, e.g., formulating questions, designing studies,
organizing, curating, connecting, correlating and integrating crossdomain data,
drawing inferences and interpreting results, have become the rate-limiting
steps to progress. Advancing the capability and capacity for evidence-based
improvements in science, engineering, and public policy requires support for
(1) computational abstractions of the relevant domains coupled with
computational methods and tools for their analysis, synthesis, simulation,
visualization, sharing, and integration; (2) cognitive tools that leverage and
extend the reach of human intellect, and partner with humans on all aspects of
the activity; (3) nimble and trustworthy data cyber-infrastructures that
connect, manage a variety of instruments, multiple interrelated data types and
associated metadata, data representations, processes, protocols and workflows;
and enforce applicable security and data access and use policies; and (4)
organizational and social structures and processes for collaborative and
coordinated activity across disciplinary and institutional boundaries.


Iterated Function System Models in Data Analysis: Detection and
  Separation

  We investigate the use of iterated function system (IFS) models for data
analysis. An IFS is a discrete dynamical system in which each time step
corresponds to the application of one of a finite collection of maps. The maps,
which represent distinct dynamical regimes, may act in some pre-determined
sequence or may be applied in random order. An algorithm is developed to detect
the sequence of regime switches under the assumption of continuity. This method
is tested on a simple IFS and applied to an experimental computer performance
data set. This methodology has a wide range of potential uses: from
change-point detection in time-series data to the field of digital
communications.


Prediction in Projection

  Prediction models that capture and use the structure of state-space dynamics
can be very effective. In practice, however, one rarely has access to full
information about that structure, and accurate reconstruction of the dynamics
from scalar time-series data---e.g., via delay-coordinate embedding---can be a
real challenge. In this paper, we show that forecast models that employ
incomplete embeddings of the dynamics can produce surprisingly accurate
predictions of the state of a dynamical system. In particular, we demonstrate
the effectiveness of a simple near-neighbor forecast technique that works with
a two-dimensional embedding. Even though correctness of the topology is not
guaranteed for incomplete reconstructions like this, the dynamical structure
that they capture allows for accurate predictions---in many cases, even more
accurate than predictions generated using a full embedding. This could be very
useful in the context of real-time forecasting, where the human effort required
to produce a correct delay-coordinate embedding is prohibitive.


On the importance of nonlinear modeling in computer performance
  prediction

  Computers are nonlinear dynamical systems that exhibit complex and sometimes
even chaotic behavior. The models used in the computer systems community,
however, are linear. This paper is an exploration of that disconnect: when
linear models are adequate for predicting computer performance and when they
are not. Specifically, we build linear and nonlinear models of the processor
load of an Intel i7-based computer as it executes a range of different
programs. We then use those models to predict the processor loads forward in
time and compare those forecasts to the true continuations of the time series


Big Data, Data Science, and Civil Rights

  Advances in data analytics bring with them civil rights implications.
Data-driven and algorithmic decision making increasingly determine how
businesses target advertisements to consumers, how police departments monitor
individuals or groups, how banks decide who gets a loan and who does not, how
employers hire, how colleges and universities make admissions and financial aid
decisions, and much more. As data-driven decisions increasingly affect every
corner of our lives, there is an urgent need to ensure they do not become
instruments of discrimination, barriers to equality, threats to social justice,
and sources of unfairness. In this paper, we argue for a concrete research
agenda aimed at addressing these concerns, comprising five areas of emphasis:
(i) Determining if models and modeling procedures exhibit objectionable bias;
(ii) Building awareness of fairness into machine learning methods; (iii)
Improving the transparency and control of data- and model-driven decision
making; (iv) Looking beyond the algorithm(s) for sources of bias and
unfairness-in the myriad human decisions made during the problem formulation
and modeling process; and (v) Supporting the cross-disciplinary scholarship
necessary to do all of that well.


Computational Topology Techniques for Characterizing Time-Series Data

  Topological data analysis (TDA), while abstract, allows a characterization of
time-series data obtained from nonlinear and complex dynamical systems. Though
it is surprising that such an abstract measure of structure - counting pieces
and holes - could be useful for real-world data, TDA lets us compare different
systems, and even do membership testing or change-point detection. However, TDA
is computationally expensive and involves a number of free parameters. This
complexity can be obviated by coarse-graining, using a construct called the
witness complex. The parametric dependence gives rise to the concept of
persistent homology: how shape changes with scale. Its results allow us to
distinguish time-series data from different systems - e.g., the same note
played on different musical instruments.


The Resolved Narrow Line Region in NGC4151

  We present slitless spectra of the Narrow Line Region (NLR) in NGC4151 from
the Space Telescope Imaging Spectrograph (STIS) on HST, and investigate the
kinematics and physical conditions of the emission line clouds in this region.
Using medium resolution (~0.5 Angstrom) slitless spectra at two roll angles and
narrow band undispersed images, we have mapped the NLR velocity field from 1.2
kpc to within 13 pc (H_o=75 km/s/Mpc) of the nucleus. The inner biconical cloud
distribution exhibits recessional velocities relative to the nucleus to the NE
and approaching velocities to the SW of the nucleus. We find evidence for at
least two kinematic components in the NLR. One kinematic component is
characterized by Low Velocities and Low Velocity Dispersions (LVLVD clouds: |v|
< 400 km/s, and Delta_v < 130 km/s). This population extends through the NLR
and their observed kinematics may be gravitationally associated with the host
galaxy. Another component is characterized by High Velocities and High Velocity
Dispersions (HVHVD clouds: 400 < |v| < ~1700 km/s, Delta_v > 130 km/s). This
set of clouds is located within 1.1 arcsec (~70pc) of the nucleus and has
radial velocities which are too high to be gravitational in origin, but show no
strong correlation between velocity or velocity dispersion and the position of
the radio knots. Outflow scenarios will be discussed as the driving mechanism
for these HVHVD clouds.


Optical Variability of the Three Brightest Nearby Quasars

  We report on the relative optical variability of the three brightest nearby
quasars, 3C 273, PDS 456, and PHL 1811. All three have comparable absolute
magnitudes, but PDS 456 and PHL 1811 are radio quiet. PDS 456 is a broad-line
object, but PHL 1811 could be classified as a high-luminosity Narrow-Line
Seyfert 1 (NLS1). Both of the radio-quiet quasars show significant variability
on a timescale of a few days. The seasonal rms V-band variability amplitudes of
3C 273 and PDS 456 are indistinguishable, and the seasonal rms variability
amplitude of PHL 1811 was only exceeded by 3C 273 once in 30 years of
monitoring. We find no evidence that the optical variability of 3C 273 is
greater than or more rapid than the variability of the comparably-bright,
radio-quiet quasars. This suggests that not only do radio-loud and radio-quiet
AGNs have similar spectral energy distributions, but that the variability
mechanisms are also similar. The optical variability of 3C 273 is not dominated
by a "blazer" component.


Strange Beta: An Assistance System for Indoor Rock Climbing Route
  Setting Using Chaotic Variations and Machine Learning

  This paper applies machine learning and the mathematics of chaos to the task
of designing indoor rock-climbing routes. Chaotic variation has been used to
great advantage on music and dance, but the challenges here are quite
different, beginning with the representation. We present a formalized system
for transcribing rock climbing problems, then describe a variation generator
that is designed to support human route-setters in designing new and
interesting climbing problems. This variation generator, termed Strange Beta,
combines chaos and machine learning, using the former to introduce novelty and
the latter to smooth transitions in a manner that is consistent with the style
of the climbs This entails parsing the domain-specific natural language that
rock climbers use to describe routes and movement and then learning the
patterns in the results. We validated this approach with a pilot study in a
small university rock climbing gym, followed by a large blinded study in a
commercial climbing gym, in cooperation with experienced climbers and expert
route setters. The results show that {\sc Strange Beta} can help a human setter
produce routes that are at least as good as, and in some cases better than,
those produced in the traditional manner.


Model-free quantification of time-series predictability

  This paper provides insight into when, why, and how forecast strategies fail
when they are applied to complicated time series. We conjecture that the
inherent complexity of real-world time-series data---which results from the
dimension, nonlinearity, and non-stationarity of the generating process, as
well as from measurement issues like noise, aggregation, and finite data
length---is both empirically quantifiable and directly correlated with
predictability. In particular, we argue that redundancy is an effective way to
measure complexity and predictive structure in an experimental time series and
that weighted permutation entropy is an effective way to estimate that
redundancy. To validate these conjectures, we study 120 different time-series
data sets. For each time series, we construct predictions using a wide variety
of forecast models, then compare the accuracy of the predictions with the
permutation entropy of that time series. We use the results to develop a
model-free heuristic that can help practitioners recognize when a particular
prediction method is not well matched to the task at hand: that is, when the
time series has more predictive structure than that method can capture and
exploit.


Simplicial Multivalued Maps and the Witness Complex for Dynamical
  Analysis of Time Series

  Topology based analysis of time-series data from dynamical systems is
powerful: it potentially allows for computer-based proofs of the existence of
various classes of regular and chaotic invariant sets for high-dimensional
dynamics. Standard methods are based on a cubical discretization of the
dynamics and use the time series to construct an outer approximation of the
underlying dynamical system. The resulting multivalued map can be used to
compute the Conley index of isolated invariant sets of cubes. In this paper we
introduce a discretization that uses instead a simplicial complex constructed
from a witness-landmark relationship. The goal is to obtain a natural
discretization that is more tightly connected with the invariant density of the
time series itself. The time-ordering of the data also directly leads to a map
on this simplicial complex that we call the witness map. We obtain conditions
under which this witness map gives an outer approximation of the dynamics, and
thus can be used to compute the Conley index of isolated invariant sets. The
method is illustrated by a simple example using data from the classical H\'enon
map.


Not In Our Backyard: Spectroscopic Support for the CLASH z=11 Candidate
  MACS0647-JD

  We report on our first set of spectroscopic Hubble Space Telescope
observations of the z~11 candidate galaxy strongly lensed by the
MACSJ0647.7+7015 galaxy cluster. The three lensed images are faint and we show
that these early slitless grism observations are of sufficient depth to
investigate whether this high-redshift candidate, identified by its strong
photometric break at ~1.5 micron, could possibly be an emission line galaxy at
a much lower redshift. While such an interloper would imply the existence of a
rather peculiar object, we show here that such strong emission lines would
clearly have been detected. Comparing realistic, two-dimensional simulations to
these new observations we would expect the necessary emission lines to be
detected at >5 sigma while we see no evidence for such lines in the dispersed
data of any of the three lensed images. We therefore exclude that this object
could be a low redshift emission line interloper, which significantly increases
the likelihood of this candidate being a bona fide z~11 galaxy.


Exploring the Topology of Dynamical Reconstructions

  Computing the state-space topology of a dynamical system from scalar data
requires accurate reconstruction of those dynamics and construction of an
appropriate simplicial complex from the results. The reconstruction process
involves a number of free parameters and the computation of homology for a
large number of simplices can be expensive. This paper is a study of how to
avoid a full (diffeomorphic) reconstruction and how to decrease the
computational burden. Using trajectories from the classic Lorenz system, we
reconstruct the dynamics using the method of delays, then build a parsimonious
simplicial complex---the "witness complex"---to compute its homology.
Surprisingly, we find that the witness complex correctly resolves the homology
of the underlying invariant set from noisy samples of that set even if the
reconstruction dimension is well below the thresholds specified in the
embedding theorems for assuring topological conjugacy between the true and
reconstructed dynamics. We conjecture that this is because the requirements for
reconstructing homology, are less stringent than those in the embedding
theorems. In particular, to faithfully reconstruct the homology, a
homeomorphism is sufficient---as opposed to a diffeomorphism, as is necessary
for the full dynamics. We provide preliminary evidence that a homeomorphism, in
the form of a delay-coordinate reconstruction map, may manifest at a lower
dimension than that required to achieve an embedding.


A new method for choosing parameters in delay reconstruction-based
  forecast strategies

  Delay-coordinate reconstruction is a proven modeling strategy for building
effective forecasts of nonlinear time series. The first step in this process is
the estimation of good values for two parameters, the time delay and the
embedding dimension. Many heuristics and strategies have been proposed in the
literature for estimating these values. Few, if any, of these methods were
developed with forecasting in mind, however, and their results are not optimal
for that purpose. Even so, these heuristics---intended for other
applications---are routinely used when building delay coordinate
reconstruction-based forecast models. In this paper, we propose a new strategy
for choosing optimal parameter values for forecast methods that are based on
delay-coordinate reconstructions. The basic calculation involves maximizing the
shared information between each delay vector and the future state of the
system. We illustrate the effectiveness of this method on several synthetic and
experimental systems, showing that this metric can be calculated quickly and
reliably from a relatively short time series, and that it provides a direct
indication of how well a near-neighbor based forecasting method will work on a
given delay reconstruction of that time series. This allows a practitioner to
choose reconstruction parameters that avoid any pathologies, regardless of the
underlying mechanism, and maximize the predictive information contained in the
reconstruction.


Prediction in Projection: A new paradigm in delay-coordinate
  reconstruction

  Delay-coordinate embedding is a powerful, time-tested mathematical framework
for reconstructing the dynamics of a system from a series of scalar
observations. Most of the associated theory and heuristics are overly stringent
for real-world data, however, and real-time use is out of the question due to
the expert human intuition needed to use these heuristics correctly. The
approach outlined in this thesis represents a paradigm shift away from that
traditional approach. I argue that perfect reconstructions are not only
unnecessary for the purposes of delay-coordinate based forecasting, but that
they can often be less effective than reduced-order versions of those same
models. I demonstrate this using a range of low- and high-dimensional dynamical
systems, showing that forecast models that employ imperfect reconstructions of
the dynamics---i.e., models that are not necessarily true embeddings---can
produce surprisingly accurate predictions of the future state of these systems.
I develop a theoretical framework for understanding why this is so. This
framework, which combines information theory and computational topology, also
allows one to quantify the amount of predictive structure in a given time
series, and even to choose which forecast method will be the most effective for
those data.


Climate entropy production recorded in a deep Antarctic ice core

  Paleoclimate records are extremely rich sources of information about the past
history of the Earth system. Information theory, the branch of mathematics
capable of quantifying the degree to which the present is informed by the past,
provides a new means for studying these records. Here, we demonstrate that
estimates of the Shannon entropy rate of the water-isotope data from the West
Antarctica Ice Sheet (WAIS) Divide ice core, calculated using weighted
permutation entropy (WPE), can bring out valuable new information from this
record. We find that WPE correlates with accumulation, reveals possible
signatures of geothermal heating at the base of the core, and clearly brings
out laboratory and data-processing effects that are difficult to see in the raw
data. For example, the signatures of Dansgaard-Oeschger events in the
information record are small, suggesting that these abrupt warming events may
not represent significant changes in the climate system dynamics. While the
potential power of information theory in paleoclimatology problems is
significant, the associated methods require careful handling and well-dated,
high-resolution data. The WAIS Divide ice core is the first such record that
can support this kind of analysis. As more high-resolution records become
available, information theory will likely become a common forensic tool in
climate science.


Anomaly Detection in Paleoclimate Records using Permutation Entropy

  Permutation entropy techniques can be useful in identifying anomalies in
paleoclimate data records, including noise, outliers, and post-processing
issues. We demonstrate this using weighted and unweighted permutation entropy
of water-isotope records in a deep polar ice core. In one region of these
isotope records, our previous calculations revealed an abrupt change in the
complexity of the traces: specifically, in the amount of new information that
appeared at every time step. We conjectured that this effect was due to noise
introduced by an older laboratory instrument. In this paper, we validate that
conjecture by re-analyzing a section of the ice core using a more-advanced
version of the laboratory instrument. The anomalous noise levels are absent
from the permutation entropy traces of the new data. In other sections of the
core, we show that permutation entropy techniques can be used to identify
anomalies in the raw data that are not associated with climatic or
glaciological processes, but rather effects occurring during field work,
laboratory analysis, or data post-processing. These examples make it clear that
permutation entropy is a useful forensic tool for identifying sections of data
that require targeted re-analysis---and can even be useful in guiding that
analysis.


The Ionized Gas and Nuclear Environment in NGC 3783 II. Averaged
  HST/STIS and FUSE Spectra

  We present observations of the intrinsic absorption in the Seyfert 1 galaxy
NGC 3783 obtained with the STIS/HST and FUSE. We have coadded multiple STIS and
FUSE observations to obtain a high S/N averaged spectrum spanning 905-1730 A.
The averaged spectrum reveals absorption in O VI, N V, C IV, N III, C III and
the Lyman lines up to LyE in the three blueshifted kinematic components
previously detected in the STIS spectrum (at radial velocities of -1320, -724,
and -548 km/s). The highest velocity component exhibits absorption in Si IV. We
also detect metastable C III* in this component, indicating a high density in
this absorber. We separate the individual covering factors of the continuum and
emission-line sources as a function of velocity in each kinematic component
using the LyA and LyB lines. Additionally, we find that the continuum covering
factor varies with velocity within the individual kinematic components,
decreasing smoothly in the wings of the absorption by at least 60%. The
covering factor of Si IV is found to be less than half that of H I and N V in
the high velocity component. Additionally, the FWHM of N III and Si IV are
narrower than the higher ionization lines in this component. These results
indicate there is substructure within this absorber. We derive a lower limit on
the total column (N_H>=10^{19}cm^{-2}) and ionization parameter (U>=0.005) in
the low ionization subcomponent of this absorber. The metastable-to-total C III
column density ratio implies n_e~10^9 cm^{-3} and an upper limit on the
distance of the absorber from the ionizing continuum of R<=8x10^{17} cm.


Further Definition of the Mass-Metallicity Relation in Globular Cluster
  Systems Around Brightest Cluster Galaxies

  We combine the globular cluster data for fifteen Brightest Cluster Galaxies
and use this material to trace the mass-metallicity relations (MMR) in their
globular cluster systems (GCSs). This work extends previous studies which
correlate the properties of the MMR with those of the host galaxy. Our combined
data sets show a mean trend for the metal-poor (MP) subpopulation which
corresponds to a scaling of heavy-element abundance with cluster mass Z ~
M^(0.30+/-0.05). No trend is seen for the metal-rich (MR) subpopulation which
has a scaling relation that is consistent with zero. We also find that the
scaling exponent is independent of the GCS specific frequency and host galaxy
luminosity, except perhaps for dwarf galaxies.
  We present new photometry in (g',i') obtained with Gemini/GMOS for the
globular cluster populations around the southern giant ellipticals NGC 5193 and
IC 4329. Both galaxies have rich cluster populations which show up as normal,
bimodal sequences in the colour-magnitude diagram.
  We test the observed MMRs and argue that they are statistically real, and not
an artifact caused by the method we used. We also argue against asymmetric
contamination causing the observed MMR as our mean results are no different
from other contamination-free studies. Finally, we compare our method to the
standard bimodal fitting method (KMM or RMIX) and find our results are
consistent.
  Interpretation of these results is consistent with recent models for globular
cluster formation in which the MMR is determined by GC self-enrichment during
their brief formation period.


Disk-Jet Connection in the Radio Galaxy 3C 120

  We present the results of extensive multi-frequency monitoring of the radio
galaxy 3C 120 between 2002 and 2007 at X-ray, optical, and radio wave bands, as
well as imaging with the Very Long Baseline Array (VLBA). Over the 5 yr of
observation, significant dips in the X-ray light curve are followed by
ejections of bright superluminal knots in the VLBA images. Consistent with
this, the X-ray flux and 37 GHz flux are anti-correlated with X-ray leading the
radio variations. This implies that, in this radio galaxy, the radiative state
of accretion disk plus corona system, where the X-rays are produced, has a
direct effect on the events in the jet, where the radio emission originates.
The X-ray power spectral density of 3C 120 shows a break, with steeper slope at
shorter timescale and the break timescale is commensurate with the mass of the
central black hole based on observations of Seyfert galaxies and black hole
X-ray binaries. These findings provide support for the paradigm that black hole
X-ray binaries and active galactic nuclei are fundamentally similar systems,
with characteristic time and size scales linearly proportional to the mass of
the central black hole. The X-ray and optical variations are strongly
correlated in 3C 120, which implies that the optical emission in this object
arises from the same general region as the X-rays, i.e., in the accretion
disk-corona system. We numerically model multi-wavelength light curves of 3C
120 from such a system with the optical-UV emission produced in the disk and
the X-rays generated by scattering of thermal photons by hot electrons in the
corona. From the comparison of the temporal properties of the model light
curves to that of the observed variability, we constrain the physical size of
the corona and the distances of the emitting regions from the central BH.


The First Post-Kepler Brightness Dips of KIC 8462852

  We present a photometric detection of the first brightness dips of the unique
variable star KIC 8462852 since the end of the Kepler space mission in 2013
May. Our regular photometric surveillance started in October 2015, and a
sequence of dipping began in 2017 May continuing on through the end of 2017,
when the star was no longer visible from Earth. We distinguish four main 1-2.5%
dips, named "Elsie," "Celeste," "Skara Brae," and "Angkor", which persist on
timescales from several days to weeks. Our main results so far are: (i) there
are no apparent changes of the stellar spectrum or polarization during the
dips; (ii) the multiband photometry of the dips shows differential reddening
favoring non-grey extinction. Therefore, our data are inconsistent with dip
models that invoke optically thick material, but rather they are in-line with
predictions for an occulter consisting primarily of ordinary dust, where much
of the material must be optically thin with a size scale <<1um, and may also be
consistent with models invoking variations intrinsic to the stellar
photosphere. Notably, our data do not place constraints on the color of the
longer-term "secular" dimming, which may be caused by independent processes, or
probe different regimes of a single process.


