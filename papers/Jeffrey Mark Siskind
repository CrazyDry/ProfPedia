Simultaneous Object Detection, Tracking, and Event Recognition

  The common internal structure and algorithmic organization of objectdetection, detection-based tracking, and event recognition facilitates ageneral approach to integrating these three components. This supportsmultidirectional information flow between these components allowing objectdetection to influence tracking and event recognition and event recognition toinfluence tracking and object detection. The performance of the combination canexceed the performance of the components in isolation. This can be done withlinear asymptotic complexity.

Automatic differentiation in machine learning: a survey

  Derivatives, mostly in the form of gradients and Hessians, are ubiquitous inmachine learning. Automatic differentiation (AD), also called algorithmicdifferentiation or simply "autodiff", is a family of techniques similar to butmore general than backpropagation for efficiently and accurately evaluatingderivatives of numeric functions expressed as computer programs. AD is a smallbut established field with applications in areas including computational fluiddynamics, atmospheric sciences, and engineering design optimization. Until veryrecently, the fields of machine learning and AD have largely been unaware ofeach other and, in some cases, have independently discovered each other'sresults. Despite its relevance, general-purpose AD has been missing from themachine learning toolbox, a situation slowly changing with its ongoing adoptionunder the names "dynamic computational graphs" and "differentiableprogramming". We survey the intersection of AD and machine learning, coverapplications where AD has direct relevance, and address the main implementationtechniques. By precisely defining the main differentiation techniques and theirinterrelationships, we aim to bring clarity to the usage of the terms"autodiff", "automatic differentiation", and "symbolic differentiation" asthese are encountered more and more in machine learning settings.

AD in Fortran, Part 1: Design

  We propose extensions to Fortran which integrate forward and reverseAutomatic Differentiation (AD) directly into the programming model.Irrespective of implementation technology, embedding AD constructs directlyinto the language extends the reach and convenience of AD while allowingabstraction of concepts of interest to scientific-computing practice, such asroot finding, optimization, and finding equilibria of continuous games.Multiple different subprograms for these tasks can share common interfaces,regardless of whether and how they use AD internally. A programmer can maximizea function F by calling a library maximizer, XSTAR=ARGMAX(F,X0), whichinternally constructs derivatives of F by AD, without having to learn how touse any particular AD tool. We illustrate the utility of these extensions byexample: programs become much more concise and closer to traditionalmathematical notation. A companion paper describes how these extensions can beimplemented by a program that generates input to existing Fortran-based ADtools.

AD in Fortran, Part 2: Implementation via Prepreprocessor

  We describe an implementation of the Farfel Fortran AD extensions. Theseextensions integrate forward and reverse AD directly into the programmingmodel, with attendant benefits to flexibility, modularity, and ease of use. Theimplementation we describe is a "prepreprocessor" that generates input toexisting Fortran-based AD tools. In essence, blocks of code which are targetedfor AD by Farfel constructs are put into subprograms which capture theirlexical variable context, and these are closure-converted into top-levelsubprograms and specialized to eliminate EXTERNAL arguments, rendering themamenable to existing AD preprocessors, which are then invoked, possiblyrepeatedly if the AD is nested.

Video In Sentences Out

  We present a system that produces sentential descriptions of video: who didwhat to whom, and where and how they did it. Action class is rendered as averb, participant objects as noun phrases, properties of those objects asadjectival modifiers in those noun phrases,spatial relations between thoseparticipants as prepositional phrases, and characteristics of the event asprepositional-phrase adjuncts and adverbial modifiers. Extracting theinformation needed to render these linguistic entities requires an approach toevent recognition that recovers object tracks, the track-to-role assignments,and changing body posture.

Seeing Unseeability to See the Unseeable

  We present a framework that allows an observer to determine occluded portionsof a structure by finding the maximum-likelihood estimate of those occludedportions consistent with visible image evidence and a consistency model. Doingthis requires determining which portions of the structure are occluded in thefirst place. Since each process relies on the other, we determine a solution toboth problems in tandem. We extend our framework to determine confidence ofone's assessment of which portions of an observed structure are occluded, andthe estimate of that occluded structure, by determining the sensitivity ofone's assessment to potential new observations. We further extend our frameworkto determine a robotic action whose execution would allow a new observationthat would maximally increase one's confidence.

Discriminative Training: Learning to Describe Video with Sentences, from  Video Described with Sentences

  We present a method for learning word meanings from complex and realisticvideo clips by discriminatively training (DT) positive sentential labelsagainst negative ones, and then use the trained word models to generatesentential descriptions for new video. This new work is inspired by recent workwhich adopts a maximum likelihood (ML) framework to address the same problemusing only positive sentential labels. The new method, like the ML-based one,is able to automatically determine which words in the sentence correspond towhich concepts in the video (i.e., ground words to meanings) in a weaklysupervised fashion. While both DT and ML yield comparable results withsufficient training data, DT outperforms ML significantly with smaller trainingsets because it can exploit negative training labels to better constrain thelearning problem.

Seeing What You're Told: Sentence-Guided Activity Recognition In Video

  We present a system that demonstrates how the compositional structure ofevents, in concert with the compositional structure of language, can interplaywith the underlying focusing mechanisms in video action recognition, therebyproviding a medium, not only for top-down and bottom-up integration, but alsofor multi-modal integration between vision and language. We show how the rolesplayed by participants (nouns), their characteristics (adjectives), the actionsperformed (verbs), the manner of such actions (adverbs), and changing spatialrelations between participants (prepositions) in the form of whole sententialdescriptions mediated by a grammar, guides the activity-recognition process.Further, the utility and expressiveness of our framework is demonstrated byperforming three separate tasks in the domain of multi-activity videos:sentence-guided focus of attention, generation of sentential descriptions ofvideo, and query-based video search, simply by leveraging the framework indifferent manners.

Video In Sentences Out

  We present a system that produces sentential descriptions of video: who didwhat to whom, and where and how they did it. Action class is rendered as averb, participant objects as noun phrases, properties of those objects asadjectival modifiers in those noun phrases, spatial relations between thoseparticipants as prepositional phrases, and characteristics of the event asprepositional-phrase adjuncts and adverbial modifiers. Extracting theinformation needed to render these linguistic entities requires an approach toevent recognition that recovers object tracks, the trackto-role assignments,and changing body posture.

A Faster Method for Tracking and Scoring Videos Corresponding to  Sentences

  Prior work presented the sentence tracker, a method for scoring how well asentence describes a video clip or alternatively how well a video clip depictsa sentence. We present an improved method for optimizing the same cost functionemployed by this prior work, reducing the space complexity from exponential inthe sentence length to polynomial, as well as producing a qualitativelyidentical result in time polynomial in the sentence length instead ofexponential. Since this new method is plug-compatible with the prior method, itcan be used for the same applications: video retrieval with sentential queries,generating sentential descriptions of video clips, and focusing the attentionof a tracker with a sentence, while allowing these applications to scale withsignificantly larger numbers of object detections, word meanings modeled withHMMs with significantly larger numbers of states, and significantly longersentences, with no appreciable degradation in quality of results.

Sentence Directed Video Object Codetection

  We tackle the problem of video object codetection by leveraging the weaksemantic constraint implied by sentences that describe the video content.Unlike most existing work that focuses on codetecting large objects which areusually salient both in size and appearance, we can codetect objects that aresmall or medium sized. Our method assumes no human pose or depth informationsuch as is required by the most recent state-of-the-art method. We employ weaksemantic constraint on the codetection process by pairing the video withsentences. Although the semantic information is usually simple and weak, it cangreatly boost the performance of our codetection framework by reducing thesearch space of the hypothesized object detections. Our experiment demonstratesan average IoU score of 0.423 on a new challenging dataset which contains 15object classes and 150 videos with 12,509 frames in total, and an average IoUscore of 0.373 on a subset of an existing dataset, originally intended foractivity recognition, which contains 5 object classes and 75 videos with 8,854frames in total.

Robot Language Learning, Generation, and Comprehension

  We present a unified framework which supports grounding natural-languagesemantics in robotic driving. This framework supports acquisition (learninggrounded meanings of nouns and prepositions from human annotation of roboticdriving paths), generation (using such acquired meanings to generate sententialdescription of new robotic driving paths), and comprehension (using suchacquired meanings to support automated driving to accomplish navigational goalsspecified in natural language). We evaluate the performance of these threetasks by having independent human judges rate the semantic fidelity of thesentences associated with paths, achieving overall average correctness of 94.6%and overall average completeness of 85.6%.

Collecting and Annotating the Large Continuous Action Dataset

  We make available to the community a new dataset to supportaction-recognition research. This dataset is different from prior datasets inseveral key ways. It is significantly larger. It contains streaming video withlong segments containing multiple action occurrences that often overlap inspace and/or time. All actions were filmed in the same collection ofbackgrounds so that background gives little clue as to action class. We hadfive humans replicate the annotation of temporal extent of action occurrenceslabeled with their class and measured a surprisingly low level of intercoderagreement. A baseline experiment shows that recent state-of-the-art methodsperform poorly on this dataset. This suggests that this will be a challengingdataset to foster advances in action-recognition research. This manuscriptserves to describe the novel content and characteristics of the LCA dataset,present the design decisions made when filming the dataset, and document thenovel methods employed to annotate the dataset.

DiffSharp: Automatic Differentiation Library

  In this paper we introduce DiffSharp, an automatic differentiation (AD)library designed with machine learning in mind. AD is a family of techniquesthat evaluate derivatives at machine precision with only a small constantfactor of overhead, by systematically applying the chain rule of calculus atthe elementary operator level. DiffSharp aims to make an extensive array of ADtechniques available, in convenient form, to the machine learning community.These including arbitrary nesting of forward/reverse AD operations, AD withlinear algebra primitives, and a functional API that emphasizes the use ofhigher-order functions and composition. The library exposes this functionalitythrough an API that provides gradients, Hessians, Jacobians, directionalderivatives, and matrix-free Hessian- and Jacobian-vector products. Bearing theperformance requirements of the latest machine learning techniques in mind, theunderlying computations are run through a high-performance BLAS/LAPACK backend,using OpenBLAS by default. GPU support is currently being implemented.

Binomial Checkpointing for Arbitrary Programs with No User Annotation

  Heretofore, automatic checkpointing at procedure-call boundaries, to reducethe space complexity of reverse mode, has been provided by systems likeTapenade. However, binomial checkpointing, or treeverse, has only been providedin Automatic Differentiation (AD) systems in special cases, e.g., throughuser-provided pragmas on DO loops in Tapenade, or as the nested tapingmechanism in adol-c for time integration processes, which requires that usercode be refactored. We present a framework for applying binomial checkpointingto arbitrary code with no special annotation or refactoring required. This isaccomplished by applying binomial checkpointing directly to a program trace.This trace is produced by a general-purpose checkpointing mechanism that isorthogonal to AD.

Efficient Implementation of a Higher-Order Language with Built-In AD

  We show that Automatic Differentiation (AD) operators can be provided in adynamic language without sacrificing numeric performance. To achieve this,general forward and reverse AD functions are added to a simple high-leveldynamic language, and support for them is included in an aggressive optimizingcompiler. Novel technical mechanisms are discussed, which have the ability tomigrate the AD transformations from run-time to compile-time. The resultingsystem, although only a research prototype, exhibits startlingly goodperformance. In fact, despite the potential inefficiencies entailed by supportof a functional-programming language and a first-class AD operator, performanceis competitive with the fastest available preprocessor-based Fortran ADsystems. On benchmarks involving nested use of the AD operators, it can evendramatically exceed their performance.

DiffSharp: An AD Library for .NET Languages

  DiffSharp is an algorithmic differentiation or automatic differentiation (AD)library for the .NET ecosystem, which is targeted by the C# and F# languages,among others. The library has been designed with machine learning applicationsin mind, allowing very succinct implementations of models and optimizationroutines. DiffSharp is implemented in F# and exposes forward and reverse ADoperators as general nestable higher-order functions, usable by any .NETlanguage. It provides high-performance linear algebra primitives---scalars,vectors, and matrices, with a generalization to tensors underway---that arefully supported by all the AD operators, and which use a BLAS/LAPACK backendvia the highly optimized OpenBLAS library. DiffSharp currently uses operatoroverloading, but we are developing a transformation-based version of thelibrary using F#'s "code quotation" metaprogramming facility. Work on aCUDA-based GPU backend is also underway.

Evolving the Incremental λ Calculus into a Model of Forward  Automatic Differentiation (AD)

  Formal transformations somehow resembling the usual derivative aresurprisingly common in computer science, with two notable examples beingderivatives of regular expressions and derivatives of types. A newcomer to thislist is the incremental $\lambda$-calculus, or ILC, a "theory of changes" thatdeploys a formal apparatus allowing the automatic generation of efficientupdate functions which perform incremental computation. The ILC is not onlydefined, but given a formal machine-understandable definition---accompanied bymechanically verifiable proofs of various properties, including in particularcorrectness of various sorts. Here, we show how the ILC can be mutated intopropagating tangents, thus serving as a model of Forward Accumulation ModeAutomatic Differentiation. This mutation is done in several steps. These stepscan also be applied to the proofs, resulting in machine-checked proofs of thecorrectness of this model of forward AD.

Divide-and-Conquer Checkpointing for Arbitrary Programs with No User  Annotation

  Classical reverse-mode automatic differentiation (AD) imposes only a smallconstant-factor overhead in operation count over the original computation, buthas storage requirements that grow, in the worst case, in proportion to thetime consumed by the original computation. This storage blowup can beameliorated by checkpointing, a process that reorders application of classicalreverse-mode AD over an execution interval to tradeoff space \vs\ time.Application of checkpointing in a divide-and-conquer fashion to strategicallychosen nested execution intervals can break classical reverse-mode AD intostages which can reduce the worst-case growth in storage from linear tosublinear. Doing this has been fully automated only for computations ofparticularly simple form, with checkpoints spanning execution intervalsresulting from a limited set of program constructs. Here we show how thetechnique can be automated for arbitrary computations. The essential innovationis to apply the technique at the level of the language implementation itself,thus allowing checkpoints to span any execution interval.

Large-Scale Automatic Labeling of Video Events with Verbs Based on  Event-Participant Interaction

  We present an approach to labeling short video clips with English verbs asevent descriptions. A key distinguishing aspect of this work is that it labelsvideos with verbs that describe the spatiotemporal interaction between eventparticipants, humans and objects interacting with each other, abstracting awayall object-class information and fine-grained image characteristics, andrelying solely on the coarse-grained motion of the event participants. We applyour approach to a large set of 22 distinct verb classes and a corpus of 2,584videos, yielding two surprising outcomes. First, a classification accuracy ofgreater than 70% on a 1-out-of-22 labeling task and greater than 85% on avariety of 1-out-of-10 subsets of this labeling task is independent of thechoice of which of two different time-series classifiers we employ. Second, weachieve this level of accuracy using a highly impoverished intermediaterepresentation consisting solely of the bounding boxes of one or two eventparticipants as a function of time. This indicates that successful eventrecognition depends more on the choice of appropriate features thatcharacterize the linguistic invariants of the event classes than on theparticular classifier algorithms.

The Compositional Nature of Verb and Argument Representations in the  Human Brain

  How does the human brain represent simple compositions of objects, actors,andactions? We had subjects view action sequence videos during neuroimaging (fMRI)sessions and identified lexical descriptions of those videos by decoding (SVM)the brain representations based only on their fMRI activation patterns. As aprecursor to this result, we had demonstrated that we could reliably and withhigh probability decode action labels corresponding to one of six action videos(dig, walk, etc.), again while subjects viewed the action sequence duringscanning (fMRI). This result was replicated at two different brain imagingsites with common protocols but different subjects, showing common brain areas,including areas known for episodic memory (PHG, MTL, high level visualpathways, etc.,i.e. the 'what' and 'where' systems, and TPJ, i.e. 'theory ofmind'). Given these results, we were also able to successfully show a keyaspect of language compositionality based on simultaneous decoding of objectclass and actor identity. Finally, combining these novel steps in 'brainreading' allowed us to accurately estimate brain representations supportingcompositional decoding of a complex event composed of an actor, a verb, adirection, and an object.

Felzenszwalb-Baum-Welch: Event Detection by Changing Appearance

  We propose a method which can detect events in videos by modeling the changein appearance of the event participants over time. This method makes itpossible to detect events which are characterized not by motion, but by thechanging state of the people or objects involved. This is accomplished by usingobject detectors as output models for the states of a hidden Markov model(HMM). The method allows an HMM to model the sequence of poses of the eventparticipants over time, and is effective for poses of humans and inanimateobjects. The ability to use existing object-detection methods as part of anevent model makes it possible to leverage ongoing work in the object-detectioncommunity. A novel training method uses an EM loop to simultaneously learn thetemporal structure and object models automatically, without the need to specifyeither the individual poses to be modeled or the frames in which they occur.The E-step estimates the latent assignment of video frames to HMM states, whilethe M-step estimates both the HMM transition probabilities and state outputmodels, including the object detectors, which are trained on the weightedsubset of frames assigned to their state. A new dataset was gathered becauselittle work has been done on events characterized by changing object pose, andsuitable datasets are not available. Our method produced results superior tothat of comparison systems on this dataset.

Saying What You're Looking For: Linguistics Meets Video Search

  We present an approach to searching large video corpora for video clips whichdepict a natural-language query in the form of a sentence. This approach usescompositional semantics to encode subtle meaning that is lost in other systems,such as the difference between two sentences which have identical words butentirely different meaning: "The person rode the horse} vs. \emph{The horserode the person". Given a video-sentence pair and a natural-language parser,along with a grammar that describes the space of sentential queries, we producea score which indicates how well the video depicts the sentence. We producesuch a score for each video clip in a corpus and return a ranked list of clips.Furthermore, this approach addresses two fundamental problems simultaneously:detecting and tracking objects, and recognizing whether those tracks depict thequery. Because both tracking and object detection are unreliable, this usesknowledge about the intended sentential query to focus the tracker on therelevant participants and ensures that the resulting tracks are described bythe sentential query. While earlier work was limited to single-word querieswhich correspond to either verbs or nouns, we show how one can search forcomplex queries which contain multiple phrases, such as prepositional phrases,and modifiers, such as adverbs. We demonstrate this approach by searching for141 queries involving people and horses interacting with each other in 10full-length Hollywood movies.

The Compositional Nature of Event Representations in the Human Brain

  How does the human brain represent simple compositions of constituents:actors, verbs, objects, directions, and locations? Subjects viewed videosduring neuroimaging (fMRI) sessions from which sentential descriptions of thosevideos were identified by decoding the brain representations based only ontheir fMRI activation patterns. Constituents (e.g., "fold" and "shirt") wereindependently decoded from a single presentation. Independent constituentclassification was then compared to joint classification of aggregate concepts(e.g., "fold-shirt"); results were similar as measured by accuracy andcorrelation. The brain regions used for independent constituent classificationare largely disjoint and largely cover those used for joint classification.This allows recovery of sentential descriptions of stimulus videos by composingthe results of the independent constituent classifiers. Furthermore,classifiers trained on the words one set of subjects think of when watching avideo can recognise sentences a different subject thinks of when watching adifferent video.

Tricks from Deep Learning

  The deep learning community has devised a diverse set of methods to makegradient optimization, using large datasets, of large and highly complex modelswith deeply cascaded nonlinearities, practical. Taken as a whole, these methodsconstitute a breakthrough, allowing computational structures which are quitewide, very deep, and with an enormous number and variety of free parameters tobe effectively optimized. The result now dominates much of practical machinelearning, with applications in machine translation, computer vision, and speechrecognition. Many of these methods, viewed through the lens of algorithmicdifferentiation (AD), can be seen as either addressing issues with the gradientitself, or finding ways of achieving increased efficiency using tricks that areAD-related, but not provided by current AD systems.  The goal of this paper is to explain not just those methods of most relevanceto AD, but also the technical constraints and mindset which led to theirdiscovery. After explaining this context, we present a "laundry list" ofmethods developed by the deep learning community. Two of these are discussed infurther mathematical detail: a way to dramatically reduce the size of the tapewhen performing reverse-mode AD on a (theoretically) time-reversible processlike an ODE integrator; and a new mathematical insight that allows for theimplementation of a stochastic Newton's method.

Confusion of Tagged Perturbations in Forward Automatic Differentiation  of Higher-Order Functions

  Forward Automatic Differentiation (AD) is a technique for augmenting programsto compute derivatives. The essence of Forward AD is to attach perturbations toeach number, and propagate these through the computation. When derivatives arenested, the distinct derivative calculations, and their associatedperturbations, must be distinguished. This is typically accomplished bycreating a unique tag for each derivative calculation, tagging theperturbations, and overloading the arithmetic operators. We exhibit a subtlebug, present in fielded implementations, in which perturbations are confuseddespite the tagging machinery. The essence of the bug is this: each invocationof a derivative creates a unique tag but a unique tag is needed for eachderivative calculation. When taking derivatives of higher-order functions,these need not correspond! The derivative of a higher-order function $f$ thatreturns a function $g$ will be a function $f'$ that returns a function$\bar{g}$ that performs a derivative calculation. A single invocation of $f'$will create a single fresh tag but that same tag will be used for eachderivative calculation resulting from an invocation of $\bar{g}$. Thissituation arises when taking derivatives of curried functions. Two potentialsolutions are presented, and their serious deficiencies discussed. One requireseta expansion to delay the creation of fresh tags from the invocation of $f'$to the invocation of $\bar{g}$, which can be difficult or even impossible insome circumstances. The other requires $f'$ to wrap $\bar{g}$ with tagrenaming, which is difficult to implement without violating the desirablecomplexity properties of forward AD.

Training on the test set? An analysis of Spampinato et al. [31]

  A recent paper [31] claims to classify brain processing evoked in subjectswatching ImageNet stimuli as measured with EEG and to use a representationderived from this processing to create a novel object classifier. That paper,together with a series of subsequent papers [8, 15, 17, 20, 21, 30, 35], claimsto revolutionize the field by achieving extremely successful results on severalcomputer-vision tasks, including object classification, transfer learning, andgeneration of images depicting human perception and thought using brain-derivedrepresentations measured through EEG. Our novel experiments and analysesdemonstrate that their results crucially depend on the block design that theyuse, where all stimuli of a given class are presented together, and fail with arapid-event design, where stimuli of different classes are randomly intermixed.The block design leads to classification of arbitrary brain states based onblock-level temporal correlations that tend to exist in all EEG data, ratherthan stimulus-related activity. Because every trial in their test sets comesfrom the same block as many trials in the corresponding training sets, theirblock design thus leads to surreptitiously training on the test set. Thisinvalidates all subsequent analyses performed on this data in multiplepublished papers and calls into question all of the purported results. Wefurther show that a novel object classifier constructed with a random codebookperforms as well as or better than a novel object classifier constructed withthe representation extracted from EEG data, suggesting that the performance oftheir classifier constructed with a representation extracted from EEG data doesnot benefit at all from the brain-derived representation. Our results calibratethe underlying difficulty of the tasks involved and caution against sensationaland overly optimistic, but false, claims to the contrary.

