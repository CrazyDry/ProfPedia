Community Detection via Facility Location

  In this paper we apply theoretical and practical results from facilitylocation theory to the problem of community detection in networks. The resultis an algorithm that computes bounds on a minimization variant of localmodularity. We also define the concept of an edge support and a new measure ofthe goodness of community structures with respect to this concept. We presentpreliminary results and note that our methods are massively parallelizable.

Shannon Meets Nash on the Interference Channel

  The interference channel is the simplest communication scenario wheremultiple autonomous users compete for shared resources. We combine game theoryand information theory to define a notion of a Nash equilibrium region of theinterference channel. The notion is game theoretic: it captures the selfishbehavior of each user as they compete. The notion is also informationtheoretic: it allows each user to use arbitrary communication strategies as itoptimizes its own performance. We give an exact characterization of the Nashequilibrium region of the two-user linear deterministic interference channeland an approximate characterization of the Nash equilibrium region of thetwo-user Gaussian interference channel to within 1 bit/s/Hz..

The Cost of Free Spectrum

  There has been growing interest in increasing the amount of radio spectrumavailable for unlicensed broad-band wireless access. That includes "prime"spectrum at lower frequencies, which is also suitable for wide area coverage bylicensed cellular providers. While additional unlicensed spectrum would allowfor market expansion, it could influence competition among providers andincrease congestion (interference) among consumers of wireless services. Westudy the value (social welfare and consumer surplus) obtained by addingunlicensed spectrum to an existing allocation of licensed spectrum amongincumbent service providers. We assume a population of customers who choose aprovider based on the minimum delivered price, given by the weighted sum of theprice of the service and a congestion cost, which depends on the number ofsubscribers in a band. We consider models in which this weighting is uniformacross the customer population and where the weighting is either high or low,reflecting different sensitivities to latency. For the models considered, wefind that the social welfare depends on the amount of additional unlicensedspectrum, and can actually decrease over a significant range of unlicensedbandwidths. Furthermore, with nonuniform weighting, introducing unlicensedspectrum can also reduce consumer welfare.

Searching and Bargaining with Middlemen

  We study decentralized markets with the presence of middlemen, modeled by anon-cooperative bargaining game in trading networks. Our goal is to investigatehow the network structure of the market and the role of middlemen influence themarket's efficiency and fairness. We introduce the concept of limit stationaryequilibrium in a general trading network and use it to analyze how competitionamong middlemen is influenced by the network structure, how endogenous delayemerges in trade and how surplus is shared between producers and consumers.

Tolerating the Community Detection Resolution Limit with Edge Weighting

  Communities of vertices within a giant network such as the World-Wide Web arelikely to be vastly smaller than the network itself. However, Fortunato andBarth\'{e}lemy have proved that modularity maximization algorithms forcommunity detection may fail to resolve communities with fewer than$\sqrt{L/2}$ edges, where $L$ is the number of edges in the entire network.This resolution limit leads modularity maximization algorithms to havenotoriously poor accuracy on many real networks. Fortunato and Barth\'{e}lemy'sargument can be extended to networks with weighted edges as well, and we derivethis corollary argument. We conclude that weighted modularity algorithms mayfail to resolve communities with fewer than $\sqrt{W \epsilon/2}$ total edgeweight, where $W$ is the total edge weight in the network and $\epsilon$ is themaximum weight of an inter-community edge. If $\epsilon$ is small, then smallcommunities can be resolved.  Given a weighted or unweighted network, we describe how to derive new edgeweights in order to achieve a low $\epsilon$, we modify the ``CNM'' communitydetection algorithm to maximize weighted modularity, and show that theresulting algorithm has greatly improved accuracy. In experiments with anemerging community standard benchmark, we find that our simple CNM variant iscompetitive with the most accurate community detection methods yet proposed.

Medium Access Control for Wireless Networks with Peer-to-Peer State  Exchange

  Distributed medium access control (MAC) protocols are proposed for wirelessnetworks assuming that one-hop peers can periodically exchange a small amountof state information. Each station maintains a state and makes statetransitions and transmission decisions based on its state and recent stateinformation collected from its one-hop peers. A station can adapt its packetlength and the size of its state space to the amount of traffic in itsneighborhood. It is shown that these protocols converge to a steady state,where stations take turns to transmit in each neighborhood without collision.In other words, an efficient time-division multiple access (TDMA) like scheduleis formed in a distributed manner, as long as the topology of the networkremains static or changes slowly with respect to the execution of the protocol.

Explaining Snapshots of Network Diffusions: Structural and Hardness  Results

  Much research has been done on studying the diffusion of ideas ortechnologies on social networks including the \textit{Influence Maximization}problem and many of its variations. Here, we investigate a type of inverseproblem. Given a snapshot of the diffusion process, we seek to understand ifthe snapshot is feasible for a given dynamic, i.e., whether there is a limitednumber of nodes whose initial adoption can result in the snapshot in finitetime. While similar questions have been considered for epidemic dynamics, here,we consider this problem for variations of the deterministic Linear ThresholdModel, which is more appropriate for modeling strategic agents. Specifically,we consider both sequential and simultaneous dynamics when deactivations areallowed and when they are not. Even though we show hardness results for allvariations we consider, we show that the case of sequential dynamics withdeactivations allowed is significantly harder than all others. In contrast,sequential dynamics make the problem trivial on cliques even though it'scomplexity for simultaneous dynamics is unknown. We complement our hardnessresults with structural insights that can help better understand diffusions ofsocial networks under various dynamics.

Coordination protocol for inter-operator spectrum sharing in co-primary  5G small cell networks

  We consider spectrum sharing between a limited set of operators havingsimilar rights for accessing spectrum. A coordination protocol acting on thelevel of the Radio Access Network (RAN) is designed. The protocol isnon-cooperative, but assumes an agreement to a set of negotiation rules. Thesignaling overhead is low, and knowledge of competitor's channel stateinformation is not assumed. No monetary transactions are involved; instead,spectrum sharing is based on a RAN-internal virtual currency. The protocol isapplicable in a scenario of mutual renting and when the operators form aspectrum pool. The protocol is reactive to variations in interference and loadof the operators, and shows gains in a simulated small cell scenario comparedto not using any coordination protocol.

The Impact of Unlicensed Access on Small-Cell Resource Allocation

  Small cells deployed in licensed spectrum and unlicensed access via WiFiprovide different ways of expanding wireless services to low mobility users.That reduces the demand for conventional macro-cellular networks, which arebetter suited for wide-area mobile coverage. The mix of these technologies seenin practice depends in part on the decisions made by wireless service providersthat seek to maximize revenue, and allocations of licensed and unlicensedspectrum by regulators. To understand these interactions we present a model inwhich a service provider allocates available licensed spectrum across twoseparate bands, one for macro- and one for small-cells, in order to serve twotypes of users: mobile and fixed. We assume a service model in which theproviders can charge a (different) price per unit rate for each type of service(macro- or small-cell); unlicensed access is free. With this setup we study howthe addition of unlicensed spectrum affects prices and the optimal allocationof bandwidth across macro-/small-cells. We also characterize the optimalfraction of unlicensed spectrum when new bandwidth becomes available.

The Value of Sharing Intermittent Spectrum

  Recent initiatives by regulatory agencies to increase spectrum resourcesavailable for broadband access include rules for sharing spectrum withhigh-priority incumbents. We study a model in which wireless Service Providers(SPs) charge for access to their own exclusive-use (licensed) band along withaccess to an additional shared band. The total, or delivered price in each bandis the announced price plus a congestion cost, which depends on the load, ortotal users normalized by the bandwidth. The shared band is intermittentlyavailable with some probability, due to incumbent activity, and whenunavailable, any traffic carried on that band must be shifted to licensedbands. The SPs then compete for quantity of users. We show that the value ofthe shared band depends on the relative sizes of the SPs: large SPs with morebandwidth are better able to absorb the variability caused by intermittencythan smaller SPs. However, as the amount of shared spectrum increases, thelarge SPs may not make use of it. In that scenario shared spectrum creates morevalue than splitting it among the SPs for exclusive use. We also show thatfixing the average amount of available shared bandwidth, increasing thereliability of the band is preferable to increasing the bandwidth.

The impact of bundling licensed and unlicensed wireless service

  Unlicensed spectrum has been viewed as a way to increase competition inwireless access and promote innovation in new technologies and business models.However, several recent papers have shown that the openness of such spectrumcan also lead to it becoming over congested when used by competing wirelessservice providers (SPs). This in turn can result in the SPs making no profitand may deter them from entering the market. However, this prior work assumesthat unlicensed access is a separate service from any service offered usinglicensed spectrum. Here, we instead consider the more common case were serviceproviders bundle both licensed and unlicensed spectrum as a single service andoffer this with a single price. We analyze a model for such a market and showthat in this case SPs are able to gain higher profit than the case withoutbundling. It is also possible to get higher social welfare with bundling.Moreover, we explore the case where SPs are allowed to manage the customers'average percentage of time they receive service on unlicensed spectrum andcharacterize the social welfare gap between the profit maximizing and socialwelfare maximizing setting.

Fictitious GAN: Training GANs with Historical Models

  Generative adversarial networks (GANs) are powerful tools for learninggenerative models. In practice, the training may suffer from lack ofconvergence. GANs are commonly viewed as a two-player zero-sum game between twoneural networks. Here, we leverage this game theoretic view to study theconvergence behavior of the training process. Inspired by the fictitious playlearning process, a novel training method, referred to as Fictitious GAN, isintroduced. Fictitious GAN trains the deep neural networks using a mixture ofhistorical models. Specifically, the discriminator (resp. generator) is updatedaccording to the best-response to the mixture outputs from a sequence ofpreviously trained generators (resp. discriminators). It is shown thatFictitious GAN can effectively resolve some convergence issues that cannot beresolved by the standard training approach. It is proved that asymptoticallythe average of the generator outputs has the same distribution as the datasamples.

A fixed-point model for semi-persistent scheduling of vehicular safety  messages

  In this paper, we focus on the performance analysis of a semi-persistentscheduling scheme for vehicular safety communications, motivated by the Mode 4medium access control protocol in 3GPP Release 14 for Cellular-V2X. Ananalytical model is built and a fixed point method is used to calculate thecollision probability and average delay in both fully connected and partiallyconnected cases under the assumption of perfect PHY performance. We use MonteCarlo simulation to verify the results obtained in the analytical model. Thesimulation results show that our analytical model can give a good estimation ofthe collision probability and average delay. We verify that a trade-off betweendelay and collision probability can be achieved with a flexible resource blockselection. Monte Carlo simulation results show that with the flexible resourceselection scheme average delay can be shortened significantly with only a smallcompromise in collision probability.

Competition with Three-Tier Spectrum Access and Spectrum Monitoring

  The Citizens Broadband Radio Service (CBRS) recently adopted in the U.S.enables two tiers of commercial users to share spectrum with a third tier ofincumbent users. This sharing can be further assisted by Environmental SensingCapability operators (ESCs), that monitor the spectrum occupancy to determinewhen use of the spectrum will not harm incumbents. Two key aspects of thisframework that impact how firms may compete are the differences in informationprovided by different ESCs and the different tiers in which a firm may accessthe spectrum. We develop a game theoretic model that captures both of thesefeatures and analyze it to gain insight into their impact. Specifically, weconsider a priority access (PA) tier firm has access to the both licensed bandand unlicensed band, and a general authorized access (GAA) tier firm has accessonly to the unlicensed band. The PA tier and GAA tier firms compete for users.Our analysis reveals that the amount of unlicensed and licensed bandwidth inthe CBRS must be chosen judiciously in order to maximize the social welfare. Wealso show that a limited amount of unlicensed access of PA tier firm isbeneficial to the user's surplus as well as to the social welfare.

Spatial Interference Cancelation for Mobile Ad Hoc Networks: Perfect CSI

  Interference between nodes directly limits the capacity of mobile ad hocnetworks. This paper focuses on spatial interference cancelation with perfectchannel state information (CSI), and analyzes the corresponding networkcapacity. Specifically, by using multiple antennas, zero-forcing beamforming isapplied at each receiver for canceling the strongest interferers. Given spatialinterference cancelation, the network transmission capacity is analyzed in thispaper, which is defined as the maximum transmitting node density underconstraints on outage and the signal-to-interference-noise ratio. Assuming thePoisson distribution for the locations of network nodes and spatially i.i.d.Rayleigh fading channels, mathematical tools from stochastic geometry areapplied for deriving scaling laws for transmission capacity. Specifically, forsmall target outage probability, transmission capacity is proved to increasefollowing a power law, where the exponent is the inverse of the size of antennaarray or larger depending on the pass loss exponent. As shown by simulations,spatial interference cancelation increases transmission capacity by an order ofmagnitude or more even if only one extra antenna is added to each node.

Spatial Interference Cancellation for Multi-Antenna Mobile Ad Hoc  Networks

  Interference between nodes is a critical impairment in mobile ad hoc networks(MANETs). This paper studies the role of multiple antennas in mitigating suchinterference. Specifically, a network is studied in which receivers applyzero-forcing beamforming to cancel the strongest interferers. Assuming anetwork with Poisson distributed transmitters and independent Rayleigh fadingchannels, the transmission capacity is derived, which gives the maximum numberof successful transmissions per unit area. Mathematical tools from stochasticgeometry are applied to obtain the asymptotic transmission capacity scaling andcharacterize the impact of inaccurate channel state information (CSI). It isshown that, if each node cancels L interferers, the transmission capacitydecreases as the outage probability to the power of 1/(L+1) as the outageprobability vanishes. For fixed outage probability, as L grows, thetransmission capacity increases as L to the power of (1-2/alpha) where alpha isthe path-loss exponent. Moreover, CSI inaccuracy is shown to have no effect onthe transmission capacity scaling as the outage probability vanishes, providedthat the CSI training sequence has an appropriate length, which we derived.Numerical results suggest that canceling merely one interferer by each nodeincreases the transmission capacity by an order of magnitude or more, even whenthe CSI is imperfect.

Adaptive Beamforming in Interference Networks via Bi-Directional  Training

  We study distributed algorithms for adjusting beamforming vectors andreceiver filters in multiple-input multiple-output (MIMO) interferencenetworks, with the assumption that each user uses a single beam and a linearfilter at the receiver. In such a setting there have been several distributedalgorithms studied for maximizing the sum-rate or sum-utility assuming perfectchannel state information (CSI) at the transmitters and receivers. The focus ofthis paper is to study adaptive algorithms for time-varying channels, withoutassuming any CSI at the transmitters or receivers. Specifically, we consider anadaptive version of the recent Max-SINR algorithm for a time-division duplexsystem. This algorithm uses a period of bi-directional training followed by ablock of data transmission. Training in the forward direction is sent using thecurrent beam-formers and used to adapt the receive filters. Training in thereverse direction is sent using the current receive filters as beams and usedto adapt the transmit beamformers. The adaptation of both receive filters andbeamformers is done using a least-squares objective for the current block. Inorder to improve the performance when the training data is limited, we alsoconsider using exponentially weighted data from previous blocks. Numericalresults are presented that compare the performance of the algorithms indifferent settings.

The value of Side Information in Secondary Spectrum Markets

  In a secondary spectrum market primaries set prices for their unused channelsto the secondaries. The payoff of a primary depends on the availability ofunused channels of its competitors. We consider a model were a primary canacquire its competitor's channel state information (C-CSI) at a cost. Weformulate a game between two primaries where each primary decides whether toacquire C-CSI or not and then selects its price based on that. We firstcharacterize the Nash Equilibrium (NE) of this game for a symmetric model wherethe C-CSI is perfect. We show that the payoff of a primary is independent ofthe C-CSI acquisition cost. We then generalize our analysis to allow forimperfect estimation and cases where the two primaries have different C-CSIcosts or different channel availabilities. Our results show interestingly thatthe payoff of a primary increases when there is estimation error. We also showthat surprisingly, the expected payoff of a primary may decrease when the C-CSIacquisition cost decreases when primaries have different availabilities.

Co-primary inter-operator spectrum sharing over a limited spectrum pool  using repeated games

  We consider two small cell operators deployed in the same geographical area,sharing spectrum resources from a common pool. A method is investigated tocoordinate the utilization of the spectrum pool without monetary transactionsand without revealing operator-specific information to other parties. For this,we construct a protocol based on asking and receiving spectrum usage favors bythe operators, and keeping a book of the favors. A spectrum usage favor isexchanged between the operators if one is asking for a permission to use someof the resources from the pool on an exclusive basis, and the other is willingto accept that. As a result, the proposed method does not force an operator totake action. An operator with a high load may take spectrum usage favors froman operator that has few users to serve, and it is likely to return thesefavors in the future to show a cooperative spirit and maintain reciprocity. Weformulate the interactions between the operators as a repeated game anddetermine rules to decide whether to ask or grant a favor at each stage game.We illustrate that under frequent network load variations, which are expectedto be prominent in small cell deployments, both operators can attain higheruser rates as compared to the case of no coordination of the resourceutilization.

A Perspective on Future Research Directions in Information Theory

  Information theory is rapidly approaching its 70th birthday. What arepromising future directions for research in information theory? Where willinformation theory be having the most impact in 10-20 years? What new andemerging areas are ripe for the most impact, of the sort that informationtheory has had on the telecommunications industry over the last 60 years? Howshould the IEEE Information Theory Society promote high-risk new researchdirections and broaden the reach of information theory, while continuing to betrue to its ideals and insisting on the intellectual rigor that makes itsbreakthroughs so powerful? These are some of the questions that an ad hoccommittee (composed of the present authors) explored over the past two years.We have discussed and debated these questions, and solicited detailed inputsfrom experts in fields including genomics, biology, economics, andneuroscience. This report is the result of these discussions.

Competitive Resource Allocation in HetNets: the Impact of Small-cell  Spectrum Constraints and Investment Costs

  Heterogeneous wireless networks with small-cell deployments in licensed andunlicensed spectrum bands are a promising approach for expanding wirelessconnectivity and service. As a result, wireless service providers (SPs) areadding small-cells to augment their existing macro-cell deployments. This addedflexibility complicates network management, in particular, service pricing andspectrum allocations across macro- and small-cells. Further, these decisionsdepend on the degree of competition among SPs. Restrictions on shared spectrumaccess imposed by regulators, such as low power constraints that lead tosmall-cell deployments, along with the investment cost needed to add smallcells to an existing network, also impact strategic decisions and marketefficiency. If the revenue generated by small-cells does not cover theinvestment cost, then there will be no deployment even if it increases socialwelfare. We study the implications of such spectrum constraints and investmentcosts on resource allocation and pricing decisions by competitive SPs, alongwith the associated social welfare. Our results show that while the optimalresource allocation taking constraints and investment into account can beuniquely determined, adding those features with strategic SPs can have asubstantial effect on the equilibrium market structure.

Joint Transmission with Limited Backhaul Connectivity

  Downlink beamforming techniques with low signaling overhead are proposed forjoint processing coordinated (JP) multi-point transmission. The objective is tomaximize the weighted sum rate within joint transmission clusters. As theconsidered weighted sum rate maximization is a non-convex problem, successiveconvex approximation techniques, based on weighted mean-squared errorminimization, are applied to devise algorithms with tractable computationalcomplexity. Decentralized algorithms are proposed to enable JP even withlimited backhaul connectivity. These algorithms rely provide a variety ofalternatives for signaling overhead, computational complexity and convergencebehavior. Time division duplexing is exploited to design transceiver trainingtechniques for two scenarios: stream specific estimation and direct estimation.In the stream specific estimation, the base station and user equipment estimateall of the stream specific precoded pilots individually and construct thetransmit/receive covariance matrices based on these pilot estimates. With thedirect estimation, only the intended transmission is separately estimated andthe covariance matrices constructed directly from the aggregate system-widepilots. The impact of feedback/backhaul signaling quantization is considered,in order to further reduce the signaling overhead. Also, user admission isbeing considered for time-correlated channels. The enhanced transceiverconvergence rate enables periodic beamformer reinitialization, which greatlyimproves the achieved system performance in dense networks.

Analyzing Location-Based Advertising for Vehicle Service Providers Using  Effective Resistances

  Vehicle service providers can display commercial ads in their vehicles basedon passengers' origins and destinations to create a new revenue stream. In thiswork, we study a vehicle service provider who can generate different adrevenues when displaying ads on different arcs (i.e., origin-destinationpairs). The provider needs to ensure the vehicle flow balance at each location,which makes it challenging to analyze the provider's vehicle assignment andpricing decisions for different arcs. For example, the provider's price for itsservice on an arc depends on the ad revenues on other arcs as well as on thearc in question. To tackle the problem, we show that the traffic networkcorresponds to an electrical network. When the effective resistance between twolocations is small, there are many paths between the two locations and theprovider can easily route vehicles between them. We characterize the dependenceof an arc's optimal price on any other arc's ad revenue using the effectiveresistances between these two arcs' origins and destinations. Furthermore, westudy the provider's optimal selection of advertisers when it can only displayads for a limited number of advertisers. If each advertiser has one target arcfor advertising, the provider should display ads for the advertiser whosetarget arc has a small effective resistance. We investigate the performance ofour advertiser selection strategy based on a real-world dataset.

The Impact of Small-Cell Bandwidth Requirements on Strategic Operators

  Small-cell deployment in licensed and unlicensed spectrum is considered to beone of the key approaches to cope with the ongoing wireless data demandexplosion. Compared to traditional cellular base stations with largetransmission power, small-cells typically have relatively low transmissionpower, which makes them attractive for some spectrum bands that have strictpower regulations, for example, the 3.5GHz band [1]. In this paper we considera heterogeneous wireless network consisting of one or more service providers(SPs). Each SP operates in both macro-cells and small-cells, and providesservice to two types of users: mobile and fixed. Mobile users can onlyassociate with macro-cells whereas fixed users can connect to either macro- orsmall-cells. The SP charges a price per unit rate for each type of service.Each SP is given a fixed amount of bandwidth and splits it between macro- andsmall-cells. Motivated by bandwidth regulations, such as those for the 3.5Gzband, we assume a minimum amount of bandwidth has to be set aside forsmall-cells. We study the optimal pricing and bandwidth allocation strategiesin both monopoly and competitive scenarios. In the monopoly scenario thestrategy is unique. In the competitive scenario there exists a unique Nashequilibrium, which depends on the regulatory constraints. We also analyze thesocial welfare achieved, and compare it to that without the small-cellbandwidth constraints. Finally, we discuss implications of our results on theeffectiveness of the minimum bandwidth constraint on influencing small-celldeployments.

Theia: Faint objects in motion or the new astrometry frontier

  In the context of the ESA M5 (medium mission) call we proposed a newsatellite mission, Theia, based on relative astrometry and extreme precision tostudy the motion of very faint objects in the Universe. Theia is primarilydesigned to study the local dark matter properties, the existence of Earth-likeexoplanets in our nearest star systems and the physics of compact objects.Furthermore, about 15 $\%$ of the mission time was dedicated to an openobservatory for the wider community to propose complementary science cases.With its unique metrology system and "point and stare" strategy, Theia'sprecision would have reached the sub micro-arcsecond level. This is about 1000times better than ESA/Gaia's accuracy for the brightest objects and representsa factor 10-30 improvement for the faintest stars (depending on the exactobservational program). In the version submitted to ESA, we proposed an optical(350-1000nm) on-axis TMA telescope. Due to ESA Technology readiness level, thecamera's focal plane would have been made of CCD detectors but we anticipatedan upgrade with CMOS detectors. Photometric measurements would have beenperformed during slew time and stabilisation phases needed for reaching therequired astrometric precision.

