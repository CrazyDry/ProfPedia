Detection of Abnormal Input-Output Associations

  We study a novel outlier detection problem that aims to identify abnormalinput-output associations in data, whose instances consist of multi-dimensionalinput (context) and output (responses) pairs. We present our approach thatworks by analyzing data in the conditional (input--output) relation space,captured by a decomposable probabilistic model. Experimental resultsdemonstrate the ability of our approach in identifying multivariate conditionaloutliers.

Monte-Carlo optimizations for resource allocation problems in stochastic  network systems

  Real-world distributed systems and networks are often unreliable and subjectto random failures of its components. Such a stochastic behavior affectsadversely the complexity of optimization tasks performed routinely upon suchsystems, in particular, various resource allocation tasks. In this work weinvestigate and develop Monte Carlo solutions for a class of two-stageoptimization problems in stochastic networks in which the expected value ofresource allocations before and after stochastic failures needs to beoptimized. The limitation of these problems is that their exact solutions areexponential in the number of unreliable network components: thus, exact methodsdo not scale-up well to large networks often seen in practice. We first provethat Monte Carlo optimization methods can overcome the exponential bottleneckof exact methods. Next we support our theoretical findings on resourceallocation experiments and show a very good scale-up potential of the newmethods to large stochastic networks.

A Clustering Approach to Solving Large Stochastic Matching Problems

  In this work we focus on efficient heuristics for solving a class ofstochastic planning problems that arise in a variety of business, investment,and industrial applications. The problem is best described in terms of futurebuy and sell contracts. By buying less reliable, but less expensive, buy(supply) contracts, a company or a trader can cover a position of more reliableand more expensive sell contracts. The goal is to maximize the expected netgain (profit) by constructing a dose to optimum portfolio out of the availablebuy and sell contracts. This stochastic planning problem can be formulated as atwo-stage stochastic linear programming problem with recourse. However, thisformalization leads to solutions that are exponential in the number of possiblefailure combinations. Thus, this approach is not feasible for large scaleproblems. In this work we investigate heuristic approximation techniquesalleviating the efficiency problem. We primarily focus on the clusteringapproach and devise heuristics for finding clusterings leading to goodapproximations. We illustrate the quality and feasibility of the approachthrough experimental data.

Hierarchical Solution of Markov Decision Processes using Macro-actions

  We investigate the use of temporally abstract actions, or macro-actions, inthe solution of Markov decision processes. Unlike current models that combineboth primitive actions and macro-actions and leave the state space unchanged,we propose a hierarchical model (using an abstract MDP) that works withmacro-actions only, and that significantly reduces the size of the state space.This is achieved by treating macroactions as local policies that act in certainregions of state space, and by restricting states in the abstract MDP to thoseat the boundaries of regions. The abstract MDP approximates the original andcan be solved more efficiently. We discuss several ways in which macro-actionscan be generated to ensure good solution quality. Finally, we consider ways inwhich macro-actions can be reused to solve multiple, related MDPs; and we showthat this can justify the computational overhead of macro-action generation.

A Mixtures-of-Experts Framework for Multi-Label Classification

  We develop a novel probabilistic approach for multi-label classification thatis based on the mixtures-of-experts architecture combined with recentlyintroduced conditional tree-structured Bayesian networks. Our approach capturesdifferent input-output relations from multi-label data using the efficienttree-structured classifiers, while the mixtures-of-experts architecture aims tocompensate for the tree-structured restrictions and build a more accuratemodel. We develop and present algorithms for learning the model from data andfor performing multi-label predictions on future data instances. Experiments onmultiple benchmark datasets demonstrate that our approach achieves highlycompetitive results and outperforms the existing state-of-the-art multi-labelclassification methods.

Partitioned Linear Programming Approximations for MDPs

  Approximate linear programming (ALP) is an efficient approach to solvinglarge factored Markov decision processes (MDPs). The main idea of the method isto approximate the optimal value function by a set of basis functions andoptimize their weights by linear programming (LP). This paper proposes a newALP approximation. Comparing to the standard ALP formulation, we decompose theconstraint space into a set of low-dimensional spaces. This structure allowsfor solving the new LP efficiently. In particular, the constraints of the LPcan be satisfied in a compact form without an exponential dependence on thetreewidth of ALP constraints. We study both practical and theoretical aspectsof the proposed approach. Moreover, we demonstrate its scale-up potential on anMDP with more than 2^100 states.

Solving Factored MDPs with Continuous and Discrete Variables

  Although many real-world stochastic planning problems are more naturallyformulated by hybrid models with both discrete and continuous variables,current state-of-the-art methods cannot adequately address these problems. Wepresent the first framework that can exploit problem structure for modeling andsolving hybrid problems efficiently. We formulate these problems as hybridMarkov decision processes (MDPs with continuous and discrete state and actionvariables), which we assume can be represented in a factored way using a hybriddynamic Bayesian network (hybrid DBN). This formulation also allows us to applyour methods to collaborative multiagent settings. We present a new linearprogram approximation method that exploits the structure of the hybrid MDP andlets us compute approximate value functions more efficiently. In particular, wedescribe a new factored discretization of continuous variables that avoids theexponential blow-up of traditional approaches. We provide theoretical bounds onthe quality of such an approximation and on its scale-up potential. We supportour theoretical arguments with experiments on a set of control problems with upto 28-dimensional continuous state space and 22-dimensional action space.

Variational Dual-Tree Framework for Large-Scale Transition Matrix  Approximation

  In recent years, non-parametric methods utilizing random walks on graphs havebeen used to solve a wide range of machine learning problems, but in theirsimplest form they do not scale well due to the quadratic complexity. In thispaper, a new dual-tree based variational approach for approximating thetransition matrix and efficiently performing the random walk is proposed. Theapproach exploits a connection between kernel density estimation, mixturemodeling, and random walk on graphs in an optimization of the transition matrixfor the data graph that ties together edge transitions probabilities that aresimilar. Compared to the de facto standard approximation method based onk-nearestneighbors, we demonstrate order of magnitudes speedup withoutsacrificing accuracy for Label Propagation tasks on benchmark data sets insemi-supervised learning.

The Bregman Variational Dual-Tree Framework

  Graph-based methods provide a powerful tool set for many non-parametricframeworks in Machine Learning. In general, the memory and computationalcomplexity of these methods is quadratic in the number of examples in the datawhich makes them quickly infeasible for moderate to large scale datasets. Asignificant effort to find more efficient solutions to the problem has beenmade in the literature. One of the state-of-the-art methods that has beenrecently introduced is the Variational Dual-Tree (VDT) framework. Despite someof its unique features, VDT is currently restricted only to Euclidean spaceswhere the Euclidean distance quantifies the similarity. In this paper, weextend the VDT framework beyond the Euclidean distance to more general Bregmandivergences that include the Euclidean distance as a special case. Byexploiting the properties of the general Bregman divergence, we show how thenew framework can maintain all the pivotal features of the VDT framework andyet significantly improve its performance in non-Euclidean domains. We applythe proposed framework to different text categorization problems anddemonstrate its benefits over the original VDT.

Sparse Linear Dynamical System with Its Application in Multivariate  Clinical Time Series

  Linear Dynamical System (LDS) is an elegant mathematical framework formodeling and learning multivariate time series. However, in general, it isdifficult to set the dimension of its hidden state space. A small number ofhidden states may not be able to model the complexities of a time series, whilea large number of hidden states can lead to overfitting. In this paper, westudy methods that impose an $\ell_1$ regularization on the transition matrixof an LDS model to alleviate the problem of choosing the optimal number ofhidden states. We incorporate a generalized gradient descent method into theMaximum a Posteriori (MAP) framework and use Expectation Maximization (EM) toiteratively achieve sparsity on the transition matrix of an LDS model. We showthat our Sparse Linear Dynamical System (SLDS) improves the predictiveperformance when compared to ordinary LDS on a multivariate clinical timeseries dataset.

Binary Classifier Calibration: Bayesian Non-Parametric Approach

  A set of probabilistic predictions is well calibrated if the events that arepredicted to occur with probability p do in fact occur about p fraction of thetime. Well calibrated predictions are particularly important when machinelearning models are used in decision analysis. This paper presents two newnon-parametric methods for calibrating outputs of binary classification models:a method based on the Bayes optimal selection and a method based on theBayesian model averaging. The advantage of these methods is that they areindependent of the algorithm used to learn a predictive model, and they can beapplied in a post-processing step, after the model is learned. This makes themapplicable to a wide variety of machine learning models and methods. Thesecalibration methods, as well as other methods, are tested on a variety ofdatasets in terms of both discrimination and calibration performance. Theresults show the methods either outperform or are comparable in performance tothe state-of-the-art calibration methods.

Binary Classifier Calibration: Non-parametric approach

  Accurate calibration of probabilistic predictive models learned is criticalfor many practical prediction and decision-making tasks. There are two maincategories of methods for building calibrated classifiers. One approach is todevelop methods for learning probabilistic models that are well-calibrated, abinitio. The other approach is to use some post-processing methods fortransforming the output of a classifier to be well calibrated, as for examplehistogram binning, Platt scaling, and isotonic regression. One advantage of thepost-processing approach is that it can be applied to any existingprobabilistic classification model that was constructed using anymachine-learning method.  In this paper, we first introduce two measures for evaluating how well aclassifier is calibrated. We prove three theorems showing that using a simplehistogram binning post-processing method, it is possible to make a classifierbe well calibrated while retaining its discrimination capability. Also, bycasting the histogram binning method as a density-based non-parametric binaryclassifier, we can extend it using two simple non-parametric density estimationmethods. We demonstrate the performance of the proposed calibration methods onsynthetic and real datasets. Experimental results show that the proposedmethods either outperform or are comparable to existing calibration methods.

MCODE: Multivariate Conditional Outlier Detection

  Outlier detection aims to identify unusual data instances that deviate fromexpected patterns. The outlier detection is particularly challenging whenoutliers are context dependent and when they are defined by unusualcombinations of multiple outcome variable values. In this paper, we develop andstudy a new conditional outlier detection approach for multivariate outcomespaces that works by (1) transforming the conditional detection to the outlierdetection problem in a new (unconditional) space and (2) defining outlierscores by analyzing the data in the new space. Our approach relies on theclassifier chain decomposition of the multi-dimensional classification problemthat lets us transform the output space into a probability vector, oneprobability for each dimension of the output space. Outlier scores applied tothese transformed vectors are then used to detect the outliers. Experiments onmultiple multi-dimensional classification problems with the different outlierinjection rates show that our methodology is robust and able to successfullyidentify outliers when outliers are either sparse (manifested in one or veryfew dimensions) or dense (affecting multiple dimensions).

Active Perceptual Similarity Modeling with Auxiliary Information

  Learning a model of perceptual similarity from a collection of objects is afundamental task in machine learning underlying numerous applications. A commonway to learn such a model is from relative comparisons in the form of triplets:responses to queries of the form "Is object a more similar to b than it is toc?". If no consideration is made in the determination of which queries to ask,existing similarity learning methods can require a prohibitively large numberof responses. In this work, we consider the problem of actively learning fromtriplets -finding which queries are most useful for learning. Different fromprevious active triplet learning approaches, we incorporate auxiliaryinformation into our similarity model and introduce an active learning schemeto find queries that are informative for quickly learning both the relevantaspects of auxiliary data and the directly-learned similarity components.Compared to prior approaches, we show that we can learn just as effectivelywith much fewer queries. For evaluation, we introduce a new dataset ofexhaustive triplet comparisons obtained from humans and demonstrate improvedperformance for different types of auxiliary information.

Detecting Unusual Input-Output Associations in Multivariate Conditional  Data

  Despite tremendous progress in outlier detection research in recent years,the majority of existing methods are designed only to detect unconditionaloutliers that correspond to unusual data patterns expressed in the joint spaceof all data attributes. Such methods are not applicable when we seek to detectconditional outliers that reflect unusual responses associated with a givencontext or condition. This work focuses on multivariate conditional outlierdetection, a special type of the conditional outlier detection problem, wheredata instances consist of multi-dimensional input (context) and output(responses) pairs. We present a novel outlier detection framework thatidentifies abnormal input-output associations in data with the help of adecomposable conditional probabilistic model that is learned from all datainstances. Since components of this model can vary in their quality, we combinethem with the help of weights reflecting their reliability in assessment ofoutliers. We study two ways of calculating the component weights: global thatrelies on all data, and local that relies only on instances similar to thetarget instance. Experimental results on data from various domains demonstratethe ability of our framework to successfully identify multivariate conditionaloutliers.

Relative Comparison Kernel Learning with Auxiliary Kernels

  In this work we consider the problem of learning a positive semidefinitekernel matrix from relative comparisons of the form: "object A is more similarto object B than it is to C", where comparisons are given by humans. Existingsolutions to this problem assume many comparisons are provided to learn a highquality kernel. However, this can be considered unrealistic for many real-worldtasks since relative assessments require human input, which is often costly ordifficult to obtain. Because of this, only a limited number of thesecomparisons may be provided. In this work, we explore methods for aiding theprocess of learning a kernel with the help of auxiliary kernels built from moreeasily extractable information regarding the relationships among objects. Wepropose a new kernel learning approach in which the target kernel is defined asa conic combination of auxiliary kernels and a kernel whose elements arelearned directly. We formulate a convex optimization to solve for this targetkernel that adds only minor overhead to methods that use no auxiliaryinformation. Empirical results show that in the presence of few trainingrelative comparisons, our method can learn kernels that generalize to moreout-of-sample comparisons than methods that do not utilize auxiliaryinformation, as well as similar methods that learn metrics over objects.

Efficient Online Relative Comparison Kernel Learning

  Learning a kernel matrix from relative comparison human feedback is animportant problem with applications in collaborative filtering, objectretrieval, and search. For learning a kernel over a large number of objects,existing methods face significant scalability issues inhibiting the applicationof these methods to settings where a kernel is learned in an online and timelyfashion. In this paper we propose a novel framework called Efficient onlineRelative comparison Kernel LEarning (ERKLE), for efficiently learning thesimilarity of a large set of objects in an online manner. We learn a kernelfrom relative comparisons via stochastic gradient descent, one query responseat a time, by taking advantage of the sparse and low-rank properties of thegradient to efficiently restrict the kernel to lie in the space of positivesemidefinite matrices. In addition, we derive a passive-aggressive onlineupdate for minimally satisfying new relative comparisons as to not disrupt theinfluence of previously obtained comparisons. Experimentally, we demonstrate aconsiderable improvement in speed while obtaining improved or comparableaccuracy compared to current methods in the online learning setting.

Sparse Multidimensional Patient Modeling using Auxiliary Confidence  Labels

  In this work, we focus on the problem of learning a classification model thatperforms inference on patient Electronic Health Records (EHRs). Often, a largeamount of costly expert supervision is required to learn such a model. Toreduce this cost, we obtain confidence labels that indicate how sure an expertis in the class labels she provides. If meaningful confidence information canbe incorporated into a learning method, fewer patient instances may need to belabeled to learn an accurate model. In addition, while accuracy of predictionsis important for any inference model, a model of patients must be interpretableso that clinicians can understand how the model is making decisions. To theseends, we develop a novel metric learning method called Confidence bAsed MEtricLearning (CAMEL) that supports inclusion of confidence labels, but alsoemphasizes interpretability in three ways. First, our method induces sparsity,thus producing simple models that use only a few features from patient EHRs.Second, CAMEL naturally produces confidence scores that can be taken intoconsideration when clinicians make treatment decisions. Third, the metricslearned by CAMEL induce multidimensional spaces where each dimension representsa different "factor" that clinicians can use to assess patients. In ourexperimental evaluation, we show on a real-world clinical data set that ourCAMEL methods are able to learn models that are as or more accurate as othermethods that use the same supervision. Furthermore, we show that when CAMELuses confidence scores it is able to learn models as or more accurate as otherswe tested while using only 10% of the training instances. Finally, we performqualitative assessments on the metrics learned by CAMEL and show that theyidentify and clearly articulate important factors in how the model performsinference.

