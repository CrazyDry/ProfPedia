Detection of Abnormal Input-Output Associations

  We study a novel outlier detection problem that aims to identify abnormal
input-output associations in data, whose instances consist of multi-dimensional
input (context) and output (responses) pairs. We present our approach that
works by analyzing data in the conditional (input--output) relation space,
captured by a decomposable probabilistic model. Experimental results
demonstrate the ability of our approach in identifying multivariate conditional
outliers.


Monte-Carlo optimizations for resource allocation problems in stochastic
  network systems

  Real-world distributed systems and networks are often unreliable and subject
to random failures of its components. Such a stochastic behavior affects
adversely the complexity of optimization tasks performed routinely upon such
systems, in particular, various resource allocation tasks. In this work we
investigate and develop Monte Carlo solutions for a class of two-stage
optimization problems in stochastic networks in which the expected value of
resource allocations before and after stochastic failures needs to be
optimized. The limitation of these problems is that their exact solutions are
exponential in the number of unreliable network components: thus, exact methods
do not scale-up well to large networks often seen in practice. We first prove
that Monte Carlo optimization methods can overcome the exponential bottleneck
of exact methods. Next we support our theoretical findings on resource
allocation experiments and show a very good scale-up potential of the new
methods to large stochastic networks.


A Clustering Approach to Solving Large Stochastic Matching Problems

  In this work we focus on efficient heuristics for solving a class of
stochastic planning problems that arise in a variety of business, investment,
and industrial applications. The problem is best described in terms of future
buy and sell contracts. By buying less reliable, but less expensive, buy
(supply) contracts, a company or a trader can cover a position of more reliable
and more expensive sell contracts. The goal is to maximize the expected net
gain (profit) by constructing a dose to optimum portfolio out of the available
buy and sell contracts. This stochastic planning problem can be formulated as a
two-stage stochastic linear programming problem with recourse. However, this
formalization leads to solutions that are exponential in the number of possible
failure combinations. Thus, this approach is not feasible for large scale
problems. In this work we investigate heuristic approximation techniques
alleviating the efficiency problem. We primarily focus on the clustering
approach and devise heuristics for finding clusterings leading to good
approximations. We illustrate the quality and feasibility of the approach
through experimental data.


Hierarchical Solution of Markov Decision Processes using Macro-actions

  We investigate the use of temporally abstract actions, or macro-actions, in
the solution of Markov decision processes. Unlike current models that combine
both primitive actions and macro-actions and leave the state space unchanged,
we propose a hierarchical model (using an abstract MDP) that works with
macro-actions only, and that significantly reduces the size of the state space.
This is achieved by treating macroactions as local policies that act in certain
regions of state space, and by restricting states in the abstract MDP to those
at the boundaries of regions. The abstract MDP approximates the original and
can be solved more efficiently. We discuss several ways in which macro-actions
can be generated to ensure good solution quality. Finally, we consider ways in
which macro-actions can be reused to solve multiple, related MDPs; and we show
that this can justify the computational overhead of macro-action generation.


A Mixtures-of-Experts Framework for Multi-Label Classification

  We develop a novel probabilistic approach for multi-label classification that
is based on the mixtures-of-experts architecture combined with recently
introduced conditional tree-structured Bayesian networks. Our approach captures
different input-output relations from multi-label data using the efficient
tree-structured classifiers, while the mixtures-of-experts architecture aims to
compensate for the tree-structured restrictions and build a more accurate
model. We develop and present algorithms for learning the model from data and
for performing multi-label predictions on future data instances. Experiments on
multiple benchmark datasets demonstrate that our approach achieves highly
competitive results and outperforms the existing state-of-the-art multi-label
classification methods.


Variational Dual-Tree Framework for Large-Scale Transition Matrix
  Approximation

  In recent years, non-parametric methods utilizing random walks on graphs have
been used to solve a wide range of machine learning problems, but in their
simplest form they do not scale well due to the quadratic complexity. In this
paper, a new dual-tree based variational approach for approximating the
transition matrix and efficiently performing the random walk is proposed. The
approach exploits a connection between kernel density estimation, mixture
modeling, and random walk on graphs in an optimization of the transition matrix
for the data graph that ties together edge transitions probabilities that are
similar. Compared to the de facto standard approximation method based on
k-nearestneighbors, we demonstrate order of magnitudes speedup without
sacrificing accuracy for Label Propagation tasks on benchmark data sets in
semi-supervised learning.


Solving Factored MDPs with Continuous and Discrete Variables

  Although many real-world stochastic planning problems are more naturally
formulated by hybrid models with both discrete and continuous variables,
current state-of-the-art methods cannot adequately address these problems. We
present the first framework that can exploit problem structure for modeling and
solving hybrid problems efficiently. We formulate these problems as hybrid
Markov decision processes (MDPs with continuous and discrete state and action
variables), which we assume can be represented in a factored way using a hybrid
dynamic Bayesian network (hybrid DBN). This formulation also allows us to apply
our methods to collaborative multiagent settings. We present a new linear
program approximation method that exploits the structure of the hybrid MDP and
lets us compute approximate value functions more efficiently. In particular, we
describe a new factored discretization of continuous variables that avoids the
exponential blow-up of traditional approaches. We provide theoretical bounds on
the quality of such an approximation and on its scale-up potential. We support
our theoretical arguments with experiments on a set of control problems with up
to 28-dimensional continuous state space and 22-dimensional action space.


Binary Classifier Calibration: Bayesian Non-Parametric Approach

  A set of probabilistic predictions is well calibrated if the events that are
predicted to occur with probability p do in fact occur about p fraction of the
time. Well calibrated predictions are particularly important when machine
learning models are used in decision analysis. This paper presents two new
non-parametric methods for calibrating outputs of binary classification models:
a method based on the Bayes optimal selection and a method based on the
Bayesian model averaging. The advantage of these methods is that they are
independent of the algorithm used to learn a predictive model, and they can be
applied in a post-processing step, after the model is learned. This makes them
applicable to a wide variety of machine learning models and methods. These
calibration methods, as well as other methods, are tested on a variety of
datasets in terms of both discrimination and calibration performance. The
results show the methods either outperform or are comparable in performance to
the state-of-the-art calibration methods.


Binary Classifier Calibration: Non-parametric approach

  Accurate calibration of probabilistic predictive models learned is critical
for many practical prediction and decision-making tasks. There are two main
categories of methods for building calibrated classifiers. One approach is to
develop methods for learning probabilistic models that are well-calibrated, ab
initio. The other approach is to use some post-processing methods for
transforming the output of a classifier to be well calibrated, as for example
histogram binning, Platt scaling, and isotonic regression. One advantage of the
post-processing approach is that it can be applied to any existing
probabilistic classification model that was constructed using any
machine-learning method.
  In this paper, we first introduce two measures for evaluating how well a
classifier is calibrated. We prove three theorems showing that using a simple
histogram binning post-processing method, it is possible to make a classifier
be well calibrated while retaining its discrimination capability. Also, by
casting the histogram binning method as a density-based non-parametric binary
classifier, we can extend it using two simple non-parametric density estimation
methods. We demonstrate the performance of the proposed calibration methods on
synthetic and real datasets. Experimental results show that the proposed
methods either outperform or are comparable to existing calibration methods.


MCODE: Multivariate Conditional Outlier Detection

  Outlier detection aims to identify unusual data instances that deviate from
expected patterns. The outlier detection is particularly challenging when
outliers are context dependent and when they are defined by unusual
combinations of multiple outcome variable values. In this paper, we develop and
study a new conditional outlier detection approach for multivariate outcome
spaces that works by (1) transforming the conditional detection to the outlier
detection problem in a new (unconditional) space and (2) defining outlier
scores by analyzing the data in the new space. Our approach relies on the
classifier chain decomposition of the multi-dimensional classification problem
that lets us transform the output space into a probability vector, one
probability for each dimension of the output space. Outlier scores applied to
these transformed vectors are then used to detect the outliers. Experiments on
multiple multi-dimensional classification problems with the different outlier
injection rates show that our methodology is robust and able to successfully
identify outliers when outliers are either sparse (manifested in one or very
few dimensions) or dense (affecting multiple dimensions).


Sparse Linear Dynamical System with Its Application in Multivariate
  Clinical Time Series

  Linear Dynamical System (LDS) is an elegant mathematical framework for
modeling and learning multivariate time series. However, in general, it is
difficult to set the dimension of its hidden state space. A small number of
hidden states may not be able to model the complexities of a time series, while
a large number of hidden states can lead to overfitting. In this paper, we
study methods that impose an $\ell_1$ regularization on the transition matrix
of an LDS model to alleviate the problem of choosing the optimal number of
hidden states. We incorporate a generalized gradient descent method into the
Maximum a Posteriori (MAP) framework and use Expectation Maximization (EM) to
iteratively achieve sparsity on the transition matrix of an LDS model. We show
that our Sparse Linear Dynamical System (SLDS) improves the predictive
performance when compared to ordinary LDS on a multivariate clinical time
series dataset.


Partitioned Linear Programming Approximations for MDPs

  Approximate linear programming (ALP) is an efficient approach to solving
large factored Markov decision processes (MDPs). The main idea of the method is
to approximate the optimal value function by a set of basis functions and
optimize their weights by linear programming (LP). This paper proposes a new
ALP approximation. Comparing to the standard ALP formulation, we decompose the
constraint space into a set of low-dimensional spaces. This structure allows
for solving the new LP efficiently. In particular, the constraints of the LP
can be satisfied in a compact form without an exponential dependence on the
treewidth of ALP constraints. We study both practical and theoretical aspects
of the proposed approach. Moreover, we demonstrate its scale-up potential on an
MDP with more than 2^100 states.


Active Perceptual Similarity Modeling with Auxiliary Information

  Learning a model of perceptual similarity from a collection of objects is a
fundamental task in machine learning underlying numerous applications. A common
way to learn such a model is from relative comparisons in the form of triplets:
responses to queries of the form "Is object a more similar to b than it is to
c?". If no consideration is made in the determination of which queries to ask,
existing similarity learning methods can require a prohibitively large number
of responses. In this work, we consider the problem of actively learning from
triplets -finding which queries are most useful for learning. Different from
previous active triplet learning approaches, we incorporate auxiliary
information into our similarity model and introduce an active learning scheme
to find queries that are informative for quickly learning both the relevant
aspects of auxiliary data and the directly-learned similarity components.
Compared to prior approaches, we show that we can learn just as effectively
with much fewer queries. For evaluation, we introduce a new dataset of
exhaustive triplet comparisons obtained from humans and demonstrate improved
performance for different types of auxiliary information.


The Bregman Variational Dual-Tree Framework

  Graph-based methods provide a powerful tool set for many non-parametric
frameworks in Machine Learning. In general, the memory and computational
complexity of these methods is quadratic in the number of examples in the data
which makes them quickly infeasible for moderate to large scale datasets. A
significant effort to find more efficient solutions to the problem has been
made in the literature. One of the state-of-the-art methods that has been
recently introduced is the Variational Dual-Tree (VDT) framework. Despite some
of its unique features, VDT is currently restricted only to Euclidean spaces
where the Euclidean distance quantifies the similarity. In this paper, we
extend the VDT framework beyond the Euclidean distance to more general Bregman
divergences that include the Euclidean distance as a special case. By
exploiting the properties of the general Bregman divergence, we show how the
new framework can maintain all the pivotal features of the VDT framework and
yet significantly improve its performance in non-Euclidean domains. We apply
the proposed framework to different text categorization problems and
demonstrate its benefits over the original VDT.


Detecting Unusual Input-Output Associations in Multivariate Conditional
  Data

  Despite tremendous progress in outlier detection research in recent years,
the majority of existing methods are designed only to detect unconditional
outliers that correspond to unusual data patterns expressed in the joint space
of all data attributes. Such methods are not applicable when we seek to detect
conditional outliers that reflect unusual responses associated with a given
context or condition. This work focuses on multivariate conditional outlier
detection, a special type of the conditional outlier detection problem, where
data instances consist of multi-dimensional input (context) and output
(responses) pairs. We present a novel outlier detection framework that
identifies abnormal input-output associations in data with the help of a
decomposable conditional probabilistic model that is learned from all data
instances. Since components of this model can vary in their quality, we combine
them with the help of weights reflecting their reliability in assessment of
outliers. We study two ways of calculating the component weights: global that
relies on all data, and local that relies only on instances similar to the
target instance. Experimental results on data from various domains demonstrate
the ability of our framework to successfully identify multivariate conditional
outliers.


Relative Comparison Kernel Learning with Auxiliary Kernels

  In this work we consider the problem of learning a positive semidefinite
kernel matrix from relative comparisons of the form: "object A is more similar
to object B than it is to C", where comparisons are given by humans. Existing
solutions to this problem assume many comparisons are provided to learn a high
quality kernel. However, this can be considered unrealistic for many real-world
tasks since relative assessments require human input, which is often costly or
difficult to obtain. Because of this, only a limited number of these
comparisons may be provided. In this work, we explore methods for aiding the
process of learning a kernel with the help of auxiliary kernels built from more
easily extractable information regarding the relationships among objects. We
propose a new kernel learning approach in which the target kernel is defined as
a conic combination of auxiliary kernels and a kernel whose elements are
learned directly. We formulate a convex optimization to solve for this target
kernel that adds only minor overhead to methods that use no auxiliary
information. Empirical results show that in the presence of few training
relative comparisons, our method can learn kernels that generalize to more
out-of-sample comparisons than methods that do not utilize auxiliary
information, as well as similar methods that learn metrics over objects.


Efficient Online Relative Comparison Kernel Learning

  Learning a kernel matrix from relative comparison human feedback is an
important problem with applications in collaborative filtering, object
retrieval, and search. For learning a kernel over a large number of objects,
existing methods face significant scalability issues inhibiting the application
of these methods to settings where a kernel is learned in an online and timely
fashion. In this paper we propose a novel framework called Efficient online
Relative comparison Kernel LEarning (ERKLE), for efficiently learning the
similarity of a large set of objects in an online manner. We learn a kernel
from relative comparisons via stochastic gradient descent, one query response
at a time, by taking advantage of the sparse and low-rank properties of the
gradient to efficiently restrict the kernel to lie in the space of positive
semidefinite matrices. In addition, we derive a passive-aggressive online
update for minimally satisfying new relative comparisons as to not disrupt the
influence of previously obtained comparisons. Experimentally, we demonstrate a
considerable improvement in speed while obtaining improved or comparable
accuracy compared to current methods in the online learning setting.


Sparse Multidimensional Patient Modeling using Auxiliary Confidence
  Labels

  In this work, we focus on the problem of learning a classification model that
performs inference on patient Electronic Health Records (EHRs). Often, a large
amount of costly expert supervision is required to learn such a model. To
reduce this cost, we obtain confidence labels that indicate how sure an expert
is in the class labels she provides. If meaningful confidence information can
be incorporated into a learning method, fewer patient instances may need to be
labeled to learn an accurate model. In addition, while accuracy of predictions
is important for any inference model, a model of patients must be interpretable
so that clinicians can understand how the model is making decisions. To these
ends, we develop a novel metric learning method called Confidence bAsed MEtric
Learning (CAMEL) that supports inclusion of confidence labels, but also
emphasizes interpretability in three ways. First, our method induces sparsity,
thus producing simple models that use only a few features from patient EHRs.
Second, CAMEL naturally produces confidence scores that can be taken into
consideration when clinicians make treatment decisions. Third, the metrics
learned by CAMEL induce multidimensional spaces where each dimension represents
a different "factor" that clinicians can use to assess patients. In our
experimental evaluation, we show on a real-world clinical data set that our
CAMEL methods are able to learn models that are as or more accurate as other
methods that use the same supervision. Furthermore, we show that when CAMEL
uses confidence scores it is able to learn models as or more accurate as others
we tested while using only 10% of the training instances. Finally, we perform
qualitative assessments on the metrics learned by CAMEL and show that they
identify and clearly articulate important factors in how the model performs
inference.


