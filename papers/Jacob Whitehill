Understanding ACT-R - an Outsider's Perspective

  The ACT-R theory of cognition developed by John Anderson and colleagues
endeavors to explain how humans recall chunks of information and how they solve
problems. ACT-R also serves as a theoretical basis for "cognitive tutors",
i.e., automatic tutoring systems that help students learn mathematics, computer
programming, and other subjects. The official ACT-R definition is distributed
across a large body of literature spanning many articles and monographs, and
hence it is difficult for an "outsider" to learn the most important aspects of
the theory. This paper aims to provide a tutorial to the core components of the
ACT-R theory.


Discriminately Decreasing Discriminability with Learned Image Filters

  In machine learning and computer vision, input images are often filtered to
increase data discriminability. In some situations, however, one may wish to
purposely decrease discriminability of one classification task (a "distractor"
task), while simultaneously preserving information relevant to another (the
task-of-interest): For example, it may be important to mask the identity of
persons contained in face images before submitting them to a crowdsourcing site
(e.g., Mechanical Turk) when labeling them for certain facial attributes.
Another example is inter-dataset generalization: when training on a dataset
with a particular covariance structure among multiple attributes, it may be
useful to suppress one attribute while preserving another so that a trained
classifier does not learn spurious correlations between attributes. In this
paper we present an algorithm that finds optimal filters to give high
discriminability to one task while simultaneously giving low discriminability
to a distractor task. We present results showing the effectiveness of the
proposed technique on both simulated data and natural face images.


A Crowdsourcing Approach To Collecting Tutorial Videos -- Toward
  Personalized Learning-at-Scale

  We investigated the feasibility of crowdsourcing full-fledged tutorial videos
from ordinary people on the Web on how to solve math problems related to
logarithms. This kind of approach (a form of learnersourcing) to efficiently
collecting tutorial videos and other learning resources could be useful for
realizing personalized learning-at-scale, whereby students receive specific
learning resources -- drawn from a large and diverse set -- that are tailored
to their individual and time-varying needs. Results of our study, in which we
collected 399 videos from 66 unique "teachers" on Mechanical Turk, suggest that
(1) approximately 100 videos -- over $80\%$ of which are mathematically fully
correct -- can be crowdsourced per week for \$5/video; (2) the crowdsourced
videos exhibit significant diversity in terms of language style, presentation
media, and pedagogical approach; (3) the average learning gains (posttest minus
pretest score) associated with watching the videos was stat.~sig.~higher than
for a control video ($0.105$ versus $0.045$); and (4) the average learning
gains ($0.1416$) from watching the best tested crowdsourced videos was
comparable to the learning gains ($0.1506$) from watching a popular Khan
Academy video on logarithms.


Exploiting an Oracle that Reports AUC Scores in Machine Learning
  Contests

  In machine learning contests such as the ImageNet Large Scale Visual
Recognition Challenge and the KDD Cup, contestants can submit candidate
solutions and receive from an oracle (typically the organizers of the
competition) the accuracy of their guesses compared to the ground-truth labels.
One of the most commonly used accuracy metrics for binary classification tasks
is the Area Under the Receiver Operating Characteristics Curve (AUC). In this
paper we provide proofs-of-concept of how knowledge of the AUC of a set of
guesses can be used, in two different kinds of attacks, to improve the accuracy
of those guesses. On the other hand, we also demonstrate the intractability of
one kind of AUC exploit by proving that the number of possible binary labelings
of $n$ examples for which a candidate solution obtains a AUC score of $c$ grows
exponentially in $n$, for every $c\in (0,1)$.


Delving Deeper into MOOC Student Dropout Prediction

  In order to obtain reliable accuracy estimates for automatic MOOC dropout
predictors, it is important to train and test them in a manner consistent with
how they will be used in practice. Yet most prior research on MOOC dropout
prediction has measured test accuracy on the same course used for training the
classifier, which can lead to overly optimistic accuracy estimates. In order to
understand better how accuracy is affected by the training+testing regime, we
compared the accuracy of a standard dropout prediction architecture
(clickstream features + logistic regression) across 4 different training
paradigms. Results suggest that (1) training and testing on the same course
("post-hoc") can overestimate accuracy by several percentage points; (2)
dropout classifiers trained on proxy labels based on students' persistence are
surprisingly competitive with post-hoc training (87.33% versus 90.20% AUC
averaged over 8 weeks of 40 HarvardX MOOCs); and (3) classifier performance
does not vary significantly with the academic discipline. Finally, we also
research new dropout prediction architectures based on deep, fully-connected,
feed-forward neural networks and find that (4) networks with as many as 5
hidden layers can statistically significantly increase test accuracy over that
of logistic regression.


Climbing the Kaggle Leaderboard by Exploiting the Log-Loss Oracle

  In the context of data-mining competitions (e.g., Kaggle, KDDCup, ILSVRC
Challenge), we show how access to an oracle that reports a contestant's
log-loss score on the test set can be exploited to deduce the ground-truth of
some of the test examples. By applying this technique iteratively to batches of
$m$ examples (for small $m$), all of the test labels can eventually be
inferred. In this paper, (1) We demonstrate this attack on the first stage of a
recent Kaggle competition (Intel & MobileODT Cancer Screening) and use it to
achieve a log-loss of $0.00000$ (and thus attain a rank of #4 out of 848
contestants), without ever training a classifier to solve the actual task. (2)
We prove an upper bound on the batch size $m$ as a function of the
floating-point resolution of the probability estimates that the contestant
submits for the labels. (3) We derive, and demonstrate in simulation, a more
flexible attack that can be used even when the oracle reports the accuracy on
an unknown (but fixed) subset of the test set's labels. These results underline
the importance of evaluating contestants based only on test data that the
oracle does not examine.


How Does Knowledge of the AUC Constrain the Set of Possible Ground-truth
  Labelings?

  Recent work on privacy-preserving machine learning has considered how
data-mining competitions such as Kaggle could potentially be "hacked", either
intentionally or inadvertently, by using information from an oracle that
reports a classifier's accuracy on the test set. For binary classification
tasks in particular, one of the most common accuracy metrics is the Area Under
the ROC Curve (AUC), and in this paper we explore the mathematical structure of
how the AUC is computed from an n-vector of real-valued "guesses" with respect
to the ground-truth labels. We show how knowledge of a classifier's AUC on the
test set can constrain the set of possible ground-truth labelings, and we
derive an algorithm both to compute the exact number of such labelings and to
enumerate efficiently over them. Finally, we provide empirical evidence that,
surprisingly, the number of compatible labelings can actually decrease as n
grows, until a test set-dependent threshold is reached.


Automatic Classifiers as Scientific Instruments: One Step Further Away
  from Ground-Truth

  Automatic detectors of facial expression, gesture, affect, etc., can serve as
scientific instruments to measure many behavioral and social phenomena (e.g.,
emotion, empathy, stress, engagement, etc.), and this has great potential to
advance basic science. However, when a detector $d$ is trained to approximate
an existing measurement tool (e.g., observation protocol, questionnaire), then
care must be taken when interpreting measurements collected using $d$ since
they are one step further removed from the underlying construct. We examine how
the accuracy of $d$, as quantified by the correlation $q$ of $d$'s outputs with
the ground-truth construct $U$, impacts the estimated correlation between $U$
(e.g., stress) and some other phenomenon $V$ (e.g., academic performance). In
particular: (1) We show that if the true correlation between $U$ and $V$ is
$r$, then the expected sample correlation, over all vectors $\mathcal{T}^n$
whose correlation with $U$ is $q$, is $qr$. (2) We derive a formula to compute
the probability that the sample correlation (over $n$ subjects) using $d$ is
positive, given that the true correlation between $U$ and $V$ is negative (and
vice-versa). We show that this probability is non-negligible (around $10-15\%$)
for values of $n$ and $q$ that have been used in recent affective computing
studies. (3) With the goal to reduce the variance of correlations estimated by
an automatic detector, we show empirically that training multiple neural
networks $d^{(1)},\ldots,d^{(m)}$ using different training configurations
(e.g., architectures, hyperparameters) for the same detection task provides
only limited `coverage' of $\mathcal{T}^n$.


