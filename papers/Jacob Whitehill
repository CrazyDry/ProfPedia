Understanding ACT-R - an Outsider's Perspective

  The ACT-R theory of cognition developed by John Anderson and colleaguesendeavors to explain how humans recall chunks of information and how they solveproblems. ACT-R also serves as a theoretical basis for "cognitive tutors",i.e., automatic tutoring systems that help students learn mathematics, computerprogramming, and other subjects. The official ACT-R definition is distributedacross a large body of literature spanning many articles and monographs, andhence it is difficult for an "outsider" to learn the most important aspects ofthe theory. This paper aims to provide a tutorial to the core components of theACT-R theory.

Discriminately Decreasing Discriminability with Learned Image Filters

  In machine learning and computer vision, input images are often filtered toincrease data discriminability. In some situations, however, one may wish topurposely decrease discriminability of one classification task (a "distractor"task), while simultaneously preserving information relevant to another (thetask-of-interest): For example, it may be important to mask the identity ofpersons contained in face images before submitting them to a crowdsourcing site(e.g., Mechanical Turk) when labeling them for certain facial attributes.Another example is inter-dataset generalization: when training on a datasetwith a particular covariance structure among multiple attributes, it may beuseful to suppress one attribute while preserving another so that a trainedclassifier does not learn spurious correlations between attributes. In thispaper we present an algorithm that finds optimal filters to give highdiscriminability to one task while simultaneously giving low discriminabilityto a distractor task. We present results showing the effectiveness of theproposed technique on both simulated data and natural face images.

Exploiting an Oracle that Reports AUC Scores in Machine Learning  Contests

  In machine learning contests such as the ImageNet Large Scale VisualRecognition Challenge and the KDD Cup, contestants can submit candidatesolutions and receive from an oracle (typically the organizers of thecompetition) the accuracy of their guesses compared to the ground-truth labels.One of the most commonly used accuracy metrics for binary classification tasksis the Area Under the Receiver Operating Characteristics Curve (AUC). In thispaper we provide proofs-of-concept of how knowledge of the AUC of a set ofguesses can be used, in two different kinds of attacks, to improve the accuracyof those guesses. On the other hand, we also demonstrate the intractability ofone kind of AUC exploit by proving that the number of possible binary labelingsof $n$ examples for which a candidate solution obtains a AUC score of $c$ growsexponentially in $n$, for every $c\in (0,1)$.

A Crowdsourcing Approach To Collecting Tutorial Videos -- Toward  Personalized Learning-at-Scale

  We investigated the feasibility of crowdsourcing full-fledged tutorial videosfrom ordinary people on the Web on how to solve math problems related tologarithms. This kind of approach (a form of learnersourcing) to efficientlycollecting tutorial videos and other learning resources could be useful forrealizing personalized learning-at-scale, whereby students receive specificlearning resources -- drawn from a large and diverse set -- that are tailoredto their individual and time-varying needs. Results of our study, in which wecollected 399 videos from 66 unique "teachers" on Mechanical Turk, suggest that(1) approximately 100 videos -- over $80\%$ of which are mathematically fullycorrect -- can be crowdsourced per week for \$5/video; (2) the crowdsourcedvideos exhibit significant diversity in terms of language style, presentationmedia, and pedagogical approach; (3) the average learning gains (posttest minuspretest score) associated with watching the videos was stat.~sig.~higher thanfor a control video ($0.105$ versus $0.045$); and (4) the average learninggains ($0.1416$) from watching the best tested crowdsourced videos wascomparable to the learning gains ($0.1506$) from watching a popular KhanAcademy video on logarithms.

Delving Deeper into MOOC Student Dropout Prediction

  In order to obtain reliable accuracy estimates for automatic MOOC dropoutpredictors, it is important to train and test them in a manner consistent withhow they will be used in practice. Yet most prior research on MOOC dropoutprediction has measured test accuracy on the same course used for training theclassifier, which can lead to overly optimistic accuracy estimates. In order tounderstand better how accuracy is affected by the training+testing regime, wecompared the accuracy of a standard dropout prediction architecture(clickstream features + logistic regression) across 4 different trainingparadigms. Results suggest that (1) training and testing on the same course("post-hoc") can overestimate accuracy by several percentage points; (2)dropout classifiers trained on proxy labels based on students' persistence aresurprisingly competitive with post-hoc training (87.33% versus 90.20% AUCaveraged over 8 weeks of 40 HarvardX MOOCs); and (3) classifier performancedoes not vary significantly with the academic discipline. Finally, we alsoresearch new dropout prediction architectures based on deep, fully-connected,feed-forward neural networks and find that (4) networks with as many as 5hidden layers can statistically significantly increase test accuracy over thatof logistic regression.

Climbing the Kaggle Leaderboard by Exploiting the Log-Loss Oracle

  In the context of data-mining competitions (e.g., Kaggle, KDDCup, ILSVRCChallenge), we show how access to an oracle that reports a contestant'slog-loss score on the test set can be exploited to deduce the ground-truth ofsome of the test examples. By applying this technique iteratively to batches of$m$ examples (for small $m$), all of the test labels can eventually beinferred. In this paper, (1) We demonstrate this attack on the first stage of arecent Kaggle competition (Intel & MobileODT Cancer Screening) and use it toachieve a log-loss of $0.00000$ (and thus attain a rank of #4 out of 848contestants), without ever training a classifier to solve the actual task. (2)We prove an upper bound on the batch size $m$ as a function of thefloating-point resolution of the probability estimates that the contestantsubmits for the labels. (3) We derive, and demonstrate in simulation, a moreflexible attack that can be used even when the oracle reports the accuracy onan unknown (but fixed) subset of the test set's labels. These results underlinethe importance of evaluating contestants based only on test data that theoracle does not examine.

How Does Knowledge of the AUC Constrain the Set of Possible Ground-truth  Labelings?

  Recent work on privacy-preserving machine learning has considered howdata-mining competitions such as Kaggle could potentially be "hacked", eitherintentionally or inadvertently, by using information from an oracle thatreports a classifier's accuracy on the test set. For binary classificationtasks in particular, one of the most common accuracy metrics is the Area Underthe ROC Curve (AUC), and in this paper we explore the mathematical structure ofhow the AUC is computed from an n-vector of real-valued "guesses" with respectto the ground-truth labels. We show how knowledge of a classifier's AUC on thetest set can constrain the set of possible ground-truth labelings, and wederive an algorithm both to compute the exact number of such labelings and toenumerate efficiently over them. Finally, we provide empirical evidence that,surprisingly, the number of compatible labelings can actually decrease as ngrows, until a test set-dependent threshold is reached.

Automatic Classifiers as Scientific Instruments: One Step Further Away  from Ground-Truth

  Automatic detectors of facial expression, gesture, affect, etc., can serve asscientific instruments to measure many behavioral and social phenomena (e.g.,emotion, empathy, stress, engagement, etc.), and this has great potential toadvance basic science. However, when a detector $d$ is trained to approximatean existing measurement tool (e.g., observation protocol, questionnaire), thencare must be taken when interpreting measurements collected using $d$ sincethey are one step further removed from the underlying construct. We examine howthe accuracy of $d$, as quantified by the correlation $q$ of $d$'s outputs withthe ground-truth construct $U$, impacts the estimated correlation between $U$(e.g., stress) and some other phenomenon $V$ (e.g., academic performance). Inparticular: (1) We show that if the true correlation between $U$ and $V$ is$r$, then the expected sample correlation, over all vectors $\mathcal{T}^n$whose correlation with $U$ is $q$, is $qr$. (2) We derive a formula to computethe probability that the sample correlation (over $n$ subjects) using $d$ ispositive, given that the true correlation between $U$ and $V$ is negative (andvice-versa). We show that this probability is non-negligible (around $10-15\%$)for values of $n$ and $q$ that have been used in recent affective computingstudies. (3) With the goal to reduce the variance of correlations estimated byan automatic detector, we show empirically that training multiple neuralnetworks $d^{(1)},\ldots,d^{(m)}$ using different training configurations(e.g., architectures, hyperparameters) for the same detection task providesonly limited `coverage' of $\mathcal{T}^n$.

