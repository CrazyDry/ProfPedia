Graphene Based Waveguide Polarizers: In-Depth Physical Analysis and  Relevant Parameters

  Optical polarizing devices exploiting graphene embedded in waveguides havebeen demonstrated in the literature recently and both the TE- and TM-passbehaviors were reported. The determination of the passing polarization isusually attributed to graphene's Fermi level (and, therefore, doping level),with, however, no direct confirmation of this assumption provided. Here weshow, through numerical simulation, that rather than graphene's Fermi level,the passing polarization is determined by waveguide parameters, such as thesuperstrate refractive index and the waveguide's height. The results provide aconsistent explanation for experimental results reported in the literature. Inaddition, we show that with an accurate graphene modeling, a waveguide cannotbe switched between TE pass and TM pass via Fermi level tuning. Therefore, theusually overlooked contribution of the waveguide design is shown to beessential for the development of optimized TE- or TM-pass polarizers, which weshow to be due to the control it provides on the fraction of the electric fieldthat is tangential to graphene.

Iterated Regret Minimization: A More Realistic Solution Concept

  For some well-known games, such as the Traveler's Dilemma or the CentipedeGame, traditional game-theoretic solution concepts--and most notably Nashequilibrium--predict outcomes that are not consistent with empiricalobservations. In this paper, we introduce a new solution concept, iteratedregret minimization, which exhibits the same qualitative behavior as thatobserved in experiments in many games of interest, including Traveler'sDilemma, the Centipede Game, Nash bargaining, and Bertrand competition. As thename suggests, iterated regret minimization involves the iterated deletion ofstrategies that do not minimize regret.

On Varieties of Lines on Linear Sections of Grassmannians

  General linear sections of codimension 2 of the Grassmannians G(1,4) andG(1,5) appear in the classification of Fano manifolds of high index. UnlikeGrassmannians, these manifolds are not homogeneous. Nevertheless, theirautomorphisms groups have finitely many orbits. In this work we first computethe orbits of these actions. Then we give a description of the variety of lines(under the Pl\"ucker embedding) passing through a fixed point in each orbit ofthe action. As an application we show that these Fano manifolds are not weakly2-Fano, completing the classification of weakly 2-Fano manifolds of high index,initiated by Carolina Araujo and Ana-Maria Castravet.

A mountain pass theorem for minimal hypersurfaces with fixed boundary

  In this work, we prove the existence of a third embedded minimal hypersurfacespanning a closed submanifold $\gamma$ contained in the boundary of a compactRiemannian manifold with convex boundary, when it is known a priori theexistence of two strictly stable minimal hypersurfaces that bound $\gamma$. Inorder to do so, we develop min-max methods similar to those of De Lellis andRamic, references in the paper, adapted to the discrete setting of Almgren andPitts.

On free boundary minimal surfaces in the Riemannian Schwarzschild  manifold

  Is it possible to obtain unbounded minimal surfaces in certain asymptoticallyflat 3-manifolds as a limit of solutions to a natural mountain pass problemwith diverging boundaries? In this work, we give evidence that this might betrue by analyzing related aspects in the case of the exact RiemannianSchwarzschild manifold.  More precisely, we observe that the simplest minimal surface in this spacehas Morse index one. We prove also a relationship between the length of theboundary and the density at infinity of general minimal surfaces satisfying afree-boundary condition along the horizon.

Arago (1810): the first experimental result against the ether

  95 years before Special Relativity was born, Arago attempted to detect theabsolute motion of the Earth by measuring the deflection of starlight passingthrough a prism fixed to the Earth. The null result of this experiment gaverise to the Fresnel's hypothesis of an ether partly dragged by a movingsubstance. In the context of Einstein's Relativity, the sole frame which isprivileged in Arago's experiment is the proper frame of the prism, and the nullresult only says that Snell's law is valid in that frame. We revisit thehistory of this premature first evidence against the ether theory and calculatethe Fresnel's dragging coefficient by applying the Huygens' construction in theframe of the prism. We expose the dissimilar treatment received by the ray andthe wave front as an unavoidable consequence of the classical notions of spaceand time.

Classification of symmetric periodic trajectories in ellipsoidal  billiards

  We classify nonsingular symmetric periodic trajectories (SPTs) of billiardsinside ellipsoids of R^{n+1} without any symmetry of revolution. SPTs aredefined as periodic trajectories passing through some symmetry set. We provethat there are exactly 2^{2n}(2^{n+1}-1) classes of such trajectories. We haveimplemented an algorithm to find minimal SPTs of each of the 12 classes in the2D case (R^2) and each of the 112 classes in the 3D case (R^3). They haveperiods 3, 4 or 6 in the 2D case; and 4, 5, 6, 8 or 10 in the 3D case. Wedisplay a selection of 3D minimal SPTs. Some of them have properties thatcannot take place in the 2D case.

A Fast Incremental Gaussian Mixture Model

  This work builds upon previous efforts in online incremental learning, namelythe Incremental Gaussian Mixture Network (IGMN). The IGMN is capable oflearning from data streams in a single-pass by improving its model afteranalyzing each data point and discarding it thereafter. Nevertheless, itsuffers from the scalability point-of-view, due to its asymptotic timecomplexity of $\operatorname{O}\bigl(NKD^3\bigr)$ for $N$ data points, $K$Gaussian components and $D$ dimensions, rendering it inadequate forhigh-dimensional data. In this paper, we manage to reduce this complexity to$\operatorname{O}\bigl(NKD^2\bigr)$ by deriving formulas for working directlywith precision matrices instead of covariance matrices. The final result is amuch faster and scalable algorithm which can be applied to high dimensionaltasks. This is confirmed by applying the modified algorithm to high-dimensionalclassification datasets.

Magnetic vortex chirality switching in Permalloy nanowires with  asymmetric notches

  We have investigated the motion of vortex domain walls passing across nonsymmetric triangular notches in single Permalloy nanowires. We have measuredhysteresis cycles using the focused magneto-optical Kerr effect before andbeyond the notch, which allowed to probe beyond the notch the occurrenceprobability of clockwise (CW) and counter-clockwise (CCW) walls in tail-to-tail(TT) and head-to-head (HH) configurations. We present experimental evidence ofchirality flipping provided by the vortex -- notch interaction. With a low exitangle the probability of chirality flipping increases and here with the lowestangle of 15$^o$ the probability of propagation of the energetically favoreddomain wall configuration (CCW for TT or CW for HH walls) is $\approx 75\%$.Micromagnetic simulations reveal details of the chirality reversal dynamics.

The Resilience of Life to Astrophysical Events

  Much attention has been given in the literature to the effects ofastrophysical events on human and land-based life. However, little has beendiscussed on the resilience of life itself. Here we instead explore thestatistics of events that completely sterilise an Earth-like planet with planetradii in the range $0.5-1.5 R_{Earth}$ and temperatures of $\sim 300 \;\text{K}$, eradicating all forms of life. We consider the relative likelihoodof complete global sterilisation events from three astrophysical sources --supernovae, gamma-ray bursts, large asteroid impacts, and passing-by stars. Toassess such probabilities we consider what cataclysmic event could lead to theannihilation of not just human life, but also extremophiles, through theboiling of all water in Earth's oceans. Surprisingly we find that althoughhuman life is somewhat fragile to nearby events, the resilience of Ecdysozoasuch as \emph{Milnesium tardigradum} renders global sterilisation an unlikelyevent.

The Swift-Hohenberg Equation under directional-quenching: finding  heteroclinic connections using spatial and spectral decompositions

  We study the existence of patterns (nontrivial, stationary solutions) forone-dimensional Swift-Hohenberg Equation in a directional quenching scenario,that is, on $x\leq 0$ the energy potential associated to the equation isbistable, whereas on $x\geq 0$ it is monostable. This heterogeneity in themedium induces a symmetry break that makes the existence of heteroclinic orbitsof the type point-to-periodic not only plausible but, as we prove here, true.In this search, we use an interesting result of "Fefferman, Charles, J.Lee-Thorp, and Michael Weinstein. Topologically protected states inone-dimensional systems. Vol. 247. No. 1173. American Mathematical Society,2017" in order to understand the multiscale structure of the problem, namely,how fast/slow scales interact with each other. In passing, we advocate for anew approach in finding connecting orbits, using what we call `far/neardecompositions', relying both on information about the spatial behavior of thesolutions and on Fourier analysis. Our method is functional analytic and PDEbased, relying minimally on dynamical system techniques and making no use ofcomparison principles whatsoever.

Voting with Coarse Beliefs

  The classic Gibbard-Satterthwaite theorem says that every strategy-proofvoting rule with at least three possible candidates must be dictatorial.Similar impossibility results hold even if we consider a weaker notion ofstrategy-proofness where voters believe that the other voters' preferences arei.i.d.~(independent and identically distributed). In this paper, we take abounded-rationality approach to this problem and consider a setting wherevoters have "coarse" beliefs (a notion that has gained popularity in thebehavioral economics literature). In particular, we construct good voting rulesthat satisfy a notion of strategy-proofness with respect to coarsei.i.d.~beliefs, thus circumventing the above impossibility results.

Algorithmic Rationality: Game Theory with Costly Computation

  We develop a general game-theoretic framework for reasoning about strategicagents performing possibly costly computation. In this framework, manytraditional game-theoretic results (such as the existence of a Nashequilibrium) no longer hold. Nevertheless, we can use the framework to providepsychologically appealing explanations of observed behavior in well-studiedgames (such as finitely repeated prisoner's dilemma and rock-paper-scissors).Furthermore, we provide natural conditions on games sufficient to guaranteethat equilibria exist.

Bayesian Games with Intentions

  We show that standard Bayesian games cannot represent the full spectrum ofbelief-dependent preferences. However, by introducing a fundamental distinctionbetween intended and actual strategies, we remove this limitation. We defineBayesian games with intentions, generalizing both Bayesian games andpsychological games, and prove that Nash equilibria in psychological gamescorrespond to a special class of equilibria as defined in our setting.

Sequential Equilibrium in Computational Games

  We examine sequential equilibrium in the context of computational games,where agents are charged for computation. In such games, an agent canrationally choose to forget, so issues of imperfect recall arise. In thissetting, we consider two notions of sequential equilibrium. One is an ex antenotion, where a player chooses his strategy before the game starts and iscommitted to it, but chooses it in such a way that it remains optimal even offthe equilibrium path. The second is an interim notion, where a player canreconsider at each information set whether he is doing the "right" thing, andif not, can change his strategy. The two notions agree in games of perfectrecall, but not in games of imperfect recall. Although the interim notion seemsmore appealing, \fullv{Halpern and Pass [2011] argue that there are some deepconceptual problems with it in standard games of imperfect recall. We show thatthe conceptual problems largely disappear in the computational setting.Moreover, in this setting, under natural assumptions, the two notions coincide.

Saturated Overburden Scattering and the Multiscatter Frontier:  Discovering Dark Matter at the Planck Mass and Beyond

  We show that underground experiments like LUX/LZ, PandaX-II, XENON, and PICOcould discover dark matter up to the Planck mass and beyond, with new searchesfor dark matter that scatters multiple times in these detectors. This opens upsignificant discovery potential via re-analysis of existing and future data. Wealso identify a new effect which substantially enhances experimentalsensitivity to large dark matter scattering cross-sections: while passingthrough atmospheric or solid overburden, there is a maximum number of scattersthat dark matter undergoes, determined by the total number of scattering sitesit passes, such as nuclei and electrons. This extends the reach of somepublished limits and future analyses to arbitrarily large dark matterscattering cross-sections.

A DEEP analysis of the META-DES framework for dynamic selection of  ensemble of classifiers

  Dynamic ensemble selection (DES) techniques work by estimating the level ofcompetence of each classifier from a pool of classifiers. Only the mostcompetent ones are selected to classify a given test sample. Hence, the keyissue in DES is the criterion used to estimate the level of competence of theclassifiers in predicting the label of a given test sample. In order to performa more robust ensemble selection, we proposed the META-DES framework usingmeta-learning, where multiple criteria are encoded as meta-features and arepassed down to a meta-classifier that is trained to estimate the competencelevel of a given classifier. In this technical report, we present astep-by-step analysis of each phase of the framework during training and test.We show how each set of meta-features is extracted as well as their impact onthe estimation of the competence level of the base classifier. Moreover, ananalysis of the impact of several factors in the system performance, such asthe number of classifiers in the pool, the use of different linear baseclassifiers, as well as the size of the validation data. We show that using thedynamic selection of linear classifiers through the META-DES framework, we cansolve complex non-linear classification problems where other combinationtechniques such as AdaBoost cannot.

Transition from oscillatory to excitable regime in a system forced at  three times its natural frequency

  The effect of a temporal modulation at three times the critical frequency ona Hopf bifurcation is studied in the framework of amplitude equations. Weconsider a complex Ginzburg-Landau equation with an extra quadratic term,resulting from the strong coupling between the external field and the unstablemodes. We show that, by increasing the intensity of the forcing, one passesfrom an oscillatory regime to an excitable one with three equivalent frequencylocked states. In the oscillatory regime, topological defects are one-armedphase spirals, while in the excitable regime they correspond to three-armedexcitable amplitude spirals. Analytical results show that the transitionbetween these two regimes occurs at a critical value of the forcing intensity.The transition between phase and amplitude spirals is confirmed by numericalanalysis and it might be observed in periodically forced reaction-diffusionsystems.

An Algebraic Analysis of Conchoids to Algebraic Curves

  We study conchoids to algebraic curve from the perspective of algebraicgeometry, analyzing their main algebraic properties. We introduce the formaldefinition of conchoid of an algebraic curve by means of incidence diagrams. Weprove that, with the exception of a circle centered at the focus and taking $d$as its radius, the conchoid is an algebraic curve having at most twoirreducible components. In addition, we introduce the notions of special andsimple components of a conchoid. Moreover we state that, with the exception oflines passing through the focus, the conchoid always has at least one simplecomponent and that, for almost every distance, all the components of theconchoid are simple. We state that, in the reducible case, simple conchoidcomponents are birationally equivalent to the initial curve, and we show howspecial components can be used to decide whether a given algebraic curve is theconchoid of another curve.

Game Theory with Costly Computation

  We develop a general game-theoretic framework for reasoning about strategicagents performing possibly costly computation. In this framework, manytraditional game-theoretic results (such as the existence of a Nashequilibrium) no longer hold. Nevertheless, we can use the framework to providepsychologically appealing explanations to observed behavior in well-studiedgames (such as finitely repeated prisoner's dilemma and rock-paper-scissors).Furthermore, we provide natural conditions on games sufficient to guaranteethat equilibria exist. As an application of this framework, we consider anotion of game-theoretic implementation of mediators in computational games. Weshow that a special case of this notion is equivalent to a variant of thetraditional cryptographic definition of protocol security; this result showsthat, when taking computation into account, the two approaches used for dealingwith "deviating" players in two different communities -- Nash equilibrium ingame theory and zero-knowledge "simulation" in cryptography -- are intimatelyrelated.

A Logical Characterization of Iterated Admissibility

  Brandenburger, Friedenberg, and Keisler provide an epistemic characterizationof iterated admissibility (i.e., iterated deletion of weakly dominatedstrategies) where uncertainty is represented using LPSs (lexicographicprobability sequences). Their characterization holds in a rich structure calleda complete structure, where all types are possible. Here, a logicalcharaacterization of iterated admisibility is given that involves only standardprobability and holds in all structures, not just complete structures. Astronger notion of strong admissibility is then defined. Roughly speaking,strong admissibility is meant to capture the intuition that "all the agentknows" is that the other agents satisfy the appropriate rationalityassumptions. Strong admissibility makes it possible to relate admissibility,canonical structures (as typically considered in completeness proofs in modallogic), complete structures, and the notion of ``all I know''.

Statistically-secure ORAM with $\tilde{O}(\log^2 n)$ Overhead

  We demonstrate a simple, statistically secure, ORAM with computationaloverhead $\tilde{O}(\log^2 n)$; previous ORAM protocols achieve onlycomputational security (under computational assumptions) or require$\tilde{\Omega}(\log^3 n)$ overheard. An additional benefit of our ORAM is itsconceptual simplicity, which makes it easy to implement in both software and(commercially available) hardware.  Our construction is based on recent ORAM constructions due to Shi, Chan,Stefanov, and Li (Asiacrypt 2011) and Stefanov and Shi (ArXiv 2012), but withsome crucial modifications in the algorithm that simplifies the ORAM and enableour analysis. A central component in our analysis is reducing the analysis ofour algorithm to a "supermarket" problem; of independent interest (and ofimportance to our analysis,) we provide an upper bound on the rate of "upset"customers in the "supermarket" problem.

Decision Theory with Resource-Bounded Agents

  There have been two major lines of research aimed at capturingresource-bounded players in game theory. The first, initiated by Rubinstein,charges an agent for doing costly computation; the second, initiated by Neyman,does not charge for computation, but limits the computation that agents can do,typically by modeling agents as finite automata. We review recent work onapplying both approaches in the context of decision theory. For the firstapproach, we take the objects of choice in a decision problem to be Turingmachines, and charge players for the ``complexity'' of the Turing machinechosen (e.g., its running time). This approach can be used to explainwell-known phenomena like first-impression-matters biases (i.e., people tend toput more weight on evidence they hear early on) and belief polarization (twopeople with different prior beliefs, hearing the same evidence, can end up withdiametrically opposed conclusions) as the outcomes of quite rational decisions.For the second approach, we model people as finite automata, and provide asimple algorithm that, on a problem that captures a number of settings ofinterest, provably performs optimally as the number of states in the automatonincreases.

On turning waves for the inhomogeneous Muskat problem: a  computer-assisted proof

  We exhibit a family of graphs that develop turning singularities (i.e. theirLipschitz seminorm blows up and they cease to be a graph, passing from thestable to the unstable regime) for the inhomogeneous, two-phase Muskat problemwhere the permeability is given by a nonnegative step function. We study theinfluence of different choices of the permeability and different boundaryconditions (both at infinity and considering finite/infinite depth) in thedevelopment or prevention of singularities for short time. In the general case(inhomogeneous, confined) we prove a bifurcation diagram concerning theappearance or not of singularities when the depth of the medium and thepermeabilities change. The proofs are carried out using a combination ofclassical analysis techniques and computer-assisted verification.

The Truth Behind the Myth of the Folk Theorem

  We study the problem of computing an $\epsilon$-Nash equilibrium in repeatedgames. Earlier work by Borgs et al. [2010] suggests that this problem isintractable. We show that if we make a slight change to their model---modelingthe players as polynomial-time Turing machines that maintain state ---and makesome standard cryptographic hardness assumptions (the existence of public-keyencryption), the problem can actually be solved in polynomial time. Ouralgorithm works not only for games with a finite number of players, but alsofor constant-degree graphical games.  As Nash equilibrium is a weak solution concept for extensive form games, weadditionally define and study an appropriate notion of a subgame-perfectequilibrium for computationally bounded players, and show how to efficientlyfind such an equilibrium in repeated games (again, making standardcryptographic hardness assumptions).

Computational Extensive-Form Games

  We define solution concepts appropriate for computationally bounded playersplaying a fixed finite game. To do so, we need to define what it means for a\emph{computational game}, which is a sequence of games that get larger in someappropriate sense, to represent a single finite underlying extensive-form game.Roughly speaking, we require all the games in the sequence to have essentiallythe same structure as the underlying game, except that two histories that areindistinguishable (i.e., in the same information set) in the underlying gamemay correspond to histories that are only computationally indistinguishable inthe computational game. We define a computational version of both Nashequilibrium and sequential equilibrium for computational games, and show thatevery Nash (resp., sequential) equilibrium in the underlying game correspondsto a computational Nash (resp., sequential) equilibrium in the computationalgame. One advantage of our approach is that if a cryptographic protocolrepresents an abstract game, then we can analyze its strategic behavior in theabstract game, and thus separate the cryptographic analysis of the protocolfrom the strategic analysis.

Language-based Games

  We introduce language-based games, a generalization of psychological games[6] that can also capture reference-dependent preferences [7]. The idea is toextend the domain of the utility function to situations, maximal consistentsets in some language. The role of the underlying language in this framework isthus particularly critical. Of special interest are languages that can expressonly coarse beliefs [9]. Despite the expressive power of the approach, we showthat it can describe games in a simple, natural way. Nash equilibrium andrationalizability are generalized to this setting; Nash equilibrium is shownnot to exist in general, while the existence of rationalizable strategies isproved under mild conditions.

I Don't Want to Think About it Now:Decision Theory With Costly  Computation

  Computation plays a major role in decision making. Even if an agent iswilling to ascribe a probability to all states and a utility to all outcomes,and maximize expected utility, doing so might present serious computationalproblems. Moreover, computing the outcome of a given act might be difficult. Ina companion paper we develop a framework for game theory with costlycomputation, where the objects of choice are Turing machines. Here we applythat framework to decision theory. We show how well-known phenomena likefirst-impression-matters biases (i.e., people tend to put more weight onevidence they hear early on), belief polarization (two people with differentprior beliefs, hearing the same evidence, can end up with diametrically opposedconclusions), and the status quo bias (people are much more likely to stickwith what they already have) can be easily captured in that framework. Finally,we use the framework to define some new notions: value of computationalinformation (a computational variant of value of information) and andcomputational value of conversation.

Stronger Impossibility Results for Strategy-Proof Voting with i.i.d.  Beliefs

  The classic Gibbard-Satterthwaite theorem says that every strategy-proofvoting rule with at least three possible candidates must be dictatorial. In\cite{McL11}, McLennan showed that a similar impossibility result holds even ifwe consider a weaker notion of strategy-proofness where voters believe that theother voters' preferences are i.i.d.~(independent and identically distributed):If an anonymous voting rule (with at least three candidates) is strategy-proofw.r.t.~all i.i.d.~beliefs and is also Pareto efficient, then the voting rulemust be a random dictatorship. In this paper, we strengthen McLennan's resultby relaxing Pareto efficiency to $\epsilon$-Pareto efficiency where Paretoefficiency can be violated with probability $\epsilon$, and we further relax$\epsilon$-Pareto efficiency to a very weak notion of efficiency which we call$\epsilon$-super-weak unanimity. We then show the following: If an anonymousvoting rule (with at least three candidates) is strategy-proof w.r.t.~alli.i.d.~beliefs and also satisfies $\epsilon$-super-weak unanimity, then thevoting rule must be $O(\epsilon)$-close to random dictatorship.

On the Non-Existence of Nash Equilibrium in Games with Resource-Bounded  Players

  We consider sequences of games $\mathcal{G}=\{G_1,G_2,\ldots\}$ where, forall $n$, $G_n$ has the same set of players. Such sequences arise in theanalysis of running time of players in games, in electronic money systems suchas Bitcoin and in cryptographic protocols. Assuming that one-way functionsexist, we prove that there is a sequence of 2-player zero-sum Bayesian games$\mathcal{G}$ such that, for all $n$, the size of every action in $G_n$ ispolynomial in $n$, the utility function is polynomial computable in $n$, andyet there is no polynomial-time Nash equilibrium, where we use a notion of Nashequilibrium that is tailored to sequences of games. We also demonstrate thatNash equilibrium may not exist when considering players that are constrained toperform at most $T$ computational steps in each of the games$\{G_i\}_{i=1}^{\infty}$. These examples may shed light on competitive settingswhere the availability of more running time or faster algorithms lead to a"computational arms race", precluding the existence of equilibrium. They alsopoint to inherent limitations of concepts such "best response" and Nashequilibrium in games with resource-bounded players.

How to Measure the Quantum Measure

  The histories-based framework of Quantum Measure Theory assigns a generalizedprobability or measure $\mu(E)$ to every (suitably regular) set $E$ ofhistories. Even though $\mu(E)$ cannot in general be interpreted as theexpectation value of a selfadjoint operator (or POVM), we describe anarrangement which makes it possible to determine $\mu(E)$ experimentally forany desired $E$. Taking, for simplicity, the system in question to be aparticle passing through a series of Stern-Gerlach devices or beam-splitters,we show how to couple a set of ancillas to it, and then to perform on them asuitable unitary transformation followed by a final measurement, such that theprobability of a final outcome of "yes" is related to $\mu(E)$ by a knownfactor of proportionality. Finally, we discuss in what sense a positive outcomeof the final measurement should count as a minimally disturbing verificationthat the microscopic event $E$ actually happened.

Socially Optimal Mining Pools

  Mining for Bitcoins is a high-risk high-reward activity. Miners, seeking toreduce their variance and earn steadier rewards, collaborate in poolingstrategies where they jointly mine for Bitcoins. Whenever some pool participantis successful, the earned rewards are appropriately split among all poolparticipants. Currently a dozen of different pooling strategies (i.e., methodsfor distributing the rewards) are in use for Bitcoin mining.  We here propose a formal model of utility and social welfare for Bitcoinmining (and analogous mining systems) based on the theory of discountedexpected utility, and next study pooling strategies that maximize the socialwelfare of miners. Our main result shows that one of the pooling strategiesactually employed in practice--the so-called geometric pay pool--achieves theoptimal steady-state utility for miners when its parameters are setappropriately.  Our results apply not only to Bitcoin mining pools, but any other form ofpooled mining or crowdsourcing computations where the participants engage inrepeated random trials towards a common goal, and where "partial" solutions canbe efficiently verified.

AMIDST: a Java Toolbox for Scalable Probabilistic Machine Learning

  The AMIDST Toolbox is a software for scalable probabilistic machine learningwith a spe- cial focus on (massive) streaming data. The toolbox supports aflexible modeling language based on probabilistic graphical models with latentvariables and temporal dependencies. The specified models can be learnt fromlarge data sets using parallel or distributed implementa- tions of Bayesianlearning algorithms for either streaming or batch data. These algorithms arebased on a flexible variational message passing scheme, which supports discreteand continu- ous variables from a wide range of probability distributions.AMIDST also leverages existing functionality and algorithms by interfacing tosoftware tools such as Flink, Spark, MOA, Weka, R and HUGIN. AMIDST is an opensource toolbox written in Java and available at http://www.amidsttoolbox.comunder the Apache Software License version 2.0.

A Knowledge-Based Analysis of the Blockchain Protocol

  At the heart of the Bitcoin is a blockchain protocol, a protocol forachieving consensus on a public ledger that records bitcoin transactions. Tothe extent that a blockchain protocol is used for applications such as contractsigning and making certain transactions (such as house sales) public, we needto understand what guarantees the protocol gives us in terms of agents'knowledge. Here, we provide a complete characterization of agent's knowledgewhen running a blockchain protocol using a variant of common knowledge thattakes into account the fact that agents can enter and leave the system, it isnot known which agents are in fact following the protocol (some agents may wantto deviate if they can gain by doing so), and the fact that the guaranteesprovided by blockchain protocols are probabilistic. We then consider somescenarios involving contracts and show that this level of knowledge sufficesfor some scenarios, but not others.

Paradoxes in Fair Computer-Aided Decision Making

  Computer-aided decision making--where a human decision-maker is aided by acomputational classifier in making a decision--is becoming increasinglyprevalent. For instance, judges in at least nine states make use of algorithmictools meant to determine "recidivism risk scores" for criminal defendants insentencing, parole, or bail decisions. A subject of much recent debate iswhether such algorithmic tools are "fair" in the sense that they do notdiscriminate against certain groups (e.g., races) of people.  Our main result shows that for "non-trivial" computer-aided decision making,either the classifier must be discriminatory, or a rational decision-makerusing the output of the classifier is forced to be discriminatory. We furtherprovide a complete characterization of situations where fair computer-aideddecision making is possible.

Testing Randomness in Quantum Mechanics

  Pseudo-random number generators are widely used in many branches of science,mainly in applications related to Monte Carlo methods, although they aredeterministic in design and, therefore, unsuitable for tackling fundamentalproblems in security and cryptography. The natural laws of the microscopicrealm provide a fairly simple method to generate non-deterministic sequences ofrandom numbers, based on measurements of quantum states. In practice, however,the experimental devices on which quantum random number generators are basedare often unable to pass some tests of randomness. In this review, we brieflydiscuss two such tests, point out the challenges that we have encountered andfinally present a fairly simple method that successfully generatesnon-deterministic maximally random sequences.

META-DES: A Dynamic Ensemble Selection Framework using Meta-Learning

  Dynamic ensemble selection systems work by estimating the level of competenceof each classifier from a pool of classifiers. Only the most competent ones areselected to classify a given test sample. This is achieved by defining acriterion to measure the level of competence of a base classifier, such as, itsaccuracy in local regions of the feature space around the query instance.However, using only one criterion about the behavior of a base classifier isnot sufficient to accurately estimate its level of competence. In this paper,we present a novel dynamic ensemble selection framework using meta-learning. Wepropose five distinct sets of meta-features, each one corresponding to adifferent criterion to measure the level of competence of a classifier for theclassification of input samples. The meta-features are extracted from thetraining data and used to train a meta-classifier to predict whether or not abase classifier is competent enough to classify an input instance. During thegeneralization phase, the meta-features are extracted from the query instanceand passed down as input to the meta-classifier. The meta-classifier estimates,whether a base classifier is competent enough to be added to the ensemble.Experiments are conducted over several small sample size classificationproblems, i.e., problems with a high degree of uncertainty due to the lack oftraining data. Experimental results show the proposed meta-learning frameworkgreatly improves classification accuracy when compared against currentstate-of-the-art dynamic ensemble selection techniques.

Generalizing circles over algebraic extensions

  This paper deals with a family of spatial rational curves that wereintroduced by Andradas, Recio and Sendra, under the name of hypercircles, as analgorithmic cornerstone tool in the context of improving the rationalparametrization (simplifying the coefficients of the rational functions, whenpossible) of algebraic varieties. A real circle can be defined as the image ofthe real axis under a Moebius transformation in the complex field. Likewise,and roughly speaking, a hypercircle can be defined as the image of a line ("the${\mathbb{K}}$-axis") in a $n$-degree finite algebraic extension$\mathbb{K}(\alpha)\thickapprox\mathbb{K}^n$ under the transformation$\frac{at+b}{ct+d}:\mathbb{K}(\alpha)\to\mathbb{K}(\alpha)$.  The aim of this article is to extend, to the case of hypercircles, some ofthe specific properties of circles. We show that hypercircles are precisely,via $\mathbb{K}$-projective transformations, the rational normal curve of asuitable degree. We also obtain a complete description of the points atinfinity of these curves (generalizing the cyclic structure at infinity ofcircles). We characterize hypercircles as those curves of degree equal to thedimension of the ambient affine space and with infinitely many${\mathbb{K}}$-rational points, passing through these points at infinity.Moreover, we give explicit formulae for the parametrization and implicitationof hypercircles. Besides the intrinsic interest of this very special family ofcurves, the understanding of its properties has a direct application to thesimplification of parametrizations problem, as shown in the last section.

Infrared Emission by Dust Around lambda Bootis Stars: Debris Disks or  Thermally Emitting Nebulae?

  We present a model that describes stellar infrared excesses due to heating ofthe interstellar (IS) dust by a hot star passing through a diffuse IS cloud.This model is applied to six lambda Bootis stars with infrared excesses.Plausible values for the IS medium (ISM) density and relative velocity betweenthe cloud and the star yield fits to the excess emission. This result isconsistent with the diffusion/accretion hypothesis that lambda Bootis stars (A-to F-type stars with large underabundances of Fe-peak elements) owe theircharacteristics to interactions with the ISM. This proposal invokes radiationpressure from the star to repel the IS dust and excavate a paraboloidal dustcavity in the IS cloud, while the metal-poor gas is accreted onto the stellarphotosphere. However, the measurements of the infrared excesses can also be fitby planetary debris disk models. A more detailed consideration of theconditions to produce lambda Bootis characteristics indicates that the majorityof infrared-excess stars within the Local Bubble probably have debris disks.Nevertheless, more distant stars may often have excesses due to heating ofinterstellar material such as in our model.

The transmission spectrum of Earth through lunar eclipse observations

  Of the 342 planets discovered so far orbiting other stars, 58 "transit" thestellar disk, meaning that they can be detected by a periodic decrease in thestarlight flux. The light from the star passes through the atmosphere of theplanet, and in a few cases the basic atmospheric composition of the planet canbe estimated. As we get closer to finding analogues of Earth, an importantconsideration toward the characterization of exoplanetary atmospheres is whatthe transmission spectrum of our planet looks like. Here we report the opticaland near-infrared transmission spectrum of the Earth, obtained during a lunareclipse. Some biologically relevant atmospheric features that are weak in thereflected spectrum (such as ozone, molecular oxygen, water, carbon dioxide andmethane) are much stronger in the transmission spectrum, and indeed strongerthan predicted by modelling. We also find the fingerprints of the Earth'sionosphere and of the major atmospheric constituent, diatomic nitrogen (N2),which are missing in the reflected spectrum.

On the Power of Many One-Bit Provers

  We study the class of languages, denoted by $\MIP[k, 1-\epsilon, s]$, whichhave $k$-prover games where each prover just sends a \emph{single} bit, withcompleteness $1-\epsilon$ and soundness error $s$. For the case that $k=1$(i.e., for the case of interactive proofs), Goldreich, Vadhan and Wigderson({\em Computational Complexity'02}) demonstrate that $\SZK$ exactlycharacterizes languages having 1-bit proof systems with"non-trivial" soundness(i.e., $1/2 < s \leq 1-2\epsilon$). We demonstrate that for the case that$k\geq 2$, 1-bit $k$-prover games exhibit a significantly richer structure:  + (Folklore) When $s \leq \frac{1}{2^k} - \epsilon$, $\MIP[k, 1-\epsilon, s]= \BPP$;  + When $\frac{1}{2^k} + \epsilon \leq s < \frac{2}{2^k}-\epsilon$, $\MIP[k,1-\epsilon, s] = \SZK$;  + When $s \ge \frac{2}{2^k} + \epsilon$, $\AM \subseteq \MIP[k, 1-\epsilon,s]$;  + For $s \le 0.62 k/2^k$ and sufficiently large $k$, $\MIP[k, 1-\epsilon, s]\subseteq \EXP$;  + For $s \ge 2k/2^{k}$, $\MIP[k, 1, 1-\epsilon, s] = \NEXP$.  As such, 1-bit $k$-prover games yield a natural "quantitative" approach torelating complexity classes such as $\BPP$,$\SZK$,$\AM$, $\EXP$, and $\NEXP$.We leave open the question of whether a more fine-grained hierarchy (between$\AM$ and $\NEXP$) can be established for the case when $s \geq \frac{2}{2^k} +\epsilon$.

Game Theory with Translucent Players

  A traditional assumption in game theory is that players are opaque to oneanother---if a player changes strategies, then this change in strategies doesnot affect the choice of other players' strategies. In many situations this isan unrealistic assumption. We develop a framework for reasoning about gameswhere the players may be translucent to one another; in particular, a playermay believe that if she were to change strategies, then the other player wouldalso change strategies. Translucent players may achieve significantly moreefficient outcomes than opaque ones.  Our main result is a characterization of strategies consistent withappropriate analogues of common belief of rationality. Common CounterfactualBelief of Rationality (CCBR) holds if (1) everyone is rational, (2) everyonecounterfactually believes that everyone else is rational (i.e., all players ibelieve that everyone else would still be rational even if $i$ were to switchstrategies), (3) everyone counterfactually believes that everyone else isrational, and counterfactually believes that everyone else is rational, and soon. CCBR characterizes the set of strategies surviving iterated removal ofminimax dominated strategies, where a strategy s for player i is minimaxdominated by s' if the worst-case payoff for i using s' is better than the bestpossible payoff using s.

Game Theory with Translucent Players

  A traditional assumption in game theory is that players are opaque to oneanother -- if a player changes strategies, then this change in strategies doesnot affect the choice of other players' strategies. In many situations this isan unrealistic assumption. We develop a framework for reasoning about gameswhere the players may be translucent to one another; in particular, a playermay believe that if she were to change strategies, then the other player wouldalso change strategies. Translucent players may achieve significantly moreefficient outcomes than opaque ones.  Our main result is a characterization of strategies consistent withappropriate analogues of common belief of rationality. Common CounterfactualBelief of Rationality (CCBR) holds if (1) everyone is rational, (2) everyonecounterfactually believes that everyone else is rational (i.e., all players ibelieve that everyone else would still be rational even if i were to switchstrategies), (3) everyone counterfactually believes that everyone else isrational, and counterfactually believes that everyone else is rational, and soon. CCBR characterizes the set of strategies surviving iterated removal ofminimax dominated strategies: a strategy $\sigma_i$ is minimax dominated for iif there exists a strategy $\sigma'_i$ for i such that $\min_{\mu'_{-i}}u_i(\sigma_i, \mu_{-i}') > \max_{\mu_{-i}} u_i(\sigma_i, \mu_{-i})$.

D-Oscillons in the Standard Model-Extension

  In this work we investigate the consequences of the Lorentz symmetryviolation on extremely long-living, time-dependent, and spatially localizedfield configurations, named oscillons. This is accomplished in ($D+1$)dimensions for two interacting scalar field theories in the so-called StandardModel-Extension context. We show that $D$-dimensional scalar field lumps canpresent a typical size $R_{\min }\ll R_{KK}$, where $R_{KK}$ is the associatedlength scale of extra dimensions in Kaluza-Klein theories. Here, the size$R_{\min }$ is shown to strongly depend on the terms that control the Lorentzviolation of the theory. This implies either contraction or dilation of theaverage radius $R_{\min}$, and a new rule for its composition, likewise.Moreover, we show that the spatial dimensions for existence of oscillatinglumps have an upper limit, opening new possibilities to probe the existence ofa $D$ -dimensional oscillons at TeV energy scale. Moreover, in a cosmologicalscenario with Lorentz symmetry breaking, we argue that in the early Universewith an extremely high energy density and a strong Lorentz violation, thetypical size $R_{\min }$ was highly dilated. With the expansion and subsequentcooling of the Universe, we propose that it passed through a phase transitiontowards a Lorentz symmetry, wherein $R_{\min }$ tends to be compact.

Thermodynamic Properties of the van der Waals Fluid

  The van der Waals (vdW) theory of fluids is the first and simplest theorythat takes into account interactions between the particles of a system thatresult in a phase transition versus temperature. Combined with Maxwell'sconstruction, this mean-field theory predicts the conditions for equilibriumcoexistence between the gas and liquid phases and the first-order transitionbetween them. However, important properties of the vdW fluid have not beensystematically investigated. Here we report a comprehensive study of theseproperties. Ambiguities about the physical interpretation of the Boyletemperature and the influence of the vdW molecular interactions on the pressureof the vdW gas are resolved. Thermodynamic variables and properties areformulated in reduced units that allow all properties to be expressed as lawsof corresponding states that apply to all vdW fluids. Lekner's parametricsolution for the vdW gas-liquid coexistence curve in the pressure-temperatureplane and related thermodynamic properties [Am. J. Phys. 50, 161 (1982)] isexplained and significantly extended. Hysteresis in the first-order transitiontemperature on heating and cooling is examined and the maximum degrees ofsuperheating and supercooling determined. The latent heat of vaporization andthe entropy change on crossing the coexistence curve are investigated. Thetemperature dependences of the isothermal compressibility, thermal expansioncoefficient and heat capacity at constant pressure for a range of pressuresabove, at and below the critical pressure are systematically studied fromnumerical calculations including their critical behaviors and theirdiscontinuities on crossing the coexistence curve. Joule-Thomson expansion ofthe vdW gas is investigated in detail and the pressure and temperatureconditions for liquifying a vdW gas on passing through the throttle aredetermined.

A WISE Census of Young Stellar Objects in Perseus OB2 Association

  We have performed a WISE (Wide-Field Infrared Survey Explorer) based study toidentify and characterize young stellar objects (YSOs) in 12x12 degree PerseusOB2 association. Spectral energy distribution (SED) slope in range of 3.4-12micron and a 5sigma selection criteria were used to select our initial sample.Further manual inspection reduced our final catalog to 156 known and 119 YSOcandidate. The spatial distribution of newly found YSOs all over the fieldshows an older generation of star formation which most of its massive membershave evolved into main sequence stars. In contrast, the majority of youngermembers lie within the Perseus molecular cloud and currently active starforming clusters such as NGC1333 and IC348. We also identified additional 66point sources which passed YSO selection criteria but are likely AGB stars.However their spatial distribution suggests that they may contain a fraction ofthe YSOs. Comparing our results with the commonly used color-color selections,we found that while color selection method fails in picking up bright butevolved weak disks, our SED fitting method can identify such sources, includingtransitional disks. In addition we have less contamination with backgroundsources such as galaxies, but in a price of loosing fainter (Jmag > 12) YSOs.Finally we employed a Bayesian Monte Carlo SED fitting method to determine thecharacteristics of each YSO candidate. Distribution of SED slopes and modeldriven age and mass confirms separated YSO populations with suggested three agegroups of younger than 1 Myr old, 1-5 Myr old, and older than 5 Myrs whichagrees with the age of Per OB2 association and currently star forming siteswithin the cloud.

Communication Complexity of Byzantine Agreement, Revisited

  With decentralized cryptocurrencies, an increasingly important problem is howto design communication-efficient Byzantine Agreement (BA) protocols: in thispaper, communication efficiency means that the number of pairwise messagesnecessary for reaching agreement is subquadratic in the total number of players$n$ (or alternatively, only sublinear number of nodes need to multicastmessages to all other nodes). A few existing works have shown how to achievesubquadratic BA under an {\it adaptive} adversary, including the breakthroughresult by King and Saia (PODC'10), the celebrated Nakamoto consensus protocol,and a few proof-of-stake protocols such as Algorand.  Intriguingly, all these subquadratic protocols coincidentally make a commonrelaxation about the adaptivity of the attacker, that is, if an honest nodesends a message $\msg$ in some round $r$, the attacker may adaptively corruptthe node and make the now-corrupt node send additional messages in the sameround, but it {\it cannot erase the message $\msg$ that was already sent} ---henceforth we say that such an adversary cannot perform ``after-the-factremoval''. By contrast, most natural (super-)quadratic BA protocols in theliterature can be proven secure under a strongly adaptive adversary capable ofafter-the-fact removal. Besides the above relaxation, all known subquadraticprotocols make additional strong assumptions such as plain or proof-of-workrandom oracles and/or the ability of honest nodes to erase secrets from memory(henceforth called the ``memory-erasure'' model). In this paper, we first provethat in fact, disallowing after-the-fact removal is necessary for achievingsubquadratic-communication BA. Moreover, this lower bound holds in a verystrong sense, even when allowing setup assumptions such as PKI, random oracles,or memory-erasures.

The Fifteenth Data Release of the Sloan Digital Sky Surveys: First  Release of MaNGA Derived Quantities, Data Visualization Tools and Stellar  Library

  Twenty years have passed since first light for the Sloan Digital Sky Survey(SDSS). Here, we release data taken by the fourth phase of SDSS (SDSS-IV)across its first three years of operation (July 2014-July 2017). This is thethird data release for SDSS-IV, and the fifteenth from SDSS (Data ReleaseFifteen; DR15). New data come from MaNGA - we release 4824 datacubes, as wellas the first stellar spectra in the MaNGA Stellar Library (MaStar), the firstset of survey-supported analysis products (e.g. stellar and gas kinematics,emission line, and other maps) from the MaNGA Data Analysis Pipeline (DAP), anda new data visualisation and access tool we call "Marvin". The next datarelease, DR16, will include new data from both APOGEE-2 and eBOSS; thosesurveys release no new data here, but we document updates and corrections totheir data processing pipelines. The release is cumulative; it also includesthe most recent reductions and calibrations of all data taken by SDSS sincefirst light. In this paper we describe the location and format of the data andtools and cite technical references describing how it was obtained andprocessed. The SDSS website (www.sdss.org) has also been updated, providinglinks to data downloads, tutorials and examples of data use. While SDSS-IV willcontinue to collect astronomical data until 2020, and will be followed bySDSS-V (2020-2025), we end this paper by describing plans to ensure thesustainability of the SDSS data archive for many years beyond the collection ofdata.

