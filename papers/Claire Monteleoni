Convergence rate of stochastic k-means

  We analyze online and mini-batch k-means variants. Both scale up the widely
used Lloyd 's algorithm via stochastic approximation, and have become popular
for large-scale clustering and unsupervised feature learning. We show, for the
first time, that they have global convergence towards local optima at
$O(\frac{1}{t})$ rate under general conditions. In addition, we show if the
dataset is clusterable, with suitable initialization, mini-batch k-means
converges to an optimal k-means solution with $O(\frac{1}{t})$ convergence rate
with high probability. The k-means objective is non-convex and
non-differentiable: we exploit ideas from non-convex gradient-based
optimization by providing a novel characterization of the trajectory of k-means
algorithm on its solution space, and circumvent its non-differentiability via
geometric insights about k-means update.


Convergence rate of stochastic k-means

  We analyze online \cite{BottouBengio} and mini-batch \cite{Sculley} $k$-means
variants. Both scale up the widely used $k$-means algorithm via stochastic
approximation, and have become popular for large-scale clustering and
unsupervised feature learning. We show, for the first time, that starting with
any initial solution, they converge to a "local optimum" at rate
$O(\frac{1}{t})$ (in terms of the $k$-means objective) under general
conditions. In addition, we show if the dataset is clusterable, when
initialized with a simple and scalable seeding algorithm, mini-batch $k$-means
converges to an optimal $k$-means solution at rate $O(\frac{1}{t})$ with high
probability. The $k$-means objective is non-convex and non-differentiable: we
exploit ideas from recent work on stochastic gradient descent for non-convex
problems \cite{ge:sgd_tensor, balsubramani13} by providing a novel
characterization of the trajectory of $k$-means algorithm on its solution
space, and circumvent the non-differentiability problem via geometric insights
about $k$-means update.


Cooperative Online Learning: Keeping your Neighbors Updated

  We study an asynchronous online learning setting with a network of agents. At
each time step, some of the agents are activated, requested to make a
prediction, and pay the corresponding loss. The loss function is then revealed
to these agents and also to their neighbors in the network. When activations
are stochastic, we show that the regret achieved by $N$ agents running the
standard online Mirror Descent is $O(\sqrt{\alpha T})$, where $T$ is the
horizon and $\alpha \le N$ is the independence number of the network. This is
in contrast to the regret $\Omega(\sqrt{N T})$ which $N$ agents incur in the
same setting when feedback is not shared. We also show a matching lower bound
of order $\sqrt{\alpha T}$ that holds for any given network. When the pattern
of agent activations is arbitrary, the problem changes significantly: we prove
a $\Omega(T)$ lower bound on the regret that holds for any online algorithm
oblivious to the feedback source.


Differentially Private Empirical Risk Minimization

  Privacy-preserving machine learning algorithms are crucial for the
increasingly common setting in which personal data, such as medical or
financial records, are analyzed. We provide general techniques to produce
privacy-preserving approximations of classifiers learned via (regularized)
empirical risk minimization (ERM). These algorithms are private under the
$\epsilon$-differential privacy definition due to Dwork et al. (2006). First we
apply the output perturbation ideas of Dwork et al. (2006), to ERM
classification. Then we propose a new method, objective perturbation, for
privacy-preserving machine learning algorithm design. This method entails
perturbing the objective function before optimizing over classifiers. If the
loss and regularizer satisfy certain convexity and differentiability criteria,
we prove theoretical results showing that our algorithms preserve privacy, and
provide generalization bounds for linear and nonlinear kernels. We further
present a privacy-preserving technique for tuning the parameters in general
machine learning algorithms, thereby providing end-to-end privacy guarantees
for the training process. We apply these results to produce privacy-preserving
analogues of regularized logistic regression and support vector machines. We
obtain encouraging results from evaluating their performance on real
demographic and benchmark data sets. Our results show that both theoretically
and empirically, objective perturbation is superior to the previous
state-of-the-art, output perturbation, in managing the inherent tradeoff
between privacy and learning performance.


