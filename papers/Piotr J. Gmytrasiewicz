Monte Carlo Sampling Methods for Approximating Interactive POMDPs

  Partially observable Markov decision processes (POMDPs) provide a principledframework for sequential planning in uncertain single agent settings. Anextension of POMDPs to multiagent settings, called interactive POMDPs(I-POMDPs), replaces POMDP belief spaces with interactive hierarchical beliefsystems which represent an agent's belief about the physical world, aboutbeliefs of other agents, and about their beliefs about others' beliefs. Thismodification makes the difficulties of obtaining solutions due to complexity ofthe belief and policy spaces even more acute. We describe a general method forobtaining approximate solutions of I-POMDPs based on particle filtering (PF).We introduce the interactive PF, which descends the levels of the interactivebelief hierarchies and samples and propagates beliefs at each level. Theinteractive PF is able to mitigate the belief space complexity, but it does notaddress the policy space complexity. To mitigate the policy space complexity --sometimes also called the curse of history -- we utilize a complementary methodbased on sampling likely observations while building the look aheadreachability tree. While this approach does not completely address the curse ofhistory, it beats back the curse's impact substantially. We provideexperimental results and chart future work.

