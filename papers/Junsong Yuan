Depth-Based 3D Hand Pose Estimation: From Current Achievements to Future  Goals

  In this paper, we strive to answer two questions: What is the current stateof 3D hand pose estimation from depth images? And, what are the next challengesthat need to be tackled? Following the successful Hands In the MillionChallenge (HIM2017), we investigate the top 10 state-of-the-art methods onthree tasks: single frame 3D pose estimation, 3D hand tracking, and hand poseestimation during object interaction. We analyze the performance of differentCNN structures with regard to hand shape, joint visibility, view point andarticulation distributions. Our findings include: (1) isolated 3D hand poseestimation achieves low mean errors (10 mm) in the view point range of [70,120] degrees, but it is far from being solved for extreme view points; (2) 3Dvolumetric representations outperform 2D CNNs, better capturing the spatialstructure of the depth data; (3) Discriminative methods still generalize poorlyto unseen hand shapes; (4) While joint occlusions pose a challenge for mostmethods, explicit modeling of structure constraints can significantly narrowthe gap between errors on visible and occluded joints.

Robust 3D Hand Pose Estimation in Single Depth Images: from Single-View  CNN to Multi-View CNNs

  Articulated hand pose estimation plays an important role in human-computerinteraction. Despite the recent progress, the accuracy of existing methods isstill not satisfactory, partially due to the difficulty of embeddedhigh-dimensional and non-linear regression problem. Different from the existingdiscriminative methods that regress for the hand pose with a single depthimage, we propose to first project the query depth image onto three orthogonalplanes and utilize these multi-view projections to regress for 2D heat-mapswhich estimate the joint positions on each plane. These multi-view heat-mapsare then fused to produce final 3D hand pose estimation with learned posepriors. Experiments show that the proposed method largely outperformsstate-of-the-art on a challenging dataset. Moreover, a cross-dataset experimentalso demonstrates the good generalization ability of the proposed method.

Influence Maximization Meets Efficiency and Effectiveness: A Hop-Based  Approach

  Influence Maximization is an extensively-studied problem that targets atselecting a set of initial seed nodes in the Online Social Networks (OSNs) tospread the influence as widely as possible. However, it remains an openchallenge to design fast and accurate algorithms to find solutions inlarge-scale OSNs. Prior Monte-Carlo-simulation-based methods are slow and notscalable, while other heuristic algorithms do not have any theoreticalguarantee and they have been shown to produce poor solutions for quite somecases. In this paper, we propose hop-based algorithms that can easily scale tomillions of nodes and billions of edges. Unlike previous heuristics, ourproposed hop-based approaches can provide certain theoretical guarantees.Experimental evaluations with real OSN datasets demonstrate the efficiency andeffectiveness of our algorithms.

Kernel Cross-Correlator

  Cross-correlator plays a significant role in many visual perception tasks,such as object detection and tracking. Beyond the linear cross-correlator, thispaper proposes a kernel cross-correlator (KCC) that breaks traditionallimitations. First, by introducing the kernel trick, the KCC extends the linearcross-correlation to non-linear space, which is more robust to signal noisesand distortions. Second, the connection to the existing works shows that KCCprovides a unified solution for correlation filters. Third, KCC is applicableto any kernel function and is not limited to circulant structure on trainingdata, thus it is able to predict affine transformations with customizedproperties. Last, by leveraging the fast Fourier transform (FFT), KCCeliminates direct calculation of kernel vectors, thus achieves betterperformance yet still with a reasonable computational cost. Comprehensiveexperiments on visual tracking and human activity recognition using wearabledevices demonstrate its robustness, flexibility, and efficiency. The sourcecodes of both experiments are released at https://github.com/wang-chen/KCC

Non-iterative RGB-D-inertial Odometry

  This paper presents a non-iterative solution to RGB-D-inertial odometrysystem. Traditional odometry methods resort to iterative algorithms which areusually computationally expensive or require well-designed initialization. Toovercome this problem, this paper proposes to combine a non-iterative front-end(odometry) with an iterative back-end (loop closure) for the RGB-D-inertialSLAM system. The main contribution lies in the novel non-iterative front-end,which leverages on inertial fusion and kernel cross-correlators (KCC) to matchpoint clouds in frequency domain. Dominated by the fast Fourier transform(FFT), our method is only of complexity $\mathcal{O}(n\log{n})$, where $n$ isthe number of points. Map fusion is conducted by element-wise operations, sothat both time and space complexity are further reduced. Extensive experimentsshow that, due to the lightweight of the proposed front-end, the framework isable to run at a much faster speed yet still with comparable accuracy with thestate-of-the-arts.

Towards Profit Maximization for Online Social Network Providers

  Online Social Networks (OSNs) attract billions of users to share informationand communicate where viral marketing has emerged as a new way to promote thesales of products. An OSN provider is often hired by an advertiser to conductviral marketing campaigns. The OSN provider generates revenue from thecommission paid by the advertiser which is determined by the spread of itsproduct information. Meanwhile, to propagate influence, the activitiesperformed by users such as viewing video ads normally induce diffusion cost tothe OSN provider. In this paper, we aim to find a seed set to optimize a newprofit metric that combines the benefit of influence spread with the cost ofinfluence propagation for the OSN provider. Under many diffusion models, ourprofit metric is the difference between two submodular functions which ischallenging to optimize as it is neither submodular nor monotone. We design ageneral two-phase framework to select seeds for profit maximization and developseveral bounds to measure the quality of the seed set constructed. Experimentalresults with real OSN datasets show that our approach can achieve highapproximation guarantees and significantly outperform the baseline algorithms,including state-of-the-art influence maximization algorithms.

Exploiting Local Feature Patterns for Unsupervised Domain Adaptation

  Unsupervised domain adaptation methods aim to alleviate performancedegradation caused by domain-shift by learning domain-invariantrepresentations. Existing deep domain adaptation methods focus on holisticfeature alignment by matching source and target holistic feature distributions,without considering local features and their multi-mode statistics. We showthat the learned local feature patterns are more generic and transferable and afurther local feature distribution matching enables fine-grained featurealignment. In this paper, we present a method for learning domain-invariantlocal feature patterns and jointly aligning holistic and local featurestatistics. Comparisons to the state-of-the-art unsupervised domain adaptationmethods on two popular benchmark datasets demonstrate the superiority of ourapproach and its effectiveness on alleviating negative transfer.

3D Hand Shape and Pose Estimation from a Single RGB Image

  This work addresses a novel and challenging problem of estimating the full 3Dhand shape and pose from a single RGB image. Most current methods in 3D handanalysis from monocular RGB images only focus on estimating the 3D locations ofhand keypoints, which cannot fully express the 3D shape of hand. In contrast,we propose a Graph Convolutional Neural Network (Graph CNN) based method toreconstruct a full 3D mesh of hand surface that contains richer information ofboth 3D hand shape and pose. To train networks with full supervision, we createa large-scale synthetic dataset containing both ground truth 3D meshes and 3Dposes. When fine-tuning the networks on real-world datasets without 3D groundtruth, we propose a weakly-supervised approach by leveraging the depth map as aweak supervision in training. Through extensive evaluations on our proposed newdatasets and two public datasets, we show that our proposed method can produceaccurate and reasonable 3D hand mesh, and can achieve superior 3D hand poseestimation accuracy when compared with state-of-the-art methods.

Kervolutional Neural Networks

  Convolutional neural networks (CNNs) have enabled the state-of-the-artperformance in many computer vision tasks. However, little effort has beendevoted to establishing convolution in non-linear space. Existing works mainlyleverage on the activation layers, which can only provide point-wisenon-linearity. To solve this problem, a new operation, kervolution (kernelconvolution), is introduced to approximate complex behaviors of humanperception systems leveraging on the kernel trick. It generalizes convolution,enhances the model capacity, and captures higher order interactions offeatures, via patch-wise kernel functions, but without introducing additionalparameters. Extensive experiments show that kervolutional neural networks (KNN)achieve higher accuracy and faster convergence than baseline CNN.

Non-Iterative SLAM

  The goal of this paper is to create a new framework for dense SLAM that islight enough for micro-robot systems based on depth camera and inertial sensor.Feature-based and direct methods are two mainstreams in visual SLAM. Bothmethods minimize photometric or reprojection error by iterative solutions,which are computationally expensive. To overcome this problem, we propose anon-iterative framework to reduce computational requirement. First, theattitude and heading reference system (AHRS) and axonometric projection areutilized to decouple the 6 Degree-of-Freedom (DoF) data, so that point cloudscan be matched in independent spaces respectively. Second, based on singlekey-frame training, the matching process is carried out in frequency domain byFourier transformation, which provides a closed-form non-iterative solution. Inthis manner, the time complexity is reduced to $\mathcal{O}(n \log{n})$, where$n$ is the number of matched points in each frame. To the best of ourknowledge, this method is the first non-iterative and online trainable approachfor data association in visual SLAM. Compared with the state-of-the-arts, itruns at a faster speed and obtains 3-D maps with higher resolution yet stillwith comparable accuracy.

Actor-Action Semantic Segmentation with Region Masks

  In this paper, we study the actor-action semantic segmentation problem, whichrequires joint labeling of both actor and action categories in video frames.One major challenge for this task is that when an actor performs an action,different body parts of the actor provide different types of cues for theaction category and may receive inconsistent action labeling when they arelabeled independently. To address this issue, we propose an end-to-endregion-based actor-action segmentation approach which relies on region masksfrom an instance segmentation algorithm. Our main novelty is to avoid labelingpixels in a region mask independently - instead we assign a single action labelto these pixels to achieve consistent action labeling. When a pixel belongs tomultiple region masks, max pooling is applied to resolve labeling conflicts.Our approach uses a two-stream network as the front-end (which learns featurescapturing both appearance and motion information), and uses two region-basedsegmentation networks as the back-end (which takes the fused features from thetwo-stream network as the input and predicts actor-action labeling).Experiments on the A2D dataset demonstrate that both the region-basedsegmentation strategy and the fused features from the two-stream networkcontribute to the performance improvements. The proposed approach outperformsthe state-of-the-art results by more than 8% in mean class accuracy, and morethan 5% in mean class IOU, which validates its effectiveness.

PointCloud Saliency Maps

  3D point-cloud recognition with PointNet and its variants has receivedremarkable progress. A missing ingredient, however, is the ability toautomatically evaluate point-wise importance w.r.t.\! classificationperformance, which is usually reflected by a saliency map. A saliency map is animportant tool as it allows one to perform further processes on point-clouddata. In this paper, we propose a novel way of characterizing critical pointsand segments to build point-cloud saliency maps. Our method assigns each pointa score reflecting its contribution to the model-recognition loss. The saliencymap explicitly explains which points are the key for model recognition.Furthermore, aggregations of highly-scored points indicate importantsegments/subsets in a point-cloud. Our motivation for constructing a saliencymap is by point dropping, which is a non-differentiable operator. To overcomethis issue, we approximate point-dropping with a differentiable procedure ofshifting points towards the cloud centroid. Consequently, each saliency scorecan be efficiently measured by the corresponding gradient of the loss w.r.t thepoint under the spherical coordinates. Extensive evaluations on severalstate-of-the-art point-cloud recognition models, including PointNet, PointNet++and DGCNN, demonstrate the veracity and generality of our proposed saliencymap. Code for experiments is released on\url{https://github.com/tianzheng4/PointCloud-Saliency-Maps}.

Progress Regression RNN for Online Spatial-Temporal Action Localization  in Unconstrained Videos

  Previous spatial-temporal action localization methods commonly follow thepipeline of object detection to estimate bounding boxes and labels of actions.However, the temporal relation of an action has not been fully explored. Inthis paper, we propose an end-to-end Progress Regression Recurrent NeuralNetwork (PR-RNN) for online spatial-temporal action localization, which learnsto infer the action by temporal progress regression. Two new action attributes,called progression and progress rate, are introduced to describe the temporalengagement and relative temporal position of an action. In our method,frame-level features are first extracted by a Fully Convolutional Network(FCN). Subsequently, detection results and action progress attributes areregressed by the Convolutional Gated Recurrent Unit (ConvGRU) based on all theobserved frames instead of a single frame or a short clip. Finally, a novelonline linking method is designed to connect single-frame results tospatial-temporal tubes with the help of the estimated action progressattributes. Extensive experiments demonstrate that the progress attributesimprove the localization accuracy by providing more precise temporal positionof an action in unconstrained videos. Our proposed PR-RNN achieves thestateof-the-art performance for most of the IoU thresholds on two benchmarkdatasets.

