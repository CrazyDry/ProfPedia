From Black-Scholes to Online Learning: Dynamic Hedging under Adversarial  Environments

  We consider a non-stochastic online learning approach to price financialoptions by modeling the market dynamic as a repeated game between the nature(adversary) and the investor. We demonstrate that such framework yieldsanalogous structure as the Black-Scholes model, the widely popular optionpricing model in stochastic finance, for both European and American optionswith convex payoffs. In the case of non-convex options, we constructapproximate pricing algorithms, and demonstrate that their efficiency can beanalyzed through the introduction of an artificial probability measure, inparallel to the so-called risk-neutral measure in the finance literature, eventhough our framework is completely adversarial. Continuous-time convergenceresults and extensions to incorporate price jumps are also presented.

Stock Market Prediction from WSJ: Text Mining via Sparse Matrix  Factorization

  We revisit the problem of predicting directional movements of stock pricesbased on news articles: here our algorithm uses daily articles from The WallStreet Journal to predict the closing stock prices on the same day. We proposea unified latent space model to characterize the "co-movements" between stockprices and news articles. Unlike many existing approaches, our new model isable to simultaneously leverage the correlations: (a) among stock prices, (b)among news articles, and (c) between stock prices and news articles. Thus, ourmodel is able to make daily predictions on more than 500 stocks (most of whichare not even mentioned in any news article) while having low complexity. Wecarry out extensive backtesting on trading strategies based on our algorithm.The result shows that our model has substantially better accuracy rate (55.7%)compared to many widely used algorithms. The return (56%) and Sharpe ratio dueto a trading strategy based on our model are also much higher than baselineindices.

The Diffusion of Networking Technologies

  There has been significant interest in the networking community on the impactof cascade effects on the diffusion of networking technology upgrades in theInternet. Thinking of the global Internet as a graph, where each noderepresents an economically-motivated Internet Service Provider (ISP), a keyproblem is to determine the smallest set of nodes that can trigger a cascadethat causes every other node in the graph to adopt the protocol. We design thefirst approximation algorithm with a provable performance guarantee for thisproblem, in a model that captures the following key issue: a node's decision toupgrade should be influenced by the decisions of the remote nodes it wishes tocommunicate with.  Given an internetwork G(V,E) and threshold function \theta, we assume thatnode $u$ activates (upgrades to the new technology) when it is adjacent to aconnected component of active nodes in G of size exceeding node $u$'s threshold\theta(u). Our objective is to choose the smallest set of nodes that can causethe rest of the graph to activate. Our main contribution is an approximationalgorithm based on linear programming, which we complement with computationalhardness results and a near-optimum integrality gap. Our algorithm, which doesnot rely on submodular optimization techniques, also highlights the substantialalgorithmic difference between our problem and similar questions studied in thecontext of social networks.

Towards Non-Parametric Learning to Rank

  This paper studies a stylized, yet natural, learning-to-rank problem andpoints out the critical incorrectness of a widely used nearest neighboralgorithm. We consider a model with $n$ agents (users) $\{x_i\}_{i \in [n]}$and $m$ alternatives (items) $\{y_j\}_{j \in [m]}$, each of which is associatedwith a latent feature vector. Agents rank items nondeterministically accordingto the Plackett-Luce model, where the higher the utility of an item to theagent, the more likely this item will be ranked high by the agent. Our goal isto find neighbors of an arbitrary agent or alternative in the latent space.  We first show that the Kendall-tau distance based kNN produces incorrectresults in our model. Next, we fix the problem by introducing a new algorithmwith features constructed from "global information" of the data matrix. Ourapproach is in sharp contrast to most existing feature engineering methods.Finally, we design another new algorithm identifying similar alternatives. Theconstruction of alternative features can be done using "local information,"highlighting the algorithmic difference between finding similar agents andsimilar alternatives.

Total Variation with Overlapping Group Sparsity and Lp Quasinorm for  Infrared Image Deblurring under Salt-and-Pepper Noise

  Because of the limitations of the infrared imaging principle and theproperties of infrared imaging systems, infrared images have some drawbacks,including a lack of details, indistinct edges, and a large amount ofsalt-andpepper noise. To improve the sparse characteristics of the image whilemaintaining the image edges and weakening staircase artifacts, this paperproposes a method that uses the Lp quasinorm instead of the L1 norm and forinfrared image deblurring with an overlapping group sparse total variationmethod. The Lp quasinorm introduces another degree of freedom, better describesimage sparsity characteristics, and improves image restoration. Furthermore, weadopt the accelerated alternating direction method of multipliers and fastFourier transform theory in the proposed method to improve the efficiency androbustness of our algorithm. Experiments show that under different conditionsfor blur and salt-and-pepper noise, the proposed method leads to excellentperformance in terms of objective evaluation and subjective visual results.

Statistically-secure ORAM with $\tilde{O}(\log^2 n)$ Overhead

  We demonstrate a simple, statistically secure, ORAM with computationaloverhead $\tilde{O}(\log^2 n)$; previous ORAM protocols achieve onlycomputational security (under computational assumptions) or require$\tilde{\Omega}(\log^3 n)$ overheard. An additional benefit of our ORAM is itsconceptual simplicity, which makes it easy to implement in both software and(commercially available) hardware.  Our construction is based on recent ORAM constructions due to Shi, Chan,Stefanov, and Li (Asiacrypt 2011) and Stefanov and Shi (ArXiv 2012), but withsome crucial modifications in the algorithm that simplifies the ORAM and enableour analysis. A central component in our analysis is reducing the analysis ofour algorithm to a "supermarket" problem; of independent interest (and ofimportance to our analysis,) we provide an upper bound on the rate of "upset"customers in the "supermarket" problem.

Mind Your Own Bandwidth: An Edge Solution to Peak-hour Broadband  Congestion

  Motivated by recent increases in network traffic, we propose a decentralizednetwork edge-based solution to peak-hour broadband congestion that incentivizesusers to moderate their bandwidth demands to their actual needs. Our solutionis centered on smart home gateways that allocate bandwidth in a two-levelhierarchy: first, a gateway purchases guaranteed bandwidth from the InternetService Provider (ISP) with virtual credits. It then self-limits its bandwidthusage and distributes the bandwidth among its apps and devices according totheir relative priorities. To this end, we design a credit allocation andredistribution mechanism for the first level, and implement our gateways oncommodity wireless routers for the second level. We demonstrate our system'seffectiveness and practicality with theoretical analysis, simulations andexperiments on real traffic. Compared to a baseline equal sharing algorithm,our solution significantly improves users' overall satisfaction and yields afair allocation of bandwidth across users.

From which world is your graph?

  Discovering statistical structure from links is a fundamental problem in theanalysis of social networks. Choosing a misspecified model, or equivalently, anincorrect inference algorithm will result in an invalid analysis or evenfalsely uncover patterns that are in fact artifacts of the model. This workfocuses on unifying two of the most widely used link-formation models: thestochastic blockmodel (SBM) and the small world (or latent space) model (SWM).Integrating techniques from kernel learning, spectral graph theory, andnonlinear dimensionality reduction, we develop the first statistically soundpolynomial-time algorithm to discover latent patterns in sparse graphs for bothmodels. When the network comes from an SBM, the algorithm outputs a blockstructure. When it is from an SWM, the algorithm outputs estimates of eachnode's latent position.

DistCache: Provable Load Balancing for Large-Scale Storage Systems with  Distributed Caching

  Load balancing is critical for distributed storage to meet strictservice-level objectives (SLOs). It has been shown that a fast cache canguarantee load balancing for a clustered storage system. However, when thesystem scales out to multiple clusters, the fast cache itself would become thebottleneck. Traditional mechanisms like cache partition and cache replicationeither result in load imbalance between cache nodes or have high overhead forcache coherence.  We present DistCache, a new distributed caching mechanism that providesprovable load balancing for large-scale storage systems. DistCache co-designscache allocation with cache topology and query routing. The key idea is topartition the hot objects with independent hash functions between cache nodesin different layers, and to adaptively route queries with thepower-of-two-choices. We prove that DistCache enables the cache throughput toincrease linearly with the number of cache nodes, by unifying techniques fromexpander graphs, network flows, and queuing theory. DistCache is a generalsolution that can be applied to many storage systems. We demonstrate thebenefits of DistCache by providing the design, implementation, and evaluationof the use case for emerging switch-based caching.

AMS Without 4-Wise Independence on Product Domains

  In their seminal work, Alon, Matias, and Szegedy introduced several sketchingtechniques, including showing that 4-wise independence is sufficient to obtaingood approximations of the second frequency moment. In this work, we show thattheir sketching technique can be extended to product domains $[n]^k$ by usingthe product of 4-wise independent functions on $[n]$. Our work extends that ofIndyk and McGregor, who showed the result for $k = 2$. Their primary motivationwas the problem of identifying correlations in data streams. In their model, astream of pairs $(i,j) \in [n]^2$ arrive, giving a joint distribution $(X,Y)$,and they find approximation algorithms for how close the joint distribution isto the product of the marginal distributions under various metrics, whichnaturally corresponds to how close $X$ and $Y$ are to being independent. Byusing our technique, we obtain a new result for the problem of approximatingthe $\ell_2$ distance between the joint distribution and the product of themarginal distributions for $k$-ary vectors, instead of just pairs, in a singlepass. Our analysis gives a randomized algorithm that is a $(1 \pm \epsilon)$approximation (with probability $1-\delta$) that requires space logarithmic in$n$ and $m$ and proportional to $3^k$.

Information Dissemination via Random Walks in d-Dimensional Space

  We study a natural information dissemination problem for multiple mobileagents in a bounded Euclidean space. Agents are placed uniformly at random inthe $d$-dimensional space $\{-n, ..., n\}^d$ at time zero, and one of theagents holds a piece of information to be disseminated. All the agents thenperform independent random walks over the space, and the information istransmitted from one agent to another if the two agents are sufficiently close.We wish to bound the total time before all agents receive the information (withhigh probability). Our work extends Pettarin et al.'s work (Infectious randomwalks, arXiv:1007.1604v2, 2011), which solved the problem for $d \leq 2$. Wepresent tight bounds up to polylogarithmic factors for the case $d = 3$. (Whileour results extend to higher dimensions, for space and readabilityconsiderations we provide only the case $d=3$ here.) Our results show thebehavior when $d \geq 3$ is qualitatively different from the case $d \leq 2$.In particular, as the ratio between the volume of the space and the number ofagents varies, we show an interesting phase transition for three dimensionsthat does not occur in one or two dimensions.

Distributed Non-Stochastic Experts

  We consider the online distributed non-stochastic experts problem, where thedistributed system consists of one coordinator node that is connected to $k$sites, and the sites are required to communicate with each other via thecoordinator. At each time-step $t$, one of the $k$ site nodes has to pick anexpert from the set ${1, ..., n}$, and the same site receives information aboutpayoffs of all experts for that round. The goal of the distributed system is tominimize regret at time horizon $T$, while simultaneously keeping communicationto a minimum.  The two extreme solutions to this problem are: (i) Full communication: Thisessentially simulates the non-distributed setting to obtain the optimal$O(\sqrt{\log(n)T})$ regret bound at the cost of $T$ communication. (ii) Nocommunication: Each site runs an independent copy : the regret is$O(\sqrt{log(n)kT})$ and the communication is 0. This paper shows thedifficulty of simultaneously achieving regret asymptotically better than$\sqrt{kT}$ and communication better than $T$. We give a novel algorithm thatfor an oblivious adversary achieves a non-trivial trade-off: regret$O(\sqrt{k^{5(1+\epsilon)/6} T})$ and communication $O(T/k^{\epsilon})$, forany value of $\epsilon \in (0, 1/5)$. We also consider a variant of the model,where the coordinator picks the expert. In this model, we show that thelabel-efficient forecaster of Cesa-Bianchi et al. (2005) already gives usstrategy that is near optimal in regret vs communication trade-off.

High-speed real-time single-pixel microscopy based on Fourier sampling

  Single-pixel cameras based on the concepts of compressed sensing (CS)leverage the inherent structure of images to retrieve them with far fewermeasurements and operate efficiently over a significantly broader spectralrange than conventional silicon-based cameras. Recently, photonic time-stretch(PTS) technique facilitates the emergence of high-speed single-pixel cameras. Asignificant breakthrough in imaging speed of single-pixel cameras enablesobservation of fast dynamic phenomena. However, according to CS theory, imagereconstruction is an iterative process that consumes enormous amounts ofcomputational time and cannot be performed in real time. To address thischallenge, we propose a novel single-pixel imaging technique that can producehigh-quality images through rapid acquisition of their effective spatialFourier spectrum. We employ phase-shifting sinusoidal structured illuminationinstead of random illumination for spectrum acquisition and apply inverseFourier transform to the obtained spectrum for image restoration. We evaluatethe performance of our prototype system by recognizing quick response (QR)codes and flow cytometric screening of cells. A frame rate of 625 kHz and acompression ratio of 10% are experimentally demonstrated in accordance with therecognition rate of the QR code. An imaging flow cytometer enablinghigh-content screening with an unprecedented throughput of 100,000 cells/s isalso demonstrated. For real-time imaging applications, the proposedsingle-pixel microscope can significantly reduce the time required for imagereconstruction by two orders of magnitude, which can be widely applied inindustrial quality control and label-free biomedical imaging.

Multi-Objective De Novo Drug Design with Conditional Graph Generative  Model

  Recently, deep generative models have revealed itself as a promising way ofperforming de novo molecule design. However, previous research has focusedmainly on generating SMILES strings instead of molecular graphs. Althoughcurrent graph generative models are available, they are often too general andcomputationally expensive, which restricts their application to molecules withsmall sizes. In this work, a new de novo molecular design framework is proposedbased on a type sequential graph generators that do not use atom levelrecurrent units. Compared with previous graph generative models, the proposedmethod is much more tuned for molecule generation and have been scaled up tocover significantly larger molecules in the ChEMBL database. It is shown thatthe graph-based model outperforms SMILES based models in a variety of metrics,especially in the rate of valid outputs. For the application of drug designtasks, conditional graph generative model is employed. This method offershigher flexibility compared to previous fine-tuning based approach and issuitable for generation based on multiple objectives. This approach is appliedto solve several drug design problems, including the generation of compoundscontaining a given scaffold, generation of compounds with specificdrug-likeness and synthetic accessibility requirements, as well as generatingdual inhibitors against JNK3 and GSK3$\beta$. Results show high enrichmentrates for outputs satisfying the given requirements.

Chernoff-Hoeffding Bounds for Markov Chains: Generalized and Simplified

  We prove the first Chernoff-Hoeffding bounds for general nonreversiblefinite-state Markov chains based on the standard L_1 (variation distance)mixing-time of the chain. Specifically, consider an ergodic Markov chain M anda weight function f: [n] -> [0,1] on the state space [n] of M with mean mu =E_{v <- pi}[f(v)], where pi is the stationary distribution of M. A t-steprandom walk (v_1,...,v_t) on M starting from the stationary distribution pi hasexpected total weight E[X] = mu t, where X = sum_{i=1}^t f(v_i). Let T be theL_1 mixing-time of M. We show that the probability of X deviating from its meanby a multiplicative factor of delta, i.e., Pr [ |X - mu t| >= delta mu t ], isat most exp(-Omega(delta^2 mu t / T)) for 0 <= delta <= 1, and exp(-Omega(deltamu t / T)) for delta > 1. In fact, the bounds hold even if the weight functionsf_i's for i in [t] are distinct, provided that all of them have the same meanmu.  We also obtain a simplified proof for the Chernoff-Hoeffding bounds based onthe spectral expansion lambda of M, which is the square root of the secondlargest eigenvalue (in absolute value) of M tilde{M}, where tilde{M} is thetime-reversal Markov chain of M. We show that the probability Pr [ |X - mu t|>= delta mu t ] is at most exp(-Omega(delta^2 (1-lambda) mu t)) for 0 <= delta<= 1, and exp(-Omega(delta (1-lambda) mu t)) for delta > 1.  Both of our results extend to continuous-time Markov chains, and to the casewhere the walk starts from an arbitrary distribution x, at a price of amultiplicative factor depending on the distribution x in the concentrationbounds.

Learning about social learning in MOOCs: From statistical analysis to  generative model

  We study user behavior in the courses offered by a major Massive Online OpenCourse (MOOC) provider during the summer of 2013. Since social learning is akey element of scalable education in MOOCs and is done via online discussionforums, our main focus is in understanding forum activities. Two salientfeatures of MOOC forum activities drive our research: 1. High decline rate: forall courses studied, the volume of discussions in the forum declinescontinuously throughout the duration of the course. 2. High-volume, noisydiscussions: at least 30% of the courses produce new discussion threads atrates that are infeasible for students or teaching staff to read through.Furthermore, a substantial portion of the discussions are not directlycourse-related.  We investigate factors that correlate with the decline of activity in theonline discussion forums and find effective strategies to classify threads andrank their relevance. Specifically, we use linear regression models to analyzethe time series of the count data for the forum activities and make a number ofobservations, e.g., the teaching staff's active participation in the discussionincreases the discussion volume but does not slow down the decline rate. Wethen propose a unified generative model for the discussion threads, whichallows us both to choose efficient thread classifiers and design an effectivealgorithm for ranking thread relevance. Our ranking algorithm is furthercompared against two baseline algorithms, using human evaluation from AmazonMechanical Turk.  The authors on this paper are listed in alphabetical order. For media andpress coverage, please refer to us collectively, as "researchers from the EDGELab at Princeton University, together with collaborators at Boston Universityand Microsoft Corporation."

