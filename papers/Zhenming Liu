From Black-Scholes to Online Learning: Dynamic Hedging under Adversarial
  Environments

  We consider a non-stochastic online learning approach to price financial
options by modeling the market dynamic as a repeated game between the nature
(adversary) and the investor. We demonstrate that such framework yields
analogous structure as the Black-Scholes model, the widely popular option
pricing model in stochastic finance, for both European and American options
with convex payoffs. In the case of non-convex options, we construct
approximate pricing algorithms, and demonstrate that their efficiency can be
analyzed through the introduction of an artificial probability measure, in
parallel to the so-called risk-neutral measure in the finance literature, even
though our framework is completely adversarial. Continuous-time convergence
results and extensions to incorporate price jumps are also presented.


Stock Market Prediction from WSJ: Text Mining via Sparse Matrix
  Factorization

  We revisit the problem of predicting directional movements of stock prices
based on news articles: here our algorithm uses daily articles from The Wall
Street Journal to predict the closing stock prices on the same day. We propose
a unified latent space model to characterize the "co-movements" between stock
prices and news articles. Unlike many existing approaches, our new model is
able to simultaneously leverage the correlations: (a) among stock prices, (b)
among news articles, and (c) between stock prices and news articles. Thus, our
model is able to make daily predictions on more than 500 stocks (most of which
are not even mentioned in any news article) while having low complexity. We
carry out extensive backtesting on trading strategies based on our algorithm.
The result shows that our model has substantially better accuracy rate (55.7%)
compared to many widely used algorithms. The return (56%) and Sharpe ratio due
to a trading strategy based on our model are also much higher than baseline
indices.


The Diffusion of Networking Technologies

  There has been significant interest in the networking community on the impact
of cascade effects on the diffusion of networking technology upgrades in the
Internet. Thinking of the global Internet as a graph, where each node
represents an economically-motivated Internet Service Provider (ISP), a key
problem is to determine the smallest set of nodes that can trigger a cascade
that causes every other node in the graph to adopt the protocol. We design the
first approximation algorithm with a provable performance guarantee for this
problem, in a model that captures the following key issue: a node's decision to
upgrade should be influenced by the decisions of the remote nodes it wishes to
communicate with.
  Given an internetwork G(V,E) and threshold function \theta, we assume that
node $u$ activates (upgrades to the new technology) when it is adjacent to a
connected component of active nodes in G of size exceeding node $u$'s threshold
\theta(u). Our objective is to choose the smallest set of nodes that can cause
the rest of the graph to activate. Our main contribution is an approximation
algorithm based on linear programming, which we complement with computational
hardness results and a near-optimum integrality gap. Our algorithm, which does
not rely on submodular optimization techniques, also highlights the substantial
algorithmic difference between our problem and similar questions studied in the
context of social networks.


Towards Non-Parametric Learning to Rank

  This paper studies a stylized, yet natural, learning-to-rank problem and
points out the critical incorrectness of a widely used nearest neighbor
algorithm. We consider a model with $n$ agents (users) $\{x_i\}_{i \in [n]}$
and $m$ alternatives (items) $\{y_j\}_{j \in [m]}$, each of which is associated
with a latent feature vector. Agents rank items nondeterministically according
to the Plackett-Luce model, where the higher the utility of an item to the
agent, the more likely this item will be ranked high by the agent. Our goal is
to find neighbors of an arbitrary agent or alternative in the latent space.
  We first show that the Kendall-tau distance based kNN produces incorrect
results in our model. Next, we fix the problem by introducing a new algorithm
with features constructed from "global information" of the data matrix. Our
approach is in sharp contrast to most existing feature engineering methods.
Finally, we design another new algorithm identifying similar alternatives. The
construction of alternative features can be done using "local information,"
highlighting the algorithmic difference between finding similar agents and
similar alternatives.


Total Variation with Overlapping Group Sparsity and Lp Quasinorm for
  Infrared Image Deblurring under Salt-and-Pepper Noise

  Because of the limitations of the infrared imaging principle and the
properties of infrared imaging systems, infrared images have some drawbacks,
including a lack of details, indistinct edges, and a large amount of
salt-andpepper noise. To improve the sparse characteristics of the image while
maintaining the image edges and weakening staircase artifacts, this paper
proposes a method that uses the Lp quasinorm instead of the L1 norm and for
infrared image deblurring with an overlapping group sparse total variation
method. The Lp quasinorm introduces another degree of freedom, better describes
image sparsity characteristics, and improves image restoration. Furthermore, we
adopt the accelerated alternating direction method of multipliers and fast
Fourier transform theory in the proposed method to improve the efficiency and
robustness of our algorithm. Experiments show that under different conditions
for blur and salt-and-pepper noise, the proposed method leads to excellent
performance in terms of objective evaluation and subjective visual results.


Statistically-secure ORAM with $\tilde{O}(\log^2 n)$ Overhead

  We demonstrate a simple, statistically secure, ORAM with computational
overhead $\tilde{O}(\log^2 n)$; previous ORAM protocols achieve only
computational security (under computational assumptions) or require
$\tilde{\Omega}(\log^3 n)$ overheard. An additional benefit of our ORAM is its
conceptual simplicity, which makes it easy to implement in both software and
(commercially available) hardware.
  Our construction is based on recent ORAM constructions due to Shi, Chan,
Stefanov, and Li (Asiacrypt 2011) and Stefanov and Shi (ArXiv 2012), but with
some crucial modifications in the algorithm that simplifies the ORAM and enable
our analysis. A central component in our analysis is reducing the analysis of
our algorithm to a "supermarket" problem; of independent interest (and of
importance to our analysis,) we provide an upper bound on the rate of "upset"
customers in the "supermarket" problem.


Mind Your Own Bandwidth: An Edge Solution to Peak-hour Broadband
  Congestion

  Motivated by recent increases in network traffic, we propose a decentralized
network edge-based solution to peak-hour broadband congestion that incentivizes
users to moderate their bandwidth demands to their actual needs. Our solution
is centered on smart home gateways that allocate bandwidth in a two-level
hierarchy: first, a gateway purchases guaranteed bandwidth from the Internet
Service Provider (ISP) with virtual credits. It then self-limits its bandwidth
usage and distributes the bandwidth among its apps and devices according to
their relative priorities. To this end, we design a credit allocation and
redistribution mechanism for the first level, and implement our gateways on
commodity wireless routers for the second level. We demonstrate our system's
effectiveness and practicality with theoretical analysis, simulations and
experiments on real traffic. Compared to a baseline equal sharing algorithm,
our solution significantly improves users' overall satisfaction and yields a
fair allocation of bandwidth across users.


From which world is your graph?

  Discovering statistical structure from links is a fundamental problem in the
analysis of social networks. Choosing a misspecified model, or equivalently, an
incorrect inference algorithm will result in an invalid analysis or even
falsely uncover patterns that are in fact artifacts of the model. This work
focuses on unifying two of the most widely used link-formation models: the
stochastic blockmodel (SBM) and the small world (or latent space) model (SWM).
Integrating techniques from kernel learning, spectral graph theory, and
nonlinear dimensionality reduction, we develop the first statistically sound
polynomial-time algorithm to discover latent patterns in sparse graphs for both
models. When the network comes from an SBM, the algorithm outputs a block
structure. When it is from an SWM, the algorithm outputs estimates of each
node's latent position.


DistCache: Provable Load Balancing for Large-Scale Storage Systems with
  Distributed Caching

  Load balancing is critical for distributed storage to meet strict
service-level objectives (SLOs). It has been shown that a fast cache can
guarantee load balancing for a clustered storage system. However, when the
system scales out to multiple clusters, the fast cache itself would become the
bottleneck. Traditional mechanisms like cache partition and cache replication
either result in load imbalance between cache nodes or have high overhead for
cache coherence.
  We present DistCache, a new distributed caching mechanism that provides
provable load balancing for large-scale storage systems. DistCache co-designs
cache allocation with cache topology and query routing. The key idea is to
partition the hot objects with independent hash functions between cache nodes
in different layers, and to adaptively route queries with the
power-of-two-choices. We prove that DistCache enables the cache throughput to
increase linearly with the number of cache nodes, by unifying techniques from
expander graphs, network flows, and queuing theory. DistCache is a general
solution that can be applied to many storage systems. We demonstrate the
benefits of DistCache by providing the design, implementation, and evaluation
of the use case for emerging switch-based caching.


AMS Without 4-Wise Independence on Product Domains

  In their seminal work, Alon, Matias, and Szegedy introduced several sketching
techniques, including showing that 4-wise independence is sufficient to obtain
good approximations of the second frequency moment. In this work, we show that
their sketching technique can be extended to product domains $[n]^k$ by using
the product of 4-wise independent functions on $[n]$. Our work extends that of
Indyk and McGregor, who showed the result for $k = 2$. Their primary motivation
was the problem of identifying correlations in data streams. In their model, a
stream of pairs $(i,j) \in [n]^2$ arrive, giving a joint distribution $(X,Y)$,
and they find approximation algorithms for how close the joint distribution is
to the product of the marginal distributions under various metrics, which
naturally corresponds to how close $X$ and $Y$ are to being independent. By
using our technique, we obtain a new result for the problem of approximating
the $\ell_2$ distance between the joint distribution and the product of the
marginal distributions for $k$-ary vectors, instead of just pairs, in a single
pass. Our analysis gives a randomized algorithm that is a $(1 \pm \epsilon)$
approximation (with probability $1-\delta$) that requires space logarithmic in
$n$ and $m$ and proportional to $3^k$.


Information Dissemination via Random Walks in d-Dimensional Space

  We study a natural information dissemination problem for multiple mobile
agents in a bounded Euclidean space. Agents are placed uniformly at random in
the $d$-dimensional space $\{-n, ..., n\}^d$ at time zero, and one of the
agents holds a piece of information to be disseminated. All the agents then
perform independent random walks over the space, and the information is
transmitted from one agent to another if the two agents are sufficiently close.
We wish to bound the total time before all agents receive the information (with
high probability). Our work extends Pettarin et al.'s work (Infectious random
walks, arXiv:1007.1604v2, 2011), which solved the problem for $d \leq 2$. We
present tight bounds up to polylogarithmic factors for the case $d = 3$. (While
our results extend to higher dimensions, for space and readability
considerations we provide only the case $d=3$ here.) Our results show the
behavior when $d \geq 3$ is qualitatively different from the case $d \leq 2$.
In particular, as the ratio between the volume of the space and the number of
agents varies, we show an interesting phase transition for three dimensions
that does not occur in one or two dimensions.


Distributed Non-Stochastic Experts

  We consider the online distributed non-stochastic experts problem, where the
distributed system consists of one coordinator node that is connected to $k$
sites, and the sites are required to communicate with each other via the
coordinator. At each time-step $t$, one of the $k$ site nodes has to pick an
expert from the set ${1, ..., n}$, and the same site receives information about
payoffs of all experts for that round. The goal of the distributed system is to
minimize regret at time horizon $T$, while simultaneously keeping communication
to a minimum.
  The two extreme solutions to this problem are: (i) Full communication: This
essentially simulates the non-distributed setting to obtain the optimal
$O(\sqrt{\log(n)T})$ regret bound at the cost of $T$ communication. (ii) No
communication: Each site runs an independent copy : the regret is
$O(\sqrt{log(n)kT})$ and the communication is 0. This paper shows the
difficulty of simultaneously achieving regret asymptotically better than
$\sqrt{kT}$ and communication better than $T$. We give a novel algorithm that
for an oblivious adversary achieves a non-trivial trade-off: regret
$O(\sqrt{k^{5(1+\epsilon)/6} T})$ and communication $O(T/k^{\epsilon})$, for
any value of $\epsilon \in (0, 1/5)$. We also consider a variant of the model,
where the coordinator picks the expert. In this model, we show that the
label-efficient forecaster of Cesa-Bianchi et al. (2005) already gives us
strategy that is near optimal in regret vs communication trade-off.


High-speed real-time single-pixel microscopy based on Fourier sampling

  Single-pixel cameras based on the concepts of compressed sensing (CS)
leverage the inherent structure of images to retrieve them with far fewer
measurements and operate efficiently over a significantly broader spectral
range than conventional silicon-based cameras. Recently, photonic time-stretch
(PTS) technique facilitates the emergence of high-speed single-pixel cameras. A
significant breakthrough in imaging speed of single-pixel cameras enables
observation of fast dynamic phenomena. However, according to CS theory, image
reconstruction is an iterative process that consumes enormous amounts of
computational time and cannot be performed in real time. To address this
challenge, we propose a novel single-pixel imaging technique that can produce
high-quality images through rapid acquisition of their effective spatial
Fourier spectrum. We employ phase-shifting sinusoidal structured illumination
instead of random illumination for spectrum acquisition and apply inverse
Fourier transform to the obtained spectrum for image restoration. We evaluate
the performance of our prototype system by recognizing quick response (QR)
codes and flow cytometric screening of cells. A frame rate of 625 kHz and a
compression ratio of 10% are experimentally demonstrated in accordance with the
recognition rate of the QR code. An imaging flow cytometer enabling
high-content screening with an unprecedented throughput of 100,000 cells/s is
also demonstrated. For real-time imaging applications, the proposed
single-pixel microscope can significantly reduce the time required for image
reconstruction by two orders of magnitude, which can be widely applied in
industrial quality control and label-free biomedical imaging.


Multi-Objective De Novo Drug Design with Conditional Graph Generative
  Model

  Recently, deep generative models have revealed itself as a promising way of
performing de novo molecule design. However, previous research has focused
mainly on generating SMILES strings instead of molecular graphs. Although
current graph generative models are available, they are often too general and
computationally expensive, which restricts their application to molecules with
small sizes. In this work, a new de novo molecular design framework is proposed
based on a type sequential graph generators that do not use atom level
recurrent units. Compared with previous graph generative models, the proposed
method is much more tuned for molecule generation and have been scaled up to
cover significantly larger molecules in the ChEMBL database. It is shown that
the graph-based model outperforms SMILES based models in a variety of metrics,
especially in the rate of valid outputs. For the application of drug design
tasks, conditional graph generative model is employed. This method offers
higher flexibility compared to previous fine-tuning based approach and is
suitable for generation based on multiple objectives. This approach is applied
to solve several drug design problems, including the generation of compounds
containing a given scaffold, generation of compounds with specific
drug-likeness and synthetic accessibility requirements, as well as generating
dual inhibitors against JNK3 and GSK3$\beta$. Results show high enrichment
rates for outputs satisfying the given requirements.


Chernoff-Hoeffding Bounds for Markov Chains: Generalized and Simplified

  We prove the first Chernoff-Hoeffding bounds for general nonreversible
finite-state Markov chains based on the standard L_1 (variation distance)
mixing-time of the chain. Specifically, consider an ergodic Markov chain M and
a weight function f: [n] -> [0,1] on the state space [n] of M with mean mu =
E_{v <- pi}[f(v)], where pi is the stationary distribution of M. A t-step
random walk (v_1,...,v_t) on M starting from the stationary distribution pi has
expected total weight E[X] = mu t, where X = sum_{i=1}^t f(v_i). Let T be the
L_1 mixing-time of M. We show that the probability of X deviating from its mean
by a multiplicative factor of delta, i.e., Pr [ |X - mu t| >= delta mu t ], is
at most exp(-Omega(delta^2 mu t / T)) for 0 <= delta <= 1, and exp(-Omega(delta
mu t / T)) for delta > 1. In fact, the bounds hold even if the weight functions
f_i's for i in [t] are distinct, provided that all of them have the same mean
mu.
  We also obtain a simplified proof for the Chernoff-Hoeffding bounds based on
the spectral expansion lambda of M, which is the square root of the second
largest eigenvalue (in absolute value) of M tilde{M}, where tilde{M} is the
time-reversal Markov chain of M. We show that the probability Pr [ |X - mu t|
>= delta mu t ] is at most exp(-Omega(delta^2 (1-lambda) mu t)) for 0 <= delta
<= 1, and exp(-Omega(delta (1-lambda) mu t)) for delta > 1.
  Both of our results extend to continuous-time Markov chains, and to the case
where the walk starts from an arbitrary distribution x, at a price of a
multiplicative factor depending on the distribution x in the concentration
bounds.


Learning about social learning in MOOCs: From statistical analysis to
  generative model

  We study user behavior in the courses offered by a major Massive Online Open
Course (MOOC) provider during the summer of 2013. Since social learning is a
key element of scalable education in MOOCs and is done via online discussion
forums, our main focus is in understanding forum activities. Two salient
features of MOOC forum activities drive our research: 1. High decline rate: for
all courses studied, the volume of discussions in the forum declines
continuously throughout the duration of the course. 2. High-volume, noisy
discussions: at least 30% of the courses produce new discussion threads at
rates that are infeasible for students or teaching staff to read through.
Furthermore, a substantial portion of the discussions are not directly
course-related.
  We investigate factors that correlate with the decline of activity in the
online discussion forums and find effective strategies to classify threads and
rank their relevance. Specifically, we use linear regression models to analyze
the time series of the count data for the forum activities and make a number of
observations, e.g., the teaching staff's active participation in the discussion
increases the discussion volume but does not slow down the decline rate. We
then propose a unified generative model for the discussion threads, which
allows us both to choose efficient thread classifiers and design an effective
algorithm for ranking thread relevance. Our ranking algorithm is further
compared against two baseline algorithms, using human evaluation from Amazon
Mechanical Turk.
  The authors on this paper are listed in alphabetical order. For media and
press coverage, please refer to us collectively, as "researchers from the EDGE
Lab at Princeton University, together with collaborators at Boston University
and Microsoft Corporation."


