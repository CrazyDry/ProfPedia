Selective Memoization

  This paper presents language techniques for applying memoization selectively.
The techniques provide programmer control over equality, space usage, and
identification of precise dependences so that memoization can be applied
according to the needs of an application. Two key properties of the approach
are that it accepts and efficient implementation and yields programs whose
performance can be analyzed using standard analysis techniques. We describe our
approach in the context of a functional language called MFL and an
implementation as a Standard ML library. The MFL language employs a modal type
system to enable the programmer to express programs that reveal their true data
dependences when executed. We prove that the MFL language is sound by showing
that that MFL programs yield the same result as they would with respect to a
standard, non-memoizing semantics. The SML implementation cannot support the
modal type system of MFL statically but instead employs run-time checks to
ensure correct usage of primitives.


A Consistent Semantics of Self-Adjusting Computation

  This paper presents a semantics of self-adjusting computation and proves that
the semantics are correct and consistent. The semantics integrate change
propagation with the classic idea of memoization to enable reuse of
computations under mutation to memory. During evaluation, reuse of a
computation via memoization triggers a change propagation that adjusts the
reused computation to reflect the mutated memory. Since the semantics integrate
memoization and change-propagation, it involves both non-determinism (due to
memoization) and mutation (due to change propagation). Our consistency theorem
states that the non-determinism is not harmful: any two evaluations of the same
program starting at the same state yield the same result. Our correctness
theorem states that mutation is not harmful: self-adjusting programs are
consistent with purely functional programming. We formalize the semantics and
their meta-theory in the LF logical framework and machine check our proofs
using Twelf.


Adaptive Inference on General Graphical Models

  Many algorithms and applications involve repeatedly solving variations of the
same inference problem; for example we may want to introduce new evidence to
the model or perform updates to conditional dependencies. The goal of adaptive
inference is to take advantage of what is preserved in the model and perform
inference more rapidly than from scratch. In this paper, we describe techniques
for adaptive inference on general graphs that support marginal computation and
updates to the conditional probabilities and dependencies in logarithmic time.
We give experimental results for an implementation of our algorithm, and
demonstrate its potential performance benefit in the study of protein
structure.


Self-Adjusting Stack Machines

  Self-adjusting computation offers a language-based approach to writing
programs that automatically respond to dynamically changing data. Recent work
made significant progress in developing sound semantics and associated
implementations of self-adjusting computation for high-level, functional
languages. These techniques, however, do not address issues that arise for
low-level languages, i.e., stack-based imperative languages that lack strong
type systems and automatic memory management.
  In this paper, we describe techniques for self-adjusting computation which
are suitable for low-level languages. Necessarily, we take a different approach
than previous work: instead of starting with a high-level language with
additional primitives to support self-adjusting computation, we start with a
low-level intermediate language, whose semantics is given by a stack-based
abstract machine. We prove that this semantics is sound: it always updates
computations in a way that is consistent with full reevaluation. We give a
compiler and runtime system for the intermediate language used by our abstract
machine. We present an empirical evaluation that shows that our approach is
efficient in practice, and performs favorably compared to prior proposals.


Provenance as Dependency Analysis

  Provenance is information recording the source, derivation, or history of
some information. Provenance tracking has been studied in a variety of
settings; however, although many design points have been explored, the
mathematical or semantic foundations of data provenance have received
comparatively little attention. In this paper, we argue that dependency
analysis techniques familiar from program analysis and program slicing provide
a formal foundation for forms of provenance that are intended to show how (part
of) the output of a query depends on (parts of) its input. We introduce a
semantic characterization of such dependency provenance, show that this form of
provenance is not computable, and provide dynamic and static approximation
techniques.


Database Queries that Explain their Work

  Provenance for database queries or scientific workflows is often motivated as
providing explanation, increasing understanding of the underlying data sources
and processes used to compute the query, and reproducibility, the capability to
recompute the results on different inputs, possibly specialized to a part of
the output. Many provenance systems claim to provide such capabilities;
however, most lack formal definitions or guarantees of these properties, while
others provide formal guarantees only for relatively limited classes of
changes. Building on recent work on provenance traces and slicing for
functional programming languages, we introduce a detailed tracing model of
provenance for multiset-valued Nested Relational Calculus, define trace slicing
algorithms that extract subtraces needed to explain or recompute specific parts
of the output, and define query slicing and differencing techniques that
support explanation. We state and prove correctness properties for these
techniques and present a proof-of-concept implementation in Haskell.


A Core Calculus for Provenance

  Provenance is an increasing concern due to the ongoing revolution in sharing
and processing scientific data on the Web and in other computer systems. It is
proposed that many computer systems will need to become provenance-aware in
order to provide satisfactory accountability, reproducibility, and trust for
scientific or other high-value data. To date, there is not a consensus
concerning appropriate formal models or security properties for provenance. In
previous work, we introduced a formal framework for provenance security and
proposed formal definitions of properties called disclosure and obfuscation.
  In this article, we study refined notions of positive and negative disclosure
and obfuscation in a concrete setting, that of a general-purpose programing
language. Previous models of provenance have focused on special-purpose
languages such as workflows and database queries. We consider a higher-order,
functional language with sums, products, and recursive types and functions, and
equip it with a tracing semantics in which traces themselves can be replayed as
computations. We present an annotation-propagation framework that supports many
provenance views over traces, including standard forms of provenance studied
previously. We investigate some relationships among provenance views and
develop some partial solutions to the disclosure and obfuscation problems,
including correct algorithms for disclosure and positive obfuscation based on
trace slicing.


Provenance Traces

  Provenance is information about the origin, derivation, ownership, or history
of an object. It has recently been studied extensively in scientific databases
and other settings due to its importance in helping scientists judge data
validity, quality and integrity. However, most models of provenance have been
stated as ad hoc definitions motivated by informal concepts such as "comes
from", "influences", "produces", or "depends on". These models lack clear
formalizations describing in what sense the definitions capture these intuitive
concepts. This makes it difficult to compare approaches, evaluate their
effectiveness, or argue about their validity.
  We introduce provenance traces, a general form of provenance for the nested
relational calculus (NRC), a core database query language. Provenance traces
can be thought of as concrete data structures representing the operational
semantics derivation of a computation; they are related to the traces that have
been used in self-adjusting computation, but differ in important respects. We
define a tracing operational semantics for NRC queries that produces both an
ordinary result and a trace of the execution. We show that three pre-existing
forms of provenance for the NRC can be extracted from provenance traces.
Moreover, traces satisfy two semantic guarantees: consistency, meaning that the
traces describe what actually happened during execution, and fidelity, meaning
that the traces "explain" how the expression would behave if the input were
changed. These guarantees are much stronger than those contemplated for
previous approaches to provenance; thus, provenance traces provide a general
semantic foundation for comparing and unifying models of provenance in
databases.


Competitive Parallelism: Getting Your Priorities Right

  Multi-threaded programs have traditionally fallen into one of two domains:
cooperative and competitive. These two domains have traditionally remained
mostly disjoint, with cooperative threading used for increasing throughput in
compute-intensive applications such as scientific workloads and cooperative
threading used for increasing responsiveness in interactive applications such
as GUIs and games. As multicore hardware becomes increasingly mainstream, there
is a need for bridging these two disjoint worlds, because many applications mix
interaction and computation and would benefit from both cooperative and
competitive threading.
  In this paper, we present techniques for programming and reasoning about
parallel interactive applications that can use both cooperative and competitive
threading. Our techniques enable the programmer to write rich parallel
interactive programs by creating and synchronizing with threads as needed, and
by assigning threads user-defined and partially ordered priorities. To ensure
important responsiveness properties, we present a modal type system analogous
to S4 modal logic that precludes low-priority threads from delaying
high-priority threads, thereby statically preventing a crucial set of
priority-inversion bugs. We then present a cost model that allows reasoning
about responsiveness and completion time of well-typed programs. The cost model
extends the traditional work-span model for cooperative threading to account
for competitive scheduling decisions needed to ensure responsiveness. Finally,
we show that our proposed techniques are realistic by implementing them as an
extension to the Standard ML language.


Parallel Batch-Dynamic Graph Connectivity

  With the rapid growth of graph datasets over the past decade, a new kind of
dynamic algorithm, supporting the ability to ingest batches of updates and
exploit parallelism is needed in order to efficiently process large streams of
updates. In this paper, we study batch and parallel algorithms for the dynamic
connectivity problem, a fundamental problem that has received considerable
attention in sequential setting. Perhaps the best known sequential algorithm is
the elegant level-set algorithm of Holm, de Lichtenberg and Thorup (HDT), which
achieves $O(\log^2 n)$ amortized time per edge insertion or deletion, and
$O(\log n)$ time per query.
  In this paper, we design a parallel batch-dynamic connectivity algorithm that
is work-efficient with respect to the HDT algorithm for small batch sizes, and
is asymptotically faster when the average batch size is sufficiently large.
Given a sequence of batched updates, where $\Delta$ is the average batch size
of all deletions, our algorithm achieves $O(\log n \log(1 + n / \Delta))$
expected amortized work per edge insertion and deletion and $O(\log^3 n)$ depth
w.h.p. Our algorithm answers a batch of $k$ connectivity queries in $O(k \log(1
+ n/k))$ expected work and $O(\log n)$ depth w.h.p. To the best of our
knowledge, our algorithm is the first parallel batch-dynamic algorithm for
connectivity.


Parallel Work Inflation, Memory Effects, and their Empirical Analysis

  In this paper, we propose an empirical method for evaluating the performance
of parallel code. Our method is based on a simple idea that is surprisingly
effective in helping to identify causes of poor performance, such as high
parallelization overheads, lack of adequate parallelism, and memory effects.
Our method relies on only the measurement of the run time of a baseline
sequential program, the run time of the parallel program, the single-processor
run time of the parallel program, and the total amount of time processors spend
idle, waiting for work.
  In our proposed approach, we establish an equality between the observed
parallel speedups and three terms that we call parallel work, idle time, and
work-inflation, where all terms except work inflation can be measured
empirically, with precision. We then use the equality to calculate the
difficult-to-measure work-inflation term, which includes increased
communication costs and memory effects due to parallel execution. By isolating
the main factors of poor performance, our method enables the programmer to
assign blame to certain properties of the code, such as parallel grain size,
amount of parallelism, and memory usage.
  We present a mathematical model, inspired by the work-span model, that
enables us to justify the interpretation of our measurements. We also introduce
a method to help the programmer to visualize both the relative impact of the
various causes of poor performance and the scaling trends in the causes of poor
performance. Our method fits in a sweet spot in between state-of-the-art
profiling and visualization tools. We illustrate our method by several
empirical studies and we describe a few experiments that emphasize the care
that is required to accurately interpret speedup plots.


