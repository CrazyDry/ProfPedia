Selective Memoization

  This paper presents language techniques for applying memoization selectively.The techniques provide programmer control over equality, space usage, andidentification of precise dependences so that memoization can be appliedaccording to the needs of an application. Two key properties of the approachare that it accepts and efficient implementation and yields programs whoseperformance can be analyzed using standard analysis techniques. We describe ourapproach in the context of a functional language called MFL and animplementation as a Standard ML library. The MFL language employs a modal typesystem to enable the programmer to express programs that reveal their true datadependences when executed. We prove that the MFL language is sound by showingthat that MFL programs yield the same result as they would with respect to astandard, non-memoizing semantics. The SML implementation cannot support themodal type system of MFL statically but instead employs run-time checks toensure correct usage of primitives.

A Consistent Semantics of Self-Adjusting Computation

  This paper presents a semantics of self-adjusting computation and proves thatthe semantics are correct and consistent. The semantics integrate changepropagation with the classic idea of memoization to enable reuse ofcomputations under mutation to memory. During evaluation, reuse of acomputation via memoization triggers a change propagation that adjusts thereused computation to reflect the mutated memory. Since the semantics integratememoization and change-propagation, it involves both non-determinism (due tomemoization) and mutation (due to change propagation). Our consistency theoremstates that the non-determinism is not harmful: any two evaluations of the sameprogram starting at the same state yield the same result. Our correctnesstheorem states that mutation is not harmful: self-adjusting programs areconsistent with purely functional programming. We formalize the semantics andtheir meta-theory in the LF logical framework and machine check our proofsusing Twelf.

Adaptive Inference on General Graphical Models

  Many algorithms and applications involve repeatedly solving variations of thesame inference problem; for example we may want to introduce new evidence tothe model or perform updates to conditional dependencies. The goal of adaptiveinference is to take advantage of what is preserved in the model and performinference more rapidly than from scratch. In this paper, we describe techniquesfor adaptive inference on general graphs that support marginal computation andupdates to the conditional probabilities and dependencies in logarithmic time.We give experimental results for an implementation of our algorithm, anddemonstrate its potential performance benefit in the study of proteinstructure.

Self-Adjusting Stack Machines

  Self-adjusting computation offers a language-based approach to writingprograms that automatically respond to dynamically changing data. Recent workmade significant progress in developing sound semantics and associatedimplementations of self-adjusting computation for high-level, functionallanguages. These techniques, however, do not address issues that arise forlow-level languages, i.e., stack-based imperative languages that lack strongtype systems and automatic memory management.  In this paper, we describe techniques for self-adjusting computation whichare suitable for low-level languages. Necessarily, we take a different approachthan previous work: instead of starting with a high-level language withadditional primitives to support self-adjusting computation, we start with alow-level intermediate language, whose semantics is given by a stack-basedabstract machine. We prove that this semantics is sound: it always updatescomputations in a way that is consistent with full reevaluation. We give acompiler and runtime system for the intermediate language used by our abstractmachine. We present an empirical evaluation that shows that our approach isefficient in practice, and performs favorably compared to prior proposals.

Provenance as Dependency Analysis

  Provenance is information recording the source, derivation, or history ofsome information. Provenance tracking has been studied in a variety ofsettings; however, although many design points have been explored, themathematical or semantic foundations of data provenance have receivedcomparatively little attention. In this paper, we argue that dependencyanalysis techniques familiar from program analysis and program slicing providea formal foundation for forms of provenance that are intended to show how (partof) the output of a query depends on (parts of) its input. We introduce asemantic characterization of such dependency provenance, show that this form ofprovenance is not computable, and provide dynamic and static approximationtechniques.

Database Queries that Explain their Work

  Provenance for database queries or scientific workflows is often motivated asproviding explanation, increasing understanding of the underlying data sourcesand processes used to compute the query, and reproducibility, the capability torecompute the results on different inputs, possibly specialized to a part ofthe output. Many provenance systems claim to provide such capabilities;however, most lack formal definitions or guarantees of these properties, whileothers provide formal guarantees only for relatively limited classes ofchanges. Building on recent work on provenance traces and slicing forfunctional programming languages, we introduce a detailed tracing model ofprovenance for multiset-valued Nested Relational Calculus, define trace slicingalgorithms that extract subtraces needed to explain or recompute specific partsof the output, and define query slicing and differencing techniques thatsupport explanation. We state and prove correctness properties for thesetechniques and present a proof-of-concept implementation in Haskell.

A Core Calculus for Provenance

  Provenance is an increasing concern due to the ongoing revolution in sharingand processing scientific data on the Web and in other computer systems. It isproposed that many computer systems will need to become provenance-aware inorder to provide satisfactory accountability, reproducibility, and trust forscientific or other high-value data. To date, there is not a consensusconcerning appropriate formal models or security properties for provenance. Inprevious work, we introduced a formal framework for provenance security andproposed formal definitions of properties called disclosure and obfuscation.  In this article, we study refined notions of positive and negative disclosureand obfuscation in a concrete setting, that of a general-purpose programinglanguage. Previous models of provenance have focused on special-purposelanguages such as workflows and database queries. We consider a higher-order,functional language with sums, products, and recursive types and functions, andequip it with a tracing semantics in which traces themselves can be replayed ascomputations. We present an annotation-propagation framework that supports manyprovenance views over traces, including standard forms of provenance studiedpreviously. We investigate some relationships among provenance views anddevelop some partial solutions to the disclosure and obfuscation problems,including correct algorithms for disclosure and positive obfuscation based ontrace slicing.

Provenance Traces

  Provenance is information about the origin, derivation, ownership, or historyof an object. It has recently been studied extensively in scientific databasesand other settings due to its importance in helping scientists judge datavalidity, quality and integrity. However, most models of provenance have beenstated as ad hoc definitions motivated by informal concepts such as "comesfrom", "influences", "produces", or "depends on". These models lack clearformalizations describing in what sense the definitions capture these intuitiveconcepts. This makes it difficult to compare approaches, evaluate theireffectiveness, or argue about their validity.  We introduce provenance traces, a general form of provenance for the nestedrelational calculus (NRC), a core database query language. Provenance tracescan be thought of as concrete data structures representing the operationalsemantics derivation of a computation; they are related to the traces that havebeen used in self-adjusting computation, but differ in important respects. Wedefine a tracing operational semantics for NRC queries that produces both anordinary result and a trace of the execution. We show that three pre-existingforms of provenance for the NRC can be extracted from provenance traces.Moreover, traces satisfy two semantic guarantees: consistency, meaning that thetraces describe what actually happened during execution, and fidelity, meaningthat the traces "explain" how the expression would behave if the input werechanged. These guarantees are much stronger than those contemplated forprevious approaches to provenance; thus, provenance traces provide a generalsemantic foundation for comparing and unifying models of provenance indatabases.

Competitive Parallelism: Getting Your Priorities Right

  Multi-threaded programs have traditionally fallen into one of two domains:cooperative and competitive. These two domains have traditionally remainedmostly disjoint, with cooperative threading used for increasing throughput incompute-intensive applications such as scientific workloads and cooperativethreading used for increasing responsiveness in interactive applications suchas GUIs and games. As multicore hardware becomes increasingly mainstream, thereis a need for bridging these two disjoint worlds, because many applications mixinteraction and computation and would benefit from both cooperative andcompetitive threading.  In this paper, we present techniques for programming and reasoning aboutparallel interactive applications that can use both cooperative and competitivethreading. Our techniques enable the programmer to write rich parallelinteractive programs by creating and synchronizing with threads as needed, andby assigning threads user-defined and partially ordered priorities. To ensureimportant responsiveness properties, we present a modal type system analogousto S4 modal logic that precludes low-priority threads from delayinghigh-priority threads, thereby statically preventing a crucial set ofpriority-inversion bugs. We then present a cost model that allows reasoningabout responsiveness and completion time of well-typed programs. The cost modelextends the traditional work-span model for cooperative threading to accountfor competitive scheduling decisions needed to ensure responsiveness. Finally,we show that our proposed techniques are realistic by implementing them as anextension to the Standard ML language.

Parallel Batch-Dynamic Graph Connectivity

  With the rapid growth of graph datasets over the past decade, a new kind ofdynamic algorithm, supporting the ability to ingest batches of updates andexploit parallelism is needed in order to efficiently process large streams ofupdates. In this paper, we study batch and parallel algorithms for the dynamicconnectivity problem, a fundamental problem that has received considerableattention in sequential setting. Perhaps the best known sequential algorithm isthe elegant level-set algorithm of Holm, de Lichtenberg and Thorup (HDT), whichachieves $O(\log^2 n)$ amortized time per edge insertion or deletion, and$O(\log n)$ time per query.  In this paper, we design a parallel batch-dynamic connectivity algorithm thatis work-efficient with respect to the HDT algorithm for small batch sizes, andis asymptotically faster when the average batch size is sufficiently large.Given a sequence of batched updates, where $\Delta$ is the average batch sizeof all deletions, our algorithm achieves $O(\log n \log(1 + n / \Delta))$expected amortized work per edge insertion and deletion and $O(\log^3 n)$ depthw.h.p. Our algorithm answers a batch of $k$ connectivity queries in $O(k \log(1+ n/k))$ expected work and $O(\log n)$ depth w.h.p. To the best of ourknowledge, our algorithm is the first parallel batch-dynamic algorithm forconnectivity.

Parallel Work Inflation, Memory Effects, and their Empirical Analysis

  In this paper, we propose an empirical method for evaluating the performanceof parallel code. Our method is based on a simple idea that is surprisinglyeffective in helping to identify causes of poor performance, such as highparallelization overheads, lack of adequate parallelism, and memory effects.Our method relies on only the measurement of the run time of a baselinesequential program, the run time of the parallel program, the single-processorrun time of the parallel program, and the total amount of time processors spendidle, waiting for work.  In our proposed approach, we establish an equality between the observedparallel speedups and three terms that we call parallel work, idle time, andwork-inflation, where all terms except work inflation can be measuredempirically, with precision. We then use the equality to calculate thedifficult-to-measure work-inflation term, which includes increasedcommunication costs and memory effects due to parallel execution. By isolatingthe main factors of poor performance, our method enables the programmer toassign blame to certain properties of the code, such as parallel grain size,amount of parallelism, and memory usage.  We present a mathematical model, inspired by the work-span model, thatenables us to justify the interpretation of our measurements. We also introducea method to help the programmer to visualize both the relative impact of thevarious causes of poor performance and the scaling trends in the causes of poorperformance. Our method fits in a sweet spot in between state-of-the-artprofiling and visualization tools. We illustrate our method by severalempirical studies and we describe a few experiments that emphasize the carethat is required to accurately interpret speedup plots.

