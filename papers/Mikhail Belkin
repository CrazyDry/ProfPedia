Does data interpolation contradict statistical optimality?

  We show that learning methods interpolating the training data can achieve
optimal rates for the problems of nonparametric regression and prediction with
square loss.


Temperature performance analysis of terahertz quantum cascade lasers:
  Vertical versus diagonal designs

  Resonant phonon depopulation terahertz quantum cascade lasers based on
vertical and diagonal lasing transitions are systematically compared using a
well established ensemble Monte Carlo approach. The analysis shows that for
operating temperatures below 200 K, diagonal designs may offer superior
temperature performance at lasing frequencies of about 3.5 THz and above;
however, vertical structures are more advantageous for good temperature
performance at lower frequencies.


Probabilistic Zero-shot Classification with Semantic Rankings

  In this paper we propose a non-metric ranking-based representation of
semantic similarity that allows natural aggregation of semantic information
from multiple heterogeneous sources. We apply the ranking-based representation
to zero-shot learning problems, and present deterministic and probabilistic
zero-shot classifiers which can be built from pre-trained classifiers without
retraining. We demonstrate their the advantages on two large real-world image
datasets. In particular, we show that aggregating different sources of semantic
information, including crowd-sourcing, leads to more accurate classification.


Gradient Nonlinear Pancharatnam-Berry Metasurfaces

  We apply the Pancharatnam-Berry phase approach to plasmonic metasurfaces
loaded by highly nonlinear multi-quantum well substrates, establishing a
platform to control the nonlinear wavefront at will based on giant localized
nonlinear effects. We apply this approach to design flat nonlinear metasurfaces
for efficient second-harmonic radiation, including beam steering, focusing, and
polarization manipulation. Our findings open a new direction for nonlinear
optics, in which phase matching issues are relaxed, and an unprecedented level
of local wavefront control is achieved over thin devices with giant nonlinear
responses.


Parametrized Accelerated Methods Free of Condition Number

  Analyses of accelerated (momentum-based) gradient descent usually assume
bounded condition number to obtain exponential convergence rates. However, in
many real problems, e.g., kernel methods or deep neural networks, the condition
number, even locally, can be unbounded, unknown or mis-estimated. This poses
problems in both implementing and analyzing accelerated algorithms. In this
paper, we address this issue by proposing parametrized accelerated methods by
considering the condition number as a free parameter. We provide spectral-level
analysis for several important accelerated algorithms, obtain explicit
expressions and improve worst case convergence rates. Moreover, we show that
those algorithm converge exponentially even when the condition number is
unknown or mis-estimated.


Two models of double descent for weak features

  The "double descent" risk curve was recently proposed to qualitatively
describe the out-of-sample prediction accuracy of variably-parameterized
machine learning models. This article provides a precise mathematical analysis
for the shape of this curve in two simple data models with the least
squares/least norm predictor. Specifically, it is shown that the risk peaks
when the number of features $p$ is close to the sample size $n$, but also that
the risk decreases towards its minimum as $p$ increases beyond $n$. This
behavior is contrasted with that of "prescient" models that select features in
an a priori optimal order.


Approximation beats concentration? An approximation view on inference
  with smooth radial kernels

  Positive definite kernels and their associated Reproducing Kernel Hilbert
Spaces provide a mathematically compelling and practically competitive
framework for learning from data.
  In this paper we take the approximation theory point of view to explore
various aspects of smooth kernels related to their inferential properties. We
analyze eigenvalue decay of kernels operators and matrices, properties of
eigenfunctions/eigenvectors and "Fourier" coefficients of functions in the
kernel space restricted to a discrete set of data points. We also investigate
the fitting capacity of kernels, giving explicit bounds on the fat shattering
dimension of the balls in Reproducing Kernel Hilbert spaces. Interestingly, the
same properties that make kernels very effective approximators for functions in
their "native" kernel space, also limit their capacity to represent arbitrary
functions. We discuss various implications, including those for gradient
descent type methods.
  It is important to note that most of our bounds are measure independent.
Moreover, at least in moderate dimension, the bounds for eigenvalues are much
tighter than the bounds which can be obtained from the usual matrix
concentration results. For example, we see that the eigenvalues of kernel
matrices show nearly exponential decay with constants depending only on the
kernel and the domain. We call this "approximation beats concentration"
phenomenon as even when the data are sampled from a probability distribution,
some of their aspects are better understood in terms of approximation theory.


Enhancement of the spontaneous emission in subwavelength
  quasi-two-dimensional waveguides and resonators

  We consider a quantum-electrodynamic problem of the spontaneous emission from
a two-dimensional (2D) emitter, such as a quantum well or a 2D semiconductor,
placed in a quasi-2D waveguide or cavity with subwavelength confinement in one
direction. We apply the Heisenberg-Langevin approach which includes dissipation
and fluctuations in the electron ensemble and in the electromagnetic field of a
cavity on equal footing. The Langevin noise operators that we introduce do not
depend on any particular model of dissipative reservoir and can be applied to
any dissipation mechanism. Moreover, our approach is applicable to
nonequilibrium electron systems, e.g. in the presence of pumping, beyond the
applicability of the standard fluctuation-dissipation theorem. We derive
analytic results for simple but practically important geometries: strip lines
and rectangular cavities. Our results show that a significant enhancement of
the spontaneous emission, by a factor of order 100 or higher, is possible for
quantum wells and other 2D emitters in a subwavelength cavity.


Using eigenvectors of the bigram graph to infer morpheme identity

  This paper describes the results of some experiments exploring statistical
methods to infer syntactic behavior of words and morphemes from a raw corpus in
an unsupervised fashion. It shares certain points in common with Brown et al
(1992) and work that has grown out of that: it employs statistical techniques
to analyze syntactic behavior based on what words occur adjacent to a given
word. However, we use an eigenvector decomposition of a nearest-neighbor graph
to produce a two-dimensional rendering of the words of a corpus in which words
of the same syntactic category tend to form neighborhoods. We exploit this
technique for extending the value of automatic learning of morphology. In
particular, we look at the suffixes derived from a corpus by unsupervised
learning of morphology, and we ask which of these suffixes have a consistent
syntactic function (e.g., in English, -tion is primarily a mark of nouns, but
-s marks both noun plurals and 3rd person present on verbs), and we determine
that this method works well for this task.


Consistency of spectral clustering

  Consistency is a key property of all statistical procedures analyzing
randomly sampled data. Surprisingly, despite decades of work, little is known
about consistency of most clustering algorithms. In this paper we investigate
consistency of the popular family of spectral clustering algorithms, which
clusters the data with the help of eigenvectors of graph Laplacian matrices. We
develop new methods to establish that, for increasing sample size, those
eigenvectors converge to the eigenvectors of certain limit operators. As a
result, we can prove that one of the two major classes of spectral clustering
(normalized clustering) converges under very general conditions, while the
other (unnormalized clustering) is only consistent under strong additional
assumptions, which are not always satisfied in real data. We conclude that our
analysis provides strong evidence for the superiority of normalized spectral
clustering.


Polynomial Learning of Distribution Families

  The question of polynomial learnability of probability distributions,
particularly Gaussian mixture distributions, has recently received significant
attention in theoretical computer science and machine learning. However,
despite major progress, the general question of polynomial learnability of
Gaussian mixture distributions still remained open. The current work resolves
the question of polynomial learnability for Gaussian mixtures in high dimension
with an arbitrary fixed number of components. The result on learning Gaussian
mixtures relies on an analysis of distributions belonging to what we call
"polynomial families" in low dimension. These families are characterized by
their moments being polynomial in parameters and include almost all common
probability distributions as well as their mixtures and products. Using tools
from real algebraic geometry, we show that parameters of any distribution
belonging to such a family can be learned in polynomial time and using a
polynomial number of sample points. The result on learning polynomial families
is quite general and is of independent interest. To estimate parameters of a
Gaussian mixture distribution in high dimensions, we provide a deterministic
algorithm for dimensionality reduction. This allows us to reduce learning a
high-dimensional mixture to a polynomial number of parameter estimations in low
dimension. Combining this reduction with the results on polynomial families
yields our result on learning arbitrary Gaussian mixtures in high dimensions.


Behavior of Graph Laplacians on Manifolds with Boundary

  In manifold learning, algorithms based on graph Laplacians constructed from
data have received considerable attention both in practical applications and
theoretical analysis. In particular, the convergence of graph Laplacians
obtained from sampled data to certain continuous operators has become an active
research topic recently. Most of the existing work has been done under the
assumption that the data is sampled from a manifold without boundary or that
the functions of interests are evaluated at a point away from the boundary.
However, the question of boundary behavior is of considerable practical and
theoretical interest. In this paper we provide an analysis of the behavior of
graph Laplacians at a point near or on the boundary, discuss their convergence
rates and their implications and provide some numerical results. It turns out
that while points near the boundary occupy only a small part of the total
volume of a manifold, the behavior of graph Laplacian there has different
scaling properties from its behavior elsewhere on the manifold, with global
effects on the whole manifold, an observation with potentially important
implications for the general problem of learning on manifolds.


Learning Privately from Multiparty Data

  Learning a classifier from private data collected by multiple parties is an
important problem that has many potential applications. How can we build an
accurate and differentially private global classifier by combining
locally-trained classifiers from different parties, without access to any
party's private data? We propose to transfer the `knowledge' of the local
classifier ensemble by first creating labeled data from auxiliary unlabeled
data, and then train a global $\epsilon$-differentially private classifier. We
show that majority voting is too sensitive and therefore propose a new risk
weighted by class probabilities estimated from the ensemble. Relative to a
non-private solution, our private solution has a generalization error bounded
by $O(\epsilon^{-2}M^{-2})$ where $M$ is the number of parties. This allows
strong privacy without performance loss when $M$ is large, such as in
crowdsensing applications. We demonstrate the performance of our method with
realistic tasks of activity recognition, network intrusion detection, and
malicious URL detection.


The Hidden Convexity of Spectral Clustering

  In recent years, spectral clustering has become a standard method for data
analysis used in a broad range of applications. In this paper we propose a new
class of algorithms for multiway spectral clustering based on optimization of a
certain "contrast function" over the unit sphere. These algorithms, partly
inspired by certain Independent Component Analysis techniques, are simple, easy
to implement and efficient.
  Geometrically, the proposed algorithms can be interpreted as hidden basis
recovery by means of function optimization. We give a complete characterization
of the contrast functions admissible for provable basis recovery. We show how
these conditions can be interpreted as a "hidden convexity" of our optimization
problem on the sphere; interestingly, we use efficient convex maximization
rather than the more common convex minimization. We also show encouraging
experimental results on real and simulated data.


Crowd-ML: A Privacy-Preserving Learning Framework for a Crowd of Smart
  Devices

  Smart devices with built-in sensors, computational capabilities, and network
connectivity have become increasingly pervasive. The crowds of smart devices
offer opportunities to collectively sense and perform computing tasks in an
unprecedented scale. This paper presents Crowd-ML, a privacy-preserving machine
learning framework for a crowd of smart devices, which can solve a wide range
of learning problems for crowdsensing data with differential privacy
guarantees. Crowd-ML endows a crowdsensing system with an ability to learn
classifiers or predictors online from crowdsensing data privately with minimal
computational overheads on devices and servers, suitable for a practical and
large-scale employment of the framework. We analyze the performance and the
scalability of Crowd-ML, and implement the system with off-the-shelf
smartphones as a proof of concept. We demonstrate the advantages of Crowd-ML
with real and simulated experiments under various conditions.


Graphons, mergeons, and so on!

  In this work we develop a theory of hierarchical clustering for graphs. Our
modeling assumption is that graphs are sampled from a graphon, which is a
powerful and general model for generating graphs and analyzing large networks.
Graphons are a far richer class of graph models than stochastic blockmodels,
the primary setting for recent progress in the statistical theory of graph
clustering. We define what it means for an algorithm to produce the "correct"
clustering, give sufficient conditions in which a method is statistically
consistent, and provide an explicit algorithm satisfying these properties.


Electrical Tuning of Polarizaion-state Using Graphene-Integrated
  Metasurfaces

  Plasmonic metasurfaces have been employed for tuning and controlling light
enabling various novel applications. Their appeal is enhanced with the
incorporation of an active element with the metasurfaces paving the way for
dynamic control. In this letter, we realize a dynamic polarization state
generator using graphene-integrated anisotropic metasurface (GIAM), where a
linear incidence polarization is controllably converted into an elliptical one.
The anisotropic metasurface leads to an intrinsic polarization conversion when
illuminated with non-orthogonal incident polarization. Additionally, the
single-layer graphene allows us to tune the phase and intensity of the
reflected light on the application of a gate voltage, enabling dynamic
polarization control. The stokes polarization parameters of the reflected light
are measured using rotating polarizer method and it is demonstrated that a
large change in the ellipticity as well as orientation angle can be induced by
this device. We also provide experimental evidence that the titl angle can
change independent of the ellipticity going from positive values to nearly zero
to negative values while ellipticity is constant.


Unperturbed: spectral analysis beyond Davis-Kahan

  Classical matrix perturbation results, such as Weyl's theorem for eigenvalues
and the Davis-Kahan theorem for eigenvectors, are general purpose. These
classical bounds are tight in the worst case, but in many settings sub-optimal
in the typical case. In this paper, we present perturbation bounds which
consider the nature of the perturbation and its interaction with the
unperturbed structure in order to obtain significant improvements over the
classical theory in many scenarios, such as when the perturbation is random. We
demonstrate the utility of these new results by analyzing perturbations in the
stochastic blockmodel where we derive much tighter bounds than provided by the
classical theory. We use our new perturbation theory to show that a very simple
and natural clustering algorithm -- whose analysis was difficult using the
classical tools -- nevertheless recovers the communities of the blockmodel
exactly even in very sparse graphs.


Memorization in Overparameterized Autoencoders

  Memorization of data in deep neural networks has become a subject of
significant research interest. We prove that over-parameterized single layer
fully connected autoencoders memorize training data: they produce outputs in (a
non-linear version of) the span of the training examples. In contrast to fully
connected autoencoders, we prove that depth is necessary for memorization in
convolutional autoencoders. Moreover, we observe that adding nonlinearity to
deep convolutional autoencoders results in a stronger form of memorization:
instead of outputting points in the span of the training images, deep
convolutional autoencoders tend to output individual training images. Since
convolutional autoencoder components are building blocks of deep convolutional
networks, we envision that our findings will shed light on the important
phenomenon of memorization in over-parameterized deep networks.


Accelerating Stochastic Training for Over-parametrized Learning

  We introduce MaSS (Momentum-added Stochastic Solver), an accelerated SGD
method for optimizing over-parametrized models. Our method is simple and
efficient to implement and does not require adapting hyper-parameters or
computing full gradients in the course of optimization.
  Experimental evaluation of MaSS for several standard architectures of deep
networks, including ResNet and convolutional networks, shows improved
performance over Adam and SGD both in optimization and generalization.
  We prove accelerated convergence of MaSS over SGD and provide analysis for
hyper-parameter selection in the quadratic case as well as some results in
general strongly convex setting. In contrast, we show theoretically and verify
empirically that the standard SGD+Nesterov can diverge for common choices of
hyper-parameter values.
  We also analyze the practically important question of the dependence of the
convergence rate and optimal hyper-parameters as functions of the mini-batch
size, demonstrating three distinct regimes: linear scaling, diminishing returns
and saturation.


Kernel Machines Beat Deep Neural Networks on Mask-based Single-channel
  Speech Enhancement

  We apply a fast kernel method for mask-based single-channel speech
enhancement. Specifically, our method solves a kernel regression problem
associated to a non-smooth kernel function (exponential power kernel) with a
highly efficient iterative method (EigenPro). Due to the simplicity of this
method, its hyper-parameters such as kernel bandwidth can be automatically and
efficiently selected using line search with subsamples of training data. We
observe an empirical correlation between the regression loss (mean square
error) and regular metrics for speech enhancement. This observation justifies
our training target and motivates us to achieve lower regression loss by
training separate kernel model per frequency subband. We compare our method
with the state-of-the-art deep neural networks on mask-based HINT and TIMIT.
Experimental results show that our kernel method consistently outperforms deep
neural networks while requiring less training time.


On exponential convergence of SGD in non-convex over-parametrized
  learning

  Large over-parametrized models learned via stochastic gradient descent (SGD)
methods have become a key element in modern machine learning. Although SGD
methods are very effective in practice, most theoretical analyses of SGD
suggest slower convergence than what is empirically observed. In our recent
work [8] we analyzed how interpolation, common in modern over-parametrized
learning, results in exponential convergence of SGD with constant step size for
convex loss functions. In this note, we extend those results to a much broader
non-convex function class satisfying the Polyak-Lojasiewicz (PL) condition. A
number of important non-convex problems in machine learning, including some
classes of neural networks, have been recently shown to satisfy the PL
condition. We argue that the PL condition provides a relevant and attractive
setting for many machine learning problems, particularly in the
over-parametrized regime.


Purcell enhancement of the parametric down-conversion in two-dimensional
  nonlinear materials

  Ultracompact nonlinear optical devices utilizing two-dimensional (2D)
materials and nanostructures are emerging as important elements of photonic
circuits. Integration of the nonlinear material into a subwavelength cavity or
waveguide leads to a strong Purcell enhancement of the nonlinear processes and
compensates for a small interaction volume. The generic feature of such devices
which makes them especially challenging for analysis is strong dissipation of
both the nonlinear polarization and highly confined modes of a subwavelength
cavity. Here we solve a quantum-electrodynamic problem of the spontaneous and
stimulated parametric down-conversion in a nonlinear quasi-2D waveguide or
cavity. We develop a rigorous Heisenberg-Langevin approach which includes
dissipation and fluctuations in the electron ensemble and in the
electromagnetic field of a cavity on equal footing. Within a relatively simple
model, we take into account the nonlinear coupling of the quantized cavity
modes, their interaction with a dissipative reservoir and the outside world,
amplification of thermal noise and zero-point fluctuations of the
electromagnetic field, and other relevant effects. We derive closed-form
analytic results for relevant quantities such as the spontaneous parametric
signal power and the threshold for parametric instability. We find a strong
reduction in the parametric instability threshold for 2D nonlinear materials in
a subwavelength cavity and provide a comparison with conventional nonlinear
photonic devices.


Data spectroscopy: Eigenspaces of convolution operators and clustering

  This paper focuses on obtaining clustering information about a distribution
from its i.i.d. samples. We develop theoretical results to understand and use
clustering information contained in the eigenvectors of data adjacency matrices
based on a radial kernel function with a sufficiently fast tail decay. In
particular, we provide population analyses to gain insights into which
eigenvectors should be used and when the clustering information for the
distribution can be recovered from the sample. We learn that a fixed number of
top eigenvectors might at the same time contain redundant clustering
information and miss relevant clustering information. We use this insight to
design the data spectroscopic clustering (DaSpec) algorithm that utilizes
properly selected eigenvectors to determine the number of clusters
automatically and to group the data accordingly. Our findings extend the
intuitions underlying existing spectral techniques such as spectral clustering
and Kernel Principal Components Analysis, and provide new understanding into
their usability and modes of failure. Simulation studies and experiments on
real-world data are conducted to show the potential of our algorithm. In
particular, DaSpec is found to handle unbalanced groups and recover clusters of
different shapes better than the competing methods.


Learning Gaussian Mixtures with Arbitrary Separation

  In this paper we present a method for learning the parameters of a mixture of
$k$ identical spherical Gaussians in $n$-dimensional space with an arbitrarily
small separation between the components. Our algorithm is polynomial in all
parameters other than $k$. The algorithm is based on an appropriate grid search
over the space of parameters. The theoretical analysis of the algorithm hinges
on a reduction of the problem to 1 dimension and showing that two 1-dimensional
mixtures whose densities are close in the $L^2$ norm must have similar means
and mixing coefficients. To produce such a lower bound for the $L^2$ norm in
terms of the distances between the corresponding means, we analyze the behavior
of the Fourier transform of a mixture of Gaussians in 1 dimension around the
origin, which turns out to be closely related to the properties of the
Vandermonde matrix obtained from the component means. Analysis of this matrix
together with basic function approximation results allows us to provide a lower
bound for the norm of the mixture in the Fourier domain.
  In recent years much research has been aimed at understanding the
computational aspects of learning parameters of Gaussians mixture distributions
in high dimension. To the best of our knowledge all existing work on learning
parameters of Gaussian mixtures assumes minimum separation between components
of the mixture which is an increasing function of either the dimension of the
space $n$ or the number of components $k$. In our paper we prove the first
result showing that parameters of a $n$-dimensional Gaussian mixture model with
arbitrarily small component separation can be learned in time polynomial in
$n$.


Blind Signal Separation in the Presence of Gaussian Noise

  A prototypical blind signal separation problem is the so-called cocktail
party problem, with n people talking simultaneously and n different microphones
within a room. The goal is to recover each speech signal from the microphone
inputs. Mathematically this can be modeled by assuming that we are given
samples from an n-dimensional random variable X=AS, where S is a vector whose
coordinates are independent random variables corresponding to each speaker. The
objective is to recover the matrix A^{-1} given random samples from X. A range
of techniques collectively known as Independent Component Analysis (ICA) have
been proposed to address this problem in the signal processing and machine
learning literature. Many of these techniques are based on using the kurtosis
or other cumulants to recover the components.
  In this paper we propose a new algorithm for solving the blind signal
separation problem in the presence of additive Gaussian noise, when we are
given samples from X=AS+\eta, where \eta is drawn from an unknown, not
necessarily spherical n-dimensional Gaussian distribution. Our approach is
based on a method for decorrelating a sample with additive Gaussian noise under
the assumption that the underlying distribution is a linear transformation of a
distribution with independent components. Our decorrelation routine is based on
the properties of cumulant tensors and can be combined with any standard
cumulant-based method for ICA to get an algorithm that is provably robust in
the presence of Gaussian noise. We derive polynomial bounds for the sample
complexity and error propagation of our method.


Laplacian Support Vector Machines Trained in the Primal

  In the last few years, due to the growing ubiquity of unlabeled data, much
effort has been spent by the machine learning community to develop better
understanding and improve the quality of classifiers exploiting unlabeled data.
Following the manifold regularization approach, Laplacian Support Vector
Machines (LapSVMs) have shown the state of the art performance in
semi--supervised classification. In this paper we present two strategies to
solve the primal LapSVM problem, in order to overcome some issues of the
original dual formulation. Whereas training a LapSVM in the dual requires two
steps, using the primal form allows us to collapse training to a single step.
Moreover, the computational complexity of the training algorithm is reduced
from O(n^3) to O(n^2) using preconditioned conjugate gradient, where n is the
combined number of labeled and unlabeled examples. We speed up training by
using an early stopping strategy based on the prediction on unlabeled data or,
if available, on labeled validation examples. This allows the algorithm to
quickly compute approximate solutions with roughly the same classification
accuracy as the optimal ones, considerably reducing the training time. Due to
its simplicity, training LapSVM in the primal can be the starting point for
additional enhancements of the original LapSVM formulation, such as those for
dealing with large datasets. We present an extensive experimental evaluation on
real world data showing the benefits of the proposed approach.


Eigenvectors of Orthogonally Decomposable Functions

  The Eigendecomposition of quadratic forms (symmetric matrices) guaranteed by
the spectral theorem is a foundational result in applied mathematics. Motivated
by a shared structure found in inferential problems of recent interest---namely
orthogonal tensor decompositions, Independent Component Analysis (ICA), topic
models, spectral clustering, and Gaussian mixture learning---we generalize the
eigendecomposition from quadratic forms to a broad class of "orthogonally
decomposable" functions. We identify a key role of convexity in our extension,
and we generalize two traditional characterizations of eigenvectors: First, the
eigenvectors of a quadratic form arise from the optima structure of the
quadratic form on the sphere. Second, the eigenvectors are the fixed points of
the power iteration.
  In our setting, we consider a simple first order generalization of the power
method which we call gradient iteration. It leads to efficient and easily
implementable methods for basis recovery. It includes influential Machine
Learning methods such as cumulant-based FastICA and the tensor power iteration
for orthogonally decomposable tensors as special cases.
  We provide a complete theoretical analysis of gradient iteration using the
structure theory of discrete dynamical systems to show almost sure convergence
and fast (super-linear) convergence rates. The analysis also extends to the
case when the observed function is only approximately orthogonally
decomposable, with bounds that are polynomial in dimension and other relevant
parameters, such as perturbation size. Our perturbation results can be
considered as a non-linear version of the classical Davis-Kahan theorem for
perturbations of eigenvectors of symmetric matrices.


A Pseudo-Euclidean Iteration for Optimal Recovery in Noisy ICA

  Independent Component Analysis (ICA) is a popular model for blind signal
separation. The ICA model assumes that a number of independent source signals
are linearly mixed to form the observed signals. We propose a new algorithm,
PEGI (for pseudo-Euclidean Gradient Iteration), for provable model recovery for
ICA with Gaussian noise. The main technical innovation of the algorithm is to
use a fixed point iteration in a pseudo-Euclidean (indefinite "inner product")
space. The use of this indefinite "inner product" resolves technical issues
common to several existing algorithms for noisy ICA. This leads to an algorithm
which is conceptually simple, efficient and accurate in testing.
  Our second contribution is combining PEGI with the analysis of objectives for
optimal recovery in the noisy ICA model. It has been observed that the direct
approach of demixing with the inverse of the mixing matrix is suboptimal for
signal recovery in terms of the natural Signal to Interference plus Noise Ratio
(SINR) criterion. There have been several partial solutions proposed in the ICA
literature. It turns out that any solution to the mixing matrix reconstruction
problem can be used to construct an SINR-optimal ICA demixing, despite the fact
that SINR itself cannot be computed from data. That allows us to obtain a
practical and provably SINR-optimal recovery method for ICA with arbitrary
Gaussian noise.


Beyond Hartigan Consistency: Merge Distortion Metric for Hierarchical
  Clustering

  Hierarchical clustering is a popular method for analyzing data which
associates a tree to a dataset. Hartigan consistency has been used extensively
as a framework to analyze such clustering algorithms from a statistical point
of view. Still, as we show in the paper, a tree which is Hartigan consistent
with a given density can look very different than the correct limit tree.
Specifically, Hartigan consistency permits two types of undesirable
configurations which we term over-segmentation and improper nesting. Moreover,
Hartigan consistency is a limit property and does not directly quantify
difference between trees.
  In this paper we identify two limit properties, separation and minimality,
which address both over-segmentation and improper nesting and together imply
(but are not implied by) Hartigan consistency. We proceed to introduce a merge
distortion metric between hierarchical clusterings and show that convergence in
our distance implies both separation and minimality. We also prove that uniform
separation and minimality imply convergence in the merge distortion metric.
Furthermore, we show that our merge distortion metric is stable under
perturbations of the density.
  Finally, we demonstrate applicability of these concepts by proving
convergence results for two clustering algorithms. First, we show convergence
(and hence separation and minimality) of the recent robust single linkage
algorithm of Chaudhuri and Dasgupta (2010). Second, we provide convergence
results on manifolds for topological split tree clustering.


Experimental Demonstration of Phase Modulation and Motion Sensing Using
  Graphene-Integrated Metasurfaces

  Plasmonic metasurfaces are able to modify the wavefront by altering the light
intensity, phase and polarization state. Active plasmonic metasurfaces would
allow dynamic modulation of the wavefront which give rise to interesting
application such as beam-steering, holograms and tunable waveplates. Graphene
is an interesting material with dynamic property which can be controlled by
electrical gating at an ultra-fast speed. We use a graphene-integrated
metasurface to induce a tunable phase change to the wavefront. The metasurface
supports a Fano resonance which produces high-quality resonances around 7.7
microns. The phase change is measured using a Michleson interferometry setup.
It is shown that the reflection phase can change up to 55 degrees. In
particular the phase can change by 28 degrees while the amplitude is nearly
constant. The anisotropic optical response of the metasurface is used to
modulate the ellipticity of the reflected wave in response to an incident field
at 45 degree. We show a proof of concept application of our system in
potentially ultra-fast laser interferometry with sub-micron accuracy.


Diving into the shallows: a computational perspective on large-scale
  shallow learning

  In this paper we first identify a basic limitation in gradient descent-based
optimization methods when used in conjunctions with smooth kernels. An analysis
based on the spectral properties of the kernel demonstrates that only a
vanishingly small portion of the function space is reachable after a polynomial
number of gradient descent iterations. This lack of approximating power
drastically limits gradient descent for a fixed computational budget leading to
serious over-regularization/underfitting. The issue is purely algorithmic,
persisting even in the limit of infinite data.
  To address this shortcoming in practice, we introduce EigenPro iteration,
based on a preconditioning scheme using a small number of approximately
computed eigenvectors. It can also be viewed as learning a new kernel optimized
for gradient descent. It turns out that injecting this small (computationally
inexpensive and SGD-compatible) amount of approximate second-order information
leads to major improvements in convergence. For large data, this translates
into significant performance boost over the standard kernel methods. In
particular, we are able to consistently match or improve the state-of-the-art
results recently reported in the literature with a small fraction of their
computational budget.
  Finally, we feel that these results show a need for a broader computational
perspective on modern large-scale learning to complement more traditional
statistical and convergence analyses. In particular, many phenomena of
large-scale high-dimensional inference are best understood in terms of
optimization on infinite dimensional Hilbert spaces, where standard algorithms
can sometimes have properties at odds with finite-dimensional intuition. A
systematic analysis concentrating on the approximation power of such algorithms
within a budget of computation may lead to progress both in theory and
practice.


The Power of Interpolation: Understanding the Effectiveness of SGD in
  Modern Over-parametrized Learning

  In this paper we aim to formally explain the phenomenon of fast convergence
of SGD observed in modern machine learning. The key observation is that most
modern learning architectures are over-parametrized and are trained to
interpolate the data by driving the empirical loss (classification and
regression) close to zero. While it is still unclear why these interpolated
solutions perform well on test data, we show that these regimes allow for fast
convergence of SGD, comparable in number of iterations to full gradient
descent.
  For convex loss functions we obtain an exponential convergence bound for {\it
mini-batch} SGD parallel to that for full gradient descent. We show that there
is a critical batch size $m^*$ such that: (a) SGD iteration with mini-batch
size $m\leq m^*$ is nearly equivalent to $m$ iterations of mini-batch size $1$
(\emph{linear scaling regime}). (b) SGD iteration with mini-batch $m> m^*$ is
nearly equivalent to a full gradient descent iteration (\emph{saturation
regime}).
  Moreover, for the quadratic loss, we derive explicit expressions for the
optimal mini-batch and step size and explicitly characterize the two regimes
above. The critical mini-batch size can be viewed as the limit for effective
mini-batch parallelization. It is also nearly independent of the data size,
implying $O(n)$ acceleration over GD per unit of computation. We give
experimental evidence on real data which closely follows our theoretical
analyses.
  Finally, we show how our results fit in the recent developments in training
deep neural networks and discuss connections to adaptive rates for SGD and
variance reduction.


Fast Interactive Image Retrieval using large-scale unlabeled data

  An interactive image retrieval system learns which images in the database
belong to a user's query concept, by analyzing the example images and feedback
provided by the user. The challenge is to retrieve the relevant images with
minimal user interaction. In this work, we propose to solve this problem by
posing it as a binary classification task of classifying all images in the
database as being relevant or irrelevant to the user's query concept. Our
method combines active learning with graph-based semi-supervised learning
(GSSL) to tackle this problem. Active learning reduces the number of user
interactions by querying the labels of the most informative points and GSSL
allows to use abundant unlabeled data along with the limited labeled data
provided by the user. To efficiently find the most informative point, we use an
uncertainty sampling based method that queries the label of the point nearest
to the decision boundary of the classifier. We estimate this decision boundary
using our heuristic of adaptive threshold. To utilize huge volumes of unlabeled
data we use an efficient approximation based method that reduces the complexity
of GSSL from $O(n^3)$ to $O(n)$, making GSSL scalable. We make the classifier
robust to the diversity and noisy labels associated with images in large
databases by incorporating information from multiple modalities such as visual
information extracted from deep learning based models and semantic information
extracted from the WordNet. High F1 scores within few relevance feedback rounds
in our experiments with concepts defined on AnimalWithAttributes and Imagenet
(1.2 million images) datasets indicate the effectiveness and scalability of our
approach.


Unveiling spectral purity and tunability of terahertz quantum cascade
  laser sources based on intra-cavity difference frequency generation

  Terahertz sources based on intra-cavity difference-frequency generation in
mid-infrared quantum cascade lasers (THz DFG-QCLs) have recently emerged as the
first monolithic electrically-pumped semiconductor sources capable of operating
at room-temperature (RT) across the 1-6 THz range. Despite tremendous progress
in power output, that now exceeds 1mW in pulsed and 10 {\mu}W in
continuous-wave regime at room-temperature, knowledge of the major figure of
merits of these devices for high precision spectroscopy, such as spectral
purity and absolute frequency tunability, is still lacking. Here, by exploiting
a metrological grade system comprising a terahertz frequency comb synthesizer,
we measure, for the first time, the free-running emission linewidth (LW), the
tuning characteristics, and the absolute frequency of individual emission lines
of these sources with an uncertainty of 4 x 10-10. The unveiled emission LW
(400 kHz at 1ms integration time) indicates that DFG-QCLs are well suited to
operate as local oscillators and to be used for a variety of metrological,
spectroscopic, communication, and imaging applications requiring
narrow-linewidth THz sources.


Overfitting or perfect fitting? Risk bounds for classification and
  regression rules that interpolate

  Many modern machine learning models are trained to achieve zero or near-zero
training error in order to obtain near-optimal (but non-zero) test error. This
phenomenon of strong generalization performance for "overfitted" / interpolated
classifiers appears to be ubiquitous in high-dimensional data, having been
observed in deep networks, kernel machines, boosting and random forests. Their
performance is consistently robust even when the data contain large amounts of
label noise.
  Very little theory is available to explain these observations. The vast
majority of theoretical analyses of generalization allows for interpolation
only when there is little or no label noise. This paper takes a step toward a
theoretical foundation for interpolated classifiers by analyzing local
interpolating schemes, including geometric simplicial interpolation algorithm
and singularly weighted $k$-nearest neighbor schemes. Consistency or
near-consistency is proved for these schemes in classification and regression
problems. Moreover, the nearest neighbor schemes exhibit optimal rates under
some standard statistical assumptions.
  Finally, this paper suggests a way to explain the phenomenon of adversarial
examples, which are seemingly ubiquitous in modern machine learning, and also
discusses some connections to kernel machines and random forests in the
interpolated regime.


Kernel machines that adapt to GPUs for effective large batch training

  Modern machine learning models are typically trained using Stochastic
Gradient Descent (SGD) on massively parallel computing resources such as GPUs.
Increasing mini-batch size is a simple and direct way to utilize the parallel
computing capacity. For small batch an increase in batch size results in the
proportional reduction in the training time, a phenomenon known as linear
scaling. However, increasing batch size beyond a certain value leads to no
further improvement in training time. In this paper we develop the first
analytical framework that extends linear scaling to match the parallel
computing capacity of a resource. The framework is designed for a class of
classical kernel machines. It automatically modifies a standard kernel machine
to output a mathematically equivalent prediction function, yet allowing for
extended linear scaling, i.e., higher effective parallelization and faster
training time on given hardware.
  The resulting algorithms are accurate, principled and very fast. For example,
using a single Titan Xp GPU, training on ImageNet with $1.3\times 10^6$ data
points and $1000$ labels takes under an hour, while smaller datasets, such as
MNIST, take seconds. As the parameters are chosen analytically, based on the
theoretical bounds, little tuning beyond selecting the kernel and the kernel
parameter is needed, further facilitating the practical use of these methods.


Reconciling modern machine learning and the bias-variance trade-off

  The question of generalization in machine learning---how algorithms are able
to learn predictors from a training sample to make accurate predictions
out-of-sample---is revisited in light of the recent breakthroughs in modern
machine learning technology.
  The classical approach to understanding generalization is based on
bias-variance trade-offs, where model complexity is carefully calibrated so
that the fit on the training sample reflects performance out-of-sample.
  However, it is now common practice to fit highly complex models like deep
neural networks to data with (nearly) zero training error, and yet these
interpolating predictors are observed to have good out-of-sample accuracy even
for noisy data.
  How can the classical understanding of generalization be reconciled with
these observations from modern machine learning practice?
  In this paper, we bridge the two regimes by exhibiting a new "double descent"
risk curve that extends the traditional U-shaped bias-variance curve beyond the
point of interpolation.
  Specifically, the curve shows that as soon as the model complexity is high
enough to achieve interpolation on the training sample---a point that we call
the "interpolation threshold"---the risk of suitably chosen interpolating
predictors from these models can, in fact, be decreasing as the model
complexity increases, often below the risk achieved using non-interpolating
models.
  The double descent risk curve is demonstrated for a broad range of models,
including neural networks and random forests, and a mechanism for producing
this behavior is posited.


Graph Laplacians on Singular Manifolds: Toward understanding complex
  spaces: graph Laplacians on manifolds with singularities and boundaries

  Recently, much of the existing work in manifold learning has been done under
the assumption that the data is sampled from a manifold without boundaries and
singularities or that the functions of interest are evaluated away from such
points. At the same time, it can be argued that singularities and boundaries
are an important aspect of the geometry of realistic data.
  In this paper we consider the behavior of graph Laplacians at points at or
near boundaries and two main types of other singularities: intersections, where
different manifolds come together and sharp "edges", where a manifold sharply
changes direction. We show that the behavior of graph Laplacian near these
singularities is quite different from that in the interior of the manifolds. In
fact, a phenomenon somewhat reminiscent of the Gibbs effect in the analysis of
Fourier series, can be observed in the behavior of graph Laplacian near such
points. Unlike in the interior of the domain, where graph Laplacian converges
to the Laplace-Beltrami operator, near singularities graph Laplacian tends to a
first-order differential operator, which exhibits different scaling behavior as
a function of the kernel width. One important implication is that while points
near the singularities occupy only a small part of the total volume, the
difference in scaling results in a disproportionately large contribution to the
total behavior. Another significant finding is that while the scaling behavior
of the operator is the same near different types of singularities, they are
very distinct at a more refined level of analysis.
  We believe that a comprehensive understanding of these structures in addition
to the standard case of a smooth manifold can take us a long way toward better
methods for analysis of complex non-linear data and can lead to significant
progress in algorithm design.


Inverse Density as an Inverse Problem: The Fredholm Equation Approach

  In this paper we address the problem of estimating the ratio $\frac{q}{p}$
where $p$ is a density function and $q$ is another density, or, more generally
an arbitrary function. Knowing or approximating this ratio is needed in various
problems of inference and integration, in particular, when one needs to average
a function with respect to one probability distribution, given a sample from
another. It is often referred as {\it importance sampling} in statistical
inference and is also closely related to the problem of {\it covariate shift}
in transfer learning as well as to various MCMC methods. It may also be useful
for separating the underlying geometry of a space, say a manifold, from the
density function defined on it.
  Our approach is based on reformulating the problem of estimating
$\frac{q}{p}$ as an inverse problem in terms of an integral operator
corresponding to a kernel, and thus reducing it to an integral equation, known
as the Fredholm problem of the first kind. This formulation, combined with the
techniques of regularization and kernel methods, leads to a principled
kernel-based framework for constructing algorithms and for analyzing them
theoretically.
  The resulting family of algorithms (FIRE, for Fredholm Inverse Regularized
Estimator) is flexible, simple and easy to implement.
  We provide detailed theoretical analysis including concentration bounds and
convergence rates for the Gaussian kernel in the case of densities defined on
$\R^d$, compact domains in $\R^d$ and smooth $d$-dimensional sub-manifolds of
the Euclidean space.
  We also show experimental results including applications to classification
and semi-supervised learning within the covariate shift framework and
demonstrate some encouraging experimental comparisons. We also show how the
parameters of our algorithms can be chosen in a completely unsupervised manner.


The More, the Merrier: the Blessing of Dimensionality for Learning Large
  Gaussian Mixtures

  In this paper we show that very large mixtures of Gaussians are efficiently
learnable in high dimension. More precisely, we prove that a mixture with known
identical covariance matrices whose number of components is a polynomial of any
fixed degree in the dimension n is polynomially learnable as long as a certain
non-degeneracy condition on the means is satisfied. It turns out that this
condition is generic in the sense of smoothed complexity, as soon as the
dimensionality of the space is high enough. Moreover, we prove that no such
condition can possibly exist in low dimension and the problem of learning the
parameters is generically hard. In contrast, much of the existing work on
Gaussian Mixtures relies on low-dimensional projections and thus hits an
artificial barrier. Our main result on mixture recovery relies on a new
"Poissonization"-based technique, which transforms a mixture of Gaussians to a
linear map of a product distribution. The problem of learning this map can be
efficiently solved using some recent results on tensor decompositions and
Independent Component Analysis (ICA), thus giving an algorithm for recovering
the mixture. In addition, we combine our low-dimensional hardness results for
Gaussian mixtures with Poissonization to show how to embed difficult instances
of low-dimensional Gaussian mixtures into the ICA setting, thus establishing
exponential information-theoretic lower bounds for underdetermined ICA in low
dimension. To the best of our knowledge, this is the first such result in the
literature. In addition to contributing to the problem of Gaussian mixture
learning, we believe that this work is among the first steps toward better
understanding the rare phenomenon of the "blessing of dimensionality" in the
computational aspects of statistical inference.


To understand deep learning we need to understand kernel learning

  Generalization performance of classifiers in deep learning has recently
become a subject of intense study. Deep models, typically over-parametrized,
tend to fit the training data exactly. Despite this "overfitting", they perform
well on test data, a phenomenon not yet fully understood.
  The first point of our paper is that strong performance of overfitted
classifiers is not a unique feature of deep learning. Using six real-world and
two synthetic datasets, we establish experimentally that kernel machines
trained to have zero classification or near zero regression error perform very
well on test data, even when the labels are corrupted with a high level of
noise. We proceed to give a lower bound on the norm of zero loss solutions for
smooth kernels, showing that they increase nearly exponentially with data size.
We point out that this is difficult to reconcile with the existing
generalization bounds. Moreover, none of the bounds produce non-trivial results
for interpolating solutions.
  Second, we show experimentally that (non-smooth) Laplacian kernels easily fit
random labels, a finding that parallels results for ReLU neural networks. In
contrast, fitting noisy data requires many more epochs for smooth Gaussian
kernels. Similar performance of overfitted Laplacian and Gaussian classifiers
on test, suggests that generalization is tied to the properties of the kernel
function rather than the optimization process.
  Certain key phenomena of deep learning are manifested similarly in kernel
methods in the modern "overfitted" regime. The combination of the experimental
and theoretical results presented in this paper indicates a need for new
theoretical ideas for understanding properties of classical kernel methods. We
argue that progress on understanding deep learning will be difficult until more
tractable "shallow" kernel methods are better understood.


