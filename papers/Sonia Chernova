Interactive Policy Learning through Confidence-Based Autonomy

  We present Confidence-Based Autonomy (CBA), an interactive algorithm for
policy learning from demonstration. The CBA algorithm consists of two
components which take advantage of the complimentary abilities of humans and
computer agents. The first component, Confident Execution, enables the agent to
identify states in which demonstration is required, to request a demonstration
from the human teacher and to learn a policy based on the acquired data. The
algorithm selects demonstrations based on a measure of action selection
confidence, and our results show that using Confident Execution the agent
requires fewer demonstrations to learn the policy than when demonstrations are
selected by a human teacher. The second algorithmic component, Corrective
Demonstration, enables the teacher to correct any mistakes made by the agent
through additional demonstrations in order to improve the policy and future
task performance. CBA and its individual components are compared and evaluated
in a complex simulated driving domain. The complete CBA algorithm results in
the best overall learning performance, successfully reproducing the behavior of
the teacher while balancing the tradeoff between number of demonstrations and
number of incorrect actions during learning.


Effects of Interruptibility-Aware Robot Behavior

  As robots become increasingly prevalent in human environments, there will
inevitably be times when a robot needs to interrupt a human to initiate an
interaction. Our work introduces the first interruptibility-aware mobile robot
system, and evaluates the effects of interruptibility-awareness on human task
performance, robot task performance, and on human interpretation of the robot's
social aptitude. Our results show that our robot is effective at predicting
interruptibility at high accuracy, allowing it to interrupt at more appropriate
times. Results of a large-scale user study show that while participants are
able to maintain task performance even in the presence of interruptions,
interruptibility-awareness improves the robot's task performance and improves
participant social perception of the robot.


Action Categorization for Computationally Improved Task Learning and
  Planning

  This paper explores the problem of task learning and planning, contributing
the Action-Category Representation (ACR) to improve computational performance
of both Planning and Reinforcement Learning (RL). ACR is an algorithm-agnostic,
abstract data representation that maps objects to action categories (groups of
actions), inspired by the psychological concept of action codes. We validate
our approach in StarCraft and Lightworld domains; our results demonstrate
several benefits of ACR relating to improved computational performance of
planning and RL, by reducing the action space for the agent.


Tool Macgyvering: Tool Construction Using Geometric Reasoning

  MacGyvering is defined as creating or repairing something in an inventive or
improvised way by utilizing objects that are available at hand. In this paper,
we explore a subset of Macgyvering problems involving tool construction, i.e.,
creating tools from parts available in the environment. We formalize the
overall problem domain of tool Macgyvering, introducing three levels of
complexity for tool construction and substitution problems, and presenting a
novel computational framework aimed at solving one level of the tool
Macgyvering problem, specifically contributing a novel algorithm for tool
construction based on geometric reasoning. We validate our approach by
constructing three tools using a 7-DOF robot arm.


Situated Structure Learning of a Bayesian Logic Network for Commonsense
  Reasoning

  This paper details the implementation of an algorithm for automatically
generating a high-level knowledge network to perform commonsense reasoning,
specifically with the application of robotic task repair. The network is
represented using a Bayesian Logic Network (BLN) (Jain, Waldherr, and Beetz
2009), which combines a set of directed relations between abstract concepts,
including IsA, AtLocation, HasProperty, and UsedFor, with a corresponding
probability distribution that models the uncertainty inherent in these
relations. Inference over this network enables reasoning over the abstract
concepts in order to perform appropriate object substitution or to locate
missing objects in the robot's environment. The structure of the network is
generated by combining information from two existing knowledge sources:
ConceptNet (Speer and Havasi 2012), and WordNet (Miller 1995). This is done in
a "situated" manner by only including information relevant a given context.
Results show that the generated network is able to accurately predict object
categories, locations, properties, and affordances in three different household
scenarios.


Learning Generalizable Robot Skills from Demonstrations in Cluttered
  Environments

  Learning from Demonstration (LfD) is a popular approach to endowing robots
with skills without having to program them by hand. Typically, LfD relies on
human demonstrations in clutter-free environments. This prevents the
demonstrations from being affected by irrelevant objects, whose influence can
obfuscate the true intention of the human or the constraints of the desired
skill. However, it is unrealistic to assume that the robot's environment can
always be restructured to remove clutter when capturing human demonstrations.
To contend with this problem, we develop an importance weighted batch and
incremental skill learning approach, building on a recent inference-based
technique for skill representation and reproduction. Our approach reduces
unwanted environmental influences on the learned skill, while still capturing
the salient human behavior. We provide both batch and incremental versions of
our approach and validate our algorithms on a 7-DOF JACO2 manipulator with
reaching and placing skills.


Unbiasing Semantic Segmentation For Robot Perception using Synthetic
  Data Feature Transfer

  Robot perception systems need to perform reliable image segmentation in
real-time on noisy, raw perception data. State-of-the-art segmentation
approaches use large CNN models and carefully constructed datasets; however,
these models focus on accuracy at the cost of real-time inference. Furthermore,
the standard semantic segmentation datasets are not large enough for training
CNNs without augmentation and are not representative of noisy, uncurated robot
perception data. We propose improving the performance of real-time segmentation
frameworks on robot perception data by transferring features learned from
synthetic segmentation data. We show that pretraining real-time segmentation
architectures with synthetic segmentation data instead of ImageNet improves
fine-tuning performance by reducing the bias learned in pretraining and closing
the \textit{transfer gap} as a result. Our experiments show that our real-time
robot perception models pretrained on synthetic data outperform those
pretrained on ImageNet for every scale of fine-tuning data examined. Moreover,
the degree to which synthetic pretraining outperforms ImageNet pretraining
increases as the availability of robot data decreases, making our approach
attractive for robotics domains where dataset collection is hard and/or
expensive.


RoboCSE: Robot Common Sense Embedding

  Autonomous service robots require computational frameworks that allow them to
generalize knowledge to new situations in a manner that models uncertainty
while scaling to real-world problem sizes. The Robot Common Sense Embedding
(RoboCSE) showcases a class of computational frameworks, multi-relational
embeddings, that have not been leveraged in robotics to model semantic
knowledge. We validate RoboCSE on a realistic home environment simulator
(AI2Thor) to measure how well it generalizes learned knowledge about object
affordances, locations, and materials. Our experiments show that RoboCSE can
perform prediction better than a baseline that uses pre-trained embeddings,
such as Word2Vec, achieving statistically significant improvements while using
orders of magnitude less memory than our Bayesian Logic Network baseline. In
addition, we show that predictions made by RoboCSE are robust to significant
reductions in data available for training as well as domain transfer to
MatterPort3D, achieving statistically significant improvements over a baseline
that memorizes training data.


STRATA: A Unified Framework for Task Assignments in Large Teams of
  Heterogeneous Robots

  Large teams of robots have the potential to solve complex multi-task problems
that are intractable for a single robot working independently. However, solving
complex multi-task problems requires leveraging the relative strengths of
different robots in the team. We present Stochastic TRAit-based Task Assignment
(STRATA), a unified framework that models large teams of heterogeneous robots
and performs optimal task assignments. Specifically, given information on which
traits (capabilities) are required for various tasks, STRATA computes the
optimal assignments of robots to tasks such that the task-trait requirements
are achieved. Inspired by prior work in robot swarms and biodiversity, we
categorize robots into different species (groups) based on their traits. We
model each trait as a continuous variable and differentiate between traits that
can and cannot be aggregated from different robots. STRATA is capable of
reasoning about both species-level and robot-level differences in traits.
Further, we define measures of diversity for any given team based on the team's
continuous-space trait model. We illustrate the necessity and effectiveness of
STRATA using detailed simulations and a capture the flag game environment.


Skill Acquisition via Automated Multi-Coordinate Cost Balancing

  We propose a learning framework, named Multi-Coordinate Cost Balancing
(MCCB), to address the problem of acquiring point-to-point movement skills from
demonstrations. MCCB encodes demonstrations simultaneously in multiple
differential coordinates that specify local geometric properties. MCCB
generates reproductions by solving a convex optimization problem with a
multi-coordinate cost function and linear constraints on the reproductions,
such as initial, target, and via points. Further, since the relative importance
of each coordinate system in the cost function might be unknown for a given
skill, MCCB learns optimal weighting factors that balance the cost function. We
demonstrate the effectiveness of MCCB via detailed experiments conducted on one
handwriting dataset and three complex skill datasets.


Semi-Supervised Haptic Material Recognition for Robots using Generative
  Adversarial Networks

  Material recognition enables robots to incorporate knowledge of material
properties into their interactions with everyday objects. For example, material
recognition opens up opportunities for clearer communication with a robot, such
as "bring me the metal coffee mug", and recognizing plastic versus metal is
crucial when using a microwave or oven. However, collecting labeled training
data with a robot is often more difficult than unlabeled data. We present a
semi-supervised learning approach for material recognition that uses generative
adversarial networks (GANs) with haptic features such as force, temperature,
and vibration. Our approach achieves state-of-the-art results and enables a
robot to estimate the material class of household objects with ~90% accuracy
when 92% of the training data are unlabeled. We explore how well this approach
can recognize the material of new objects and we discuss challenges facing
generalization. To motivate learning from unlabeled training data, we also
compare results against several common supervised learning classifiers. In
addition, we have released the dataset used for this work which consists of
time-series haptic measurements from a robot that conducted thousands of
interactions with 72 household objects.


Classification of Household Materials via Spectroscopy

  Recognizing an object's material can inform a robot on the object's fragility
or appropriate use. To estimate an object's material during manipulation, many
prior works have explored the use of haptic sensing. In this paper, we explore
a technique for robots to estimate the materials of objects using spectroscopy.
We demonstrate that spectrometers provide several benefits for material
recognition, including fast response times and accurate measurements with low
noise. Furthermore, spectrometers do not require direct contact with an object.
To explore this, we collected a dataset of spectral measurements from two
commercially available spectrometers during which a robotic platform interacted
with 50 flat material objects, and we show that a neural network model can
accurately analyze these measurements. Due to the similarity between
consecutive spectral measurements, our model achieved a material classification
accuracy of 94.6% when given only one spectral sample per object. Similar to
prior works with haptic sensors, we found that generalizing material
recognition to new objects posed a greater challenge, for which we achieved an
accuracy of 79.1% via leave-one-object-out cross-validation. Finally, we
demonstrate how a PR2 robot can leverage spectrometers to estimate the
materials of everyday objects found in the home. From this work, we find that
spectroscopy poses a promising approach for material classification during
robotic manipulation.


