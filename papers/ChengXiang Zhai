Noun-Phrase Analysis in Unrestricted Text for Information Retrieval

  Information retrieval is an important application area of natural-language
processing where one encounters the genuine challenge of processing large
quantities of unrestricted natural-language text. This paper reports on the
application of a few simple, yet robust and efficient noun-phrase analysis
techniques to create better indexing phrases for information retrieval. In
particular, we describe a hybrid approach to the extraction of meaningful
(continuous or discontinuous) subcompounds from complex noun phrases using both
corpus statistics and linguistic heuristics. Results of experiments show that
indexing based on such extracted subcompounds improves both recall and
precision in an information retrieval system. The noun-phrase analysis
techniques are also potentially useful for book indexing and automatic
thesaurus extraction.


Exploiting Context to Identify Lexical Atoms -- A Statistical View of
  Linguistic Context

  Interpretation of natural language is inherently context-sensitive. Most
words in natural language are ambiguous and their meanings are heavily
dependent on the linguistic context in which they are used. The study of
lexical semantics can not be separated from the notion of context. This paper
takes a contextual approach to lexical semantics and studies the linguistic
context of lexical atoms, or "sticky" phrases such as "hot dog". Since such
lexical atoms may occur frequently in unrestricted natural language text,
recognizing them is crucial for understanding naturally-occurring text. The
paper proposes several heuristic approaches to exploiting the linguistic
context to identify lexical atoms from arbitrary natural language text.


Fast Statistical Parsing of Noun Phrases for Document Indexing

  Information Retrieval (IR) is an important application area of Natural
Language Processing (NLP) where one encounters the genuine challenge of
processing large quantities of unrestricted natural language text. While much
effort has been made to apply NLP techniques to IR, very few NLP techniques
have been evaluated on a document collection larger than several megabytes.
Many NLP techniques are simply not efficient enough, and not robust enough, to
handle a large amount of text. This paper proposes a new probabilistic model
for noun phrase parsing, and reports on the application of such a parsing
technique to enhance document indexing. The effectiveness of using syntactic
phrases provided by the parser to supplement single words for indexing is
evaluated with a 250 megabytes document collection. The experiment's results
show that supplementing single words with syntactic phrases for indexing
consistently and significantly improves retrieval performance.


Numerical Facet Range Partition: Evaluation Metric and Methods

  Faceted navigation is a very useful component in today's search engines. It
is especially useful when user has an exploratory information need or prefer
certain attribute values than others. Existing work has tried to optimize
faceted systems in many aspects, but little work has been done on optimizing
numerical facet ranges (e.g., price ranges of product). In this paper, we
introduce for the first time the research problem on numerical facet range
partition and formally frame it as an optimization problem. To enable
quantitative evaluation of a partition algorithm, we propose an evaluation
metric to be applied to search engine logs. We further propose two range
partition algorithms that computationally optimize the defined metric.
Experimental results on a two-month search log from a major e-Commerce engine
show that our proposed method can significantly outperform baseline.


Identifying Compromised Accounts on Social Media Using Statistical Text
  Analysis

  Compromised social media accounts are legitimate user accounts that have been
hijacked by a third (malicious) party and can cause various kinds of damage.
Early detection of such compromised accounts is very important in order to
control the damage. In this work we propose a novel general framework for
discovering compromised accounts by utilizing statistical text analysis. The
framework is built on the observation that users will use language that is
measurably different from the language that a hacker (or spammer) would use,
when the account is compromised. We use the framework to develop specific
algorithms based on language modeling and use the similarity of language models
of users and spammers as features in a supervised learning setup to identify
compromised accounts. Evaluation results on a large Twitter corpus of over 129
million tweets show promising results of the proposed approach.


JIM: Joint Influence Modeling for Collective Search Behavior

  Previous work has shown that popular trending events are important external
factors which pose significant influence on user search behavior and also
provided a way to computationally model this influence. However, their problem
formulation was based on the strong assumption that each event poses its
influence independently. This assumption is unrealistic as there are many
correlated events in the real world which influence each other and thus, would
pose a joint influence on the user search behavior rather than posing influence
independently. In this paper, we study this novel problem of Modeling the Joint
Influences posed by multiple correlated events on user search behavior. We
propose a Joint Influence Model based on the Multivariate Hawkes Process which
captures the inter-dependency among multiple events in terms of their influence
upon user search behavior. We evaluate the proposed Joint Influence Model using
two months query-log data from https://search.yahoo.com/. Experimental results
show that the model can indeed capture the temporal dynamics of the joint
influence over time and also achieves superior performance over different
baseline methods when applied to solve various interesting prediction problems
as well as real-word application scenarios, e.g., query auto-completion.


Preference-based Graphic Models for Collaborative Filtering

  Collaborative filtering is a very useful general technique for exploiting the
preference patterns of a group of users to predict the utility of items to a
particular user. Previous research has studied several probabilistic graphic
models for collaborative filtering with promising results. However, while these
models have succeeded in capturing the similarity among users and items in one
way or the other, none of them has considered the fact that users with similar
interests in items can have very different rating patterns; some users tend to
assign a higher rating to all items than other users. In this paper, we propose
and study of two new graphic models that address the distinction between user
preferences and ratings. In one model, called the decoupled model, we introduce
two different variables to decouple a users preferences FROM his ratings. IN
the other, called the preference model, we model the orderings OF items
preferred BY a USER, rather than the USERs numerical ratings of items.
Empirical study over two datasets of movie ratings shows that appropriate
modeling of the distinction between user preferences and ratings improves the
performance substantially and consistently. Specifically, the proposed
decoupled model outperforms all five existing approaches that we compare with
significantly, but the preference model is not very successful. These results
suggest that explicit modeling of the underlying user preferences is very
important for collaborative filtering, but we can not afford ignoring the
rating information completely.


Modeling Diverse Relevance Patterns in Ad-hoc Retrieval

  Assessing relevance between a query and a document is challenging in ad-hoc
retrieval due to its diverse patterns, i.e., a document could be relevant to a
query as a whole or partially as long as it provides sufficient information for
users' need. Such diverse relevance patterns require an ideal retrieval model
to be able to assess relevance in the right granularity adaptively.
Unfortunately, most existing retrieval models compute relevance at a single
granularity, either document-wide or passage-level, or use fixed combination
strategy, restricting their ability in capturing diverse relevance patterns. In
this work, we propose a data-driven method to allow relevance signals at
different granularities to compete with each other for final relevance
assessment. Specifically, we propose a HIerarchical Neural maTching model
(HiNT) which consists of two stacked components, namely local matching layer
and global decision layer. The local matching layer focuses on producing a set
of local relevance signals by modeling the semantic matching between a query
and each passage of a document. The global decision layer accumulates local
signals into different granularities and allows them to compete with each other
to decide the final relevance score. Experimental results demonstrate that our
HiNT model outperforms existing state-of-the-art retrieval models significantly
on benchmark ad-hoc retrieval datasets.


Non-Autoregressive Machine Translation with Auxiliary Regularization

  As a new neural machine translation approach, Non-Autoregressive machine
Translation (NAT) has attracted attention recently due to its high efficiency
in inference. However, the high efficiency has come at the cost of not
capturing the sequential dependency on the target side of translation, which
causes NAT to suffer from two kinds of translation errors: 1) repeated
translations (due to indistinguishable adjacent decoder hidden states), and 2)
incomplete translations (due to incomplete transfer of source side information
via the decoder hidden states).
  In this paper, we propose to address these two problems by improving the
quality of decoder hidden representations via two auxiliary regularization
terms in the training process of an NAT model. First, to make the hidden states
more distinguishable, we regularize the similarity between consecutive hidden
states based on the corresponding target tokens. Second, to force the hidden
states to contain all the information in the source sentence, we leverage the
dual nature of translation tasks (e.g., English to German and German to
English) and minimize a backward reconstruction error to ensure that the hidden
states of the NAT decoder are able to recover the source side sentence.
Extensive experiments conducted on several benchmark datasets show that both
regularization strategies are effective and can alleviate the issues of
repeated translations and incomplete translations in NAT models. The accuracy
of NAT models is therefore improved significantly over the state-of-the-art NAT
models with even better efficiency for inference.


On Application of Learning to Rank for E-Commerce Search

  E-Commerce (E-Com) search is an emerging important new application of
information retrieval. Learning to Rank (LETOR) is a general effective strategy
for optimizing search engines, and is thus also a key technology for E-Com
search. While the use of LETOR for web search has been well studied, its use
for E-Com search has not yet been well explored. In this paper, we discuss the
practical challenges in applying learning to rank methods to E-Com search,
including the challenges in feature representation, obtaining reliable
relevance judgments, and optimally exploiting multiple user feedback signals
such as click rates, add-to-cart ratios, order rates, and revenue. We study
these new challenges using experiments on industry data sets and report several
interesting findings that can provide guidance on how to optimally apply LETOR
to E-Com search: First, popularity-based features defined solely on product
items are very useful and LETOR methods were able to effectively optimize their
combination with relevance-based features. Second, query attribute sparsity
raises challenges for LETOR, and selecting features to reduce/avoid sparsity is
beneficial. Third, while crowdsourcing is often useful for obtaining relevance
judgments for Web search, it does not work as well for E-Com search due to
difficulty in eliciting sufficiently fine grained relevance judgments. Finally,
among the multiple feedback signals, the order rate is found to be the most
robust training objective, followed by click rate, while add-to-cart ratio
seems least robust, suggesting that an effective practical strategy may be to
initially use click rates for training and gradually shift to using order rates
as they become available.


