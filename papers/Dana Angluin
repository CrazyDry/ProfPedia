Random DFAs are Efficiently PAC Learnable

  This paper has been withdrawn due to an error found by Dana Angluin and Lev
Reyzin.


Query learning of derived $ω$-tree languages in polynomial time

  We present the first polynomial time algorithm to learn nontrivial classes of
languages of infinite trees. Specifically, our algorithm uses membership and
equivalence queries to learn classes of $\omega$-tree languages derived from
weak regular $\omega$-word languages in polynomial time. The method is a
general polynomial time reduction of learning a class of derived $\omega$-tree
languages to learning the underlying class of $\omega$-word languages, for any
class of $\omega$-word languages recognized by a deterministic B\"{u}chi
acceptor. Our reduction, combined with the polynomial time learning algorithm
of Maler and Pnueli [1995] for the class of weak regular $\omega$-word
languages yields the main result. We also show that subset queries that return
counterexamples can be implemented in polynomial time using subset queries that
return no counterexamples for deterministic or non-deterministic finite word
acceptors, and deterministic or non-deterministic B\"{u}chi $\omega$-word
acceptors.
  A previous claim of an algorithm to learn regular $\omega$-trees due to
Jayasrirani, Begam and Thomas [2008] is unfortunately incorrect, as shown in
Angluin [2016].


The computational power of population protocols

  We consider the model of population protocols introduced by Angluin et al.,
in which anonymous finite-state agents stably compute a predicate of the
multiset of their inputs via two-way interactions in the all-pairs family of
communication networks. We prove that all predicates stably computable in this
model (and certain generalizations of it) are semilinear, answering a central
open question about the power of the model. Removing the assumption of two-way
interaction, we also consider several variants of the model in which agents
communicate by anonymous message-passing where the recipient of each message is
chosen by an adversary and the sender is not identified to the recipient. These
one-way models are distinguished by whether messages are delivered immediately
or after a delay, whether a sender can record that it has sent a message, and
whether a recipient can queue incoming messages, refusing to accept new
messages until it has had a chance to send out messages of its own. We
characterize the classes of predicates stably computable in each of these
one-way models using natural subclasses of the semilinear predicates.


Regular omega-Languages with an Informative Right Congruence

  A regular language is almost fully characterized by its right congruence
relation. Indeed, a regular language can always be recognized by a DFA
isomorphic to the automaton corresponding to its right congruence, henceforth
the Rightcon automaton. The same does not hold for regular omega-languages. The
right congruence of a regular omega-language is not informative enough; many
regular omega-languages have a trivial right congruence, and in general it is
not always possible to define an omega-automaton recognizing a given language
that is isomorphic to the rightcon automaton.
  The class of weak regular omega-languages does have an informative right
congruence. That is, any weak regular omega-language can always be recognized
by a deterministic B\"uchi automaton that is isomorphic to the rightcon
automaton. Weak regular omega-languages reside in the lower levels of the
expressiveness hierarchy of regular omega-languages. Are there more expressive
sub-classes of regular omega languages that have an informative right
congruence? Can we fully characterize the class of languages with a trivial
right congruence? In this paper we try to place some additional pieces of this
big puzzle.


Families of DFAs as Acceptors of $ω$-Regular Languages

  Families of DFAs (FDFAs) provide an alternative formalism for recognizing
$\omega$-regular languages. The motivation for introducing them was a desired
correlation between the automaton states and right congruence relations, in a
manner similar to the Myhill-Nerode theorem for regular languages. This
correlation is beneficial for learning algorithms, and indeed it was recently
shown that $\omega$-regular languages can be learned from membership and
equivalence queries, using FDFAs as the acceptors.
  In this paper, we look into the question of how suitable FDFAs are for
defining omega-regular languages. Specifically, we look into the complexity of
performing Boolean operations, such as complementation and intersection, on
FDFAs, the complexity of solving decision problems, such as emptiness and
language containment, and the succinctness of FDFAs compared to standard
deterministic and nondeterministic $\omega$-automata.
  We show that FDFAs enjoy the benefits of deterministic automata with respect
to Boolean operations and decision problems. Namely, they can all be performed
in nondeterministic logarithmic space. We provide polynomial translations of
deterministic B\"uchi and co-B\"uchi automata to FDFAs and of FDFAs to
nondeterministic B\"uchi automata (NBAs). We show that translation of an NBA to
an FDFA may involve an exponential blowup. Last, we show that FDFAs are more
succinct than deterministic parity automata (DPAs) in the sense that
translating a DPA to an FDFA can always be done with only a polynomial
increase, yet the other direction involves an inevitable exponential blowup in
the worst case.


Learning and Verifying Quantified Boolean Queries by Example

  To help a user specify and verify quantified queries --- a class of database
queries known to be very challenging for all but the most expert users --- one
can question the user on whether certain data objects are answers or
non-answers to her intended query. In this paper, we analyze the number of
questions needed to learn or verify qhorn queries, a special class of Boolean
quantified queries whose underlying form is conjunctions of quantified Horn
expressions. We provide optimal polynomial-question and polynomial-time
learning and verification algorithms for two subclasses of the class qhorn with
upper constant limits on a query's causal density.


Context-Free Transductions with Neural Stacks

  This paper analyzes the behavior of stack-augmented recurrent neural network
(RNN) models. Due to the architectural similarity between stack RNNs and
pushdown transducers, we train stack RNN models on a number of tasks, including
string reversal, context-free language modelling, and cumulative XOR
evaluation. Examining the behavior of our networks, we show that
stack-augmented RNNs can discover intuitive stack-based strategies for solving
our tasks. However, stack RNNs are more difficult to train than classical
architectures such as LSTMs. Rather than employ stack-based strategies, more
complex networks often find approximate solutions by using the stack as
unstructured memory.


