On the Complexity of Solving Markov Decision Problems

  Markov decision problems (MDPs) provide the foundations for a number ofproblems of interest to AI researchers studying automated planning andreinforcement learning. In this paper, we summarize results regarding thecomplexity of solving MDPs and the running time of MDP solution algorithms. Weargue that, although MDPs can be solved efficiently in theory, more study isneeded to reveal practical algorithms for solving large problems quickly. Toencourage future research, we sketch some alternative methods of analysis thatrely on the structure of MDPs.

Bayesian Optimization with Exponential Convergence

  This paper presents a Bayesian optimization method with exponentialconvergence without the need of auxiliary optimization and without thedelta-cover sampling. Most Bayesian optimization methods require auxiliaryoptimization: an additional non-convex global optimization problem, which canbe time-consuming and hard to implement in practice. Also, the existingBayesian optimization method with exponential convergence requires access tothe delta-cover sampling, which was considered to be impractical. Our approacheliminates both requirements and achieves an exponential convergence rate.

Backward-Forward Search for Manipulation Planning

  In this paper we address planning problems in high-dimensional hybridconfiguration spaces, with a particular focus on manipulation planning problemsinvolving many objects. We present the hybrid backward-forward (HBF) planningalgorithm that uses a backward identification of constraints to direct thesampling of the infinite action space in a forward search from the initialstate towards a goal configuration. The resulting planner is probabilisticallycomplete and can effectively construct long manipulation plans requiring bothprehensile and nonprehensile actions in cluttered environments.

Learning to Rank for Synthesizing Planning Heuristics

  We investigate learning heuristics for domain-specific planning. Prior workframed learning a heuristic as an ordinary regression problem. However, in agreedy best-first search, the ordering of states induced by a heuristic is moreindicative of the resulting planner's performance than mean squared error.Thus, we instead frame learning a heuristic as a learning to rank problem whichwe solve using a RankSVM formulation. Additionally, we introduce new methodsfor computing features that capture temporal interactions in an approximateplan. Our experiments on recent International Planning Competition problemsshow that the RankSVM learned heuristics outperform both the originalheuristics and heuristics learned through ordinary regression.

Generalization in Deep Learning

  Throughout this chapter, we provide theoretical insights into why and howdeep learning can generalize well, despite its large capacity, complexity,possible algorithmic instability, nonrobustness, and sharp minima, respondingto an open question in the literature. We also propose new open problems anddiscuss the limitations of our results.

STRIPStream: Integrating Symbolic Planners and Blackbox Samplers

  Many planning applications involve complex relationships defined onhigh-dimensional, continuous variables. For example, robotic manipulationrequires planning with kinematic, collision, visibility, and motion constraintsinvolving robot configurations, object transforms, and robot trajectories.These constraints typically require specialized procedures to sample satisfyingvalues. We extend the STRIPS planning language to support a generic,declarative specification for these procedures while treating theirimplementation as black boxes. We also describe cost-sensitive planning withinthis framework. We provide several domain-independent algorithms that reduceSTRIPStream problems to a sequence of finite-domain STRIPS planning problems.Finally, we evaluate our algorithms on three robotic planning domains.

Learning to Cooperate via Policy Search

  Cooperative games are those in which both agents share the same payoffstructure. Value-based reinforcement-learning algorithms, such as variants ofQ-learning, have been applied to learning cooperative games, but they onlyapply when the game state is completely observable to both agents. Policysearch methods are a reasonable alternative to value-based methods forpartially observable environments. In this paper, we provide a gradient-baseddistributed policy-search method for cooperative games and compare the notionof local optimum to that of Nash equilibrium. We demonstrate the effectivenessof this method experimentally in a small, partially observable simulated soccerdomain.

Learning Probabilistic Relational Dynamics for Multiple Tasks

  The ways in which an agent's actions affect the world can often be modeledcompactly using a set of relational probabilistic planning rules. This paperaddresses the problem of learning such rule sets for multiple related tasks. Wetake a hierarchical Bayesian approach, in which the system learns a priordistribution over rule sets. We present a class of prior distributionsparameterized by a rule set prototype that is stochastically modified toproduce a task-specific rule set. We also describe a coordinate ascentalgorithm that iteratively optimizes the task-specific rule sets and the priordistribution. Experiments using this algorithm show that transferringinformation from related tasks significantly reduces the amount of trainingdata required to predict action effects in blocks-world domains.

CAPIR: Collaborative Action Planning with Intention Recognition

  We apply decision theoretic techniques to construct non-player charactersthat are able to assist a human player in collaborative games. The method isbased on solving Markov decision processes, which can be difficult when thegame state is described by many variables. To scale to more complex games, themethod allows decomposition of a game task into subtasks, each of which can bemodelled by a Markov decision process. Intention recognition is used to inferthe subtask that the human is currently performing, allowing the helper toassist the human in performing the correct task. Experiments show that themethod can be effective, giving near-human level performance in helping a humanin a collaborative game.

The Thing That We Tried Didn't Work Very Well : Deictic Representation  in Reinforcement Learning

  Most reinforcement learning methods operate on propositional representationsof the world state. Such representations are often intractably large andgeneralize poorly. Using a deictic representation is believed to be a viablealternative: they promise generalization while allowing the use of existingreinforcement-learning methods. Yet, there are few experiments on learning withdeictic representations reported in the literature. In this paper we explorethe effectiveness of two forms of deictic representation and a na\"{i}vepropositional representation in a simple blocks-world domain. We find,empirically, that the deictic representations actually worsen learningperformance. We conclude with a discussion of possible causes of these resultsand strategies for more effective learning in domains with objects.

Adaptive Importance Sampling for Estimation in Structured Domains

  Sampling is an important tool for estimating large, complex sums andintegrals over high dimensional spaces. For instance, important sampling hasbeen used as an alternative to exact methods for inference in belief networks.Ideally, we want to have a sampling distribution that provides optimal-varianceestimators. In this paper, we present methods that improve the samplingdistribution by systematically adapting it as we obtain information from thesamples. We present a stochastic-gradient-descent method for sequentiallyupdating the sampling distribution based on the direct minization of thevariance. We also present other stochastic-gradient-descent methods based onthe minimizationof typical notions of distance between the current samplingdistribution and approximations of the target, optimal distribution. We finallyvalidate and compare the different methods empirically by applying them to theproblem of action evaluation in influence diagrams.

Solving POMDPs by Searching the Space of Finite Policies

  Solving partially observable Markov decision processes (POMDPs) is highlyintractable in general, at least in part because the optimal policy may beinfinitely large. In this paper, we explore the problem of finding the optimalpolicy from a restricted set of policies, represented as finite state automataof a given size. This problem is also intractable, but we show that thecomplexity can be greatly reduced when the POMDP and/or policy are furtherconstrained. We demonstrate good empirical results with a branch-and-boundmethod for finding globally optimal deterministic policies, and agradient-ascent method for finding locally optimal stochastic policies.

Learning Finite-State Controllers for Partially Observable Environments

  Reactive (memoryless) policies are sufficient in completely observable Markovdecision processes (MDPs), but some kind of memory is usually necessary foroptimal control of a partially observable MDP. Policies with finite memory canbe represented as finite-state automata. In this paper, we extend Baird andMoore's VAPS algorithm to the problem of learning general finite-stateautomata. Because it performs stochastic gradient descent, this algorithm canbe shown to converge to a locally optimal finite-state controller. We providethe details of the algorithm and then consider the question of under whatconditions stochastic gradient descent will outperform exact gradient descent.We conclude with empirical results comparing the performance of stochastic andexact gradient descent, and showing the ability of our algorithm to extract theuseful information contained in the sequence of past observations to compensatefor the lack of observability at each time-step.

Accelerating EM: An Empirical Study

  Many applications require that we learn the parameters of a model from data.EM is a method used to learn the parameters of probabilistic models for whichthe data for some of the variables in the models is either missing or hidden.There are instances in which this method is slow to converge. Therefore,several accelerations have been proposed to improve the method. None of theproposed acceleration methods are theoretically dominant and experimentalcomparisons are lacking. In this paper, we present the different proposedaccelerations and try to compare them experimentally. From the results of theexperiments, we argue that some acceleration of EM is always possible, but thatwhich acceleration is superior depends on properties of the problem.

Hierarchical Solution of Markov Decision Processes using Macro-actions

  We investigate the use of temporally abstract actions, or macro-actions, inthe solution of Markov decision processes. Unlike current models that combineboth primitive actions and macro-actions and leave the state space unchanged,we propose a hierarchical model (using an abstract MDP) that works withmacro-actions only, and that significantly reduces the size of the state space.This is achieved by treating macroactions as local policies that act in certainregions of state space, and by restricting states in the abstract MDP to thoseat the boundaries of regions. The abstract MDP approximates the original andcan be solved more efficiently. We discuss several ways in which macro-actionscan be generated to ensure good solution quality. Finally, we consider ways inwhich macro-actions can be reused to solve multiple, related MDPs; and we showthat this can justify the computational overhead of macro-action generation.

Deliberation Scheduling for Time-Critical Sequential Decision Making

  We describe a method for time-critical decision making involving sequentialtasks and stochastic processes. The method employs several iterative refinementroutines for solving different aspects of the decision making problem. Thispaper concentrates on the meta-level control problem of deliberationscheduling, allocating computational resources to these routines. We providedifferent models corresponding to optimization problems that capture thedifferent circumstances and computational strategies for decision making undertime constraints. We consider precursor models in which all decision making isperformed prior to execution and recurrent models in which decision making isperformed in parallel with execution, accounting for the states observed duringexecution and anticipating future states. We describe algorithms for precursorand recurrent models and provide the results of our empirical investigations todate.

Learning to Cooperate via Policy Search

  Cooperative games are those in which both agents share the same payoffstructure. Value-based reinforcement-learning algorithms, such as variants ofQ-learning, have been applied to learning cooperative games, but they onlyapply when the game state is completely observable to both agents. Policysearch methods are a reasonable alternative to value-based methods forpartially observable environments. In this paper, we provide a gradient-baseddistributed policy-search method for cooperative games and compare the notionof local optimum to that of Nash equilibrium. We demonstrate the effectivenessof this method experimentally in a small, partially observable simulated soccerdomain.

Object-based World Modeling in Semi-Static Environments with Dependent  Dirichlet-Process Mixtures

  To accomplish tasks in human-centric indoor environments, robots need torepresent and understand the world in terms of objects and their attributes. Werefer to this attribute-based representation as a world model, and consider howto acquire it via noisy perception and maintain it over time, as objects areadded, changed, and removed in the world. Previous work has framed this asmultiple-target tracking problem, where objects are potentially in motion atall times. Although this approach is general, it is computationally expensive.We argue that such generality is not needed in typical world modeling tasks,where objects only change state occasionally. More efficient approaches areenabled by restricting ourselves to such semi-static environments.  We consider a previously-proposed clustering-based world modeling approachthat assumed static environments, and extend it to semi-static domains byapplying a dependent Dirichlet-process (DDP) mixture model. We derive a novelMAP inference algorithm under this model, subject to data associationconstraints. We demonstrate our approach improves computational performance insemi-static environments.

Focused Model-Learning and Planning for Non-Gaussian Continuous  State-Action Systems

  We introduce a framework for model learning and planning in stochasticdomains with continuous state and action spaces and non-Gaussian transitionmodels. It is efficient because (1) local models are estimated only when theplanner requires them; (2) the planner focuses on the most relevant states tothe current planning problem; and (3) the planner focuses on the mostinformative and/or high-value actions. Our theoretical analysis shows thevalidity and asymptotic optimality of the proposed approach. Empirically, wedemonstrate the effectiveness of our algorithm on a simulated multi-modalpushing problem.

STRIPS Planning in Infinite Domains

  Many robotic planning applications involve continuous actions with highlynon-linear constraints, which cannot be modeled using modern planners thatconstruct a propositional representation. We introduce STRIPStream: anextension of the STRIPS language which can model these domains by supportingthe specification of blackbox generators to handle complex constraints. Theoutputs of these generators interact with actions through possibly infinitestreams of objects and static predicates. We provide two algorithms which bothreduce STRIPStream problems to a sequence of finite-domain planning problems.The representation and algorithms are entirely domain independent. Wedemonstrate our framework on simple illustrative domains, and then on ahigh-dimensional, continuous robotic task and motion planning domain.

Provably Safe Robot Navigation with Obstacle Uncertainty

  As drones and autonomous cars become more widespread it is becomingincreasingly important that robots can operate safely under realisticconditions. The noisy information fed into real systems means that robots mustuse estimates of the environment to plan navigation. Efficiently guaranteeingthat the resulting motion plans are safe under these circumstances has proveddifficult. We examine how to guarantee that a trajectory or policy is safe withonly imperfect observations of the environment. We examine the implications ofvarious mathematical formalisms of safety and arrive at a mathematical notionof safety of a long-term execution, even when conditioned on observationalinformation. We present efficient algorithms that can prove that trajectoriesor policies are safe with much tighter bounds than in previous work. Notably,the complexity of the environment does not affect our methods ability toevaluate if a trajectory or policy is safe. We then use these safety checkingmethods to design a safe variant of the RRT planning algorithm.

Selecting Representative Examples for Program Synthesis

  Program synthesis is a class of regression problems where one seeks asolution, in the form of a source-code program, mapping the inputs to theircorresponding outputs exactly. Due to its precise and combinatorial nature,program synthesis is commonly formulated as a constraint satisfaction problem,where input-output examples are encoded as constraints and solved with aconstraint solver. A key challenge of this formulation is scalability: whileconstraint solvers work well with a few well-chosen examples, a large set ofexamples can incur significant overhead in both time and memory. We describe amethod to discover a subset of examples that is both small and representative:the subset is constructed iteratively, using a neural network to predict theprobability of unchosen examples conditioned on the chosen examples in thesubset, and greedily adding the least probable example. We empirically evaluatethe representativeness of the subsets constructed by our method, anddemonstrate such subsets can significantly improve synthesis time andstability.

Sampling-Based Methods for Factored Task and Motion Planning

  This paper presents a general-purpose formulation of a large class ofdiscrete-time planning problems, with hybrid state and control-spaces, asfactored transition systems. Factoring allows state transitions to be describedas the intersection of several constraints each affecting a subset of the stateand control variables. Robotic manipulation problems with many movable objectsinvolve constraints that only affect several variables at a time and thereforeexhibit large amounts of factoring. We develop a theoretical framework forsolving factored transition systems with sampling-based algorithms. Theframework characterizes conditions on the submanifold in which solutions lie,leading to a characterization of robust feasibility that incorporatesdimensionality-reducing constraints. It then connects those conditions tocorresponding conditional samplers that can be composed to produce values onthis submanifold. We present two domain-independent, probabilistically completeplanning algorithms that take, as input, a set of conditional samplers. Wedemonstrate the empirical efficiency of these algorithms on a set ofchallenging task and motion planning problems involving picking, placing, andpushing.

Generalization in Machine Learning via Analytical Learning Theory

  This paper introduces a novel measure-theoretic theory for machine learningthat does not require statistical assumptions. Based on this theory, a newregularization method in deep learning is derived and shown to outperformprevious methods in CIFAR-10, CIFAR-100, and SVHN. Moreover, the proposedtheory provides a theoretical basis for a family of practically successfulregularization methods in deep learning. We discuss several consequences of ourresults on one-shot learning, representation learning, deep learning, andcurriculum learning. Unlike statistical learning theory, the proposed learningtheory analyzes each problem instance individually via measure theory, ratherthan a set of problem instances via statistics. As a result, it providesdifferent types of results and insights when compared to statistical learningtheory.

Learning What Information to Give in Partially Observed Domains

  In many robotic applications, an autonomous agent must act within and explorea partially observed environment that is unobserved by its human teammate. Weconsider such a setting in which the agent can, while acting, transmitdeclarative information to the human that helps them understand aspects of thisunseen environment. In this work, we address the algorithmic question of howthe agent should plan out what actions to take and what information totransmit. Naturally, one would expect the human to have preferences, which wemodel information-theoretically by scoring transmitted information based on thechange it induces in weighted entropy of the human's belief state. We formulatethis setting as a belief MDP and give a tractable algorithm for solving itapproximately. Then, we give an algorithm that allows the agent to learn thehuman's preferences online, through exploration. We validate our approachexperimentally in simulated discrete and continuous partially observedsearch-and-recover domains. Visit http://tinyurl.com/chitnis-corl-18 for asupplementary video.

Learning to guide task and motion planning using score-space  representation

  In this paper, we propose a learning algorithm that speeds up the search intask and motion planning problems. Our algorithm proposes solutions to threedifferent challenges that arise in learning to improve planning efficiency:what to predict, how to represent a planning problem instance, and how totransfer knowledge from one problem instance to another. We propose a methodthat predicts constraints on the search space based on a generic representationof a planning problem instance, called score-space, where we represent aproblem instance in terms of the performance of a set of solutions attempted sofar. Using this representation, we transfer knowledge, in the form ofconstraints, from previous problems based on the similarity in score space. Wedesign a sequential algorithm that efficiently predicts these constraints, andevaluate it in three different challenging task and motion planning problems.Results indicate that our approach performs orders of magnitudes faster than anunguided planner

Learning Quickly to Plan Quickly Using Modular Meta-Learning

  Multi-object manipulation problems in continuous state and action spaces canbe solved by planners that search over sampled values for the continuousparameters of operators. The efficiency of these planners depends critically onthe effectiveness of the samplers used, but effective sampling in turn dependson details of the robot, environment, and task. Our strategy is to learnfunctions called "specializers" that generate values for continuous operatorparameters, given a state description and values for the discrete parameters.Rather than trying to learn a single specializer for each operator from largeamounts of data on a single task, we take a modular meta-learning approach. Wetrain on multiple tasks and learn a variety of specializers that, on a newtask, can be quickly adapted using relatively little data -- thus, our system"learns quickly to plan quickly" using these specializers. We validate ourapproach experimentally in simulated 3D pick-and-place tasks with continuousstate and action spaces. Visit http://tinyurl.com/chitnis-icra-19 for asupplementary video.

Learning sparse relational transition models

  We present a representation for describing transition models in complexuncertain domains using relational rules. For any action, a rule selects a setof relevant objects and computes a distribution over properties of just thoseobjects in the resulting state given their properties in the previous state. Aniterative greedy algorithm is used to construct a set of deictic referencesthat determine which objects are relevant in any given state. Feed-forwardneural networks are used to learn the transition distribution on the relevantobjects' properties. This strategy is demonstrated to be both more versatileand more sample efficient than learning a monolithic transition model in asimulated domain in which a robot pushes stacks of objects on a clutteredtable.

Effect of Depth and Width on Local Minima in Deep Learning

  In this paper, we analyze the effects of depth and width on the quality oflocal minima, without strong over-parameterization and simplificationassumptions in the literature. Without any simplification assumption, for deepnonlinear neural networks with the squared loss, we theoretically show that thequality of local minima tends to improve towards the global minimum value asdepth and width increase. Furthermore, with a locally-induced structure on deepnonlinear neural networks, the values of local minima of neural networks aretheoretically proven to be no worse than the globally optimal values ofcorresponding classical machine learning models. We empirically support ourtheoretical observation with a synthetic dataset as well as MNIST, CIFAR-10 andSVHN datasets. When compared to previous studies with strongover-parameterization assumptions, the results in this paper do not requireover-parameterization, and instead show the gradual effects ofover-parameterization as consequences of general results.

Regret bounds for meta Bayesian optimization with an unknown Gaussian  process prior

  Bayesian optimization usually assumes that a Bayesian prior is given.However, the strong theoretical guarantees in Bayesian optimization are oftenregrettably compromised in practice because of unknown parameters in the prior.In this paper, we adopt a variant of empirical Bayes and show that, byestimating the Gaussian process prior from offline data sampled from the sameprior and constructing unbiased estimators of the posterior, variants of bothGP-UCB and probability of improvement achieve a near-zero regret bound, whichdecreases to a constant proportional to the observational noise as the numberof offline data and the number of online evaluations increase. Empirically, wehave verified our approach on challenging simulated robotic problems featuringtask and motion planning.

Elimination of All Bad Local Minima in Deep Learning

  In this paper, we theoretically prove that we can eliminate all suboptimallocal minima by adding one neuron per output unit to any deep neural network,for multi-class classification, binary classification, and regression with anarbitrary loss function. At every local minimum of any deep neural network withadded neurons, the set of parameters of the original neural network (withoutadded neurons) is guaranteed to be a global minimum of the original neuralnetwork. The effects of the added neurons are proven to automatically vanish atevery local minimum. Unlike many related results in the literature, ourtheoretical results are directly applicable to common deep learning tasksbecause the results only rely on the assumptions that automatically hold in thecommon tasks. Moreover, we discuss several limitations in eliminating thesuboptimal local minima in this manner by providing additional theoreticalresults and several examples.

Look before you sweep: Visibility-aware motion planning

  This paper addresses the problem of planning for a robot with a directionalobstacle-detection sensor that must move through a cluttered environment. Theplanning objective is to remain safe by finding a path for the complete robot,including sensor, that guarantees that the robot will not move into any part ofthe workspace before it has been seen by the sensor. Although a great deal ofwork has addressed a version of this problem in which the "field of view" ofthe sensor is a sphere around the robot, there is very little work addressingrobots with a narrow or occluded field of view. We give a formal definition ofthe problem, several solution methods with different computational trade-offs,and experimental results in illustrative domains.

Every Local Minimum is a Global Minimum of an Induced Model

  For non-convex optimization in machine learning, this paper proves that everylocal minimum achieves the global optimality of the perturbable gradient basismodel at any differentiable point. As a result, non-convex machine learning istheoretically as supported as convex machine learning with a hand-crafted basisin terms of the loss at differentiable local minima, except in the case when apreference is given to the hand-crafted basis over the perturbable gradientbasis. The proofs of these results are derived under mild assumptions.Accordingly, the proven results are directly applicable to many machinelearning models, including practical deep neural networks, without anymodification of practical methods. Furthermore, as special cases of our generalresults, this paper improves or complements several state-of-the-arttheoretical results in the literature with a simple and unified prooftechnique.

FFRob: Leveraging Symbolic Planning for Efficient Task and Motion  Planning

  Mobile manipulation problems involving many objects are challenging to solvedue to the high dimensionality and multi-modality of their hybrid configurationspaces. Planners that perform a purely geometric search are prohibitively slowfor solving these problems because they are unable to factor the configurationspace. Symbolic task planners can efficiently construct plans involving manyvariables but cannot represent the geometric and kinematic constraints requiredin manipulation. We present the FFRob algorithm for solving task and motionplanning problems. First, we introduce Extended Action Specification (EAS) as ageneral purpose planning representation that supports arbitrary predicates asconditions. We adapt existing heuristic search ideas for solving \proc{strips}planning problems, particularly delete-relaxations, to solve EAS probleminstances. We then apply the EAS representation and planners to manipulationproblems resulting in FFRob. FFRob iteratively discretizes task and motionplanning problems using batch sampling of manipulation primitives and amulti-query roadmap structure that can be conditionalized to evaluatereachability under different placements of movable objects. This structureenables the EAS planner to efficiently compute heuristics that incorporategeometric and kinematic planning constraints to give a tight estimate of thedistance to the goal. Additionally, we show FFRob is probabilistically completeand has finite expected runtime. Finally, we empirically demonstrate FFRob'seffectiveness on complex and diverse task and motion planning tasks includingrearrangement planning and navigation among movable objects.

Guiding the search in continuous state-action spaces by learning an  action sampling distribution from off-target samples

  In robotics, it is essential to be able to plan efficiently inhigh-dimensional continuous state-action spaces for long horizons. For suchcomplex planning problems, unguided uniform sampling of actions until a path toa goal is found is hopelessly inefficient, and gradient-based approaches oftenfall short when the optimization manifold of a given problem is not smooth. Inthis paper we present an approach that guides the search of a state-spaceplanner, such as A*, by learning an action-sampling distribution that cangeneralize across different instances of a planning problem. The motivation isthat, unlike typical learning approaches for planning for continuous actionspace that estimate a policy, an estimated action sampler is more robust toerror since it has a planner to fall back on. We use a Generative AdversarialNetwork (GAN), and address an important issue: search experience consists of arelatively large number of actions that are not on a solution path and arelatively small number of actions that actually are on a solution path. Weintroduce a new technique, based on an importance-ratio estimation method, forusing samples from a non-target distribution to make GAN learning moredata-efficient. We provide theoretical guarantees and empirical evaluation inthree challenging continuous robot planning problems to illustrate theeffectiveness of our algorithm.

Integrating Human-Provided Information Into Belief State Representation  Using Dynamic Factorization

  In partially observed environments, it can be useful for a human to providethe robot with declarative information that represents probabilistic relationalconstraints on properties of objects in the world, augmenting the robot'ssensory observations. For instance, a robot tasked with a search-and-rescuemission may be informed by the human that two victims are probably in the sameroom. An important question arises: how should we represent the robot'sinternal knowledge so that this information is correctly processed and combinedwith raw sensory information? In this paper, we provide an efficient beliefstate representation that dynamically selects an appropriate factoring,combining aspects of the belief when they are correlated through informationand separating them when they are not. This strategy works in open domains, inwhich the set of possible objects is not known in advance, and providessignificant improvements in inference time over a static factoring, leading tomore efficient planning for complex partially observed tasks. We validate ourapproach experimentally in two open-domain planning problems: a 2D discretegridworld task and a 3D continuous cooking task. A supplementary video can befound at http://tinyurl.com/chitnis-iros-18.

Active model learning and diverse action sampling for task and motion  planning

  The objective of this work is to augment the basic abilities of a robot bylearning to use new sensorimotor primitives to enable the solution of complexlong-horizon problems. Solving long-horizon problems in complex domainsrequires flexible generative planning that can combine primitive abilities innovel combinations to solve problems as they arise in the world. In order toplan to combine primitive actions, we must have models of the preconditions andeffects of those actions: under what circumstances will executing thisprimitive achieve some particular effect in the world?  We use, and develop novel improvements on, state-of-the-art methods foractive learning and sampling. We use Gaussian process methods for learning theconditions of operator effectiveness from small numbers of expensive trainingexamples collected by experimentation on a robot. We develop adaptive samplingmethods for generating diverse elements of continuous sets (such as robotconfigurations and object poses) during planning for solving a new task, sothat planning is as efficient as possible. We demonstrate these methods in anintegrated system, combining newly learned models with an efficientcontinuous-space robot task and motion planner to learn to solve long horizonproblems more efficiently than was previously possible.

