Decentralized Knowledge and Learning in Strategic Multi-user
  Communication

  Please see the content of this report.


Business Mode Selection in Digital Content Markets

  NA


User Subscription, Revenue Maximization, and Competition in
  Communications Markets

  An updated version of this paper (but with a different title) can be found at
arXiv:1204.4262


Distributed Power Allocation in Multi-User Multi-Channel Relay Networks

  This paper has been withdrawn by the authors as they feel it inappropriate to
publish this paper for the time being.


A Simple Characterization of Strategic Behaviors in Broadcast Channels

  In this paper, we consider the problem of resource allocation among two
competing users sharing a binary symmetric broadcast channel. We model the
interaction between autonomous selfish users in the resource allocation and
analyze their strategic behavior in manipulating the allocation outcome. We
analytically show that users will improve their performance (i.e. gain higher
allocated rates) if they have more information about the strategy of the
competing user.


Designing Incentive Schemes Based on Intervention: The Case of Perfect
  Monitoring

  This paper studies a class of incentive schemes based on intervention, where
there exists an intervention device that is able to monitor the actions of
users and to take an action that affects the payoffs of users. We consider the
case of perfect monitoring, where the intervention device can immediately
observe the actions of users without errors. We also assume that there exist
actions of the intervention device that are most and least preferred by all the
users and the intervention device, regardless of the actions of users. We
derive analytical results about the outcomes achievable with intervention, and
illustrate our results with an example based on the Cournot model.


Designing Rating Systems to Promote Mutual Security for Interconnected
  Networks

  Interconnected autonomous systems often share security risks. However, an
autonomous system lacks the incentive to make (sufficient) security investments
if the cost exceeds its own benefit even though doing that would be socially
beneficial. In this paper, we develop a systematic and rigorous framework for
analyzing and significantly improving the mutual security of a collection of
ASs that interact frequently over a long period of time. Using this framework,
we show that simple incentive schemes based on rating systems can be designed
to encourage the autonomous systems' security investments, thereby
significantly improving their mutual security.


A Theory of Individualism, Collectivism and Economic Outcomes

  This paper presents a dynamic model to study the impact on the economic
outcomes in different societies during the Malthusian Era of individualism
(time spent working alone) and collectivism (complementary time spent working
with others). The model is driven by opposing forces: a greater degree of
collectivism provides a higher safety net for low quality workers but a greater
degree of individualism allows high quality workers to leave larger bequests.
The model suggests that more individualistic societies display smaller
populations, greater per capita income and greater income inequality. Some
(limited) historical evidence is consistent with these predictions.


Forecasting Disease Trajectories in Alzheimer's Disease Using Deep
  Learning

  Joint models for longitudinal and time-to-event data are commonly used in
longitudinal studies to forecast disease trajectories over time. Despite the
many advantages of joint modeling, the standard forms suffer from limitations
that arise from a fixed model specification and computational difficulties when
applied to large datasets. We adopt a deep learning approach to address these
limitations, enhancing existing methods with the flexibility and scalability of
deep neural networks while retaining the benefits of joint modeling. Using data
from the Alzheimer's Disease Neuroimaging Institute, we show improvements in
performance and scalability compared to traditional methods.


Stackelberg Contention Games in Multiuser Networks

  Interactions among selfish users sharing a common transmission channel can be
modeled as a non-cooperative game using the game theory framework. When selfish
users choose their transmission probabilities independently without any
coordination mechanism, Nash equilibria usually result in a network collapse.
We propose a methodology that transforms the non-cooperative game into a
Stackelberg game. Stackelberg equilibria of the Stackelberg game can overcome
the deficiency of the Nash equilibria of the original game. A particular type
of Stackelberg intervention is constructed to show that any positive payoff
profile feasible with independent transmission probabilities can be achieved as
a Stackelberg equilibrium payoff profile. We discuss criteria to select an
operating point of the network and informational requirements for the
Stackelberg game. We relax the requirements and examine the effects of
relaxation on performance.


Mission-Aware Medium Access Control in Random Access Networks

  We study mission-critical networking in wireless communication networks,
where network users are subject to critical events such as emergencies and
crises. If a critical event occurs to a user, the user needs to send necessary
information for help as early as possible. However, most existing medium access
control (MAC) protocols are not adequate to meet the urgent need for
information transmission by users in a critical situation. In this paer, we
propose a novel class of MAC protocols that utilize available past information
as well as current information. Our proposed protocols are mission-aware since
they prescribe different transmission decision rules to users in different
situations. We show that the proposed protocols perform well not only when the
system faces a critical situation but also when there is no critical situation.
By utilizing past information, the proposed protocols coordinate transmissions
by users to achieve high throughput in the normal phase of operation and to let
a user in a critical situation make successful transmissions while it is in the
critical situation. Moreover, the proposed protocols require short memory and
no message exchanges.


Medium Access Control Protocols With Memory

  Many existing medium access control (MAC) protocols utilize past information
(e.g., the results of transmission attempts) to adjust the transmission
parameters of users. This paper provides a general framework to express and
evaluate distributed MAC protocols utilizing a finite length of memory for a
given form of feedback information. We define protocols with memory in the
context of a slotted random access network with saturated arrivals. We
introduce two performance metrics, throughput and average delay, and formulate
the problem of finding an optimal protocol. We first show that a TDMA outcome,
which is the best outcome in the considered scenario, can be obtained after a
transient period by a protocol with (N-1)-slot memory, where N is the total
number of users. Next, we analyze the performance of protocols with 1-slot
memory using a Markov chain and numerical methods. Protocols with 1-slot memory
can achieve throughput arbitrarily close to 1 (i.e., 100% channel utilization)
at the expense of large average delay, by correlating successful users in two
consecutive slots. Finally, we apply our framework to wireless local area
networks.


Linearly Coupled Communication Games

  This paper discusses a special type of multi-user communication scenario, in
which users' utilities are linearly impacted by their competitors' actions.
First, we explicitly characterize the Nash equilibrium and Pareto boundary of
the achievable utility region. Second, the price of anarchy incurred by the
non-collaborative Nash strategy is quantified. Third, to improve the
performance in the non-cooperative scenarios, we investigate the properties of
an alternative solution concept named conjectural equilibrium, in which
individual users compensate for their lack of information by forming internal
beliefs about their competitors. The global convergence of the best response
and Jacobi update dynamics that achieve various conjectural equilibria are
analyzed. It is shown that the Pareto boundaries of the investigated linearly
coupled games can be sustained as stable conjectural equilibria if the belief
functions are properly initialized. The investigated models apply to a variety
of realistic applications encountered in the multiple access design, including
wireless random access and flow control.


Cognitive MAC Protocols Using Memory for Distributed Spectrum Sharing
  Under Limited Spectrum Sensing

  The main challenges of cognitive radio include spectrum sensing at the
physical (PHY) layer to detect the activity of primary users and spectrum
sharing at the medium access control (MAC) layer to coordinate access among
coexisting secondary users. In this paper, we consider a cognitive radio
network in which a primary user shares a channel with secondary users that
cannot distinguish the signals of the primary user from those of a secondary
user. We propose a class of distributed cognitive MAC protocols to achieve
efficient spectrum sharing among the secondary users while protecting the
primary user from potential interference by the secondary users. By using a MAC
protocol with one-slot memory, we can obtain high channel utilization by the
secondary users while limiting interference to the primary user at a low level.
The results of this paper suggest the possibility of utilizing MAC design in
cognitive radio networks to overcome limitations in spectrum sensing at the PHY
layer as well as to achieve spectrum sharing at the MAC layer.


Adaptive MAC Protocols Using Memory for Networks with Critical Traffic

  We consider wireless communication networks where network users are subject
to critical events such as emergencies and crises. If a critical event occurs
to a user, the user needs to send critical traffic as early as possible.
However, most existing medium access control (MAC) protocols are not adequate
to meet the urgent need for data transmission by users with critical traffic.
In this paper, we devise a class of distributed MAC protocols that achieve
coordination using the finite-length memory of users containing their own
observations and traffic types. We formulate a protocol design problem and find
optimal protocols that solve the problem. We show that the proposed protocols
enable a user with critical traffic to transmit its critical traffic without
interruption from other users after a short delay while allowing users to share
the channel efficiently when there is no critical traffic. Moreover, the
proposed protocols require short memory and can be implemented without explicit
message passing.


Structural Solutions For Additively Coupled Sum Constrained Games

  We propose and analyze a broad family of games played by resource-constrained
players, which are characterized by the following central features: 1) each
user has a multi-dimensional action space, subject to a single sum resource
constraint; 2) each user's utility in a particular dimension depends on an
additive coupling between the user's action in the same dimension and the
actions of the other users; and 3) each user's total utility is the sum of the
utilities obtained in each dimension. Familiar examples of such multi-user
environments in communication systems include power control over
frequency-selective Gaussian interference channels and flow control in Jackson
networks. In settings where users cannot exchange messages in real-time, we
study how users can adjust their actions based on their local observations. We
derive sufficient conditions under which a unique Nash equilibrium exists and
the best-response algorithm converges globally and linearly to the Nash
equilibrium. In settings where users can exchange messages in real-time, we
focus on user choices that optimize the overall utility. We provide the
convergence conditions of two distributed action update mechanisms, gradient
play and Jacobi update.


Near-Optimal Deviation-Proof Medium Access Control Designs in Wireless
  Networks

  Distributed medium access control (MAC) protocols are essential for the
proliferation of low cost, decentralized wireless local area networks (WLANs).
Most MAC protocols are designed with the presumption that nodes comply with
prescribed rules. However, selfish nodes have natural motives to manipulate
protocols in order to improve their own performance. This often degrades the
performance of other nodes as well as that of the overall system. In this work,
we propose a class of protocols that limit the performance gain which nodes can
obtain through selfish manipulation while incurring only a small efficiency
loss. The proposed protocols are based on the idea of a review strategy, with
which nodes collect signals about the actions of other nodes over a period of
time, use a statistical test to infer whether or not other nodes are following
the prescribed protocol, and trigger a punishment if a departure from the
protocol is perceived. We consider the cases of private and public signals and
provide analytical and numerical results to demonstrate the properties of the
proposed protocols.


Designing Incentive Schemes Based on Intervention: The Case of Imperfect
  Monitoring

  We propose an incentive scheme based on intervention to sustain cooperation
among self-interested users. In the proposed scheme, an intervention device
collects imperfect signals about the actions of the users for a test period,
and then chooses the level of intervention that degrades the performance of the
network for the remaining time period. We analyze the problems of designing an
optimal intervention rule given a test period and choosing an optimal length of
the test period. The intervention device can provide the incentive for
cooperation by exerting intervention following signals that involve a high
likelihood of deviation. Increasing the length of the test period has two
counteracting effects on the performance: It improves the quality of signals,
but at the same time it weakens the incentive for cooperation due to increased
delay.


The Theory of Intervention Games for Resource Sharing in Wireless
  Communications

  This paper develops a game-theoretic framework for the design and analysis of
a new class of incentive schemes called intervention schemes. We formulate
intervention games, propose a solution concept of intervention equilibrium, and
prove its existence in a finite intervention game. We apply our framework to
resource sharing scenarios in wireless communications, whose non-cooperative
outcomes without intervention yield suboptimal performance. We derive
analytical results and analyze illustrative examples in the cases of imperfect
and perfect monitoring. In the case of imperfect monitoring, intervention
schemes can improve the suboptimal performance of non-cooperative equilibrium
when the intervention device has a sufficiently accurate monitoring technology,
although it may not be possible to achieve the best feasible performance. In
the case of perfect monitoring, the best feasible performance can be obtained
with an intervention scheme when the intervention device has a sufficiently
strong intervention capability.


Robust Stackelberg game in communication systems

  This paper studies multi-user communication systems with two groups of users:
leaders which possess system information, and followers which have no system
information using the formulation of Stackelberg games. In such games, the
leaders play and choose their actions based on their information about the
system and the followers choose their actions myopically according to their
observations of the aggregate impact of other users. However, obtaining the
exact value of these parameters is not practical in communication systems. To
study the effect of uncertainty and preserve the players' utilities in these
conditions, we introduce a robust equilibrium for Stackelberg games. In this
framework, the leaders' information and the followers' observations are
uncertain parameters, and the leaders and the followers choose their actions by
solving the worst-case robust optimizations. We show that the followers'
uncertain parameters always increase the leaders' utilities and decrease the
followers' utilities. Conversely, the leaders' uncertain information reduces
the leaders' utilities and increases the followers' utilities. We illustrate
our theoretical results with the numerical results obtained based on the power
control games in the interference channels.


Designing Practical Distributed Exchange for Online Communities

  In many online systems, individuals provide services for each other; the
recipient of the service obtains a benefit but the provider of the service
incurs a cost. If benefit exceeds cost, provision of the service increases
social welfare and should therefore be encouraged -- but the individuals
providing the service gain no (immediate) benefit from providing the service
and hence have an incentive to withhold service. Hence there is scope for
designing a system that improves welfare by encouraging exchange. To operate
successfully within the confines of the online environment, such a system
should be distributed, practicable, and consistent with individual incentives.
This paper proposes and analyzes a simple such system that relies on the
exchange of {\em tokens}; the emphasis is on the design of a protocol (number
of tokens and suggested strategies). We provide estimates for the efficiency of
such protocols and show that choosing the right protocol will lead to almost
full efficiency if agents are sufficiently patient. However, choosing the wrong
protocols may lead to an enormous loss of efficiency.


Pricing and Intervention in Slotted-Aloha: Technical Report

  In many wireless communication networks a common channel is shared by
multiple users who must compete to gain access to it. The operation of the
network by self-interested and strategic users usually leads to the overuse of
the channel resources and to substantial inefficiencies. Hence, incentive
schemes are needed to overcome the inefficiencies of non-cooperative
equilibrium. In this work we consider a slotted-Aloha like random access
protocol and two incentive schemes: pricing and intervention. We provide some
criteria for the designer of the protocol to choose one scheme between them and
to design the best policy for the selected scheme, depending on the system
parameters. Our results show that intervention can achieve the maximum
efficiency in the perfect monitoring scenario. In the imperfect monitoring
scenario, instead, the performance of the system depends on the information
held by the different entities and, in some cases, there exists a threshold for
the number of users such that, for a number of users lower than the threshold,
intervention outperforms pricing, whereas, for a number of users higher than
the threshold pricing outperforms intervention.


Socially-Optimal Design of Service Exchange Platforms with Imperfect
  Monitoring

  In service exchange platforms, anonymous users exchange services with each
other: clients request services and are matched to servers who provide
services. Because providing good-quality services requires effort, in any
single interaction a server will have no incentive to exert effort and will
shirk. We show that if current servers will later become clients and want
good-quality services, shirking can be eliminated by rating protocols, which
maintain ratings for each user, prescribe behavior in each client-server
interaction, and update ratings based on whether observed/reported behavior
conforms with prescribed behavior. The rating protocols proposed are the first
to achieve social optimum even when observation/reporting is imperfect (quality
is incorrectly assessed/reported or reports are lost). The proposed protocols
are remarkably simple, requiring only binary ratings and three possible
prescribed behaviors. Key to the efficacy of the proposed protocols is that
they are nonstationary, and tailor prescriptions to both current and past
rating distributions.


Structure-Aware Stochastic Control for Transmission Scheduling

  In this paper, we consider the problem of real-time transmission scheduling
over time-varying channels. We first formulate the transmission scheduling
problem as a Markov decision process (MDP) and systematically unravel the
structural properties (e.g. concavity in the state-value function and
monotonicity in the optimal scheduling policy) exhibited by the optimal
solutions. We then propose an online learning algorithm which preserves these
structural properties and achieves -optimal solutions for an arbitrarily small
. The advantages of the proposed online method are that: (i) it does not
require a priori knowledge of the traffic arrival and channel statistics and
(ii) it adaptively approximates the state-value functions using piece-wise
linear functions and has low storage and computation complexity. We also extend
the proposed low-complexity online learning solution to the prioritized data
transmission. The simulation results demonstrate that the proposed method
achieves significantly better utility (or delay)-energy trade-offs when
comparing to existing state-of-art online optimization methods.


Distributed Online Big Data Classification Using Context Information

  Distributed, online data mining systems have emerged as a result of
applications requiring analysis of large amounts of correlated and
high-dimensional data produced by multiple distributed data sources. We propose
a distributed online data classification framework where data is gathered by
distributed data sources and processed by a heterogeneous set of distributed
learners which learn online, at run-time, how to classify the different data
streams either by using their locally available classification functions or by
helping each other by classifying each other's data. Importantly, since the
data is gathered at different locations, sending the data to another learner to
process incurs additional costs such as delays, and hence this will be only
beneficial if the benefits obtained from a better classification will exceed
the costs. We model the problem of joint classification by the distributed and
heterogeneous learners from multiple data sources as a distributed contextual
bandit problem where each data is characterized by a specific context. We
develop a distributed online learning algorithm for which we can prove
sublinear regret. Compared to prior work in distributed online data mining, our
work is the first to provide analytic regret results characterizing the
performance of the proposed algorithm.


Adaptive Ensemble Learning with Confidence Bounds

  Extracting actionable intelligence from distributed, heterogeneous,
correlated and high-dimensional data sources requires run-time processing and
learning both locally and globally. In the last decade, a large number of
meta-learning techniques have been proposed in which local learners make online
predictions based on their locally-collected data instances, and feed these
predictions to an ensemble learner, which fuses them and issues a global
prediction. However, most of these works do not provide performance guarantees
or, when they do, these guarantees are asymptotic. None of these existing works
provide confidence estimates about the issued predictions or rate of learning
guarantees for the ensemble learner. In this paper, we provide a systematic
ensemble learning method called Hedged Bandits, which comes with both long run
(asymptotic) and short run (rate of learning) performance guarantees. Moreover,
our approach yields performance guarantees with respect to the optimal local
prediction strategy, and is also able to adapt its predictions in a data-driven
manner. We illustrate the performance of Hedged Bandits in the context of
medical informatics and show that it outperforms numerous online and offline
ensemble learning methods.


Personalized Course Sequence Recommendations

  Given the variability in student learning it is becoming increasingly
important to tailor courses as well as course sequences to student needs. This
paper presents a systematic methodology for offering personalized course
sequence recommendations to students. First, a forward-search
backward-induction algorithm is developed that can optimally select course
sequences to decrease the time required for a student to graduate. The
algorithm accounts for prerequisite requirements (typically present in higher
level education) and course availability. Second, using the tools of
multi-armed bandits, an algorithm is developed that can optimally recommend a
course sequence that both reduces the time to graduate while also increasing
the overall GPA of the student. The algorithm dynamically learns how students
with different contextual backgrounds perform for given course sequences and
then recommends an optimal course sequence for new students. Using real-world
student data from the UCLA Mechanical and Aerospace Engineering department, we
illustrate how the proposed algorithms outperform other methods that do not
include student contextual information when making course sequence
recommendations.


Data-Driven Online Decision Making with Costly Information Acquisition

  In most real-world settings such as recommender systems, finance, and
healthcare, collecting useful information is costly and requires an active
choice on the part of the decision maker. The decision-maker needs to learn
simultaneously what observations to make and what actions to take. This paper
incorporates the information acquisition decision into an online learning
framework. We propose two different algorithms for this dual learning problem:
Sim-OOS and Seq-OOS where observations are made simultaneously and
sequentially, respectively. We prove that both algorithms achieve a regret that
is sublinear in time. The developed framework and algorithms can be used in
many applications including medical informatics, recommender systems and
actionable intelligence in transportation, finance, cyber-security etc., in
which collecting information prior to making decisions is costly. We validate
our algorithms in a breast cancer example setting in which we show substantial
performance gains for our proposed algorithms.


Intervention Mechanism Design for Networks With Selfish Users

  We consider a multi-user network where a network manager and selfish users
interact. The network manager monitors the behavior of users and intervenes in
the interaction among users if necessary, while users make decisions
independently to optimize their individual objectives. In this paper, we
develop a framework of intervention mechanism design, which is aimed to
optimize the objective of the manager, or the network performance, taking the
incentives of selfish users into account. Our framework is general enough to
cover a wide range of application scenarios, and it has advantages over
existing approaches such as Stackelberg strategies and pricing. To design an
intervention mechanism and to predict the resulting operating point, we
formulate a new class of games called intervention games and a new solution
concept called intervention equilibrium. We provide analytic results about
intervention equilibrium and optimal intervention mechanisms in the case of a
benevolent manager with perfect monitoring. We illustrate these results with a
random access model. Our illustrative example suggests that intervention
requires less knowledge about users than pricing.


Forecasting Popularity of Videos using Social Media

  This paper presents a systematic online prediction method (Social-Forecast)
that is capable to accurately forecast the popularity of videos promoted by
social media. Social-Forecast explicitly considers the dynamically changing and
evolving propagation patterns of videos in social media when making popularity
forecasts, thereby being situation and context aware. Social-Forecast aims to
maximize the forecast reward, which is defined as a tradeoff between the
popularity prediction accuracy and the timeliness with which a prediction is
issued. The forecasting is performed online and requires no training phase or a
priori knowledge. We analytically bound the prediction performance loss of
Social-Forecast as compared to that obtained by an omniscient oracle and prove
that the bound is sublinear in the number of video arrivals, thereby
guaranteeing its short-term performance as well as its asymptotic convergence
to the optimal performance. In addition, we conduct extensive experiments using
real-world data traces collected from the videos shared in RenRen, one of the
largest online social networks in China. These experiments show that our
proposed method outperforms existing view-based approaches for popularity
prediction (which are not context-aware) by more than 30% in terms of
prediction rewards.


Designing Efficient Resource Sharing For Impatient Players Using Limited
  Monitoring

  The problem of efficient sharing of a resource is nearly ubiquitous. Except
for pure public goods, each agent's use creates a negative externality; often
the negative externality is so strong that efficient sharing is impossible in
the short run. We show that, paradoxically, the impossibility of efficient
sharing in the short run enhances the possibility of efficient sharing in the
long run, even if outcomes depend stochastically on actions, monitoring is
limited and users are not patient. We base our analysis on the familiar
framework of repeated games with imperfect public monitoring, but we extend the
framework to view the monitoring structure as chosen by a designer who balances
the benefits and costs of more accurate observations and reports. Our
conclusions are much stronger than in the usual folk theorems: we do not
require a rich signal structure or patient users and provide an explicit online
construction of equilibrium strategies.


Jamming Bandits

  Can an intelligent jammer learn and adapt to unknown environments in an
electronic warfare-type scenario? In this paper, we answer this question in the
positive, by developing a cognitive jammer that adaptively and optimally
disrupts the communication between a victim transmitter-receiver pair. We
formalize the problem using a novel multi-armed bandit framework where the
jammer can choose various physical layer parameters such as the signaling
scheme, power level and the on-off/pulsing duration in an attempt to obtain
power efficient jamming strategies. We first present novel online learning
algorithms to maximize the jamming efficacy against static transmitter-receiver
pairs and prove that our learning algorithm converges to the optimal (in terms
of the error rate inflicted at the victim and the energy used) jamming
strategy. Even more importantly, we prove that the rate of convergence to the
optimal jamming strategy is sub-linear, i.e. the learning is fast in comparison
to existing reinforcement learning algorithms, which is particularly important
in dynamically changing wireless environments. Also, we characterize the
performance of the proposed bandit-based learning algorithm against multiple
static and adaptive transmitter-receiver pairs.


Towards a Theory of Societal Co-Evolution: Individualism versus
  Collectivism

  Substantial empirical research has shown that the level of individualism vs.
collectivism is one of the most critical and important determinants of societal
traits, such as economic growth, economic institutions and health conditions.
But the exact nature of this impact has thus far not been well understood in an
analytical setting. In this work, we develop one of the first theoretical
models that analytically studies the impact of individualism-collectivism on
the society. We model the growth of an individual's welfare (wealth, resources
and health) as depending not only on himself, but also on the level of
collectivism, i.e. the level of dependence on the rest of the individuals in
the society, which leads to a co-evolutionary setting. Based on our model, we
are able to predict the impact of individualism-collectivism on various
societal metrics, such as average welfare, average life-time, total population,
cumulative welfare and average inequality. We analytically show that
individualism has a positive impact on average welfare and cumulative welfare,
but comes with the drawbacks of lower average life-time, lower total population
and higher average inequality.


Dynamic Network Formation with Foresighted Agents

  What networks can form and persist when agents are self-interested? Can such
networks be efficient? A substantial theoretical literature predicts that the
only networks that can form and persist must have very special shapes and that
such networks cannot be efficient, but these predictions are in stark contrast
to empirical findings. In this paper, we present a new model of network
formation. In contrast to the existing literature, our model is dynamic (rather
than static), we model agents as foresighted (rather than myopic) and we allow
for the possibility that agents are heterogeneous (rather than homogeneous). We
show that a very wide variety of networks can form and persist; in particular,
efficient networks can form and persist if they provide every agent a strictly
positive payoff. For the widely-studied connections model, we provide a full
characterization of the set of efficient networks that can form and persist.
Our predictions are consistent with empirical findings.


Personalized Risk Scoring for Critical Care Patients using Mixtures of
  Gaussian Process Experts

  We develop a personalized real time risk scoring algorithm that provides
timely and granular assessments for the clinical acuity of ward patients based
on their (temporal) lab tests and vital signs. Heterogeneity of the patients
population is captured via a hierarchical latent class model. The proposed
algorithm aims to discover the number of latent classes in the patients
population, and train a mixture of Gaussian Process (GP) experts, where each
expert models the physiological data streams associated with a specific class.
Self-taught transfer learning is used to transfer the knowledge of latent
classes learned from the domain of clinically stable patients to the domain of
clinically deteriorating patients. For new patients, the posterior beliefs of
all GP experts about the patient's clinical status given her physiological data
stream are computed, and a personalized risk score is evaluated as a weighted
average of those beliefs, where the weights are learned from the patient's
hospital admission information. Experiments on a heterogeneous cohort of 6,313
patients admitted to Ronald Regan UCLA medical center show that our risk score
outperforms the currently deployed risk scores, such as MEWS and Rothman
scores.


Distributed Learning for Stochastic Generalized Nash Equilibrium
  Problems

  This work examines a stochastic formulation of the generalized Nash
equilibrium problem (GNEP) where agents are subject to randomness in the
environment of unknown statistical distribution. We focus on fully-distributed
online learning by agents and employ penalized individual cost functions to
deal with coupled constraints. Three stochastic gradient strategies are
developed with constant step-sizes. We allow the agents to use heterogeneous
step-sizes and show that the penalty solution is able to approach the Nash
equilibrium in a stable manner within $O(\mu_\text{max})$, for small step-size
value $\mu_\text{max}$ and sufficiently large penalty parameters. The operation
of the algorithm is illustrated by considering the network Cournot competition
problem.


A Semi-Markov Switching Linear Gaussian Model for Censored Physiological
  Data

  Critically ill patients in regular wards are vulnerable to unanticipated
clinical dete- rioration which requires timely transfer to the intensive care
unit (ICU). To allow for risk scoring and patient monitoring in such a setting,
we develop a novel Semi- Markov Switching Linear Gaussian Model (SSLGM) for the
inpatients' physiol- ogy. The model captures the patients' latent clinical
states and their corresponding observable lab tests and vital signs. We present
an efficient unsupervised learn- ing algorithm that capitalizes on the
informatively censored data in the electronic health records (EHR) to learn the
parameters of the SSLGM; the learned model is then used to assess the new
inpatients' risk for clinical deterioration in an online fashion, allowing for
timely ICU admission. Experiments conducted on a het- erogeneous cohort of
6,094 patients admitted to a large academic medical center show that the
proposed model significantly outperforms the currently deployed risk scores
such as Rothman index, MEWS, SOFA and APACHE.


A Hidden Absorbing Semi-Markov Model for Informatively Censored Temporal
  Data: Learning and Inference

  Modeling continuous-time physiological processes that manifest a patient's
evolving clinical states is a key step in approaching many problems in
healthcare. In this paper, we develop the Hidden Absorbing Semi-Markov Model
(HASMM): a versatile probabilistic model that is capable of capturing the
modern electronic health record (EHR) data. Unlike exist- ing models, an HASMM
accommodates irregularly sampled, temporally correlated, and informatively
censored physiological data, and can describe non-stationary clinical state
transitions. Learning an HASMM from the EHR data is achieved via a novel
forward- filtering backward-sampling Monte-Carlo EM algorithm that exploits the
knowledge of the end-point clinical outcomes (informative censoring) in the EHR
data, and implements the E-step by sequentially sampling the patients' clinical
states in the reverse-time direction while conditioning on the future states.
Real-time inferences are drawn via a forward- filtering algorithm that operates
on a virtually constructed discrete-time embedded Markov chain that mirrors the
patient's continuous-time state trajectory. We demonstrate the di- agnostic and
prognostic utility of the HASMM in a critical care prognosis setting using a
real-world dataset for patients admitted to the Ronald Reagan UCLA Medical
Center.


Learning from Clinical Judgments: Semi-Markov-Modulated Marked Hawkes
  Processes for Risk Prognosis

  Critically ill patients in regular wards are vulnerable to unanticipated
adverse events which require prompt transfer to the intensive care unit (ICU).
To allow for accurate prognosis of deteriorating patients, we develop a novel
continuous-time probabilistic model for a monitored patient's temporal sequence
of physiological data. Our model captures "informatively sampled" patient
episodes: the clinicians' decisions on when to observe a hospitalized patient's
vital signs and lab tests over time are represented by a marked Hawkes process,
with intensity parameters that are modulated by the patient's latent clinical
states, and with observable physiological data (mark process) modeled as a
switching multi-task Gaussian process. In addition, our model captures
"informatively censored" patient episodes by representing the patient's latent
clinical states as an absorbing semi-Markov jump process. The model parameters
are learned from offline patient episodes in the electronic health records via
an EM-based algorithm. Experiments conducted on a cohort of patients admitted
to a major medical center over a 3-year period show that risk prognosis based
on our model significantly outperforms the currently deployed medical risk
scores and other baseline machine learning algorithms.


Deep Counterfactual Networks with Propensity-Dropout

  We propose a novel approach for inferring the individualized causal effects
of a treatment (intervention) from observational data. Our approach
conceptualizes causal inference as a multitask learning problem; we model a
subject's potential outcomes using a deep multitask network with a set of
shared layers among the factual and counterfactual outcomes, and a set of
outcome-specific layers. The impact of selection bias in the observational data
is alleviated via a propensity-dropout regularization scheme, in which the
network is thinned for every training example via a dropout probability that
depends on the associated propensity score. The network is trained in
alternating phases, where in each phase we use the training examples of one of
the two potential outcomes (treated and control populations) to update the
weights of the shared layers and the respective outcome-specific layers.
Experiments conducted on data based on a real-world observational study show
that our algorithm outperforms the state-of-the-art.


RadialGAN: Leveraging multiple datasets to improve target-specific
  predictive models using Generative Adversarial Networks

  Training complex machine learning models for prediction often requires a
large amount of data that is not always readily available. Leveraging these
external datasets from related but different sources is therefore an important
task if good predictive models are to be built for deployment in settings where
data can be rare. In this paper we propose a novel approach to the problem in
which we use multiple GAN architectures to learn to translate from one dataset
to another, thereby allowing us to effectively enlarge the target dataset, and
therefore learn better predictive models than if we simply used the target
dataset. We show the utility of such an approach, demonstrating that our method
improves the prediction performance on the target domain over using just the
target dataset and also show that our framework outperforms several other
benchmarks on a collection of real-world medical datasets.


AutoPrognosis: Automated Clinical Prognostic Modeling via Bayesian
  Optimization with Structured Kernel Learning

  Clinical prognostic models derived from largescale healthcare data can inform
critical diagnostic and therapeutic decisions. To enable off-theshelf usage of
machine learning (ML) in prognostic research, we developed AUTOPROGNOSIS: a
system for automating the design of predictive modeling pipelines tailored for
clinical prognosis. AUTOPROGNOSIS optimizes ensembles of pipeline
configurations efficiently using a novel batched Bayesian optimization (BO)
algorithm that learns a low-dimensional decomposition of the pipelines
high-dimensional hyperparameter space in concurrence with the BO procedure.
This is achieved by modeling the pipelines performances as a black-box function
with a Gaussian process prior, and modeling the similarities between the
pipelines baseline algorithms via a sparse additive kernel with a Dirichlet
prior. Meta-learning is used to warmstart BO with external data from similar
patient cohorts by calibrating the priors using an algorithm that mimics the
empirical Bayes method. The system automatically explains its predictions by
presenting the clinicians with logical association rules that link patients
features to predicted risk strata. We demonstrate the utility of AUTOPROGNOSIS
using 10 major patient cohorts representing various aspects of cardiovascular
patient care.


Disease-Atlas: Navigating Disease Trajectories with Deep Learning

  Joint models for longitudinal and time-to-event data are commonly used in
longitudinal studies to forecast disease trajectories over time. While there
are many advantages to joint modeling, the standard forms suffer from
limitations that arise from a fixed model specification, and computational
difficulties when applied to high-dimensional datasets. In this paper, we
propose a deep learning approach to address these limitations, enhancing
existing methods with the inherent flexibility and scalability of deep neural
networks, while retaining the benefits of joint modeling. Using longitudinal
data from a real-world medical dataset, we demonstrate improvements in
performance and scalability, as well as robustness in the presence of
irregularly sampled data.


GAIN: Missing Data Imputation using Generative Adversarial Nets

  We propose a novel method for imputing missing data by adapting the
well-known Generative Adversarial Nets (GAN) framework. Accordingly, we call
our method Generative Adversarial Imputation Nets (GAIN). The generator (G)
observes some components of a real data vector, imputes the missing components
conditioned on what is actually observed, and outputs a completed vector. The
discriminator (D) then takes a completed vector and attempts to determine which
components were actually observed and which were imputed. To ensure that D
forces G to learn the desired distribution, we provide D with some additional
information in the form of a hint vector. The hint reveals to D partial
information about the missingness of the original sample, which is used by D to
focus its attention on the imputation quality of particular components. This
hint ensures that G does in fact learn to generate according to the true data
distribution. We tested our method on various datasets and found that GAIN
significantly outperforms state-of-the-art imputation methods.


Piecewise Approximations of Black Box Models for Model Interpretation

  Machine Learning models have proved extremely successful for a wide variety
of supervised learning problems, but the predictions of many of these models
are difficult to interpret. A recent literature interprets the predictions of
more general "black-box" machine learning models by approximating these models
in terms of simpler models such as piecewise linear or piecewise constant
models. Existing literature constructs these approximations in an ad-hoc
manner. We provide a tractable dynamic programming algorithm that partitions
the feature space into clusters in a principled way and then uses this
partition to provide both piecewise constant and piecewise linear
interpretations of an arbitrary "black-box" model. When loss is measured in
terms of mean squared error, our approximation is optimal (under certain
conditions); for more general loss functions, our interpretation is probably
approximately optimal (in the sense of PAC learning). Experiments with real and
synthetic data show that it continues to provide significant improvements (in
terms of mean squared error) over competing approaches.


Measuring the quality of Synthetic data for use in competitions

  Machine learning has the potential to assist many communities in using the
large datasets that are becoming more and more available. Unfortunately, much
of that potential is not being realized because it would require sharing data
in a way that compromises privacy. In order to overcome this hurdle, several
methods have been proposed that generate synthetic data while preserving the
privacy of the real data. In this paper we consider a key characteristic that
synthetic data should have in order to be useful for machine learning
researchers - the relative performance of two algorithms (trained and tested)
on the synthetic dataset should be the same as their relative performance (when
trained and tested) on the original dataset.


Siamese Survival Analysis with Competing Risks

  Survival analysis in the presence of multiple possible adverse events, i.e.,
competing risks, is a pervasive problem in many industries (healthcare,
finance, etc.). Since only one event is typically observed, the incidence of an
event of interest is often obscured by other related competing events. This
nonidentifiability, or inability to estimate true cause-specific survival
curves from empirical data, further complicates competing risk survival
analysis. We introduce Siamese Survival Prognosis Network (SSPN), a novel deep
learning architecture for estimating personalized risk scores in the presence
of competing risks. SSPN circumvents the nonidentifiability problem by avoiding
the estimation of cause-specific survival curves and instead determines
pairwise concordant time-dependent risks, where longer event times are assigned
lower risks. Furthermore, SSPN is able to directly optimize an approximation to
the C-discrimination index, rather than relying on well-known metrics which are
unable to capture the unique requirements of survival analysis with competing
risks.


Forecasting Individualized Disease Trajectories using Interpretable Deep
  Learning

  Disease progression models are instrumental in predicting individual-level
health trajectories and understanding disease dynamics. Existing models are
capable of providing either accurate predictions of patients prognoses or
clinically interpretable representations of disease pathophysiology, but not
both. In this paper, we develop the phased attentive state space (PASS) model
of disease progression, a deep probabilistic model that captures complex
representations for disease progression while maintaining clinical
interpretability. Unlike Markovian state space models which assume memoryless
dynamics, PASS uses an attention mechanism to induce "memoryful" state
transitions, whereby repeatedly updated attention weights are used to focus on
past state realizations that best predict future states. This gives rise to
complex, non-stationary state dynamics that remain interpretable through the
generated attention weights, which designate the relationships between the
realized state variables for individual patients. PASS uses phased LSTM units
(with time gates controlled by parametrized oscillations) to generate the
attention weights in continuous time, which enables handling
irregularly-sampled and potentially missing medical observations. Experiments
on data from a realworld cohort of patients show that PASS successfully
balances the tradeoff between accuracy and interpretability: it demonstrates
superior predictive accuracy and learns insightful individual-level
representations of disease progression.


Generalized Concordance for Competing Risks

  Existing metrics in competing risks survival analysis such as concordance and
accuracy do not evaluate a model's ability to jointly predict the event type
and the event time. To address these limitations, we propose a new metric,
which we call the generalized concordance. The different components of the
generalized concordance correspond to the probabilities that a model makes an
error in the event-type prediction only, or the discrimination only or both. We
develop a consistent estimator for the new metric that accounts for the
censoring bias. Using the real and synthetic data experiments, we show that
models selected using the existing metrics are worse than those selected using
generalized concordance at jointly predicting the event type and event time. We
use the new metric to develop a variable importance ranking approach, which we
call the stepwise competing risks regression. The purpose of this approach is
to identify the factors that are important for predicting both the event type
and the event time. We use real and synthetic datasets to show that the
existing approaches for variable importance ranking often fail to recognize the
importance of the event-specific risk factors, whereas, our approach does not.


Estimation of Individual Treatment Effect in Latent Confounder Models
  via Adversarial Learning

  Estimating the individual treatment effect (ITE) from observational data is
essential in medicine. A central challenge in estimating the ITE is handling
confounders, which are factors that affect both an intervention and its
outcome. Most previous work relies on the unconfoundedness assumption, which
posits that all the confounders are measured in the observational data.
However, if there are unmeasurable (latent) confounders, then confounding bias
is introduced. Fortunately, noisy proxies for the latent confounders are often
available and can be used to make an unbiased estimate of the ITE. In this
paper, we develop a novel adversarial learning framework to make unbiased
estimates of the ITE using noisy proxies.


