The Libra Toolkit for Probabilistic Models

  The Libra Toolkit is a collection of algorithms for learning and inferencewith discrete probabilistic models, including Bayesian networks, Markovnetworks, dependency networks, and sum-product networks. Compared to othertoolkits, Libra places a greater emphasis on learning the structure oftractable models in which exact inference is efficient. It also includes avariety of algorithms for learning graphical models in which inference ispotentially intractable, and for performing exact and approximate inference.Libra is released under a 2-clause BSD license to encourage broad use inacademia and industry.

Learning Arithmetic Circuits

  Graphical models are usually learned without regard to the cost of doinginference with them. As a result, even if a good model is learned, it mayperform poorly at prediction, because it requires approximate inference. Wepropose an alternative: learning models with a score function that directlypenalizes the cost of inference. Specifically, we learn arithmetic circuitswith a penalty on the number of edges in the circuit (in which the cost ofinference is linear). Our algorithm is equivalent to learning a Bayesiannetwork with context-specific independence by greedily splitting conditionaldistributions, at each step scoring the candidates by compiling the resultingnetwork into an arithmetic circuit, and using its size as the penalty. We showhow this can be done efficiently, without compiling a circuit from scratch foreach candidate. Experiments on several real-world domains show that ouralgorithm is able to learn tractable models with very large treewidth, andyields more accurate predictions than a standard context-specific Bayesiannetwork learner, in far less time.

Closed-Form Learning of Markov Networks from Dependency Networks

  Markov networks (MNs) are a powerful way to compactly represent a jointprobability distribution, but most MN structure learning methods are very slow,due to the high cost of evaluating candidates structures. Dependency networks(DNs) represent a probability distribution as a set of conditional probabilitydistributions. DNs are very fast to learn, but the conditional distributionsmay be inconsistent with each other and few inference algorithms support DNs.In this paper, we present a closed-form method for converting a DN into an MN,allowing us to enjoy both the efficiency of DN learning and the convenience ofthe MN representation. When the DN is consistent, this conversion is exact. Forinconsistent DNs, we present averaging methods that significantly improve theapproximation. In experiments on 12 standard datasets, our methods are ordersof magnitude faster than and often more accurate than combining conditionaldistributions using weight learning.

HotFlip: White-Box Adversarial Examples for Text Classification

  We propose an efficient method to generate white-box adversarial examples totrick a character-level neural classifier. We find that only a fewmanipulations are needed to greatly decrease the accuracy. Our method relies onan atomic flip operation, which swaps one token for another, based on thegradients of the one-hot input vectors. Due to efficiency of our method, we canperform adversarial training which makes the model more robust to attacks attest time. With the use of a few semantics-preserving constraints, wedemonstrate that HotFlip can be adapted to attack a word-level classifier aswell.

Ontology Matching with Knowledge Rules

  Ontology matching is the process of automatically determining the semanticequivalences between the concepts of two ontologies. Most ontology matchingalgorithms are based on two types of strategies: terminology-based strategies,which align concepts based on their names or descriptions, and structure-basedstrategies, which exploit concept hierarchies to find the alignment. In manydomains, there is additional information about the relationships of conceptsrepresented in various ways, such as Bayesian networks, decision trees, andassociation rules. We propose to use the similarities between theserelationships to find more accurate alignments. We accomplish this by definingsoft constraints that prefer alignments where corresponding concepts have thesame local relationships encoded as knowledge rules. We use a probabilisticframework to integrate this new knowledge-based strategy with standardterminology-based and structure-based strategies. Furthermore, our method isparticularly effective in identifying correspondences between complex concepts.Our method achieves substantially better F-score than the previousstate-of-the-art on three ontology matching domains.

A Probabilistic Approach to Knowledge Translation

  In this paper, we focus on a novel knowledge reuse scenario where theknowledge in the source schema needs to be translated to a semanticallyheterogeneous target schema. We refer to this task as "knowledge translation"(KT). Unlike data translation and transfer learning, KT does not require anydata from the source or target schema. We adopt a probabilistic approach to KTby representing the knowledge in the source schema, the mapping between thesource and target schemas, and the resulting knowledge in the target schema allas probability distributions, specially using Markov random fields and Markovlogic networks. Given the source knowledge and mappings, we use standardlearning and inference algorithms for probabilistic graphical models to find anexplicit probability distribution in the target schema that minimizes theKullback-Leibler divergence from the implicit distribution. This gives us acompact probabilistic model that represents knowledge from the source schema aswell as possible, respecting the uncertainty in both the source knowledge andthe mapping. In experiments on both propositional and relational domains, wefind that the knowledge obtained by KT is comparable to other approaches thatrequire data, demonstrating that knowledge can be reused without data.

On Adversarial Examples for Character-Level Neural Machine Translation

  Evaluating on adversarial examples has become a standard procedure to measurerobustness of deep learning models. Due to the difficulty of creating white-boxadversarial examples for discrete text input, most analyses of the robustnessof NLP models have been done through black-box adversarial examples. Weinvestigate adversarial examples for character-level neural machine translation(NMT), and contrast black-box adversaries with a novel white-box adversary,which employs differentiable string-edit operations to rank adversarialchanges. We propose two novel types of attacks which aim to remove or change aword in a translation, rather than simply break the NMT. We demonstrate thatwhite-box adversarial examples are significantly stronger than their black-boxcounterparts in different attack scenarios, which show more seriousvulnerabilities than previously known. In addition, after performingadversarial training, which takes only 3 times longer than regular training, wecan improve the model's robustness significantly.

Neural-Symbolic Learning and Reasoning: A Survey and Interpretation

  The study and understanding of human behaviour is relevant to computerscience, artificial intelligence, neural computation, cognitive science,philosophy, psychology, and several other areas. Presupposing cognition asbasis of behaviour, among the most prominent tools in the modelling ofbehaviour are computational-logic systems, connectionist models of cognition,and models of uncertainty. Recent studies in cognitive science, artificialintelligence, and psychology have produced a number of cognitive models ofreasoning, learning, and language that are underpinned by computation. Inaddition, efforts in computer science research have led to the development ofcognitive computational systems integrating machine learning and automatedreasoning. Such systems have shown promise in a range of applications,including computational biology, fault diagnosis, training and assessment insimulators, and software verification. This joint survey reviews the personalideas and views of several researchers on neural-symbolic learning andreasoning. The article is organised in three parts: Firstly, we frame the scopeand goals of neural-symbolic computation and have a look at the theoreticalfoundations. We then proceed to describe the realisations of neural-symboliccomputation, systems, and applications. Finally we present the challengesfacing the area and avenues for further research.

