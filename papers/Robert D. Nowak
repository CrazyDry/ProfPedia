The Geometry of Generalized Binary Search

  This paper investigates the problem of determining a binary-valued function
through a sequence of strategically selected queries. The focus is an algorithm
called Generalized Binary Search (GBS). GBS is a well-known greedy algorithm
for determining a binary-valued function through a sequence of strategically
selected queries. At each step, a query is selected that most evenly splits the
hypotheses under consideration into two disjoint subsets, a natural
generalization of the idea underlying classic binary search. This paper
develops novel incoherence and geometric conditions under which GBS achieves
the information-theoretically optimal query complexity; i.e., given a
collection of N hypotheses, GBS terminates with the correct function after no
more than a constant times log N queries. Furthermore, a noise-tolerant version
of GBS is developed that also achieves the optimal query complexity. These
results are applied to learning halfspaces, a problem arising routinely in
image processing and machine learning.


Sparse Estimation with Strongly Correlated Variables using Ordered
  Weighted L1 Regularization

  This paper studies ordered weighted L1 (OWL) norm regularization for sparse
estimation problems with strongly correlated variables. We prove sufficient
conditions for clustering based on the correlation/colinearity of variables
using the OWL norm, of which the so-called OSCAR is a particular case. Our
results extend previous ones for OSCAR in several ways: for the squared error
loss, our conditions hold for the more general OWL norm and under weaker
assumptions; we also establish clustering conditions for the absolute error
loss, which is, as far as we know, a novel result. Furthermore, we characterize
the statistical performance of OWL norm regularization for generative models in
which certain clusters of regression variables are strongly (even perfectly)
correlated, but variables in different clusters are uncorrelated. We show that
if the true p-dimensional signal generating the data involves only s of the
clusters, then O(s log p) samples suffice to accurately estimate the signal,
regardless of the number of coefficients within the clusters. The estimation of
s-sparse signals with completely independent variables requires just as many
measurements. In other words, using the OWL we pay no price (in terms of the
number of measurements) for the presence of strongly correlated variables.


Deterministic Conditions for Subspace Identifiability from Incomplete
  Sampling

  Consider a generic $r$-dimensional subspace of $\mathbb{R}^d$, $r<d$, and
suppose that we are only given projections of this subspace onto small subsets
of the canonical coordinates. The paper establishes necessary and sufficient
deterministic conditions on the subsets for subspace identifiability.


Thermalization of magnetically trapped metastable helium

  We have observed thermalization by elastic collisions of magnetically trapped
metastable helium atoms. Our method directly samples the reconstruction of a
thermal energy distribution after the application of an RF knife. The
relaxation time of our sample towards equilibrium gives an elastic collision
rate constant close to the unitarity limit.


Near-Optimal Compressive Binary Search

  We propose a simple modification to the recently proposed compressive binary
search. The modification removes an unnecessary and suboptimal factor of log
log n from the SNR requirement, making the procedure optimal (up to a small
constant). Simulations show that the new procedure performs significantly
better in practice as well. We also contrast this problem with the more well
known problem of noisy binary search.


Concentration Inequalities for the Empirical Distribution

  We study concentration inequalities for the Kullback--Leibler (KL) divergence
between the empirical distribution and the true distribution. Applying a
recursion technique, we improve over the method of types bound uniformly in all
regimes of sample size $n$ and alphabet size $k$, and the improvement becomes
more significant when $k$ is large. We discuss the applications of our results
in obtaining tighter concentration inequalities for $L_1$ deviations of the
empirical distribution from the true distribution, and the difference between
concentration around the expectation or zero.


Detecting Weak but Hierarchically-Structured Patterns in Networks

  The ability to detect weak distributed activation patterns in networks is
critical to several applications, such as identifying the onset of anomalous
activity or incipient congestion in the Internet, or faint traces of a
biochemical spread by a sensor network. This is a challenging problem since
weak distributed patterns can be invisible in per node statistics as well as a
global network-wide aggregate. Most prior work considers situations in which
the activation/non-activation of each node is statistically independent, but
this is unrealistic in many problems. In this paper, we consider structured
patterns arising from statistical dependencies in the activation process. Our
contributions are three-fold. First, we propose a sparsifying transform that
succinctly represents structured activation patterns that conform to a
hierarchical dependency graph. Second, we establish that the proposed transform
facilitates detection of very weak activation patterns that cannot be detected
with existing methods. Third, we show that the structure of the hierarchical
dependency graph governing the activation process, and hence the network
transform, can be learnt from very few (logarithmic in network size)
independent snapshots of network activity.


On the Limits of Sequential Testing in High Dimensions

  This paper presents results pertaining to sequential methods for support
recovery of sparse signals in noise. Specifically, we show that any sequential
measurement procedure fails provided the average number of measurements per
dimension grows slower then log s / D(f0||f1) where s is the level of sparsity,
and D(f0||f1) the Kullback-Leibler divergence between the underlying
distributions. For comparison, we show any non-sequential procedure fails
provided the number of measurements grows at a rate less than log n /
D(f1||f0), where n is the total dimension of the problem. Lastly, we show that
a simple procedure termed sequential thresholding guarantees exact support
recovery provided the average number of measurements per dimension grows faster
than (log s + log log n) / D(f0||f1), a mere additive factor more than the
lower bound.


Linear Bandits with Feature Feedback

  This paper explores a new form of the linear bandit problem in which the
algorithm receives the usual stochastic rewards as well as stochastic feedback
about which features are relevant to the rewards, the latter feedback being the
novel aspect. The focus of this paper is the development of new theory and
algorithms for linear bandits with feature feedback. We show that linear
bandits with feature feedback can achieve regret over time horizon $T$ that
scales like $k\sqrt{T}$, without prior knowledge of which features are relevant
nor the number $k$ of relevant features. In comparison, the regret of
traditional linear bandits is $d\sqrt{T}$, where $d$ is the total number of
(relevant and irrelevant) features, so the improvement can be dramatic if $k\ll
d$. The computational complexity of the new algorithm is proportional to $k$
rather than $d$, making it much more suitable for real-world applications
compared to traditional linear bandits. We demonstrate the performance of the
new algorithm with synthetic and real human-labeled data.


Multiscale likelihood analysis and complexity penalized estimation

  We describe here a framework for a certain class of multiscale likelihood
factorizations wherein, in analogy to a wavelet decomposition of an L^2
function, a given likelihood function has an alternative representation as a
product of conditional densities reflecting information in both the data and
the parameter vector localized in position and scale. The framework is
developed as a set of sufficient conditions for the existence of such
factorizations, formulated in analogy to those underlying a standard
multiresolution analysis for wavelets, and hence can be viewed as a
multiresolution analysis for likelihoods. We then consider the use of these
factorizations in the task of nonparametric, complexity penalized likelihood
estimation. We study the risk properties of certain thresholding and
partitioning estimators, and demonstrate their adaptivity and near-optimality,
in a minimax sense over a broad range of function spaces, based on squared
Hellinger distance as a loss function. In particular, our results provide an
illustration of how properties of classical wavelet-based estimators can be
obtained in a single, unified framework that includes models for continuous,
count and categorical data types.


Domain wall mobility in nanowires: transverse versus vortex walls

  The motion of domain walls in ferromagnetic, cylindrical nanowires is
investigated numerically by solving the Landau-Lifshitz-Gilbert equation for a
classical spin model in which energy contributions from exchange, crystalline
anisotropy, dipole-dipole interaction, and a driving magnetic field are
considered. Depending on the diameter, either transverse domain walls or vortex
walls are found. The transverse domain wall is observed for diameters smaller
than the exchange length of the given material. Here, the system behaves
effectively one-dimensional and the domain wall mobility agrees with a result
derived for a one-dimensional wall by Slonczewski. For low damping the domain
wall mobility decreases with decreasing damping constant. With increasing
diameter, a crossover to a vortex wall sets in which enhances the domain wall
mobility drastically. For a vortex wall the domain wall mobility is described
by the Walker-formula, with a domain wall width depending on the diameter of
the wire. The main difference is the dependence on damping: for a vortex wall
the domain wall mobility can be drastically increased for small values of the
damping constant up to a factor of $1/\alpha^2$.


Convex Approaches to Model Wavelet Sparsity Patterns

  Statistical dependencies among wavelet coefficients are commonly represented
by graphical models such as hidden Markov trees(HMTs). However, in linear
inverse problems such as deconvolution, tomography, and compressed sensing, the
presence of a sensing or observation matrix produces a linear mixing of the
simple Markovian dependency structure. This leads to reconstruction problems
that are non-convex optimizations. Past work has dealt with this issue by
resorting to greedy or suboptimal iterative reconstruction methods. In this
paper, we propose new modeling approaches based on group-sparsity penalties
that leads to convex optimizations that can be solved exactly and efficiently.
We show that the methods we develop perform significantly better in
deconvolution and compressed sensing applications, while being as
computationally efficient as standard coefficient-wise approaches such as
lasso.


Query Complexity of Derivative-Free Optimization

  This paper provides lower bounds on the convergence rate of Derivative Free
Optimization (DFO) with noisy function evaluations, exposing a fundamental and
unavoidable gap between the performance of algorithms with access to gradients
and those with access to only function evaluations. However, there are
situations in which DFO is unavoidable, and for such situations we propose a
new DFO algorithm that is proved to be near optimal for the class of strongly
convex objective functions. A distinctive feature of the algorithm is that it
uses only Boolean-valued function comparisons, rather than function
evaluations. This makes the algorithm useful in an even wider range of
applications, such as optimization based on paired comparisons from human
subjects, for example. We also show that regardless of whether DFO is based on
noisy function evaluations or Boolean-valued function comparisons, the
convergence rate is the same.


Active Learning for Undirected Graphical Model Selection

  This paper studies graphical model selection, i.e., the problem of estimating
a graph of statistical relationships among a collection of random variables.
Conventional graphical model selection algorithms are passive, i.e., they
require all the measurements to have been collected before processing begins.
We propose an active learning algorithm that uses junction tree representations
to adapt future measurements based on the information gathered from prior
measurements. We prove that, under certain conditions, our active learning
algorithm requires fewer scalar measurements than any passive algorithm to
reliably estimate a graph. A range of numerical results validate our theory and
demonstrates the benefits of active learning.


Algebraic Variety Models for High-Rank Matrix Completion

  We consider a generalization of low-rank matrix completion to the case where
the data belongs to an algebraic variety, i.e. each data point is a solution to
a system of polynomial equations. In this case the original matrix is possibly
high-rank, but it becomes low-rank after mapping each column to a higher
dimensional space of monomial features. Many well-studied extensions of linear
models, including affine subspaces and their union, can be described by a
variety model. In addition, varieties can be used to model a richer class of
nonlinear quadratic and higher degree curves and surfaces. We study the
sampling requirements for matrix completion under a variety model with a focus
on a union of affine subspaces. We also propose an efficient matrix completion
algorithm that minimizes a convex or non-convex surrogate of the rank of the
matrix of monomial features. Our algorithm uses the well-known "kernel trick"
to avoid working directly with the high-dimensional monomial matrix. We show
the proposed algorithm is able to recover synthetically generated data up to
the predicted sampling complexity bounds. The proposed algorithm also
outperforms standard low rank matrix completion and subspace clustering
techniques in experiments with real data.


Sequential Testing for Sparse Recovery

  This paper studies sequential methods for recovery of sparse signals in high
dimensions. When compared to fixed sample size procedures, in the sparse
setting, sequential methods can result in a large reduction in the number of
samples needed for reliable signal support recovery. Starting with a lower
bound, we show any coordinate-wise sequential sampling procedure fails in the
high dimensional limit provided the average number of measurements per
dimension is less then log s/D(P_0||P_1) where s is the level of sparsity and
D(P_0||P_1) the Kullback-Leibler divergence between the underlying
distributions. A series of Sequential Probability Ratio Tests (SPRT) which
require complete knowledge of the underlying distributions is shown to achieve
this bound. Motivated by real world experiments and recent work in adaptive
sensing, we introduce a simple procedure termed Sequential Thresholding which
can be implemented when the underlying testing problem satisfies a monotone
likelihood ratio assumption. Sequential Thresholding guarantees exact support
recovery provided the average number of measurements per dimension grows faster
than log s/ D(P_0||P_1), achieving the lower bound. For comparison, we show any
non-sequential procedure fails provided the number of measurements grows at a
rate less than log n/D(P_1||P_0), where n is the total dimension of the
problem.


Active Ranking using Pairwise Comparisons

  This paper examines the problem of ranking a collection of objects using
pairwise comparisons (rankings of two objects). In general, the ranking of $n$
objects can be identified by standard sorting methods using $n log_2 n$
pairwise comparisons. We are interested in natural situations in which
relationships among the objects may allow for ranking using far fewer pairwise
comparisons. Specifically, we assume that the objects can be embedded into a
$d$-dimensional Euclidean space and that the rankings reflect their relative
distances from a common reference point in $R^d$. We show that under this
assumption the number of possible rankings grows like $n^{2d}$ and demonstrate
an algorithm that can identify a randomly selected ranking using just slightly
more than $d log n$ adaptively selected pairwise comparisons, on average. If
instead the comparisons are chosen at random, then almost all pairwise
comparisons must be made in order to identify any ranking. In addition, we
propose a robust, error-tolerant algorithm that only requires that the pairwise
comparisons are probably correct. Experimental studies with synthetic and real
datasets support the conclusions of our theoretical analysis.


A Characterization of Deterministic Sampling Patterns for Low-Rank
  Matrix Completion

  Low-rank matrix completion (LRMC) problems arise in a wide variety of
applications. Previous theory mainly provides conditions for completion under
missing-at-random samplings. This paper studies deterministic conditions for
completion. An incomplete $d \times N$ matrix is finitely rank-$r$ completable
if there are at most finitely many rank-$r$ matrices that agree with all its
observed entries. Finite completability is the tipping point in LRMC, as a few
additional samples of a finitely completable matrix guarantee its unique
completability. The main contribution of this paper is a deterministic sampling
condition for finite completability. We use this to also derive deterministic
sampling conditions for unique completability that can be efficiently verified.
We also show that under uniform random sampling schemes, these conditions are
satisfied with high probability if $O(\max\{r,\log d\})$ entries per column are
observed. These findings have several implications on LRMC regarding lower
bounds, sample and computational complexity, the role of coherence, adaptive
settings and the validation of any completion algorithm. We complement our
theoretical results with experiments that support our findings and motivate
future analysis of uncharted sampling regimes.


Learning the Interference Graph of a Wireless Network

  A key challenge in wireless networking is the management of interference
between transmissions. Identifying which transmitters interfere with each other
is a crucial first step. In this paper we cast the task of estimating the a
wireless interference environment as a graph learning problem. Nodes represent
transmitters and edges represent the presence of interference between pairs of
transmitters. We passively observe network traffic transmission patterns and
collect information on transmission successes and failures. We establish bounds
on the number of observations (each a snapshot of a network traffic pattern)
required to identify the interference graph reliably with high probability.
  Our main results are scaling laws that tell us how the number of observations
must grow in terms of the total number of nodes $n$ in the network and the
maximum number of interfering transmitters $d$ per node (maximum node degree).
The effects of hidden terminal interference (i.e., interference not detectable
via carrier sensing) on the observation requirements are also quantified. We
show that to identify the graph it is necessary and sufficient that the
observation period grows like $d^2 \log n$, and we propose a practical
algorithm that reliably identifies the graph from this length of observation.
The observation requirements scale quite mildly with network size, and networks
with sparse interference (small $d$) can be identified more rapidly.
Computational experiments based on a realistic simulations of the traffic and
protocol lend additional support to these conclusions.


Adaptive Hausdorff estimation of density level sets

  Consider the problem of estimating the $\gamma$-level set
$G^*_{\gamma}=\{x:f(x)\geq\gamma\}$ of an unknown $d$-dimensional density
function $f$ based on $n$ independent observations $X_1,...,X_n$ from the
density. This problem has been addressed under global error criteria related to
the symmetric set difference. However, in certain applications a spatially
uniform mode of convergence is desirable to ensure that the estimated set is
close to the target set everywhere. The Hausdorff error criterion provides this
degree of uniformity and, hence, is more appropriate in such situations. It is
known that the minimax optimal rate of error convergence for the Hausdorff
metric is $(n/\log n)^{-1/(d+2\alpha)}$ for level sets with boundaries that
have a Lipschitz functional form, where the parameter $\alpha$ characterizes
the regularity of the density around the level of interest. However, the
estimators proposed in previous work are nonadaptive to the density regularity
and require knowledge of the parameter $\alpha$. Furthermore, previously
developed estimators achieve the minimax optimal rate for rather restricted
classes of sets (e.g., the boundary fragment and star-shaped sets) that
effectively reduce the set estimation problem to a function estimation problem.
This characterization precludes level sets with multiple connected components,
which are fundamental to many applications. This paper presents a fully
data-driven procedure that is adaptive to unknown regularity conditions and
achieves near minimax optimal Hausdorff error control for a class of density
level sets with very general shapes and multiple connected components.


The Sample Complexity of Search over Multiple Populations

  This paper studies the sample complexity of searching over multiple
populations. We consider a large number of populations, each corresponding to
either distribution P0 or P1. The goal of the search problem studied here is to
find one population corresponding to distribution P1 with as few samples as
possible. The main contribution is to quantify the number of samples needed to
correctly find one such population. We consider two general approaches:
non-adaptive sampling methods, which sample each population a predetermined
number of times until a population following P1 is found, and adaptive sampling
methods, which employ sequential sampling schemes for each population. We first
derive a lower bound on the number of samples required by any sampling scheme.
We then consider an adaptive procedure consisting of a series of sequential
probability ratio tests, and show it comes within a constant factor of the
lower bound. We give explicit expressions for this constant when samples of the
populations follow Gaussian and Bernoulli distributions. An alternative
adaptive scheme is discussed which does not require full knowledge of P1, and
comes within a constant factor of the optimal scheme. For comparison, a lower
bound on the sampling requirements of any non-adaptive scheme is presented.


Near-Optimal Adaptive Compressed Sensing

  This paper proposes a simple adaptive sensing and group testing algorithm for
sparse signal recovery. The algorithm, termed Compressive Adaptive Sense and
Search (CASS), is shown to be near-optimal in that it succeeds at the lowest
possible signal-to-noise-ratio (SNR) levels, improving on previous work in
adaptive compressed sensing. Like traditional compressed sensing based on
random non-adaptive design matrices, the CASS algorithm requires only k log n
measurements to recover a k-sparse signal of dimension n. However, CASS
succeeds at SNR levels that are a factor log n less than required by standard
compressed sensing. From the point of view of constructing and implementing the
sensing operation as well as computing the reconstruction, the proposed
algorithm is substantially less computationally intensive than standard
compressed sensing. CASS is also demonstrated to perform considerably better in
practice through simulation. To the best of our knowledge, this is the first
demonstration of an adaptive compressed sensing algorithm with near-optimal
theoretical guarantees and excellent practical performance. This paper also
shows that methods like compressed sensing, group testing, and pooling have an
advantage beyond simply reducing the number of measurements or tests --
adaptive versions of such methods can also improve detection and estimation
performance when compared to non-adaptive direct (uncompressed) sensing.


Tensor Methods for Nonlinear Matrix Completion

  In the low rank matrix completion (LRMC) problem, the low rank assumption
means that the columns (or rows) of the matrix to be completed are points on a
low-dimensional linear algebraic variety. This paper extends this thinking to
cases where the columns are points on a low-dimensional nonlinear algebraic
variety, a problem we call Low Algebraic Dimension Matrix Completion (LADMC).
Matrices whose columns belong to a union of subspaces (UoS) are an important
special case. We propose a LADMC algorithm that leverages existing LRMC methods
on a tensorized representation of the data. For example, a second-order
tensorization representation is formed by taking the outer product of each
column with itself, and we consider higher order tensorizations as well. This
approach will succeed in many cases where traditional LRMC is guaranteed to
fail because the data are low-rank in the tensorized representation but not in
the original representation. We also provide a formal mathematical
justification for the success of our method. In particular, we show bounds of
the rank of these data in the tensorized representation, and we prove sampling
requirements to guarantee uniqueness of the solution. Interestingly, the
sampling requirements of our LADMC algorithm nearly match the information
theoretic lower bounds for matrix completion under a UoS model. We also provide
experimental results showing that the new approach significantly outperforms
existing state-of-the-art methods for matrix completion in many situations.


Doppler Monitoring of five K2 Transiting Planetary Systems

  In an effort to measure the masses of planets discovered by the NASA {\it K2}
mission, we have conducted precise Doppler observations of five stars with
transiting planets. We present the results of a joint analysis of these new
data and previously published Doppler data. The first star, an M dwarf known as
K2-3 or EPIC~201367065, has three transiting planets ("b", with radius
$2.1~R_{\oplus}$; "c", $1.7~R_{\oplus}$; and "d", $1.5~R_{\oplus}$). Our
analysis leads to the mass constraints: $M_{b}=8.1^{+2.0}_{-1.9}~M_{\oplus}$
and $M_{c}$ < $ 4.2~M_{\oplus}$~(95\%~conf.). The mass of planet d is poorly
constrained because its orbital period is close to the stellar rotation period,
making it difficult to disentangle the planetary signal from spurious Doppler
shifts due to stellar activity. The second star, a G dwarf known as K2-19 or
EPIC~201505350, has two planets ("b", $7.7~R_{\oplus}$; and "c",
$4.9~R_{\oplus}$) in a 3:2 mean-motion resonance, as well as a shorter-period
planet ("d", $1.1~R_{\oplus}$). We find $M_{b}$= $28.5^{+5.4}_{-5.0}
~M_{\oplus}$, $M_{c}$= $25.6^{+7.1}_{-7.1} ~M_{\oplus}$ and $M_{d}$ <
$14.0~M_{\oplus} $~(95\%~conf.). The third star, a G dwarf known as K2-24 or
EPIC~203771098, hosts two transiting planets ("b", $5.7~R_{\oplus}$; and "c",
$7.8~R_{\oplus}$) with orbital periods in a nearly 2:1 ratio. We find $M_{b}$=
$19.8^{+4.5}_{-4.4} ~M_{\oplus}$ and $M_{c}$ =
$26.0^{+5.8}_{-6.1}~M_{\oplus}$.....


Scalable Generalized Linear Bandits: Online Computation and Hashing

  Generalized Linear Bandits (GLBs), a natural extension of the stochastic
linear bandits, has been popular and successful in recent years. However,
existing GLBs scale poorly with the number of rounds and the number of arms,
limiting their utility in practice. This paper proposes new, scalable solutions
to the GLB problem in two respects. First, unlike existing GLBs, whose
per-time-step space and time complexity grow at least linearly with time $t$,
we propose a new algorithm that performs online computations to enjoy a
constant space and time complexity. At its heart is a novel Generalized Linear
extension of the Online-to-confidence-set Conversion (GLOC method) that takes
\emph{any} online learning algorithm and turns it into a GLB algorithm. As a
special case, we apply GLOC to the online Newton step algorithm, which results
in a low-regret GLB algorithm with much lower time and memory complexity than
prior work. Second, for the case where the number $N$ of arms is very large, we
propose new algorithms in which each next arm is selected via an inner product
search. Such methods can be implemented via hashing algorithms (i.e.,
"hash-amenable") and result in a time complexity sublinear in $N$. While a
Thompson sampling extension of GLOC is hash-amenable, its regret bound for
$d$-dimensional arm sets scales with $d^{3/2}$, whereas GLOC's regret bound
scales with $d$. Towards closing this gap, we propose a new hash-amenable
algorithm whose regret bound scales with $d^{5/4}$. Finally, we propose a fast
approximate hash-key computation (inner product) with a better accuracy than
the state-of-the-art, which can be of independent interest. We conclude the
paper with preliminary experimental results confirming the merits of our
methods.


Sketching Sparse Matrices

  This paper considers the problem of recovering an unknown sparse p\times p
matrix X from an m\times m matrix Y=AXB^T, where A and B are known m \times p
matrices with m << p.
  The main result shows that there exist constructions of the "sketching"
matrices A and B so that even if X has O(p) non-zeros, it can be recovered
exactly and efficiently using a convex program as long as these non-zeros are
not concentrated in any single row/column of X. Furthermore, it suffices for
the size of Y (the sketch dimension) to scale as m = O(\sqrt{# nonzeros in X}
\times log p). The results also show that the recovery is robust and stable in
the sense that if X is equal to a sparse matrix plus a perturbation, then the
convex program we propose produces an approximation with accuracy proportional
to the size of the perturbation. Unlike traditional results on sparse recovery,
where the sensing matrix produces independent measurements, our sensing
operator is highly constrained (it assumes a tensor product structure).
Therefore, proving recovery guarantees require non-standard techniques. Indeed
our approach relies on a novel result concerning tensor products of bipartite
graphs, which may be of independent interest.
  This problem is motivated by the following application, among others.
Consider a p\times n data matrix D, consisting of n observations of p
variables. Assume that the correlation matrix X:=DD^{T} is (approximately)
sparse in the sense that each of the p variables is significantly correlated
with only a few others. Our results show that these significant correlations
can be detected even if we have access to only a sketch of the data S=AD with A
\in R^{m\times p}.


The K2-ESPRINT Project V: a short-period giant planet orbiting a
  subgiant star

  We report on the discovery and characterization of the transiting planet
K2-39b (EPIC 206247743b). With an orbital period of 4.6 days, it is the
shortest-period planet orbiting a subgiant star known to date. Such planets are
rare, with only a handful of known cases. The reason for this is poorly
understood, but may reflect differences in planet occurrence around the
relatively high-mass stars that have been surveyed, or may be the result of
tidal destruction of such planets. K2-39 is an evolved star with a
spectroscopically derived stellar radius and mass of
$3.88^{+0.48}_{-0.42}~\mathrm{R_\odot}$ and
$1.53^{+0.13}_{-0.12}~\mathrm{M_\odot}$, respectively, and a very close-in
transiting planet, with $a/R_\star = 3.4$. Radial velocity (RV) follow-up using
the HARPS, FIES and PFS instruments leads to a planetary mass of
$50.3^{+9.7}_{-9.4}~\mathrm{M_\oplus}$. In combination with a radius
measurement of $8.3 \pm 1.1~\mathrm{R_\oplus}$, this results in a mean
planetary density of $0.50^{+0.29}_{-0.17}$ g~cm$^{-3}$. We furthermore
discover a long-term RV trend, which may be caused by a long-period planet or
stellar companion. Because K2-39b has a short orbital period, its existence
makes it seem unlikely that tidal destruction is wholly responsible for the
differences in planet populations around subgiant and main-sequence stars.
Future monitoring of the transits of this system may enable the detection of
period decay and constrain the tidal dissipation rates of subgiant stars.


Measurement of $ν_μ$-induced charged-current neutral pion production
  cross sections on mineral oil at $E_ν\in0.5-2.0$ GeV

  Using a custom 3 \v{C}erenkov-ring fitter, we report cross sections for
$\nu_\mu$-induced charged-current single $\pi^0$ production on mineral oil
(\chtwo) from a sample of 5810 candidate events with 57% signal purity over an
energy range of $0.5-2.0$GeV. This includes measurements of the absolute total
cross section as a function of neutrino energy, and flux-averaged differential
cross sections measured in terms of $Q^2$, $\mu^-$ kinematics, and $\pi^0$
kinematics. The sample yields a flux-averaged total cross section of
$(9.2\pm0.3_{stat.}\pm1.5_{syst.})\times10^{-39}$cm$^2$/CH$_2$ at mean neutrino
energy of 0.965GeV.


The discovery and mass measurement of a new ultra-short-period planet:
  EPIC~228732031b

  We report the discovery of a new ultra-short-period planet and summarize the
properties of all such planets for which the mass and radius have been
measured. The new planet, EPIC~228732031b, was discovered in {\it K2} Campaign
10. It has a radius of 1.81$^{+0.16}_{-0.12}~R_{\oplus}$ and orbits a G dwarf
with a period of 8.9 hours. Radial velocities obtained with Magellan/PFS and
TNG/HARPS-N show evidence for stellar activity along with orbital motion. We
determined the planetary mass using two different methods: (1) the "floating
chunk offset" method, based only on changes in velocity observed on the same
night, and (2) a Gaussian process regression based on both the radial-velocity
and photometric time series. The results are consistent and lead to a mass
measurement of $6.5 \pm 1.6~M_{\oplus}$, and a mean density of
$6.0^{+3.0}_{-2.7}$~g~cm$^{-3}$.


First Muon-Neutrino Disappearance Study with an Off-Axis Beam

  We report a measurement of muon-neutrino disappearance in the T2K experiment.
The 295-km muon-neutrino beam from Tokai to Kamioka is the first implementation
of the off-axis technique in a long-baseline neutrino oscillation experiment.
With data corresponding to 1.43 10**20 protons on target, we observe 31
fully-contained single muon-like ring events in Super-Kamiokande, compared with
an expectation of 104 +- 14 (syst) events without neutrino oscillations. The
best-fit point for two-flavor nu_mu -> nu_tau oscillations is sin**2(2
theta_23) = 0.98 and |\Delta m**2_32| = 2.65 10**-3 eV**2. The boundary of the
90 % confidence region includes the points (sin**2(2 theta_23),|\Delta
m**2_32|) = (1.0, 3.1 10**-3 eV**2), (0.84, 2.65 10**-3 eV**2) and (1.0, 2.2
10**-3 eV**2).


Measurement of the neutrino-oxygen neutral-current interaction cross
  section by observing nuclear deexcitation $γ$ rays

  We report the first measurement of the neutrino-oxygen neutral-current
quasielastic (NCQE) cross section. It is obtained by observing nuclear
deexcitation $\gamma$-rays which follow neutrino-oxygen interactions at the
Super-Kamiokande water Cherenkov detector. We use T2K data corresponding to
$3.01 \times 10^{20}$ protons on target. By selecting only events during the
T2K beam window and with well-reconstructed vertices in the fiducial volume,
the large background rate from natural radioactivity is dramatically reduced.
We observe 43 events in the $4-30$ MeV reconstructed energy window, compared
with an expectation of 51.0, which includes an estimated 16.2 background
events. The background is primarily nonquasielastic neutral-current
interactions and has only 1.2 events from natural radioactivity. The
flux-averaged NCQE cross section we measure is $1.55 \times 10^{-38}$ cm$^2$
with a 68\% confidence interval of $(1.22, 2.20) \times 10^{-38}$ cm$^2$ at a
median neutrino energy of 630 MeV, compared with the theoretical prediction of
$2.01 \times 10^{-38}$ cm$^2$.


Measurements of the T2K neutrino beam properties using the INGRID
  on-axis near detector

  Precise measurement of neutrino beam direction and intensity was achieved
based on a new concept with modularized neutrino detectors. INGRID (Interactive
Neutrino GRID) is an on-axis near detector for the T2K long baseline neutrino
oscillation experiment. INGRID consists of 16 identical modules arranged in
horizontal and vertical arrays around the beam center. The module has a
sandwich structure of iron target plates and scintillator trackers. INGRID
directly monitors the muon neutrino beam profile center and intensity using the
number of observed neutrino events in each module. The neutrino beam direction
is measured with accuracy better than 0.4 mrad from the measured profile
center. The normalized event rate is measured with 4% precision.


The Long-Baseline Neutrino Experiment: Exploring Fundamental Symmetries
  of the Universe

  The preponderance of matter over antimatter in the early Universe, the
dynamics of the supernova bursts that produced the heavy elements necessary for
life and whether protons eventually decay --- these mysteries at the forefront
of particle physics and astrophysics are key to understanding the early
evolution of our Universe, its current state and its eventual fate. The
Long-Baseline Neutrino Experiment (LBNE) represents an extensively developed
plan for a world-class experiment dedicated to addressing these questions. LBNE
is conceived around three central components: (1) a new, high-intensity
neutrino source generated from a megawatt-class proton accelerator at Fermi
National Accelerator Laboratory, (2) a near neutrino detector just downstream
of the source, and (3) a massive liquid argon time-projection chamber deployed
as a far detector deep underground at the Sanford Underground Research
Facility. This facility, located at the site of the former Homestake Mine in
Lead, South Dakota, is approximately 1,300 km from the neutrino source at
Fermilab -- a distance (baseline) that delivers optimal sensitivity to neutrino
charge-parity symmetry violation and mass ordering effects. This ambitious yet
cost-effective design incorporates scalability and flexibility and can
accommodate a variety of upgrades and contributions. With its exceptional
combination of experimental configuration, technical capabilities, and
potential for transformative discoveries, LBNE promises to be a vital facility
for the field of particle physics worldwide, providing physicists from around
the globe with opportunities to collaborate in a twenty to thirty year program
of exciting science. In this document we provide a comprehensive overview of
LBNE's scientific objectives, its place in the landscape of neutrino physics
worldwide, the technologies it will incorporate and the capabilities it will
possess.


Indication of Electron Neutrino Appearance from an Accelerator-produced
  Off-axis Muon Neutrino Beam

  The T2K experiment observes indications of $\nu_\mu\rightarrow \nu_e$
appearance in data accumulated with $1.43\times10^{20}$ protons on target. Six
events pass all selection criteria at the far detector. In a three-flavor
neutrino oscillation scenario with $|\Delta m_{23}^2|=2.4\times10^{-3}$ eV$^2$,
$\sin^2 2\theta_{23}=1$ and $\sin^2 2\theta_{13}=0$, the expected number of
such events is 1.5$\pm$0.3(syst.). Under this hypothesis, the probability to
observe six or more candidate events is 7$\times10^{-3}$, equivalent to
2.5$\sigma$ significance. At 90% C.L., the data are consistent with
0.03(0.04)$<\sin^2 2\theta_{13}<$ 0.28(0.34) for $\delta_{\rm CP}=0$ and a
normal (inverted) hierarchy.


The Compact Linear $e^+e^-$ Collider (CLIC) - 2018 Summary Report

  The Compact Linear Collider (CLIC) is a TeV-scale high-luminosity linear
$e^+e^-$ collider under development at CERN. Following the CLIC conceptual
design published in 2012, this report provides an overview of the CLIC project,
its current status, and future developments. It presents the CLIC physics
potential and reports on design, technology, and implementation aspects of the
accelerator and the detector. CLIC is foreseen to be built and operated in
stages, at centre-of-mass energies of 380 GeV, 1.5 TeV and 3 TeV, respectively.
CLIC uses a two-beam acceleration scheme, in which 12 GHz accelerating
structures are powered via a high-current drive beam. For the first stage, an
alternative with X-band klystron powering is also considered. CLIC accelerator
optimisation, technical developments and system tests have resulted in an
increased energy efficiency (power around 170 MW) for the 380 GeV stage,
together with a reduced cost estimate at the level of 6 billion CHF. The
detector concept has been refined using improved software tools. Significant
progress has been made on detector technology developments for the tracking and
calorimetry systems. A wide range of CLIC physics studies has been conducted,
both through full detector simulations and parametric studies, together
providing a broad overview of the CLIC physics potential. Each of the three
energy stages adds cornerstones of the full CLIC physics programme, such as
Higgs width and couplings, top-quark properties, Higgs self-coupling, direct
searches, and many precision electroweak measurements. The interpretation of
the combined results gives crucial and accurate insight into new physics,
largely complementary to LHC and HL-LHC. The construction of the first CLIC
energy stage could start by 2026. First beams would be available by 2035,
marking the beginning of a broad CLIC physics programme spanning 25-30 years.


The T2K Experiment

  The T2K experiment is a long-baseline neutrino oscillation experiment. Its
main goal is to measure the last unknown lepton sector mixing angle
{\theta}_{13} by observing {\nu}_e appearance in a {\nu}_{\mu} beam. It also
aims to make a precision measurement of the known oscillation parameters,
{\Delta}m^{2}_{23} and sin^{2} 2{\theta}_{23}, via {\nu}_{\mu} disappearance
studies. Other goals of the experiment include various neutrino cross section
measurements and sterile neutrino searches. The experiment uses an intense
proton beam generated by the J-PARC accelerator in Tokai, Japan, and is
composed of a neutrino beamline, a near detector complex (ND280), and a far
detector (Super-Kamiokande) located 295 km away from J-PARC. This paper
provides a comprehensive review of the instrumentation aspect of the T2K
experiment and a summary of the vital information for each subsystem.


