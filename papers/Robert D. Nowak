The Geometry of Generalized Binary Search

  This paper investigates the problem of determining a binary-valued functionthrough a sequence of strategically selected queries. The focus is an algorithmcalled Generalized Binary Search (GBS). GBS is a well-known greedy algorithmfor determining a binary-valued function through a sequence of strategicallyselected queries. At each step, a query is selected that most evenly splits thehypotheses under consideration into two disjoint subsets, a naturalgeneralization of the idea underlying classic binary search. This paperdevelops novel incoherence and geometric conditions under which GBS achievesthe information-theoretically optimal query complexity; i.e., given acollection of N hypotheses, GBS terminates with the correct function after nomore than a constant times log N queries. Furthermore, a noise-tolerant versionof GBS is developed that also achieves the optimal query complexity. Theseresults are applied to learning halfspaces, a problem arising routinely inimage processing and machine learning.

Sparse Estimation with Strongly Correlated Variables using Ordered  Weighted L1 Regularization

  This paper studies ordered weighted L1 (OWL) norm regularization for sparseestimation problems with strongly correlated variables. We prove sufficientconditions for clustering based on the correlation/colinearity of variablesusing the OWL norm, of which the so-called OSCAR is a particular case. Ourresults extend previous ones for OSCAR in several ways: for the squared errorloss, our conditions hold for the more general OWL norm and under weakerassumptions; we also establish clustering conditions for the absolute errorloss, which is, as far as we know, a novel result. Furthermore, we characterizethe statistical performance of OWL norm regularization for generative models inwhich certain clusters of regression variables are strongly (even perfectly)correlated, but variables in different clusters are uncorrelated. We show thatif the true p-dimensional signal generating the data involves only s of theclusters, then O(s log p) samples suffice to accurately estimate the signal,regardless of the number of coefficients within the clusters. The estimation ofs-sparse signals with completely independent variables requires just as manymeasurements. In other words, using the OWL we pay no price (in terms of thenumber of measurements) for the presence of strongly correlated variables.

Deterministic Conditions for Subspace Identifiability from Incomplete  Sampling

  Consider a generic $r$-dimensional subspace of $\mathbb{R}^d$, $r<d$, andsuppose that we are only given projections of this subspace onto small subsetsof the canonical coordinates. The paper establishes necessary and sufficientdeterministic conditions on the subsets for subspace identifiability.

Thermalization of magnetically trapped metastable helium

  We have observed thermalization by elastic collisions of magnetically trappedmetastable helium atoms. Our method directly samples the reconstruction of athermal energy distribution after the application of an RF knife. Therelaxation time of our sample towards equilibrium gives an elastic collisionrate constant close to the unitarity limit.

Near-Optimal Compressive Binary Search

  We propose a simple modification to the recently proposed compressive binarysearch. The modification removes an unnecessary and suboptimal factor of loglog n from the SNR requirement, making the procedure optimal (up to a smallconstant). Simulations show that the new procedure performs significantlybetter in practice as well. We also contrast this problem with the more wellknown problem of noisy binary search.

Concentration Inequalities for the Empirical Distribution

  We study concentration inequalities for the Kullback--Leibler (KL) divergencebetween the empirical distribution and the true distribution. Applying arecursion technique, we improve over the method of types bound uniformly in allregimes of sample size $n$ and alphabet size $k$, and the improvement becomesmore significant when $k$ is large. We discuss the applications of our resultsin obtaining tighter concentration inequalities for $L_1$ deviations of theempirical distribution from the true distribution, and the difference betweenconcentration around the expectation or zero.

Detecting Weak but Hierarchically-Structured Patterns in Networks

  The ability to detect weak distributed activation patterns in networks iscritical to several applications, such as identifying the onset of anomalousactivity or incipient congestion in the Internet, or faint traces of abiochemical spread by a sensor network. This is a challenging problem sinceweak distributed patterns can be invisible in per node statistics as well as aglobal network-wide aggregate. Most prior work considers situations in whichthe activation/non-activation of each node is statistically independent, butthis is unrealistic in many problems. In this paper, we consider structuredpatterns arising from statistical dependencies in the activation process. Ourcontributions are three-fold. First, we propose a sparsifying transform thatsuccinctly represents structured activation patterns that conform to ahierarchical dependency graph. Second, we establish that the proposed transformfacilitates detection of very weak activation patterns that cannot be detectedwith existing methods. Third, we show that the structure of the hierarchicaldependency graph governing the activation process, and hence the networktransform, can be learnt from very few (logarithmic in network size)independent snapshots of network activity.

On the Limits of Sequential Testing in High Dimensions

  This paper presents results pertaining to sequential methods for supportrecovery of sparse signals in noise. Specifically, we show that any sequentialmeasurement procedure fails provided the average number of measurements perdimension grows slower then log s / D(f0||f1) where s is the level of sparsity,and D(f0||f1) the Kullback-Leibler divergence between the underlyingdistributions. For comparison, we show any non-sequential procedure failsprovided the number of measurements grows at a rate less than log n /D(f1||f0), where n is the total dimension of the problem. Lastly, we show thata simple procedure termed sequential thresholding guarantees exact supportrecovery provided the average number of measurements per dimension grows fasterthan (log s + log log n) / D(f0||f1), a mere additive factor more than thelower bound.

Linear Bandits with Feature Feedback

  This paper explores a new form of the linear bandit problem in which thealgorithm receives the usual stochastic rewards as well as stochastic feedbackabout which features are relevant to the rewards, the latter feedback being thenovel aspect. The focus of this paper is the development of new theory andalgorithms for linear bandits with feature feedback. We show that linearbandits with feature feedback can achieve regret over time horizon $T$ thatscales like $k\sqrt{T}$, without prior knowledge of which features are relevantnor the number $k$ of relevant features. In comparison, the regret oftraditional linear bandits is $d\sqrt{T}$, where $d$ is the total number of(relevant and irrelevant) features, so the improvement can be dramatic if $k\lld$. The computational complexity of the new algorithm is proportional to $k$rather than $d$, making it much more suitable for real-world applicationscompared to traditional linear bandits. We demonstrate the performance of thenew algorithm with synthetic and real human-labeled data.

Multiscale likelihood analysis and complexity penalized estimation

  We describe here a framework for a certain class of multiscale likelihoodfactorizations wherein, in analogy to a wavelet decomposition of an L^2function, a given likelihood function has an alternative representation as aproduct of conditional densities reflecting information in both the data andthe parameter vector localized in position and scale. The framework isdeveloped as a set of sufficient conditions for the existence of suchfactorizations, formulated in analogy to those underlying a standardmultiresolution analysis for wavelets, and hence can be viewed as amultiresolution analysis for likelihoods. We then consider the use of thesefactorizations in the task of nonparametric, complexity penalized likelihoodestimation. We study the risk properties of certain thresholding andpartitioning estimators, and demonstrate their adaptivity and near-optimality,in a minimax sense over a broad range of function spaces, based on squaredHellinger distance as a loss function. In particular, our results provide anillustration of how properties of classical wavelet-based estimators can beobtained in a single, unified framework that includes models for continuous,count and categorical data types.

Domain wall mobility in nanowires: transverse versus vortex walls

  The motion of domain walls in ferromagnetic, cylindrical nanowires isinvestigated numerically by solving the Landau-Lifshitz-Gilbert equation for aclassical spin model in which energy contributions from exchange, crystallineanisotropy, dipole-dipole interaction, and a driving magnetic field areconsidered. Depending on the diameter, either transverse domain walls or vortexwalls are found. The transverse domain wall is observed for diameters smallerthan the exchange length of the given material. Here, the system behaveseffectively one-dimensional and the domain wall mobility agrees with a resultderived for a one-dimensional wall by Slonczewski. For low damping the domainwall mobility decreases with decreasing damping constant. With increasingdiameter, a crossover to a vortex wall sets in which enhances the domain wallmobility drastically. For a vortex wall the domain wall mobility is describedby the Walker-formula, with a domain wall width depending on the diameter ofthe wire. The main difference is the dependence on damping: for a vortex wallthe domain wall mobility can be drastically increased for small values of thedamping constant up to a factor of $1/\alpha^2$.

Convex Approaches to Model Wavelet Sparsity Patterns

  Statistical dependencies among wavelet coefficients are commonly representedby graphical models such as hidden Markov trees(HMTs). However, in linearinverse problems such as deconvolution, tomography, and compressed sensing, thepresence of a sensing or observation matrix produces a linear mixing of thesimple Markovian dependency structure. This leads to reconstruction problemsthat are non-convex optimizations. Past work has dealt with this issue byresorting to greedy or suboptimal iterative reconstruction methods. In thispaper, we propose new modeling approaches based on group-sparsity penaltiesthat leads to convex optimizations that can be solved exactly and efficiently.We show that the methods we develop perform significantly better indeconvolution and compressed sensing applications, while being ascomputationally efficient as standard coefficient-wise approaches such aslasso.

Query Complexity of Derivative-Free Optimization

  This paper provides lower bounds on the convergence rate of Derivative FreeOptimization (DFO) with noisy function evaluations, exposing a fundamental andunavoidable gap between the performance of algorithms with access to gradientsand those with access to only function evaluations. However, there aresituations in which DFO is unavoidable, and for such situations we propose anew DFO algorithm that is proved to be near optimal for the class of stronglyconvex objective functions. A distinctive feature of the algorithm is that ituses only Boolean-valued function comparisons, rather than functionevaluations. This makes the algorithm useful in an even wider range ofapplications, such as optimization based on paired comparisons from humansubjects, for example. We also show that regardless of whether DFO is based onnoisy function evaluations or Boolean-valued function comparisons, theconvergence rate is the same.

Active Learning for Undirected Graphical Model Selection

  This paper studies graphical model selection, i.e., the problem of estimatinga graph of statistical relationships among a collection of random variables.Conventional graphical model selection algorithms are passive, i.e., theyrequire all the measurements to have been collected before processing begins.We propose an active learning algorithm that uses junction tree representationsto adapt future measurements based on the information gathered from priormeasurements. We prove that, under certain conditions, our active learningalgorithm requires fewer scalar measurements than any passive algorithm toreliably estimate a graph. A range of numerical results validate our theory anddemonstrates the benefits of active learning.

Algebraic Variety Models for High-Rank Matrix Completion

  We consider a generalization of low-rank matrix completion to the case wherethe data belongs to an algebraic variety, i.e. each data point is a solution toa system of polynomial equations. In this case the original matrix is possiblyhigh-rank, but it becomes low-rank after mapping each column to a higherdimensional space of monomial features. Many well-studied extensions of linearmodels, including affine subspaces and their union, can be described by avariety model. In addition, varieties can be used to model a richer class ofnonlinear quadratic and higher degree curves and surfaces. We study thesampling requirements for matrix completion under a variety model with a focuson a union of affine subspaces. We also propose an efficient matrix completionalgorithm that minimizes a convex or non-convex surrogate of the rank of thematrix of monomial features. Our algorithm uses the well-known "kernel trick"to avoid working directly with the high-dimensional monomial matrix. We showthe proposed algorithm is able to recover synthetically generated data up tothe predicted sampling complexity bounds. The proposed algorithm alsooutperforms standard low rank matrix completion and subspace clusteringtechniques in experiments with real data.

Sequential Testing for Sparse Recovery

  This paper studies sequential methods for recovery of sparse signals in highdimensions. When compared to fixed sample size procedures, in the sparsesetting, sequential methods can result in a large reduction in the number ofsamples needed for reliable signal support recovery. Starting with a lowerbound, we show any coordinate-wise sequential sampling procedure fails in thehigh dimensional limit provided the average number of measurements perdimension is less then log s/D(P_0||P_1) where s is the level of sparsity andD(P_0||P_1) the Kullback-Leibler divergence between the underlyingdistributions. A series of Sequential Probability Ratio Tests (SPRT) whichrequire complete knowledge of the underlying distributions is shown to achievethis bound. Motivated by real world experiments and recent work in adaptivesensing, we introduce a simple procedure termed Sequential Thresholding whichcan be implemented when the underlying testing problem satisfies a monotonelikelihood ratio assumption. Sequential Thresholding guarantees exact supportrecovery provided the average number of measurements per dimension grows fasterthan log s/ D(P_0||P_1), achieving the lower bound. For comparison, we show anynon-sequential procedure fails provided the number of measurements grows at arate less than log n/D(P_1||P_0), where n is the total dimension of theproblem.

Active Ranking using Pairwise Comparisons

  This paper examines the problem of ranking a collection of objects usingpairwise comparisons (rankings of two objects). In general, the ranking of $n$objects can be identified by standard sorting methods using $n log_2 n$pairwise comparisons. We are interested in natural situations in whichrelationships among the objects may allow for ranking using far fewer pairwisecomparisons. Specifically, we assume that the objects can be embedded into a$d$-dimensional Euclidean space and that the rankings reflect their relativedistances from a common reference point in $R^d$. We show that under thisassumption the number of possible rankings grows like $n^{2d}$ and demonstratean algorithm that can identify a randomly selected ranking using just slightlymore than $d log n$ adaptively selected pairwise comparisons, on average. Ifinstead the comparisons are chosen at random, then almost all pairwisecomparisons must be made in order to identify any ranking. In addition, wepropose a robust, error-tolerant algorithm that only requires that the pairwisecomparisons are probably correct. Experimental studies with synthetic and realdatasets support the conclusions of our theoretical analysis.

A Characterization of Deterministic Sampling Patterns for Low-Rank  Matrix Completion

  Low-rank matrix completion (LRMC) problems arise in a wide variety ofapplications. Previous theory mainly provides conditions for completion undermissing-at-random samplings. This paper studies deterministic conditions forcompletion. An incomplete $d \times N$ matrix is finitely rank-$r$ completableif there are at most finitely many rank-$r$ matrices that agree with all itsobserved entries. Finite completability is the tipping point in LRMC, as a fewadditional samples of a finitely completable matrix guarantee its uniquecompletability. The main contribution of this paper is a deterministic samplingcondition for finite completability. We use this to also derive deterministicsampling conditions for unique completability that can be efficiently verified.We also show that under uniform random sampling schemes, these conditions aresatisfied with high probability if $O(\max\{r,\log d\})$ entries per column areobserved. These findings have several implications on LRMC regarding lowerbounds, sample and computational complexity, the role of coherence, adaptivesettings and the validation of any completion algorithm. We complement ourtheoretical results with experiments that support our findings and motivatefuture analysis of uncharted sampling regimes.

Learning the Interference Graph of a Wireless Network

  A key challenge in wireless networking is the management of interferencebetween transmissions. Identifying which transmitters interfere with each otheris a crucial first step. In this paper we cast the task of estimating the awireless interference environment as a graph learning problem. Nodes representtransmitters and edges represent the presence of interference between pairs oftransmitters. We passively observe network traffic transmission patterns andcollect information on transmission successes and failures. We establish boundson the number of observations (each a snapshot of a network traffic pattern)required to identify the interference graph reliably with high probability.  Our main results are scaling laws that tell us how the number of observationsmust grow in terms of the total number of nodes $n$ in the network and themaximum number of interfering transmitters $d$ per node (maximum node degree).The effects of hidden terminal interference (i.e., interference not detectablevia carrier sensing) on the observation requirements are also quantified. Weshow that to identify the graph it is necessary and sufficient that theobservation period grows like $d^2 \log n$, and we propose a practicalalgorithm that reliably identifies the graph from this length of observation.The observation requirements scale quite mildly with network size, and networkswith sparse interference (small $d$) can be identified more rapidly.Computational experiments based on a realistic simulations of the traffic andprotocol lend additional support to these conclusions.

Adaptive Hausdorff estimation of density level sets

  Consider the problem of estimating the $\gamma$-level set$G^*_{\gamma}=\{x:f(x)\geq\gamma\}$ of an unknown $d$-dimensional densityfunction $f$ based on $n$ independent observations $X_1,...,X_n$ from thedensity. This problem has been addressed under global error criteria related tothe symmetric set difference. However, in certain applications a spatiallyuniform mode of convergence is desirable to ensure that the estimated set isclose to the target set everywhere. The Hausdorff error criterion provides thisdegree of uniformity and, hence, is more appropriate in such situations. It isknown that the minimax optimal rate of error convergence for the Hausdorffmetric is $(n/\log n)^{-1/(d+2\alpha)}$ for level sets with boundaries thathave a Lipschitz functional form, where the parameter $\alpha$ characterizesthe regularity of the density around the level of interest. However, theestimators proposed in previous work are nonadaptive to the density regularityand require knowledge of the parameter $\alpha$. Furthermore, previouslydeveloped estimators achieve the minimax optimal rate for rather restrictedclasses of sets (e.g., the boundary fragment and star-shaped sets) thateffectively reduce the set estimation problem to a function estimation problem.This characterization precludes level sets with multiple connected components,which are fundamental to many applications. This paper presents a fullydata-driven procedure that is adaptive to unknown regularity conditions andachieves near minimax optimal Hausdorff error control for a class of densitylevel sets with very general shapes and multiple connected components.

The Sample Complexity of Search over Multiple Populations

  This paper studies the sample complexity of searching over multiplepopulations. We consider a large number of populations, each corresponding toeither distribution P0 or P1. The goal of the search problem studied here is tofind one population corresponding to distribution P1 with as few samples aspossible. The main contribution is to quantify the number of samples needed tocorrectly find one such population. We consider two general approaches:non-adaptive sampling methods, which sample each population a predeterminednumber of times until a population following P1 is found, and adaptive samplingmethods, which employ sequential sampling schemes for each population. We firstderive a lower bound on the number of samples required by any sampling scheme.We then consider an adaptive procedure consisting of a series of sequentialprobability ratio tests, and show it comes within a constant factor of thelower bound. We give explicit expressions for this constant when samples of thepopulations follow Gaussian and Bernoulli distributions. An alternativeadaptive scheme is discussed which does not require full knowledge of P1, andcomes within a constant factor of the optimal scheme. For comparison, a lowerbound on the sampling requirements of any non-adaptive scheme is presented.

Near-Optimal Adaptive Compressed Sensing

  This paper proposes a simple adaptive sensing and group testing algorithm forsparse signal recovery. The algorithm, termed Compressive Adaptive Sense andSearch (CASS), is shown to be near-optimal in that it succeeds at the lowestpossible signal-to-noise-ratio (SNR) levels, improving on previous work inadaptive compressed sensing. Like traditional compressed sensing based onrandom non-adaptive design matrices, the CASS algorithm requires only k log nmeasurements to recover a k-sparse signal of dimension n. However, CASSsucceeds at SNR levels that are a factor log n less than required by standardcompressed sensing. From the point of view of constructing and implementing thesensing operation as well as computing the reconstruction, the proposedalgorithm is substantially less computationally intensive than standardcompressed sensing. CASS is also demonstrated to perform considerably better inpractice through simulation. To the best of our knowledge, this is the firstdemonstration of an adaptive compressed sensing algorithm with near-optimaltheoretical guarantees and excellent practical performance. This paper alsoshows that methods like compressed sensing, group testing, and pooling have anadvantage beyond simply reducing the number of measurements or tests --adaptive versions of such methods can also improve detection and estimationperformance when compared to non-adaptive direct (uncompressed) sensing.

Tensor Methods for Nonlinear Matrix Completion

  In the low rank matrix completion (LRMC) problem, the low rank assumptionmeans that the columns (or rows) of the matrix to be completed are points on alow-dimensional linear algebraic variety. This paper extends this thinking tocases where the columns are points on a low-dimensional nonlinear algebraicvariety, a problem we call Low Algebraic Dimension Matrix Completion (LADMC).Matrices whose columns belong to a union of subspaces (UoS) are an importantspecial case. We propose a LADMC algorithm that leverages existing LRMC methodson a tensorized representation of the data. For example, a second-ordertensorization representation is formed by taking the outer product of eachcolumn with itself, and we consider higher order tensorizations as well. Thisapproach will succeed in many cases where traditional LRMC is guaranteed tofail because the data are low-rank in the tensorized representation but not inthe original representation. We also provide a formal mathematicaljustification for the success of our method. In particular, we show bounds ofthe rank of these data in the tensorized representation, and we prove samplingrequirements to guarantee uniqueness of the solution. Interestingly, thesampling requirements of our LADMC algorithm nearly match the informationtheoretic lower bounds for matrix completion under a UoS model. We also provideexperimental results showing that the new approach significantly outperformsexisting state-of-the-art methods for matrix completion in many situations.

Doppler Monitoring of five K2 Transiting Planetary Systems

  In an effort to measure the masses of planets discovered by the NASA {\it K2}mission, we have conducted precise Doppler observations of five stars withtransiting planets. We present the results of a joint analysis of these newdata and previously published Doppler data. The first star, an M dwarf known asK2-3 or EPIC~201367065, has three transiting planets ("b", with radius$2.1~R_{\oplus}$; "c", $1.7~R_{\oplus}$; and "d", $1.5~R_{\oplus}$). Ouranalysis leads to the mass constraints: $M_{b}=8.1^{+2.0}_{-1.9}~M_{\oplus}$and $M_{c}$ < $ 4.2~M_{\oplus}$~(95\%~conf.). The mass of planet d is poorlyconstrained because its orbital period is close to the stellar rotation period,making it difficult to disentangle the planetary signal from spurious Dopplershifts due to stellar activity. The second star, a G dwarf known as K2-19 orEPIC~201505350, has two planets ("b", $7.7~R_{\oplus}$; and "c",$4.9~R_{\oplus}$) in a 3:2 mean-motion resonance, as well as a shorter-periodplanet ("d", $1.1~R_{\oplus}$). We find $M_{b}$= $28.5^{+5.4}_{-5.0}~M_{\oplus}$, $M_{c}$= $25.6^{+7.1}_{-7.1} ~M_{\oplus}$ and $M_{d}$ <$14.0~M_{\oplus} $~(95\%~conf.). The third star, a G dwarf known as K2-24 orEPIC~203771098, hosts two transiting planets ("b", $5.7~R_{\oplus}$; and "c",$7.8~R_{\oplus}$) with orbital periods in a nearly 2:1 ratio. We find $M_{b}$=$19.8^{+4.5}_{-4.4} ~M_{\oplus}$ and $M_{c}$ =$26.0^{+5.8}_{-6.1}~M_{\oplus}$.....

Scalable Generalized Linear Bandits: Online Computation and Hashing

  Generalized Linear Bandits (GLBs), a natural extension of the stochasticlinear bandits, has been popular and successful in recent years. However,existing GLBs scale poorly with the number of rounds and the number of arms,limiting their utility in practice. This paper proposes new, scalable solutionsto the GLB problem in two respects. First, unlike existing GLBs, whoseper-time-step space and time complexity grow at least linearly with time $t$,we propose a new algorithm that performs online computations to enjoy aconstant space and time complexity. At its heart is a novel Generalized Linearextension of the Online-to-confidence-set Conversion (GLOC method) that takes\emph{any} online learning algorithm and turns it into a GLB algorithm. As aspecial case, we apply GLOC to the online Newton step algorithm, which resultsin a low-regret GLB algorithm with much lower time and memory complexity thanprior work. Second, for the case where the number $N$ of arms is very large, wepropose new algorithms in which each next arm is selected via an inner productsearch. Such methods can be implemented via hashing algorithms (i.e.,"hash-amenable") and result in a time complexity sublinear in $N$. While aThompson sampling extension of GLOC is hash-amenable, its regret bound for$d$-dimensional arm sets scales with $d^{3/2}$, whereas GLOC's regret boundscales with $d$. Towards closing this gap, we propose a new hash-amenablealgorithm whose regret bound scales with $d^{5/4}$. Finally, we propose a fastapproximate hash-key computation (inner product) with a better accuracy thanthe state-of-the-art, which can be of independent interest. We conclude thepaper with preliminary experimental results confirming the merits of ourmethods.

Sketching Sparse Matrices

  This paper considers the problem of recovering an unknown sparse p\times pmatrix X from an m\times m matrix Y=AXB^T, where A and B are known m \times pmatrices with m << p.  The main result shows that there exist constructions of the "sketching"matrices A and B so that even if X has O(p) non-zeros, it can be recoveredexactly and efficiently using a convex program as long as these non-zeros arenot concentrated in any single row/column of X. Furthermore, it suffices forthe size of Y (the sketch dimension) to scale as m = O(\sqrt{# nonzeros in X}\times log p). The results also show that the recovery is robust and stable inthe sense that if X is equal to a sparse matrix plus a perturbation, then theconvex program we propose produces an approximation with accuracy proportionalto the size of the perturbation. Unlike traditional results on sparse recovery,where the sensing matrix produces independent measurements, our sensingoperator is highly constrained (it assumes a tensor product structure).Therefore, proving recovery guarantees require non-standard techniques. Indeedour approach relies on a novel result concerning tensor products of bipartitegraphs, which may be of independent interest.  This problem is motivated by the following application, among others.Consider a p\times n data matrix D, consisting of n observations of pvariables. Assume that the correlation matrix X:=DD^{T} is (approximately)sparse in the sense that each of the p variables is significantly correlatedwith only a few others. Our results show that these significant correlationscan be detected even if we have access to only a sketch of the data S=AD with A\in R^{m\times p}.

The K2-ESPRINT Project V: a short-period giant planet orbiting a  subgiant star

  We report on the discovery and characterization of the transiting planetK2-39b (EPIC 206247743b). With an orbital period of 4.6 days, it is theshortest-period planet orbiting a subgiant star known to date. Such planets arerare, with only a handful of known cases. The reason for this is poorlyunderstood, but may reflect differences in planet occurrence around therelatively high-mass stars that have been surveyed, or may be the result oftidal destruction of such planets. K2-39 is an evolved star with aspectroscopically derived stellar radius and mass of$3.88^{+0.48}_{-0.42}~\mathrm{R_\odot}$ and$1.53^{+0.13}_{-0.12}~\mathrm{M_\odot}$, respectively, and a very close-intransiting planet, with $a/R_\star = 3.4$. Radial velocity (RV) follow-up usingthe HARPS, FIES and PFS instruments leads to a planetary mass of$50.3^{+9.7}_{-9.4}~\mathrm{M_\oplus}$. In combination with a radiusmeasurement of $8.3 \pm 1.1~\mathrm{R_\oplus}$, this results in a meanplanetary density of $0.50^{+0.29}_{-0.17}$ g~cm$^{-3}$. We furthermorediscover a long-term RV trend, which may be caused by a long-period planet orstellar companion. Because K2-39b has a short orbital period, its existencemakes it seem unlikely that tidal destruction is wholly responsible for thedifferences in planet populations around subgiant and main-sequence stars.Future monitoring of the transits of this system may enable the detection ofperiod decay and constrain the tidal dissipation rates of subgiant stars.

Measurement of $ν_μ$-induced charged-current neutral pion production  cross sections on mineral oil at $E_ν\in0.5-2.0$ GeV

  Using a custom 3 \v{C}erenkov-ring fitter, we report cross sections for$\nu_\mu$-induced charged-current single $\pi^0$ production on mineral oil(\chtwo) from a sample of 5810 candidate events with 57% signal purity over anenergy range of $0.5-2.0$GeV. This includes measurements of the absolute totalcross section as a function of neutrino energy, and flux-averaged differentialcross sections measured in terms of $Q^2$, $\mu^-$ kinematics, and $\pi^0$kinematics. The sample yields a flux-averaged total cross section of$(9.2\pm0.3_{stat.}\pm1.5_{syst.})\times10^{-39}$cm$^2$/CH$_2$ at mean neutrinoenergy of 0.965GeV.

The discovery and mass measurement of a new ultra-short-period planet:  EPIC~228732031b

  We report the discovery of a new ultra-short-period planet and summarize theproperties of all such planets for which the mass and radius have beenmeasured. The new planet, EPIC~228732031b, was discovered in {\it K2} Campaign10. It has a radius of 1.81$^{+0.16}_{-0.12}~R_{\oplus}$ and orbits a G dwarfwith a period of 8.9 hours. Radial velocities obtained with Magellan/PFS andTNG/HARPS-N show evidence for stellar activity along with orbital motion. Wedetermined the planetary mass using two different methods: (1) the "floatingchunk offset" method, based only on changes in velocity observed on the samenight, and (2) a Gaussian process regression based on both the radial-velocityand photometric time series. The results are consistent and lead to a massmeasurement of $6.5 \pm 1.6~M_{\oplus}$, and a mean density of$6.0^{+3.0}_{-2.7}$~g~cm$^{-3}$.

First Muon-Neutrino Disappearance Study with an Off-Axis Beam

  We report a measurement of muon-neutrino disappearance in the T2K experiment.The 295-km muon-neutrino beam from Tokai to Kamioka is the first implementationof the off-axis technique in a long-baseline neutrino oscillation experiment.With data corresponding to 1.43 10**20 protons on target, we observe 31fully-contained single muon-like ring events in Super-Kamiokande, compared withan expectation of 104 +- 14 (syst) events without neutrino oscillations. Thebest-fit point for two-flavor nu_mu -> nu_tau oscillations is sin**2(2theta_23) = 0.98 and |\Delta m**2_32| = 2.65 10**-3 eV**2. The boundary of the90 % confidence region includes the points (sin**2(2 theta_23),|\Deltam**2_32|) = (1.0, 3.1 10**-3 eV**2), (0.84, 2.65 10**-3 eV**2) and (1.0, 2.210**-3 eV**2).

Measurement of the neutrino-oxygen neutral-current interaction cross  section by observing nuclear deexcitation $γ$ rays

  We report the first measurement of the neutrino-oxygen neutral-currentquasielastic (NCQE) cross section. It is obtained by observing nucleardeexcitation $\gamma$-rays which follow neutrino-oxygen interactions at theSuper-Kamiokande water Cherenkov detector. We use T2K data corresponding to$3.01 \times 10^{20}$ protons on target. By selecting only events during theT2K beam window and with well-reconstructed vertices in the fiducial volume,the large background rate from natural radioactivity is dramatically reduced.We observe 43 events in the $4-30$ MeV reconstructed energy window, comparedwith an expectation of 51.0, which includes an estimated 16.2 backgroundevents. The background is primarily nonquasielastic neutral-currentinteractions and has only 1.2 events from natural radioactivity. Theflux-averaged NCQE cross section we measure is $1.55 \times 10^{-38}$ cm$^2$with a 68\% confidence interval of $(1.22, 2.20) \times 10^{-38}$ cm$^2$ at amedian neutrino energy of 630 MeV, compared with the theoretical prediction of$2.01 \times 10^{-38}$ cm$^2$.

Measurements of the T2K neutrino beam properties using the INGRID  on-axis near detector

  Precise measurement of neutrino beam direction and intensity was achievedbased on a new concept with modularized neutrino detectors. INGRID (InteractiveNeutrino GRID) is an on-axis near detector for the T2K long baseline neutrinooscillation experiment. INGRID consists of 16 identical modules arranged inhorizontal and vertical arrays around the beam center. The module has asandwich structure of iron target plates and scintillator trackers. INGRIDdirectly monitors the muon neutrino beam profile center and intensity using thenumber of observed neutrino events in each module. The neutrino beam directionis measured with accuracy better than 0.4 mrad from the measured profilecenter. The normalized event rate is measured with 4% precision.

The Long-Baseline Neutrino Experiment: Exploring Fundamental Symmetries  of the Universe

  The preponderance of matter over antimatter in the early Universe, thedynamics of the supernova bursts that produced the heavy elements necessary forlife and whether protons eventually decay --- these mysteries at the forefrontof particle physics and astrophysics are key to understanding the earlyevolution of our Universe, its current state and its eventual fate. TheLong-Baseline Neutrino Experiment (LBNE) represents an extensively developedplan for a world-class experiment dedicated to addressing these questions. LBNEis conceived around three central components: (1) a new, high-intensityneutrino source generated from a megawatt-class proton accelerator at FermiNational Accelerator Laboratory, (2) a near neutrino detector just downstreamof the source, and (3) a massive liquid argon time-projection chamber deployedas a far detector deep underground at the Sanford Underground ResearchFacility. This facility, located at the site of the former Homestake Mine inLead, South Dakota, is approximately 1,300 km from the neutrino source atFermilab -- a distance (baseline) that delivers optimal sensitivity to neutrinocharge-parity symmetry violation and mass ordering effects. This ambitious yetcost-effective design incorporates scalability and flexibility and canaccommodate a variety of upgrades and contributions. With its exceptionalcombination of experimental configuration, technical capabilities, andpotential for transformative discoveries, LBNE promises to be a vital facilityfor the field of particle physics worldwide, providing physicists from aroundthe globe with opportunities to collaborate in a twenty to thirty year programof exciting science. In this document we provide a comprehensive overview ofLBNE's scientific objectives, its place in the landscape of neutrino physicsworldwide, the technologies it will incorporate and the capabilities it willpossess.

Indication of Electron Neutrino Appearance from an Accelerator-produced  Off-axis Muon Neutrino Beam

  The T2K experiment observes indications of $\nu_\mu\rightarrow \nu_e$appearance in data accumulated with $1.43\times10^{20}$ protons on target. Sixevents pass all selection criteria at the far detector. In a three-flavorneutrino oscillation scenario with $|\Delta m_{23}^2|=2.4\times10^{-3}$ eV$^2$,$\sin^2 2\theta_{23}=1$ and $\sin^2 2\theta_{13}=0$, the expected number ofsuch events is 1.5$\pm$0.3(syst.). Under this hypothesis, the probability toobserve six or more candidate events is 7$\times10^{-3}$, equivalent to2.5$\sigma$ significance. At 90% C.L., the data are consistent with0.03(0.04)$<\sin^2 2\theta_{13}<$ 0.28(0.34) for $\delta_{\rm CP}=0$ and anormal (inverted) hierarchy.

The Compact Linear $e^+e^-$ Collider (CLIC) - 2018 Summary Report

  The Compact Linear Collider (CLIC) is a TeV-scale high-luminosity linear$e^+e^-$ collider under development at CERN. Following the CLIC conceptualdesign published in 2012, this report provides an overview of the CLIC project,its current status, and future developments. It presents the CLIC physicspotential and reports on design, technology, and implementation aspects of theaccelerator and the detector. CLIC is foreseen to be built and operated instages, at centre-of-mass energies of 380 GeV, 1.5 TeV and 3 TeV, respectively.CLIC uses a two-beam acceleration scheme, in which 12 GHz acceleratingstructures are powered via a high-current drive beam. For the first stage, analternative with X-band klystron powering is also considered. CLIC acceleratoroptimisation, technical developments and system tests have resulted in anincreased energy efficiency (power around 170 MW) for the 380 GeV stage,together with a reduced cost estimate at the level of 6 billion CHF. Thedetector concept has been refined using improved software tools. Significantprogress has been made on detector technology developments for the tracking andcalorimetry systems. A wide range of CLIC physics studies has been conducted,both through full detector simulations and parametric studies, togetherproviding a broad overview of the CLIC physics potential. Each of the threeenergy stages adds cornerstones of the full CLIC physics programme, such asHiggs width and couplings, top-quark properties, Higgs self-coupling, directsearches, and many precision electroweak measurements. The interpretation ofthe combined results gives crucial and accurate insight into new physics,largely complementary to LHC and HL-LHC. The construction of the first CLICenergy stage could start by 2026. First beams would be available by 2035,marking the beginning of a broad CLIC physics programme spanning 25-30 years.

The T2K Experiment

  The T2K experiment is a long-baseline neutrino oscillation experiment. Itsmain goal is to measure the last unknown lepton sector mixing angle{\theta}_{13} by observing {\nu}_e appearance in a {\nu}_{\mu} beam. It alsoaims to make a precision measurement of the known oscillation parameters,{\Delta}m^{2}_{23} and sin^{2} 2{\theta}_{23}, via {\nu}_{\mu} disappearancestudies. Other goals of the experiment include various neutrino cross sectionmeasurements and sterile neutrino searches. The experiment uses an intenseproton beam generated by the J-PARC accelerator in Tokai, Japan, and iscomposed of a neutrino beamline, a near detector complex (ND280), and a fardetector (Super-Kamiokande) located 295 km away from J-PARC. This paperprovides a comprehensive review of the instrumentation aspect of the T2Kexperiment and a summary of the vital information for each subsystem.

