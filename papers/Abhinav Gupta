Sentiment Classification using Images and Label Embeddings

  In this project we analysed how much semantic information images carry, and
how much value image data can add to sentiment analysis of the text associated
with the images. To better understand the contribution from images, we compared
models which only made use of image data, models which only made use of text
data, and models which combined both data types. We also analysed if this
approach could help sentiment classifiers generalize to unknown sentiments.


Mid-level Elements for Object Detection

  Building on the success of recent discriminative mid-level elements, we
propose a surprisingly simple approach for object detection which performs
comparable to the current state-of-the-art approaches on PASCAL VOC comp-3
detection challenge (no external data). Through extensive experiments and
ablation analysis, we show how our approach effectively improves upon the
HOG-based pipelines by adding an intermediate mid-level representation for the
task of object detection. This representation is easily interpretable and
allows us to visualize what our object detector "sees". We also discuss the
insights our approach shares with CNN-based methods, such as sharing
representation between categories helps.


Autonomous Ingress of a UAV through a window using Monocular Vision

  The use of autonomous UAVs for surveillance purposes and other reconnaissance
tasks is increasingly becoming popular and convenient.These tasks requires the
ability to successfully ingress through the rectangular openings or windows of
the target structure.In this paper, a method to robustly detect the window in
the surrounding using basic image processing techniques and efficient distance
measure, is proposed.Furthermore, a navigation scheme which incorporates this
detection method for performing navigation task has also been proposed.The
whole navigation task is performed and tested in the simulation environment
GAZEBO.


Parametric Synthesis of Text on Stylized Backgrounds using PGGANs

  We describe a novel method of generating high-resolution real-world images of
text where the style and textual content of the images are described
parametrically. Our method combines text to image retrieval techniques with
progressive growing of Generative Adversarial Networks (PGGANs) to achieve
conditional generation of photo-realistic images that reflect specific styles,
as well as artifacts seen in real-world images. We demonstrate our method in
the context of automotive license plates. We assess the impact of varying the
number of training images of each style on the fidelity of the generated style,
and demonstrate the quality of the generated images using license plate
recognition systems.


Training Region-based Object Detectors with Online Hard Example Mining

  The field of object detection has made significant advances riding on the
wave of region-based ConvNets, but their training procedure still includes many
heuristics and hyperparameters that are costly to tune. We present a simple yet
surprisingly effective online hard example mining (OHEM) algorithm for training
region-based ConvNet detectors. Our motivation is the same as it has always
been -- detection datasets contain an overwhelming number of easy examples and
a small number of hard examples. Automatic selection of these hard examples can
make training more effective and efficient. OHEM is a simple and intuitive
algorithm that eliminates several heuristics and hyperparameters in common use.
But more importantly, it yields consistent and significant boosts in detection
performance on benchmarks like PASCAL VOC 2007 and 2012. Its effectiveness
increases as datasets become larger and more difficult, as demonstrated by the
results on the MS COCO dataset. Moreover, combined with complementary advances
in the field, OHEM leads to state-of-the-art results of 78.9% and 76.3% mAP on
PASCAL VOC 2007 and 2012 respectively.


Contribution of Kaluza-Klein modes to vacuum energy in models with large
  extra dimensions $&$ the Cosmological constant

  In this paper, the generation of topological energy in models with large
extra dimensions is investigated. The origin of this energy is attributed to a
topological deformation of the standard Minkowski vacuum due to
compactification of extra dimensions. This deformation is seen to give rise to
an effective, finite energy density due to massive Kaluza-Klein modes of
gravitation. It's renormalized value is seen to depend on the size of the extra
dimensions instead of the UV cut-off of the theory. It is shown that if this
energy density is to contribute to the observed cosmological constant, there
will be extremely stringent bounds on the number of extra dimensions and their
size.


Beyond Skip Connections: Top-Down Modulation for Object Detection

  In recent years, we have seen tremendous progress in the field of object
detection. Most of the recent improvements have been achieved by targeting
deeper feedforward networks. However, many hard object categories such as
bottle, remote, etc. require representation of fine details and not just
coarse, semantic representations. But most of these fine details are lost in
the early convolutional layers. What we need is a way to incorporate finer
details from lower layers into the detection architecture. Skip connections
have been proposed to combine high-level and low-level features, but we argue
that selecting the right features from low-level requires top-down contextual
information. Inspired by the human visual pathway, in this paper we propose
top-down modulations as a way to incorporate fine details into the detection
framework. Our approach supplements the standard bottom-up, feedforward ConvNet
with a top-down modulation (TDM) network, connected using lateral connections.
These connections are responsible for the modulation of lower layer filters,
and the top-down network handles the selection and integration of contextual
information and low-level features. The proposed TDM architecture provides a
significant boost on the COCO testdev benchmark, achieving 28.6 AP for VGG16,
35.2 AP for ResNet101, and 37.3 for InceptionResNetv2 network, without any
bells and whistles (e.g., multi-scale, iterative box refinement, etc.).


A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection

  How do we learn an object detector that is invariant to occlusions and
deformations? Our current solution is to use a data-driven strategy -- collect
large-scale datasets which have object instances under different conditions.
The hope is that the final classifier can use these examples to learn
invariances. But is it really possible to see all the occlusions in a dataset?
We argue that like categories, occlusions and object deformations also follow a
long-tail. Some occlusions and deformations are so rare that they hardly
happen; yet we want to learn a model invariant to such occurrences. In this
paper, we propose an alternative solution. We propose to learn an adversarial
network that generates examples with occlusions and deformations. The goal of
the adversary is to generate examples that are difficult for the object
detector to classify. In our framework both the original detector and adversary
are learned in a joint manner. Our experimental results indicate a 2.3% mAP
boost on VOC07 and a 2.6% mAP boost on VOC2012 object detection challenge
compared to the Fast-RCNN pipeline. We also release the code for this paper.


Cross-stitch Networks for Multi-task Learning

  Multi-task learning in Convolutional Networks has displayed remarkable
success in the field of recognition. This success can be largely attributed to
learning shared representations from multiple supervisory tasks. However,
existing multi-task approaches rely on enumerating multiple network
architectures specific to the tasks at hand, that do not generalize. In this
paper, we propose a principled approach to learn shared representations in
ConvNets using multi-task learning. Specifically, we propose a new sharing
unit: "cross-stitch" unit. These units combine the activations from multiple
networks and can be trained end-to-end. A network with cross-stitch units can
learn an optimal combination of shared and task-specific representations. Our
proposed method generalizes across multiple tasks and shows dramatically
improved performance over baseline methods for categories with few training
examples.


Stability of traveling, pre-tensioned, heavy cables

  We study the dynamics of an inclined tensioned, heavy cable traveling with a
constant speed in the vertical plane. The cable is modeled as a beam resisting
bending and shear. The governing equation for the transverse in-plane
vibrations of the cable are derived through the Newton-Euler method. The cable
dynamics is also studied in the limit of zero bending stiffness. In all cases,
application of en- ergy balance reveals that the total energy of the system
fluctuates even though the oscillations are small and bounded in time,
indicating that the system is nonconser- vative. A comprehensive stability
analysis is carried out in the parameter space of inclination, traveling speed,
pre-tension, bending rigidity and the slenderness of the cable. Effect of
damping is also considered. We conclude that, while pre-tension, rigidity and
slenderness enhance the stability of the traveling cable, the angle of
inclination affects the stability adversely. These results may act as
guidelines for safer design and operation.


Neural Signatures for Licence Plate Re-identification

  The problem of vehicle licence plate re-identification is generally
considered as a one-shot image retrieval problem. The objective of this task is
to learn a feature representation (called a "signature") for licence plates.
Incoming licence plate images are converted to signatures and matched to a
previously collected template database through a distance measure. Then, the
input image is recognized as the template whose signature is "nearest" to the
input signature. The template database is restricted to contain only a single
signature per unique licence plate for our problem.
  We measure the performance of deep convolutional net-based features adapted
from face recognition on this task. In addition, we also test a hybrid approach
combining the Fisher vector with a neural network-based embedding called "f2nn"
trained with the Triplet loss function. We find that the hybrid approach
performs comparably while providing computational benefits. The signature
generated by the hybrid approach also shows higher generalizability to datasets
more dissimilar to the training corpus.


Multi-task learning for Joint Language Understanding and Dialogue State
  Tracking

  This paper presents a novel approach for multi-task learning of language
understanding (LU) and dialogue state tracking (DST) in task-oriented dialogue
systems. Multi-task training enables the sharing of the neural network layers
responsible for encoding the user utterance for both LU and DST and improves
performance while reducing the number of network parameters. In our proposed
framework, DST operates on a set of candidate values for each slot that has
been mentioned so far. These candidate sets are generated using LU slot
annotations for the current user utterance, dialogue acts corresponding to the
preceding system utterance and the dialogue state estimated for the previous
turn, enabling DST to handle slots with a large or unbounded set of possible
values and deal with slot values not seen during training. Furthermore, to
bridge the gap between training and inference, we investigate the use of
scheduled sampling on LU output for the current user utterance as well as the
DST output for the preceding turn.


Unitarity constraints on the stabilized Randall-Sundrum scenario

  Recently proposed stabilization mechanism of the Randall-Sundrum metric gives
rise to a scalar radion, which couples universally to matter with a weak
interaction ($\simeq 1$ TeV) scale. Demanding that gauge boson scattering as
described by the effective low enerrgy theory be unitary upto a given scale
leads to significant constraints on the mass of such a radion.


An Implementation of Faster RCNN with Study for Region Sampling

  We adapted the join-training scheme of Faster RCNN framework from Caffe to
TensorFlow as a baseline implementation for object detection. Our code is made
publicly available. This report documents the simplifications made to the
original pipeline, with justifications from ablation analysis on both PASCAL
VOC 2007 and COCO 2014. We further investigated the role of non-maximal
suppression (NMS) in selecting regions-of-interest (RoIs) for region
classification, and found that a biased sampling toward small regions helps
performance and can achieve on-par mAP to NMS-based sampling when converged
sufficiently.


PyText: A Seamless Path from NLP research to production

  We introduce PyText - a deep learning based NLP modeling framework built on
PyTorch. PyText addresses the often-conflicting requirements of enabling rapid
experimentation and of serving models at scale. It achieves this by providing
simple and extensible interfaces for model components, and by using PyTorch's
capabilities of exporting models for inference via the optimized Caffe2
execution engine. We report our own experience of migrating experimentation and
production workflows to PyText, which enabled us to iterate faster on novel
modeling ideas and then seamlessly ship them at industrial scale.


Visual Features for Context-Aware Speech Recognition

  Automatic transcriptions of consumer-generated multi-media content such as
"Youtube" videos still exhibit high word error rates. Such data typically
occupies a very broad domain, has been recorded in challenging conditions, with
cheap hardware and a focus on the visual modality, and may have been
post-processed or edited. In this paper, we extend our earlier work on adapting
the acoustic model of a DNN-based speech recognition system to an RNN language
model and show how both can be adapted to the objects and scenes that can be
automatically detected in the video. We are working on a corpus of "how-to"
videos from the web, and the idea is that an object that can be seen ("car"),
or a scene that is being detected ("kitchen") can be used to condition both
models on the "context" of the recording, thereby reducing perplexity and
improving transcription. We achieve good improvements in both cases and compare
and analyze the respective reductions in word error rate. We expect that our
results can be used for any type of speech processing in which "context"
information is available, for example in robotics, man-machine interaction, or
when indexing large audio-visual archives, and should ultimately help to bring
together the "video-to-text" and "speech-to-text" communities.


An Efficient Approach to Encoding Context for Spoken Language
  Understanding

  In task-oriented dialogue systems, spoken language understanding, or SLU,
refers to the task of parsing natural language user utterances into semantic
frames. Making use of context from prior dialogue history holds the key to more
effective SLU. State of the art approaches to SLU use memory networks to encode
context by processing multiple utterances from the dialogue at each turn,
resulting in significant trade-offs between accuracy and computational
efficiency. On the other hand, downstream components like the dialogue state
tracker (DST) already keep track of the dialogue state, which can serve as a
summary of the dialogue history. In this work, we propose an efficient approach
to encoding context from prior utterances for SLU. More specifically, our
architecture includes a separate recurrent neural network (RNN) based encoding
module that accumulates dialogue context to guide the frame parsing sub-tasks
and can be shared between SLU and DST. In our experiments, we demonstrate the
effectiveness of our approach on dialogues from two domains.


Design and Development of Underwater Vehicle: ANAHITA

  Anahita is an autonomous underwater vehicle which is currently being
developed by interdisciplinary team of students at Indian Institute of
Technology(IIT) Kanpur with aim to provide a platform for research in AUV to
undergraduate students. This is the second vehicle which is being designed by
AUV-IITK team to participate in 6th NIOT-SAVe competition organized by the
National Institute of Ocean Technology, Chennai. The Vehicle has been
completely redesigned with the major improvements in modularity and ease of
access of all the components, keeping the design very compact and efficient.
New advancements in the vehicle include, power distribution system and
monitoring system. The sensors include the inertial measurement units (IMU),
hydrophone array, a depth sensor, and two RGB cameras. The current vehicle
features hot swappable battery pods giving a huge advantage over the previous
vehicle, for longer runtime.


Learning Exploration Policies for Navigation

  Numerous past works have tackled the problem of task-driven navigation. But,
how to effectively explore a new environment to enable a variety of down-stream
tasks has received much less attention. In this work, we study how agents can
autonomously explore realistic and complex 3D environments without the context
of task-rewards. We propose a learning-based approach and investigate different
policy architectures, reward functions, and training paradigms. We find that
the use of policies with spatial memory that are bootstrapped with imitation
learning and finally finetuned with coverage rewards derived purely from
on-board sensors can be effective at exploring novel environments. We show that
our learned exploration policies can explore better than classical approaches
based on geometry alone and generic learning-based exploration techniques.
Finally, we also show how such task-agnostic exploration can be used for
down-stream tasks. Code and Videos are available at:
https://sites.google.com/view/exploration-for-nav.


Revisiting Unreasonable Effectiveness of Data in Deep Learning Era

  The success of deep learning in vision can be attributed to: (a) models with
high capacity; (b) increased computational power; and (c) availability of
large-scale labeled data. Since 2012, there have been significant advances in
representation capabilities of the models and computational capabilities of
GPUs. But the size of the biggest dataset has surprisingly remained constant.
What will happen if we increase the dataset size by 10x or 100x? This paper
takes a step towards clearing the clouds of mystery surrounding the
relationship between `enormous data' and visual deep learning. By exploiting
the JFT-300M dataset which has more than 375M noisy labels for 300M images, we
investigate how the performance of current vision tasks would change if this
data was used for representation learning. Our paper delivers some surprising
(and some expected) findings. First, we find that the performance on vision
tasks increases logarithmically based on volume of training data size. Second,
we show that representation learning (or pre-training) still holds a lot of
promise. One can improve performance on many vision tasks by just training a
better base model. Finally, as expected, we present new state-of-the-art
results for different vision tasks including image classification, object
detection, semantic segmentation and human pose estimation. Our sincere hope is
that this inspires vision community to not undervalue the data and develop
collective efforts in building larger datasets.


Beating the Multiplicative Weights Update Algorithm

  Multiplicative weights update algorithms have been used extensively in
designing iterative algorithms for many computational tasks. The core idea is
to maintain a distribution over a set of experts and update this distribution
in an online fashion based on the parameters of the underlying optimization
problem. In this report, we study the behavior of a special MWU algorithm used
for generating a global coin flip in the presence of an adversary that tampers
the experts' advice. Specifically, we focus our attention on two adversarial
strategies: (1) non-adaptive, in which the adversary chooses a fixed set of
experts a priori and corrupts their advice in each round; and (2) adaptive, in
which this set is chosen as the rounds of the algorithm progress. We formulate
these adversarial strategies as being greedy in terms of trying to maximize the
share of the corrupted experts in the final weighted advice the MWU computes
and provide the underlying optimization problem that needs to be solved to
achieve this goal. We provide empirical results to show that in the presence of
either of the above adversaries, the MWU algorithm takes $\mathcal{O}(n)$
rounds in expectation to produce the desired output. This result compares well
with the current state of the art of $\mathcal{O}(n^3)$ for the general
Byzantine consensus problem. Finally, we briefly discuss the extension of these
adversarial strategies for a general MWU algorithm and provide an outline for
the framework in that setting.


Higgs-Axion interplay and anomalous magnetic phase diagram in TlCuCl$_3$

  What is so unique in TlCuCl3 which drives so many unique magnetic features in
this compound? To study these properties, here we employ a combination of
ab-initio band structure, tight-binding model, and an effective quantum field
theory. Within a density-functional theory (DFT) calculation, we find an
unexpected bulk Dirac cone without spin-orbit coupling (SOC). Tracing back to
its origin, we identify, for the first time, the presence of a
Su-Schrieffer-Heeger (SSH) like dimerized Cu chain lying in the 3D crystal
structure. The SSH chain, combined with SOC, stipulates an anisotropic 3D Dirac
cone where chiral and helical states are intertwined. As a Heisenberg
interaction is introduced, we show that the dimerized Cu sublattices of the SSH
chain condensate into spin-singlet, dimerized magnets. In the magnetic ground
state, we also find a topological phase, distinguished by the axion angle.
Finally, to study how the topological axion term couples to magnetic
excitations, we derive a Chern-Simons-Ginzburg-Landau action from the 3D SSH
Hamiltonian. We find that axion term provides an additional mass term to the
Higgs mode, and a lifetime to paramagnons, which are independent of the quantum
critical physics. The axion-Higgs interplay can be probed with electric and
magnetic field applied parallel or anti-parallel to each other.


τPolarization asymmetry in $B \to X_s τ^+ τ^-$ in SUSY models
  with large $tanβ$

  Rare B decays provides an opportunity to probe for new physics beyond the
standard model. the effective Hamiltonian for the decay $b \to s l^+ l^-$
predicts the characteristic polarization for the final state lepton. Lepton
polarization has, in addition to a longitudinal component $P_L$, two orthogonal
components $P_T$ and $P_N$ lying in and perpendicular to the decay plane. In
this article we perform a study of the $\tau$-polarisation asymmetry in the
case of SUSY models with large $\tan\beta$ in the inclusive decay $B \to X_s
\tau^+ \tau^-$.


Longitudinal Polarization in $K_L \to μ^+ μ^-$ in MSSM with large
  $tanβ$

  A complete experiment on decay $K_L \to l^+ l^-$ will not only consist of
measurement of the decay rates but also lepton polarization etc. These
additional observations will yield tests of CP invariance in these decays. In
$K_L$ and $K_S$ decays, the e mode is slower than the $\mu$ mode by roughly
$(m_e/m_\mu)^2$ \cite{sehgal1}. As well discussed in literature \cite{herczeg}
the Standard Model contribution to the lepton polarization is of order $2
\times \sim 10^{-3}$. We show that in MSSM with large \tanbeta and light higgs
masses ($\sim 2 M_W$), the longitudinal lepton polarization in $K_L \to \mu^+
\mu^-$ can be enhanced to a higher value, of about $10^{-2}$.


Neutrinos as Source of Ultra High Energy Cosmic Rays in Extra Dimensions

  If the neutrinos are to be identified with the primary source of ultra-high
energy cosmic rays(UHECR), their interaction on relic neutrinos is of great
importance in understanding their long intergalactic journey. In theories with
large compact dimensions, the exchange of a tower of massive spin-2 gravitons
(Kaluza-Klein excitations) gives extra contribution to $\nu\bar{\nu}
\longrightarrow f\bar{f}$ and $\gamma\gamma$ processes along with the opening
of a new channel for the neutrinos to annihilate with the relic cosmic neutrino
background $\nu\bar{\nu} \longrightarrow G_{kk}$ to produce bulk gravitons in
the extra dimensions. This will affect their attenuation. We compute the
contribution of these Kaluza-Klein excitations to the above processes and find
that for parameters of the theory constrained by supernova cooling, the
contribution does indeed become the dominant contribution above $\sqrt{s}
\simeq 300$ GeV.


Two loop radion correction to $K_L$ - $K_S$ mass difference in the
  stabilised Randall-Sundrum brane world scenario

  In the stabilised Randall-Sundrum brane world scenario, the radion can have
phenomenologically testable effects, which can be measured against precisely
measured electroweak physics data. We investigate the effect of two loop radion
corrections to $K_L$ - $K_S$ mass difference to set a bound on the radion mass
and vacuum expectation value. It is found that the leading two loop corrections
are of the order $[Log(\frac{\Lambda^2}{m_\phi^2}) ]^2$ where $\Lambda$ is the
cut-off scale ${\cal O}(\sim$TeV) and $m_\phi$ is the radion mass.


Neutrino Masses from Non-minimal Gravitational Interactions of Massive
  Neutral Fermions

  A new mechanism is proposed for generating neutrino masses radiatively
through a non-minimal coupling to gravity of fermionic bilinears involving
massive neutral fermions. Such coupling terms can arise in theories where the
gravity sector is augmented by a scalar field. They necessarily violate the
principle of equivalence, but such violations are not ruled out by present
experiments. It is shown that the proposed mechanism is realised most
convincingly in theories of the Randall- Sundrum type, where gravity couples
strongly in the TeV range. The mechanism has the potential for solving both the
solar and atmospheric neutrino problems. The smallness of neutrino masses in
this scenario is due to the fact that the interaction of the massive neutral
fermions arises entirely from higher-dimensional operators in the effective
Lagrangian.


Radiation from a charged particle and radiation reaction -- revisited

  We study the electromagnetic fields of an arbitrarily moving charged particle
and the radiation reaction on the charged particle using a novel approach. We
first show that the fields of an arbitrarily moving charged particle in an
inertial frame can be related in a simple manner to the fields of a uniformly
accelerated charged particle in its rest frame. Since the latter field is
static and easily obtainable, it is possible to derive the fields of an
arbitrarily moving charged particle by a coordinate transformation. More
importantly, this formalism allows us to calculate the self-force on a charged
particle in a remarkably simple manner. We show that the original expression
for this force, obtained by Dirac, can be rederived with much less computation
and in an intuitively simple manner using our formalism.


Unsupervised Discovery of Mid-Level Discriminative Patches

  The goal of this paper is to discover a set of discriminative patches which
can serve as a fully unsupervised mid-level visual representation. The desired
patches need to satisfy two requirements: 1) to be representative, they need to
occur frequently enough in the visual world; 2) to be discriminative, they need
to be different enough from the rest of the visual world. The patches could
correspond to parts, objects, "visual phrases", etc. but are not restricted to
be any one of them. We pose this as an unsupervised discriminative clustering
problem on a huge dataset of image patches. We use an iterative procedure which
alternates between clustering and training discriminative classifiers, while
applying careful cross-validation at each step to prevent overfitting. The
paper experimentally demonstrates the effectiveness of discriminative patches
as an unsupervised mid-level visual representation, suggesting that it could be
used in place of visual words for many tasks. Furthermore, discriminative
patches can also be used in a supervised regime, such as scene classification,
where they demonstrate state-of-the-art performance on the MIT Indoor-67
dataset.


Dense Optical Flow Prediction from a Static Image

  Given a scene, what is going to move, and in what direction will it move?
Such a question could be considered a non-semantic form of action prediction.
In this work, we present a convolutional neural network (CNN) based approach
for motion prediction. Given a static image, this CNN predicts the future
motion of each and every pixel in the image in terms of optical flow. Our CNN
model leverages the data in tens of thousands of realistic videos to train our
model. Our method relies on absolutely no human labeling and is able to predict
motion based on the context of the scene. Because our CNN model makes no
assumptions about the underlying scene, it can predict future optical flow on a
diverse set of scenarios. We outperform all previous approaches by large
margins.


Unsupervised Learning of Visual Representations using Videos

  Is strong supervision necessary for learning a good visual representation? Do
we really need millions of semantically-labeled images to train a Convolutional
Neural Network (CNN)? In this paper, we present a simple yet surprisingly
powerful approach for unsupervised learning of CNN. Specifically, we use
hundreds of thousands of unlabeled videos from the web to learn visual
representations. Our key idea is that visual tracking provides the supervision.
That is, two patches connected by a track should have similar visual
representation in deep feature space since they probably belong to the same
object or object part. We design a Siamese-triplet network with a ranking loss
function to train this CNN representation. Without using a single image from
ImageNet, just using 100K unlabeled videos and the VOC 2012 dataset, we train
an ensemble of unsupervised networks that achieves 52% mAP (no bounding box
regression). This performance comes tantalizingly close to its
ImageNet-supervised counterpart, an ensemble which achieves a mAP of 54.4%. We
also show that our unsupervised network can perform competitively in other
tasks such as surface-normal estimation.


In Defense of the Direct Perception of Affordances

  The field of functional recognition or affordance estimation from images has
seen a revival in recent years. As originally proposed by Gibson, the
affordances of a scene were directly perceived from the ambient light: in other
words, functional properties like sittable were estimated directly from
incoming pixels. Recent work, however, has taken a mediated approach in which
affordances are derived by first estimating semantics or geometry and then
reasoning about the affordances. In a tribute to Gibson, this paper explores
his theory of affordances as originally proposed. We propose two approaches for
direct perception of affordances and show that they obtain good results and can
out-perform mediated approaches. We hope this paper can rekindle discussion
around direct perception and its implications in the long term.


Webly Supervised Learning of Convolutional Networks

  We present an approach to utilize large amounts of web data for learning
CNNs. Specifically inspired by curriculum learning, we present a two-step
approach for CNN training. First, we use easy images to train an initial visual
representation. We then use this initial CNN and adapt it to harder, more
realistic images by leveraging the structure of data and categories. We
demonstrate that our two-stage CNN outperforms a fine-tuned CNN trained on
ImageNet on Pascal VOC 2012. We also demonstrate the strength of webly
supervised learning by localizing objects in web images and training a R-CNN
style detector. It achieves the best performance on VOC 2007 where no VOC
training data is used. Finally, we show our approach is quite robust to noise
and performs comparably even when we use image search results from March 2013
(pre-CNN image search era).


Unsupervised Visual Representation Learning by Context Prediction

  This work explores the use of spatial context as a source of free and
plentiful supervisory signal for training a rich visual representation. Given
only a large, unlabeled image collection, we extract random pairs of patches
from each image and train a convolutional neural net to predict the position of
the second patch relative to the first. We argue that doing well on this task
requires the model to learn to recognize objects and their parts. We
demonstrate that the feature representation learned using this within-image
context indeed captures visual similarity across images. For example, this
representation allows us to perform unsupervised visual discovery of objects
like cats, people, and even birds from the Pascal VOC 2011 detection dataset.
Furthermore, we show that the learned ConvNet can be used in the R-CNN
framework and provides a significant boost over a randomly-initialized ConvNet,
resulting in state-of-the-art performance among algorithms which use only
Pascal-provided training set annotations.


Actions ~ Transformations

  What defines an action like "kicking ball"? We argue that the true meaning of
an action lies in the change or transformation an action brings to the
environment. In this paper, we propose a novel representation for actions by
modeling an action as a transformation which changes the state of the
environment before the action happens (precondition) to the state after the
action (effect). Motivated by recent advancements of video representation using
deep learning, we design a Siamese network which models the action as a
transformation on a high-level feature space. We show that our model gives
improvements on standard action recognition datasets including UCF101 and
HMDB51. More importantly, our approach is able to generalize beyond learned
action categories and shows significant performance improvement on
cross-category generalization on our new ACT dataset.


An Uncertain Future: Forecasting from Static Images using Variational
  Autoencoders

  In a given scene, humans can often easily predict a set of immediate future
events that might happen. However, generalized pixel-level anticipation in
computer vision systems is difficult because machine learning struggles with
the ambiguity inherent in predicting the future. In this paper, we focus on
predicting the dense trajectory of pixels in a scene, specifically what will
move in the scene, where it will travel, and how it will deform over the course
of one second. We propose a conditional variational autoencoder as a solution
to this problem. In this framework, direct inference from the image shapes the
distribution of possible trajectories, while latent variables encode any
necessary information that is not available in the image. We show that our
method is able to successfully predict events in a wide variety of scenes and
can produce multiple different predictions when the future is ambiguous. Our
algorithm is trained on thousands of diverse, realistic videos and requires
absolutely no human labeling. In addition to non-semantic action prediction, we
find that our method learns a representation that is applicable to semantic
vision tasks.


Generative Image Modeling using Style and Structure Adversarial Networks

  Current generative frameworks use end-to-end learning and generate images by
sampling from uniform noise distribution. However, these approaches ignore the
most basic principle of image formation: images are product of: (a) Structure:
the underlying 3D model; (b) Style: the texture mapped onto structure. In this
paper, we factorize the image generation process and propose Style and
Structure Generative Adversarial Network (S^2-GAN). Our S^2-GAN has two
components: the Structure-GAN generates a surface normal map; the Style-GAN
takes the surface normal map as input and generates the 2D image. Apart from a
real vs. generated loss function, we use an additional loss with computed
surface normals from generated images. The two GANs are first trained
independently, and then merged together via joint learning. We show our S^2-GAN
model is interpretable, generates more realistic images and can be used to
learn unsupervised RGBD representations.


A Fast Unified Model for Parsing and Sentence Understanding

  Tree-structured neural networks exploit valuable syntactic parse information
as they interpret the meanings of sentences. However, they suffer from two key
technical problems that make them slow and unwieldy for large-scale NLP tasks:
they usually operate on parsed sentences and they do not directly support
batched computation. We address these issues by introducing the Stack-augmented
Parser-Interpreter Neural Network (SPINN), which combines parsing and
interpretation within a single tree-sequence hybrid model by integrating
tree-structured sentence interpretation into the linear sequential structure of
a shift-reduce parser. Our model supports batched computation for a speedup of
up to 25 times over other tree-structured models, and its integrated parser can
operate on unparsed data with little loss in accuracy. We evaluate it on the
Stanford NLI entailment task and show that it significantly outperforms other
sentence-encoding models.


Learning a Predictable and Generative Vector Representation for Objects

  What is a good vector representation of an object? We believe that it should
be generative in 3D, in the sense that it can produce new 3D objects; as well
as be predictable from 2D, in the sense that it can be perceived from 2D
images. We propose a novel architecture, called the TL-embedding network, to
learn an embedding space with these properties. The network consists of two
components: (a) an autoencoder that ensures the representation is generative;
and (b) a convolutional network that ensures the representation is predictable.
This enables tackling a number of tasks including voxel prediction from 2D
images and 3D model retrieval. Extensive experimental analysis demonstrates the
usefulness and versatility of this embedding.


ActionVLAD: Learning spatio-temporal aggregation for action
  classification

  In this work, we introduce a new video representation for action
classification that aggregates local convolutional features across the entire
spatio-temporal extent of the video. We do so by integrating state-of-the-art
two-stream networks with learnable spatio-temporal feature aggregation. The
resulting architecture is end-to-end trainable for whole-video classification.
We investigate different strategies for pooling across space and time and
combining signals from the different streams. We find that: (i) it is important
to pool jointly across space and time, but (ii) appearance and motion streams
are best aggregated into their own separate representations. Finally, we show
that our representation outperforms the two-stream base architecture by a large
margin (13% relative) as well as out-performs other baselines with comparable
base architectures on HMDB51, UCF101, and Charades video classification
benchmarks.


What's in a Question: Using Visual Questions as a Form of Supervision

  Collecting fully annotated image datasets is challenging and expensive. Many
types of weak supervision have been explored: weak manual annotations, web
search results, temporal continuity, ambient sound and others. We focus on one
particular unexplored mode: visual questions that are asked about images. The
key observation that inspires our work is that the question itself provides
useful information about the image (even without the answer being available).
For instance, the question "what is the breed of the dog?" informs the AI that
the animal in the scene is a dog and that there is only one dog present. We
make three contributions: (1) providing an extensive qualitative and
quantitative analysis of the information contained in human visual questions,
(2) proposing two simple but surprisingly effective modifications to the
standard visual question answering models that allow them to make use of weak
supervision in the form of unanswered questions associated with images and (3)
demonstrating that a simple data augmentation strategy inspired by our insights
results in a 7.1% improvement on the standard VQA benchmark.


Learning to Fly by Crashing

  How do you learn to navigate an Unmanned Aerial Vehicle (UAV) and avoid
obstacles? One approach is to use a small dataset collected by human experts:
however, high capacity learning algorithms tend to overfit when trained with
little data. An alternative is to use simulation. But the gap between
simulation and real world remains large especially for perception problems. The
reason most research avoids using large-scale real data is the fear of crashes!
In this paper, we propose to bite the bullet and collect a dataset of crashes
itself! We build a drone whose sole purpose is to crash into objects: it
samples naive trajectories and crashes into random objects. We crash our drone
11,500 times to create one of the biggest UAV crash dataset. This dataset
captures the different ways in which a UAV can crash. We use all this negative
flying data in conjunction with positive data sampled from the same
trajectories to learn a simple yet powerful policy for UAV navigation. We show
that this simple self-supervised model is quite effective in navigating the UAV
even in extremely cluttered environments with dynamic obstacles including
humans. For supplementary video see: https://youtu.be/u151hJaGKUo


Designing Deep Networks for Surface Normal Estimation

  In the past few years, convolutional neural nets (CNN) have shown incredible
promise for learning visual representations. In this paper, we use CNNs for the
task of predicting surface normals from a single image. But what is the right
architecture we should use? We propose to build upon the decades of hard work
in 3D scene understanding, to design new CNN architecture for the task of
surface normal estimation. We show by incorporating several constraints
(man-made, manhattan world) and meaningful intermediate representations (room
layout, edge labels) in the architecture leads to state of the art performance
on surface normal estimation. We also show that our network is quite robust and
show state of the art results on other datasets as well without any
fine-tuning.


Marr Revisited: 2D-3D Alignment via Surface Normal Prediction

  We introduce an approach that leverages surface normal predictions, along
with appearance cues, to retrieve 3D models for objects depicted in 2D still
images from a large CAD object library. Critical to the success of our approach
is the ability to recover accurate surface normals for objects in the depicted
scene. We introduce a skip-network model built on the pre-trained Oxford VGG
convolutional neural network (CNN) for surface normal prediction. Our model
achieves state-of-the-art accuracy on the NYUv2 RGB-D dataset for surface
normal prediction, and recovers fine object detail compared to previous
methods. Furthermore, we develop a two-stream network over the input image and
predicted surface normals that jointly learns pose and style for CAD model
retrieval. When using the predicted surface normals, our two-stream network
matches prior work using surface normals computed from RGB-D images on the task
of pose prediction, and achieves state of the art when using RGB-D input.
Finally, our two-stream network allows us to retrieve CAD models that better
match the style and pose of a depicted object compared with baseline
approaches.


Pose from Action: Unsupervised Learning of Pose Features based on Motion

  Human actions are comprised of a sequence of poses. This makes videos of
humans a rich and dense source of human poses. We propose an unsupervised
method to learn pose features from videos that exploits a signal which is
complementary to appearance and can be used as supervision: motion. The key
idea is that humans go through poses in a predictable manner while performing
actions. Hence, given two poses, it should be possible to model the motion that
caused the change between them. We represent each of the poses as a feature in
a CNN (Appearance ConvNet) and generate a motion encoding from optical flow
maps using a separate CNN (Motion ConvNet). The data for this task is
automatically generated allowing us to train without human supervision. We
demonstrate the strength of the learned representation by finetuning the
trained model for Pose Estimation on the FLIC dataset, for static image action
recognition on PASCAL and for action recognition in videos on UCF101 and
HMDB51.


PixelNet: Towards a General Pixel-level Architecture

  We explore architectures for general pixel-level prediction problems, from
low-level edge detection to mid-level surface normal estimation to high-level
semantic segmentation. Convolutional predictors, such as the
fully-convolutional network (FCN), have achieved remarkable success by
exploiting the spatial redundancy of neighboring pixels through convolutional
processing. Though computationally efficient, we point out that such approaches
are not statistically efficient during learning precisely because spatial
redundancy limits the information learned from neighboring pixels. We
demonstrate that (1) stratified sampling allows us to add diversity during
batch updates and (2) sampled multi-scale features allow us to explore more
nonlinear predictors (multiple fully-connected layers followed by ReLU) that
improve overall accuracy. Finally, our objective is to show how a architecture
can get performance better than (or comparable to) the architectures designed
for a particular task. Interestingly, our single architecture produces
state-of-the-art results for semantic segmentation on PASCAL-Context, surface
normal estimation on NYUDv2 dataset, and edge detection on BSDS without
contextual post-processing.


Learning to Push by Grasping: Using multiple tasks for effective
  learning

  Recently, end-to-end learning frameworks are gaining prevalence in the field
of robot control. These frameworks input states/images and directly predict the
torques or the action parameters. However, these approaches are often critiqued
due to their huge data requirements for learning a task. The argument of the
difficulty in scalability to multiple tasks is well founded, since training
these tasks often require hundreds or thousands of examples. But do end-to-end
approaches need to learn a unique model for every task? Intuitively, it seems
that sharing across tasks should help since all tasks require some common
understanding of the environment. In this paper, we attempt to take the next
step in data-driven end-to-end learning frameworks: move from the realm of
task-specific models to joint learning of multiple robot tasks. In an
astonishing result we show that models with multi-task learning tend to perform
better than task-specific models trained with same amounts of data. For
example, a deep-network learned with 2.5K grasp and 2.5K push examples performs
better on grasping than a network trained on 5K grasp examples.


Supervision via Competition: Robot Adversaries for Learning Tasks

  There has been a recent paradigm shift in robotics to data-driven learning
for planning and control. Due to large number of experiences required for
training, most of these approaches use a self-supervised paradigm: using
sensors to measure success/failure. However, in most cases, these sensors
provide weak supervision at best. In this work, we propose an adversarial
learning framework that pits an adversary against the robot learning the task.
In an effort to defeat the adversary, the original robot learns to perform the
task with more robustness leading to overall improved performance. We show that
this adversarial framework forces the the robot to learn a better grasping
model in order to overcome the adversary. By grasping 82% of presented novel
objects compared to 68% without an adversary, we demonstrate the utility of
creating adversaries. We also demonstrate via experiments that having robots in
adversarial setting might be a better learning strategy as compared to having
collaborative multiple robots.


The More You Know: Using Knowledge Graphs for Image Classification

  One characteristic that sets humans apart from modern learning-based computer
vision algorithms is the ability to acquire knowledge about the world and use
that knowledge to reason about the visual world. Humans can learn about the
characteristics of objects and the relationships that occur between them to
learn a large variety of visual concepts, often with few examples. This paper
investigates the use of structured prior knowledge in the form of knowledge
graphs and shows that using this knowledge improves performance on image
classification. We build on recent work on end-to-end learning on graphs,
introducing the Graph Search Neural Network as a way of efficiently
incorporating large knowledge graphs into a vision classification pipeline. We
show in a number of experiments that our method outperforms standard neural
network baselines for multi-label classification.


Asynchronous Temporal Fields for Action Recognition

  Actions are more than just movements and trajectories: we cook to eat and we
hold a cup to drink from it. A thorough understanding of videos requires going
beyond appearance modeling and necessitates reasoning about the sequence of
activities, as well as the higher-level constructs such as intentions. But how
do we model and reason about these? We propose a fully-connected temporal CRF
model for reasoning over various aspects of activities that includes objects,
actions, and intentions, where the potentials are predicted by a deep network.
End-to-end training of such structured models is a challenging endeavor: For
inference and learning we need to construct mini-batches consisting of whole
videos, leading to mini-batches with only a few videos. This causes
high-correlation between data points leading to breakdown of the backprop
algorithm. To address this challenge, we present an asynchronous variational
inference method that allows efficient end-to-end training. Our method achieves
a classification mAP of 22.4% on the Charades benchmark, outperforming the
state-of-the-art (17.2% mAP), and offers equal gains on the task of temporal
localization.


