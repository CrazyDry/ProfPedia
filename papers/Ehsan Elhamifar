Block-Sparse Recovery via Convex Optimization

  Given a dictionary that consists of multiple blocks and a signal that livesin the range space of only a few blocks, we study the problem of finding ablock-sparse representation of the signal, i.e., a representation that uses theminimum number of blocks. Motivated by signal/image processing and computervision applications, such as face recognition, we consider the block-sparserecovery problem in the case where the number of atoms in each block isarbitrary, possibly much larger than the dimension of the underlying subspace.To find a block-sparse representation of a signal, we propose two classes ofnon-convex optimization programs, which aim to minimize the number of nonzerocoefficient blocks and the number of nonzero reconstructed vectors from theblocks, respectively. Since both classes of problems are NP-hard, we proposeconvex relaxations and derive conditions under which each class of the convexprograms is equivalent to the original non-convex formulation. Our conditionsdepend on the notions of mutual and cumulative subspace coherence of adictionary, which are natural generalizations of existing notions of mutual andcumulative coherence. We evaluate the performance of the proposed convexprograms through simulations as well as real experiments on face recognition.We show that treating the face recognition problem as a block-sparse recoveryproblem improves the state-of-the-art results by 10% with only 25% of thetraining data.

Dissimilarity-based Sparse Subset Selection

  Finding an informative subset of a large collection of data points or modelsis at the center of many problems in computer vision, recommender systems,bio/health informatics as well as image and natural language processing. Givenpairwise dissimilarities between the elements of a `source set' and a `targetset,' we consider the problem of finding a subset of the source set, calledrepresentatives or exemplars, that can efficiently describe the target set. Weformulate the problem as a row-sparsity regularized trace minimization problem.Since the proposed formulation is, in general, NP-hard, we consider a convexrelaxation. The solution of our optimization finds representatives and theassignment of each element of the target set to each representative, hence,obtaining a clustering. We analyze the solution of our proposed optimization asa function of the regularization parameter. We show that when the two setsjointly partition into multiple groups, our algorithm finds representativesfrom all groups and reveals clustering of the sets. In addition, we show thatthe proposed framework can effectively deal with outliers. Our algorithm workswith arbitrary dissimilarities, which can be asymmetric or violate the triangleinequality. To efficiently implement our algorithm, we consider an AlternatingDirection Method of Multipliers (ADMM) framework, which results in quadraticcomplexity in the problem size. We show that the ADMM implementation allows toparallelize the algorithm, hence further reducing the computational time.Finally, by experiments on real-world datasets, we show that our proposedalgorithm improves the state of the art on the two problems of scenecategorization using representative images and time-series modeling andsegmentation using representative~models.

Approximate Subspace-Sparse Recovery with Corrupted Data via Constrained  $\ell_1$-Minimization

  High-dimensional data often lie in low-dimensional subspaces corresponding todifferent classes they belong to. Finding sparse representations of data pointsin a dictionary built using the collection of data helps to uncoverlow-dimensional subspaces and address problems such as clustering,classification, subset selection and more. In this paper, we address theproblem of recovering sparse representations for noisy data points in adictionary whose columns correspond to corrupted data lying close to a union ofsubspaces. We consider a constrained $\ell_1$-minimization and study conditionsunder which the solution of the proposed optimization satisfies the approximatesubspace-sparse recovery condition. More specifically, we show that each noisydata point, perturbed from a subspace by a noise of the magnitude of$\varepsilon$, will be reconstructed using data points from the same subspacewith a small error of the order of $O(\varepsilon)$ and that the coefficientscorresponding to data points in other subspaces will be sufficiently small,\ie, of the order of $O(\varepsilon)$. We do not impose any randomnessassumption on the arrangement of subspaces or distribution of data points ineach subspace. Our framework is based on a novel generalization of thenull-space property to the setting where data lie in multiple subspaces, thenumber of data points in each subspace exceeds the dimension of the subspace,and all data points are corrupted by noise. Moreover, assuming a randomdistribution for data points, we further show that coefficients from thedesired support not only reconstruct a given point with high accuracy, but alsohave sufficiently large values, \ie, of the order of $O(1)$.

Sparse Subspace Clustering: Algorithm, Theory, and Applications

  In many real-world problems, we are dealing with collections ofhigh-dimensional data, such as images, videos, text and web documents, DNAmicroarray data, and more. Often, high-dimensional data lie close tolow-dimensional structures corresponding to several classes or categories thedata belongs to. In this paper, we propose and study an algorithm, calledSparse Subspace Clustering (SSC), to cluster data points that lie in a union oflow-dimensional subspaces. The key idea is that, among infinitely many possiblerepresentations of a data point in terms of other points, a sparserepresentation corresponds to selecting a few points from the same subspace.This motivates solving a sparse optimization program whose solution is used ina spectral clustering framework to infer the clustering of data into subspaces.Since solving the sparse optimization program is in general NP-hard, weconsider a convex relaxation and show that, under appropriate conditions on thearrangement of subspaces and the distribution of data, the proposedminimization program succeeds in recovering the desired sparse representations.The proposed algorithm can be solved efficiently and can handle data pointsnear the intersections of subspaces. Another key advantage of the proposedalgorithm with respect to the state of the art is that it can deal with datanuisances, such as noise, sparse outlying entries, and missing entries,directly by incorporating the model of the data into the sparse optimizationprogram. We demonstrate the effectiveness of the proposed algorithm throughexperiments on synthetic data as well as the two real-world problems of motionsegmentation and face clustering.

On the Lagrangian Biduality of Sparsity Minimization Problems

  Recent results in Compressive Sensing have shown that, under certainconditions, the solution to an underdetermined system of linear equations withsparsity-based regularization can be accurately recovered by solving convexrelaxations of the original problem. In this work, we present a novelprimal-dual analysis on a class of sparsity minimization problems. We show thatthe Lagrangian bidual (i.e., the Lagrangian dual of the Lagrangian dual) of thesparsity minimization problems can be used to derive interesting convexrelaxations: the bidual of the $\ell_0$-minimization problem is the$\ell_1$-minimization problem; and the bidual of the $\ell_{0,1}$-minimizationproblem for enforcing group sparsity on structured data is the$\ell_{1,\infty}$-minimization problem. The analysis provides a means tocompute per-instance non-trivial lower bounds on the (group) sparsity of thedesired solutions. In a real-world application, the bidual relaxation improvesthe performance of a sparsity-based classification framework applied to robustface recognition.

Robust subspace clustering

  Subspace clustering refers to the task of finding a multi-subspacerepresentation that best fits a collection of points taken from ahigh-dimensional space. This paper introduces an algorithm inspired by sparsesubspace clustering (SSC) [In IEEE Conference on Computer Vision and PatternRecognition, CVPR (2009) 2790-2797] to cluster noisy data, and develops somenovel theory demonstrating its correctness. In particular, the theory usesideas from geometric functional analysis to show that the algorithm canaccurately recover the underlying subspaces under minimal requirements on theirorientation, and on the number of samples per subspace. Synthetic as well asreal data experiments complement our theoretical study, illustrating ourapproach and demonstrating its effectiveness.

