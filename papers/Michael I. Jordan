Leo Breiman

  Statistics is a uniquely difficult field to convey to the uninitiated. Itsits astride the abstract and the concrete, the theoretical and the applied. Ithas a mathematical flavor and yet it is not simply a branch of mathematics. Itscore problems blend into those of the disciplines that probe into the nature ofintelligence and thought, in particular philosophy, psychology and artificialintelligence. Debates over foundational issues have waxed and waned, but thefield has not yet arrived at a single foundational perspective.

On statistics, computation and scalability

  How should statistical procedures be designed so as to be scalablecomputationally to the massive datasets that are increasingly the norm? Whencoupled with the requirement that an answer to an inferential question bedelivered within a certain time budget, this question has significantrepercussions for the field of statistics. With the goal of identifying"time-data tradeoffs," we investigate some of the statistical consequences ofcomputational perspectives on scability, in particular divide-and-conquermethodology and hierarchies of convex relaxations.

Comment on "Support Vector Machines with Applications"

  Comment on "Support Vector Machines with Applications" [math.ST/0612817]

Computing Upper and Lower Bounds on Likelihoods in Intractable Networks

  We present deterministic techniques for computing upper and lower bounds onmarginal probabilities in sigmoid and noisy-OR networks. These techniquesbecome useful when the size of the network (or clique size) precludes exactcomputations. We illustrate the tightness of the bounds by numericalexperiments.

Gradient Descent Converges to Minimizers

  We show that gradient descent converges to a local minimizer, almost surelywith random initialization. This is proved by applying the Stable ManifoldTheorem from dynamical systems theory.

A Short Note on Concentration Inequalities for Random Vectors with  SubGaussian Norm

  In this note, we derive concentration inequalities for random vectors withsubGaussian norm (a generalization of both subGaussian random vectors and normbounded random vectors), which are tight up to logarithmic factors.

Subtree power analysis finds optimal species for comparative genomics

  Sequence comparison across multiple organisms aids in the detection ofregions under selection. However, resource limitations require a prioritizationof genomes to be sequenced. This prioritization should be grounded in twoconsiderations: the lineal scope encompassing the biological phenomena ofinterest, and the optimal species within that scope for detecting functionalelements. We introduce a statistical framework for optimal species subsetselection, based on maximizing power to detect conserved sites. In a study ofvertebrate species, we show that the optimal species subset is not in generalthe most evolutionarily diverged subset. Our results suggest that marsupialsare prime sequencing candidates.

Associative Geometries. I: Torsors, linear relations and Grassmannians

  We define and investigate a geometric object, called an associative geometry,corresponding to an associative algebra (and, more generally, to an associativepair). Associative geometries combine aspects of Lie groups and of generalizedprojective geometries, where the former correspond to the Lie product of anassociative algebra and the latter to its Jordan product. A further developmentof the theory encompassing involutive associative algebras will be given insubsequent work.

Associative Geometries. I: Torsors, linear relations and Grassmannians

  We define and investigate a geometric object, called an associative geometry,corresponding to an associative algebra (and, more generally, to an associativepair). Associative geometries combine aspects of Lie groups and of generalizedprojective geometries, where the former correspond to the Lie product of anassociative algebra and the latter to its Jordan product. A further developmentof the theory encompassing involutive associative algebras will be given insubsequent work.

Modeling Events with Cascades of Poisson Processes

  We present a probabilistic model of events in continuous time in which eachevent triggers a Poisson process of successor events. The ensemble of observedevents is thereby modeled as a superposition of Poisson processes. Efficientinference is feasible under this model with an EM algorithm. Moreover, the EMalgorithm can be implemented as a distributed algorithm, permitting the modelto be applied to very large datasets. We apply these techniques to the modelingof Twitter messages and the revision history of Wikipedia.

Bayesian Multicategory Support Vector Machines

  We show that the multi-class support vector machine (MSVM) proposed by Leeet. al. (2004), can be viewed as a MAP estimation procedure under anappropriate probabilistic interpretation of the classifier. We also show thatthis interpretation can be extended to a hierarchical Bayesian architecture andto a fully-Bayesian inference procedure for multi-class classification based ondata augmentation. We present empirical results that show that the advantagesof the Bayesian formalism are obtained without a loss in classificationaccuracy.

Privacy Aware Learning

  We study statistical risk minimization problems under a privacy model inwhich the data is kept confidential even from the learner. In this localprivacy framework, we establish sharp upper and lower bounds on the convergencerates of statistical estimation procedures. As a consequence, we exhibit aprecise tradeoff between the amount of privacy the data preserves and theutility, as measured by convergence rate, of any statistical estimator orlearning procedure.

Loopy Belief Propogation and Gibbs Measures

  We address the question of convergence in the loopy belief propagation (LBP)algorithm. Specifically, we relate convergence of LBP to the existence of aweak limit for a sequence of Gibbs measures defined on the LBP s associatedcomputation tree.Using tools FROM the theory OF Gibbs measures we developeasily testable sufficient conditions FOR convergence.The failure OFconvergence OF LBP implies the existence OF multiple phases FOR the associatedGibbs specification.These results give new insight INTO the mechanics OF thealgorithm.

A Nested HDP for Hierarchical Topic Models

  We develop a nested hierarchical Dirichlet process (nHDP) for hierarchicaltopic modeling. The nHDP is a generalization of the nested Chinese restaurantprocess (nCRP) that allows each word to follow its own path to a topic nodeaccording to a document-specific distribution on a shared tree. This alleviatesthe rigid, single-path formulation of the nCRP, allowing a document to moreeasily express thematic borrowings as a random effect. We demonstrate ouralgorithm on 1.8 million documents from The New York Times.

Lower bounds on the performance of polynomial-time algorithms for sparse  linear regression

  Under a standard assumption in complexity theory (NP not in P/poly), wedemonstrate a gap between the minimax prediction risk for sparse linearregression that can be achieved by polynomial-time algorithms, and thatachieved by optimal algorithms. In particular, when the design matrix isill-conditioned, the minimax prediction loss achievable by polynomial-timealgorithms can be substantially greater than that of an optimal algorithm. Thisresult is the first known gap between polynomial and optimal algorithms forsparse linear regression, and does not depend on conjectures in average-casecomplexity.

A Kernelized Stein Discrepancy for Goodness-of-fit Tests and Model  Evaluation

  We derive a new discrepancy statistic for measuring differences between twoprobability distributions based on combining Stein's identity with thereproducing kernel Hilbert space theory. We apply our result to test how well aprobabilistic model fits a set of observations, and derive a new class ofpowerful goodness-of-fit tests that are widely applicable for complex and highdimensional distributions, even for those with computationally intractablenormalization constants. Both theoretical and empirical properties of ourmethods are studied thoroughly.

Kernel Feature Selection via Conditional Covariance Minimization

  We propose a method for feature selection that employs kernel-based measuresof independence to find a subset of covariates that is maximally predictive ofthe response. Building on past work in kernel dimension reduction, we show howto perform feature selection via a constrained optimization problem involvingthe trace of the conditional covariance operator. We prove various consistencyresults for this procedure, and also demonstrate that our method comparesfavorably with other state-of-the-art algorithms on a variety of synthetic andreal data sets.

First-order Methods Almost Always Avoid Saddle Points

  We establish that first-order methods avoid saddle points for almost allinitializations. Our results apply to a wide variety of first-order methods,including gradient descent, block coordinate descent, mirror descent andvariants thereof. The connecting thread is that such algorithms can be studiedfrom a dynamical systems perspective in which appropriate instantiations of theStable Manifold Theorem allow for a global stability analysis. Thus, neitheraccess to second-order derivative information nor randomness beyondinitialization is necessary to provably avoid saddle points.

Stochastic Cubic Regularization for Fast Nonconvex Optimization

  This paper proposes a stochastic variant of a classic algorithm---thecubic-regularized Newton method [Nesterov and Polyak 2006]. The proposedalgorithm efficiently escapes saddle points and finds approximate local minimafor general smooth, nonconvex functions in only$\mathcal{\tilde{O}}(\epsilon^{-3.5})$ stochastic gradient and stochasticHessian-vector product evaluations. The latter can be computed as efficientlyas stochastic gradients. This improves upon the$\mathcal{\tilde{O}}(\epsilon^{-4})$ rate of stochastic gradient descent. Ourrate matches the best-known result for finding local minima without requiringany delicate acceleration or variance-reduction techniques.

A Deep Generative Model for Semi-Supervised Classification with Noisy  Labels

  Class labels are often imperfectly observed, due to mistakes and to genuineambiguity among classes. We propose a new semi-supervised deep generative modelthat explicitly models noisy labels, called the Mislabeled VAE (M-VAE). TheM-VAE can perform better than existing deep generative models which do notaccount for label noise. Additionally, the derivation of M-VAE gives newtheoretical insights into the popular M1+M2 semi-supervised model.

Probabilistic Multilevel Clustering via Composite Transportation  Distance

  We propose a novel probabilistic approach to multilevel clustering problemsbased on composite transportation distance, which is a variant oftransportation distance where the underlying metric is Kullback-Leiblerdivergence. Our method involves solving a joint optimization problem overspaces of probability measures to simultaneously discover grouping structureswithin groups and among groups. By exploiting the connection of our method tothe problem of finding composite transportation barycenters, we develop fastand efficient optimization algorithms even for potentially large-scalemultilevel datasets. Finally, we present experimental results with bothsynthetic and real data to demonstrate the efficiency and scalability of theproposed approach.

Gen-Oja: A Simple and Efficient Algorithm for Streaming Generalized  Eigenvector Computation

  In this paper, we study the problems of principal Generalized Eigenvectorcomputation and Canonical Correlation Analysis in the stochastic setting. Wepropose a simple and efficient algorithm, Gen-Oja, for these problems. We provethe global convergence of our algorithm, borrowing ideas from the theory offast-mixing Markov chains and two-time-scale stochastic approximation, showingthat it achieves the optimal rate of convergence. In the process, we developtools for understanding stochastic processes with Markovian noise which mightbe of independent interest.

Quantitative Central Limit Theorems for Discrete Stochastic Processes

  In this paper, we establish a generalization of the classical Central LimitTheorem for a family of stochastic processes that includes stochastic gradientdescent and related gradient-based algorithms. Under certain regularityassumptions, we show that the iterates of these stochastic processes convergeto an invariant distribution at a rate of $O\lrp{1/\sqrt{k}}$ where $k$ is thenumber of steps; this rate is provably tight.

Is There an Analog of Nesterov Acceleration for MCMC?

  We formulate gradient-based Markov chain Monte Carlo (MCMC) sampling asoptimization on the space of probability measures, with Kullback-Leibler (KL)divergence as the objective function. We show that an underdamped form of theLangevin algorithm perform accelerated gradient descent in this metric. Tocharacterize the convergence of the algorithm, we construct a Lyapunovfunctional and exploit hypocoercivity of the underdamped Langevin algorithm. Asan application, we show that accelerated rates can be obtained for a class ofnonconvex functions with the Langevin algorithm.

LS-Tree: Model Interpretation When the Data Are Linguistic

  We study the problem of interpreting trained classification models in thesetting of linguistic data sets. Leveraging a parse tree, we propose to assignleast-squares based importance scores to each word of an instance by exploitingsyntactic constituency structure. We establish an axiomatic characterization ofthese importance scores by relating them to the Banzhaf value in coalitionalgame theory. Based on these importance scores, we develop a principled methodfor detecting and quantifying interactions between words in a sentence. Wedemonstrate that the proposed method can aid in interpretability anddiagnostics for several widely-used language models.

MAD-Bayes: MAP-based Asymptotic Derivations from Bayes

  The classical mixture of Gaussians model is related to K-means viasmall-variance asymptotics: as the covariances of the Gaussians tend to zero,the negative log-likelihood of the mixture of Gaussians model approaches theK-means objective, and the EM algorithm approaches the K-means algorithm. Kulis& Jordan (2012) used this observation to obtain a novel K-means-like algorithmfrom a Gibbs sampler for the Dirichlet process (DP) mixture. We insteadconsider applying small-variance asymptotics directly to the posterior inBayesian nonparametric models. This framework is independent of any specificBayesian inference algorithm, and it has the major advantage that itgeneralizes immediately to a range of models beyond the DP mixture. Toillustrate, we apply our framework to the feature learning setting, where thebeta process and Indian buffet process provide an appropriate Bayesiannonparametric prior. We obtain a novel objective function that goes beyondclustering to learn (and penalize new) groupings for which we relax the mutualexclusivity and exhaustivity assumptions of clustering. We demonstrate severalother algorithms, all of which are scalable and simple to implement. Empiricalresults demonstrate the benefits of the new framework.

Non-convex Finite-Sum Optimization Via SCSG Methods

  We develop a class of algorithms, as variants of the stochasticallycontrolled stochastic gradient (SCSG) methods (Lei and Jordan, 2016), for thesmooth non-convex finite-sum optimization problem. Assuming the smoothness ofeach component, the complexity of SCSG to reach a stationary point with$\mathbb{E} \|\nabla f(x)\|^{2}\le \epsilon$ is $O\left (\min\{\epsilon^{-5/3},\epsilon^{-1}n^{2/3}\}\right)$, which strictly outperforms the stochasticgradient descent. Moreover, SCSG is never worse than the state-of-the-artmethods based on variance reduction and it significantly outperforms them whenthe target accuracy is low. A similar acceleration is also achieved when thefunctions satisfy the Polyak-Lojasiewicz condition. Empirical experimentsdemonstrate that SCSG outperforms stochastic gradient methods on trainingmulti-layers neural networks in terms of both training and validation loss.

On Symplectic Optimization

  Accelerated gradient methods have had significant impact in machine learning-- in particular the theoretical side of machine learning -- due to theirability to achieve oracle lower bounds. But their heuristic construction hashindered their full integration into the practical machine-learning algorithmictoolbox, and has limited their scope. In this paper we build on recent workwhich casts acceleration as a phenomenon best explained in continuous time, andwe augment that picture by providing a systematic methodology for convertingcontinuous-time dynamics into discrete-time algorithms while retaining oraclerates. Our framework is based on ideas from Hamiltonian dynamical systems andsymplectic integration. These ideas have had major impact in many areas inapplied mathematics, but have not yet been seen to have a relationship withoptimization.

Synaptic Transmission: An Information-Theoretic Perspective

  Here we analyze synaptic transmission from an information-theoreticperspective. We derive closed-form expressions for the lower-bounds on thecapacity of a simple model of a cortical synapse under two explicit codingparadigms. Under the ``signal estimation'' paradigm, we assume the signal to beencoded in the mean firing rate of a Poisson neuron. The performance of anoptimal linear estimator of the signal then provides a lower bound on thecapacity for signal estimation. Under the ``signal detection'' paradigm, thepresence or absence of the signal has to be detected. Performance of theoptimal spike detector allows us to compute a lower bound on the capacity forsignal detection. We find that single synapses (for empirically measuredparameter values) transmit information poorly but significant improvement canbe achieved with a small amount of redundancy.

G2C2 I: Homogeneous SDSS photometry for Galactic GCs

  We present $g^\prime$ and $z^\prime$ aperture photometry for 96 GalacticGlobular Clusters, making this the largest homogeneous catalog of photometryfor these objects in the SDSS filter system. For a subset of 56 clusters wealso provide photometry in $r^\prime$ and $i^\prime$. We carry out comparisonswith previous photometry as well as with the SDSS dataset. The data will beuseful for a series of applications in Galactic and extragalactic astrophysics.Future papers will analyse the colour-metallicity relation, colour-magnitudediagrams, and structural parameters. The compilation of results based on thisdataset will be collected in the Galactic Globular Cluster Catalog (G2C2).

MLI: An API for Distributed Machine Learning

  MLI is an Application Programming Interface designed to address thechallenges of building Machine Learn- ing algorithms in a distributed settingbased on data-centric computing. Its primary goal is to simplify thedevelopment of high-performance, scalable, distributed algorithms. Our initialresults show that, relative to existing systems, this interface can be used tobuild distributed implementations of a wide variety of common Machine Learningalgorithms with minimal complexity and highly competitive performance andscalability.

A Point Source Excess in Abell 1185: Intergalactic Globular Clusters?

  Deep imaging with WFPC2 and the Hubble Space Telescope has been used tosearch for a population of intergalactic globular clusters (GCs) belonging toAbell 1185, a richness I cluster at cz = 9800 km/s. The field is noteworthy inthat it contains no bright galaxies and yet is centered on the peak of thecluster's X-ray surface brightess distribution. We detect a total of 99 pointsources in this field to a limiting magnitude I_F814W ~ 26. An identicalanalysis of the Hubble Deep Field North, which serves as our control field,reveals a total of 12 objects in the same magnitude range. We discuss possibleexplanations for this discrepancy, and conclude that the point source excess islikely due to the presence of GCs within A1185. The number and spatialdistribution of these GCs are consistent with their being intergalactic innature, although we cannot rule out the possibility that some of the GCs may beassociated with neighboring bright galaxies. Deeper imaging with the AdvancedCamera for Surveys may resolve this ambiguity.

The ACS Fornax Cluster Survey. I. Introduction to the Survey and Data  Reduction Procedures

  The Fornax Cluster is a conspicuous cluster of galaxies in the southernhemisphere and the second largest collection of early-type galaxies within <~20 Mpc after the Virgo Cluster. In this paper,we present a brief introductionto the ACS Fornax Cluster Survey - a program to image, in the F475W (g_475) andF850LP (z_850) bandpasses, 43 early-type galaxies in Fornax using the AdvancedCamera for Surveys (ACS) on the Hubble Space Telescope. Combined with acompanion survey of Virgo, the ACS Virgo Cluster Survey, this represents themost comprehensive imaging survey to date of early-type galaxies in clusterenvironments in terms of depth, spatial resolution, sample size andhomogeneity. We describe the selection of the program galaxies, their basicproperties, and the main science objectives of the survey which include themeasurement of luminosities, colors and structural parameters for globularclusters associated with these galaxies, an analysis of their isophotalproperties and surface brightness profiles, and an accurate calibration of thesurface brightness fluctuation distance indicator. Finally, we discuss the datareduction procedures adopted for the survey.

Revisiting k-means: New Algorithms via Bayesian Nonparametrics

  Bayesian models offer great flexibility for clusteringapplications---Bayesian nonparametrics can be used for modeling infinitemixtures, and hierarchical Bayesian models can be utilized for sharing clustersacross multiple data sets. For the most part, such flexibility is lacking inclassical clustering methods such as k-means. In this paper, we revisit thek-means clustering algorithm from a Bayesian nonparametric viewpoint. Inspiredby the asymptotic connection between k-means and mixtures of Gaussians, we showthat a Gibbs sampling algorithm for the Dirichlet process mixture approaches ahard clustering algorithm in the limit, and further that the resultingalgorithm monotonically minimizes an elegant underlying k-means-like clusteringobjective that includes a penalty for the number of clusters. We generalizethis analysis to the case of clustering multiple data sets through a similarasymptotic argument with the hierarchical Dirichlet process. We also discussfurther extensions that highlight the benefits of our analysis: i) a spectralrelaxation involving thresholded eigenvectors, and ii) a normalized cut graphclustering algorithm that does not fix the number of clusters in the graph.

MALT-45: A 7 mm survey of the southern Galaxy - I. Techniques and  spectral line data

  We present the first results from the MALT-45 (Millimetre Astronomer's LegacyTeam - 45 GHz) Galactic Plane survey. We have observed 5 square-degrees ($l =330 - 335$, $b = \pm0.5$) for spectral lines in the 7 mm band (42-44 and 48-49GHz), including $\text{CS}$ $(1-0)$, class I $\text{CH}_3\text{OH}$ masers inthe $7(0,7)-6(1,6)$ $\text{A}^{+}$ transition and $\text{SiO}$ $(1-0)$$v=0,1,2,3$. MALT-45 is the first unbiased, large-scale, sensitive spectralline survey in this frequency range. In this paper, we present data from thesurvey as well as a few intriguing results; rigorous analyses of these sciencecases are reserved for future publications. Across the survey region, wedetected 77 class I $\text{CH}_3\text{OH}$ masers, of which 58 are newdetections, along with many sites of thermal and maser $\text{SiO}$ emissionand thermal $\text{CS}$. We found that 35 class I $\text{CH}_3\text{OH}$ maserswere associated with the published locations of class II$\text{CH}_3\text{OH}$, $\text{H}_2\text{O}$ and $\text{OH}$ masers but 42 haveno known masers within 60 arcsec. We compared the MALT-45 $\text{CS}$ with$\text{NH}_3$ (1,1) to reveal regions of $\text{CS}$ depletion and highopacity, as well as evolved star-forming regions with a high ratio of$\text{CS}$ to $\text{NH}_3$. All $\text{SiO}$ masers are new detections, andappear to be associated with evolved stars from the $\it{Spitzer}$ GalacticLegacy Infrared Mid-Plane Survey Extraordinaire (GLIMPSE). Generally, within$\text{SiO}$ regions of multiple vibrational modes, the intensity decreases as$v=1,2,3$, but there are a few exceptions where $v=2$ is stronger than $v=1$.

A direct formulation for sparse PCA using semidefinite programming

  We examine the problem of approximating, in the Frobenius-norm sense, apositive, semidefinite symmetric matrix by a rank-one matrix, with an upperbound on the cardinality of its eigenvector. The problem arises in thedecomposition of a covariance matrix into sparse factors, and has wideapplications ranging from biology to finance. We use a modification of theclassical variational representation of the largest eigenvalue of a symmetricmatrix, where cardinality is constrained, and derive a semidefinite programmingbased relaxation for our problem. We also discuss Nesterov's smoothminimization technique applied to the SDP arising in the direct sparse PCAmethod.

On optimal quantization rules for some problems in sequential  decentralized detection

  We consider the design of systems for sequential decentralized detection, aproblem that entails several interdependent choices: the choice of a stoppingrule (specifying the sample size), a global decision function (a choice betweentwo competing hypotheses), and a set of quantization rules (the local decisionson the basis of which the global decision is made). This paper addresses anopen problem of whether in the Bayesian formulation of sequential decentralizeddetection, optimal local decision functions can be found within the class ofstationary rules. We develop an asymptotic approximation to the optimal cost ofstationary quantization rules and exploit this approximation to show thatstationary quantizers are not optimal in a broad class of settings. We alsoconsider the class of blockwise stationary quantizers, and show thatasymptotically optimal quantizers are likelihood-based threshold rules.

The nested Chinese restaurant process and Bayesian nonparametric  inference of topic hierarchies

  We present the nested Chinese restaurant process (nCRP), a stochastic processwhich assigns probability distributions to infinitely-deep,infinitely-branching trees. We show how this stochastic process can be used asa prior distribution in a Bayesian nonparametric model of document collections.Specifically, we present an application to information retrieval in whichdocuments are modeled as paths down a random tree, and the preferentialattachment dynamics of the nCRP leads to clustering of documents according tosharing of topics at multiple levels of abstraction. Given a corpus ofdocuments, a posterior inference algorithm finds an approximation to aposterior distribution over trees, topics and allocations of words to levels ofthe tree. We demonstrate this algorithm on collections of scientific abstractsfrom several journals. This model exemplifies a recent trend in statisticalmachine learning--the use of Bayesian nonparametric methods to inferdistributions on flexible data structures.

Estimating divergence functionals and the likelihood ratio by convex  risk minimization

  We develop and analyze $M$-estimation methods for divergence functionals andthe likelihood ratios of two probability distributions. Our method is based ona non-asymptotic variational characterization of $f$-divergences, which allowsthe problem of estimating divergences to be tackled via convex empirical riskoptimization. The resulting estimators are simple to implement, requiring onlythe solution of standard convex programs. We present an analysis of consistencyand convergence for these estimators. Given conditions only on the ratios ofdensities, we show that our estimators can achieve optimal minimax rates forthe likelihood ratio and the divergence functionals in certain regimes. Wederive an efficient optimization algorithm for computing our estimates, andillustrate their convergence behavior and practical viability by simulations.

Kernel dimension reduction in regression

  We present a new methodology for sufficient dimension reduction (SDR). Ourmethodology derives directly from the formulation of SDR in terms of theconditional independence of the covariate $X$ from the response $Y$, given theprojection of $X$ on the central subspace [cf. J. Amer. Statist. Assoc. 86(1991) 316--342 and Regression Graphics (1998) Wiley]. We show that thisconditional independence assertion can be characterized in terms of conditionalcovariance operators on reproducing kernel Hilbert spaces and we show how thischaracterization leads to an $M$-estimator for the central subspace. Theresulting estimator is shown to be consistent under weak conditions; inparticular, we do not have to impose linearity or ellipticity conditions of thekinds that are generally invoked for SDR methods. We also present empiricalresults showing that the new methodology is competitive in practice.

Bayesian Nonparametric Inference of Switching Linear Dynamical Systems

  Many complex dynamical phenomena can be effectively modeled by a system thatswitches among a set of conditionally linear dynamical modes. We consider twosuch models: the switching linear dynamical system (SLDS) and the switchingvector autoregressive (VAR) process. Our Bayesian nonparametric approachutilizes a hierarchical Dirichlet process prior to learn an unknown number ofpersistent, smooth dynamical modes. We additionally employ automatic relevancedetermination to infer a sparse set of dynamic dependencies allowing us tolearn SLDS with varying state dimension or switching VAR processes with varyingautoregressive order. We develop a sampling algorithm that combines a truncatedapproximation to the Dirichlet process with efficient joint sampling of themode and state sequences. The utility and flexibility of our model aredemonstrated on synthetic data, sequences of dancing honey bees, the IBOVESPAstock index, and a maneuvering target tracking application.

Tree-Structured Stick Breaking Processes for Hierarchical Data

  Many data are naturally modeled by an unobserved hierarchical structure. Inthis paper we propose a flexible nonparametric prior over unknown datahierarchies. The approach uses nested stick-breaking processes to allow fortrees of unbounded width and depth, where data can live at any node and areinfinitely exchangeable. One can view our model as providing infinite mixtureswhere the components have a dependency structure corresponding to anevolutionary diffusion down a tree. By using a stick-breaking approach, we canapply Markov chain Monte Carlo methods based on slice sampling to performBayesian inference and simulate from the posterior distribution on trees. Weapply our method to hierarchical clustering of images and topic modeling oftext data.

Heavy-Tailed Processes for Selective Shrinkage

  Heavy-tailed distributions are frequently used to enhance the robustness ofregression and classification methods to outliers in output space. Often,however, we are confronted with "outliers" in input space, which are isolatedobservations in sparsely populated regions. We show that heavy-tailedstochastic processes (which we construct from Gaussian processes via a copula),can be used to improve robustness of regression and classification estimatorsto such outliers by selectively shrinking them more strongly in sparse regionsthan in dense regions. We carry out a theoretical analysis to show thatselective shrinkage occurs, provided the marginals of the heavy-tailed processhave sufficiently heavy tails. The analysis is complemented by experiments onbiological data which indicate significant improvements of estimates in sparseregions while producing competitive results in dense regions.

Multiway Spectral Clustering: A Margin-Based Perspective

  Spectral clustering is a broad class of clustering procedures in which anintractable combinatorial optimization formulation of clustering is "relaxed"into a tractable eigenvector problem, and in which the relaxed solution issubsequently "rounded" into an approximate discrete solution to the originalproblem. In this paper we present a novel margin-based perspective on multiwayspectral clustering. We show that the margin-based perspective illuminates boththe relaxation and rounding aspects of spectral clustering, providing a unifiedanalysis of existing algorithms and guiding the design of new algorithms. Wealso present connections between spectral clustering and several other topicsin statistics, specifically minimum-variance clustering, Procrustes analysisand Gaussian intrinsic autoregression.

Cluster Forests

  With inspiration from Random Forests (RF) in the context of classification, anew clustering ensemble method---Cluster Forests (CF) is proposed.Geometrically, CF randomly probes a high-dimensional data cloud to obtain "goodlocal clusterings" and then aggregates via spectral clustering to obtaincluster assignments for the whole dataset. The search for good localclusterings is guided by a cluster quality measure kappa. CF progressivelyimproves each local clustering in a fashion that resembles the tree growth inRF. Empirical studies on several real-world datasets under two differentperformance metrics show that CF compares favorably to its competitors.Theoretical analysis reveals that the kappa measure makes it possible to growthe local clustering in a desirable way---it is "noise-resistant". Aclosed-form expression is obtained for the mis-clustering rate of spectralclustering under a perturbation model, which yields new insights into someaspects of spectral clustering.

Ergodic Mirror Descent

  We generalize stochastic subgradient descent methods to situations in whichwe do not receive independent samples from the distribution over which weoptimize, but instead receive samples that are coupled over time. We show thatas long as the source of randomness is suitably ergodic---it converges quicklyenough to a stationary distribution---the method enjoys strong convergenceguarantees, both in expectation and with high probability. This result hasimplications for stochastic optimization in high-dimensional spaces,peer-to-peer distributed optimization schemes, decision problems with dependentdata, and stochastic optimization problems over combinatorial spaces.

Beta processes, stick-breaking, and power laws

  The beta-Bernoulli process provides a Bayesian nonparametric prior for modelsinvolving collections of binary-valued features. A draw from the beta processyields an infinite collection of probabilities in the unit interval, and a drawfrom the Bernoulli process turns these into binary-valued features. Recent workhas provided stick-breaking representations for the beta process analogous tothe well-known stick-breaking representation for the Dirichlet process. Wederive one such stick-breaking representation directly from thecharacterization of the beta process as a completely random measure. Thisapproach motivates a three-parameter generalization of the beta process, and westudy the power laws that can be obtained from this generalized beta process.We present a posterior inference algorithm for the beta-Bernoulli process thatexploits the stick-breaking representation, and we present experimental resultsfor a discrete factor-analysis model.

Distributed Matrix Completion and Robust Factorization

  If learning methods are to scale to the massive sizes of modern datasets, itis essential for the field of machine learning to embrace parallel anddistributed computing. Inspired by the recent development of matrixfactorization methods with rich theory but poor computational complexity and bythe relative ease of mapping matrices onto distributed architectures, weintroduce a scalable divide-and-conquer framework for noisy matrixfactorization. We present a thorough theoretical analysis of this framework inwhich we characterize the statistical errors introduced by the "divide" stepand control their magnitude in the "conquer" step, so that the overallalgorithm enjoys high-probability estimation guarantees comparable to those ofits base algorithm. We also present experiments in collaborative filtering andvideo background modeling that demonstrate the near-linear to superlinearspeed-ups attainable with this approach.

The Stick-Breaking Construction of the Beta Process as a Poisson Process

  We show that the stick-breaking construction of the beta process due toPaisley, et al. (2010) can be obtained from the characterization of the betaprocess as a Poisson process. Specifically, we show that the mean measure ofthe underlying Poisson process is equal to that of the beta process. We usethis underlying representation to derive error bounds on truncated betaprocesses that are tighter than those in the literature. We also develop a newMCMC inference algorithm for beta processes, based in part on our new Poissonprocess construction.

Learning Dependency-Based Compositional Semantics

  Suppose we want to build a system that answers a natural language question byrepresenting its semantics as a logical form and computing the answer given astructured database of facts. The core part of such a system is the semanticparser that maps questions to logical forms. Semantic parsers are typicallytrained from examples of questions annotated with their target logical forms,but this type of annotation is expensive.  Our goal is to learn a semantic parser from question-answer pairs instead,where the logical form is modeled as a latent variable. Motivated by thischallenging learning problem, we develop a new semantic formalism,dependency-based compositional semantics (DCS), which has favorable linguistic,statistical, and computational properties. We define a log-linear distributionover DCS logical forms and estimate the parameters using a simple procedurethat alternates between beam search and numerical optimization. On two standardsemantic parsing benchmarks, our system outperforms all existingstate-of-the-art systems, despite using no annotated logical forms.

