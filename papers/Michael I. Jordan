Leo Breiman

  Statistics is a uniquely difficult field to convey to the uninitiated. It
sits astride the abstract and the concrete, the theoretical and the applied. It
has a mathematical flavor and yet it is not simply a branch of mathematics. Its
core problems blend into those of the disciplines that probe into the nature of
intelligence and thought, in particular philosophy, psychology and artificial
intelligence. Debates over foundational issues have waxed and waned, but the
field has not yet arrived at a single foundational perspective.


On statistics, computation and scalability

  How should statistical procedures be designed so as to be scalable
computationally to the massive datasets that are increasingly the norm? When
coupled with the requirement that an answer to an inferential question be
delivered within a certain time budget, this question has significant
repercussions for the field of statistics. With the goal of identifying
"time-data tradeoffs," we investigate some of the statistical consequences of
computational perspectives on scability, in particular divide-and-conquer
methodology and hierarchies of convex relaxations.


Comment on "Support Vector Machines with Applications"

  Comment on "Support Vector Machines with Applications" [math.ST/0612817]


Computing Upper and Lower Bounds on Likelihoods in Intractable Networks

  We present deterministic techniques for computing upper and lower bounds on
marginal probabilities in sigmoid and noisy-OR networks. These techniques
become useful when the size of the network (or clique size) precludes exact
computations. We illustrate the tightness of the bounds by numerical
experiments.


Gradient Descent Converges to Minimizers

  We show that gradient descent converges to a local minimizer, almost surely
with random initialization. This is proved by applying the Stable Manifold
Theorem from dynamical systems theory.


A Short Note on Concentration Inequalities for Random Vectors with
  SubGaussian Norm

  In this note, we derive concentration inequalities for random vectors with
subGaussian norm (a generalization of both subGaussian random vectors and norm
bounded random vectors), which are tight up to logarithmic factors.


Subtree power analysis finds optimal species for comparative genomics

  Sequence comparison across multiple organisms aids in the detection of
regions under selection. However, resource limitations require a prioritization
of genomes to be sequenced. This prioritization should be grounded in two
considerations: the lineal scope encompassing the biological phenomena of
interest, and the optimal species within that scope for detecting functional
elements. We introduce a statistical framework for optimal species subset
selection, based on maximizing power to detect conserved sites. In a study of
vertebrate species, we show that the optimal species subset is not in general
the most evolutionarily diverged subset. Our results suggest that marsupials
are prime sequencing candidates.


Associative Geometries. I: Torsors, linear relations and Grassmannians

  We define and investigate a geometric object, called an associative geometry,
corresponding to an associative algebra (and, more generally, to an associative
pair). Associative geometries combine aspects of Lie groups and of generalized
projective geometries, where the former correspond to the Lie product of an
associative algebra and the latter to its Jordan product. A further development
of the theory encompassing involutive associative algebras will be given in
subsequent work.


Associative Geometries. I: Torsors, linear relations and Grassmannians

  We define and investigate a geometric object, called an associative geometry,
corresponding to an associative algebra (and, more generally, to an associative
pair). Associative geometries combine aspects of Lie groups and of generalized
projective geometries, where the former correspond to the Lie product of an
associative algebra and the latter to its Jordan product. A further development
of the theory encompassing involutive associative algebras will be given in
subsequent work.


Privacy Aware Learning

  We study statistical risk minimization problems under a privacy model in
which the data is kept confidential even from the learner. In this local
privacy framework, we establish sharp upper and lower bounds on the convergence
rates of statistical estimation procedures. As a consequence, we exhibit a
precise tradeoff between the amount of privacy the data preserves and the
utility, as measured by convergence rate, of any statistical estimator or
learning procedure.


Modeling Events with Cascades of Poisson Processes

  We present a probabilistic model of events in continuous time in which each
event triggers a Poisson process of successor events. The ensemble of observed
events is thereby modeled as a superposition of Poisson processes. Efficient
inference is feasible under this model with an EM algorithm. Moreover, the EM
algorithm can be implemented as a distributed algorithm, permitting the model
to be applied to very large datasets. We apply these techniques to the modeling
of Twitter messages and the revision history of Wikipedia.


Loopy Belief Propogation and Gibbs Measures

  We address the question of convergence in the loopy belief propagation (LBP)
algorithm. Specifically, we relate convergence of LBP to the existence of a
weak limit for a sequence of Gibbs measures defined on the LBP s associated
computation tree.Using tools FROM the theory OF Gibbs measures we develop
easily testable sufficient conditions FOR convergence.The failure OF
convergence OF LBP implies the existence OF multiple phases FOR the associated
Gibbs specification.These results give new insight INTO the mechanics OF the
algorithm.


A Nested HDP for Hierarchical Topic Models

  We develop a nested hierarchical Dirichlet process (nHDP) for hierarchical
topic modeling. The nHDP is a generalization of the nested Chinese restaurant
process (nCRP) that allows each word to follow its own path to a topic node
according to a document-specific distribution on a shared tree. This alleviates
the rigid, single-path formulation of the nCRP, allowing a document to more
easily express thematic borrowings as a random effect. We demonstrate our
algorithm on 1.8 million documents from The New York Times.


A Kernelized Stein Discrepancy for Goodness-of-fit Tests and Model
  Evaluation

  We derive a new discrepancy statistic for measuring differences between two
probability distributions based on combining Stein's identity with the
reproducing kernel Hilbert space theory. We apply our result to test how well a
probabilistic model fits a set of observations, and derive a new class of
powerful goodness-of-fit tests that are widely applicable for complex and high
dimensional distributions, even for those with computationally intractable
normalization constants. Both theoretical and empirical properties of our
methods are studied thoroughly.


Bayesian Multicategory Support Vector Machines

  We show that the multi-class support vector machine (MSVM) proposed by Lee
et. al. (2004), can be viewed as a MAP estimation procedure under an
appropriate probabilistic interpretation of the classifier. We also show that
this interpretation can be extended to a hierarchical Bayesian architecture and
to a fully-Bayesian inference procedure for multi-class classification based on
data augmentation. We present empirical results that show that the advantages
of the Bayesian formalism are obtained without a loss in classification
accuracy.


Lower bounds on the performance of polynomial-time algorithms for sparse
  linear regression

  Under a standard assumption in complexity theory (NP not in P/poly), we
demonstrate a gap between the minimax prediction risk for sparse linear
regression that can be achieved by polynomial-time algorithms, and that
achieved by optimal algorithms. In particular, when the design matrix is
ill-conditioned, the minimax prediction loss achievable by polynomial-time
algorithms can be substantially greater than that of an optimal algorithm. This
result is the first known gap between polynomial and optimal algorithms for
sparse linear regression, and does not depend on conjectures in average-case
complexity.


Kernel Feature Selection via Conditional Covariance Minimization

  We propose a method for feature selection that employs kernel-based measures
of independence to find a subset of covariates that is maximally predictive of
the response. Building on past work in kernel dimension reduction, we show how
to perform feature selection via a constrained optimization problem involving
the trace of the conditional covariance operator. We prove various consistency
results for this procedure, and also demonstrate that our method compares
favorably with other state-of-the-art algorithms on a variety of synthetic and
real data sets.


First-order Methods Almost Always Avoid Saddle Points

  We establish that first-order methods avoid saddle points for almost all
initializations. Our results apply to a wide variety of first-order methods,
including gradient descent, block coordinate descent, mirror descent and
variants thereof. The connecting thread is that such algorithms can be studied
from a dynamical systems perspective in which appropriate instantiations of the
Stable Manifold Theorem allow for a global stability analysis. Thus, neither
access to second-order derivative information nor randomness beyond
initialization is necessary to provably avoid saddle points.


Stochastic Cubic Regularization for Fast Nonconvex Optimization

  This paper proposes a stochastic variant of a classic algorithm---the
cubic-regularized Newton method [Nesterov and Polyak 2006]. The proposed
algorithm efficiently escapes saddle points and finds approximate local minima
for general smooth, nonconvex functions in only
$\mathcal{\tilde{O}}(\epsilon^{-3.5})$ stochastic gradient and stochastic
Hessian-vector product evaluations. The latter can be computed as efficiently
as stochastic gradients. This improves upon the
$\mathcal{\tilde{O}}(\epsilon^{-4})$ rate of stochastic gradient descent. Our
rate matches the best-known result for finding local minima without requiring
any delicate acceleration or variance-reduction techniques.


A Deep Generative Model for Semi-Supervised Classification with Noisy
  Labels

  Class labels are often imperfectly observed, due to mistakes and to genuine
ambiguity among classes. We propose a new semi-supervised deep generative model
that explicitly models noisy labels, called the Mislabeled VAE (M-VAE). The
M-VAE can perform better than existing deep generative models which do not
account for label noise. Additionally, the derivation of M-VAE gives new
theoretical insights into the popular M1+M2 semi-supervised model.


Probabilistic Multilevel Clustering via Composite Transportation
  Distance

  We propose a novel probabilistic approach to multilevel clustering problems
based on composite transportation distance, which is a variant of
transportation distance where the underlying metric is Kullback-Leibler
divergence. Our method involves solving a joint optimization problem over
spaces of probability measures to simultaneously discover grouping structures
within groups and among groups. By exploiting the connection of our method to
the problem of finding composite transportation barycenters, we develop fast
and efficient optimization algorithms even for potentially large-scale
multilevel datasets. Finally, we present experimental results with both
synthetic and real data to demonstrate the efficiency and scalability of the
proposed approach.


Gen-Oja: A Simple and Efficient Algorithm for Streaming Generalized
  Eigenvector Computation

  In this paper, we study the problems of principal Generalized Eigenvector
computation and Canonical Correlation Analysis in the stochastic setting. We
propose a simple and efficient algorithm, Gen-Oja, for these problems. We prove
the global convergence of our algorithm, borrowing ideas from the theory of
fast-mixing Markov chains and two-time-scale stochastic approximation, showing
that it achieves the optimal rate of convergence. In the process, we develop
tools for understanding stochastic processes with Markovian noise which might
be of independent interest.


Quantitative Central Limit Theorems for Discrete Stochastic Processes

  In this paper, we establish a generalization of the classical Central Limit
Theorem for a family of stochastic processes that includes stochastic gradient
descent and related gradient-based algorithms. Under certain regularity
assumptions, we show that the iterates of these stochastic processes converge
to an invariant distribution at a rate of $O\lrp{1/\sqrt{k}}$ where $k$ is the
number of steps; this rate is provably tight.


Is There an Analog of Nesterov Acceleration for MCMC?

  We formulate gradient-based Markov chain Monte Carlo (MCMC) sampling as
optimization on the space of probability measures, with Kullback-Leibler (KL)
divergence as the objective function. We show that an underdamped form of the
Langevin algorithm perform accelerated gradient descent in this metric. To
characterize the convergence of the algorithm, we construct a Lyapunov
functional and exploit hypocoercivity of the underdamped Langevin algorithm. As
an application, we show that accelerated rates can be obtained for a class of
nonconvex functions with the Langevin algorithm.


LS-Tree: Model Interpretation When the Data Are Linguistic

  We study the problem of interpreting trained classification models in the
setting of linguistic data sets. Leveraging a parse tree, we propose to assign
least-squares based importance scores to each word of an instance by exploiting
syntactic constituency structure. We establish an axiomatic characterization of
these importance scores by relating them to the Banzhaf value in coalitional
game theory. Based on these importance scores, we develop a principled method
for detecting and quantifying interactions between words in a sentence. We
demonstrate that the proposed method can aid in interpretability and
diagnostics for several widely-used language models.


MAD-Bayes: MAP-based Asymptotic Derivations from Bayes

  The classical mixture of Gaussians model is related to K-means via
small-variance asymptotics: as the covariances of the Gaussians tend to zero,
the negative log-likelihood of the mixture of Gaussians model approaches the
K-means objective, and the EM algorithm approaches the K-means algorithm. Kulis
& Jordan (2012) used this observation to obtain a novel K-means-like algorithm
from a Gibbs sampler for the Dirichlet process (DP) mixture. We instead
consider applying small-variance asymptotics directly to the posterior in
Bayesian nonparametric models. This framework is independent of any specific
Bayesian inference algorithm, and it has the major advantage that it
generalizes immediately to a range of models beyond the DP mixture. To
illustrate, we apply our framework to the feature learning setting, where the
beta process and Indian buffet process provide an appropriate Bayesian
nonparametric prior. We obtain a novel objective function that goes beyond
clustering to learn (and penalize new) groupings for which we relax the mutual
exclusivity and exhaustivity assumptions of clustering. We demonstrate several
other algorithms, all of which are scalable and simple to implement. Empirical
results demonstrate the benefits of the new framework.


Non-convex Finite-Sum Optimization Via SCSG Methods

  We develop a class of algorithms, as variants of the stochastically
controlled stochastic gradient (SCSG) methods (Lei and Jordan, 2016), for the
smooth non-convex finite-sum optimization problem. Assuming the smoothness of
each component, the complexity of SCSG to reach a stationary point with
$\mathbb{E} \|\nabla f(x)\|^{2}\le \epsilon$ is $O\left (\min\{\epsilon^{-5/3},
\epsilon^{-1}n^{2/3}\}\right)$, which strictly outperforms the stochastic
gradient descent. Moreover, SCSG is never worse than the state-of-the-art
methods based on variance reduction and it significantly outperforms them when
the target accuracy is low. A similar acceleration is also achieved when the
functions satisfy the Polyak-Lojasiewicz condition. Empirical experiments
demonstrate that SCSG outperforms stochastic gradient methods on training
multi-layers neural networks in terms of both training and validation loss.


On Symplectic Optimization

  Accelerated gradient methods have had significant impact in machine learning
-- in particular the theoretical side of machine learning -- due to their
ability to achieve oracle lower bounds. But their heuristic construction has
hindered their full integration into the practical machine-learning algorithmic
toolbox, and has limited their scope. In this paper we build on recent work
which casts acceleration as a phenomenon best explained in continuous time, and
we augment that picture by providing a systematic methodology for converting
continuous-time dynamics into discrete-time algorithms while retaining oracle
rates. Our framework is based on ideas from Hamiltonian dynamical systems and
symplectic integration. These ideas have had major impact in many areas in
applied mathematics, but have not yet been seen to have a relationship with
optimization.


Synaptic Transmission: An Information-Theoretic Perspective

  Here we analyze synaptic transmission from an information-theoretic
perspective. We derive closed-form expressions for the lower-bounds on the
capacity of a simple model of a cortical synapse under two explicit coding
paradigms. Under the ``signal estimation'' paradigm, we assume the signal to be
encoded in the mean firing rate of a Poisson neuron. The performance of an
optimal linear estimator of the signal then provides a lower bound on the
capacity for signal estimation. Under the ``signal detection'' paradigm, the
presence or absence of the signal has to be detected. Performance of the
optimal spike detector allows us to compute a lower bound on the capacity for
signal detection. We find that single synapses (for empirically measured
parameter values) transmit information poorly but significant improvement can
be achieved with a small amount of redundancy.


G2C2 I: Homogeneous SDSS photometry for Galactic GCs

  We present $g^\prime$ and $z^\prime$ aperture photometry for 96 Galactic
Globular Clusters, making this the largest homogeneous catalog of photometry
for these objects in the SDSS filter system. For a subset of 56 clusters we
also provide photometry in $r^\prime$ and $i^\prime$. We carry out comparisons
with previous photometry as well as with the SDSS dataset. The data will be
useful for a series of applications in Galactic and extragalactic astrophysics.
Future papers will analyse the colour-metallicity relation, colour-magnitude
diagrams, and structural parameters. The compilation of results based on this
dataset will be collected in the Galactic Globular Cluster Catalog (G2C2).


MLI: An API for Distributed Machine Learning

  MLI is an Application Programming Interface designed to address the
challenges of building Machine Learn- ing algorithms in a distributed setting
based on data-centric computing. Its primary goal is to simplify the
development of high-performance, scalable, distributed algorithms. Our initial
results show that, relative to existing systems, this interface can be used to
build distributed implementations of a wide variety of common Machine Learning
algorithms with minimal complexity and highly competitive performance and
scalability.


A Point Source Excess in Abell 1185: Intergalactic Globular Clusters?

  Deep imaging with WFPC2 and the Hubble Space Telescope has been used to
search for a population of intergalactic globular clusters (GCs) belonging to
Abell 1185, a richness I cluster at cz = 9800 km/s. The field is noteworthy in
that it contains no bright galaxies and yet is centered on the peak of the
cluster's X-ray surface brightess distribution. We detect a total of 99 point
sources in this field to a limiting magnitude I_F814W ~ 26. An identical
analysis of the Hubble Deep Field North, which serves as our control field,
reveals a total of 12 objects in the same magnitude range. We discuss possible
explanations for this discrepancy, and conclude that the point source excess is
likely due to the presence of GCs within A1185. The number and spatial
distribution of these GCs are consistent with their being intergalactic in
nature, although we cannot rule out the possibility that some of the GCs may be
associated with neighboring bright galaxies. Deeper imaging with the Advanced
Camera for Surveys may resolve this ambiguity.


The ACS Fornax Cluster Survey. I. Introduction to the Survey and Data
  Reduction Procedures

  The Fornax Cluster is a conspicuous cluster of galaxies in the southern
hemisphere and the second largest collection of early-type galaxies within <~
20 Mpc after the Virgo Cluster. In this paper,we present a brief introduction
to the ACS Fornax Cluster Survey - a program to image, in the F475W (g_475) and
F850LP (z_850) bandpasses, 43 early-type galaxies in Fornax using the Advanced
Camera for Surveys (ACS) on the Hubble Space Telescope. Combined with a
companion survey of Virgo, the ACS Virgo Cluster Survey, this represents the
most comprehensive imaging survey to date of early-type galaxies in cluster
environments in terms of depth, spatial resolution, sample size and
homogeneity. We describe the selection of the program galaxies, their basic
properties, and the main science objectives of the survey which include the
measurement of luminosities, colors and structural parameters for globular
clusters associated with these galaxies, an analysis of their isophotal
properties and surface brightness profiles, and an accurate calibration of the
surface brightness fluctuation distance indicator. Finally, we discuss the data
reduction procedures adopted for the survey.


Revisiting k-means: New Algorithms via Bayesian Nonparametrics

  Bayesian models offer great flexibility for clustering
applications---Bayesian nonparametrics can be used for modeling infinite
mixtures, and hierarchical Bayesian models can be utilized for sharing clusters
across multiple data sets. For the most part, such flexibility is lacking in
classical clustering methods such as k-means. In this paper, we revisit the
k-means clustering algorithm from a Bayesian nonparametric viewpoint. Inspired
by the asymptotic connection between k-means and mixtures of Gaussians, we show
that a Gibbs sampling algorithm for the Dirichlet process mixture approaches a
hard clustering algorithm in the limit, and further that the resulting
algorithm monotonically minimizes an elegant underlying k-means-like clustering
objective that includes a penalty for the number of clusters. We generalize
this analysis to the case of clustering multiple data sets through a similar
asymptotic argument with the hierarchical Dirichlet process. We also discuss
further extensions that highlight the benefits of our analysis: i) a spectral
relaxation involving thresholded eigenvectors, and ii) a normalized cut graph
clustering algorithm that does not fix the number of clusters in the graph.


MALT-45: A 7 mm survey of the southern Galaxy - I. Techniques and
  spectral line data

  We present the first results from the MALT-45 (Millimetre Astronomer's Legacy
Team - 45 GHz) Galactic Plane survey. We have observed 5 square-degrees ($l =
330 - 335$, $b = \pm0.5$) for spectral lines in the 7 mm band (42-44 and 48-49
GHz), including $\text{CS}$ $(1-0)$, class I $\text{CH}_3\text{OH}$ masers in
the $7(0,7)-6(1,6)$ $\text{A}^{+}$ transition and $\text{SiO}$ $(1-0)$
$v=0,1,2,3$. MALT-45 is the first unbiased, large-scale, sensitive spectral
line survey in this frequency range. In this paper, we present data from the
survey as well as a few intriguing results; rigorous analyses of these science
cases are reserved for future publications. Across the survey region, we
detected 77 class I $\text{CH}_3\text{OH}$ masers, of which 58 are new
detections, along with many sites of thermal and maser $\text{SiO}$ emission
and thermal $\text{CS}$. We found that 35 class I $\text{CH}_3\text{OH}$ masers
were associated with the published locations of class II
$\text{CH}_3\text{OH}$, $\text{H}_2\text{O}$ and $\text{OH}$ masers but 42 have
no known masers within 60 arcsec. We compared the MALT-45 $\text{CS}$ with
$\text{NH}_3$ (1,1) to reveal regions of $\text{CS}$ depletion and high
opacity, as well as evolved star-forming regions with a high ratio of
$\text{CS}$ to $\text{NH}_3$. All $\text{SiO}$ masers are new detections, and
appear to be associated with evolved stars from the $\it{Spitzer}$ Galactic
Legacy Infrared Mid-Plane Survey Extraordinaire (GLIMPSE). Generally, within
$\text{SiO}$ regions of multiple vibrational modes, the intensity decreases as
$v=1,2,3$, but there are a few exceptions where $v=2$ is stronger than $v=1$.


A direct formulation for sparse PCA using semidefinite programming

  We examine the problem of approximating, in the Frobenius-norm sense, a
positive, semidefinite symmetric matrix by a rank-one matrix, with an upper
bound on the cardinality of its eigenvector. The problem arises in the
decomposition of a covariance matrix into sparse factors, and has wide
applications ranging from biology to finance. We use a modification of the
classical variational representation of the largest eigenvalue of a symmetric
matrix, where cardinality is constrained, and derive a semidefinite programming
based relaxation for our problem. We also discuss Nesterov's smooth
minimization technique applied to the SDP arising in the direct sparse PCA
method.


The nested Chinese restaurant process and Bayesian nonparametric
  inference of topic hierarchies

  We present the nested Chinese restaurant process (nCRP), a stochastic process
which assigns probability distributions to infinitely-deep,
infinitely-branching trees. We show how this stochastic process can be used as
a prior distribution in a Bayesian nonparametric model of document collections.
Specifically, we present an application to information retrieval in which
documents are modeled as paths down a random tree, and the preferential
attachment dynamics of the nCRP leads to clustering of documents according to
sharing of topics at multiple levels of abstraction. Given a corpus of
documents, a posterior inference algorithm finds an approximation to a
posterior distribution over trees, topics and allocations of words to levels of
the tree. We demonstrate this algorithm on collections of scientific abstracts
from several journals. This model exemplifies a recent trend in statistical
machine learning--the use of Bayesian nonparametric methods to infer
distributions on flexible data structures.


Estimating divergence functionals and the likelihood ratio by convex
  risk minimization

  We develop and analyze $M$-estimation methods for divergence functionals and
the likelihood ratios of two probability distributions. Our method is based on
a non-asymptotic variational characterization of $f$-divergences, which allows
the problem of estimating divergences to be tackled via convex empirical risk
optimization. The resulting estimators are simple to implement, requiring only
the solution of standard convex programs. We present an analysis of consistency
and convergence for these estimators. Given conditions only on the ratios of
densities, we show that our estimators can achieve optimal minimax rates for
the likelihood ratio and the divergence functionals in certain regimes. We
derive an efficient optimization algorithm for computing our estimates, and
illustrate their convergence behavior and practical viability by simulations.


Kernel dimension reduction in regression

  We present a new methodology for sufficient dimension reduction (SDR). Our
methodology derives directly from the formulation of SDR in terms of the
conditional independence of the covariate $X$ from the response $Y$, given the
projection of $X$ on the central subspace [cf. J. Amer. Statist. Assoc. 86
(1991) 316--342 and Regression Graphics (1998) Wiley]. We show that this
conditional independence assertion can be characterized in terms of conditional
covariance operators on reproducing kernel Hilbert spaces and we show how this
characterization leads to an $M$-estimator for the central subspace. The
resulting estimator is shown to be consistent under weak conditions; in
particular, we do not have to impose linearity or ellipticity conditions of the
kinds that are generally invoked for SDR methods. We also present empirical
results showing that the new methodology is competitive in practice.


Tree-Structured Stick Breaking Processes for Hierarchical Data

  Many data are naturally modeled by an unobserved hierarchical structure. In
this paper we propose a flexible nonparametric prior over unknown data
hierarchies. The approach uses nested stick-breaking processes to allow for
trees of unbounded width and depth, where data can live at any node and are
infinitely exchangeable. One can view our model as providing infinite mixtures
where the components have a dependency structure corresponding to an
evolutionary diffusion down a tree. By using a stick-breaking approach, we can
apply Markov chain Monte Carlo methods based on slice sampling to perform
Bayesian inference and simulate from the posterior distribution on trees. We
apply our method to hierarchical clustering of images and topic modeling of
text data.


Heavy-Tailed Processes for Selective Shrinkage

  Heavy-tailed distributions are frequently used to enhance the robustness of
regression and classification methods to outliers in output space. Often,
however, we are confronted with "outliers" in input space, which are isolated
observations in sparsely populated regions. We show that heavy-tailed
stochastic processes (which we construct from Gaussian processes via a copula),
can be used to improve robustness of regression and classification estimators
to such outliers by selectively shrinking them more strongly in sparse regions
than in dense regions. We carry out a theoretical analysis to show that
selective shrinkage occurs, provided the marginals of the heavy-tailed process
have sufficiently heavy tails. The analysis is complemented by experiments on
biological data which indicate significant improvements of estimates in sparse
regions while producing competitive results in dense regions.


Multiway Spectral Clustering: A Margin-Based Perspective

  Spectral clustering is a broad class of clustering procedures in which an
intractable combinatorial optimization formulation of clustering is "relaxed"
into a tractable eigenvector problem, and in which the relaxed solution is
subsequently "rounded" into an approximate discrete solution to the original
problem. In this paper we present a novel margin-based perspective on multiway
spectral clustering. We show that the margin-based perspective illuminates both
the relaxation and rounding aspects of spectral clustering, providing a unified
analysis of existing algorithms and guiding the design of new algorithms. We
also present connections between spectral clustering and several other topics
in statistics, specifically minimum-variance clustering, Procrustes analysis
and Gaussian intrinsic autoregression.


Cluster Forests

  With inspiration from Random Forests (RF) in the context of classification, a
new clustering ensemble method---Cluster Forests (CF) is proposed.
Geometrically, CF randomly probes a high-dimensional data cloud to obtain "good
local clusterings" and then aggregates via spectral clustering to obtain
cluster assignments for the whole dataset. The search for good local
clusterings is guided by a cluster quality measure kappa. CF progressively
improves each local clustering in a fashion that resembles the tree growth in
RF. Empirical studies on several real-world datasets under two different
performance metrics show that CF compares favorably to its competitors.
Theoretical analysis reveals that the kappa measure makes it possible to grow
the local clustering in a desirable way---it is "noise-resistant". A
closed-form expression is obtained for the mis-clustering rate of spectral
clustering under a perturbation model, which yields new insights into some
aspects of spectral clustering.


Ergodic Mirror Descent

  We generalize stochastic subgradient descent methods to situations in which
we do not receive independent samples from the distribution over which we
optimize, but instead receive samples that are coupled over time. We show that
as long as the source of randomness is suitably ergodic---it converges quickly
enough to a stationary distribution---the method enjoys strong convergence
guarantees, both in expectation and with high probability. This result has
implications for stochastic optimization in high-dimensional spaces,
peer-to-peer distributed optimization schemes, decision problems with dependent
data, and stochastic optimization problems over combinatorial spaces.


Distributed Matrix Completion and Robust Factorization

  If learning methods are to scale to the massive sizes of modern datasets, it
is essential for the field of machine learning to embrace parallel and
distributed computing. Inspired by the recent development of matrix
factorization methods with rich theory but poor computational complexity and by
the relative ease of mapping matrices onto distributed architectures, we
introduce a scalable divide-and-conquer framework for noisy matrix
factorization. We present a thorough theoretical analysis of this framework in
which we characterize the statistical errors introduced by the "divide" step
and control their magnitude in the "conquer" step, so that the overall
algorithm enjoys high-probability estimation guarantees comparable to those of
its base algorithm. We also present experiments in collaborative filtering and
video background modeling that demonstrate the near-linear to superlinear
speed-ups attainable with this approach.


Matrix concentration inequalities via the method of exchangeable pairs

  This paper derives exponential concentration inequalities and polynomial
moment inequalities for the spectral norm of a random matrix. The analysis
requires a matrix extension of the scalar concentration theory developed by
Sourav Chatterjee using Stein's method of exchangeable pairs. When applied to a
sum of independent random matrices, this approach yields matrix generalizations
of the classical inequalities due to Hoeffding, Bernstein, Khintchine and
Rosenthal. The same technique delivers bounds for sums of dependent random
matrices and more general matrix-valued functions of dependent random
variables.


Optimization of Structured Mean Field Objectives

  In intractable, undirected graphical models, an intuitive way of creating
structured mean field approximations is to select an acyclic tractable
subgraph. We show that the hardness of computing the objective function and
gradient of the mean field objective qualitatively depends on a simple graph
property. If the tractable subgraph has this property- we call such subgraphs
v-acyclic-a very fast block coordinate ascent algorithm is possible. If not,
optimization is harder, but we show a new algorithm based on the construction
of an auxiliary exponential family that can be used to make inference possible
in this case as well. We discuss the advantages and disadvantages of each
regime and compare the algorithms empirically.


Nested Hierarchical Dirichlet Processes

  We develop a nested hierarchical Dirichlet process (nHDP) for hierarchical
topic modeling. The nHDP is a generalization of the nested Chinese restaurant
process (nCRP) that allows each word to follow its own path to a topic node
according to a document-specific distribution on a shared tree. This alleviates
the rigid, single-path formulation of the nCRP, allowing a document to more
easily express thematic borrowings as a random effect. We derive a stochastic
variational inference algorithm for the model, in addition to a greedy subtree
selection method for each document, which allows for efficient inference using
massive collections of text documents. We demonstrate our algorithm on 1.8
million documents from The New York Times and 3.3 million documents from
Wikipedia.


Ancestor Sampling for Particle Gibbs

  We present a novel method in the family of particle MCMC methods that we
refer to as particle Gibbs with ancestor sampling (PG-AS). Similarly to the
existing PG with backward simulation (PG-BS) procedure, we use backward
sampling to (considerably) improve the mixing of the PG kernel. Instead of
using separate forward and backward sweeps as in PG-BS, however, we achieve the
same effect in a single forward sweep. We apply the PG-AS framework to the
challenging class of non-Markovian state-space models. We develop a truncation
strategy of these models that is applicable in principle to any
backward-simulation-based method, but which is particularly well suited to the
PG-AS framework. In particular, as we show in a simulation study, PG-AS can
yield an order-of-magnitude improved accuracy relative to PG-BS due to its
robustness to the truncation error. Several application examples are discussed,
including Rao-Blackwellized particle smoothing and inference in degenerate
state-space models.


Computational and Statistical Tradeoffs via Convex Relaxation

  In modern data analysis, one is frequently faced with statistical inference
problems involving massive datasets. Processing such large datasets is usually
viewed as a substantial computational challenge. However, if data are a
statistician's main resource then access to more data should be viewed as an
asset rather than as a burden. In this paper we describe a computational
framework based on convex relaxation to reduce the computational complexity of
an inference procedure when one has access to increasingly larger datasets.
Convex relaxation techniques have been widely used in theoretical computer
science as they give tractable approximation algorithms to many computationally
intractable tasks. We demonstrate the efficacy of this methodology in
statistical estimation in providing concrete time-data tradeoffs in a class of
denoising problems. Thus, convex relaxation offers a principled approach to
exploit the statistical gains from larger datasets to reduce the runtime of
inference algorithms.


