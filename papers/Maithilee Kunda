The Toybox Dataset of Egocentric Visual Object Transformations

  In object recognition research, many commonly used datasets (e.g., ImageNet
and similar) contain relatively sparse distributions of object instances and
views, e.g., one might see a thousand different pictures of a thousand
different giraffes, mostly taken from a few conventionally photographed angles.
These distributional properties constrain the types of computational
experiments that are able to be conducted with such datasets, and also do not
reflect naturalistic patterns of embodied visual experience. As a contribution
to the small (but growing) number of multi-view object datasets that have been
created to bridge this gap, we introduce a new video dataset called Toybox that
contains egocentric (i.e., first-person perspective) videos of common household
objects and toys being manually manipulated to undergo structured
transformations, such as rotation, translation, and zooming. To illustrate
potential uses of Toybox, we also present initial neural network experiments
that examine 1) how training on different distributions of object instances and
views affects recognition performance, and 2) how viewpoint-dependent object
concepts are represented within the hidden layers of a trained network.


Quantifying Human Behavior on the Block Design Test Through Automated
  Multi-Level Analysis of Overhead Video

  The block design test is a standardized, widely used neuropsychological
assessment of visuospatial reasoning that involves a person recreating a series
of given designs out of a set of colored blocks. In current testing procedures,
an expert neuropsychologist observes a person's accuracy and completion time as
well as overall impressions of the person's problem-solving procedures, errors,
etc., thus obtaining a holistic though subjective and often qualitative view of
the person's cognitive processes. We propose a new framework that combines room
sensors and AI techniques to augment the information available to
neuropsychologists from block design and similar tabletop assessments. In
particular, a ceiling-mounted camera captures an overhead view of the table
surface. From this video, we demonstrate how automated classification using
machine learning can produce a frame-level description of the state of the
block task and the person's actions over the course of each test problem. We
also show how a sequence-comparison algorithm can classify one individual's
problem-solving strategy relative to a database of simulated strategies, and
how these quantitative results can be visualized for use by neuropsychologists.


