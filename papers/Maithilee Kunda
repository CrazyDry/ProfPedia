The Toybox Dataset of Egocentric Visual Object Transformations

  In object recognition research, many commonly used datasets (e.g., ImageNetand similar) contain relatively sparse distributions of object instances andviews, e.g., one might see a thousand different pictures of a thousanddifferent giraffes, mostly taken from a few conventionally photographed angles.These distributional properties constrain the types of computationalexperiments that are able to be conducted with such datasets, and also do notreflect naturalistic patterns of embodied visual experience. As a contributionto the small (but growing) number of multi-view object datasets that have beencreated to bridge this gap, we introduce a new video dataset called Toybox thatcontains egocentric (i.e., first-person perspective) videos of common householdobjects and toys being manually manipulated to undergo structuredtransformations, such as rotation, translation, and zooming. To illustratepotential uses of Toybox, we also present initial neural network experimentsthat examine 1) how training on different distributions of object instances andviews affects recognition performance, and 2) how viewpoint-dependent objectconcepts are represented within the hidden layers of a trained network.

Quantifying Human Behavior on the Block Design Test Through Automated  Multi-Level Analysis of Overhead Video

  The block design test is a standardized, widely used neuropsychologicalassessment of visuospatial reasoning that involves a person recreating a seriesof given designs out of a set of colored blocks. In current testing procedures,an expert neuropsychologist observes a person's accuracy and completion time aswell as overall impressions of the person's problem-solving procedures, errors,etc., thus obtaining a holistic though subjective and often qualitative view ofthe person's cognitive processes. We propose a new framework that combines roomsensors and AI techniques to augment the information available toneuropsychologists from block design and similar tabletop assessments. Inparticular, a ceiling-mounted camera captures an overhead view of the tablesurface. From this video, we demonstrate how automated classification usingmachine learning can produce a frame-level description of the state of theblock task and the person's actions over the course of each test problem. Wealso show how a sequence-comparison algorithm can classify one individual'sproblem-solving strategy relative to a database of simulated strategies, andhow these quantitative results can be visualized for use by neuropsychologists.

