UPSET and ANGRI : Breaking High Performance Image Classifiers

  In this paper, targeted fooling of high performance image classifiers is
achieved by developing two novel attack methods. The first method generates
universal perturbations for target classes and the second generates image
specific perturbations. Extensive experiments are conducted on MNIST and
CIFAR10 datasets to provide insights about the proposed algorithms and show
their effectiveness.


Information-theoretic Dictionary Learning for Image Classification

  We present a two-stage approach for learning dictionaries for object
classification tasks based on the principle of information maximization. The
proposed method seeks a dictionary that is compact, discriminative, and
generative. In the first stage, dictionary atoms are selected from an initial
dictionary by maximizing the mutual information measure on dictionary
compactness, discrimination and reconstruction. In the second stage, the
selected dictionary atoms are updated for improved reconstructive and
discriminative power using a simple gradient ascent algorithm on mutual
information. Experiments using real datasets demonstrate the effectiveness of
our approach for image classification tasks.


Triplet Similarity Embedding for Face Verification

  In this work, we present an unconstrained face verification algorithm and
evaluate it on the recently released IJB-A dataset that aims to push the
boundaries of face verification methods. The proposed algorithm couples a deep
CNN-based approach with a low-dimensional discriminative embedding learnt using
triplet similarity constraints in a large margin fashion. Aside from yielding
performance improvement, this embedding provides significant advantages in
terms of memory and post-processing operations like hashing and visualization.
Experiments on the IJB-A dataset show that the proposed algorithm outperforms
state of the art methods in verification and identification metrics, while
requiring less training time.


Deep Feature-based Face Detection on Mobile Devices

  We propose a deep feature-based face detector for mobile devices to detect
user's face acquired by the front facing camera. The proposed method is able to
detect faces in images containing extreme pose and illumination variations as
well as partial faces. The main challenge in developing deep feature-based
algorithms for mobile devices is the constrained nature of the mobile platform
and the non-availability of CUDA enabled GPUs on such devices. Our
implementation takes into account the special nature of the images captured by
the front-facing camera of mobile devices and exploits the GPUs present in
mobile devices without CUDA-based frameorks, to meet these challenges.


Unconstrained Face Verification using Deep CNN Features

  In this paper, we present an algorithm for unconstrained face verification
based on deep convolutional features and evaluate it on the newly released
IARPA Janus Benchmark A (IJB-A) dataset. The IJB-A dataset includes real-world
unconstrained faces from 500 subjects with full pose and illumination
variations which are much harder than the traditional Labeled Face in the Wild
(LFW) and Youtube Face (YTF) datasets. The deep convolutional neural network
(DCNN) is trained using the CASIA-WebFace dataset. Extensive experiments on the
IJB-A dataset are provided.


A Deep Pyramid Deformable Part Model for Face Detection

  We present a face detection algorithm based on Deformable Part Models and
deep pyramidal features. The proposed method called DP2MFD is able to detect
faces of various sizes and poses in unconstrained conditions. It reduces the
gap in training and testing of DPM on deep features by adding a normalization
layer to the deep convolutional neural network (CNN). Extensive experiments on
four publicly available unconstrained face detection datasets show that our
method is able to capture the meaningful structure of faces and performs
significantly better than many competitive face detection algorithms.


An All-In-One Convolutional Neural Network for Face Analysis

  We present a multi-purpose algorithm for simultaneous face detection, face
alignment, pose estimation, gender recognition, smile detection, age estimation
and face recognition using a single deep convolutional neural network (CNN).
The proposed method employs a multi-task learning framework that regularizes
the shared parameters of CNN and builds a synergy among different domains and
tasks. Extensive experiments show that the network has a better understanding
of face and achieves state-of-the-art result for most of these tasks.


Sparse Dictionary-based Attributes for Action Recognition and
  Summarization

  We present an approach for dictionary learning of action attributes via
information maximization. We unify the class distribution and appearance
information into an objective function for learning a sparse dictionary of
action attributes. The objective function maximizes the mutual information
between what has been learned and what remains to be learned in terms of
appearance information and class distribution for each dictionary atom. We
propose a Gaussian Process (GP) model for sparse representation to optimize the
dictionary objective function. The sparse coding property allows a kernel with
compact support in GP to realize a very efficient dictionary learning process.
Hence we can describe an action video by a set of compact and discriminative
action attributes. More importantly, we can recognize modeled action categories
in a sparse feature space, which can be generalized to unseen and unmodeled
action categories. Experimental results demonstrate the effectiveness of our
approach in action recognition and summarization.


A Unified Approach for Modeling and Recognition of Individual Actions
  and Group Activities

  Recognizing group activities is challenging due to the difficulties in
isolating individual entities, finding the respective roles played by the
individuals and representing the complex interactions among the participants.
Individual actions and group activities in videos can be represented in a
common framework as they share the following common feature: both are composed
of a set of low-level features describing motions, e.g., optical flow for each
pixel or a trajectory for each feature point, according to a set of composition
constraints in both temporal and spatial dimensions. In this paper, we present
a unified model to assess the similarity between two given individual or group
activities. Our approach avoids explicit extraction of individual actors,
identifying and representing the inter-person interactions. With the proposed
approach, retrieval from a video database can be performed through
Query-by-Example; and activities can be recognized by querying videos
containing known activities. The suggested video matching process can be
performed in an unsupervised manner. We demonstrate the performance of our
approach by recognizing a set of human actions and football plays.


Adaptive-Rate Compressive Sensing Using Side Information

  We provide two novel adaptive-rate compressive sensing (CS) strategies for
sparse, time-varying signals using side information. Our first method utilizes
extra cross-validation measurements, and the second one exploits extra
low-resolution measurements. Unlike the majority of current CS techniques, we
do not assume that we know an upper bound on the number of significant
coefficients that comprise the images in the video sequence. Instead, we use
the side information to predict the number of significant coefficients in the
signal at the next time instant. For each image in the video sequence, our
techniques specify a fixed number of spatially-multiplexed CS measurements to
acquire, and adjust this quantity from image to image. Our strategies are
developed in the specific context of background subtraction for surveillance
video, and we experimentally validate the proposed methods on real video
sequences.


MKL-RT: Multiple Kernel Learning for Ratio-trace Problems via Convex
  Optimization

  In the recent past, automatic selection or combination of kernels (or
features) based on multiple kernel learning (MKL) approaches has been receiving
significant attention from various research communities. Though MKL has been
extensively studied in the context of support vector machines (SVM), it is
relatively less explored for ratio-trace problems. In this paper, we show that
MKL can be formulated as a convex optimization problem for a general class of
ratio-trace problems that encompasses many popular algorithms used in various
computer vision applications. We also provide an optimization procedure that is
guaranteed to converge to the global optimum of the proposed optimization
problem. We experimentally demonstrate that the proposed MKL approach, which we
refer to as MKL-RT, can be successfully used to select features for
discriminative dimensionality reduction and cross-modal retrieval. We also show
that the proposed convex MKL-RT approach performs better than the recently
proposed non-convex MKL-DR approach.


DCNNs on a Diet: Sampling Strategies for Reducing the Training Set Size

  Large-scale supervised classification algorithms, especially those based on
deep convolutional neural networks (DCNNs), require vast amounts of training
data to achieve state-of-the-art performance. Decreasing this data requirement
would significantly speed up the training process and possibly improve
generalization. Motivated by this objective, we consider the task of adaptively
finding concise training subsets which will be iteratively presented to the
learner. We use convex optimization methods, based on an objective criterion
and feedback from the current performance of the classifier, to efficiently
identify informative samples to train on. We propose an algorithm to decompose
the optimization problem into smaller per-class problems, which can be solved
in parallel. We test our approach on standard classification tasks and
demonstrate its effectiveness in decreasing the training set size without
compromising performance. We also show that our approach can make the
classifier more robust in the presence of label noise and class imbalance.


Face Alignment by Local Deep Descriptor Regression

  We present an algorithm for extracting key-point descriptors using deep
convolutional neural networks (CNN). Unlike many existing deep CNNs, our model
computes local features around a given point in an image. We also present a
face alignment algorithm based on regression using these local descriptors. The
proposed method called Local Deep Descriptor Regression (LDDR) is able to
localize face landmarks of varying sizes, poses and occlusions with high
accuracy. Deep Descriptors presented in this paper are able to uniquely and
efficiently describe every pixel in the image and therefore can potentially
replace traditional descriptors such as SIFT and HOG. Extensive evaluations on
five publicly available unconstrained face alignment datasets show that our
deep descriptor network is able to capture strong local features around a given
landmark and performs significantly better than many competitive and
state-of-the-art face alignment algorithms.


HyperFace: A Deep Multi-task Learning Framework for Face Detection,
  Landmark Localization, Pose Estimation, and Gender Recognition

  We present an algorithm for simultaneous face detection, landmarks
localization, pose estimation and gender recognition using deep convolutional
neural networks (CNN). The proposed method called, HyperFace, fuses the
intermediate layers of a deep CNN using a separate CNN followed by a multi-task
learning algorithm that operates on the fused features. It exploits the synergy
among the tasks which boosts up their individual performances. Additionally, we
propose two variants of HyperFace: (1) HyperFace-ResNet that builds on the
ResNet-101 model and achieves significant improvement in performance, and (2)
Fast-HyperFace that uses a high recall fast face detector for generating region
proposals to improve the speed of the algorithm. Extensive experiments show
that the proposed models are able to capture both global and local information
in faces and performs significantly better than many competitive algorithms for
each of these four tasks.


Partial Face Detection for Continuous Authentication

  In this paper, a part-based technique for real time detection of users' faces
on mobile devices is proposed. This method is specifically designed for
detecting partially cropped and occluded faces captured using a smartphone's
front-facing camera for continuous authentication. The key idea is to detect
facial segments in the frame and cluster the results to obtain the region which
is most likely to contain a face. Extensive experimentation on a mobile dataset
of 50 users shows that our method performs better than many state-of-the-art
face detection methods in terms of accuracy and processing speed.


Generate To Adapt: Aligning Domains using Generative Adversarial
  Networks

  Domain Adaptation is an actively researched problem in Computer Vision. In
this work, we propose an approach that leverages unsupervised data to bring the
source and target distributions closer in a learned joint feature space. We
accomplish this by inducing a symbiotic relationship between the learned
embedding and a generative adversarial network. This is in contrast to methods
which use the adversarial framework for realistic data generation and
retraining deep models with such data. We demonstrate the strength and
generality of our approach by performing experiments on three different tasks
with varying levels of difficulty: (1) Digit classification (MNIST, SVHN and
USPS datasets) (2) Object recognition using OFFICE dataset and (3) Domain
adaptation from synthetic to real data. Our method achieves state-of-the art
performance in most experimental settings and by far the only GAN-based method
that has been shown to work well across different datasets such as OFFICE and
DIGITS.


A Convolution Tree with Deconvolution Branches: Exploiting Geometric
  Relationships for Single Shot Keypoint Detection

  Recently, Deep Convolution Networks (DCNNs) have been applied to the task of
face alignment and have shown potential for learning improved feature
representations. Although deeper layers can capture abstract concepts like
pose, it is difficult to capture the geometric relationships among the
keypoints in DCNNs. In this paper, we propose a novel convolution-deconvolution
network for facial keypoint detection. Our model predicts the 2D locations of
the keypoints and their individual visibility along with 3D head pose, while
exploiting the spatial relationships among different keypoints. Different from
existing approaches of modeling these relationships, we propose learnable
transform functions which captures the relationships between keypoints at
feature level. However, due to extensive variations in pose, not all of these
relationships act at once, and hence we propose, a pose-based routing function
which implicitly models the active relationships. Both transform functions and
the routing function are implemented through convolutions in a multi-task
framework. Our approach presents a single-shot keypoint detection method,
making it different from many existing cascade regression-based methods. We
also show that learning these relationships significantly improve the accuracy
of keypoint detections for in-the-wild face images from challenging datasets
such as AFW and AFLW.


Growing Regression Forests by Classification: Applications to Object
  Pose Estimation

  In this work, we propose a novel node splitting method for regression trees
and incorporate it into the regression forest framework. Unlike traditional
binary splitting, where the splitting rule is selected from a predefined set of
binary splitting rules via trial-and-error, the proposed node splitting method
first finds clusters of the training data which at least locally minimize the
empirical loss without considering the input space. Then splitting rules which
preserve the found clusters as much as possible are determined by casting the
problem into a classification problem. Consequently, our new node splitting
method enjoys more freedom in choosing the splitting rules, resulting in more
efficient tree structures. In addition to the Euclidean target space, we
present a variant which can naturally deal with a circular target space by the
proper use of circular statistics. We apply the regression forest employing our
node splitting to head pose estimation (Euclidean target space) and car
direction estimation (circular target space) and demonstrate that the proposed
method significantly outperforms state-of-the-art methods (38.5% and 22.5%
error reduction respectively).


Deep Multi-task Learning for Railway Track Inspection

  Railroad tracks need to be periodically inspected and monitored to ensure
safe transportation. Automated track inspection using computer vision and
pattern recognition methods have recently shown the potential to improve safety
by allowing for more frequent inspections while reducing human errors.
Achieving full automation is still very challenging due to the number of
different possible failure modes as well as the broad range of image variations
that can potentially trigger false alarms. Also, the number of defective
components is very small, so not many training examples are available for the
machine to learn a robust anomaly detector. In this paper, we show that
detection performance can be improved by combining multiple detectors within a
multi-task learning framework. We show that this approach results in better
accuracy in detecting defects on railway ties and fasteners.


Triplet Probabilistic Embedding for Face Verification and Clustering

  Despite significant progress made over the past twenty five years,
unconstrained face verification remains a challenging problem. This paper
proposes an approach that couples a deep CNN-based approach with a
low-dimensional discriminative embedding learned using triplet probability
constraints to solve the unconstrained face verification problem. Aside from
yielding performance improvements, this embedding provides significant
advantages in terms of memory and for post-processing operations like subject
specific clustering. Experiments on the challenging IJB-A dataset show that the
proposed algorithm performs comparably or better than the state of the art
methods in verification and identification metrics, while requiring much less
training data and training time. The superior performance of the proposed
method on the CFP dataset shows that the representation learned by our deep CNN
is robust to extreme pose variation. Furthermore, we demonstrate the robustness
of the deep features to challenges including age, pose, blur and clutter by
performing simple clustering experiments on both IJB-A and LFW datasets.


Attributes for Improved Attributes: A Multi-Task Network for Attribute
  Classification

  Attributes, or semantic features, have gained popularity in the past few
years in domains ranging from activity recognition in video to face
verification. Improving the accuracy of attribute classifiers is an important
first step in any application which uses these attributes. In most works to
date, attributes have been considered to be independent. However, we know this
not to be the case. Many attributes are very strongly related, such as heavy
makeup and wearing lipstick. We propose to take advantage of attribute
relationships in three ways: by using a multi-task deep convolutional neural
network (MCNN) sharing the lowest layers amongst all attributes, sharing the
higher layers for related attributes, and by building an auxiliary network on
top of the MCNN which utilizes the scores from all attributes to improve the
final classification of each attribute. We demonstrate the effectiveness of our
method by producing results on two challenging publicly available datasets.


Convolutional Neural Networks for Attribute-based Active Authentication
  on Mobile Devices

  We present a Deep Convolutional Neural Network (DCNN) architecture for the
task of continuous authentication on mobile devices. To deal with the limited
resources of these devices, we reduce the complexity of the networks by
learning intermediate features such as gender and hair color instead of
identities. We present a multi-task, part-based DCNN architecture for attribute
detection that performs better than the state-of-the-art methods in terms of
accuracy. As a byproduct of the proposed architecture, we are able to explore
the embedding space of the attributes extracted from different facial parts,
such as mouth and eyes, to discover new attributes. Furthermore, through
extensive experimentation, we show that the attribute features extracted by our
method outperform the previously presented attribute-based method and a
baseline LBP method for the task of active authentication. Lastly, we
demonstrate the effectiveness of the proposed architecture in terms of speed
and power consumption by deploying it on an actual mobile device.


FaceNet2ExpNet: Regularizing a Deep Face Recognition Net for Expression
  Recognition

  Relatively small data sets available for expression recognition research make
the training of deep networks for expression recognition very challenging.
Although fine-tuning can partially alleviate the issue, the performance is
still below acceptable levels as the deep features probably contain redun- dant
information from the pre-trained domain. In this paper, we present
FaceNet2ExpNet, a novel idea to train an expression recognition network based
on static images. We first propose a new distribution function to model the
high-level neurons of the expression network. Based on this, a two-stage
training algorithm is carefully designed. In the pre-training stage, we train
the convolutional layers of the expression net, regularized by the face net; In
the refining stage, we append fully- connected layers to the pre-trained
convolutional layers and train the whole network jointly. Visualization shows
that the model trained with our method captures improved high-level expression
semantics. Evaluations on four public expression databases, CK+, Oulu-CASIA,
TFD, and SFEW demonstrate that our method achieves better results than
state-of-the-art.


Active User Authentication for Smartphones: A Challenge Data Set and
  Benchmark Results

  In this paper, automated user verification techniques for smartphones are
investigated. A unique non-commercial dataset, the University of Maryland
Active Authentication Dataset 02 (UMDAA-02) for multi-modal user authentication
research is introduced. This paper focuses on three sensors - front camera,
touch sensor and location service while providing a general description for
other modalities. Benchmark results for face detection, face verification,
touch-based user identification and location-based next-place prediction are
presented, which indicate that more robust methods fine-tuned to the mobile
platform are needed to achieve satisfactory verification accuracy. The dataset
will be made available to the research community for promoting additional
research.


PATH: Person Authentication using Trace Histories

  In this paper, a solution to the problem of Active Authentication using trace
histories is addressed. Specifically, the task is to perform user verification
on mobile devices using historical location traces of the user as a function of
time. Considering the movement of a human as a Markovian motion, a modified
Hidden Markov Model (HMM)-based solution is proposed. The proposed method,
namely the Marginally Smoothed HMM (MSHMM), utilizes the marginal probabilities
of location and timing information of the observations to smooth-out the
emission probabilities while training. Hence, it can efficiently handle
unforeseen observations during the test phase. The verification performance of
this method is compared to a sequence matching (SM) method , a Markov
Chain-based method (MC) and an HMM with basic Laplace Smoothing (HMM-lap).
Experimental results using the location information of the UMD Active
Authentication Dataset-02 (UMDAA02) and the GeoLife dataset are presented. The
proposed MSHMM method outperforms the compared methods in terms of equal error
rate (EER). Additionally, the effects of different parameters on the proposed
method are discussed.


UMDFaces: An Annotated Face Dataset for Training Deep Networks

  Recent progress in face detection (including keypoint detection), and
recognition is mainly being driven by (i) deeper convolutional neural network
architectures, and (ii) larger datasets. However, most of the large datasets
are maintained by private companies and are not publicly available. The
academic computer vision community needs larger and more varied datasets to
make further progress.
  In this paper we introduce a new face dataset, called UMDFaces, which has
367,888 annotated faces of 8,277 subjects. We also introduce a new face
recognition evaluation protocol which will help advance the state-of-the-art in
this area. We discuss how a large dataset can be collected and annotated using
human annotators and deep networks. We provide human curated bounding boxes for
faces. We also provide estimated pose (roll, pitch and yaw), locations of
twenty-one key-points and gender information generated by a pre-trained neural
network. In addition, the quality of keypoint annotations has been verified by
humans for about 115,000 images. Finally, we compare the quality of the dataset
with other publicly available face datasets at similar scales.


Designing Deep Convolutional Neural Networks for Continuous Object
  Orientation Estimation

  Deep Convolutional Neural Networks (DCNN) have been proven to be effective
for various computer vision problems. In this work, we demonstrate its
effectiveness on a continuous object orientation estimation task, which
requires prediction of 0 to 360 degrees orientation of the objects. We do so by
proposing and comparing three continuous orientation prediction approaches
designed for the DCNNs. The first two approaches work by representing an
orientation as a point on a unit circle and minimizing either L2 loss or
angular difference loss. The third method works by first converting the
continuous orientation estimation task into a set of discrete orientation
estimation tasks and then converting the discrete orientation outputs back to
the continuous orientation using a mean-shift algorithm. By evaluating on a
vehicle orientation estimation task and a pedestrian orientation estimation
task, we demonstrate that the discretization-based approach not only works
better than the other two approaches but also achieves state-of-the-art
performance. We also demonstrate that finding an appropriate feature
representation is critical to achieve a good performance when adapting a DCNN
trained for an image recognition task.


Learning from Ambiguously Labeled Face Images

  Learning a classifier from ambiguously labeled face images is challenging
since training images are not always explicitly-labeled. For instance, face
images of two persons in a news photo are not explicitly labeled by their names
in the caption. We propose a Matrix Completion for Ambiguity Resolution (MCar)
method for predicting the actual labels from ambiguously labeled images. This
step is followed by learning a standard supervised classifier from the
disambiguated labels to classify new images. To prevent the majority labels
from dominating the result of MCar, we generalize MCar to a weighted MCar
(WMCar) that handles label imbalance. Since WMCar outputs a soft labeling
vector of reduced ambiguity for each instance, we can iteratively refine it by
feeding it as the input to WMCar. Nevertheless, such an iterative
implementation can be affected by the noisy soft labeling vectors, and thus the
performance may degrade. Our proposed Iterative Candidate Elimination (ICE)
procedure makes the iterative ambiguity resolution possible by gradually
eliminating a portion of least likely candidates in ambiguously labeled face.
We further extend MCar to incorporate the labeling constraints between
instances when such prior knowledge is available. Compared to existing methods,
our approach demonstrates improvement on several ambiguously labeled datasets.


Deep Heterogeneous Feature Fusion for Template-Based Face Recognition

  Although deep learning has yielded impressive performance for face
recognition, many studies have shown that different networks learn different
feature maps: while some networks are more receptive to pose and illumination
others appear to capture more local information. Thus, in this work, we propose
a deep heterogeneous feature fusion network to exploit the complementary
information present in features generated by different deep convolutional
neural networks (DCNNs) for template-based face recognition, where a template
refers to a set of still face images or video frames from different sources
which introduces more blur, pose, illumination and other variations than
traditional face datasets. The proposed approach efficiently fuses the
discriminative information of different deep features by 1) jointly learning
the non-linear high-dimensional projection of the deep features and 2)
generating a more discriminative template representation which preserves the
inherent geometry of the deep features in the feature space. Experimental
results on the IARPA Janus Challenge Set 3 (Janus CS3) dataset demonstrate that
the proposed method can effectively improve the recognition performance. In
addition, we also present a series of covariate experiments on the face
verification task for in-depth qualitative evaluations for the proposed
approach.


A Proximity-Aware Hierarchical Clustering of Faces

  In this paper, we propose an unsupervised face clustering algorithm called
"Proximity-Aware Hierarchical Clustering" (PAHC) that exploits the local
structure of deep representations. In the proposed method, a similarity measure
between deep features is computed by evaluating linear SVM margins. SVMs are
trained using nearest neighbors of sample data, and thus do not require any
external training data. Clusters are then formed by thresholding the similarity
scores. We evaluate the clustering performance using three challenging
unconstrained face datasets, including Celebrity in Frontal-Profile (CFP),
IARPA JANUS Benchmark A (IJB-A), and JANUS Challenge Set 3 (JANUS CS3)
datasets. Experimental results demonstrate that the proposed approach can
achieve significant improvements over state-of-the-art methods. Moreover, we
also show that the proposed clustering algorithm can be applied to curate a set
of large-scale and noisy training dataset while maintaining sufficient amount
of images and their variations due to nuisance factors. The face verification
performance on JANUS CS3 improves significantly by finetuning a DCNN model with
the curated MS-Celeb-1M dataset which contains over three million face images.


L2-constrained Softmax Loss for Discriminative Face Verification

  In recent years, the performance of face verification systems has
significantly improved using deep convolutional neural networks (DCNNs). A
typical pipeline for face verification includes training a deep network for
subject classification with softmax loss, using the penultimate layer output as
the feature descriptor, and generating a cosine similarity score given a pair
of face images. The softmax loss function does not optimize the features to
have higher similarity score for positive pairs and lower similarity score for
negative pairs, which leads to a performance gap. In this paper, we add an
L2-constraint to the feature descriptors which restricts them to lie on a
hypersphere of a fixed radius. This module can be easily implemented using
existing deep learning frameworks. We show that integrating this simple step in
the training pipeline significantly boosts the performance of face
verification. Specifically, we achieve state-of-the-art results on the
challenging IJB-A dataset, achieving True Accept Rate of 0.909 at False Accept
Rate 0.0001 on the face verification protocol. Additionally, we achieve
state-of-the-art performance on LFW dataset with an accuracy of 99.78%, and
competing performance on YTF dataset with accuracy of 96.08%.


The Do's and Don'ts for CNN-based Face Verification

  While the research community appears to have developed a consensus on the
methods of acquiring annotated data, design and training of CNNs, many
questions still remain to be answered. In this paper, we explore the following
questions that are critical to face recognition research: (i) Can we train on
still images and expect the systems to work on videos? (ii) Are deeper datasets
better than wider datasets? (iii) Does adding label noise lead to improvement
in performance of deep networks? (iv) Is alignment needed for face recognition?
We address these questions by training CNNs using CASIA-WebFace, UMDFaces, and
a new video dataset and testing on YouTube- Faces, IJB-A and a disjoint portion
of UMDFaces datasets. Our new data set, which will be made publicly available,
has 22,075 videos and 3,735,476 human annotated frames extracted from them.


Regularizing deep networks using efficient layerwise adversarial
  training

  Adversarial training has been shown to regularize deep neural networks in
addition to increasing their robustness to adversarial examples. However, its
impact on very deep state of the art networks has not been fully investigated.
In this paper, we present an efficient approach to perform adversarial training
by perturbing intermediate layer activations and study the use of such
perturbations as a regularizer during training. We use these perturbations to
train very deep models such as ResNets and show improvement in performance both
on adversarial and original test data. Our experiments highlight the benefits
of perturbing intermediate layer activations compared to perturbing only the
inputs. The results on CIFAR-10 and CIFAR-100 datasets show the merits of the
proposed adversarial training approach. Additional results on WideResNets show
that our approach provides significant improvement in classification accuracy
for a given base model, outperforming dropout and other base models of larger
size.


Synthesis-based Robust Low Resolution Face Recognition

  Recognition of low resolution face images is a challenging problem in many
practical face recognition systems. Methods have been proposed in the face
recognition literature for the problem which assume that the probe is low
resolution, but a high resolution gallery is available for recognition. These
attempts have been aimed at modifying the probe image such that the resultant
image provides better discrimination. We formulate the problem differently by
leveraging the information available in the high resolution gallery image and
propose a dictionary learning approach for classifying the low-resolution probe
image. An important feature of our algorithm is that it can handle resolution
change along with illumination variations. Furthermore, we also kernelize the
algorithm to handle non-linearity in data and present a joint dictionary
learning technique for robust recognition at low resolutions. The effectiveness
of the proposed method is demonstrated using standard datasets and a
challenging outdoor face dataset. It is shown that our method is efficient and
can perform significantly better than many competitive low resolution face
recognition algorithms.


ExprGAN: Facial Expression Editing with Controllable Expression
  Intensity

  Facial expression editing is a challenging task as it needs a high-level
semantic understanding of the input face image. In conventional methods, either
paired training data is required or the synthetic face resolution is low.
Moreover, only the categories of facial expression can be changed. To address
these limitations, we propose an Expression Generative Adversarial Network
(ExprGAN) for photo-realistic facial expression editing with controllable
expression intensity. An expression controller module is specially designed to
learn an expressive and compact expression code in addition to the
encoder-decoder network. This novel architecture enables the expression
intensity to be continuously adjusted from low to high. We further show that
our ExprGAN can be applied for other tasks, such as expression transfer, image
retrieval, and data augmentation for training improved face expression
recognition models. To tackle the small size of the training database, an
effective incremental learning scheme is proposed. Quantitative and qualitative
evaluations on the widely used Oulu-CASIA dataset demonstrate the effectiveness
of ExprGAN.


A Deep Cascade Network for Unaligned Face Attribute Classification

  Humans focus attention on different face regions when recognizing face
attributes. Most existing face attribute classification methods use the whole
image as input. Moreover, some of these methods rely on fiducial landmarks to
provide defined face parts. In this paper, we propose a cascade network that
simultaneously learns to localize face regions specific to attributes and
performs attribute classification without alignment. First, a weakly-supervised
face region localization network is designed to automatically detect regions
(or parts) specific to attributes. Then multiple part-based networks and a
whole-image-based network are separately constructed and combined together by
the region switch layer and attribute relation layer for final attribute
classification. A multi-net learning method and hint-based model compression is
further proposed to get an effective localization model and a compact
classification model, respectively. Our approach achieves significantly better
performance than state-of-the-art methods on unaligned CelebA dataset, reducing
the classification error by 30.9%.


Learning from Synthetic Data: Addressing Domain Shift for Semantic
  Segmentation

  Visual Domain Adaptation is a problem of immense importance in computer
vision. Previous approaches showcase the inability of even deep neural networks
to learn informative representations across domain shift. This problem is more
severe for tasks where acquiring hand labeled data is extremely hard and
tedious. In this work, we focus on adapting the representations learned by
segmentation networks across synthetic and real domains. Contrary to previous
approaches that use a simple adversarial objective or superpixel information to
aid the process, we propose an approach based on Generative Adversarial
Networks (GANs) that brings the embeddings closer in the learned feature space.
To showcase the generality and scalability of our approach, we show that we can
achieve state of the art results on two challenging scenarios of synthetic to
real domain adaptation. Additional exploratory experiments show that our
approach: (1) generalizes to unseen domains and (2) results in improved
alignment of source and target distributions.


Improving Network Robustness against Adversarial Attacks with Compact
  Convolution

  Though Convolutional Neural Networks (CNNs) have surpassed human-level
performance on tasks such as object classification and face verification, they
can easily be fooled by adversarial attacks. These attacks add a small
perturbation to the input image that causes the network to misclassify the
sample. In this paper, we focus on neutralizing adversarial attacks by compact
feature learning. In particular, we show that learning features in a closed and
bounded space improves the robustness of the network. We explore the effect of
L2-Softmax Loss, that enforces compactness in the learned features, thus
resulting in enhanced robustness to adversarial perturbations. Additionally, we
propose compact convolution, a novel method of convolution that when
incorporated in conventional CNNs improves their robustness. Compact
convolution ensures feature compactness at every layer such that they are
bounded and close to each other. Extensive experiments show that Compact
Convolutional Networks (CCNs) neutralize multiple types of attacks, and perform
better than existing methods in defending adversarial attacks, without
incurring any additional training overhead compared to CNNs.


Segment-based Methods for Facial Attribute Detection from Partial Faces

  State-of-the-art methods of attribute detection from faces almost always
assume the presence of a full, unoccluded face. Hence, their performance
degrades for partially visible and occluded faces. In this paper, we introduce
SPLITFACE, a deep convolutional neural network-based method that is explicitly
designed to perform attribute detection in partially occluded faces. Taking
several facial segments and the full face as input, the proposed method takes a
data driven approach to determine which attributes are localized in which
facial segments. The unique architecture of the network allows each attribute
to be predicted by multiple segments, which permits the implementation of
committee machine techniques for combining local and global decisions to boost
performance. With access to segment-based predictions, SPLITFACE can predict
well those attributes which are localized in the visible parts of the face,
without having to rely on the presence of the whole face. We use the CelebA and
LFWA facial attribute datasets for standard evaluations. We also modify both
datasets, to occlude the faces, so that we can evaluate the performance of
attribute detection algorithms on partial faces. Our evaluation shows that
SPLITFACE significantly outperforms other recent methods especially for partial
faces.


Semi-supervised FusedGAN for Conditional Image Generation

  We present FusedGAN, a deep network for conditional image synthesis with
controllable sampling of diverse images. Fidelity, diversity and controllable
sampling are the main quality measures of a good image generation model. Most
existing models are insufficient in all three aspects. The FusedGAN can perform
controllable sampling of diverse images with very high fidelity. We argue that
controllability can be achieved by disentangling the generation process into
various stages. In contrast to stacked GANs, where multiple stages of GANs are
trained separately with full supervision of labeled intermediate images, the
FusedGAN has a single stage pipeline with a built-in stacking of GANs. Unlike
existing methods, which requires full supervision with paired conditions and
images, the FusedGAN can effectively leverage more abundant images without
corresponding conditions in training, to produce more diverse samples with high
fidelity. We achieve this by fusing two generators: one for unconditional image
generation, and the other for conditional image generation, where the two
partly share a common latent space thereby disentangling the generation. We
demonstrate the efficacy of the FusedGAN in fine grained image generation tasks
such as text-to-image, and attribute-to-face generation.


From BoW to CNN: Two Decades of Texture Representation for Texture
  Classification

  Texture is a fundamental characteristic of many types of images, and texture
representation is one of the essential and challenging problems in computer
vision and pattern recognition which has attracted extensive research
attention. Since 2000, texture representations based on Bag of Words (BoW) and
on Convolutional Neural Networks (CNNs) have been extensively studied with
impressive performance. Given this period of remarkable evolution, this paper
aims to present a comprehensive survey of advances in texture representation
over the last two decades. More than 200 major publications are cited in this
survey covering different aspects of the research, which includes (i) problem
description; (ii) recent advances in the broad categories of BoW-based,
CNN-based and attribute-based methods; and (iii) evaluation issues,
specifically benchmark datasets and state of the art results. In retrospect of
what has been achieved so far, the survey discusses open challenges and
directions for future research.


Task-Aware Compressed Sensing with Generative Adversarial Networks

  In recent years, neural network approaches have been widely adopted for
machine learning tasks, with applications in computer vision. More recently,
unsupervised generative models based on neural networks have been successfully
applied to model data distributions via low-dimensional latent spaces. In this
paper, we use Generative Adversarial Networks (GANs) to impose structure in
compressed sensing problems, replacing the usual sparsity constraint. We
propose to train the GANs in a task-aware fashion, specifically for
reconstruction tasks. We also show that it is possible to train our model
without using any (or much) non-compressed data. Finally, we show that the
latent space of the GAN carries discriminative information and can further be
regularized to generate input features for general inference tasks. We
demonstrate the effectiveness of our method on a variety of reconstruction and
classification problems.


Face-MagNet: Magnifying Feature Maps to Detect Small Faces

  In this paper, we introduce the Face Magnifier Network (Face-MageNet), a face
detector based on the Faster-RCNN framework which enables the flow of
discriminative information of small scale faces to the classifier without any
skip or residual connections. To achieve this, Face-MagNet deploys a set of
ConvTranspose, also known as deconvolution, layers in the Region Proposal
Network (RPN) and another set before the Region of Interest (RoI) pooling layer
to facilitate detection of finer faces. In addition, we also design, train, and
evaluate three other well-tuned architectures that represent the conventional
solutions to the scale problem: context pooling, skip connections, and scale
partitioning. Each of these three networks achieves comparable results to the
state-of-the-art face detectors. With extensive experiments, we show that
Face-MagNet based on a VGG16 architecture achieves better results than the
recently proposed ResNet101-based HR method on the task of face detection on
WIDER dataset and also achieves similar results on the hard set as our other
method SSH.


Zero-Shot Object Detection

  We introduce and tackle the problem of zero-shot object detection (ZSD),
which aims to detect object classes which are not observed during training. We
work with a challenging set of object classes, not restricting ourselves to
similar and/or fine-grained categories as in prior works on zero-shot
classification. We present a principled approach by first adapting
visual-semantic embeddings for ZSD. We then discuss the problems associated
with selecting a background class and motivate two background-aware approaches
for learning robust detectors. One of these models uses a fixed background
class and the other is based on iterative latent assignments. We also outline
the challenge associated with using a limited number of training classes and
propose a solution based on dense sampling of the semantic label space using
auxiliary data with a large number of categories. We propose novel splits of
two standard detection datasets - MSCOCO and VisualGenome, and present
extensive empirical results in both the traditional and generalized zero-shot
settings to highlight the benefits of the proposed methods. We provide useful
insights into the algorithm and conclude by posing some open questions to
encourage further research.


Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using
  Generative Models

  In recent years, deep neural network approaches have been widely adopted for
machine learning tasks, including classification. However, they were shown to
be vulnerable to adversarial perturbations: carefully crafted small
perturbations can cause misclassification of legitimate images. We propose
Defense-GAN, a new framework leveraging the expressive capability of generative
models to defend deep neural networks against such attacks. Defense-GAN is
trained to model the distribution of unperturbed images. At inference time, it
finds a close output to a given image which does not contain the adversarial
changes. This output is then fed to the classifier. Our proposed method can be
used with any classification model and does not modify the classifier structure
or training procedure. It can also be used as a defense against any attack as
it does not assume knowledge of the process for generating the adversarial
examples. We empirically show that Defense-GAN is consistently effective
against different attack methods and improves on existing defense strategies.
Our code has been made publicly available at
https://github.com/kabkabm/defensegan


Soft Sampling for Robust Object Detection

  We study the robustness of object detection under the presence of missing
annotations. In this setting, the unlabeled object instances will be treated as
background, which will generate an incorrect training signal for the detector.
Interestingly, we observe that after dropping 30% of the annotations (and
labeling them as background), the performance of CNN-based object detectors
like Faster-RCNN only drops by 5% on the PASCAL VOC dataset. We provide a
detailed explanation for this result. To further bridge the performance gap, we
propose a simple yet effective solution, called Soft Sampling. Soft Sampling
re-weights the gradients of RoIs as a function of overlap with positive
instances. This ensures that the uncertain background regions are given a
smaller weight compared to the hardnegatives. Extensive experiments on curated
PASCAL VOC datasets demonstrate the effectiveness of the proposed Soft Sampling
method at different annotation drop rates. Finally, we show that on
OpenImagesV3, which is a real-world dataset with missing annotations, Soft
Sampling outperforms standard detection baselines by over 3%.


Entropic GANs meet VAEs: A Statistical Approach to Compute Sample
  Likelihoods in GANs

  Building on the success of deep learning, two modern approaches to learn a
probability model of the observed data are Generative Adversarial Networks
(GANs) and Variational AutoEncoders (VAEs). VAEs consider an explicit
probability model for the data and compute a generative distribution by
maximizing a variational lower-bound on the log-likelihood function. GANs,
however, compute a generative model by minimizing a distance between observed
and generated probability distributions without considering an explicit model
for the observed data. The lack of having explicit probability models in GANs
prohibits computation of sample likelihoods in their frameworks and limits
their use in statistical inference problems. In this work, we show that an
optimal transport GAN with the entropy regularization can be viewed as a
generative model that maximizes a lower-bound on average sample likelihoods, an
approach that VAEs are based on. In particular, our proof constructs an
explicit probability model for GANs that can be used to compute likelihood
statistics within GAN's framework. Our numerical results on several datasets
demonstrate consistent trends with the proposed theory.


Learning without Memorizing

  Incremental learning (IL) is an important task aimed to increase the
capability of a trained model, in terms of the number of classes recognizable
by the model. The key problem in this task is the requirement of storing data
(e.g. images) associated with existing classes, while training the classifier
to learn new classes. However, this is impractical as it increases the memory
requirement at every incremental step, which makes it impossible to implement
IL algorithms on the edge devices with limited memory. Hence, we propose a
novel approach, called "Learning without Memorizing (LwM)", to preserve the
information with respect to existing (base) classes, without storing any of
their data, while making the classifier progressively learn the new classes. In
LwM, we present an information preserving penalty: Attention Distillation Loss,
and demonstrate that penalizing the changes in classifiers' attention maps
helps to retain information of the base classes, as new classes are added. We
show that adding Attention Distillation Loss to the distillation loss which is
an existing information preserving loss consistently outperforms the
state-of-the-art performance in the iILSVRC-small and iCIFAR-100 datasets in
terms of the overall accuracy of base and incrementally learned classes.


A Proposal-Based Solution to Spatio-Temporal Action Detection in
  Untrimmed Videos

  Existing approaches for spatio-temporal action detection in videos are
limited by the spatial extent and temporal duration of the actions. In this
paper, we present a modular system for spatio-temporal action detection in
untrimmed security videos. We propose a two stage approach. The first stage
generates dense spatio-temporal proposals using hierarchical clustering and
temporal jittering techniques on frame-wise object detections. The second stage
is a Temporal Refinement I3D (TRI-3D) network that performs action
classification and temporal refinement on the generated proposals. The object
detection-based proposal generation step helps in detecting actions occurring
in a small spatial region of a video frame, while temporal jittering and
refinement helps in detecting actions of variable lengths. Experimental results
on the spatio-temporal action detection dataset - DIVA - show the effectiveness
of our system. For comparison, the performance of our system is also evaluated
on the THUMOS14 temporal action detection dataset.


Normalized Wasserstein Distance for Mixture Distributions with
  Applications in Adversarial Learning and Domain Adaptation

  Understanding proper distance measures between distributions is at the core
of several learning tasks such as generative models, domain adaptation,
clustering, etc. In this work, we focus on {\it mixture distributions} that
arise naturally in several application domains where the data contains
different sub-populations. For mixture distributions, established distance
measures such as the Wasserstein distance do not take into account imbalanced
mixture proportions. Thus, even if two mixture distributions have identical
mixture components but different mixture proportions, the Wasserstein distance
between them will be large. This often leads to undesired results in
distance-based learning methods for mixture distributions. In this paper, we
resolve this issue by introducing {\it Normalized Wasserstein} distance. The
key idea is to introduce mixture proportions as optimization variables,
effectively normalizing mixture proportions in the Wasserstein formulation.
Using the proposed normalized Wasserstein distance, instead of the vanilla one,
leads to significant gains working with mixture distributions with imbalanced
mixture proportions. We demonstrate effectiveness of the proposed distance in
GANs, domain adaptation, adversarial clustering and hypothesis testing over
mixture of Gaussians, MNIST, CIFAR-10, CelebA and VISDA datasets.


