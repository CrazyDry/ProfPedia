UPSET and ANGRI : Breaking High Performance Image Classifiers

  In this paper, targeted fooling of high performance image classifiers isachieved by developing two novel attack methods. The first method generatesuniversal perturbations for target classes and the second generates imagespecific perturbations. Extensive experiments are conducted on MNIST andCIFAR10 datasets to provide insights about the proposed algorithms and showtheir effectiveness.

Information-theoretic Dictionary Learning for Image Classification

  We present a two-stage approach for learning dictionaries for objectclassification tasks based on the principle of information maximization. Theproposed method seeks a dictionary that is compact, discriminative, andgenerative. In the first stage, dictionary atoms are selected from an initialdictionary by maximizing the mutual information measure on dictionarycompactness, discrimination and reconstruction. In the second stage, theselected dictionary atoms are updated for improved reconstructive anddiscriminative power using a simple gradient ascent algorithm on mutualinformation. Experiments using real datasets demonstrate the effectiveness ofour approach for image classification tasks.

Unconstrained Face Verification using Deep CNN Features

  In this paper, we present an algorithm for unconstrained face verificationbased on deep convolutional features and evaluate it on the newly releasedIARPA Janus Benchmark A (IJB-A) dataset. The IJB-A dataset includes real-worldunconstrained faces from 500 subjects with full pose and illuminationvariations which are much harder than the traditional Labeled Face in the Wild(LFW) and Youtube Face (YTF) datasets. The deep convolutional neural network(DCNN) is trained using the CASIA-WebFace dataset. Extensive experiments on theIJB-A dataset are provided.

A Deep Pyramid Deformable Part Model for Face Detection

  We present a face detection algorithm based on Deformable Part Models anddeep pyramidal features. The proposed method called DP2MFD is able to detectfaces of various sizes and poses in unconstrained conditions. It reduces thegap in training and testing of DPM on deep features by adding a normalizationlayer to the deep convolutional neural network (CNN). Extensive experiments onfour publicly available unconstrained face detection datasets show that ourmethod is able to capture the meaningful structure of faces and performssignificantly better than many competitive face detection algorithms.

Triplet Similarity Embedding for Face Verification

  In this work, we present an unconstrained face verification algorithm andevaluate it on the recently released IJB-A dataset that aims to push theboundaries of face verification methods. The proposed algorithm couples a deepCNN-based approach with a low-dimensional discriminative embedding learnt usingtriplet similarity constraints in a large margin fashion. Aside from yieldingperformance improvement, this embedding provides significant advantages interms of memory and post-processing operations like hashing and visualization.Experiments on the IJB-A dataset show that the proposed algorithm outperformsstate of the art methods in verification and identification metrics, whilerequiring less training time.

Deep Feature-based Face Detection on Mobile Devices

  We propose a deep feature-based face detector for mobile devices to detectuser's face acquired by the front facing camera. The proposed method is able todetect faces in images containing extreme pose and illumination variations aswell as partial faces. The main challenge in developing deep feature-basedalgorithms for mobile devices is the constrained nature of the mobile platformand the non-availability of CUDA enabled GPUs on such devices. Ourimplementation takes into account the special nature of the images captured bythe front-facing camera of mobile devices and exploits the GPUs present inmobile devices without CUDA-based frameorks, to meet these challenges.

An All-In-One Convolutional Neural Network for Face Analysis

  We present a multi-purpose algorithm for simultaneous face detection, facealignment, pose estimation, gender recognition, smile detection, age estimationand face recognition using a single deep convolutional neural network (CNN).The proposed method employs a multi-task learning framework that regularizesthe shared parameters of CNN and builds a synergy among different domains andtasks. Extensive experiments show that the network has a better understandingof face and achieves state-of-the-art result for most of these tasks.

A Unified Approach for Modeling and Recognition of Individual Actions  and Group Activities

  Recognizing group activities is challenging due to the difficulties inisolating individual entities, finding the respective roles played by theindividuals and representing the complex interactions among the participants.Individual actions and group activities in videos can be represented in acommon framework as they share the following common feature: both are composedof a set of low-level features describing motions, e.g., optical flow for eachpixel or a trajectory for each feature point, according to a set of compositionconstraints in both temporal and spatial dimensions. In this paper, we presenta unified model to assess the similarity between two given individual or groupactivities. Our approach avoids explicit extraction of individual actors,identifying and representing the inter-person interactions. With the proposedapproach, retrieval from a video database can be performed throughQuery-by-Example; and activities can be recognized by querying videoscontaining known activities. The suggested video matching process can beperformed in an unsupervised manner. We demonstrate the performance of ourapproach by recognizing a set of human actions and football plays.

Sparse Dictionary-based Attributes for Action Recognition and  Summarization

  We present an approach for dictionary learning of action attributes viainformation maximization. We unify the class distribution and appearanceinformation into an objective function for learning a sparse dictionary ofaction attributes. The objective function maximizes the mutual informationbetween what has been learned and what remains to be learned in terms ofappearance information and class distribution for each dictionary atom. Wepropose a Gaussian Process (GP) model for sparse representation to optimize thedictionary objective function. The sparse coding property allows a kernel withcompact support in GP to realize a very efficient dictionary learning process.Hence we can describe an action video by a set of compact and discriminativeaction attributes. More importantly, we can recognize modeled action categoriesin a sparse feature space, which can be generalized to unseen and unmodeledaction categories. Experimental results demonstrate the effectiveness of ourapproach in action recognition and summarization.

Growing Regression Forests by Classification: Applications to Object  Pose Estimation

  In this work, we propose a novel node splitting method for regression treesand incorporate it into the regression forest framework. Unlike traditionalbinary splitting, where the splitting rule is selected from a predefined set ofbinary splitting rules via trial-and-error, the proposed node splitting methodfirst finds clusters of the training data which at least locally minimize theempirical loss without considering the input space. Then splitting rules whichpreserve the found clusters as much as possible are determined by casting theproblem into a classification problem. Consequently, our new node splittingmethod enjoys more freedom in choosing the splitting rules, resulting in moreefficient tree structures. In addition to the Euclidean target space, wepresent a variant which can naturally deal with a circular target space by theproper use of circular statistics. We apply the regression forest employing ournode splitting to head pose estimation (Euclidean target space) and cardirection estimation (circular target space) and demonstrate that the proposedmethod significantly outperforms state-of-the-art methods (38.5% and 22.5%error reduction respectively).

Adaptive-Rate Compressive Sensing Using Side Information

  We provide two novel adaptive-rate compressive sensing (CS) strategies forsparse, time-varying signals using side information. Our first method utilizesextra cross-validation measurements, and the second one exploits extralow-resolution measurements. Unlike the majority of current CS techniques, wedo not assume that we know an upper bound on the number of significantcoefficients that comprise the images in the video sequence. Instead, we usethe side information to predict the number of significant coefficients in thesignal at the next time instant. For each image in the video sequence, ourtechniques specify a fixed number of spatially-multiplexed CS measurements toacquire, and adjust this quantity from image to image. Our strategies aredeveloped in the specific context of background subtraction for surveillancevideo, and we experimentally validate the proposed methods on real videosequences.

MKL-RT: Multiple Kernel Learning for Ratio-trace Problems via Convex  Optimization

  In the recent past, automatic selection or combination of kernels (orfeatures) based on multiple kernel learning (MKL) approaches has been receivingsignificant attention from various research communities. Though MKL has beenextensively studied in the context of support vector machines (SVM), it isrelatively less explored for ratio-trace problems. In this paper, we show thatMKL can be formulated as a convex optimization problem for a general class ofratio-trace problems that encompasses many popular algorithms used in variouscomputer vision applications. We also provide an optimization procedure that isguaranteed to converge to the global optimum of the proposed optimizationproblem. We experimentally demonstrate that the proposed MKL approach, which werefer to as MKL-RT, can be successfully used to select features fordiscriminative dimensionality reduction and cross-modal retrieval. We also showthat the proposed convex MKL-RT approach performs better than the recentlyproposed non-convex MKL-DR approach.

Deep Multi-task Learning for Railway Track Inspection

  Railroad tracks need to be periodically inspected and monitored to ensuresafe transportation. Automated track inspection using computer vision andpattern recognition methods have recently shown the potential to improve safetyby allowing for more frequent inspections while reducing human errors.Achieving full automation is still very challenging due to the number ofdifferent possible failure modes as well as the broad range of image variationsthat can potentially trigger false alarms. Also, the number of defectivecomponents is very small, so not many training examples are available for themachine to learn a robust anomaly detector. In this paper, we show thatdetection performance can be improved by combining multiple detectors within amulti-task learning framework. We show that this approach results in betteraccuracy in detecting defects on railway ties and fasteners.

Face Alignment by Local Deep Descriptor Regression

  We present an algorithm for extracting key-point descriptors using deepconvolutional neural networks (CNN). Unlike many existing deep CNNs, our modelcomputes local features around a given point in an image. We also present aface alignment algorithm based on regression using these local descriptors. Theproposed method called Local Deep Descriptor Regression (LDDR) is able tolocalize face landmarks of varying sizes, poses and occlusions with highaccuracy. Deep Descriptors presented in this paper are able to uniquely andefficiently describe every pixel in the image and therefore can potentiallyreplace traditional descriptors such as SIFT and HOG. Extensive evaluations onfive publicly available unconstrained face alignment datasets show that ourdeep descriptor network is able to capture strong local features around a givenlandmark and performs significantly better than many competitive andstate-of-the-art face alignment algorithms.

HyperFace: A Deep Multi-task Learning Framework for Face Detection,  Landmark Localization, Pose Estimation, and Gender Recognition

  We present an algorithm for simultaneous face detection, landmarkslocalization, pose estimation and gender recognition using deep convolutionalneural networks (CNN). The proposed method called, HyperFace, fuses theintermediate layers of a deep CNN using a separate CNN followed by a multi-tasklearning algorithm that operates on the fused features. It exploits the synergyamong the tasks which boosts up their individual performances. Additionally, wepropose two variants of HyperFace: (1) HyperFace-ResNet that builds on theResNet-101 model and achieves significant improvement in performance, and (2)Fast-HyperFace that uses a high recall fast face detector for generating regionproposals to improve the speed of the algorithm. Extensive experiments showthat the proposed models are able to capture both global and local informationin faces and performs significantly better than many competitive algorithms foreach of these four tasks.

Partial Face Detection for Continuous Authentication

  In this paper, a part-based technique for real time detection of users' faceson mobile devices is proposed. This method is specifically designed fordetecting partially cropped and occluded faces captured using a smartphone'sfront-facing camera for continuous authentication. The key idea is to detectfacial segments in the frame and cluster the results to obtain the region whichis most likely to contain a face. Extensive experimentation on a mobile datasetof 50 users shows that our method performs better than many state-of-the-artface detection methods in terms of accuracy and processing speed.

Triplet Probabilistic Embedding for Face Verification and Clustering

  Despite significant progress made over the past twenty five years,unconstrained face verification remains a challenging problem. This paperproposes an approach that couples a deep CNN-based approach with alow-dimensional discriminative embedding learned using triplet probabilityconstraints to solve the unconstrained face verification problem. Aside fromyielding performance improvements, this embedding provides significantadvantages in terms of memory and for post-processing operations like subjectspecific clustering. Experiments on the challenging IJB-A dataset show that theproposed algorithm performs comparably or better than the state of the artmethods in verification and identification metrics, while requiring much lesstraining data and training time. The superior performance of the proposedmethod on the CFP dataset shows that the representation learned by our deep CNNis robust to extreme pose variation. Furthermore, we demonstrate the robustnessof the deep features to challenges including age, pose, blur and clutter byperforming simple clustering experiments on both IJB-A and LFW datasets.

Attributes for Improved Attributes: A Multi-Task Network for Attribute  Classification

  Attributes, or semantic features, have gained popularity in the past fewyears in domains ranging from activity recognition in video to faceverification. Improving the accuracy of attribute classifiers is an importantfirst step in any application which uses these attributes. In most works todate, attributes have been considered to be independent. However, we know thisnot to be the case. Many attributes are very strongly related, such as heavymakeup and wearing lipstick. We propose to take advantage of attributerelationships in three ways: by using a multi-task deep convolutional neuralnetwork (MCNN) sharing the lowest layers amongst all attributes, sharing thehigher layers for related attributes, and by building an auxiliary network ontop of the MCNN which utilizes the scores from all attributes to improve thefinal classification of each attribute. We demonstrate the effectiveness of ourmethod by producing results on two challenging publicly available datasets.

Convolutional Neural Networks for Attribute-based Active Authentication  on Mobile Devices

  We present a Deep Convolutional Neural Network (DCNN) architecture for thetask of continuous authentication on mobile devices. To deal with the limitedresources of these devices, we reduce the complexity of the networks bylearning intermediate features such as gender and hair color instead ofidentities. We present a multi-task, part-based DCNN architecture for attributedetection that performs better than the state-of-the-art methods in terms ofaccuracy. As a byproduct of the proposed architecture, we are able to explorethe embedding space of the attributes extracted from different facial parts,such as mouth and eyes, to discover new attributes. Furthermore, throughextensive experimentation, we show that the attribute features extracted by ourmethod outperform the previously presented attribute-based method and abaseline LBP method for the task of active authentication. Lastly, wedemonstrate the effectiveness of the proposed architecture in terms of speedand power consumption by deploying it on an actual mobile device.

DCNNs on a Diet: Sampling Strategies for Reducing the Training Set Size

  Large-scale supervised classification algorithms, especially those based ondeep convolutional neural networks (DCNNs), require vast amounts of trainingdata to achieve state-of-the-art performance. Decreasing this data requirementwould significantly speed up the training process and possibly improvegeneralization. Motivated by this objective, we consider the task of adaptivelyfinding concise training subsets which will be iteratively presented to thelearner. We use convex optimization methods, based on an objective criterionand feedback from the current performance of the classifier, to efficientlyidentify informative samples to train on. We propose an algorithm to decomposethe optimization problem into smaller per-class problems, which can be solvedin parallel. We test our approach on standard classification tasks anddemonstrate its effectiveness in decreasing the training set size withoutcompromising performance. We also show that our approach can make theclassifier more robust in the presence of label noise and class imbalance.

FaceNet2ExpNet: Regularizing a Deep Face Recognition Net for Expression  Recognition

  Relatively small data sets available for expression recognition research makethe training of deep networks for expression recognition very challenging.Although fine-tuning can partially alleviate the issue, the performance isstill below acceptable levels as the deep features probably contain redun- dantinformation from the pre-trained domain. In this paper, we presentFaceNet2ExpNet, a novel idea to train an expression recognition network basedon static images. We first propose a new distribution function to model thehigh-level neurons of the expression network. Based on this, a two-stagetraining algorithm is carefully designed. In the pre-training stage, we trainthe convolutional layers of the expression net, regularized by the face net; Inthe refining stage, we append fully- connected layers to the pre-trainedconvolutional layers and train the whole network jointly. Visualization showsthat the model trained with our method captures improved high-level expressionsemantics. Evaluations on four public expression databases, CK+, Oulu-CASIA,TFD, and SFEW demonstrate that our method achieves better results thanstate-of-the-art.

Active User Authentication for Smartphones: A Challenge Data Set and  Benchmark Results

  In this paper, automated user verification techniques for smartphones areinvestigated. A unique non-commercial dataset, the University of MarylandActive Authentication Dataset 02 (UMDAA-02) for multi-modal user authenticationresearch is introduced. This paper focuses on three sensors - front camera,touch sensor and location service while providing a general description forother modalities. Benchmark results for face detection, face verification,touch-based user identification and location-based next-place prediction arepresented, which indicate that more robust methods fine-tuned to the mobileplatform are needed to achieve satisfactory verification accuracy. The datasetwill be made available to the research community for promoting additionalresearch.

PATH: Person Authentication using Trace Histories

  In this paper, a solution to the problem of Active Authentication using tracehistories is addressed. Specifically, the task is to perform user verificationon mobile devices using historical location traces of the user as a function oftime. Considering the movement of a human as a Markovian motion, a modifiedHidden Markov Model (HMM)-based solution is proposed. The proposed method,namely the Marginally Smoothed HMM (MSHMM), utilizes the marginal probabilitiesof location and timing information of the observations to smooth-out theemission probabilities while training. Hence, it can efficiently handleunforeseen observations during the test phase. The verification performance ofthis method is compared to a sequence matching (SM) method , a MarkovChain-based method (MC) and an HMM with basic Laplace Smoothing (HMM-lap).Experimental results using the location information of the UMD ActiveAuthentication Dataset-02 (UMDAA02) and the GeoLife dataset are presented. Theproposed MSHMM method outperforms the compared methods in terms of equal errorrate (EER). Additionally, the effects of different parameters on the proposedmethod are discussed.

UMDFaces: An Annotated Face Dataset for Training Deep Networks

  Recent progress in face detection (including keypoint detection), andrecognition is mainly being driven by (i) deeper convolutional neural networkarchitectures, and (ii) larger datasets. However, most of the large datasetsare maintained by private companies and are not publicly available. Theacademic computer vision community needs larger and more varied datasets tomake further progress.  In this paper we introduce a new face dataset, called UMDFaces, which has367,888 annotated faces of 8,277 subjects. We also introduce a new facerecognition evaluation protocol which will help advance the state-of-the-art inthis area. We discuss how a large dataset can be collected and annotated usinghuman annotators and deep networks. We provide human curated bounding boxes forfaces. We also provide estimated pose (roll, pitch and yaw), locations oftwenty-one key-points and gender information generated by a pre-trained neuralnetwork. In addition, the quality of keypoint annotations has been verified byhumans for about 115,000 images. Finally, we compare the quality of the datasetwith other publicly available face datasets at similar scales.

Designing Deep Convolutional Neural Networks for Continuous Object  Orientation Estimation

  Deep Convolutional Neural Networks (DCNN) have been proven to be effectivefor various computer vision problems. In this work, we demonstrate itseffectiveness on a continuous object orientation estimation task, whichrequires prediction of 0 to 360 degrees orientation of the objects. We do so byproposing and comparing three continuous orientation prediction approachesdesigned for the DCNNs. The first two approaches work by representing anorientation as a point on a unit circle and minimizing either L2 loss orangular difference loss. The third method works by first converting thecontinuous orientation estimation task into a set of discrete orientationestimation tasks and then converting the discrete orientation outputs back tothe continuous orientation using a mean-shift algorithm. By evaluating on avehicle orientation estimation task and a pedestrian orientation estimationtask, we demonstrate that the discretization-based approach not only worksbetter than the other two approaches but also achieves state-of-the-artperformance. We also demonstrate that finding an appropriate featurerepresentation is critical to achieve a good performance when adapting a DCNNtrained for an image recognition task.

Learning from Ambiguously Labeled Face Images

  Learning a classifier from ambiguously labeled face images is challengingsince training images are not always explicitly-labeled. For instance, faceimages of two persons in a news photo are not explicitly labeled by their namesin the caption. We propose a Matrix Completion for Ambiguity Resolution (MCar)method for predicting the actual labels from ambiguously labeled images. Thisstep is followed by learning a standard supervised classifier from thedisambiguated labels to classify new images. To prevent the majority labelsfrom dominating the result of MCar, we generalize MCar to a weighted MCar(WMCar) that handles label imbalance. Since WMCar outputs a soft labelingvector of reduced ambiguity for each instance, we can iteratively refine it byfeeding it as the input to WMCar. Nevertheless, such an iterativeimplementation can be affected by the noisy soft labeling vectors, and thus theperformance may degrade. Our proposed Iterative Candidate Elimination (ICE)procedure makes the iterative ambiguity resolution possible by graduallyeliminating a portion of least likely candidates in ambiguously labeled face.We further extend MCar to incorporate the labeling constraints betweeninstances when such prior knowledge is available. Compared to existing methods,our approach demonstrates improvement on several ambiguously labeled datasets.

Deep Heterogeneous Feature Fusion for Template-Based Face Recognition

  Although deep learning has yielded impressive performance for facerecognition, many studies have shown that different networks learn differentfeature maps: while some networks are more receptive to pose and illuminationothers appear to capture more local information. Thus, in this work, we proposea deep heterogeneous feature fusion network to exploit the complementaryinformation present in features generated by different deep convolutionalneural networks (DCNNs) for template-based face recognition, where a templaterefers to a set of still face images or video frames from different sourceswhich introduces more blur, pose, illumination and other variations thantraditional face datasets. The proposed approach efficiently fuses thediscriminative information of different deep features by 1) jointly learningthe non-linear high-dimensional projection of the deep features and 2)generating a more discriminative template representation which preserves theinherent geometry of the deep features in the feature space. Experimentalresults on the IARPA Janus Challenge Set 3 (Janus CS3) dataset demonstrate thatthe proposed method can effectively improve the recognition performance. Inaddition, we also present a series of covariate experiments on the faceverification task for in-depth qualitative evaluations for the proposedapproach.

A Proximity-Aware Hierarchical Clustering of Faces

  In this paper, we propose an unsupervised face clustering algorithm called"Proximity-Aware Hierarchical Clustering" (PAHC) that exploits the localstructure of deep representations. In the proposed method, a similarity measurebetween deep features is computed by evaluating linear SVM margins. SVMs aretrained using nearest neighbors of sample data, and thus do not require anyexternal training data. Clusters are then formed by thresholding the similarityscores. We evaluate the clustering performance using three challengingunconstrained face datasets, including Celebrity in Frontal-Profile (CFP),IARPA JANUS Benchmark A (IJB-A), and JANUS Challenge Set 3 (JANUS CS3)datasets. Experimental results demonstrate that the proposed approach canachieve significant improvements over state-of-the-art methods. Moreover, wealso show that the proposed clustering algorithm can be applied to curate a setof large-scale and noisy training dataset while maintaining sufficient amountof images and their variations due to nuisance factors. The face verificationperformance on JANUS CS3 improves significantly by finetuning a DCNN model withthe curated MS-Celeb-1M dataset which contains over three million face images.

L2-constrained Softmax Loss for Discriminative Face Verification

  In recent years, the performance of face verification systems hassignificantly improved using deep convolutional neural networks (DCNNs). Atypical pipeline for face verification includes training a deep network forsubject classification with softmax loss, using the penultimate layer output asthe feature descriptor, and generating a cosine similarity score given a pairof face images. The softmax loss function does not optimize the features tohave higher similarity score for positive pairs and lower similarity score fornegative pairs, which leads to a performance gap. In this paper, we add anL2-constraint to the feature descriptors which restricts them to lie on ahypersphere of a fixed radius. This module can be easily implemented usingexisting deep learning frameworks. We show that integrating this simple step inthe training pipeline significantly boosts the performance of faceverification. Specifically, we achieve state-of-the-art results on thechallenging IJB-A dataset, achieving True Accept Rate of 0.909 at False AcceptRate 0.0001 on the face verification protocol. Additionally, we achievestate-of-the-art performance on LFW dataset with an accuracy of 99.78%, andcompeting performance on YTF dataset with accuracy of 96.08%.

Generate To Adapt: Aligning Domains using Generative Adversarial  Networks

  Domain Adaptation is an actively researched problem in Computer Vision. Inthis work, we propose an approach that leverages unsupervised data to bring thesource and target distributions closer in a learned joint feature space. Weaccomplish this by inducing a symbiotic relationship between the learnedembedding and a generative adversarial network. This is in contrast to methodswhich use the adversarial framework for realistic data generation andretraining deep models with such data. We demonstrate the strength andgenerality of our approach by performing experiments on three different taskswith varying levels of difficulty: (1) Digit classification (MNIST, SVHN andUSPS datasets) (2) Object recognition using OFFICE dataset and (3) Domainadaptation from synthetic to real data. Our method achieves state-of-the artperformance in most experimental settings and by far the only GAN-based methodthat has been shown to work well across different datasets such as OFFICE andDIGITS.

A Convolution Tree with Deconvolution Branches: Exploiting Geometric  Relationships for Single Shot Keypoint Detection

  Recently, Deep Convolution Networks (DCNNs) have been applied to the task offace alignment and have shown potential for learning improved featurerepresentations. Although deeper layers can capture abstract concepts likepose, it is difficult to capture the geometric relationships among thekeypoints in DCNNs. In this paper, we propose a novel convolution-deconvolutionnetwork for facial keypoint detection. Our model predicts the 2D locations ofthe keypoints and their individual visibility along with 3D head pose, whileexploiting the spatial relationships among different keypoints. Different fromexisting approaches of modeling these relationships, we propose learnabletransform functions which captures the relationships between keypoints atfeature level. However, due to extensive variations in pose, not all of theserelationships act at once, and hence we propose, a pose-based routing functionwhich implicitly models the active relationships. Both transform functions andthe routing function are implemented through convolutions in a multi-taskframework. Our approach presents a single-shot keypoint detection method,making it different from many existing cascade regression-based methods. Wealso show that learning these relationships significantly improve the accuracyof keypoint detections for in-the-wild face images from challenging datasetssuch as AFW and AFLW.

The Do's and Don'ts for CNN-based Face Verification

  While the research community appears to have developed a consensus on themethods of acquiring annotated data, design and training of CNNs, manyquestions still remain to be answered. In this paper, we explore the followingquestions that are critical to face recognition research: (i) Can we train onstill images and expect the systems to work on videos? (ii) Are deeper datasetsbetter than wider datasets? (iii) Does adding label noise lead to improvementin performance of deep networks? (iv) Is alignment needed for face recognition?We address these questions by training CNNs using CASIA-WebFace, UMDFaces, anda new video dataset and testing on YouTube- Faces, IJB-A and a disjoint portionof UMDFaces datasets. Our new data set, which will be made publicly available,has 22,075 videos and 3,735,476 human annotated frames extracted from them.

Regularizing deep networks using efficient layerwise adversarial  training

  Adversarial training has been shown to regularize deep neural networks inaddition to increasing their robustness to adversarial examples. However, itsimpact on very deep state of the art networks has not been fully investigated.In this paper, we present an efficient approach to perform adversarial trainingby perturbing intermediate layer activations and study the use of suchperturbations as a regularizer during training. We use these perturbations totrain very deep models such as ResNets and show improvement in performance bothon adversarial and original test data. Our experiments highlight the benefitsof perturbing intermediate layer activations compared to perturbing only theinputs. The results on CIFAR-10 and CIFAR-100 datasets show the merits of theproposed adversarial training approach. Additional results on WideResNets showthat our approach provides significant improvement in classification accuracyfor a given base model, outperforming dropout and other base models of largersize.

Synthesis-based Robust Low Resolution Face Recognition

  Recognition of low resolution face images is a challenging problem in manypractical face recognition systems. Methods have been proposed in the facerecognition literature for the problem which assume that the probe is lowresolution, but a high resolution gallery is available for recognition. Theseattempts have been aimed at modifying the probe image such that the resultantimage provides better discrimination. We formulate the problem differently byleveraging the information available in the high resolution gallery image andpropose a dictionary learning approach for classifying the low-resolution probeimage. An important feature of our algorithm is that it can handle resolutionchange along with illumination variations. Furthermore, we also kernelize thealgorithm to handle non-linearity in data and present a joint dictionarylearning technique for robust recognition at low resolutions. The effectivenessof the proposed method is demonstrated using standard datasets and achallenging outdoor face dataset. It is shown that our method is efficient andcan perform significantly better than many competitive low resolution facerecognition algorithms.

ExprGAN: Facial Expression Editing with Controllable Expression  Intensity

  Facial expression editing is a challenging task as it needs a high-levelsemantic understanding of the input face image. In conventional methods, eitherpaired training data is required or the synthetic face resolution is low.Moreover, only the categories of facial expression can be changed. To addressthese limitations, we propose an Expression Generative Adversarial Network(ExprGAN) for photo-realistic facial expression editing with controllableexpression intensity. An expression controller module is specially designed tolearn an expressive and compact expression code in addition to theencoder-decoder network. This novel architecture enables the expressionintensity to be continuously adjusted from low to high. We further show thatour ExprGAN can be applied for other tasks, such as expression transfer, imageretrieval, and data augmentation for training improved face expressionrecognition models. To tackle the small size of the training database, aneffective incremental learning scheme is proposed. Quantitative and qualitativeevaluations on the widely used Oulu-CASIA dataset demonstrate the effectivenessof ExprGAN.

A Deep Cascade Network for Unaligned Face Attribute Classification

  Humans focus attention on different face regions when recognizing faceattributes. Most existing face attribute classification methods use the wholeimage as input. Moreover, some of these methods rely on fiducial landmarks toprovide defined face parts. In this paper, we propose a cascade network thatsimultaneously learns to localize face regions specific to attributes andperforms attribute classification without alignment. First, a weakly-supervisedface region localization network is designed to automatically detect regions(or parts) specific to attributes. Then multiple part-based networks and awhole-image-based network are separately constructed and combined together bythe region switch layer and attribute relation layer for final attributeclassification. A multi-net learning method and hint-based model compression isfurther proposed to get an effective localization model and a compactclassification model, respectively. Our approach achieves significantly betterperformance than state-of-the-art methods on unaligned CelebA dataset, reducingthe classification error by 30.9%.

Learning from Synthetic Data: Addressing Domain Shift for Semantic  Segmentation

  Visual Domain Adaptation is a problem of immense importance in computervision. Previous approaches showcase the inability of even deep neural networksto learn informative representations across domain shift. This problem is moresevere for tasks where acquiring hand labeled data is extremely hard andtedious. In this work, we focus on adapting the representations learned bysegmentation networks across synthetic and real domains. Contrary to previousapproaches that use a simple adversarial objective or superpixel information toaid the process, we propose an approach based on Generative AdversarialNetworks (GANs) that brings the embeddings closer in the learned feature space.To showcase the generality and scalability of our approach, we show that we canachieve state of the art results on two challenging scenarios of synthetic toreal domain adaptation. Additional exploratory experiments show that ourapproach: (1) generalizes to unseen domains and (2) results in improvedalignment of source and target distributions.

Improving Network Robustness against Adversarial Attacks with Compact  Convolution

  Though Convolutional Neural Networks (CNNs) have surpassed human-levelperformance on tasks such as object classification and face verification, theycan easily be fooled by adversarial attacks. These attacks add a smallperturbation to the input image that causes the network to misclassify thesample. In this paper, we focus on neutralizing adversarial attacks by compactfeature learning. In particular, we show that learning features in a closed andbounded space improves the robustness of the network. We explore the effect ofL2-Softmax Loss, that enforces compactness in the learned features, thusresulting in enhanced robustness to adversarial perturbations. Additionally, wepropose compact convolution, a novel method of convolution that whenincorporated in conventional CNNs improves their robustness. Compactconvolution ensures feature compactness at every layer such that they arebounded and close to each other. Extensive experiments show that CompactConvolutional Networks (CCNs) neutralize multiple types of attacks, and performbetter than existing methods in defending adversarial attacks, withoutincurring any additional training overhead compared to CNNs.

Segment-based Methods for Facial Attribute Detection from Partial Faces

  State-of-the-art methods of attribute detection from faces almost alwaysassume the presence of a full, unoccluded face. Hence, their performancedegrades for partially visible and occluded faces. In this paper, we introduceSPLITFACE, a deep convolutional neural network-based method that is explicitlydesigned to perform attribute detection in partially occluded faces. Takingseveral facial segments and the full face as input, the proposed method takes adata driven approach to determine which attributes are localized in whichfacial segments. The unique architecture of the network allows each attributeto be predicted by multiple segments, which permits the implementation ofcommittee machine techniques for combining local and global decisions to boostperformance. With access to segment-based predictions, SPLITFACE can predictwell those attributes which are localized in the visible parts of the face,without having to rely on the presence of the whole face. We use the CelebA andLFWA facial attribute datasets for standard evaluations. We also modify bothdatasets, to occlude the faces, so that we can evaluate the performance ofattribute detection algorithms on partial faces. Our evaluation shows thatSPLITFACE significantly outperforms other recent methods especially for partialfaces.

Semi-supervised FusedGAN for Conditional Image Generation

  We present FusedGAN, a deep network for conditional image synthesis withcontrollable sampling of diverse images. Fidelity, diversity and controllablesampling are the main quality measures of a good image generation model. Mostexisting models are insufficient in all three aspects. The FusedGAN can performcontrollable sampling of diverse images with very high fidelity. We argue thatcontrollability can be achieved by disentangling the generation process intovarious stages. In contrast to stacked GANs, where multiple stages of GANs aretrained separately with full supervision of labeled intermediate images, theFusedGAN has a single stage pipeline with a built-in stacking of GANs. Unlikeexisting methods, which requires full supervision with paired conditions andimages, the FusedGAN can effectively leverage more abundant images withoutcorresponding conditions in training, to produce more diverse samples with highfidelity. We achieve this by fusing two generators: one for unconditional imagegeneration, and the other for conditional image generation, where the twopartly share a common latent space thereby disentangling the generation. Wedemonstrate the efficacy of the FusedGAN in fine grained image generation taskssuch as text-to-image, and attribute-to-face generation.

From BoW to CNN: Two Decades of Texture Representation for Texture  Classification

  Texture is a fundamental characteristic of many types of images, and texturerepresentation is one of the essential and challenging problems in computervision and pattern recognition which has attracted extensive researchattention. Since 2000, texture representations based on Bag of Words (BoW) andon Convolutional Neural Networks (CNNs) have been extensively studied withimpressive performance. Given this period of remarkable evolution, this paperaims to present a comprehensive survey of advances in texture representationover the last two decades. More than 200 major publications are cited in thissurvey covering different aspects of the research, which includes (i) problemdescription; (ii) recent advances in the broad categories of BoW-based,CNN-based and attribute-based methods; and (iii) evaluation issues,specifically benchmark datasets and state of the art results. In retrospect ofwhat has been achieved so far, the survey discusses open challenges anddirections for future research.

Task-Aware Compressed Sensing with Generative Adversarial Networks

  In recent years, neural network approaches have been widely adopted formachine learning tasks, with applications in computer vision. More recently,unsupervised generative models based on neural networks have been successfullyapplied to model data distributions via low-dimensional latent spaces. In thispaper, we use Generative Adversarial Networks (GANs) to impose structure incompressed sensing problems, replacing the usual sparsity constraint. Wepropose to train the GANs in a task-aware fashion, specifically forreconstruction tasks. We also show that it is possible to train our modelwithout using any (or much) non-compressed data. Finally, we show that thelatent space of the GAN carries discriminative information and can further beregularized to generate input features for general inference tasks. Wedemonstrate the effectiveness of our method on a variety of reconstruction andclassification problems.

Face-MagNet: Magnifying Feature Maps to Detect Small Faces

  In this paper, we introduce the Face Magnifier Network (Face-MageNet), a facedetector based on the Faster-RCNN framework which enables the flow ofdiscriminative information of small scale faces to the classifier without anyskip or residual connections. To achieve this, Face-MagNet deploys a set ofConvTranspose, also known as deconvolution, layers in the Region ProposalNetwork (RPN) and another set before the Region of Interest (RoI) pooling layerto facilitate detection of finer faces. In addition, we also design, train, andevaluate three other well-tuned architectures that represent the conventionalsolutions to the scale problem: context pooling, skip connections, and scalepartitioning. Each of these three networks achieves comparable results to thestate-of-the-art face detectors. With extensive experiments, we show thatFace-MagNet based on a VGG16 architecture achieves better results than therecently proposed ResNet101-based HR method on the task of face detection onWIDER dataset and also achieves similar results on the hard set as our othermethod SSH.

Zero-Shot Object Detection

  We introduce and tackle the problem of zero-shot object detection (ZSD),which aims to detect object classes which are not observed during training. Wework with a challenging set of object classes, not restricting ourselves tosimilar and/or fine-grained categories as in prior works on zero-shotclassification. We present a principled approach by first adaptingvisual-semantic embeddings for ZSD. We then discuss the problems associatedwith selecting a background class and motivate two background-aware approachesfor learning robust detectors. One of these models uses a fixed backgroundclass and the other is based on iterative latent assignments. We also outlinethe challenge associated with using a limited number of training classes andpropose a solution based on dense sampling of the semantic label space usingauxiliary data with a large number of categories. We propose novel splits oftwo standard detection datasets - MSCOCO and VisualGenome, and presentextensive empirical results in both the traditional and generalized zero-shotsettings to highlight the benefits of the proposed methods. We provide usefulinsights into the algorithm and conclude by posing some open questions toencourage further research.

Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using  Generative Models

  In recent years, deep neural network approaches have been widely adopted formachine learning tasks, including classification. However, they were shown tobe vulnerable to adversarial perturbations: carefully crafted smallperturbations can cause misclassification of legitimate images. We proposeDefense-GAN, a new framework leveraging the expressive capability of generativemodels to defend deep neural networks against such attacks. Defense-GAN istrained to model the distribution of unperturbed images. At inference time, itfinds a close output to a given image which does not contain the adversarialchanges. This output is then fed to the classifier. Our proposed method can beused with any classification model and does not modify the classifier structureor training procedure. It can also be used as a defense against any attack asit does not assume knowledge of the process for generating the adversarialexamples. We empirically show that Defense-GAN is consistently effectiveagainst different attack methods and improves on existing defense strategies.Our code has been made publicly available athttps://github.com/kabkabm/defensegan

Soft Sampling for Robust Object Detection

  We study the robustness of object detection under the presence of missingannotations. In this setting, the unlabeled object instances will be treated asbackground, which will generate an incorrect training signal for the detector.Interestingly, we observe that after dropping 30% of the annotations (andlabeling them as background), the performance of CNN-based object detectorslike Faster-RCNN only drops by 5% on the PASCAL VOC dataset. We provide adetailed explanation for this result. To further bridge the performance gap, wepropose a simple yet effective solution, called Soft Sampling. Soft Samplingre-weights the gradients of RoIs as a function of overlap with positiveinstances. This ensures that the uncertain background regions are given asmaller weight compared to the hardnegatives. Extensive experiments on curatedPASCAL VOC datasets demonstrate the effectiveness of the proposed Soft Samplingmethod at different annotation drop rates. Finally, we show that onOpenImagesV3, which is a real-world dataset with missing annotations, SoftSampling outperforms standard detection baselines by over 3%.

Entropic GANs meet VAEs: A Statistical Approach to Compute Sample  Likelihoods in GANs

  Building on the success of deep learning, two modern approaches to learn aprobability model of the observed data are Generative Adversarial Networks(GANs) and Variational AutoEncoders (VAEs). VAEs consider an explicitprobability model for the data and compute a generative distribution bymaximizing a variational lower-bound on the log-likelihood function. GANs,however, compute a generative model by minimizing a distance between observedand generated probability distributions without considering an explicit modelfor the observed data. The lack of having explicit probability models in GANsprohibits computation of sample likelihoods in their frameworks and limitstheir use in statistical inference problems. In this work, we show that anoptimal transport GAN with the entropy regularization can be viewed as agenerative model that maximizes a lower-bound on average sample likelihoods, anapproach that VAEs are based on. In particular, our proof constructs anexplicit probability model for GANs that can be used to compute likelihoodstatistics within GAN's framework. Our numerical results on several datasetsdemonstrate consistent trends with the proposed theory.

Learning without Memorizing

  Incremental learning (IL) is an important task aimed to increase thecapability of a trained model, in terms of the number of classes recognizableby the model. The key problem in this task is the requirement of storing data(e.g. images) associated with existing classes, while training the classifierto learn new classes. However, this is impractical as it increases the memoryrequirement at every incremental step, which makes it impossible to implementIL algorithms on the edge devices with limited memory. Hence, we propose anovel approach, called "Learning without Memorizing (LwM)", to preserve theinformation with respect to existing (base) classes, without storing any oftheir data, while making the classifier progressively learn the new classes. InLwM, we present an information preserving penalty: Attention Distillation Loss,and demonstrate that penalizing the changes in classifiers' attention mapshelps to retain information of the base classes, as new classes are added. Weshow that adding Attention Distillation Loss to the distillation loss which isan existing information preserving loss consistently outperforms thestate-of-the-art performance in the iILSVRC-small and iCIFAR-100 datasets interms of the overall accuracy of base and incrementally learned classes.

A Proposal-Based Solution to Spatio-Temporal Action Detection in  Untrimmed Videos

  Existing approaches for spatio-temporal action detection in videos arelimited by the spatial extent and temporal duration of the actions. In thispaper, we present a modular system for spatio-temporal action detection inuntrimmed security videos. We propose a two stage approach. The first stagegenerates dense spatio-temporal proposals using hierarchical clustering andtemporal jittering techniques on frame-wise object detections. The second stageis a Temporal Refinement I3D (TRI-3D) network that performs actionclassification and temporal refinement on the generated proposals. The objectdetection-based proposal generation step helps in detecting actions occurringin a small spatial region of a video frame, while temporal jittering andrefinement helps in detecting actions of variable lengths. Experimental resultson the spatio-temporal action detection dataset - DIVA - show the effectivenessof our system. For comparison, the performance of our system is also evaluatedon the THUMOS14 temporal action detection dataset.

Normalized Wasserstein Distance for Mixture Distributions with  Applications in Adversarial Learning and Domain Adaptation

  Understanding proper distance measures between distributions is at the coreof several learning tasks such as generative models, domain adaptation,clustering, etc. In this work, we focus on {\it mixture distributions} thatarise naturally in several application domains where the data containsdifferent sub-populations. For mixture distributions, established distancemeasures such as the Wasserstein distance do not take into account imbalancedmixture proportions. Thus, even if two mixture distributions have identicalmixture components but different mixture proportions, the Wasserstein distancebetween them will be large. This often leads to undesired results indistance-based learning methods for mixture distributions. In this paper, weresolve this issue by introducing {\it Normalized Wasserstein} distance. Thekey idea is to introduce mixture proportions as optimization variables,effectively normalizing mixture proportions in the Wasserstein formulation.Using the proposed normalized Wasserstein distance, instead of the vanilla one,leads to significant gains working with mixture distributions with imbalancedmixture proportions. We demonstrate effectiveness of the proposed distance inGANs, domain adaptation, adversarial clustering and hypothesis testing overmixture of Gaussians, MNIST, CIFAR-10, CelebA and VISDA datasets.

