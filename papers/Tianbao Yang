Multiple Kernel Learning from Noisy Labels by Stochastic Programming

  We study the problem of multiple kernel learning from noisy labels. This isin contrast to most of the previous studies on multiple kernel learning thatmainly focus on developing efficient algorithms and assume perfectly labeledtraining examples. Directly applying the existing multiple kernel learningalgorithms to noisily labeled examples often leads to suboptimal performancedue to the incorrect class assignments. We address this challenge by castingmultiple kernel learning from noisy labels into a stochastic programmingproblem, and presenting a minimax formulation. We develop an efficientalgorithm for solving the related convex-concave optimization problem with afast convergence rate of $O(1/T)$ where $T$ is the number of iterations.Empirical studies on UCI data sets verify both the effectiveness of theproposed framework and the efficiency of the proposed optimization algorithm.

Regret Bound by Variation for Online Convex Optimization

  In citep{Hazan-2008-extract}, the authors showed that the regret of onlinelinear optimization can be bounded by the total variation of the cost vectors.In this paper, we extend this result to general online convex optimization. Wefirst analyze the limitations of the algorithm in \citep{Hazan-2008-extract}when applied it to online convex optimization. We then present two algorithmsfor online convex optimization whose regrets are bounded by the variation ofcost functions. We finally consider the bandit setting, and present arandomized algorithm for online bandit convex optimization with avariation-based regret bound. We show that the regret bound for online banditconvex optimization is optimal when the variation of cost functions isindependent of the number of trials.

An Efficient Primal-Dual Prox Method for Non-Smooth Optimization

  We study the non-smooth optimization problems in machine learning, where boththe loss function and the regularizer are non-smooth functions. Previousstudies on efficient empirical loss minimization assume either a smooth lossfunction or a strongly convex regularizer, making them unsuitable fornon-smooth optimization. We develop a simple yet efficient method for a familyof non-smooth optimization problems where the dual form of the loss function isbilinear in primal and dual variables. We cast a non-smooth optimizationproblem into a minimax optimization problem, and develop a primal dual proxmethod that solves the minimax optimization problem at a rate of $O(1/T)${assuming that the proximal step can be efficiently solved}, significantlyfaster than a standard subgradient descent method that has an $O(1/\sqrt{T})$convergence rate. Our empirical study verifies the efficiency of the proposedmethod for various non-smooth optimization problems that arise ubiquitously inmachine learning by comparing it to the state-of-the-art first order methods.

A New Analysis of Compressive Sensing by Stochastic Proximal Gradient  Descent

  In this manuscript, we analyze the sparse signal recovery (compressivesensing) problem from the perspective of convex optimization by stochasticproximal gradient descent. This view allows us to significantly simplify therecovery analysis of compressive sensing. More importantly, it leads to anefficient optimization algorithm for solving the regularized optimizationproblem related to the sparse recovery problem. Compared to the existingapproaches, there are two advantages of the proposed algorithm. First, itenjoys a geometric convergence rate and therefore is computationally efficient.Second, it guarantees that the support set of any intermediate solutiongenerated by the proposed algorithm is concentrated on the support set of theoptimal solution.

Analysis of Distributed Stochastic Dual Coordinate Ascent

  In \citep{Yangnips13}, the author presented distributed stochastic dualcoordinate ascent (DisDCA) algorithms for solving large-scale regularized lossminimization. Extraordinary performances have been observed and reported forthe well-motivated updates, as referred to the practical updates, compared tothe naive updates. However, no serious analysis has been provided to understandthe updates and therefore the convergence rates. In the paper, we bridge thegap by providing a theoretical analysis of the convergence rates of thepractical DisDCA algorithm. Our analysis helped by empirical studies has shownthat it could yield an exponential speed-up in the convergence by increasingthe number of dual updates at each iteration. This result justifies thesuperior performances of the practical DisDCA as compared to the naive variant.As a byproduct, our analysis also reveals the convergence behavior of theone-communication DisDCA.

On Data Preconditioning for Regularized Loss Minimization

  In this work, we study data preconditioning, a well-known and long-existingtechnique, for boosting the convergence of first-order methods for regularizedloss minimization. It is well understood that the condition number of theproblem, i.e., the ratio of the Lipschitz constant to the strong convexitymodulus, has a harsh effect on the convergence of the first-order optimizationmethods. Therefore, minimizing a small regularized loss for achieving goodgeneralization performance, yielding an ill conditioned problem, becomes thebottleneck for big data problems. We provide a theory on data preconditioningfor regularized loss minimization. In particular, our analysis exhibits anappropriate data preconditioner and characterizes the conditions on the lossfunction and on the data under which data preconditioning can reduce thecondition number and therefore boost the convergence for minimizing theregularized loss. To make the data preconditioning practically useful, weendeavor to employ and analyze a random sampling approach to efficientlycompute the preconditioned data. The preliminary experiments validate ourtheory.

Theory of Dual-sparse Regularized Randomized Reduction

  In this paper, we study randomized reduction methods, which reducehigh-dimensional features into low-dimensional space by randomized methods(e.g., random projection, random hashing), for large-scale high-dimensionalclassification. Previous theoretical results on randomized reduction methodshinge on strong assumptions about the data, e.g., low rank of the data matrixor a large separable margin of classification, which hinder their applicationsin broad domains. To address these limitations, we propose dual-sparseregularized randomized reduction methods that introduce a sparse regularizerinto the reduced dual problem. Under a mild condition that the original dualsolution is a (nearly) sparse vector, we show that the resulting dual solutionis close to the original dual solution and concentrates on its support set. Innumerical experiments, we present an empirical study to support the analysisand we also present a novel application of the dual-sparse regularizedrandomized reduction methods to reducing the communication cost of distributedlearning from large-scale high-dimensional data.

An Explicit Sampling Dependent Spectral Error Bound for Column Subset  Selection

  In this paper, we consider the problem of column subset selection. We presenta novel analysis of the spectral norm reconstruction for a simple randomizedalgorithm and establish a new bound that depends explicitly on the samplingprobabilities. The sampling dependent error bound (i) allows us to betterunderstand the tradeoff in the reconstruction error due to samplingprobabilities, (ii) exhibits more insights than existing error bounds thatexploit specific probability distributions, and (iii) implies better samplingdistributions. In particular, we show that a sampling distribution withprobabilities proportional to the square root of the statistical leveragescores is always better than uniform sampling and is better than leverage-basedsampling when the statistical leverage scores are very nonuniform. And bysolving a constrained optimization problem related to the error bound with anefficient bisection search we are able to achieve better performance than usingeither the leverage-based distribution or that proportional to the square rootof the statistical leverage scores. Numerical simulations demonstrate thebenefits of the new sampling distributions for low-rank matrix approximationand least square approximation compared to state-of-the art algorithms.

Fast Sparse Least-Squares Regression with Non-Asymptotic Guarantees

  In this paper, we study a fast approximation method for {\it large-scalehigh-dimensional} sparse least-squares regression problem by exploiting theJohnson-Lindenstrauss (JL) transforms, which embed a set of high-dimensionalvectors into a low-dimensional space. In particular, we propose to apply the JLtransforms to the data matrix and the target vector and then to solve a sparseleast-squares problem on the compressed data with a {\it slightly largerregularization parameter}. Theoretically, we establish the optimization errorbound of the learned model for two different sparsity-inducing regularizers,i.e., the elastic net and the $\ell_1$ norm. Compared with previous relevantwork, our analysis is {\it non-asymptotic and exhibits more insights} on thebound, the sample complexity and the regularization. As an illustration, wealso provide an error bound of the {\it Dantzig selector} under JL transforms.

Stochastic subGradient Methods with Linear Convergence for Polyhedral  Convex Optimization

  In this paper, we show that simple {Stochastic} subGradient Decent methodswith multiple Restarting, named {\bf RSGD}, can achieve a \textit{linearconvergence rate} for a class of non-smooth and non-strongly convexoptimization problems where the epigraph of the objective function is apolyhedron, to which we refer as {\bf polyhedral convex optimization}. Itsapplications in machine learning include $\ell_1$ constrained or regularizedpiecewise linear loss minimization and submodular function minimization. To thebest of our knowledge, this is the first result on the linear convergence rateof stochastic subgradient methods for non-smooth and non-strongly convexoptimization problems.

A Simple Analysis for Exp-concave Empirical Minimization with Arbitrary  Convex Regularizer

  In this paper, we present a simple analysis of {\bf fast rates} with {\ithigh probability} of {\bf empirical minimization} for {\it stochastic compositeoptimization} over a finite-dimensional bounded convex set with exponentialconcave loss functions and an arbitrary convex regularization. To the best ofour knowledge, this result is the first of its kind. As a byproduct, we candirectly obtain the fast rate with {\it high probability} for exponentialconcave empirical risk minimization with and without any convex regularization,which not only extends existing results of empirical risk minimization but alsoprovides a unified framework for analyzing exponential concave empirical riskminimization with and without {\it any} convex regularization. Our proof isvery simple only exploiting the covering number of a finite-dimensional boundedset and a concentration inequality of random vectors.

Stochastic Non-convex Optimization with Strong High Probability  Second-order Convergence

  In this paper, we study stochastic non-convex optimization with non-convexrandom functions. Recent studies on non-convex optimization revolve aroundestablishing second-order convergence, i.e., converging to a nearlysecond-order optimal stationary points. However, existing results on stochasticnon-convex optimization are limited, especially with a high probabilitysecond-order convergence. We propose a novel updating step (named NCG-S) byleveraging a stochastic gradient and a noisy negative curvature of a stochasticHessian, where the stochastic gradient and Hessian are based on a propermini-batch of random functions. Building on this step, we develop twoalgorithms and establish their high probability second-order convergence. Tothe best of our knowledge, the proposed stochastic algorithms are the firstwith a second-order convergence in {\it high probability} and a time complexitythat is {\it almost linear} in the problem's dimensionality.

On the Convergence of (Stochastic) Gradient Descent with Extrapolation  for Non-Convex Optimization

  Extrapolation is a well-known technique for solving convex optimization andvariational inequalities and recently attracts some attention for non-convexoptimization. Several recent works have empirically shown its success in somemachine learning tasks. However, it has not been analyzed for non-convexminimization and there still remains a gap between the theory and the practice.In this paper, we analyze gradient descent and stochastic gradient descent withextrapolation for finding an approximate first-order stationary point in smoothnon-convex optimization problems. Our convergence upper bounds show that thealgorithms with extrapolation can be accelerated than without extrapolation.

Online Stochastic Optimization with Multiple Objectives

  In this paper we propose a general framework to characterize and solve thestochastic optimization problems with multiple objectives underlying many realworld learning applications. We first propose a projection based algorithmwhich attains an $O(T^{-1/3})$ convergence rate. Then, by leveraging on thetheory of Lagrangian in constrained optimization, we devise a novel primal-dualstochastic approximation algorithm which attains the optimal convergence rateof $O(T^{-1/2})$ for general Lipschitz continuous objectives.

Improved Bound for the Nystrom's Method and its Application to Kernel  Classification

  We develop two approaches for analyzing the approximation error bound for theNystr\"{o}m method, one based on the concentration inequality of integraloperator, and one based on the compressive sensing theory. We show that theapproximation error, measured in the spectral norm, can be improved from$O(N/\sqrt{m})$ to $O(N/m^{1 - \rho})$ in the case of large eigengap, where $N$is the total number of data points, $m$ is the number of sampled data points,and $\rho \in (0, 1/2)$ is a positive constant that characterizes the eigengap.When the eigenvalues of the kernel matrix follow a $p$-power law, our analysisbased on compressive sensing theory further improves the bound to $O(N/m^{p -1})$ under an incoherence assumption, which explains why the Nystr\"{o}m methodworks well for kernel matrix with skewed eigenvalues. We present a kernelclassification approach based on the Nystr\"{o}m method and derive itsgeneralization performance using the improved bound. We show that when theeigenvalues of kernel matrix follow a $p$-power law, we can reduce the numberof support vectors to $N^{2p/(p^2 - 1)}$, a number less than $N$ when $p >1+\sqrt{2}$, without seriously sacrificing its generalization performance.

A Bayesian Framework for Community Detection Integrating Content and  Link

  This paper addresses the problem of community detection in networked datathat combines link and content analysis. Most existing work combines link andcontent information by a generative model. There are two major shortcomingswith the existing approaches. First, they assume that the probability ofcreating a link between two nodes is determined only by the communitymemberships of the nodes; however other factors (e.g. popularity) could alsoaffect the link pattern. Second, they use generative models to model thecontent of individual nodes, whereas these generative models are vulnerable tothe content attributes that are irrelevant to communities. We propose aBayesian framework for combining link and content information for communitydetection that explicitly addresses these shortcomings. A new link model ispresented that introduces a random variable to capture the node popularity whendeciding the link between two nodes; a discriminative model is used todetermine the community membership of a node by its content. An approximateinference algorithm is presented for efficient Bayesian inference. Ourempirical study shows that the proposed framework outperforms severalstate-of-theart approaches in combining link and content information forcommunity detection.

Optimal Stochastic Strongly Convex Optimization with a Logarithmic  Number of Projections

  We consider stochastic strongly convex optimization with a complex inequalityconstraint. This complex inequality constraint may lead to computationallyexpensive projections in algorithmic iterations of the stochastic gradientdescent~(SGD) methods. To reduce the computation costs pertaining to theprojections, we propose an Epoch-Projection Stochastic GradientDescent~(Epro-SGD) method. The proposed Epro-SGD method consists of a sequenceof epochs; it applies SGD to an augmented objective function at each iterationwithin the epoch, and then performs a projection at the end of each epoch.Given a strongly convex optimization and for a total number of $T$ iterations,Epro-SGD requires only $\log(T)$ projections, and meanwhile attains an optimalconvergence rate of $O(1/T)$, both in expectation and with a high probability.To exploit the structure of the optimization problem, we propose a proximalvariant of Epro-SGD, namely Epro-ORDA, based on the optimal regularized dualaveraging method. We apply the proposed methods on real-world applications; theempirical results demonstrate the effectiveness of our methods.

A Simple Homotopy Proximal Mapping for Compressive Sensing

  In this paper, we present a novel yet simple homotopy proximal mappingalgorithm for compressive sensing. The algorithm adopts a simple proximalmapping of the $\ell_1$ norm at each iteration and gradually reduces theregularization parameter for the $\ell_1$ norm. We prove a global linearconvergence of the proposed homotopy proximal mapping (HPM) algorithm forsolving compressive sensing under three different settings (i) sparse signalrecovery under noiseless measurements, (ii) sparse signal recovery under noisymeasurements, and (iii) nearly-sparse signal recovery under sub-gaussian noisymeasurements. In particular, we show that when the measurement matrix satisfiesRestricted Isometric Properties (RIP), our theoretical results in settings (i)and (ii) almost recover the best condition on the RIP constants for compressivesensing. In addition, in setting (iii), our results for sparse signal recoveryare better than the previous results, and furthermore our analysis explicitlyexhibits that more observations lead to not only more accurate recovery butalso faster convergence. Compared with previous studies on linear convergencefor sparse signal recovery, our algorithm is simple and efficient, and ourresults are better and provide more insights. Finally our empirical studiesprovide further support for the proposed homotopy proximal mapping algorithmand verify the theoretical results.

Improved Dropout for Shallow and Deep Learning

  Dropout has been witnessed with great success in training deep neuralnetworks by independently zeroing out the outputs of neurons at random. It hasalso received a surge of interest for shallow learning, e.g., logisticregression. However, the independent sampling for dropout could be suboptimalfor the sake of convergence. In this paper, we propose to use multinomialsampling for dropout, i.e., sampling features or neurons according to amultinomial distribution with different probabilities for differentfeatures/neurons. To exhibit the optimal dropout probabilities, we analyze theshallow learning with multinomial dropout and establish the risk bound forstochastic optimization. By minimizing a sampling dependent factor in the riskbound, we obtain a distribution-dependent dropout with sampling probabilitiesdependent on the second order statistics of the data distribution. To tacklethe issue of evolving distribution of neurons in deep learning, we propose anefficient adaptive dropout (named \textbf{evolutional dropout}) that computesthe sampling probabilities on-the-fly from a mini-batch of examples. Empiricalstudies on several benchmark datasets demonstrate that the proposed dropoutsachieve not only much faster convergence and but also a smaller testing errorthan the standard dropout. For example, on the CIFAR-100 data, the evolutionaldropout achieves relative improvements over 10\% on the prediction performanceand over 50\% on the convergence speed compared to the standard dropout.

Unified Convergence Analysis of Stochastic Momentum Methods for Convex  and Non-convex Optimization

  Recently, {\it stochastic momentum} methods have been widely adopted intraining deep neural networks. However, their convergence analysis is stillunderexplored at the moment, in particular for non-convex optimization. Thispaper fills the gap between practice and theory by developing a basicconvergence analysis of two stochastic momentum methods, namely stochasticheavy-ball method and the stochastic variant of Nesterov's accelerated gradientmethod. We hope that the basic convergence results developed in this paper canserve the reference to the convergence of stochastic momentum methods and alsoserve the baselines for comparison in future development of stochastic momentummethods. The novelty of convergence analysis presented in this paper is aunified framework, revealing more insights about the similarities anddifferences between different stochastic momentum methods and stochasticgradient method. The unified framework exhibits a continuous change from thegradient method to Nesterov's accelerated gradient method and finally theheavy-ball method incurred by a free parameter, which can help explain asimilar change observed in the testing error convergence behavior for deeplearning. Furthermore, our empirical results for optimizing deep neuralnetworks demonstrate that the stochastic variant of Nesterov's acceleratedgradient method achieves a good tradeoff (between speed of convergence intraining error and robustness of convergence in testing error) among the threestochastic methods.

Tracking Slowly Moving Clairvoyant: Optimal Dynamic Regret of Online  Learning with True and Noisy Gradient

  This work focuses on dynamic regret of online convex optimization thatcompares the performance of online learning to a clairvoyant who knows thesequence of loss functions in advance and hence selects the minimizer of theloss function at each step. By assuming that the clairvoyant moves slowly(i.e., the minimizers change slowly), we present several improvedvariation-based upper bounds of the dynamic regret under the true and noisygradient feedback, which are {\it optimal} in light of the presented lowerbounds. The key to our analysis is to explore a regularity metric that measuresthe temporal changes in the clairvoyant's minimizers, to which we refer as {\itpath variation}. Firstly, we present a general lower bound in terms of the pathvariation, and then show that under full information or gradient feedback weare able to achieve an optimal dynamic regret. Secondly, we present a lowerbound with noisy gradient feedback and then show that we can achieve optimaldynamic regrets under a stochastic gradient feedback and two-point banditfeedback. Moreover, for a sequence of smooth loss functions that admit a smallvariation in the gradients, our dynamic regret under the two-point banditfeedback matches what is achieved with full information.

Accelerated Stochastic Subgradient Methods under Local Error Bound  Condition

  In this paper, we propose two {\bf accelerated stochastic subgradient}methods for stochastic non-strongly convex optimization problems by leveraginga generic local error bound condition. The novelty of the proposed methods liesat smartly leveraging the recent historical solution to tackle the variance inthe stochastic subgradient. The key idea of both methods is to iterativelysolve the original problem approximately in a local region around a recenthistorical solution with size of the local region gradually decreasing as thesolution approaches the optimal set. The difference of the two methods lies athow to construct the local region. The first method uses an explicit ballconstraint and the second method uses an implicit regularization approach. Forboth methods, we establish the improved iteration complexity in a highprobability for achieving an $\epsilon$-optimal solution. Besides the improvedorder of iteration complexity with a high probability, the proposed algorithmsalso enjoy a logarithmic dependence on the distance of the initial solution tothe optimal set. We also consider applications in machine learning anddemonstrate that the proposed algorithms enjoy faster convergence than thetraditional stochastic subgradient method. For example, when applied to the$\ell_1$ regularized polyhedral loss minimization (e.g., hinge loss, absoluteloss), the proposed stochastic methods have a logarithmic iteration complexity.

Homotopy Smoothing for Non-Smooth Problems with Lower Complexity than  $O(1/ε)$

  In this paper, we develop a novel {\bf ho}moto{\bf p}y {\bf s}moothing (HOPS)algorithm for solving a family of non-smooth problems that is composed of anon-smooth term with an explicit max-structure and a smooth term or a simplenon-smooth term whose proximal mapping is easy to compute. The best knowniteration complexity for solving such non-smooth optimization problems is$O(1/\epsilon)$ without any assumption on the strong convexity. In this work,we will show that the proposed HOPS achieved a lower iteration complexity of$\widetilde O(1/\epsilon^{1-\theta})$\footnote{$\widetilde O()$ suppresses alogarithmic factor.} with $\theta\in(0,1]$ capturing the local sharpness of theobjective function around the optimal solutions. To the best of our knowledge,this is the lowest iteration complexity achieved so far for the considerednon-smooth optimization problems without strong convexity assumption. The HOPSalgorithm employs Nesterov's smoothing technique and Nesterov's acceleratedgradient method and runs in stages, which gradually decreases the smoothingparameter in a stage-wise manner until it yields a sufficiently goodapproximation of the original function. We show that HOPS enjoys a linearconvergence for many well-known non-smooth problems (e.g., empirical riskminimization with a piece-wise linear loss function and $\ell_1$ normregularizer, finding a point in a polyhedron, cone programming, etc).Experimental results verify the effectiveness of HOPS in comparison withNesterov's smoothing algorithm and the primal-dual style of first-ordermethods.

A Richer Theory of Convex Constrained Optimization with Reduced  Projections and Improved Rates

  This paper focuses on convex constrained optimization problems, where thesolution is subject to a convex inequality constraint. In particular, we aim atchallenging problems for which both projection into the constrained domain anda linear optimization under the inequality constraint are time-consuming, whichrender both projected gradient methods and conditional gradient methods (a.k.a.the Frank-Wolfe algorithm) expensive. In this paper, we develop projectionreduced optimization algorithms for both smooth and non-smooth optimizationwith improved convergence rates under a certain regularity condition of theconstraint function. We first present a general theory of optimization withonly one projection. Its application to smooth optimization with only oneprojection yields $O(1/\epsilon)$ iteration complexity, which improves over the$O(1/\epsilon^2)$ iteration complexity established before for non-smoothoptimization and can be further reduced under strong convexity. Then weintroduce a local error bound condition and develop faster algorithms fornon-strongly convex optimization at the price of a logarithmic number ofprojections. In particular, we achieve an iteration complexity of $\widetildeO(1/\epsilon^{2(1-\theta)})$ for non-smooth optimization and $\widetildeO(1/\epsilon^{1-\theta})$ for smooth optimization, where $\theta\in(0,1]$appearing the local error bound condition characterizes the functional localgrowth rate around the optimal solutions. Novel applications in solving theconstrained $\ell_1$ minimization problem and a positive semi-definiteconstrained distance metric learning problem demonstrate that the proposedalgorithms achieve significant speed-up compared with previous algorithms.

Adaptive Accelerated Gradient Converging Methods under Holderian Error  Bound Condition

  Recent studies have shown that proximal gradient (PG) method and acceleratedgradient method (APG) with restarting can enjoy a linear convergence under aweaker condition than strong convexity, namely a quadratic growth condition(QGC). However, the faster convergence of restarting APG method relies on thepotentially unknown constant in QGC to appropriately restart APG, whichrestricts its applicability. We address this issue by developing a noveladaptive gradient converging methods, i.e., leveraging the magnitude ofproximal gradient as a criterion for restart and termination. Our analysisextends to a much more general condition beyond the QGC, namely theH\"{o}lderian error bound (HEB) condition. {\it The key technique} for ourdevelopment is a novel synthesis of {\it adaptive regularization and aconditional restarting scheme}, which extends previous work focusing onstrongly convex problems to a much broader family of problems. Furthermore, wedemonstrate that our results have important implication and applications inmachine learning: (i) if the objective function is coercive and semi-algebraic,PG's convergence speed is essentially $o(\frac{1}{t})$, where $t$ is the totalnumber of iterations; (ii) if the objective function consists of an $\ell_1$,$\ell_\infty$, $\ell_{1,\infty}$, or huber norm regularization and a convexsmooth piecewise quadratic loss (e.g., squares loss, squared hinge loss andhuber loss), the proposed algorithm is parameter-free and enjoys a {\it fasterlinear convergence} than PG without any other assumptions (e.g., restrictedeigen-value condition). It is notable that our linear convergence results forthe aforementioned problems are global instead of local. To the best of ourknowledge, these improved results are the first shown in this work.

NEON+: Accelerated Gradient Methods for Extracting Negative Curvature  for Non-Convex Optimization

  Accelerated gradient (AG) methods are breakthroughs in convex optimization,improving the convergence rate of the gradient descent method for optimizationwith smooth functions. However, the analysis of AG methods for non-convexoptimization is still limited. It remains an open question whether AG methodsfrom convex optimization can accelerate the convergence of the gradient descentmethod for finding local minimum of non-convex optimization problems. Thispaper provides an affirmative answer to this question. In particular, weanalyze two renowned variants of AG methods (namely Polyak's Heavy Ball methodand Nesterov's Accelerated Gradient method) for extracting the negativecurvature from random noise, which is central to escaping from saddle points.By leveraging the proposed AG methods for extracting the negative curvature, wepresent a new AG algorithm with double loops for non-convexoptimization~\footnote{this is in contrast to a single-loop AG algorithmproposed in a recent manuscript~\citep{AGNON}, which directly analyzed theNesterov's AG method for non-convex optimization and appeared online onNovember 29, 2017. However, we emphasize that our work is an independent work,which is inspired by our earlier work~\citep{NEON17} and is based on adifferent novel analysis.}, which converges to second-order stationary point$\x$ such that $\|\nabla f(\x)\|\leq \epsilon$ and $\nabla^2 f(\x)\geq-\sqrt{\epsilon} I$ with $\widetilde O(1/\epsilon^{1.75})$ iterationcomplexity, improving that of gradient descent method by a factor of$\epsilon^{-0.25}$ and matching the best iteration complexity of second-orderHessian-free methods for non-convex optimization.

Fast Rates of ERM and Stochastic Approximation: Adaptive to Error Bound  Conditions

  Error bound conditions (EBC) are properties that characterize the growth ofan objective function when a point is moved away from the optimal set. Theyhave recently received increasing attention in the field of optimization fordeveloping optimization algorithms with fast convergence. However, the studiesof EBC in statistical learning are hitherto still limited. The maincontributions of this paper are two-fold. First, we develop fast andintermediate rates of empirical risk minimization (ERM) under EBC for riskminimization with Lipschitz continuous, and smooth convex random functions.Second, we establish fast and intermediate rates of an efficient stochasticapproximation (SA) algorithm for risk minimization with Lipschitz continuousrandom functions, which requires only one pass of $n$ samples and adapts toEBC. For both approaches, the convergence rates span a full spectrum between$\widetilde O(1/\sqrt{n})$ and $\widetilde O(1/n)$ depending on the powerconstant in EBC, and could be even faster than $O(1/n)$ in special cases forERM. Moreover, these convergence rates are automatically adaptive without usingany knowledge of EBC. Overall, this work not only strengthens the understandingof ERM for statistical learning but also brings new fast stochastic algorithmsfor solving a broad range of statistical learning problems.

Solving Weakly-Convex-Weakly-Concave Saddle-Point Problems as Successive  Strongly Monotone Variational Inequalities

  In this paper, we consider first-order algorithms for solving a class ofnon-convex non-concave min-max saddle-point problems, whose objective functionis weakly convex (resp. weakly concave) in terms of the variable ofminimization (resp. maximization). It has many important applications inmachine learning, statistics, and operations research. One such example thatattracts tremendous attention recently in machine learning is trainingGenerative Adversarial Networks. We propose an algorithmic framework motivatedby the inexact proximal point method, which solves the weakly monotonevariational inequality corresponding to the original min-max problem byapproximately solving a sequence of strongly monotone variational inequalitiesconstructed by adding a strongly monotone mapping to the original gradientmapping. In this sequence, each strongly monotone variational inequality isdefined with a proximal center that is updated using the approximate solutionof the previous variational inequality. Our algorithm generates a sequence ofsolution that provably converges to a nearly stationary solution of theoriginal min-max problem. The proposed framework is flexible because varioussubroutines can be employed for solving the strongly monotone variationalinequalities. The overall computational complexities of our methods areestablished when the employed subroutines are subgradient method, stochasticsubgradient method, gradient descent method and Nesterov's accelerated methodand variance reduction methods for a Lipschitz continuous operator. To the bestof our knowledge, this is the first work that establishes the non-asymptoticconvergence to a nearly stationary point of a non-convex non-concave min-maxproblem.

Stagewise Training Accelerates Convergence of Testing Error Over SGD

  Stagewise training strategy is widely used for learning neural networks,which runs a stochastic algorithm (e.g., SGD) starting with a relatively largestep size (aka learning rate) and geometrically decreasing the step size aftera number of iterations. It has been observed that the stagewise SGD has muchfaster convergence than the vanilla SGD with a polynomially decaying step sizein terms of both training error and testing error. {\it But how to explain thisphenomenon has been largely ignored by existing studies.} This paper providessome theoretical evidence for explaining this faster convergence. Inparticular, we consider a stagewise training strategy for minimizing empiricalrisk that satisfies the Polyak-\L ojasiewicz (PL) condition, which has beenobserved/proved for neural networks and also holds for a broad family of convexfunctions. For convex loss functions and two classes of "nice-behaviored"non-convex objectives that are close to a convex function, we establish fasterconvergence of stagewise training than the vanilla SGD under the PL conditionon both training error and testing error. Experiments on stagewise learning ofdeep residual networks exhibits that it satisfies one type of non-convexityassumption and therefore can be explained by our theory. Of independentinterest, the testing error bounds for the considered non-convex loss functionsare dimensionality and norm independent.

Learning with Non-Convex Truncated Losses by SGD

  Learning with a {\it convex loss} function has been a dominating paradigm formany years. It remains an interesting question how non-convex loss functionshelp improve the generalization of learning with broad applicability. In thispaper, we study a family of objective functions formed by truncatingtraditional loss functions, which is applicable to both shallow learning anddeep learning. Truncating loss functions has potential to be less vulnerableand more robust to large noise in observations that could be adversarial. Moreimportantly, it is a generic technique without assuming the knowledge of noisedistribution. To justify non-convex learning with truncated losses, weestablish excess risk bounds of empirical risk minimization based on truncatedlosses for heavy-tailed output, and statistical error of an approximatestationary point found by stochastic gradient descent (SGD) method. Ourexperiments for shallow and deep learning for regression with outliers,corrupted data and heavy-tailed noise further justify the proposed method.

A Unified Analysis of Stochastic Momentum Methods for Deep Learning

  Stochastic momentum methods have been widely adopted in training deep neuralnetworks. However, their theoretical analysis of convergence of the trainingobjective and the generalization error for prediction is still under-explored.This paper aims to bridge the gap between practice and theory by analyzing thestochastic gradient (SG) method, and the stochastic momentum methods includingtwo famous variants, i.e., the stochastic heavy-ball (SHB) method and thestochastic variant of Nesterov's accelerated gradient (SNAG) method. We proposea framework that unifies the three variants. We then derive the convergencerates of the norm of gradient for the non-convex optimization problem, andanalyze the generalization performance through the uniform stability approach.Particularly, the convergence analysis of the training objective exhibits thatSHB and SNAG have no advantage over SG. However, the stability analysis showsthat the momentum term can improve the stability of the learned model and henceimprove the generalization performance. These theoretical insights verify thecommon wisdom and are also corroborated by our empirical analysis on deeplearning.

Learning Discriminators as Energy Networks in Adversarial Learning

  We propose a novel framework for structured prediction via adversariallearning. Existing adversarial learning methods involve two separate networks,i.e., the structured prediction models and the discriminative models, in thetraining. The information captured by discriminative models complements that inthe structured prediction models, but few existing researches have studied onutilizing such information to improve structured prediction models at theinference stage. In this work, we propose to refine the predictions ofstructured prediction models by effectively integrating discriminative modelsinto the prediction. Discriminative models are treated as energy-based models.Similar to the adversarial learning, discriminative models are trained toestimate scores which measure the quality of predicted outputs, whilestructured prediction models are trained to predict contrastive outputs withmaximal energy scores. In this way, the gradient vanishing problem isameliorated, and thus we are able to perform inference by following the ascentgradient directions of discriminative models to refine structured predictionmodels. The proposed method is able to handle a range of tasks, e.g.,multi-label classification and image segmentation. Empirical results on thesetwo tasks validate the effectiveness of our learning method.

RSG: Beating Subgradient Method without Smoothness and Strong Convexity

  In this paper, we study the efficiency of a {\bf R}estarted {\bf S}ub{\bfG}radient (RSG) method that periodically restarts the standard subgradientmethod (SG). We show that, when applied to a broad class of convex optimizationproblems, RSG method can find an $\epsilon$-optimal solution with a lowercomplexity than the SG method. In particular, we first show that RSG can reducethe dependence of SG's iteration complexity on the distance between the initialsolution and the optimal set to that between the $\epsilon$-level set and theoptimal set {multiplied by a logarithmic factor}. Moreover, we show theadvantages of RSG over SG in solving three different families of convexoptimization problems. (a) For the problems whose epigraph is a polyhedron, RSGis shown to converge linearly. (b) For the problems with local quadratic growthproperty in the $\epsilon$-sublevel set, RSG has an$O(\frac{1}{\epsilon}\log(\frac{1}{\epsilon}))$ iteration complexity. (c) Forthe problems that admit a local Kurdyka-\L ojasiewicz property with a powerconstant of $\beta\in[0,1)$, RSG has an$O(\frac{1}{\epsilon^{2\beta}}\log(\frac{1}{\epsilon}))$ iteration complexity.The novelty of our analysis lies at exploiting the lower bound of thefirst-order optimality residual at the $\epsilon$-level set. It is this noveltythat allows us to explore the local properties of functions (e.g., localquadratic growth property, local Kurdyka-\L ojasiewicz property, more generallylocal error bound conditions) to develop the improved convergence of RSG. { Wealso develop a practical variant of RSG enjoying faster convergence than the SGmethod, which can be run without knowing the involved parameters in the localerror bound condition.} We demonstrate the effectiveness of the proposedalgorithms on several machine learning tasks including regression,classification and matrix completion.

On Noisy Negative Curvature Descent: Competing with Gradient Descent for  Faster Non-convex Optimization

  The Hessian-vector product has been utilized to find a second-orderstationary solution with strong complexity guarantee (e.g., almost linear timecomplexity in the problem's dimensionality). In this paper, we propose tofurther reduce the number of Hessian-vector products for faster non-convexoptimization. Previous algorithms need to approximate the smallest eigen-valuewith a sufficient precision (e.g., $\epsilon_2\ll 1$) in order to achieve asufficiently accurate second-order stationary solution (i.e.,$\lambda_{\min}(\nabla^2 f(\x))\geq -\epsilon_2)$. In contrast, the proposedalgorithms only need to compute the smallest eigen-vector approximating thecorresponding eigen-value up to a small power of current gradient's norm. As aresult, it can dramatically reduce the number of Hessian-vector products duringthe course of optimization before reaching first-order stationary points (e.g.,saddle points). The key building block of the proposed algorithms is a novelupdating step named the NCG step, which lets a noisy negative curvature descentcompete with the gradient descent. We show that the worst-case time complexityof the proposed algorithms with their favorable prescribed accuracyrequirements can match the best in literature for achieving a second-orderstationary point but with an arguably smaller per-iteration cost. We also showthat the proposed algorithms can benefit from inexact Hessian by developingtheir variants accepting inexact Hessian under a mild condition for achievingthe same goal. Moreover, we develop a stochastic algorithm for a finite orinfinite sum non-convex optimization problem. To the best of our knowledge, theproposed stochastic algorithm is the first one that converges to a second-orderstationary point in {\it high probability} with a time complexity independentof the sample size and almost linear in dimensionality.

First-order Stochastic Algorithms for Escaping From Saddle Points in  Almost Linear Time

  Two classes of methods have been proposed for escaping from saddle pointswith one using the second-order information carried by the Hessian and theother adding the noise into the first-order information. The existing analysisfor algorithms using noise in the first-order information is quite involved andhides the essence of added noise, which hinder further improvements of thesealgorithms. In this paper, we present a novel perspective of noise-addingtechnique, i.e., adding the noise into the first-order information can helpextract the negative curvature from the Hessian matrix, and provide a formalreasoning of this perspective by analyzing a simple first-order procedure. Moreimportantly, the proposed procedure enables one to design purely first-orderstochastic algorithms for escaping from non-degenerate saddle points with amuch better time complexity (almost linear time in terms of the problem'sdimensionality). In particular, we develop a {\bf first-order stochasticalgorithm} based on our new technique and an existing algorithm that onlyconverges to a first-order stationary point to enjoy a time complexity of{$\widetilde O(d/\epsilon^{3.5})$ for finding a nearly second-order stationarypoint $\bf{x}$ such that $\|\nabla F(bf{x})\|\leq \epsilon$ and $\nabla^2F(bf{x})\geq -\sqrt{\epsilon}I$ (in high probability), where $F(\cdot)$ denotesthe objective function and $d$ is the dimensionality of the problem. To thebest of our knowledge, this is the best theoretical result of first-orderalgorithms for stochastic non-convex optimization, which is even competitivewith if not better than existing stochastic algorithms hinging on thesecond-order information.

Universal Stagewise Learning for Non-Convex Problems with Convergence on  Averaged Solutions

  Although stochastic gradient descent (SGD) method and its variants (e.g.,stochastic momentum methods, AdaGrad) are the choice of algorithms for solvingnon-convex problems (especially deep learning), there still remain big gapsbetween the theory and the practice with many questions unresolved. Forexample, there is still a lack of theories of convergence for SGD and itsvariants that use stagewise step size and return an averaged solution inpractice. In addition, theoretical insights of why adaptive step size ofAdaGrad could improve non-adaptive step size of {\sgd} is still missing fornon-convex optimization. This paper aims to address these questions and fillthe gap between theory and practice. We propose a universal stagewiseoptimization framework for a broad family of {\bf non-smooth non-convex}(namely weakly convex) problems with the following key features: (i) at eachstage any suitable stochastic convex optimization algorithms (e.g., SGD orAdaGrad) that return an averaged solution can be employed for minimizing aregularized convex problem; (ii) the step size is decreased in a stagewisemanner; (iii) an averaged solution is returned as the final solution that isselected from all stagewise averaged solutions with sampling probabilities {\itincreasing} as the stage number. Our theoretical results of stagewise AdaGradexhibit its adaptive convergence, therefore shed insights on its fasterconvergence for problems with sparse stochastic gradients than stagewise SGD.To the best of our knowledge, these new results are the first of their kind foraddressing the unresolved issues of existing theories mentioned earlier.Besides theoretical contributions, our empirical studies show that ourstagewise SGD and ADAGRAD improve the generalization performance of existingvariants/implementations of SGD and ADAGRAD.

Stochastic Optimization for DC Functions and Non-smooth Non-convex  Regularizers with Non-asymptotic Convergence

  Difference of convex (DC) functions cover a broad family of non-convex andpossibly non-smooth and non-differentiable functions, and have wideapplications in machine learning and statistics. Although deterministicalgorithms for DC functions have been extensively studied, stochasticoptimization that is more suitable for learning with big data remainsunder-explored. In this paper, we propose new stochastic optimizationalgorithms and study their first-order convergence theories for solving a broadfamily of DC functions. We improve the existing algorithms and theories ofstochastic optimization for DC functions from both practical and theoreticalperspectives. On the practical side, our algorithm is more user-friendlywithout requiring a large mini-batch size and more efficient by savingunnecessary computations. On the theoretical side, our convergence analysisdoes not necessarily require the involved functions to be smooth with Lipschitzcontinuous gradient. Instead, the convergence rate of the proposed stochasticalgorithm is automatically adaptive to the H\"{o}lder continuity of thegradient of one component function. Moreover, we extend the proposed stochasticalgorithms for DC functions to solve problems with a general non-convexnon-differentiable regularizer, which does not necessarily have a DCdecomposition but enjoys an efficient proximal mapping. To the best of ourknowledge, this is the first work that gives the first non-asymptoticconvergence for solving non-convex optimization whose objective has a generalnon-convex non-differentiable regularizer.

Efficient Constrained Regret Minimization

  Online learning constitutes a mathematical and compelling framework toanalyze sequential decision making problems in adversarial environments. Thelearner repeatedly chooses an action, the environment responds with an outcome,and then the learner receives a reward for the played action. The goal of thelearner is to maximize his total reward. However, there are situations inwhich, in addition to maximizing the cumulative reward, there are someadditional constraints on the sequence of decisions that must be satisfied onaverage by the learner. In this paper we study an extension to the onlinelearning where the learner aims to maximize the total reward given that someadditional constraints need to be satisfied. By leveraging on the theory ofLagrangian method in constrained optimization, we propose Lagrangianexponentially weighted average (LEWA) algorithm, which is a primal-dual variantof the well known exponentially weighted average algorithm, to efficientlysolve constrained online decision making problems. Using novel theoreticalanalysis, we establish the regret and the violation of the constraint bounds infull information and bandit feedback models.

A Simple Algorithm for Semi-supervised Learning with Improved  Generalization Error Bound

  In this work, we develop a simple algorithm for semi-supervised regression.The key idea is to use the top eigenfunctions of integral operator derived fromboth labeled and unlabeled examples as the basis functions and learn theprediction function by a simple linear regression. We show that underappropriate assumptions about the integral operator, this approach is able toachieve an improved regression error bound better than existing bounds ofsupervised learning. We also verify the effectiveness of the proposed algorithmby an empirical study.

An Improved Bound for the Nystrom Method for Large Eigengap

  We develop an improved bound for the approximation error of the Nystr\"{o}mmethod under the assumption that there is a large eigengap in the spectrum ofkernel matrix. This is based on the empirical observation that the eigengap hasa significant impact on the approximation error of the Nystr\"{o}m method. Ourapproach is based on the concentration inequality of integral operator and thetheory of matrix perturbation. Our analysis shows that when there is a largeeigengap, we can improve the approximation error of the Nystr\"{o}m method from$O(N/m^{1/4})$ to $O(N/m^{1/2})$ when measured in Frobenius norm, where $N$ isthe size of the kernel matrix, and $m$ is the number of sampled columns.

Recovering the Optimal Solution by Dual Random Projection

  Random projection has been widely used in data classification. It mapshigh-dimensional data into a low-dimensional subspace in order to reduce thecomputational cost in solving the related optimization problem. While previousstudies are focused on analyzing the classification performance of using randomprojection, in this work, we consider the recovery problem, i.e., how toaccurately recover the optimal solution to the original optimization problem inthe high-dimensional space based on the solution learned from the subspacespanned by random projections. We present a simple algorithm, termed DualRandom Projection, that uses the dual solution of the low-dimensionaloptimization problem to recover the optimal solution to the original problem.Our theoretical analysis shows that with a high probability, the proposedalgorithm is able to accurately recover the optimal solution to the originalproblem, provided that the data matrix is of low rank or can be wellapproximated by a low rank matrix.

Sparse Multiple Kernel Learning with Geometric Convergence Rate

  In this paper, we study the problem of sparse multiple kernel learning (MKL),where the goal is to efficiently learn a combination of a fixed small number ofkernels from a large pool that could lead to a kernel classifier with a smallprediction error. We develop an efficient algorithm based on the greedycoordinate descent algorithm, that is able to achieve a geometric convergencerate under appropriate conditions. The convergence rate is achieved bymeasuring the size of functional gradients by an empirical $\ell_2$ norm thatdepends on the empirical data distribution. This is in contrast to previousalgorithms that use a functional norm to measure the size of gradients, whichis independent from the data samples. We also establish a generalization errorbound of the learned sparse kernel classifier using the technique of localRademacher complexity.

O(logT) Projections for Stochastic Optimization of Smooth and Strongly  Convex Functions

  Traditional algorithms for stochastic optimization require projecting thesolution at each iteration into a given domain to ensure its feasibility. Whenfacing complex domains, such as positive semi-definite cones, the projectionoperation can be expensive, leading to a high computational cost per iteration.In this paper, we present a novel algorithm that aims to reduce the number ofprojections for stochastic optimization. The proposed algorithm combines thestrength of several recent developments in stochastic optimization, includingmini-batch, extra-gradient, and epoch gradient descent, in order to effectivelyexplore the smoothness and strong convexity. We show, both in expectation andwith a high probability, that when the objective function is both smooth andstrongly convex, the proposed algorithm achieves the optimal $O(1/T)$ rate ofconvergence with only $O(\log T)$ projections. Our empirical study verifies thetheoretical result.

Analysis of Nuclear Norm Regularization for Full-rank Matrix Completion

  In this paper, we provide a theoretical analysis of the nuclear-normregularized least squares for full-rank matrix completion. Although similarformulations have been examined by previous studies, their results areunsatisfactory because only additive upper bounds are provided. Under theassumption that the top eigenspaces of the target matrix are incoherent, wederive a relative upper bound for recovering the best low-rank approximation ofthe unknown matrix. Our relative upper bound is tighter than previous additivebounds of other methods if the mass of the target matrix is concentrated on itstop eigenspaces, and also implies perfect recovery if it is low-rank. Theanalysis is built upon the optimality condition of the regularized formulationand existing guarantees for low-rank matrix completion. To the best of ourknowledge, this is first time such a relative bound is proved for theregularized formulation of matrix completion.

Distributed Stochastic Variance Reduced Gradient Methods and A Lower  Bound for Communication Complexity

  We study distributed optimization algorithms for minimizing the average ofconvex functions. The applications include empirical risk minimization problemsin statistical machine learning where the datasets are large and have to bestored on different machines. We design a distributed stochastic variancereduced gradient algorithm that, under certain conditions on the conditionnumber, simultaneously achieves the optimal parallel runtime, amount ofcommunication and rounds of communication among all distributed first-ordermethods up to constant factors. Our method and its accelerated extension alsooutperform existing distributed algorithms in terms of the rounds ofcommunication as long as the condition number is not too large compared to thesize of data in each machine. We also prove a lower bound for the number ofrounds of communication for a broad class of distributed first-order methodsincluding the proposed algorithms in this paper. We show that our accelerateddistributed stochastic variance reduced gradient algorithm achieves this lowerbound so that it uses the fewest rounds of communication among all distributedfirst-order algorithms.

Doubly Stochastic Primal-Dual Coordinate Method for Bilinear  Saddle-Point Problem

  We propose a doubly stochastic primal-dual coordinate optimization algorithmfor empirical risk minimization, which can be formulated as a bilinearsaddle-point problem. In each iteration, our method randomly samples a block ofcoordinates of the primal and dual solutions to update. The linear convergenceof our method could be established in terms of 1) the distance from the currentiterate to the optimal solution and 2) the primal-dual objective gap. We showthat the proposed method has a lower overall complexity than existingcoordinate methods when either the data matrix has a factorized structure orthe proximal mapping on each block is computationally expensive, e.g.,involving an eigenvalue decomposition. The efficiency of the proposed method isconfirmed by empirical studies on several real applications, such as themulti-task large margin nearest neighbor problem.

Online Stochastic Linear Optimization under One-bit Feedback

  In this paper, we study a special bandit setting of online stochastic linearoptimization, where only one-bit of information is revealed to the learner ateach round. This problem has found many applications including onlineadvertisement and online recommendation. We assume the binary feedback is arandom variable generated from the logit model, and aim to minimize the regretdefined by the unknown linear function. Although the existing method forgeneralized linear bandit can be applied to our problem, the high computationalcost makes it impractical for real-world problems. To address this challenge,we develop an efficient online learning algorithm by exploiting particularstructures of the observation model. Specifically, we adopt online Newton stepto estimate the unknown parameter and derive a tight confidence region based onthe exponential concavity of the logistic loss. Our analysis shows that theproposed algorithm achieves a regret bound of $O(d\sqrt{T})$, which matches theoptimal result of stochastic linear bandits.

Stochastic Proximal Gradient Descent for Nuclear Norm Regularization

  In this paper, we utilize stochastic optimization to reduce the spacecomplexity of convex composite optimization with a nuclear norm regularizer,where the variable is a matrix of size $m \times n$. By constructing a low-rankestimate of the gradient, we propose an iterative algorithm based on stochasticproximal gradient descent (SPGD), and take the last iterate of SPGD as thefinal solution. The main advantage of the proposed algorithm is that its spacecomplexity is $O(m+n)$, in contrast, most of previous algorithms have a $O(mn)$space complexity. Theoretical analysis shows that it achieves $O(\logT/\sqrt{T})$ and $O(\log T/T)$ convergence rates for general convex functionsand strongly convex functions, respectively.

Sparse Learning for Large-scale and High-dimensional Data: A Randomized  Convex-concave Optimization Approach

  In this paper, we develop a randomized algorithm and theory for learning asparse model from large-scale and high-dimensional data, which is usuallyformulated as an empirical risk minimization problem with a sparsity-inducingregularizer. Under the assumption that there exists a (approximately) sparsesolution with high classification accuracy, we argue that the dual solution isalso sparse or approximately sparse. The fact that both primal and dualsolutions are sparse motivates us to develop a randomized approach for ageneral convex-concave optimization problem. Specifically, the proposedapproach combines the strength of random projection with that of sparselearning: it utilizes random projection to reduce the dimensionality, andintroduces $\ell_1$-norm regularization to alleviate the approximation errorcaused by random projection. Theoretical analysis shows that under favoredconditions, the randomized algorithm can accurately recover the optimalsolutions to the convex-concave optimization problem (i.e., recover both theprimal and dual solutions).

Learning Attributes Equals Multi-Source Domain Generalization

  Attributes possess appealing properties and benefit many computer visionproblems, such as object recognition, learning with humans in the loop, andimage retrieval. Whereas the existing work mainly pursues utilizing attributesfor various computer vision problems, we contend that the most basicproblem---how to accurately and robustly detect attributes from images---hasbeen left under explored. Especially, the existing work rarely explicitlytackles the need that attribute detectors should generalize well acrossdifferent categories, including those previously unseen. Noting that this isanalogous to the objective of multi-source domain generalization, if we treateach category as a domain, we provide a novel perspective to attributedetection and propose to gear the techniques in multi-source domaingeneralization for the purpose of learning cross-category generalizableattribute detectors. We validate our understanding and approach with extensiveexperiments on four challenging datasets and three different problems.

