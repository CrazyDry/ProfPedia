Multiple Kernel Learning from Noisy Labels by Stochastic Programming

  We study the problem of multiple kernel learning from noisy labels. This is
in contrast to most of the previous studies on multiple kernel learning that
mainly focus on developing efficient algorithms and assume perfectly labeled
training examples. Directly applying the existing multiple kernel learning
algorithms to noisily labeled examples often leads to suboptimal performance
due to the incorrect class assignments. We address this challenge by casting
multiple kernel learning from noisy labels into a stochastic programming
problem, and presenting a minimax formulation. We develop an efficient
algorithm for solving the related convex-concave optimization problem with a
fast convergence rate of $O(1/T)$ where $T$ is the number of iterations.
Empirical studies on UCI data sets verify both the effectiveness of the
proposed framework and the efficiency of the proposed optimization algorithm.


An Efficient Primal-Dual Prox Method for Non-Smooth Optimization

  We study the non-smooth optimization problems in machine learning, where both
the loss function and the regularizer are non-smooth functions. Previous
studies on efficient empirical loss minimization assume either a smooth loss
function or a strongly convex regularizer, making them unsuitable for
non-smooth optimization. We develop a simple yet efficient method for a family
of non-smooth optimization problems where the dual form of the loss function is
bilinear in primal and dual variables. We cast a non-smooth optimization
problem into a minimax optimization problem, and develop a primal dual prox
method that solves the minimax optimization problem at a rate of $O(1/T)$
{assuming that the proximal step can be efficiently solved}, significantly
faster than a standard subgradient descent method that has an $O(1/\sqrt{T})$
convergence rate. Our empirical study verifies the efficiency of the proposed
method for various non-smooth optimization problems that arise ubiquitously in
machine learning by comparing it to the state-of-the-art first order methods.


A New Analysis of Compressive Sensing by Stochastic Proximal Gradient
  Descent

  In this manuscript, we analyze the sparse signal recovery (compressive
sensing) problem from the perspective of convex optimization by stochastic
proximal gradient descent. This view allows us to significantly simplify the
recovery analysis of compressive sensing. More importantly, it leads to an
efficient optimization algorithm for solving the regularized optimization
problem related to the sparse recovery problem. Compared to the existing
approaches, there are two advantages of the proposed algorithm. First, it
enjoys a geometric convergence rate and therefore is computationally efficient.
Second, it guarantees that the support set of any intermediate solution
generated by the proposed algorithm is concentrated on the support set of the
optimal solution.


On Data Preconditioning for Regularized Loss Minimization

  In this work, we study data preconditioning, a well-known and long-existing
technique, for boosting the convergence of first-order methods for regularized
loss minimization. It is well understood that the condition number of the
problem, i.e., the ratio of the Lipschitz constant to the strong convexity
modulus, has a harsh effect on the convergence of the first-order optimization
methods. Therefore, minimizing a small regularized loss for achieving good
generalization performance, yielding an ill conditioned problem, becomes the
bottleneck for big data problems. We provide a theory on data preconditioning
for regularized loss minimization. In particular, our analysis exhibits an
appropriate data preconditioner and characterizes the conditions on the loss
function and on the data under which data preconditioning can reduce the
condition number and therefore boost the convergence for minimizing the
regularized loss. To make the data preconditioning practically useful, we
endeavor to employ and analyze a random sampling approach to efficiently
compute the preconditioned data. The preliminary experiments validate our
theory.


Theory of Dual-sparse Regularized Randomized Reduction

  In this paper, we study randomized reduction methods, which reduce
high-dimensional features into low-dimensional space by randomized methods
(e.g., random projection, random hashing), for large-scale high-dimensional
classification. Previous theoretical results on randomized reduction methods
hinge on strong assumptions about the data, e.g., low rank of the data matrix
or a large separable margin of classification, which hinder their applications
in broad domains. To address these limitations, we propose dual-sparse
regularized randomized reduction methods that introduce a sparse regularizer
into the reduced dual problem. Under a mild condition that the original dual
solution is a (nearly) sparse vector, we show that the resulting dual solution
is close to the original dual solution and concentrates on its support set. In
numerical experiments, we present an empirical study to support the analysis
and we also present a novel application of the dual-sparse regularized
randomized reduction methods to reducing the communication cost of distributed
learning from large-scale high-dimensional data.


An Explicit Sampling Dependent Spectral Error Bound for Column Subset
  Selection

  In this paper, we consider the problem of column subset selection. We present
a novel analysis of the spectral norm reconstruction for a simple randomized
algorithm and establish a new bound that depends explicitly on the sampling
probabilities. The sampling dependent error bound (i) allows us to better
understand the tradeoff in the reconstruction error due to sampling
probabilities, (ii) exhibits more insights than existing error bounds that
exploit specific probability distributions, and (iii) implies better sampling
distributions. In particular, we show that a sampling distribution with
probabilities proportional to the square root of the statistical leverage
scores is always better than uniform sampling and is better than leverage-based
sampling when the statistical leverage scores are very nonuniform. And by
solving a constrained optimization problem related to the error bound with an
efficient bisection search we are able to achieve better performance than using
either the leverage-based distribution or that proportional to the square root
of the statistical leverage scores. Numerical simulations demonstrate the
benefits of the new sampling distributions for low-rank matrix approximation
and least square approximation compared to state-of-the art algorithms.


Regret Bound by Variation for Online Convex Optimization

  In citep{Hazan-2008-extract}, the authors showed that the regret of online
linear optimization can be bounded by the total variation of the cost vectors.
In this paper, we extend this result to general online convex optimization. We
first analyze the limitations of the algorithm in \citep{Hazan-2008-extract}
when applied it to online convex optimization. We then present two algorithms
for online convex optimization whose regrets are bounded by the variation of
cost functions. We finally consider the bandit setting, and present a
randomized algorithm for online bandit convex optimization with a
variation-based regret bound. We show that the regret bound for online bandit
convex optimization is optimal when the variation of cost functions is
independent of the number of trials.


Analysis of Distributed Stochastic Dual Coordinate Ascent

  In \citep{Yangnips13}, the author presented distributed stochastic dual
coordinate ascent (DisDCA) algorithms for solving large-scale regularized loss
minimization. Extraordinary performances have been observed and reported for
the well-motivated updates, as referred to the practical updates, compared to
the naive updates. However, no serious analysis has been provided to understand
the updates and therefore the convergence rates. In the paper, we bridge the
gap by providing a theoretical analysis of the convergence rates of the
practical DisDCA algorithm. Our analysis helped by empirical studies has shown
that it could yield an exponential speed-up in the convergence by increasing
the number of dual updates at each iteration. This result justifies the
superior performances of the practical DisDCA as compared to the naive variant.
As a byproduct, our analysis also reveals the convergence behavior of the
one-communication DisDCA.


Fast Sparse Least-Squares Regression with Non-Asymptotic Guarantees

  In this paper, we study a fast approximation method for {\it large-scale
high-dimensional} sparse least-squares regression problem by exploiting the
Johnson-Lindenstrauss (JL) transforms, which embed a set of high-dimensional
vectors into a low-dimensional space. In particular, we propose to apply the JL
transforms to the data matrix and the target vector and then to solve a sparse
least-squares problem on the compressed data with a {\it slightly larger
regularization parameter}. Theoretically, we establish the optimization error
bound of the learned model for two different sparsity-inducing regularizers,
i.e., the elastic net and the $\ell_1$ norm. Compared with previous relevant
work, our analysis is {\it non-asymptotic and exhibits more insights} on the
bound, the sample complexity and the regularization. As an illustration, we
also provide an error bound of the {\it Dantzig selector} under JL transforms.


Stochastic subGradient Methods with Linear Convergence for Polyhedral
  Convex Optimization

  In this paper, we show that simple {Stochastic} subGradient Decent methods
with multiple Restarting, named {\bf RSGD}, can achieve a \textit{linear
convergence rate} for a class of non-smooth and non-strongly convex
optimization problems where the epigraph of the objective function is a
polyhedron, to which we refer as {\bf polyhedral convex optimization}. Its
applications in machine learning include $\ell_1$ constrained or regularized
piecewise linear loss minimization and submodular function minimization. To the
best of our knowledge, this is the first result on the linear convergence rate
of stochastic subgradient methods for non-smooth and non-strongly convex
optimization problems.


A Simple Analysis for Exp-concave Empirical Minimization with Arbitrary
  Convex Regularizer

  In this paper, we present a simple analysis of {\bf fast rates} with {\it
high probability} of {\bf empirical minimization} for {\it stochastic composite
optimization} over a finite-dimensional bounded convex set with exponential
concave loss functions and an arbitrary convex regularization. To the best of
our knowledge, this result is the first of its kind. As a byproduct, we can
directly obtain the fast rate with {\it high probability} for exponential
concave empirical risk minimization with and without any convex regularization,
which not only extends existing results of empirical risk minimization but also
provides a unified framework for analyzing exponential concave empirical risk
minimization with and without {\it any} convex regularization. Our proof is
very simple only exploiting the covering number of a finite-dimensional bounded
set and a concentration inequality of random vectors.


Stochastic Non-convex Optimization with Strong High Probability
  Second-order Convergence

  In this paper, we study stochastic non-convex optimization with non-convex
random functions. Recent studies on non-convex optimization revolve around
establishing second-order convergence, i.e., converging to a nearly
second-order optimal stationary points. However, existing results on stochastic
non-convex optimization are limited, especially with a high probability
second-order convergence. We propose a novel updating step (named NCG-S) by
leveraging a stochastic gradient and a noisy negative curvature of a stochastic
Hessian, where the stochastic gradient and Hessian are based on a proper
mini-batch of random functions. Building on this step, we develop two
algorithms and establish their high probability second-order convergence. To
the best of our knowledge, the proposed stochastic algorithms are the first
with a second-order convergence in {\it high probability} and a time complexity
that is {\it almost linear} in the problem's dimensionality.


On the Convergence of (Stochastic) Gradient Descent with Extrapolation
  for Non-Convex Optimization

  Extrapolation is a well-known technique for solving convex optimization and
variational inequalities and recently attracts some attention for non-convex
optimization. Several recent works have empirically shown its success in some
machine learning tasks. However, it has not been analyzed for non-convex
minimization and there still remains a gap between the theory and the practice.
In this paper, we analyze gradient descent and stochastic gradient descent with
extrapolation for finding an approximate first-order stationary point in smooth
non-convex optimization problems. Our convergence upper bounds show that the
algorithms with extrapolation can be accelerated than without extrapolation.


Online Stochastic Optimization with Multiple Objectives

  In this paper we propose a general framework to characterize and solve the
stochastic optimization problems with multiple objectives underlying many real
world learning applications. We first propose a projection based algorithm
which attains an $O(T^{-1/3})$ convergence rate. Then, by leveraging on the
theory of Lagrangian in constrained optimization, we devise a novel primal-dual
stochastic approximation algorithm which attains the optimal convergence rate
of $O(T^{-1/2})$ for general Lipschitz continuous objectives.


A Bayesian Framework for Community Detection Integrating Content and
  Link

  This paper addresses the problem of community detection in networked data
that combines link and content analysis. Most existing work combines link and
content information by a generative model. There are two major shortcomings
with the existing approaches. First, they assume that the probability of
creating a link between two nodes is determined only by the community
memberships of the nodes; however other factors (e.g. popularity) could also
affect the link pattern. Second, they use generative models to model the
content of individual nodes, whereas these generative models are vulnerable to
the content attributes that are irrelevant to communities. We propose a
Bayesian framework for combining link and content information for community
detection that explicitly addresses these shortcomings. A new link model is
presented that introduces a random variable to capture the node popularity when
deciding the link between two nodes; a discriminative model is used to
determine the community membership of a node by its content. An approximate
inference algorithm is presented for efficient Bayesian inference. Our
empirical study shows that the proposed framework outperforms several
state-of-theart approaches in combining link and content information for
community detection.


Optimal Stochastic Strongly Convex Optimization with a Logarithmic
  Number of Projections

  We consider stochastic strongly convex optimization with a complex inequality
constraint. This complex inequality constraint may lead to computationally
expensive projections in algorithmic iterations of the stochastic gradient
descent~(SGD) methods. To reduce the computation costs pertaining to the
projections, we propose an Epoch-Projection Stochastic Gradient
Descent~(Epro-SGD) method. The proposed Epro-SGD method consists of a sequence
of epochs; it applies SGD to an augmented objective function at each iteration
within the epoch, and then performs a projection at the end of each epoch.
Given a strongly convex optimization and for a total number of $T$ iterations,
Epro-SGD requires only $\log(T)$ projections, and meanwhile attains an optimal
convergence rate of $O(1/T)$, both in expectation and with a high probability.
To exploit the structure of the optimization problem, we propose a proximal
variant of Epro-SGD, namely Epro-ORDA, based on the optimal regularized dual
averaging method. We apply the proposed methods on real-world applications; the
empirical results demonstrate the effectiveness of our methods.


A Simple Homotopy Proximal Mapping for Compressive Sensing

  In this paper, we present a novel yet simple homotopy proximal mapping
algorithm for compressive sensing. The algorithm adopts a simple proximal
mapping of the $\ell_1$ norm at each iteration and gradually reduces the
regularization parameter for the $\ell_1$ norm. We prove a global linear
convergence of the proposed homotopy proximal mapping (HPM) algorithm for
solving compressive sensing under three different settings (i) sparse signal
recovery under noiseless measurements, (ii) sparse signal recovery under noisy
measurements, and (iii) nearly-sparse signal recovery under sub-gaussian noisy
measurements. In particular, we show that when the measurement matrix satisfies
Restricted Isometric Properties (RIP), our theoretical results in settings (i)
and (ii) almost recover the best condition on the RIP constants for compressive
sensing. In addition, in setting (iii), our results for sparse signal recovery
are better than the previous results, and furthermore our analysis explicitly
exhibits that more observations lead to not only more accurate recovery but
also faster convergence. Compared with previous studies on linear convergence
for sparse signal recovery, our algorithm is simple and efficient, and our
results are better and provide more insights. Finally our empirical studies
provide further support for the proposed homotopy proximal mapping algorithm
and verify the theoretical results.


Improved Bound for the Nystrom's Method and its Application to Kernel
  Classification

  We develop two approaches for analyzing the approximation error bound for the
Nystr\"{o}m method, one based on the concentration inequality of integral
operator, and one based on the compressive sensing theory. We show that the
approximation error, measured in the spectral norm, can be improved from
$O(N/\sqrt{m})$ to $O(N/m^{1 - \rho})$ in the case of large eigengap, where $N$
is the total number of data points, $m$ is the number of sampled data points,
and $\rho \in (0, 1/2)$ is a positive constant that characterizes the eigengap.
When the eigenvalues of the kernel matrix follow a $p$-power law, our analysis
based on compressive sensing theory further improves the bound to $O(N/m^{p -
1})$ under an incoherence assumption, which explains why the Nystr\"{o}m method
works well for kernel matrix with skewed eigenvalues. We present a kernel
classification approach based on the Nystr\"{o}m method and derive its
generalization performance using the improved bound. We show that when the
eigenvalues of kernel matrix follow a $p$-power law, we can reduce the number
of support vectors to $N^{2p/(p^2 - 1)}$, a number less than $N$ when $p >
1+\sqrt{2}$, without seriously sacrificing its generalization performance.


Improved Dropout for Shallow and Deep Learning

  Dropout has been witnessed with great success in training deep neural
networks by independently zeroing out the outputs of neurons at random. It has
also received a surge of interest for shallow learning, e.g., logistic
regression. However, the independent sampling for dropout could be suboptimal
for the sake of convergence. In this paper, we propose to use multinomial
sampling for dropout, i.e., sampling features or neurons according to a
multinomial distribution with different probabilities for different
features/neurons. To exhibit the optimal dropout probabilities, we analyze the
shallow learning with multinomial dropout and establish the risk bound for
stochastic optimization. By minimizing a sampling dependent factor in the risk
bound, we obtain a distribution-dependent dropout with sampling probabilities
dependent on the second order statistics of the data distribution. To tackle
the issue of evolving distribution of neurons in deep learning, we propose an
efficient adaptive dropout (named \textbf{evolutional dropout}) that computes
the sampling probabilities on-the-fly from a mini-batch of examples. Empirical
studies on several benchmark datasets demonstrate that the proposed dropouts
achieve not only much faster convergence and but also a smaller testing error
than the standard dropout. For example, on the CIFAR-100 data, the evolutional
dropout achieves relative improvements over 10\% on the prediction performance
and over 50\% on the convergence speed compared to the standard dropout.


Unified Convergence Analysis of Stochastic Momentum Methods for Convex
  and Non-convex Optimization

  Recently, {\it stochastic momentum} methods have been widely adopted in
training deep neural networks. However, their convergence analysis is still
underexplored at the moment, in particular for non-convex optimization. This
paper fills the gap between practice and theory by developing a basic
convergence analysis of two stochastic momentum methods, namely stochastic
heavy-ball method and the stochastic variant of Nesterov's accelerated gradient
method. We hope that the basic convergence results developed in this paper can
serve the reference to the convergence of stochastic momentum methods and also
serve the baselines for comparison in future development of stochastic momentum
methods. The novelty of convergence analysis presented in this paper is a
unified framework, revealing more insights about the similarities and
differences between different stochastic momentum methods and stochastic
gradient method. The unified framework exhibits a continuous change from the
gradient method to Nesterov's accelerated gradient method and finally the
heavy-ball method incurred by a free parameter, which can help explain a
similar change observed in the testing error convergence behavior for deep
learning. Furthermore, our empirical results for optimizing deep neural
networks demonstrate that the stochastic variant of Nesterov's accelerated
gradient method achieves a good tradeoff (between speed of convergence in
training error and robustness of convergence in testing error) among the three
stochastic methods.


Tracking Slowly Moving Clairvoyant: Optimal Dynamic Regret of Online
  Learning with True and Noisy Gradient

  This work focuses on dynamic regret of online convex optimization that
compares the performance of online learning to a clairvoyant who knows the
sequence of loss functions in advance and hence selects the minimizer of the
loss function at each step. By assuming that the clairvoyant moves slowly
(i.e., the minimizers change slowly), we present several improved
variation-based upper bounds of the dynamic regret under the true and noisy
gradient feedback, which are {\it optimal} in light of the presented lower
bounds. The key to our analysis is to explore a regularity metric that measures
the temporal changes in the clairvoyant's minimizers, to which we refer as {\it
path variation}. Firstly, we present a general lower bound in terms of the path
variation, and then show that under full information or gradient feedback we
are able to achieve an optimal dynamic regret. Secondly, we present a lower
bound with noisy gradient feedback and then show that we can achieve optimal
dynamic regrets under a stochastic gradient feedback and two-point bandit
feedback. Moreover, for a sequence of smooth loss functions that admit a small
variation in the gradients, our dynamic regret under the two-point bandit
feedback matches what is achieved with full information.


Accelerated Stochastic Subgradient Methods under Local Error Bound
  Condition

  In this paper, we propose two {\bf accelerated stochastic subgradient}
methods for stochastic non-strongly convex optimization problems by leveraging
a generic local error bound condition. The novelty of the proposed methods lies
at smartly leveraging the recent historical solution to tackle the variance in
the stochastic subgradient. The key idea of both methods is to iteratively
solve the original problem approximately in a local region around a recent
historical solution with size of the local region gradually decreasing as the
solution approaches the optimal set. The difference of the two methods lies at
how to construct the local region. The first method uses an explicit ball
constraint and the second method uses an implicit regularization approach. For
both methods, we establish the improved iteration complexity in a high
probability for achieving an $\epsilon$-optimal solution. Besides the improved
order of iteration complexity with a high probability, the proposed algorithms
also enjoy a logarithmic dependence on the distance of the initial solution to
the optimal set. We also consider applications in machine learning and
demonstrate that the proposed algorithms enjoy faster convergence than the
traditional stochastic subgradient method. For example, when applied to the
$\ell_1$ regularized polyhedral loss minimization (e.g., hinge loss, absolute
loss), the proposed stochastic methods have a logarithmic iteration complexity.


Homotopy Smoothing for Non-Smooth Problems with Lower Complexity than
  $O(1/ε)$

  In this paper, we develop a novel {\bf ho}moto{\bf p}y {\bf s}moothing (HOPS)
algorithm for solving a family of non-smooth problems that is composed of a
non-smooth term with an explicit max-structure and a smooth term or a simple
non-smooth term whose proximal mapping is easy to compute. The best known
iteration complexity for solving such non-smooth optimization problems is
$O(1/\epsilon)$ without any assumption on the strong convexity. In this work,
we will show that the proposed HOPS achieved a lower iteration complexity of
$\widetilde O(1/\epsilon^{1-\theta})$\footnote{$\widetilde O()$ suppresses a
logarithmic factor.} with $\theta\in(0,1]$ capturing the local sharpness of the
objective function around the optimal solutions. To the best of our knowledge,
this is the lowest iteration complexity achieved so far for the considered
non-smooth optimization problems without strong convexity assumption. The HOPS
algorithm employs Nesterov's smoothing technique and Nesterov's accelerated
gradient method and runs in stages, which gradually decreases the smoothing
parameter in a stage-wise manner until it yields a sufficiently good
approximation of the original function. We show that HOPS enjoys a linear
convergence for many well-known non-smooth problems (e.g., empirical risk
minimization with a piece-wise linear loss function and $\ell_1$ norm
regularizer, finding a point in a polyhedron, cone programming, etc).
Experimental results verify the effectiveness of HOPS in comparison with
Nesterov's smoothing algorithm and the primal-dual style of first-order
methods.


A Richer Theory of Convex Constrained Optimization with Reduced
  Projections and Improved Rates

  This paper focuses on convex constrained optimization problems, where the
solution is subject to a convex inequality constraint. In particular, we aim at
challenging problems for which both projection into the constrained domain and
a linear optimization under the inequality constraint are time-consuming, which
render both projected gradient methods and conditional gradient methods (a.k.a.
the Frank-Wolfe algorithm) expensive. In this paper, we develop projection
reduced optimization algorithms for both smooth and non-smooth optimization
with improved convergence rates under a certain regularity condition of the
constraint function. We first present a general theory of optimization with
only one projection. Its application to smooth optimization with only one
projection yields $O(1/\epsilon)$ iteration complexity, which improves over the
$O(1/\epsilon^2)$ iteration complexity established before for non-smooth
optimization and can be further reduced under strong convexity. Then we
introduce a local error bound condition and develop faster algorithms for
non-strongly convex optimization at the price of a logarithmic number of
projections. In particular, we achieve an iteration complexity of $\widetilde
O(1/\epsilon^{2(1-\theta)})$ for non-smooth optimization and $\widetilde
O(1/\epsilon^{1-\theta})$ for smooth optimization, where $\theta\in(0,1]$
appearing the local error bound condition characterizes the functional local
growth rate around the optimal solutions. Novel applications in solving the
constrained $\ell_1$ minimization problem and a positive semi-definite
constrained distance metric learning problem demonstrate that the proposed
algorithms achieve significant speed-up compared with previous algorithms.


Adaptive Accelerated Gradient Converging Methods under Holderian Error
  Bound Condition

  Recent studies have shown that proximal gradient (PG) method and accelerated
gradient method (APG) with restarting can enjoy a linear convergence under a
weaker condition than strong convexity, namely a quadratic growth condition
(QGC). However, the faster convergence of restarting APG method relies on the
potentially unknown constant in QGC to appropriately restart APG, which
restricts its applicability. We address this issue by developing a novel
adaptive gradient converging methods, i.e., leveraging the magnitude of
proximal gradient as a criterion for restart and termination. Our analysis
extends to a much more general condition beyond the QGC, namely the
H\"{o}lderian error bound (HEB) condition. {\it The key technique} for our
development is a novel synthesis of {\it adaptive regularization and a
conditional restarting scheme}, which extends previous work focusing on
strongly convex problems to a much broader family of problems. Furthermore, we
demonstrate that our results have important implication and applications in
machine learning: (i) if the objective function is coercive and semi-algebraic,
PG's convergence speed is essentially $o(\frac{1}{t})$, where $t$ is the total
number of iterations; (ii) if the objective function consists of an $\ell_1$,
$\ell_\infty$, $\ell_{1,\infty}$, or huber norm regularization and a convex
smooth piecewise quadratic loss (e.g., squares loss, squared hinge loss and
huber loss), the proposed algorithm is parameter-free and enjoys a {\it faster
linear convergence} than PG without any other assumptions (e.g., restricted
eigen-value condition). It is notable that our linear convergence results for
the aforementioned problems are global instead of local. To the best of our
knowledge, these improved results are the first shown in this work.


NEON+: Accelerated Gradient Methods for Extracting Negative Curvature
  for Non-Convex Optimization

  Accelerated gradient (AG) methods are breakthroughs in convex optimization,
improving the convergence rate of the gradient descent method for optimization
with smooth functions. However, the analysis of AG methods for non-convex
optimization is still limited. It remains an open question whether AG methods
from convex optimization can accelerate the convergence of the gradient descent
method for finding local minimum of non-convex optimization problems. This
paper provides an affirmative answer to this question. In particular, we
analyze two renowned variants of AG methods (namely Polyak's Heavy Ball method
and Nesterov's Accelerated Gradient method) for extracting the negative
curvature from random noise, which is central to escaping from saddle points.
By leveraging the proposed AG methods for extracting the negative curvature, we
present a new AG algorithm with double loops for non-convex
optimization~\footnote{this is in contrast to a single-loop AG algorithm
proposed in a recent manuscript~\citep{AGNON}, which directly analyzed the
Nesterov's AG method for non-convex optimization and appeared online on
November 29, 2017. However, we emphasize that our work is an independent work,
which is inspired by our earlier work~\citep{NEON17} and is based on a
different novel analysis.}, which converges to second-order stationary point
$\x$ such that $\|\nabla f(\x)\|\leq \epsilon$ and $\nabla^2 f(\x)\geq
-\sqrt{\epsilon} I$ with $\widetilde O(1/\epsilon^{1.75})$ iteration
complexity, improving that of gradient descent method by a factor of
$\epsilon^{-0.25}$ and matching the best iteration complexity of second-order
Hessian-free methods for non-convex optimization.


Fast Rates of ERM and Stochastic Approximation: Adaptive to Error Bound
  Conditions

  Error bound conditions (EBC) are properties that characterize the growth of
an objective function when a point is moved away from the optimal set. They
have recently received increasing attention in the field of optimization for
developing optimization algorithms with fast convergence. However, the studies
of EBC in statistical learning are hitherto still limited. The main
contributions of this paper are two-fold. First, we develop fast and
intermediate rates of empirical risk minimization (ERM) under EBC for risk
minimization with Lipschitz continuous, and smooth convex random functions.
Second, we establish fast and intermediate rates of an efficient stochastic
approximation (SA) algorithm for risk minimization with Lipschitz continuous
random functions, which requires only one pass of $n$ samples and adapts to
EBC. For both approaches, the convergence rates span a full spectrum between
$\widetilde O(1/\sqrt{n})$ and $\widetilde O(1/n)$ depending on the power
constant in EBC, and could be even faster than $O(1/n)$ in special cases for
ERM. Moreover, these convergence rates are automatically adaptive without using
any knowledge of EBC. Overall, this work not only strengthens the understanding
of ERM for statistical learning but also brings new fast stochastic algorithms
for solving a broad range of statistical learning problems.


Solving Weakly-Convex-Weakly-Concave Saddle-Point Problems as Successive
  Strongly Monotone Variational Inequalities

  In this paper, we consider first-order algorithms for solving a class of
non-convex non-concave min-max saddle-point problems, whose objective function
is weakly convex (resp. weakly concave) in terms of the variable of
minimization (resp. maximization). It has many important applications in
machine learning, statistics, and operations research. One such example that
attracts tremendous attention recently in machine learning is training
Generative Adversarial Networks. We propose an algorithmic framework motivated
by the inexact proximal point method, which solves the weakly monotone
variational inequality corresponding to the original min-max problem by
approximately solving a sequence of strongly monotone variational inequalities
constructed by adding a strongly monotone mapping to the original gradient
mapping. In this sequence, each strongly monotone variational inequality is
defined with a proximal center that is updated using the approximate solution
of the previous variational inequality. Our algorithm generates a sequence of
solution that provably converges to a nearly stationary solution of the
original min-max problem. The proposed framework is flexible because various
subroutines can be employed for solving the strongly monotone variational
inequalities. The overall computational complexities of our methods are
established when the employed subroutines are subgradient method, stochastic
subgradient method, gradient descent method and Nesterov's accelerated method
and variance reduction methods for a Lipschitz continuous operator. To the best
of our knowledge, this is the first work that establishes the non-asymptotic
convergence to a nearly stationary point of a non-convex non-concave min-max
problem.


Stagewise Training Accelerates Convergence of Testing Error Over SGD

  Stagewise training strategy is widely used for learning neural networks,
which runs a stochastic algorithm (e.g., SGD) starting with a relatively large
step size (aka learning rate) and geometrically decreasing the step size after
a number of iterations. It has been observed that the stagewise SGD has much
faster convergence than the vanilla SGD with a polynomially decaying step size
in terms of both training error and testing error. {\it But how to explain this
phenomenon has been largely ignored by existing studies.} This paper provides
some theoretical evidence for explaining this faster convergence. In
particular, we consider a stagewise training strategy for minimizing empirical
risk that satisfies the Polyak-\L ojasiewicz (PL) condition, which has been
observed/proved for neural networks and also holds for a broad family of convex
functions. For convex loss functions and two classes of "nice-behaviored"
non-convex objectives that are close to a convex function, we establish faster
convergence of stagewise training than the vanilla SGD under the PL condition
on both training error and testing error. Experiments on stagewise learning of
deep residual networks exhibits that it satisfies one type of non-convexity
assumption and therefore can be explained by our theory. Of independent
interest, the testing error bounds for the considered non-convex loss functions
are dimensionality and norm independent.


Learning with Non-Convex Truncated Losses by SGD

  Learning with a {\it convex loss} function has been a dominating paradigm for
many years. It remains an interesting question how non-convex loss functions
help improve the generalization of learning with broad applicability. In this
paper, we study a family of objective functions formed by truncating
traditional loss functions, which is applicable to both shallow learning and
deep learning. Truncating loss functions has potential to be less vulnerable
and more robust to large noise in observations that could be adversarial. More
importantly, it is a generic technique without assuming the knowledge of noise
distribution. To justify non-convex learning with truncated losses, we
establish excess risk bounds of empirical risk minimization based on truncated
losses for heavy-tailed output, and statistical error of an approximate
stationary point found by stochastic gradient descent (SGD) method. Our
experiments for shallow and deep learning for regression with outliers,
corrupted data and heavy-tailed noise further justify the proposed method.


A Unified Analysis of Stochastic Momentum Methods for Deep Learning

  Stochastic momentum methods have been widely adopted in training deep neural
networks. However, their theoretical analysis of convergence of the training
objective and the generalization error for prediction is still under-explored.
This paper aims to bridge the gap between practice and theory by analyzing the
stochastic gradient (SG) method, and the stochastic momentum methods including
two famous variants, i.e., the stochastic heavy-ball (SHB) method and the
stochastic variant of Nesterov's accelerated gradient (SNAG) method. We propose
a framework that unifies the three variants. We then derive the convergence
rates of the norm of gradient for the non-convex optimization problem, and
analyze the generalization performance through the uniform stability approach.
Particularly, the convergence analysis of the training objective exhibits that
SHB and SNAG have no advantage over SG. However, the stability analysis shows
that the momentum term can improve the stability of the learned model and hence
improve the generalization performance. These theoretical insights verify the
common wisdom and are also corroborated by our empirical analysis on deep
learning.


Learning Discriminators as Energy Networks in Adversarial Learning

  We propose a novel framework for structured prediction via adversarial
learning. Existing adversarial learning methods involve two separate networks,
i.e., the structured prediction models and the discriminative models, in the
training. The information captured by discriminative models complements that in
the structured prediction models, but few existing researches have studied on
utilizing such information to improve structured prediction models at the
inference stage. In this work, we propose to refine the predictions of
structured prediction models by effectively integrating discriminative models
into the prediction. Discriminative models are treated as energy-based models.
Similar to the adversarial learning, discriminative models are trained to
estimate scores which measure the quality of predicted outputs, while
structured prediction models are trained to predict contrastive outputs with
maximal energy scores. In this way, the gradient vanishing problem is
ameliorated, and thus we are able to perform inference by following the ascent
gradient directions of discriminative models to refine structured prediction
models. The proposed method is able to handle a range of tasks, e.g.,
multi-label classification and image segmentation. Empirical results on these
two tasks validate the effectiveness of our learning method.


RSG: Beating Subgradient Method without Smoothness and Strong Convexity

  In this paper, we study the efficiency of a {\bf R}estarted {\bf S}ub{\bf
G}radient (RSG) method that periodically restarts the standard subgradient
method (SG). We show that, when applied to a broad class of convex optimization
problems, RSG method can find an $\epsilon$-optimal solution with a lower
complexity than the SG method. In particular, we first show that RSG can reduce
the dependence of SG's iteration complexity on the distance between the initial
solution and the optimal set to that between the $\epsilon$-level set and the
optimal set {multiplied by a logarithmic factor}. Moreover, we show the
advantages of RSG over SG in solving three different families of convex
optimization problems. (a) For the problems whose epigraph is a polyhedron, RSG
is shown to converge linearly. (b) For the problems with local quadratic growth
property in the $\epsilon$-sublevel set, RSG has an
$O(\frac{1}{\epsilon}\log(\frac{1}{\epsilon}))$ iteration complexity. (c) For
the problems that admit a local Kurdyka-\L ojasiewicz property with a power
constant of $\beta\in[0,1)$, RSG has an
$O(\frac{1}{\epsilon^{2\beta}}\log(\frac{1}{\epsilon}))$ iteration complexity.
The novelty of our analysis lies at exploiting the lower bound of the
first-order optimality residual at the $\epsilon$-level set. It is this novelty
that allows us to explore the local properties of functions (e.g., local
quadratic growth property, local Kurdyka-\L ojasiewicz property, more generally
local error bound conditions) to develop the improved convergence of RSG. { We
also develop a practical variant of RSG enjoying faster convergence than the SG
method, which can be run without knowing the involved parameters in the local
error bound condition.} We demonstrate the effectiveness of the proposed
algorithms on several machine learning tasks including regression,
classification and matrix completion.


On Noisy Negative Curvature Descent: Competing with Gradient Descent for
  Faster Non-convex Optimization

  The Hessian-vector product has been utilized to find a second-order
stationary solution with strong complexity guarantee (e.g., almost linear time
complexity in the problem's dimensionality). In this paper, we propose to
further reduce the number of Hessian-vector products for faster non-convex
optimization. Previous algorithms need to approximate the smallest eigen-value
with a sufficient precision (e.g., $\epsilon_2\ll 1$) in order to achieve a
sufficiently accurate second-order stationary solution (i.e.,
$\lambda_{\min}(\nabla^2 f(\x))\geq -\epsilon_2)$. In contrast, the proposed
algorithms only need to compute the smallest eigen-vector approximating the
corresponding eigen-value up to a small power of current gradient's norm. As a
result, it can dramatically reduce the number of Hessian-vector products during
the course of optimization before reaching first-order stationary points (e.g.,
saddle points). The key building block of the proposed algorithms is a novel
updating step named the NCG step, which lets a noisy negative curvature descent
compete with the gradient descent. We show that the worst-case time complexity
of the proposed algorithms with their favorable prescribed accuracy
requirements can match the best in literature for achieving a second-order
stationary point but with an arguably smaller per-iteration cost. We also show
that the proposed algorithms can benefit from inexact Hessian by developing
their variants accepting inexact Hessian under a mild condition for achieving
the same goal. Moreover, we develop a stochastic algorithm for a finite or
infinite sum non-convex optimization problem. To the best of our knowledge, the
proposed stochastic algorithm is the first one that converges to a second-order
stationary point in {\it high probability} with a time complexity independent
of the sample size and almost linear in dimensionality.


First-order Stochastic Algorithms for Escaping From Saddle Points in
  Almost Linear Time

  Two classes of methods have been proposed for escaping from saddle points
with one using the second-order information carried by the Hessian and the
other adding the noise into the first-order information. The existing analysis
for algorithms using noise in the first-order information is quite involved and
hides the essence of added noise, which hinder further improvements of these
algorithms. In this paper, we present a novel perspective of noise-adding
technique, i.e., adding the noise into the first-order information can help
extract the negative curvature from the Hessian matrix, and provide a formal
reasoning of this perspective by analyzing a simple first-order procedure. More
importantly, the proposed procedure enables one to design purely first-order
stochastic algorithms for escaping from non-degenerate saddle points with a
much better time complexity (almost linear time in terms of the problem's
dimensionality). In particular, we develop a {\bf first-order stochastic
algorithm} based on our new technique and an existing algorithm that only
converges to a first-order stationary point to enjoy a time complexity of
{$\widetilde O(d/\epsilon^{3.5})$ for finding a nearly second-order stationary
point $\bf{x}$ such that $\|\nabla F(bf{x})\|\leq \epsilon$ and $\nabla^2
F(bf{x})\geq -\sqrt{\epsilon}I$ (in high probability), where $F(\cdot)$ denotes
the objective function and $d$ is the dimensionality of the problem. To the
best of our knowledge, this is the best theoretical result of first-order
algorithms for stochastic non-convex optimization, which is even competitive
with if not better than existing stochastic algorithms hinging on the
second-order information.


Universal Stagewise Learning for Non-Convex Problems with Convergence on
  Averaged Solutions

  Although stochastic gradient descent (SGD) method and its variants (e.g.,
stochastic momentum methods, AdaGrad) are the choice of algorithms for solving
non-convex problems (especially deep learning), there still remain big gaps
between the theory and the practice with many questions unresolved. For
example, there is still a lack of theories of convergence for SGD and its
variants that use stagewise step size and return an averaged solution in
practice. In addition, theoretical insights of why adaptive step size of
AdaGrad could improve non-adaptive step size of {\sgd} is still missing for
non-convex optimization. This paper aims to address these questions and fill
the gap between theory and practice. We propose a universal stagewise
optimization framework for a broad family of {\bf non-smooth non-convex}
(namely weakly convex) problems with the following key features: (i) at each
stage any suitable stochastic convex optimization algorithms (e.g., SGD or
AdaGrad) that return an averaged solution can be employed for minimizing a
regularized convex problem; (ii) the step size is decreased in a stagewise
manner; (iii) an averaged solution is returned as the final solution that is
selected from all stagewise averaged solutions with sampling probabilities {\it
increasing} as the stage number. Our theoretical results of stagewise AdaGrad
exhibit its adaptive convergence, therefore shed insights on its faster
convergence for problems with sparse stochastic gradients than stagewise SGD.
To the best of our knowledge, these new results are the first of their kind for
addressing the unresolved issues of existing theories mentioned earlier.
Besides theoretical contributions, our empirical studies show that our
stagewise SGD and ADAGRAD improve the generalization performance of existing
variants/implementations of SGD and ADAGRAD.


Stochastic Optimization for DC Functions and Non-smooth Non-convex
  Regularizers with Non-asymptotic Convergence

  Difference of convex (DC) functions cover a broad family of non-convex and
possibly non-smooth and non-differentiable functions, and have wide
applications in machine learning and statistics. Although deterministic
algorithms for DC functions have been extensively studied, stochastic
optimization that is more suitable for learning with big data remains
under-explored. In this paper, we propose new stochastic optimization
algorithms and study their first-order convergence theories for solving a broad
family of DC functions. We improve the existing algorithms and theories of
stochastic optimization for DC functions from both practical and theoretical
perspectives. On the practical side, our algorithm is more user-friendly
without requiring a large mini-batch size and more efficient by saving
unnecessary computations. On the theoretical side, our convergence analysis
does not necessarily require the involved functions to be smooth with Lipschitz
continuous gradient. Instead, the convergence rate of the proposed stochastic
algorithm is automatically adaptive to the H\"{o}lder continuity of the
gradient of one component function. Moreover, we extend the proposed stochastic
algorithms for DC functions to solve problems with a general non-convex
non-differentiable regularizer, which does not necessarily have a DC
decomposition but enjoys an efficient proximal mapping. To the best of our
knowledge, this is the first work that gives the first non-asymptotic
convergence for solving non-convex optimization whose objective has a general
non-convex non-differentiable regularizer.


Efficient Constrained Regret Minimization

  Online learning constitutes a mathematical and compelling framework to
analyze sequential decision making problems in adversarial environments. The
learner repeatedly chooses an action, the environment responds with an outcome,
and then the learner receives a reward for the played action. The goal of the
learner is to maximize his total reward. However, there are situations in
which, in addition to maximizing the cumulative reward, there are some
additional constraints on the sequence of decisions that must be satisfied on
average by the learner. In this paper we study an extension to the online
learning where the learner aims to maximize the total reward given that some
additional constraints need to be satisfied. By leveraging on the theory of
Lagrangian method in constrained optimization, we propose Lagrangian
exponentially weighted average (LEWA) algorithm, which is a primal-dual variant
of the well known exponentially weighted average algorithm, to efficiently
solve constrained online decision making problems. Using novel theoretical
analysis, we establish the regret and the violation of the constraint bounds in
full information and bandit feedback models.


An Improved Bound for the Nystrom Method for Large Eigengap

  We develop an improved bound for the approximation error of the Nystr\"{o}m
method under the assumption that there is a large eigengap in the spectrum of
kernel matrix. This is based on the empirical observation that the eigengap has
a significant impact on the approximation error of the Nystr\"{o}m method. Our
approach is based on the concentration inequality of integral operator and the
theory of matrix perturbation. Our analysis shows that when there is a large
eigengap, we can improve the approximation error of the Nystr\"{o}m method from
$O(N/m^{1/4})$ to $O(N/m^{1/2})$ when measured in Frobenius norm, where $N$ is
the size of the kernel matrix, and $m$ is the number of sampled columns.


Recovering the Optimal Solution by Dual Random Projection

  Random projection has been widely used in data classification. It maps
high-dimensional data into a low-dimensional subspace in order to reduce the
computational cost in solving the related optimization problem. While previous
studies are focused on analyzing the classification performance of using random
projection, in this work, we consider the recovery problem, i.e., how to
accurately recover the optimal solution to the original optimization problem in
the high-dimensional space based on the solution learned from the subspace
spanned by random projections. We present a simple algorithm, termed Dual
Random Projection, that uses the dual solution of the low-dimensional
optimization problem to recover the optimal solution to the original problem.
Our theoretical analysis shows that with a high probability, the proposed
algorithm is able to accurately recover the optimal solution to the original
problem, provided that the data matrix is of low rank or can be well
approximated by a low rank matrix.


Sparse Multiple Kernel Learning with Geometric Convergence Rate

  In this paper, we study the problem of sparse multiple kernel learning (MKL),
where the goal is to efficiently learn a combination of a fixed small number of
kernels from a large pool that could lead to a kernel classifier with a small
prediction error. We develop an efficient algorithm based on the greedy
coordinate descent algorithm, that is able to achieve a geometric convergence
rate under appropriate conditions. The convergence rate is achieved by
measuring the size of functional gradients by an empirical $\ell_2$ norm that
depends on the empirical data distribution. This is in contrast to previous
algorithms that use a functional norm to measure the size of gradients, which
is independent from the data samples. We also establish a generalization error
bound of the learned sparse kernel classifier using the technique of local
Rademacher complexity.


O(logT) Projections for Stochastic Optimization of Smooth and Strongly
  Convex Functions

  Traditional algorithms for stochastic optimization require projecting the
solution at each iteration into a given domain to ensure its feasibility. When
facing complex domains, such as positive semi-definite cones, the projection
operation can be expensive, leading to a high computational cost per iteration.
In this paper, we present a novel algorithm that aims to reduce the number of
projections for stochastic optimization. The proposed algorithm combines the
strength of several recent developments in stochastic optimization, including
mini-batch, extra-gradient, and epoch gradient descent, in order to effectively
explore the smoothness and strong convexity. We show, both in expectation and
with a high probability, that when the objective function is both smooth and
strongly convex, the proposed algorithm achieves the optimal $O(1/T)$ rate of
convergence with only $O(\log T)$ projections. Our empirical study verifies the
theoretical result.


Analysis of Nuclear Norm Regularization for Full-rank Matrix Completion

  In this paper, we provide a theoretical analysis of the nuclear-norm
regularized least squares for full-rank matrix completion. Although similar
formulations have been examined by previous studies, their results are
unsatisfactory because only additive upper bounds are provided. Under the
assumption that the top eigenspaces of the target matrix are incoherent, we
derive a relative upper bound for recovering the best low-rank approximation of
the unknown matrix. Our relative upper bound is tighter than previous additive
bounds of other methods if the mass of the target matrix is concentrated on its
top eigenspaces, and also implies perfect recovery if it is low-rank. The
analysis is built upon the optimality condition of the regularized formulation
and existing guarantees for low-rank matrix completion. To the best of our
knowledge, this is first time such a relative bound is proved for the
regularized formulation of matrix completion.


A Simple Algorithm for Semi-supervised Learning with Improved
  Generalization Error Bound

  In this work, we develop a simple algorithm for semi-supervised regression.
The key idea is to use the top eigenfunctions of integral operator derived from
both labeled and unlabeled examples as the basis functions and learn the
prediction function by a simple linear regression. We show that under
appropriate assumptions about the integral operator, this approach is able to
achieve an improved regression error bound better than existing bounds of
supervised learning. We also verify the effectiveness of the proposed algorithm
by an empirical study.


Stochastic Proximal Gradient Descent for Nuclear Norm Regularization

  In this paper, we utilize stochastic optimization to reduce the space
complexity of convex composite optimization with a nuclear norm regularizer,
where the variable is a matrix of size $m \times n$. By constructing a low-rank
estimate of the gradient, we propose an iterative algorithm based on stochastic
proximal gradient descent (SPGD), and take the last iterate of SPGD as the
final solution. The main advantage of the proposed algorithm is that its space
complexity is $O(m+n)$, in contrast, most of previous algorithms have a $O(mn)$
space complexity. Theoretical analysis shows that it achieves $O(\log
T/\sqrt{T})$ and $O(\log T/T)$ convergence rates for general convex functions
and strongly convex functions, respectively.


Sparse Learning for Large-scale and High-dimensional Data: A Randomized
  Convex-concave Optimization Approach

  In this paper, we develop a randomized algorithm and theory for learning a
sparse model from large-scale and high-dimensional data, which is usually
formulated as an empirical risk minimization problem with a sparsity-inducing
regularizer. Under the assumption that there exists a (approximately) sparse
solution with high classification accuracy, we argue that the dual solution is
also sparse or approximately sparse. The fact that both primal and dual
solutions are sparse motivates us to develop a randomized approach for a
general convex-concave optimization problem. Specifically, the proposed
approach combines the strength of random projection with that of sparse
learning: it utilizes random projection to reduce the dimensionality, and
introduces $\ell_1$-norm regularization to alleviate the approximation error
caused by random projection. Theoretical analysis shows that under favored
conditions, the randomized algorithm can accurately recover the optimal
solutions to the convex-concave optimization problem (i.e., recover both the
primal and dual solutions).


Efficient Feature Screening for Lasso-Type Problems via Hybrid
  Safe-Strong Rules

  The lasso model has been widely used for model selection in data mining,
machine learning, and high-dimensional statistical analysis. However, due to
the ultrahigh-dimensional, large-scale data sets collected in many real-world
applications, it remains challenging to solve the lasso problems even with
state-of-the-art algorithms. Feature screening is a powerful technique for
addressing the Big Data challenge by discarding inactive features from the
lasso optimization. In this paper, we propose a family of hybrid safe-strong
rules (HSSR) which incorporate safe screening rules into the sequential strong
rule (SSR) to remove unnecessary computational burden. In particular, we
present two instances of HSSR, namely SSR-Dome and SSR-BEDPP, for the standard
lasso problem. We further extend SSR-BEDPP to the elastic net and group lasso
problems to demonstrate the generalizability of the hybrid screening idea.
Extensive numerical experiments with synthetic and real data sets are conducted
for both the standard lasso and the group lasso problems. Results show that our
proposed hybrid rules substantially outperform existing state-of-the-art rules.


Distributed Stochastic Variance Reduced Gradient Methods and A Lower
  Bound for Communication Complexity

  We study distributed optimization algorithms for minimizing the average of
convex functions. The applications include empirical risk minimization problems
in statistical machine learning where the datasets are large and have to be
stored on different machines. We design a distributed stochastic variance
reduced gradient algorithm that, under certain conditions on the condition
number, simultaneously achieves the optimal parallel runtime, amount of
communication and rounds of communication among all distributed first-order
methods up to constant factors. Our method and its accelerated extension also
outperform existing distributed algorithms in terms of the rounds of
communication as long as the condition number is not too large compared to the
size of data in each machine. We also prove a lower bound for the number of
rounds of communication for a broad class of distributed first-order methods
including the proposed algorithms in this paper. We show that our accelerated
distributed stochastic variance reduced gradient algorithm achieves this lower
bound so that it uses the fewest rounds of communication among all distributed
first-order algorithms.


Doubly Stochastic Primal-Dual Coordinate Method for Bilinear
  Saddle-Point Problem

  We propose a doubly stochastic primal-dual coordinate optimization algorithm
for empirical risk minimization, which can be formulated as a bilinear
saddle-point problem. In each iteration, our method randomly samples a block of
coordinates of the primal and dual solutions to update. The linear convergence
of our method could be established in terms of 1) the distance from the current
iterate to the optimal solution and 2) the primal-dual objective gap. We show
that the proposed method has a lower overall complexity than existing
coordinate methods when either the data matrix has a factorized structure or
the proximal mapping on each block is computationally expensive, e.g.,
involving an eigenvalue decomposition. The efficiency of the proposed method is
confirmed by empirical studies on several real applications, such as the
multi-task large margin nearest neighbor problem.


Online Stochastic Linear Optimization under One-bit Feedback

  In this paper, we study a special bandit setting of online stochastic linear
optimization, where only one-bit of information is revealed to the learner at
each round. This problem has found many applications including online
advertisement and online recommendation. We assume the binary feedback is a
random variable generated from the logit model, and aim to minimize the regret
defined by the unknown linear function. Although the existing method for
generalized linear bandit can be applied to our problem, the high computational
cost makes it impractical for real-world problems. To address this challenge,
we develop an efficient online learning algorithm by exploiting particular
structures of the observation model. Specifically, we adopt online Newton step
to estimate the unknown parameter and derive a tight confidence region based on
the exponential concavity of the logistic loss. Our analysis shows that the
proposed algorithm achieves a regret bound of $O(d\sqrt{T})$, which matches the
optimal result of stochastic linear bandits.


