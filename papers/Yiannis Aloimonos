Revisiting Active Perception

  Despite the recent successes in robotics, artificial intelligence andcomputer vision, a complete artificial agent necessarily must include activeperception. A multitude of ideas and methods for how to accomplish this havealready appeared in the past, their broader utility perhaps impeded byinsufficient computational power or costly hardware. The history of theseideas, perhaps selective due to our perspectives, is presented with the goal oforganizing the past literature and highlighting the seminal contributions. Weargue that those contributions are as relevant today as they were decades agoand, with the state of modern computational tools, are poised to find new lifein the robotic perception systems of the next decade.

From Images to Sentences through Scene Description Graphs using  Commonsense Reasoning and Knowledge

  In this paper we propose the construction of linguistic descriptions ofimages. This is achieved through the extraction of scene description graphs(SDGs) from visual scenes using an automatically constructed knowledge base.SDGs are constructed using both vision and reasoning. Specifically, commonsensereasoning is applied on (a) detections obtained from existing perceptionmethods on given images, (b) a "commonsense" knowledge base constructed usingnatural language processing of image annotations and (c) lexical ontologicalknowledge from resources such as WordNet. Amazon Mechanical Turk(AMT)-basedevaluations on Flickr8k, Flickr30k and MS-COCO datasets show that in mostcases, sentences auto-constructed from SDGs obtained by our method give a morerelevant and thorough description of an image than a recent state-of-the-artimage caption based approach. Our Image-Sentence Alignment Evaluation resultsare also comparable to that of the recent state-of-the art approaches.

Learning the Semantics of Manipulation Action

  In this paper we present a formal computational framework for modelingmanipulation actions. The introduced formalism leads to semantics ofmanipulation action and has applications to both observing and understandinghuman manipulation actions as well as executing them with a robotic mechanism(e.g. a humanoid robot). It is based on a Combinatory Categorial Grammar. Thegoal of the introduced framework is to: (1) represent manipulation actions withboth syntax and semantic parts, where the semantic part employs$\lambda$-calculus; (2) enable a probabilistic semantic parsing schema to learnthe $\lambda$-calculus representation of manipulation action from an annotatedaction corpus of videos; (3) use (1) and (2) to develop a system that visuallyobserves manipulation actions and understands their meaning while it can reasonbeyond observations using propositional logic and axiom schemata. Theexperiments conducted on a public available large manipulation action datasetvalidate the theoretical framework and our implementation.

Neural Self Talk: Image Understanding via Continuous Questioning and  Answering

  In this paper we consider the problem of continuously discovering imagecontents by actively asking image based questions and subsequently answeringthe questions being asked. The key components include a Visual QuestionGeneration (VQG) module and a Visual Question Answering module, in whichRecurrent Neural Networks (RNN) and Convolutional Neural Network (CNN) areused. Given a dataset that contains images, questions and their answers, bothmodules are trained at the same time, with the difference being VQG uses theimages as input and the corresponding questions as output, while VQA usesimages and questions as input and the corresponding answers as output. Weevaluate the self talk process subjectively using Amazon Mechanical Turk, whichshow effectiveness of the proposed method.

What Can I Do Around Here? Deep Functional Scene Understanding for  Cognitive Robots

  For robots that have the capability to interact with the physical environmentthrough their end effectors, understanding the surrounding scenes is not merelya task of image classification or object recognition. To perform actual tasks,it is critical for the robot to have a functional understanding of the visualscene. Here, we address the problem of localizing and recognition of functionalareas from an arbitrary indoor scene, formulated as a two-stage deep learningbased detection pipeline. A new scene functionality testing-bed, which iscomplied from two publicly available indoor scene datasets, is used forevaluation. Our method is evaluated quantitatively on the new dataset,demonstrating the ability to perform efficient recognition of functional areasfrom arbitrary indoor scenes. We also demonstrate that our detection model canbe generalized onto novel indoor scenes by cross validating it with the imagesfrom two different datasets.

LightNet: A Versatile, Standalone Matlab-based Environment for Deep  Learning

  LightNet is a lightweight, versatile and purely Matlab-based deep learningframework. The idea underlying its design is to provide an easy-to-understand,easy-to-use and efficient computational platform for deep learning research.The implemented framework supports major deep learning architectures such asMultilayer Perceptron Networks (MLP), Convolutional Neural Networks (CNN) andRecurrent Neural Networks (RNN). The framework also supports both CPU and GPUcomputation, and the switch between them is straightforward. Differentapplications in computer vision, natural language processing and robotics aredemonstrated as experiments.

Answering Image Riddles using Vision and Reasoning through Probabilistic  Soft Logic

  In this work, we explore a genre of puzzles ("image riddles") which involvesa set of images and a question. Answering these puzzles require bothcapabilities involving visual detection (including object, activityrecognition) and, knowledge-based or commonsense reasoning. We compile adataset of over 3k riddles where each riddle consists of 4 images and agroundtruth answer. The annotations are validated using crowd-sourcedevaluation. We also define an automatic evaluation metric to track futureprogress. Our task bears similarity with the commonly known IQ tasks such asanalogy solving, sequence filling that are often used to test intelligence.  We develop a Probabilistic Reasoning-based approach that utilizesprobabilistic commonsense knowledge to answer these riddles with a reasonableaccuracy. We demonstrate the results of our approach using both automatic andhuman evaluations. Our approach achieves some promising results for theseriddles and provides a strong baseline for future attempts. We make the entiredataset and related materials publicly available to the community inImageRiddle Website (http://bit.ly/22f9Ala).

On the Importance of Consistency in Training Deep Neural Networks

  We explain that the difficulties of training deep neural networks come from asyndrome of three consistency issues. This paper describes our efforts in theiranalysis and treatment. The first issue is the training speed inconsistency indifferent layers. We propose to address it with an intuitive,simple-to-implement, low footprint second-order method. The second issue is thescale inconsistency between the layer inputs and the layer residuals. Weexplain how second-order information provides favorable convenience in removingthis roadblock. The third and most challenging issue is the inconsistency inresidual propagation. Based on the fundamental theorem of linear algebra, weprovide a mathematical characterization of the famous vanishing gradientproblem. Thus, an important design principle for future optimization and neuralnetwork design is derived. We conclude this paper with the construction of anovel contractive neural network.

Joint direct estimation of 3D geometry and 3D motion using spatio  temporal gradients

  Conventional image motion based structure from motion methods first computeoptical flow, then solve for the 3D motion parameters based on the epipolarconstraint, and finally recover the 3D geometry of the scene. However, errorsin optical flow due to regularization can lead to large errors in 3D motion andstructure. This paper investigates whether performance and consistency can beimproved by avoiding optical flow estimation in the early stages of thestructure from motion pipeline, and it proposes a new direct method based onimage gradients (normal flow) only. The main idea lies in a reformulation ofthe positive-depth constraint, which allows the use of well-known minimizationtechniques to solve for 3D motion. The 3D motion estimate is then refined andstructure estimated adding a regularization based on depth. Experimentalcomparisons on standard synthetic datasets and the real-world driving benchmarkdataset KITTI using three different optic flow algorithms show that the methodachieves better accuracy in all but one case. Furthermore, it outperformsexisting normal flow based 3D motion estimation techniques. Finally, therecovered 3D geometry is shown to be also very accurate.

A Computational Theory for Life-Long Learning of Semantics

  Semantic vectors are learned from data to express semantic relationshipsbetween elements of information, for the purpose of solving and informingdownstream tasks. Other models exist that learn to map and classify superviseddata. However, the two worlds of learning rarely interact to inform one anotherdynamically, whether across types of data or levels of semantics, in order toform a unified model. We explore the research problem of learning these vectorsand propose a framework for learning the semantics of knowledge incrementallyand online, across multiple mediums of data, via binary vectors. We discuss theaspects of this framework to spur future research on this approach and problem.

cilantro: A Lean, Versatile, and Efficient Library for Point Cloud Data  Processing

  We introduce cilantro, an open-source C++ library for geometric andgeneral-purpose point cloud data processing. The library provides functionalitythat covers low-level point cloud operations, spatial reasoning, variousmethods for point cloud segmentation and generic data clustering, flexiblealgorithms for robust or local geometric alignment, model fitting, as well aspowerful visualization tools. To accommodate all kinds of workflows, cilantrois almost fully templated, and most of its generic algorithms operate inarbitrary data dimension. At the same time, the library is easy to use andhighly expressive, promoting a clean and concise coding style. cilantro ishighly optimized, has a minimal set of external dependencies, and supportsrapid development of performant point cloud processing software in a widevariety of contexts.

Evenly Cascaded Convolutional Networks

  We introduce Evenly Cascaded convolutional Network (ECN), a neural networktaking inspiration from the cascade algorithm of wavelet analysis. ECN employstwo feature streams - a low-level and high-level steam. At each layer thesestreams interact, such that low-level features are modulated using advancedperspectives from the high-level stream. ECN is evenly structured throughresizing feature map dimensions by a consistent ratio, which removes the burdenof ad-hoc specification of feature map dimensions. ECN produces easilyinterpretable features maps, a result whose intuition can be understood in thecontext of scale-space theory. We demonstrate that ECN's design facilitates thetraining process through providing easily trainable shortcuts. We report newstate-of-the-art results for small networks, without the need for additionaltreatment such as pruning or compression - a consequence of ECN's simplestructure and direct training. A 6-layered ECN design with under 500kparameters achieves 95.24% and 78.99% accuracy on CIFAR-10 and CIFAR-100datasets, respectively, outperforming the current state-of-the-art on smallparameter networks, and a 3 million parameter ECN produces results competitiveto the state-of-the-art.

Unsupervised Learning of Dense Optical Flow, Depth and Egomotion from  Sparse Event Data

  In this work we present a lightweight, unsupervised learning pipeline for\textit{dense} depth, optical flow and egomotion estimation from sparse eventoutput of the Dynamic Vision Sensor (DVS). To tackle this low level visiontask, we use a novel encoder-decoder neural network architecture - ECN.  Our work is the first monocular pipeline that generates dense depth andoptical flow from sparse event data only. The network works in self-supervisedmode and has just 150k parameters. We evaluate our pipeline on the MVSEC selfdriving dataset and present results for depth, optical flow and and egomotionestimation. Due to the lightweight design, the inference part of the networkruns at 250 FPS on a single GPU, making the pipeline ready for realtimerobotics applications. Our experiments demonstrate significant improvementsupon previous works that used deep learning on event data, as well as theability of our pipeline to perform well during both day and night.

Computational Tactile Flow for Anthropomorphic Grippers

  Grasping objects requires tight integration between visual and tactilefeedback. However, there is an inherent difference in the scale at which boththese input modalities operate. It is thus necessary to be able to analyzetactile feedback in isolation in order to gain information about the surfacethe end-effector is operating on, such that more fine-grained features may beextracted from the surroundings. For tactile perception of the robot, inspiredby the concept of the tactile flow in humans, we present the computationaltactile flow to improve the analysis of the tactile feedback in robots using aShadow Dexterous Hand.  In the computational tactile flow model, given a sequence of pressure valuesfrom the tactile sensors, we define a virtual surface for the pressure valuesand define the tactile flow as the optical flow of this surface. We providecase studies that demonstrate how the computational tactile flow maps revealinformation on the direction of motion and 3D structure of the surface, andfeedback regarding the action being performed by the robot.

GapFlyt: Active Vision Based Minimalist Structure-less Gap Detection For  Quadrotor Flight

  Although quadrotors, and aerial robots in general, are inherently activeagents, their perceptual capabilities in literature so far have been mostlypassive in nature. Researchers and practitioners today use traditional computervision algorithms with the aim of building a representation of generalapplicability: a 3D reconstruction of the scene. Using this representation,planning tasks are constructed and accomplished to allow the quadrotor todemonstrate autonomous behavior. These methods are inefficient as they are nottask driven and such methodologies are not utilized by flying insects andbirds. Such agents have been solving the problem of navigation and complexcontrol for ages without the need to build a 3D map and are highly task driven.In this paper, we propose this framework of bio-inspired perceptual design forquadrotors. We use this philosophy to design a minimalist sensori-motorframework for a quadrotor to fly though unknown gaps without a 3Dreconstruction of the scene using only a monocular camera and onboard sensing.We successfully evaluate and demonstrate the proposed approach in manyreal-world experiments with different settings and window shapes, achieving asuccess rate of 85% at 2.5ms$^{-1}$ even with a minimum tolerance of just 5cm.To our knowledge, this is the first paper which addresses the problem of gapdetection of an unknown shape and location with a monocular camera and onboardsensing.

SalientDSO: Bringing Attention to Direct Sparse Odometry

  Although cluttered indoor scenes have a lot of useful high-level semanticinformation which can be used for mapping and localization, most VisualOdometry (VO) algorithms rely on the usage of geometric features such aspoints, lines and planes. Lately, driven by this idea, the joint optimizationof semantic labels and obtaining odometry has gained popularity in the roboticscommunity. The joint optimization is good for accurate results but is generallyvery slow. At the same time, in the vision community, direct and sparseapproaches for VO have stricken the right balance between speed and accuracy.  We merge the successes of these two communities and present a way toincorporate semantic information in the form of visual saliency to DirectSparse Odometry - a highly successful direct sparse VO algorithm. We alsopresent a framework to filter the visual saliency based on scene parsing. Ourframework, SalientDSO, relies on the widely successful deep learning basedapproaches for visual saliency and scene parsing which drives the featureselection for obtaining highly-accurate and robust VO even in the presence ofas few as 40 point features per frame. We provide extensive quantitativeevaluation of SalientDSO on the ICL-NUIM and TUM monoVO datasets and show thatwe outperform DSO and ORB-SLAM - two very popular state-of-the-art approachesin the literature. We also collect and publicly release a CVL-UMD dataset whichcontains two indoor cluttered sequences on which we show qualitativeevaluations. To our knowledge this is the first paper to use visual saliencyand scene parsing to drive the feature selection in direct VO.

Event-based Moving Object Detection and Tracking

  Event-based vision sensors, such as the Dynamic Vision Sensor (DVS), areideally suited for real-time motion analysis. The unique properties encompassedin the readings of such sensors provide high temporal resolution, superiorsensitivity to light and low latency. These properties provide the grounds toestimate motion extremely reliably in the most sophisticated scenarios but theycome at a price - modern event-based vision sensors have extremely lowresolution and produce a lot of noise. Moreover, the asynchronous nature of theevent stream calls for novel algorithms.  This paper presents a new, efficient approach to object tracking withasynchronous cameras. We present a novel event stream representation whichenables us to utilize information about the dynamic (temporal) component of theevent stream, and not only the spatial component, at every moment of time. Thisis done by approximating the 3D geometry of the event stream with a parametricmodel; as a result, the algorithm is capable of producing themotion-compensated event stream (effectively approximating egomotion), andwithout using any form of external sensors in extremely low-light and noisyconditions without any form of feature tracking or explicit optical flowcomputation. We demonstrate our framework on the task of independent motiondetection and tracking, where we use the temporal model inconsistencies tolocate differently moving objects in challenging situations of very fastmotion.

Extracting Contact and Motion from Manipulation Videos

  When we physically interact with our environment using our hands, we touchobjects and force them to move: contact and motion are defining properties ofmanipulation. In this paper, we present an active, bottom-up method for thedetection of actor-object contacts and the extraction of moved objects andtheir motions in RGBD videos of manipulation actions. At the core of ourapproach lies non-rigid registration: we continuously warp a point cloud modelof the observed scene to the current video frame, generating a set of dense 3Dpoint trajectories. Under loose assumptions, we employ simple point cloudsegmentation techniques to extract the actor and subsequently detectactor-environment contacts based on the estimated trajectories. For each suchinteraction, using the detected contact as an attention mechanism, we obtain aninitial motion segment for the manipulated object by clustering trajectories inthe contact area vicinity and then we jointly refine the object segment andestimate its 6DOF pose in all observed frames. Because of its generality andthe fundamental, yet highly informative, nature of its outputs, our approach isapplicable to a wide range of perception and planning tasks. We qualitativelyevaluate our method on a number of input sequences and present a comprehensiverobot imitation learning example, in which we demonstrate the crucial role ofour outputs in developing action representations/plans from observation.

Topology-Aware Non-Rigid Point Cloud Registration

  In this paper, we introduce a non-rigid registration pipeline for pairs ofunorganized point clouds that may be topologically different. Standard warpfield estimation algorithms, even under robust, discontinuity-preservingregularization, tend to produce erratic motion estimates on boundariesassociated with `close-to-open' topology changes. We overcome this limitationby exploiting backward motion: in the opposite motion direction, a`close-to-open' event becomes `open-to-close', which is by default handledcorrectly. At the core of our approach lies a general, topology-agnostic warpfield estimation algorithm, similar to those employed in recently introduceddynamic reconstruction systems from RGB-D input. We improve motion estimationon boundaries associated with topology changes in an efficient post-processingphase. Based on both forward and (inverted) backward warp hypotheses, weexplicitly detect regions of the deformed geometry that undergo topologicalchanges by means of local deformation criteria and broadly classify them as`contacts' or `separations'. Subsequently, the two motion hypotheses areseamlessly blended on a local basis, according to the type and proximity ofdetected events. Our method achieves state-of-the-art motion estimationaccuracy on the MPI Sintel dataset. Experiments on a custom dataset withtopological event annotations demonstrate the effectiveness of our pipeline inestimating motion on event boundaries, as well as promising performance inexplicit topological event detection.

EV-IMO: Motion Segmentation Dataset and Learning Pipeline for Event  Cameras

  We present the first event-based learning approach for motion segmentation inindoor scenes and the first event-based dataset - EV-IMO - which includesaccurate pixel-wise motion masks, egomotion and ground truth depth. Ourapproach is based on an efficient implementation of the SfM learning pipelineusing a low parameter neural network architecture on event data. In addition tocamera egomotion and a dense depth map, the network estimates pixel-wiseindependently moving object segmentation and computes per-object 3Dtranslational velocities for moving objects. We also train a shallow networkwith just 40k parameters, which is able to compute depth and egomotion.  Our EV-IMO dataset features 32 minutes of indoor recording with up to 3 fastmoving objects simultaneously in the camera field of view. The objects and thecamera are tracked by the VICON motion capture system. By 3D scanning the roomand the objects, accurate depth map ground truth and pixel-wise object masksare obtained, which are reliable even in poor lighting conditions and duringfast motion. We then train and evaluate our learning pipeline on EV-IMO anddemonstrate that our approach far surpasses its rivals and is well suited forscene constrained robotics applications.

