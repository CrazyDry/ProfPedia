On the Complexity of Solving Markov Decision Problems

  Markov decision problems (MDPs) provide the foundations for a number ofproblems of interest to AI researchers studying automated planning andreinforcement learning. In this paper, we summarize results regarding thecomplexity of solving MDPs and the running time of MDP solution algorithms. Weargue that, although MDPs can be solved efficiently in theory, more study isneeded to reveal practical algorithms for solving large problems quickly. Toencourage future research, we sketch some alternative methods of analysis thatrely on the structure of MDPs.

Environment-Independent Task Specifications via GLTL

  We propose a new task-specification language for Markov decision processesthat is designed to be an improvement over reward functions by beingenvironment independent. The language is a variant of Linear Temporal Logic(LTL) that is extended to probabilistic specifications in a way that permitsapproximations to be learned in finite time. We provide several smallenvironments that demonstrate the advantages of our geometric LTL (GLTL)language and illustrate how it can be used to specify standardreinforcement-learning tasks straightforwardly.

Learning Approximate Stochastic Transition Models

  We examine the problem of learning mappings from state to state, suitable foruse in a model-based reinforcement-learning setting, that simultaneouslygeneralize to novel states and can capture stochastic transitions. We show thatcurrently popular generative adversarial networks struggle to learn thesestochastic transition models but a modification to their loss functions resultsin a powerful learning algorithm for this class of problems.

An Efficient Optimal-Equilibrium Algorithm for Two-player Game Trees

  Two-player complete-information game trees are perhaps the simplest possiblesetting for studying general-sum games and the computational problem of findingequilibria. These games admit a simple bottom-up algorithm for finding subgameperfect Nash equilibria efficiently. However, such an algorithm can fail toidentify optimal equilibria, such as those that maximize social welfare. Thereason is that, counterintuitively, probabilistic action choices are sometimesneeded to achieve maximum payoffs. We provide a novel polynomial-time algorithmfor this problem that explicitly reasons about stochastic decisions anddemonstrate its use in an example card game.

On the Computational Complexity of Stochastic Controller Optimization in  POMDPs

  We show that the problem of finding an optimal stochastic 'blind' controllerin a Markov decision process is an NP-hard problem. The corresponding decisionproblem is NP-hard, in PSPACE, and SQRT-SUM-hard, hence placing it in NP wouldimply breakthroughs in long-standing open problems in computer science. Ourresult establishes that the more general problem of stochastic controlleroptimization in POMDPs is also NP-hard. Nonetheless, we outline a special casethat is convex and admits efficient global solutions.

Incremental Pruning: A Simple, Fast, Exact Method for Partially  Observable Markov Decision Processes

  Most exact algorithms for general partially observable Markov decisionprocesses (POMDPs) use a form of dynamic programming in which apiecewise-linear and convex representation of one value function is transformedinto another. We examine variations of the "incremental pruning" method forsolving this problem and compare them to earlier algorithms from theoreticaland empirical perspectives. We find that incremental pruning is presently themost efficient exact method for solving POMDPs.

The Complexity of Plan Existence and Evaluation in Probabilistic Domains

  We examine the computational complexity of testing and finding small plans inprobabilistic planning domains with succinct representations. We find that manyproblems of interest are complete for a variety of complexity classes: NP,co-NP, PP, NP^PP, co-NP^PP, and PSPACE. Of these, the probabilistic classes PPand NP^PP are likely to be of special interest in the field of uncertainty inartificial intelligence and are deserving of additional study. These resultssuggest a fruitful direction of future algorithmic development.

Advantages and Limitations of using Successor Features for Transfer in  Reinforcement Learning

  One question central to Reinforcement Learning is how to learn a featurerepresentation that supports algorithm scaling and re-use of learnedinformation from different tasks. Successor Features approach this problem bylearning a feature representation that satisfies a temporal constraint. Wepresent an implementation of an approach that decouples the featurerepresentation from the reward function, making it suitable for transferringknowledge between domains. We then assess the advantages and limitations ofusing Successor Features for transfer.

Lipschitz Continuity in Model-based Reinforcement Learning

  We examine the impact of learning Lipschitz continuous models in the contextof model-based reinforcement learning. We provide a novel bound on multi-stepprediction error of Lipschitz models where we quantify the error using theWasserstein metric. We go on to prove an error bound for the value-functionestimate arising from Lipschitz models and show that the estimated valuefunction is itself Lipschitz. We conclude with empirical results that show thebenefits of controlling the Lipschitz constant of neural-network models.

Towards a Simple Approach to Multi-step Model-based Reinforcement  Learning

  When environmental interaction is expensive, model-based reinforcementlearning offers a solution by planning ahead and avoiding costly mistakes.Model-based agents typically learn a single-step transition model. In thispaper, we propose a multi-step model that predicts the outcome of an actionsequence with variable length. We show that this model is easy to learn, andthat the model can make policy-conditional predictions. We report preliminaryresults that show a clear advantage for the multi-step model compared to itsone-step counterpart.

Incremental Model-based Learners With Formal Learning-Time Guarantees

  Model-based learning algorithms have been shown to use experience efficientlywhen learning to solve Markov Decision Processes (MDPs) with finite state andaction spaces. However, their high computational cost due to repeatedly solvingan internal model inhibits their use in large-scale problems. We propose amethod based on real-time dynamic programming (RTDP) to speed up twomodel-based algorithms, RMAX and MBIE (model-based interval estimation),resulting in computationally much faster algorithms with little loss comparedto existing bounds. Specifically, our two new learning algorithms, RTDP-RMAXand RTDP-IE, have considerably smaller computational demands than RMAX andMBIE. We develop a general theoretical framework that allows us to prove thatboth are efficient learners in a PAC (probably approximately correct) sense. Wealso present an experimental evaluation of these new algorithms that helpsquantify the tradeoff between computational and experience demands.

Modeling Latent Attention Within Neural Networks

  Deep neural networks are able to solve tasks across a variety of domains andmodalities of data. Despite many empirical successes, we lack the ability toclearly understand and interpret the learned internal mechanisms thatcontribute to such effective behaviors or, more critically, failure modes. Inthis work, we present a general method for visualizing an arbitrary neuralnetwork's inner mechanisms and their power and limitations. Our dataset-centricmethod produces visualizations of how a trained network attends to componentsof its inputs. The computed "attention masks" support improved interpretabilityby highlighting which input attributes are critical in determining output. Wedemonstrate the effectiveness of our framework on a variety of deep neuralnetwork architectures in domains from computer vision, natural languageprocessing, and reinforcement learning. The primary contribution of ourapproach is an interpretable visualization of attention that provides uniqueinsights into the network's underlying decision-making process irrespective ofthe data modality.

Unsupervised Learning of Semantic Orientation from a  Hundred-Billion-Word Corpus

  The evaluative character of a word is called its semantic orientation. Apositive semantic orientation implies desirability (e.g., "honest", "intrepid")and a negative semantic orientation implies undesirability (e.g., "disturbing","superfluous"). This paper introduces a simple algorithm for unsupervisedlearning of semantic orientation from extremely large corpora. The methodinvolves issuing queries to a Web search engine and using pointwise mutualinformation to analyse the results. The algorithm is empirically evaluatedusing a training corpus of approximately one hundred billion words -- thesubset of the Web that is indexed by the chosen search engine. Tested with3,596 words (1,614 positive and 1,982 negative), the algorithm attains anaccuracy of 80%. The 3,596 test words include adjectives, adverbs, nouns, andverbs. The accuracy is comparable with the results achieved by Hatzivassiloglouand McKeown (1997), using a complex four-stage supervised learning algorithmthat is restricted to determining the semantic orientation of adjectives.

Combining Independent Modules to Solve Multiple-choice Synonym and  Analogy Problems

  Existing statistical approaches to natural language problems are very coarseapproximations to the true complexity of language processing. As such, nosingle technique will be best for all problem instances. Many researchers areexamining ensemble methods that combine the output of successful, separatelydeveloped modules to create more accurate solutions. This paper examines threemerging rules for combining probability distributions: the well known mixturerule, the logarithmic rule, and a novel product rule. These rules were appliedwith state-of-the-art results to two problems commonly used to assess humanmastery of lexical semantics -- synonym questions and analogy questions. Allthree merging rules result in ensembles that are more accurate than any oftheir component modules. The differences among the three rules are notstatistically significant, but it is suggestive that the popular mixture ruleis not the best rule for either of the two problems.

Combining Independent Modules in Lexical Multiple-Choice Problems

  Existing statistical approaches to natural language problems are very coarseapproximations to the true complexity of language processing. As such, nosingle technique will be best for all problem instances. Many researchers areexamining ensemble methods that combine the output of multiple modules tocreate more accurate solutions. This paper examines three merging rules forcombining probability distributions: the familiar mixture rule, the logarithmicrule, and a novel product rule. These rules were applied with state-of-the-artresults to two problems used to assess human mastery of lexical semantics --synonym questions and analogy questions. All three merging rules result inensembles that are more accurate than any of their component modules. Thedifferences among the three rules are not statistically significant, but it issuggestive that the popular mixture rule is not the best rule for either of thetwo problems.

Learning is planning: near Bayes-optimal reinforcement learning via  Monte-Carlo tree search

  Bayes-optimal behavior, while well-defined, is often difficult to achieve.Recent advances in the use of Monte-Carlo tree search (MCTS) have shown that itis possible to act near-optimally in Markov Decision Processes (MDPs) with verylarge or infinite state spaces. Bayes-optimal behavior in an unknown MDP isequivalent to optimal behavior in the known belief-space MDP, although the sizeof this belief-space MDP grows exponentially with the amount of historyretained, and is potentially infinite. We show how an agent can use oneparticular MCTS algorithm, Forward Search Sparse Sampling (FSSS), in anefficient way to act nearly Bayes-optimally for all but a polynomial number ofsteps, assuming that FSSS can be used to act efficiently in any possibleunderlying MDP.

Exploring compact reinforcement-learning representations with linear  regression

  This paper presents a new algorithm for online linear regression whoseefficiency guarantees satisfy the requirements of the KWIK (Knows What ItKnows) framework. The algorithm improves on the complexity bounds of thecurrent state-of-the-art procedure in this setting. We explore severalapplications of this algorithm for learning compact reinforcement-learningrepresentations. We show that KWIK linear regression can be used to learn thereward function of a factored MDP and the probabilities of action outcomes inStochastic STRIPS and Object Oriented MDPs, none of which have been proven tobe efficiently learnable in the RL setting before. We also combine KWIK linearregression with other KWIK learners to learn larger portions of these models,including experiments on learning factored MDP transition and reward functionstogether.

A Bayesian Sampling Approach to Exploration in Reinforcement Learning

  We present a modular approach to reinforcement learning that uses a Bayesianrepresentation of the uncertainty over models. The approach, BOSS (Best ofSampled Set), drives exploration by sampling multiple models from the posteriorand selecting actions optimistically. It extends previous work by providing arule for deciding when to resample and how to combine the models. We show thatour algorithm achieves nearoptimal reward with high probability with a samplecomplexity that is low relative to the speed at which the posteriordistribution converges during learning. We demonstrate that BOSS performs quitefavorably compared to state-of-the-art reinforcement-learning approaches andillustrate its flexibility by pairing it with a non-parametric model thatgeneralizes across states.

CORL: A Continuous-state Offset-dynamics Reinforcement Learner

  Continuous state spaces and stochastic, switching dynamics characterize anumber of rich, realworld domains, such as robot navigation across varyingterrain. We describe a reinforcementlearning algorithm for learning in thesedomains and prove for certain environments the algorithm is probablyapproximately correct with a sample complexity that scales polynomially withthe state-space dimension. Unfortunately, no optimal planning techniques existin general for such problems; instead we use fitted value iteration to solvethe learned MDP, and include the error due to approximate planning in ourbounds. Finally, we report an experiment using a robotic car driving overvarying terrain to demonstrate that these dynamics representations adequatelycapture real-world dynamics and that our algorithm can be used to efficientlysolve such problems.

A Polynomial-time Nash Equilibrium Algorithm for Repeated Stochastic  Games

  We present a polynomial-time algorithm that always finds an (approximate)Nash equilibrium for repeated two-player stochastic games. The algorithmexploits the folk theorem to derive a strategy profile that forms anequilibrium by buttressing mutually beneficial behavior with threats, wherepossible. One component of our algorithm efficiently searches for anapproximation of the egalitarian point, the fairest pareto-efficient solution.The paper concludes by applying the algorithm to a set of grid games toillustrate typical solutions the algorithm finds. These solutions compare veryfavorably to those found by competing algorithms, resulting in strategies withhigher social welfare, as well as guaranteed computational efficiency.

An Alternative Softmax Operator for Reinforcement Learning

  A softmax operator applied to a set of values acts somewhat like themaximization function and somewhat like an average. In sequential decisionmaking, softmax is often used in settings where it is necessary to maximizeutility but also to hedge against problems that arise from putting all of one'sweight behind a single maximum utility decision. The Boltzmann softmax operatoris the most commonly used softmax operator in this setting, but we show thatthis operator is prone to misbehavior. In this work, we study a differentiablesoftmax operator that, among other properties, is a non-expansion ensuring aconvergent behavior in learning and planning. We introduce a variant of SARSAalgorithm that, by utilizing the new operator, computes a Boltzmann policy witha state-dependent temperature parameter. We show that the algorithm isconvergent and that it performs favorably in practice.

Near Optimal Behavior via Approximate State Abstraction

  The combinatorial explosion that plagues planning and reinforcement learning(RL) algorithms can be moderated using state abstraction. Prohibitively largetask representations can be condensed such that essential information ispreserved, and consequently, solutions are tractably computable. However, exactabstractions, which treat only fully-identical situations as equivalent, failto present opportunities for abstraction in environments where no twosituations are exactly alike. In this work, we investigate approximate stateabstractions, which treat nearly-identical situations as equivalent. We presenttheoretical guarantees of the quality of behaviors derived from four types ofapproximate abstractions. Additionally, we empirically demonstrate thatapproximate abstractions lead to reduction in task complexity and bounded lossof optimality of behavior in a variety of environments.

Interactive Learning from Policy-Dependent Human Feedback

  For agents and robots to become more useful, they must be able to quicklylearn from non-technical users. This paper investigates the problem ofinteractively learning behaviors communicated by a human teacher using positiveand negative feedback. Much previous work on this problem has made theassumption that people provide feedback for decisions that is dependent on thebehavior they are teaching and is independent from the learner's currentpolicy. We present empirical results that show this assumption to befalse---whether human trainers give a positive or negative feedback for adecision is influenced by the learner's current policy. We argue thatpolicy-dependent feedback, in addition to being commonplace, enables usefultraining strategies from which agents should benefit. Based on this insight, weintroduce Convergent Actor-Critic by Humans (COACH), an algorithm for learningfrom policy-dependent feedback that converges to a local optimum. Finally, wedemonstrate that COACH can successfully learn multiple behaviors on a physicalrobot, even with noisy image features.

Summable Reparameterizations of Wasserstein Critics in the  One-Dimensional Setting

  Generative adversarial networks (GANs) are an exciting alternative toalgorithms for solving density estimation problems---using data to assess howlikely samples are to be drawn from the same distribution. Instead ofexplicitly computing these probabilities, GANs learn a generator that can matchthe given probabilistic source. This paper looks particularly at this matchingcapability in the context of problems with one-dimensional outputs. We identifya class of function decompositions with properties that make them well suitedto the critic role in a leading approach to GANs known as Wasserstein GANs. Weshow that Taylor and Fourier series decompositions belong to our class, provideexamples of these critics outperforming standard GAN approaches, and suggesthow they can be scaled to higher dimensional problems in the future.

Equivalence Between Wasserstein and Value-Aware Loss for Model-based  Reinforcement Learning

  Learning a generative model is a key component of model-based reinforcementlearning. Though learning a good model in the tabular setting is a simple task,learning a useful model in the approximate setting is challenging. In thiscontext, an important question is the loss function used for model learning asvarying the loss function can have a remarkable impact on effectiveness ofplanning. Recently Farahmand et al. (2017) proposed a value-aware modellearning (VAML) objective that captures the structure of value function duringmodel learning. Using tools from Asadi et al. (2018), we show that minimizingthe VAML objective is in fact equivalent to minimizing the Wasserstein metric.This equivalence improves our understanding of value-aware models, and alsocreates a theoretical foundation for applications of Wasserstein in model-basedreinforcement~learning.

Transfer with Model Features in Reinforcement Learning

  A key question in Reinforcement Learning is which representation an agent canlearn to efficiently reuse knowledge between different tasks. Recently theSuccessor Representation was shown to have empirical benefits for transferringknowledge between tasks with shared transition dynamics. This paper presentsModel Features: a feature representation that clusters behaviourally equivalentstates and that is equivalent to a Model-Reduction. Further, we present aSuccessor Feature model which shows that learning Successor Features isequivalent to learning a Model-Reduction. A novel optimization objective isdeveloped and we provide bounds showing that minimizing this objective resultsin an increasingly improved approximation of a Model-Reduction. Further, weprovide transfer experiments on randomly generated MDPs which vary in theirtransition and reward functions but approximately preserve behaviouralequivalence between states. These results demonstrate that Model Features aresuitable for transfer between tasks with varying transition and rewardfunctions.

Mitigating Planner Overfitting in Model-Based Reinforcement Learning

  An agent with an inaccurate model of its environment faces a difficultchoice: it can ignore the errors in its model and act in the real world inwhatever way it determines is optimal with respect to its model. Alternatively,it can take a more conservative stance and eschew its model in favor ofoptimizing its behavior solely via real-world interaction. This latter approachcan be exceedingly slow to learn from experience, while the former can lead to"planner overfitting" - aspects of the agent's behavior are optimized toexploit errors in its model. This paper explores an intermediate position inwhich the planner seeks to avoid overfitting through a kind of regularizationof the plans it considers. We present three different approaches thatdemonstrably mitigate planner overfitting in reinforcement-learningenvironments.

Successor Features Support Model-based and Model-free Reinforcement  Learning

  One key challenge in reinforcement learning is the ability to generalizeknowledge in control problems. While deep learning methods have beensuccessfully combined with model-free reinforcement-learning algorithms, how toperform model-based reinforcement learning in the presence of approximationerrors still remains an open problem. Using successor features, a featurerepresentation that predicts a temporal constraint, this paper presents threecontributions: First, it shows how learning successor features is equivalent tomodel-free learning. Then, it shows how successor features encode modelreductions that compress the state space by creating state partitions ofbisimilar states. Using this representation, an intelligent agent is guaranteedto accurately predict future reward outcomes, a key property of model-basedreinforcement-learning algorithms. Lastly, it presents a loss objective andprediction error bounds showing that accurately predicting value functions andreward sequences is possible with an approximation of successor features. Onfinite control problems, we illustrate how minimizing this loss objectiveresults in approximate bisimulations. The results presented in this paperprovide a novel understanding of representations that can support model-freeand model-based reinforcement learning.

Deep Reinforcement Learning from Policy-Dependent Human Feedback

  To widen their accessibility and increase their utility, intelligent agentsmust be able to learn complex behaviors as specified by (non-expert) humanusers. Moreover, they will need to learn these behaviors within a reasonableamount of time while efficiently leveraging the sparse feedback a human traineris capable of providing. Recent work has shown that human feedback can becharacterized as a critique of an agent's current behavior rather than as analternative reward signal to be maximized, culminating in the COnvergentActor-Critic by Humans (COACH) algorithm for making direct policy updates basedon human feedback. Our work builds on COACH, moving to a setting where theagent's policy is represented by a deep neural network. We employ a series ofmodifications on top of the original COACH algorithm that are critical forsuccessfully learning behaviors from high-dimensional observations, while alsosatisfying the constraint of obtaining reduced sample complexity. Wedemonstrate the effectiveness of our Deep COACH algorithm in the rich 3D worldof Minecraft with an agent that learns to complete tasks by mapping from rawpixels to actions using only real-time human feedback in 10-15 minutes ofinteraction.

Theory of Minds: Understanding Behavior in Groups Through Inverse  Planning

  Human social behavior is structured by relationships. We form teams, groups,tribes, and alliances at all scales of human life. These structures guidemulti-agent cooperation and competition, but when we observe others theseunderlying relationships are typically unobservable and hence must be inferred.Humans make these inferences intuitively and flexibly, often making rapidgeneralizations about the latent relationships that underlie behavior from justsparse and noisy observations. Rapid and accurate inferences are important fordetermining who to cooperate with, who to compete with, and how to cooperate inorder to compete. Towards the goal of building machine-learning algorithms withhuman-like social intelligence, we develop a generative model of multi-agentaction understanding based on a novel representation for these latentrelationships called Composable Team Hierarchies (CTH). This representation isgrounded in the formalism of stochastic games and multi-agent reinforcementlearning. We use CTH as a target for Bayesian inference yielding a newalgorithm for understanding behavior in groups that can both infer hiddenrelationships as well as predict future actions for multiple agents interactingtogether. Our algorithm rapidly recovers an underlying causal model of howagents relate in spatial stochastic games from just a few observations. Thepatterns of inference made by this algorithm closely correspond with humanjudgments and the algorithm makes the same rapid generalizations that peopledo.

$ \ell ^{p}$-improving inequalities for Discrete Spherical Averages

  Let $ \lambda ^2 \in \mathbb N $, and in dimensions $ d\geq 5$, let $ A_{\lambda } f (x)$ denote the average of $ f \;:\; \mathbb Z ^{d} \to \mathbb R$ over the lattice points on the sphere of radius $\lambda$ centered at $x$. Weprove $ \ell ^{p}$ improving properties of $ A _{\lambda }$.  \begin{equation*} \lVert A _{\lambda }\rVert _{\ell ^{p} \to \ell ^{p'}} \leqC _{d,p, \omega (\lambda ^2 )} \lambda ^{d ( 1-\frac{2}p)}, \qquad\tfrac{d-1}{d+1} < p \leq \frac{d} {d-2}. \end{equation*} It holds in dimension$ d =4$ for odd $ \lambda ^2 $. The dependence is in terms of $ \omega (\lambda^2 )$, the number of distinct prime factors of $ \lambda ^2 $. Theseinequalities are discrete versions of a classical inequality of Littman andStrichartz on the $ L ^{p}$ improving property of spherical averages on $\mathbb R ^{d}$, in particular they are scale free, in a natural sense. Theproof uses the decomposition of the corresponding multiplier whose propertieswere established by Magyar-Stein-Wainger, and Magyar. We then use a proofstrategy of Bourgain, which dominates each part of the decomposition by anendpoint estimate.

Learning Analogies and Semantic Relations

  We present an algorithm for learning from unlabeled text, based on the VectorSpace Model (VSM) of information retrieval, that can solve verbal analogyquestions of the kind found in the Scholastic Aptitude Test (SAT). A verbalanalogy has the form A:B::C:D, meaning "A is to B as C is to D"; for example,mason:stone::carpenter:wood. SAT analogy questions provide a word pair, A:B,and the problem is to select the most analogous word pair, C:D, from a set offive choices. The VSM algorithm correctly answers 47% of a collection of 374college-level analogy questions (random guessing would yield 20% correct). Wemotivate this research by relating it to work in cognitive science andlinguistics, and by applying it to a difficult problem in natural languageprocessing, determining semantic relations in noun-modifier pairs. The problemis to classify a noun-modifier pair, such as "laser printer", according to thesemantic relation between the noun (printer) and the modifier (laser). We use asupervised nearest-neighbour algorithm that assigns a class to a givennoun-modifier pair by finding the most analogous noun-modifier pair in thetraining data. With 30 classes of semantic relations, on a collection of 600labeled noun-modifier pairs, the learning algorithm attains an F value of 26.5%(random guessing: 3.3%). With 5 classes of semantic relations, the F value is43.2% (random: 20%). The performance is state-of-the-art for these challengingproblems.

Measuring Praise and Criticism: Inference of Semantic Orientation from  Association

  The evaluative character of a word is called its semantic orientation.Positive semantic orientation indicates praise (e.g., "honest", "intrepid") andnegative semantic orientation indicates criticism (e.g., "disturbing","superfluous"). Semantic orientation varies in both direction (positive ornegative) and degree (mild to strong). An automated system for measuringsemantic orientation would have application in text classification, textfiltering, tracking opinions in online discussions, analysis of surveyresponses, and automated chat systems (chatbots). This paper introduces amethod for inferring the semantic orientation of a word from its statisticalassociation with a set of positive and negative paradigm words. Two instancesof this approach are evaluated, based on two different statistical measures ofword association: pointwise mutual information (PMI) and latent semanticanalysis (LSA). The method is experimentally tested with 3,596 words (includingadjectives, adverbs, nouns, and verbs) that have been manually labeled positive(1,614 words) and negative (1,982 words). The method attains an accuracy of82.8% on the full test set, but the accuracy rises above 95% when the algorithmis allowed to abstain from classifying mild words.

Corpus-based Learning of Analogies and Semantic Relations

  We present an algorithm for learning from unlabeled text, based on the VectorSpace Model (VSM) of information retrieval, that can solve verbal analogyquestions of the kind found in the SAT college entrance exam. A verbal analogyhas the form A:B::C:D, meaning "A is to B as C is to D"; for example,mason:stone::carpenter:wood. SAT analogy questions provide a word pair, A:B,and the problem is to select the most analogous word pair, C:D, from a set offive choices. The VSM algorithm correctly answers 47% of a collection of 374college-level analogy questions (random guessing would yield 20% correct; theaverage college-bound senior high school student answers about 57% correctly).We motivate this research by applying it to a difficult problem in naturallanguage processing, determining semantic relations in noun-modifier pairs. Theproblem is to classify a noun-modifier pair, such as "laser printer", accordingto the semantic relation between the noun (printer) and the modifier (laser).We use a supervised nearest-neighbour algorithm that assigns a class to a givennoun-modifier pair by finding the most analogous noun-modifier pair in thetraining data. With 30 classes of semantic relations, on a collection of 600labeled noun-modifier pairs, the learning algorithm attains an F value of 26.5%(random guessing: 3.3%). With 5 classes of semantic relations, the F value is43.2% (random: 20%). The performance is state-of-the-art for both verbalanalogies and noun-modifier relations.

Graphical Models for Game Theory

  In this work, we introduce graphical modelsfor multi-player game theory, andgive powerful algorithms for computing their Nash equilibria in certain cases.An n-player game is given by an undirected graph on n nodes and a set of nlocal matrices. The interpretation is that the payoff to player i is determinedentirely by the actions of player i and his neighbors in the graph, and thusthe payoff matrix to player i is indexed only by these players. We thus viewthe global n-player game as being composed of interacting local games, eachinvolving many fewer players. Each player's action may have global impact, butit occurs through the propagation of local influences.Our main technical resultis an efficient algorithm for computing Nash equilibria when the underlyinggraph is a tree (or can be turned into a tree with few node mergings). Thealgorithm runs in time polynomial in the size of the representation (the graphand theassociated local game matrices), and comes in two related but distinctflavors. The first version involves an approximation step, and computes arepresentation of all approximate Nash equilibria (of which there may be anexponential number in general). The second version allows the exact computationof Nash equilibria at the expense of weakened complexity bounds. The algorithmrequires only local message-passing between nodes (and thus can be implementedby the players themselves in a distributed manner). Despite an analogy toinference in Bayes nets that we develop, the analysis of our algorithm is moreinvolved than that for the polytree algorithm in, owing partially to the factthat we must either compute, or select from, an exponential number of potentialsolutions. We discuss a number of extensions, such as the computation ofequilibria with desirable global properties (e.g. maximizing global return),and directions for further research.

