Learning Mixtures of Ranking Models

  This work concerns learning probabilistic models for ranking data in aheterogeneous population. The specific problem we study is learning theparameters of a Mallows Mixture Model. Despite being widely studied, currentheuristics for this problem do not have theoretical guarantees and can getstuck in bad local optima. We present the first polynomial time algorithm whichprovably learns the parameters of a mixture of two Mallows models. A keycomponent of our algorithm is a novel use of tensor decomposition techniques tolearn the top-k prefix in both the rankings. Before this work, even thequestion of identifiability in the case of a mixture of two Mallows models wasunresolved.

Center-based Clustering under Perturbation Stability

  Clustering under most popular objective functions is NP-hard, even toapproximate well, and so unlikely to be efficiently solvable in the worst case.Recently, Bilu and Linial \cite{Bilu09} suggested an approach aimed atbypassing this computational barrier by using properties of instances one mighthope to hold in practice. In particular, they argue that instances in practiceshould be stable to small perturbations in the metric space and give anefficient algorithm for clustering instances of the Max-Cut problem that arestable to perturbations of size $O(n^{1/2})$. In addition, they conjecture thatinstances stable to as little as O(1) perturbations should be solvable inpolynomial time. In this paper we prove that this conjecture is true for anycenter-based clustering objective (such as $k$-median, $k$-means, and$k$-center). Specifically, we show we can efficiently find the optimalclustering assuming only stability to factor-3 perturbations of the underlyingmetric in spaces without Steiner points, and stability to factor $2+\sqrt{3}$perturbations for general metrics. In particular, we show for such instancesthat the popular Single-Linkage algorithm combined with dynamic programmingwill find the optimal clustering. We also present NP-hardness results under aweaker but related condition.

Additive Approximation for Near-Perfect Phylogeny Construction

  We study the problem of constructing phylogenetic trees for a given set ofspecies. The problem is formulated as that of finding a minimum Steiner tree on$n$ points over the Boolean hypercube of dimension $d$. It is known that anoptimal tree can be found in linear time if the given dataset has a perfectphylogeny, i.e. cost of the optimal phylogeny is exactly $d$. Moreover, if thedata has a near-perfect phylogeny, i.e. the cost of the optimal Steiner tree is$d+q$, it is known that an exact solution can be found in running time which ispolynomial in the number of species and $d$, yet exponential in $q$. In thiswork, we give a polynomial-time algorithm (in both $d$ and $q$) that finds aphylogenetic tree of cost $d+O(q^2)$. This provides the best guarantees known -namely, a $(1+o(1))$-approximation - for the case $\log(d) \ll q \ll \sqrt{d}$,broadening the range of settings for which near-optimal solutions can beefficiently found. We also discuss the motivation and reasoning for studyingsuch additive approximations.

Clustering Semi-Random Mixtures of Gaussians

  Gaussian mixture models (GMM) are the most widely used statistical model forthe $k$-means clustering problem and form a popular framework for clustering inmachine learning and data analysis. In this paper, we propose a naturalsemi-random model for $k$-means clustering that generalizes the Gaussianmixture model, and that we believe will be useful in identifying robustalgorithms. In our model, a semi-random adversary is allowed to make arbitrary"monotone" or helpful changes to the data generated from the Gaussian mixturemodel.  Our first contribution is a polynomial time algorithm that provably recoversthe ground-truth up to small classification error w.h.p., assuming certainseparation between the components. Perhaps surprisingly, the algorithm weanalyze is the popular Lloyd's algorithm for $k$-means clustering that is themethod-of-choice in practice. Our second result complements the upper bound bygiving a nearly matching information-theoretic lower bound on the number ofmisclassified points incurred by any $k$-means clustering algorithm on thesemi-random model.

Local algorithms for interactive clustering

  We study the design of interactive clustering algorithms for data setssatisfying natural stability assumptions. Our algorithms start with any initialclustering and only make local changes in each step; both are desirablefeatures in many applications. We show that in this constrained setting one canstill design provably efficient algorithms that produce accurate clusterings.We also show that our algorithms perform well on real-world data.

Improved Spectral-Norm Bounds for Clustering

  Aiming to unify known results about clustering mixtures of distributionsunder separation conditions, Kumar and Kannan[2010] introduced a deterministiccondition for clustering datasets. They showed that this single deterministiccondition encompasses many previously studied clustering assumptions. Morespecifically, their proximity condition requires that in the target$k$-clustering, the projection of a point $x$ onto the line joining its clustercenter $\mu$ and some other center $\mu'$, is a large additive factor closer to$\mu$ than to $\mu'$. This additive factor can be roughly described as $k$times the spectral norm of the matrix representing the differences between thegiven (known) dataset and the means of the (unknown) target clustering.Clearly, the proximity condition implies center separation -- the distancebetween any two centers must be as large as the above mentioned bound.  In this paper we improve upon the work of Kumar and Kannan along severalaxes. First, we weaken the center separation bound by a factor of $\sqrt{k}$,and secondly we weaken the proximity condition by a factor of $k$. Using theseweaker bounds we still achieve the same guarantees when all points satisfy theproximity condition. We also achieve better guarantees when only$(1-\epsilon)$-fraction of the points satisfy the weaker proximity condition.The bulk of our analysis relies only on center separation under which one canproduce a clustering which (i) has low error, (ii) has low $k$-means cost, and(iii) has centers very close to the target centers.  Our improved separation condition allows us to match the results of thePlanted Partition Model of McSherry[2001], improve upon the results ofOstrovsky et al[2006], and improve separation results for mixture of Gaussianmodels in a particular setting.

The Power of Localization for Efficiently Learning Linear Separators  with Noise

  We introduce a new approach for designing computationally efficient learningalgorithms that are tolerant to noise, and demonstrate its effectiveness bydesigning algorithms with improved noise tolerance guarantees for learninglinear separators.  We consider both the malicious noise model and the adversarial label noisemodel. For malicious noise, where the adversary can corrupt both the label andthe features, we provide a polynomial-time algorithm for learning linearseparators in $\Re^d$ under isotropic log-concave distributions that cantolerate a nearly information-theoretically optimal noise rate of $\eta =\Omega(\epsilon)$. For the adversarial label noise model, where thedistribution over the feature vectors is unchanged, and the overall probabilityof a noisy label is constrained to be at most $\eta$, we also give apolynomial-time algorithm for learning linear separators in $\Re^d$ underisotropic log-concave distributions that can handle a noise rate of $\eta =\Omega\left(\epsilon\right)$.  We show that, in the active learning model, our algorithms achieve a labelcomplexity whose dependence on the error parameter $\epsilon$ ispolylogarithmic. This provides the first polynomial-time active learningalgorithm for learning linear separators in the presence of malicious noise oradversarial label noise.

Towards Learning Sparsely Used Dictionaries with Arbitrary Supports

  Dictionary learning is a popular approach for inferring a hidden basis ordictionary in which data has a sparse representation. Data generated from thedictionary A (an n by m matrix, with m > n in the over-complete setting) isgiven by Y = AX where X is a matrix whose columns have supports chosen from adistribution over k-sparse vectors, and the non-zero values chosen from asymmetric distribution. Given Y, the goal is to recover A and X in polynomialtime. Existing algorithms give polytime guarantees for recovering incoherentdictionaries, under strong distributional assumptions both on the supports ofthe columns of X, and on the values of the non-zero entries. In this work, westudy the following question: Can we design efficient algorithms for recoveringdictionaries when the supports of the columns of X are arbitrary?  To address this question while circumventing the issue ofnon-identifiability, we study a natural semirandom model for dictionarylearning where there are a large number of samples $y=Ax$ with arbitraryk-sparse supports for x, along with a few samples where the sparse supports arechosen uniformly at random. While the few samples with random supports ensuresidentifiability, the support distribution can look almost arbitrary inaggregate. Hence existing algorithmic techniques seem to break down as theymake strong assumptions on the supports.  Our main contribution is a new polynomial time algorithm for learningincoherent over-complete dictionaries that works under the semirandom model.Additionally the same algorithm provides polynomial time guarantees in newparameter regimes when the supports are fully random. Finally using thesetechniques, we also identify a minimal set of conditions on the supports underwhich the dictionary can be (information theoretically) recovered frompolynomial samples for almost linear sparsity, i.e., $k=\tilde{O}(n)$.

Fair k-Center Clustering for Data Summarization

  In data summarization we want to choose k prototypes in order to summarize adata set. We study a setting where the data set comprises several demographicgroups and we are restricted to choose k_i prototypes belonging to group i. Acommon approach to the problem without the fairness constraint is to optimize acentroid-based clustering objective such as k-center. A natural extension thenis to incorporate the fairness constraint into the clustering objective.Existing algorithms for doing so run in time super-quadratic in the size of thedata set. This is in contrast to the standard k-center objective that can beapproximately optimized in linear time. In this paper, we resolve this gap byproviding a simple approximation algorithm for the k-center problem under thefairness constraint with running time linear in the size of the data set and k.If the number of demographic groups is small, the approximation guarantee ofour algorithm only incurs a constant-factor overhead. We demonstrate theapplicability of our algorithm on both synthetic and real data sets.

Guarantees for Spectral Clustering with Fairness Constraints

  Given the widespread popularity of spectral clustering (SC) for partitioninggraph data, we study a version of constrained SC in which we try to incorporatethe fairness notion proposed by Chierichetti et al. (2017). According to thisnotion, a clustering is fair if every demographic group is approximatelyproportionally represented in each cluster. To this end, we develop variants ofboth normalized and unnormalized constrained SC and show that they help findfairer clusterings on both synthetic and real data. We also provide a rigoroustheoretical analysis of our algorithms. While there have been efforts toincorporate various constraints into the SC framework, theoretically analyzingthem is a challenging problem. We overcome this by proposing a natural variantof the stochastic block model where h groups have strong inter-groupconnectivity, but also exhibit a "natural" clustering structure which is fair.We prove that our algorithms can recover this fair clustering with highprobability.

Learning using Local Membership Queries

  We introduce a new model of membership query (MQ) learning, where thelearning algorithm is restricted to query points that are \emph{close} torandom examples drawn from the underlying distribution. The learning model isintermediate between the PAC model (Valiant, 1984) and the PAC+MQ model (wherethe queries are allowed to be arbitrary points).  Membership query algorithms are not popular among machine learningpractitioners. Apart from the obvious difficulty of adaptively queryinglabelers, it has also been observed that querying \emph{unnatural} points leadsto increased noise from human labelers (Lang and Baum, 1992). This motivatesour study of learning algorithms that make queries that are close to examplesgenerated from the data distribution.  We restrict our attention to functions defined on the $n$-dimensional Booleanhypercube and say that a membership query is local if its Hamming distance fromsome example in the (random) training data is at most $O(\log(n))$. We show thefollowing results in this model:  (i) The class of sparse polynomials (with coefficients in R) over $\{0,1\}^n$is polynomial time learnable under a large class of \emph{locally smooth}distributions using $O(\log(n))$-local queries. This class also includes theclass of $O(\log(n))$-depth decision trees.  (ii) The class of polynomial-sized decision trees is polynomial timelearnable under product distributions using $O(\log(n))$-local queries.  (iii) The class of polynomial size DNF formulas is learnable under theuniform distribution using $O(\log(n))$-local queries in time$n^{O(\log(\log(n)))}$.  (iv) In addition we prove a number of results relating the proposed model tothe traditional PAC model and the PAC+MQ model.

Efficient Learning of Linear Separators under Bounded Noise

  We study the learnability of linear separators in $\Re^d$ in the presence ofbounded (a.k.a Massart) noise. This is a realistic generalization of the randomclassification noise model, where the adversary can flip each example $x$ withprobability $\eta(x) \leq \eta$. We provide the first polynomial time algorithmthat can learn linear separators to arbitrarily small excess error in thisnoise model under the uniform distribution over the unit ball in $\Re^d$, forsome constant value of $\eta$. While widely studied in the statistical learningtheory community in the context of getting faster convergence rates,computationally efficient algorithms in this model had remained elusive. Ourwork provides the first evidence that one can indeed design algorithmsachieving arbitrarily small excess error in polynomial time under thisrealistic noise model and thus opens up a new and exciting line of research.  We additionally provide lower bounds showing that popular algorithms such ashinge loss minimization and averaging cannot lead to arbitrarily small excesserror under Massart noise, even under the uniform distribution. Our workinstead, makes use of a margin based technique developed in the context ofactive learning. As a result, our algorithm is also an active learningalgorithm with label complexity that is only a logarithmic the desired excesserror $\epsilon$.

Efficient PAC Learning from the Crowd

  In recent years crowdsourcing has become the method of choice for gatheringlabeled training data for learning algorithms. Standard approaches tocrowdsourcing view the process of acquiring labeled data separately from theprocess of learning a classifier from the gathered data. This can give rise tocomputational and statistical challenges. For example, in most cases there areno known computationally efficient learning algorithms that are robust to thehigh level of noise that exists in crowdsourced data, and efforts to eliminatenoise through voting often require a large number of queries per example.  In this paper, we show how by interleaving the process of labeling andlearning, we can attain computational efficiency with much less overhead in thelabeling cost. In particular, we consider the realizable setting where thereexists a true target function in $\mathcal{F}$ and consider a pool of labelers.When a noticeable fraction of the labelers are perfect, and the rest behavearbitrarily, we show that any $\mathcal{F}$ that can be efficiently learned inthe traditional realizable PAC model can be learned in a computationallyefficient manner by querying the crowd, despite high amounts of noise in theresponses. Moreover, we show that this can be done while each labeler onlylabels a constant number of examples and the number of labels requested perexample, on average, is a constant. When no perfect labelers exist, a relatedtask is to find a set of the labelers which are good but not perfect. We showthat we can identify all good labelers, when at least the majority of labelersare good.

Relax, no need to round: integrality of clustering formulations

  We study exact recovery conditions for convex relaxations of point cloudclustering problems, focusing on two of the most common optimization problemsfor unsupervised clustering: $k$-means and $k$-median clustering. Motivationsfor focusing on convex relaxations are: (a) they come with a certificate ofoptimality, and (b) they are generic tools which are relatively parameter-free,not tailored to specific assumptions over the input. More precisely, weconsider the distributional setting where there are $k$ clusters in$\mathbb{R}^m$ and data from each cluster consists of $n$ points sampled from asymmetric distribution within a ball of unit radius. We ask: what is theminimal separation distance between cluster centers needed for convexrelaxations to exactly recover these $k$ clusters as the optimal integralsolution? For the $k$-median linear programming relaxation we show a tightbound: exact recovery is obtained given arbitrarily small pairwise separation$\epsilon > 0$ between the balls. In other words, the pairwise centerseparation is $\Delta > 2+\epsilon$. Under the same distributional model, the$k$-means LP relaxation fails to recover such clusters at separation as largeas $\Delta = 4$. Yet, if we enforce PSD constraints on the $k$-means LP, we getexact cluster recovery at center separation $\Delta > 2\sqrt2(1+\sqrt{1/m})$.In contrast, common heuristics such as Lloyd's algorithm (a.k.a. the $k$-meansalgorithm) can fail to recover clusters in this setting; even with arbitrarilylarge cluster separation, k-means++ with overseeding by any constant factorfails with high probability at exact cluster recovery. To complement thetheoretical analysis, we provide an experimental study of the recoveryguarantees for these various methods, and discuss several open problems whichthese experiments suggest.

The Hardness of Approximation of Euclidean k-means

  The Euclidean $k$-means problem is a classical problem that has beenextensively studied in the theoretical computer science, machine learning andthe computational geometry communities. In this problem, we are given a set of$n$ points in Euclidean space $R^d$, and the goal is to choose $k$ centers in$R^d$ so that the sum of squared distances of each point to its nearest centeris minimized. The best approximation algorithms for this problem include apolynomial time constant factor approximation for general $k$ and a$(1+\epsilon)$-approximation which runs in time $poly(n) 2^{O(k/\epsilon)}$. Atthe other extreme, the only known computational complexity result for thisproblem is NP-hardness [ADHP'09]. The main difficulty in obtaining hardnessresults stems from the Euclidean nature of the problem, and the fact that anypoint in $R^d$ can be a potential center. This gap in understanding left openthe intriguing possibility that the problem might admit a PTAS for all $k,d$.  In this paper we provide the first hardness of approximation for theEuclidean $k$-means problem. Concretely, we show that there exists a constant$\epsilon > 0$ such that it is NP-hard to approximate the $k$-means objectiveto within a factor of $(1+\epsilon)$. We show this via an efficient reductionfrom the vertex cover problem on triangle-free graphs: given a triangle-freegraph, the goal is to choose the fewest number of vertices which are incidenton all the edges. Additionally, we give a proof that the current best hardnessresults for vertex cover can be carried over to triangle-free graphs. To showthis we transform $G$, a known hard vertex cover instance, by taking a graphproduct with a suitably chosen graph $H$, and showing that the size of the(normalized) maximum independent set is almost exactly preserved in the productgraph using a spectral analysis, which might be of independent interest.

Label optimal regret bounds for online local learning

  We resolve an open question from (Christiano, 2014b) posed in COLT'14regarding the optimal dependency of the regret achievable for online locallearning on the size of the label set. In this framework the algorithm is showna pair of items at each step, chosen from a set of $n$ items. The learner thenpredicts a label for each item, from a label set of size $L$ and receives areal valued payoff. This is a natural framework which captures many interestingscenarios such as collaborative filtering, online gambling, and online max cutamong others. (Christiano, 2014a) designed an efficient online learningalgorithm for this problem achieving a regret of $O(\sqrt{nL^3T})$, where $T$is the number of rounds. Information theoretically, one can achieve a regret of$O(\sqrt{n \log L T})$. One of the main open questions left in this frameworkconcerns closing the above gap.  In this work, we provide a complete answer to the question above via two mainresults. We show, via a tighter analysis, that the semi-definite programmingbased algorithm of (Christiano, 2014a), in fact achieves a regret of$O(\sqrt{nLT})$. Second, we show a matching computational lower bound. Namely,we show that a polynomial time algorithm for online local learning with lowerregret would imply a polynomial time algorithm for the planted clique problemwhich is widely believed to be hard. We prove a similar hardness result under arelated conjecture concerning planted dense subgraphs that we put forth. Unlikeplanted clique, the planted dense subgraph problem does not have any knownquasi-polynomial time algorithms.  Computational lower bounds for online learning are relatively rare, and wehope that the ideas developed in this work will lead to lower bounds for otheronline learning scenarios as well.

On some provably correct cases of variational inference for topic models

  Variational inference is a very efficient and popular heuristic used invarious forms in the context of latent variable models. It's closely related toExpectation Maximization (EM), and is applied when exact EM is computationallyinfeasible. Despite being immensely popular, current theoretical understandingof the effectiveness of variaitonal inference based algorithms is very limited.In this work we provide the first analysis of instances where variationalinference algorithms converge to the global optimum, in the setting of topicmodels.  More specifically, we show that variational inference provably learns theoptimal parameters of a topic model under natural assumptions on the topic-wordmatrix and the topic priors. The properties that the topic word matrix mustsatisfy in our setting are related to the topic expansion assumption introducedin (Anandkumar et al., 2013), as well as the anchor words assumption in (Aroraet al., 2012c). The assumptions on the topic priors are related to the wellknown Dirichlet prior, introduced to the area of topic modeling by (Blei etal., 2003).  It is well known that initialization plays a crucial role in how wellvariational based algorithms perform in practice. The initializations that weuse are fairly natural. One of them is similar to what is currently used inLDA-c, the most popular implementation of variational inference for topicmodels. The other one is an overlapping clustering algorithm, inspired by awork by (Arora et al., 2014) on dictionary learning, which is very simple andefficient.  While our primary goal is to provide insights into when variational inferencemight work in practice, the multiplicative, rather than the additive nature ofthe variational inference updates forces us to use fairly non-standard proofarguments, which we believe will be of general interest.

Robust Communication-Optimal Distributed Clustering Algorithms

  In this work, we study the $k$-median and $k$-means clustering problems whenthe data is distributed across many servers and can contain outliers. Whilethere has been a lot of work on these problems for worst-case instances, wefocus on gaining a finer understanding through the lens of beyond worst-caseanalysis. Our main motivation is the following: for many applications such asclustering proteins by function or clustering communities in a social network,there is some unknown target clustering, and the hope is that running a$k$-median or $k$-means algorithm will produce clusterings which are close tomatching the target clustering. Worst-case results can guarantee constantfactor approximations to the optimal $k$-median or $k$-means objective value,but not closeness to the target clustering.  Our first result is a distributed algorithm which returns a near-optimalclustering assuming a natural notion of stability, namely, approximationstability [Balcan et. al 2013], even when a constant fraction of the data areoutliers. The communication complexity is $\tilde O(sk+z)$ where $s$ is thenumber of machines, $k$ is the number of clusters, and $z$ is the number ofoutliers.  Next, we show this amount of communication cannot be improved even in thesetting when the input satisfies various non-worst-case assumptions. We give amatching $\Omega(sk+z)$ lower bound on the communication required both forapproximating the optimal $k$-means or $k$-median cost up to any constant, andfor returning a clustering that is close to the target clustering in Hammingdistance. These lower bounds hold even when the data satisfies approximationstability or other common notions of stability, and the cluster sizes arebalanced. Therefore, $\Omega(sk+z)$ is a communication bottleneck, even forreal-world instances.

Robust Vertex Enumeration for Convex Hulls in High Dimensions

  Computation of the vertices of the convex hull of a set $S$ of $n$ points in$\mathbb{R} ^m$ is a fundamental problem in computational geometry,optimization, machine learning and more. We present "All Vertex TriangleAlgorithm" (AVTA), a robust and efficient algorithm for computing the subset$\overline S$ of all $K$ vertices of $conv(S)$, the convex hull of $S$. If$\Gamma_*$ is the minimum of the distances from each vertex to the convex hullof the remaining vertices, given any $\gamma \leq \gamma_* = \Gamma_*/R$, $R$the diameter of $S$, $AVTA$ computes $\overline S$ in $O(nK(m+ \gamma^{-2}))$operations. If $\gamma_*$ is unknown but $K$ is known, AVTA computes $\overlineS$ in $O(nK(m+ \gamma_*^{-2})) \log(\gamma_*^{-1})$ operations. More generally,given $t \in (0,1)$, AVTA computes a subset $\overline S^t$ of $\overline S$ in$O(n |\overline S^t|(m+ t^{-2}))$ operations, where the distance between any $p\in conv(S)$ to $conv(\overline S^t)$ is at most $t R$. Next we consider AVTAwhere input is $S_\varepsilon$, an $\varepsilon$ perturbation of $S$. Assuminga bound on $\varepsilon$ in terms of the minimum of the distances of verticesof $conv(S)$ to the convex hull of the remaining point of $S$, we deriveanalogous complexity bounds for computing $\overline S_\varepsilon$. We alsoanalyze AVTA under random projections of $S$ or $S_\varepsilon$. Finally, viaAVTA we design new practical algorithms for two popular machine learningproblems: topic modeling and non-negative matrix factorization. For topicmodels AVTA leads to significantly better reconstruction of the topic-wordmatrix than state of the art approaches~\cite{arora2013practical,bansal2014provable}. For non-negative matrix AVTA is competitive with existingmethods~\cite{arora2012computing}. Empirically AVTA is robust and can handlelarger amounts of noise than existing methods.

Bilu-Linial stability, certified algorithms and the Independent Set  problem

  We study the notion of Bilu-Linial stability in the context of IndependentSet. A weighted instance $G=(V,E,w)$ of Independent Set is $\gamma$-stable ifit has a unique optimal solution that remains the unique optimal solution undermultiplicative perturbations of the weights by a factor of at most $\gamma\geq1$. In this work, we use the standard LP as well as the Sherali-Adams hierarchyto design algorithms for $(\Delta-1)$-stable instances on graphs of maximumdegree $\Delta$, for $(k-1)$-stable instances on $k$-colorable graphs and for$(1+\varepsilon)$-stable instances on planar graphs. We also show that theintegrality gap of relaxations of several maximization problems reducesdramatically on stable instances. For general graphs we give an algorithm for$(\varepsilon n)$-stable instances (for fixed $\varepsilon>0$), and on thenegative side we show that there are no efficient algorithms for$O(n^{\frac{1}{2}-\varepsilon})$-stable instances assuming the planted cliqueconjecture. As a side note, we exploit the connection between Vertex Cover andNode Multiway Cut and give the first results about stable instances of NodeMultiway Cut.  Moreover, we initiate the study of certified algorithms for Independent Set.The class of $\gamma$-certified algorithms is a class of $\gamma$-approximationalgorithms introduced by Makarychev and Makarychev (2018) whose returnedsolution is optimal for a perturbation of the original instance. Using resultsof Makarychev and Makarychev (2018) as well as combinatorial techniques, weobtain $\Delta$-certified algorithms for Independent Set on graphs of maximumdegree $\Delta$ and $(1+\varepsilon)$-certified algorithms on planar graphs.Finally, we prove that an algorithm of Berman and F\"{u}rer (1994) is a$\left(\frac{\Delta+1}{3}+\varepsilon\right)$-certified algorithm on graphs ofmaximum degree $\Delta$ where all weights are equal to 1.

