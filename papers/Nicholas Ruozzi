The Bethe Partition Function of Log-supermodular Graphical Models

  Sudderth, Wainwright, and Willsky have conjectured that the Betheapproximation corresponding to any fixed point of the belief propagationalgorithm over an attractive, pairwise binary graphical model provides a lowerbound on the true partition function. In this work, we resolve this conjecturein the affirmative by demonstrating that, for any graphical model with binaryvariables whose potential functions (not necessarily pairwise) are alllog-supermodular, the Bethe partition function always lower bounds the truepartition function. The proof of this result follows from a new variant of the"four functions" theorem that may be of independent interest.

Beyond Log-Supermodularity: Lower Bounds and the Bethe Partition  Function

  A recent result has demonstrated that the Bethe partition function alwayslower bounds the true partition function of binary, log-supermodular graphicalmodels. We demonstrate that these results can be extended to other interestingclasses of graphical models that are not necessarily binary orlog-supermodular: the ferromagnetic Potts model with a uniform external fieldand its generalizations and special classes of weighted graph homomorphismproblems.

Message-Passing Algorithms for Quadratic Minimization

  Gaussian belief propagation (GaBP) is an iterative algorithm for computingthe mean of a multivariate Gaussian distribution, or equivalently, the minimumof a multivariate positive definite quadratic function. Sufficient conditions,such as walk-summability, that guarantee the convergence and correctness ofGaBP are known, but GaBP may fail to converge to the correct solution given anarbitrary positive definite quadratic function. As was observed in previouswork, the GaBP algorithm fails to converge if the computation trees produced bythe algorithm are not positive definite. In this work, we will show that thefailure modes of the GaBP algorithm can be understood via graph covers, and weprove that a parameterized generalization of the min-sum algorithm can be usedto ensure that the computation trees remain positive definite whenever theinput matrix is positive definite. We demonstrate that the resulting algorithmis closely related to other iterative schemes for quadratic minimization suchas the Gauss-Seidel and Jacobi algorithms. Finally, we observe, empirically,that there always exists a choice of parameters such that the abovegeneralization of the GaBP algorithm converges.

Scalable Neural Network Compression and Pruning Using Hard Clustering  and L1 Regularization

  We propose a simple and easy to implement neural network compressionalgorithm that achieves results competitive with more complicatedstate-of-the-art methods. The key idea is to modify the original optimizationproblem by adding K independent Gaussian priors (corresponding to the k-meansobjective) over the network parameters to achieve parameter quantization, aswell as an L1 penalty to achieve pruning. Unlike many existingquantization-based methods, our method uses hard clustering assignments ofnetwork parameters, which adds minimal change or overhead to standard networktraining. We also demonstrate experimentally that tying neural networkparameters provides less gain in generalization performance than changingnetwork architecture and connectivity patterns entirely.

Message-Passing Algorithms: Reparameterizations and Splittings

  The max-product algorithm, a local message-passing scheme that attempts tocompute the most probable assignment (MAP) of a given probability distribution,has been successfully employed as a method of approximate inference forapplications arising in coding theory, computer vision, and machine learning.However, the max-product algorithm is not guaranteed to converge to the MAPassignment, and if it does, is not guaranteed to recover the MAP assignment.  Alternative convergent message-passing schemes have been proposed to overcomethese difficulties. This work provides a systematic study of suchmessage-passing algorithms that extends the known results by exhibiting newsufficient conditions for convergence to local and/or global optima, providinga combinatorial characterization of these optima based on graph covers, anddescribing a new convergent and correct message-passing algorithm whosederivation unifies many of the known convergent message-passing algorithms.  While convergent and correct message-passing algorithms represent a stepforward in the analysis of max-product style message-passing algorithms, theconditions needed to guarantee convergence to a global optimum can be toorestrictive in both theory and practice. This limitation of convergent andcorrect message-passing schemes is characterized by graph covers andillustrated by example.

Applications of Metric Coinduction

  Metric coinduction is a form of coinduction that can be used to establishproperties of objects constructed as a limit of finite approximations. One canprove a coinduction step showing that some property is preserved by one step ofthe approximation process, then automatically infer by the coinductionprinciple that the property holds of the limit object. This can often be usedto avoid complicated analytic arguments involving limits and convergence,replacing them with simpler algebraic arguments. This paper examines theapplication of this principle in a variety of areas, including infinitestreams, Markov chains, Markov decision processes, and non-well-founded sets.These results point to the usefulness of coinduction as a general prooftechnique.

Bethe Learning of Conditional Random Fields via MAP Decoding

  Many machine learning tasks can be formulated in terms of predictingstructured outputs. In frameworks such as the structured support vector machine(SVM-Struct) and the structured perceptron, discriminative functions arelearned by iteratively applying efficient maximum a posteriori (MAP) decoding.However, maximum likelihood estimation (MLE) of probabilistic models over thesesame structured spaces requires computing partition functions, which isgenerally intractable. This paper presents a method for learning discreteexponential family models using the Bethe approximation to the MLE. Remarkably,this problem also reduces to iterative (MAP) decoding. This connection emergesby combining the Bethe approximation with a Frank-Wolfe (FW) algorithm on aconvex dual objective which circumvents the intractable partition function. Theresult is a new single loop algorithm MLE-Struct, which is substantially moreefficient than previous double-loop methods for approximate maximum likelihoodestimation. Our algorithm outperforms existing methods in experiments involvingimage segmentation, matching problems from vision, and a new dataset ofuniversity roommate assignments.

