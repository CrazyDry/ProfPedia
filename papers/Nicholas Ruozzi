The Bethe Partition Function of Log-supermodular Graphical Models

  Sudderth, Wainwright, and Willsky have conjectured that the Bethe
approximation corresponding to any fixed point of the belief propagation
algorithm over an attractive, pairwise binary graphical model provides a lower
bound on the true partition function. In this work, we resolve this conjecture
in the affirmative by demonstrating that, for any graphical model with binary
variables whose potential functions (not necessarily pairwise) are all
log-supermodular, the Bethe partition function always lower bounds the true
partition function. The proof of this result follows from a new variant of the
"four functions" theorem that may be of independent interest.


Beyond Log-Supermodularity: Lower Bounds and the Bethe Partition
  Function

  A recent result has demonstrated that the Bethe partition function always
lower bounds the true partition function of binary, log-supermodular graphical
models. We demonstrate that these results can be extended to other interesting
classes of graphical models that are not necessarily binary or
log-supermodular: the ferromagnetic Potts model with a uniform external field
and its generalizations and special classes of weighted graph homomorphism
problems.


Message-Passing Algorithms for Quadratic Minimization

  Gaussian belief propagation (GaBP) is an iterative algorithm for computing
the mean of a multivariate Gaussian distribution, or equivalently, the minimum
of a multivariate positive definite quadratic function. Sufficient conditions,
such as walk-summability, that guarantee the convergence and correctness of
GaBP are known, but GaBP may fail to converge to the correct solution given an
arbitrary positive definite quadratic function. As was observed in previous
work, the GaBP algorithm fails to converge if the computation trees produced by
the algorithm are not positive definite. In this work, we will show that the
failure modes of the GaBP algorithm can be understood via graph covers, and we
prove that a parameterized generalization of the min-sum algorithm can be used
to ensure that the computation trees remain positive definite whenever the
input matrix is positive definite. We demonstrate that the resulting algorithm
is closely related to other iterative schemes for quadratic minimization such
as the Gauss-Seidel and Jacobi algorithms. Finally, we observe, empirically,
that there always exists a choice of parameters such that the above
generalization of the GaBP algorithm converges.


Scalable Neural Network Compression and Pruning Using Hard Clustering
  and L1 Regularization

  We propose a simple and easy to implement neural network compression
algorithm that achieves results competitive with more complicated
state-of-the-art methods. The key idea is to modify the original optimization
problem by adding K independent Gaussian priors (corresponding to the k-means
objective) over the network parameters to achieve parameter quantization, as
well as an L1 penalty to achieve pruning. Unlike many existing
quantization-based methods, our method uses hard clustering assignments of
network parameters, which adds minimal change or overhead to standard network
training. We also demonstrate experimentally that tying neural network
parameters provides less gain in generalization performance than changing
network architecture and connectivity patterns entirely.


Message-Passing Algorithms: Reparameterizations and Splittings

  The max-product algorithm, a local message-passing scheme that attempts to
compute the most probable assignment (MAP) of a given probability distribution,
has been successfully employed as a method of approximate inference for
applications arising in coding theory, computer vision, and machine learning.
However, the max-product algorithm is not guaranteed to converge to the MAP
assignment, and if it does, is not guaranteed to recover the MAP assignment.
  Alternative convergent message-passing schemes have been proposed to overcome
these difficulties. This work provides a systematic study of such
message-passing algorithms that extends the known results by exhibiting new
sufficient conditions for convergence to local and/or global optima, providing
a combinatorial characterization of these optima based on graph covers, and
describing a new convergent and correct message-passing algorithm whose
derivation unifies many of the known convergent message-passing algorithms.
  While convergent and correct message-passing algorithms represent a step
forward in the analysis of max-product style message-passing algorithms, the
conditions needed to guarantee convergence to a global optimum can be too
restrictive in both theory and practice. This limitation of convergent and
correct message-passing schemes is characterized by graph covers and
illustrated by example.


Applications of Metric Coinduction

  Metric coinduction is a form of coinduction that can be used to establish
properties of objects constructed as a limit of finite approximations. One can
prove a coinduction step showing that some property is preserved by one step of
the approximation process, then automatically infer by the coinduction
principle that the property holds of the limit object. This can often be used
to avoid complicated analytic arguments involving limits and convergence,
replacing them with simpler algebraic arguments. This paper examines the
application of this principle in a variety of areas, including infinite
streams, Markov chains, Markov decision processes, and non-well-founded sets.
These results point to the usefulness of coinduction as a general proof
technique.


Bethe Learning of Conditional Random Fields via MAP Decoding

  Many machine learning tasks can be formulated in terms of predicting
structured outputs. In frameworks such as the structured support vector machine
(SVM-Struct) and the structured perceptron, discriminative functions are
learned by iteratively applying efficient maximum a posteriori (MAP) decoding.
However, maximum likelihood estimation (MLE) of probabilistic models over these
same structured spaces requires computing partition functions, which is
generally intractable. This paper presents a method for learning discrete
exponential family models using the Bethe approximation to the MLE. Remarkably,
this problem also reduces to iterative (MAP) decoding. This connection emerges
by combining the Bethe approximation with a Frank-Wolfe (FW) algorithm on a
convex dual objective which circumvents the intractable partition function. The
result is a new single loop algorithm MLE-Struct, which is substantially more
efficient than previous double-loop methods for approximate maximum likelihood
estimation. Our algorithm outperforms existing methods in experiments involving
image segmentation, matching problems from vision, and a new dataset of
university roommate assignments.


