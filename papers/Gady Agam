Generating Image Sequence from Description with LSTM Conditional GAN

  Generating images from word descriptions is a challenging task. Generativeadversarial networks(GANs) are shown to be able to generate realistic images ofreal-life objects. In this paper, we propose a new neural network architectureof LSTM Conditional Generative Adversarial Networks to generate images ofreal-life objects. Our proposed model is trained on the Oxford-102 Flowers andCaltech-UCSD Birds-200-2011 datasets. We demonstrate that our proposed modelproduces the better results surpassing other state-of-art approaches.

Learning Classifiers from Synthetic Data Using a Multichannel  Autoencoder

  We propose a method for using synthetic data to help learning classifiers.Synthetic data, even is generated based on real data, normally results in ashift from the distribution of real data in feature space. To bridge the gapbetween the real and synthetic data, and jointly learn from synthetic and realdata, this paper proposes a Multichannel Autoencoder(MCAE). We show that bysuing MCAE, it is possible to learn a better feature representation forclassification. To evaluate the proposed approach, we conduct experiments ontwo types of datasets. Experimental results on two datasets validate theefficiency of our MCAE model and our methodology of generating synthetic data.

Learning from Synthetic Data Using a Stacked Multichannel Autoencoder

  Learning from synthetic data has many important and practical applications.An example of application is photo-sketch recognition. Using synthetic data ischallenging due to the differences in feature distributions between syntheticand real data, a phenomenon we term synthetic gap. In this paper, weinvestigate and formalize a general framework-Stacked Multichannel Autoencoder(SMCAE) that enables bridging the synthetic gap and learning from syntheticdata more efficiently. In particular, we show that our SMCAE can not onlytransform and use synthetic data on the challenging face-sketch recognitiontask, but that it can also help simulate real images, which can be used fortraining classifiers for recognition. Preliminary experiments validate theeffectiveness of the framework.

CGMOS: Certainty Guided Minority OverSampling

  Handling imbalanced datasets is a challenging problem that if not treatedcorrectly results in reduced classification performance. Imbalanced datasetsare commonly handled using minority oversampling, whereas the SMOTE algorithmis a successful oversampling algorithm with numerous extensions. SMOTEextensions do not have a theoretical guarantee during training to work betterthan SMOTE and in many instances their performance is data dependent. In thispaper we propose a novel extension to the SMOTE algorithm with a theoreticalguarantee for improved classification performance. The proposed approachconsiders the classification performance of both the majority and minorityclasses. In the proposed approach CGMOS (Certainty Guided MinorityOverSampling) new data points are added by considering certainty changes in thedataset. The paper provides a proof that the proposed algorithm is guaranteedto work better than SMOTE for training data. Further experimental results on 30real-world datasets show that CGMOS works better than existing algorithms whenusing 6 different classifiers.

Lecture video indexing using boosted margin maximizing neural networks

  This paper presents a novel approach for lecture video indexing using aboosted deep convolutional neural network system. The indexing is performed bymatching high quality slide images, for which text is either known orextracted, to lower resolution video frames with possible noise, perspectivedistortion, and occlusions. We propose a deep neural network integrated with aboosting framework composed of two sub-networks targeting feature extractionand similarity determination to perform the matching. The trained network isgiven as input a pair of slide image and a candidate video frame image andproduces the similarity between them. A boosting framework is integrated intoour proposed network during the training process. Experimental results showthat the proposed approach is much more capable of handling occlusion, spatialtransformations, and other types of noises when compared with known approaches.

Layered Optical Flow Estimation Using a Deep Neural Network with a Soft  Mask

  Using a layered representation for motion estimation has the advantage ofbeing able to cope with discontinuities and occlusions. In this paper, we learnto estimate optical flow by combining a layered motion representation with deeplearning. Instead of pre-segmenting the image to layers, the proposed approachautomatically generates a layered representation of optical flow using theproposed soft-mask module. The essential components of the soft-mask module aremaxout and fuse operations, which enable a disjoint layered representation ofoptical flow and more accurate flow estimation. We show that by using masks themotion estimate results in a quadratic function of input features in the outputlayer. The proposed soft-mask module can be added to any existing optical flowestimation networks by replacing their flow output layer. In this work, we useFlowNet as the base network to which we add the soft-mask module. The resultingnetwork is tested on three well-known benchmarks with both supervised andunsupervised flow estimation tasks. Evaluation results show that the proposednetwork achieve better results compared with the original FlowNet.

