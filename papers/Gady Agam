Generating Image Sequence from Description with LSTM Conditional GAN

  Generating images from word descriptions is a challenging task. Generative
adversarial networks(GANs) are shown to be able to generate realistic images of
real-life objects. In this paper, we propose a new neural network architecture
of LSTM Conditional Generative Adversarial Networks to generate images of
real-life objects. Our proposed model is trained on the Oxford-102 Flowers and
Caltech-UCSD Birds-200-2011 datasets. We demonstrate that our proposed model
produces the better results surpassing other state-of-art approaches.


Learning Classifiers from Synthetic Data Using a Multichannel
  Autoencoder

  We propose a method for using synthetic data to help learning classifiers.
Synthetic data, even is generated based on real data, normally results in a
shift from the distribution of real data in feature space. To bridge the gap
between the real and synthetic data, and jointly learn from synthetic and real
data, this paper proposes a Multichannel Autoencoder(MCAE). We show that by
suing MCAE, it is possible to learn a better feature representation for
classification. To evaluate the proposed approach, we conduct experiments on
two types of datasets. Experimental results on two datasets validate the
efficiency of our MCAE model and our methodology of generating synthetic data.


Learning from Synthetic Data Using a Stacked Multichannel Autoencoder

  Learning from synthetic data has many important and practical applications.
An example of application is photo-sketch recognition. Using synthetic data is
challenging due to the differences in feature distributions between synthetic
and real data, a phenomenon we term synthetic gap. In this paper, we
investigate and formalize a general framework-Stacked Multichannel Autoencoder
(SMCAE) that enables bridging the synthetic gap and learning from synthetic
data more efficiently. In particular, we show that our SMCAE can not only
transform and use synthetic data on the challenging face-sketch recognition
task, but that it can also help simulate real images, which can be used for
training classifiers for recognition. Preliminary experiments validate the
effectiveness of the framework.


CGMOS: Certainty Guided Minority OverSampling

  Handling imbalanced datasets is a challenging problem that if not treated
correctly results in reduced classification performance. Imbalanced datasets
are commonly handled using minority oversampling, whereas the SMOTE algorithm
is a successful oversampling algorithm with numerous extensions. SMOTE
extensions do not have a theoretical guarantee during training to work better
than SMOTE and in many instances their performance is data dependent. In this
paper we propose a novel extension to the SMOTE algorithm with a theoretical
guarantee for improved classification performance. The proposed approach
considers the classification performance of both the majority and minority
classes. In the proposed approach CGMOS (Certainty Guided Minority
OverSampling) new data points are added by considering certainty changes in the
dataset. The paper provides a proof that the proposed algorithm is guaranteed
to work better than SMOTE for training data. Further experimental results on 30
real-world datasets show that CGMOS works better than existing algorithms when
using 6 different classifiers.


Lecture video indexing using boosted margin maximizing neural networks

  This paper presents a novel approach for lecture video indexing using a
boosted deep convolutional neural network system. The indexing is performed by
matching high quality slide images, for which text is either known or
extracted, to lower resolution video frames with possible noise, perspective
distortion, and occlusions. We propose a deep neural network integrated with a
boosting framework composed of two sub-networks targeting feature extraction
and similarity determination to perform the matching. The trained network is
given as input a pair of slide image and a candidate video frame image and
produces the similarity between them. A boosting framework is integrated into
our proposed network during the training process. Experimental results show
that the proposed approach is much more capable of handling occlusion, spatial
transformations, and other types of noises when compared with known approaches.


Layered Optical Flow Estimation Using a Deep Neural Network with a Soft
  Mask

  Using a layered representation for motion estimation has the advantage of
being able to cope with discontinuities and occlusions. In this paper, we learn
to estimate optical flow by combining a layered motion representation with deep
learning. Instead of pre-segmenting the image to layers, the proposed approach
automatically generates a layered representation of optical flow using the
proposed soft-mask module. The essential components of the soft-mask module are
maxout and fuse operations, which enable a disjoint layered representation of
optical flow and more accurate flow estimation. We show that by using masks the
motion estimate results in a quadratic function of input features in the output
layer. The proposed soft-mask module can be added to any existing optical flow
estimation networks by replacing their flow output layer. In this work, we use
FlowNet as the base network to which we add the soft-mask module. The resulting
network is tested on three well-known benchmarks with both supervised and
unsupervised flow estimation tasks. Evaluation results show that the proposed
network achieve better results compared with the original FlowNet.


