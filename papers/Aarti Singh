Agent Development Toolkits

  Development of agents as well as their wide usage requires good underlying
infrastructure. Literature indicates scarcity of agent development tools in
initial years of research which limited the exploitation of this beneficial
technology. However, today a wide variety of tools are available, for
developing robust infrastructure. This technical note provides a deep overview
of such tools and contrasts features provided by them.


Detecting Weak but Hierarchically-Structured Patterns in Networks

  The ability to detect weak distributed activation patterns in networks is
critical to several applications, such as identifying the onset of anomalous
activity or incipient congestion in the Internet, or faint traces of a
biochemical spread by a sensor network. This is a challenging problem since
weak distributed patterns can be invisible in per node statistics as well as a
global network-wide aggregate. Most prior work considers situations in which
the activation/non-activation of each node is statistically independent, but
this is unrealistic in many problems. In this paper, we consider structured
patterns arising from statistical dependencies in the activation process. Our
contributions are three-fold. First, we propose a sparsifying transform that
succinctly represents structured activation patterns that conform to a
hierarchical dependency graph. Second, we establish that the proposed transform
facilitates detection of very weak activation patterns that cannot be detected
with existing methods. Third, we show that the structure of the hierarchical
dependency graph governing the activation process, and hence the network
transform, can be learnt from very few (logarithmic in network size)
independent snapshots of network activity.


Optimal rates for first-order stochastic convex optimization under
  Tsybakov noise condition

  We focus on the problem of minimizing a convex function $f$ over a convex set
$S$ given $T$ queries to a stochastic first order oracle. We argue that the
complexity of convex minimization is only determined by the rate of growth of
the function around its minimizer $x^*_{f,S}$, as quantified by a Tsybakov-like
noise condition. Specifically, we prove that if $f$ grows at least as fast as
$\|x-x^*_{f,S}\|^\kappa$ around its minimum, for some $\kappa > 1$, then the
optimal rate of learning $f(x^*_{f,S})$ is
$\Theta(T^{-\frac{\kappa}{2\kappa-2}})$. The classic rate $\Theta(1/\sqrt T)$
for convex functions and $\Theta(1/T)$ for strongly convex functions are
special cases of our result for $\kappa \rightarrow \infty$ and $\kappa=2$, and
even faster rates are attained for $\kappa <2$. We also derive tight bounds for
the complexity of learning $x_{f,S}^*$, where the optimal rate is
$\Theta(T^{-\frac{1}{2\kappa-2}})$. Interestingly, these precise rates for
convex optimization also characterize the complexity of active learning and our
results further strengthen the connections between the two fields, both of
which rely on feedback-driven queries.


Subspace Learning from Extremely Compressed Measurements

  We consider learning the principal subspace of a large set of vectors from an
extremely small number of compressive measurements of each vector. Our
theoretical results show that even a constant number of measurements per column
suffices to approximate the principal subspace to arbitrary precision, provided
that the number of vectors is large. This result is achieved by a simple
algorithm that computes the eigenvectors of an estimate of the covariance
matrix. The main insight is to exploit an averaging effect that arises from
applying a different random projection to each vector. We provide a number of
simulations confirming our theoretical results.


Risk Bounds For Mode Clustering

  Density mode clustering is a nonparametric clustering method. The clusters
are the basins of attraction of the modes of a density estimator. We study the
risk of mode-based clustering. We show that the clustering risk over the
cluster cores --- the regions where the density is high --- is very small even
in high dimensions. And under a low noise condition, the overall cluster risk
is small even beyond the cores, in high dimensions.


On the Bootstrap for Persistence Diagrams and Landscapes

  Persistent homology probes topological properties from point clouds and
functions. By looking at multiple scales simultaneously, one can record the
births and deaths of topological features as the scale varies. In this paper we
use a statistical technique, the empirical bootstrap, to separate topological
signal from topological noise. In particular, we derive confidence sets for
persistence diagrams and confidence bands for persistence landscapes.


Feature Selection For High-Dimensional Clustering

  We present a nonparametric method for selecting informative features in
high-dimensional clustering problems. We start with a screening step that uses
a test for multimodality. Then we apply kernel density estimation and mode
clustering to the selected features. The output of the method consists of a
list of relevant features, and cluster assignments. We provide explicit bounds
on the error rate of the resulting clustering. In addition, we provide the
first error bounds on mode based clustering.


Signal Representations on Graphs: Tools and Applications

  We present a framework for representing and modeling data on graphs. Based on
this framework, we study three typical classes of graph signals: smooth graph
signals, piecewise-constant graph signals, and piecewise-smooth graph signals.
For each class, we provide an explicit definition of the graph signals and
construct a corresponding graph dictionary with desirable properties. We then
study how such graph dictionary works in two standard tasks: approximation and
sampling followed with recovery, both from theoretical as well as algorithmic
perspectives. Finally, for each class, we present a case study of a real-world
problem by using the proposed methodology.


Stochastic Zeroth-order Optimization in High Dimensions

  We consider the problem of optimizing a high-dimensional convex function
using stochastic zeroth-order queries. Under sparsity assumptions on the
gradients or function values, we present two algorithms: a successive
component/feature selection algorithm and a noisy mirror descent algorithm
using Lasso gradient estimates, and show that both algorithms have convergence
rates that de- pend only logarithmically on the ambient dimension of the
problem. Empirical results confirm our theoretical findings and show that the
algorithms we design outperform classical zeroth-order optimization methods in
the high-dimensional setting.


Adaptive Hausdorff estimation of density level sets

  Consider the problem of estimating the $\gamma$-level set
$G^*_{\gamma}=\{x:f(x)\geq\gamma\}$ of an unknown $d$-dimensional density
function $f$ based on $n$ independent observations $X_1,...,X_n$ from the
density. This problem has been addressed under global error criteria related to
the symmetric set difference. However, in certain applications a spatially
uniform mode of convergence is desirable to ensure that the estimated set is
close to the target set everywhere. The Hausdorff error criterion provides this
degree of uniformity and, hence, is more appropriate in such situations. It is
known that the minimax optimal rate of error convergence for the Hausdorff
metric is $(n/\log n)^{-1/(d+2\alpha)}$ for level sets with boundaries that
have a Lipschitz functional form, where the parameter $\alpha$ characterizes
the regularity of the density around the level of interest. However, the
estimators proposed in previous work are nonadaptive to the density regularity
and require knowledge of the parameter $\alpha$. Furthermore, previously
developed estimators achieve the minimax optimal rate for rather restricted
classes of sets (e.g., the boundary fragment and star-shaped sets) that
effectively reduce the set estimation problem to a function estimation problem.
This characterization precludes level sets with multiple connected components,
which are fundamental to many applications. This paper presents a fully
data-driven procedure that is adaptive to unknown regularity conditions and
achieves near minimax optimal Hausdorff error control for a class of density
level sets with very general shapes and multiple connected components.


Stability of Density-Based Clustering

  High density clusters can be characterized by the connected components of a
level set $L(\lambda) = \{x:\ p(x)>\lambda\}$ of the underlying probability
density function $p$ generating the data, at some appropriate level
$\lambda\geq 0$. The complete hierarchical clustering can be characterized by a
cluster tree ${\cal T}= \bigcup_{\lambda} L(\lambda)$. In this paper, we study
the behavior of a density level set estimate $\widehat L(\lambda)$ and cluster
tree estimate $\widehat{\cal{T}}$ based on a kernel density estimator with
kernel bandwidth $h$. We define two notions of instability to measure the
variability of $\widehat L(\lambda)$ and $\widehat{\cal{T}}$ as a function of
$h$, and investigate the theoretical properties of these instability measures.


Active Clustering: Robust and Efficient Hierarchical Clustering using
  Adaptively Selected Similarities

  Hierarchical clustering based on pairwise similarities is a common tool used
in a broad range of scientific applications. However, in many problems it may
be expensive to obtain or compute similarities between the items to be
clustered. This paper investigates the hierarchical clustering of N items based
on a small subset of pairwise similarities, significantly less than the
complete set of N(N-1)/2 similarities. First, we show that if the intracluster
similarities exceed intercluster similarities, then it is possible to correctly
determine the hierarchical clustering from as few as 3N log N similarities. We
demonstrate this order of magnitude savings in the number of pairwise
similarities necessitates sequentially selecting which similarities to obtain
in an adaptive fashion, rather than picking them at random. We then propose an
active clustering method that is robust to a limited fraction of anomalous
similarities, and show how even in the presence of these noisy similarity
values we can resolve the hierarchical clustering using only O(N log^2 N)
pairwise similarities.


Recovering Block-structured Activations Using Compressive Measurements

  We consider the problems of detection and localization of a contiguous block
of weak activation in a large matrix, from a small number of noisy, possibly
adaptive, compressive (linear) measurements. This is closely related to the
problem of compressed sensing, where the task is to estimate a sparse vector
using a small number of linear measurements. Contrary to results in compressed
sensing, where it has been shown that neither adaptivity nor contiguous
structure help much, we show that for reliable localization the magnitude of
the weakest signals is strongly influenced by both structure and the ability to
choose measurements adaptively while for detection neither adaptivity nor
structure reduce the requirement on the magnitude of the signal. We
characterize the precise tradeoffs between the various problem parameters, the
signal strength and the number of measurements required to reliably detect and
localize the block of activation. The sufficient conditions are complemented
with information theoretic lower bounds.


Distribution-Free Distribution Regression

  `Distribution regression' refers to the situation where a response Y depends
on a covariate P where P is a probability distribution. The model is Y=f(P) +
mu where f is an unknown regression function and mu is a random error.
Typically, we do not observe P directly, but rather, we observe a sample from
P. In this paper we develop theory and methods for distribution-free versions
of distribution regression. This means that we do not make distributional
assumptions about the error term mu and covariate P. We prove that when the
effective dimension is small enough (as measured by the doubling dimension),
then the excess prediction risk converges to zero with a polynomial rate.


Confidence sets for persistence diagrams

  Persistent homology is a method for probing topological properties of point
clouds and functions. The method involves tracking the birth and death of
topological features (2000) as one varies a tuning parameter. Features with
short lifetimes are informally considered to be "topological noise," and those
with a long lifetime are considered to be "topological signal." In this paper,
we bring some statistical ideas to persistent homology. In particular, we
derive confidence sets that allow us to separate topological signal from
topological noise.


Low-Rank Matrix and Tensor Completion via Adaptive Sampling

  We study low rank matrix and tensor completion and propose novel algorithms
that employ adaptive sampling schemes to obtain strong performance guarantees.
Our algorithms exploit adaptivity to identify entries that are highly
informative for learning the column space of the matrix (tensor) and
consequently, our results hold even when the row space is highly coherent, in
contrast with previous analyses. In the absence of noise, we show that one can
exactly recover a $n \times n$ matrix of rank $r$ from merely $\Omega(n
r^{3/2}\log(r))$ matrix entries. We also show that one can recover an order $T$
tensor using $\Omega(n r^{T-1/2}T^2 \log(r))$ entries. For noisy recovery, our
algorithm consistently estimates a low rank matrix corrupted with noise using
$\Omega(n r^{3/2} \textrm{polylog}(n))$ entries. We complement our study with
simulations that verify our theory and demonstrate the scalability of our
algorithms.


Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean
  Separation

  While several papers have investigated computationally and statistically
efficient methods for learning Gaussian mixtures, precise minimax bounds for
their statistical performance as well as fundamental limits in high-dimensional
settings are not well-understood. In this paper, we provide precise information
theoretic bounds on the clustering accuracy and sample complexity of learning a
mixture of two isotropic Gaussians in high dimensions under small mean
separation. If there is a sparse subset of relevant dimensions that determine
the mean separation, then the sample complexity only depends on the number of
relevant dimensions and mean separation, and can be achieved by a simple
computationally efficient procedure. Our results provide the first step of a
theoretical basis for recent methods that combine feature selection and
clustering.


Cluster Trees on Manifolds

  In this paper we investigate the problem of estimating the cluster tree for a
density $f$ supported on or near a smooth $d$-dimensional manifold $M$
isometrically embedded in $\mathbb{R}^D$. We analyze a modified version of a
$k$-nearest neighbor based algorithm recently proposed by Chaudhuri and
Dasgupta. The main results of this paper show that under mild assumptions on
$f$ and $M$, we obtain rates of convergence that depend on $d$ only but not on
the ambient dimension $D$. We also show that similar (albeit non-algorithmic)
results can be obtained for kernel density estimators. We sketch a construction
of a sample complexity lower bound instance for a natural class of manifold
oblivious clustering algorithms. We further briefly consider the known manifold
case and show that in this case a spatially adaptive algorithm achieves better
rates.


Tight Lower Bounds for Homology Inference

  The homology groups of a manifold are important topological invariants that
provide an algebraic summary of the manifold. These groups contain rich
topological information, for instance, about the connected components, holes,
tunnels and sometimes the dimension of the manifold. In earlier work, we have
considered the statistical problem of estimating the homology of a manifold
from noiseless samples and from noisy samples under several different noise
models. We derived upper and lower bounds on the minimax risk for this problem.
In this note we revisit the noiseless case. In previous work we used Le Cam's
lemma to establish a lower bound that differed from the upper bound of Niyogi,
Smale and Weinberger by a polynomial factor in the condition number.
  In this note we use a different construction based on the direct analysis of
the likelihood ratio test to show that the upper bound of Niyogi, Smale and
Weinberger is in fact tight, thus establishing rate optimal asymptotic minimax
bounds for the problem. The techniques we use here extend in a straightforward
way to the noisy settings considered in our earlier work.


Graph Connectivity in Noisy Sparse Subspace Clustering

  Subspace clustering is the problem of clustering data points into a union of
low-dimensional linear/affine subspaces. It is the mathematical abstraction of
many important problems in computer vision, image processing and machine
learning. A line of recent work (4, 19, 24, 20) provided strong theoretical
guarantee for sparse subspace clustering (4), the state-of-the-art algorithm
for subspace clustering, on both noiseless and noisy data sets. It was shown
that under mild conditions, with high probability no two points from different
subspaces are clustered together. Such guarantee, however, is not sufficient
for the clustering to be correct, due to the notorious "graph connectivity
problem" (15). In this paper, we investigate the graph connectivity problem for
noisy sparse subspace clustering and show that a simple post-processing
procedure is capable of delivering consistent clustering under certain "general
position" or "restricted eigenvalue" assumptions. We also show that our
condition is almost tight with adversarial noise perturbation by constructing a
counter-example. These results provide the first exact clustering guarantee of
noisy SSC for subspaces of dimension greater then 3.


Signal Recovery on Graphs: Random versus Experimentally Designed
  Sampling

  We study signal recovery on graphs based on two sampling strategies: random
sampling and experimentally designed sampling. We propose a new class of smooth
graph signals, called approximately bandlimited, which generalizes the
bandlimited class and is similar to the globally smooth class. We then propose
two recovery strategies based on random sampling and experimentally designed
sampling. The proposed recovery strategy based on experimentally designed
sampling is similar to the leverage scores used in the matrix approximation. We
show that while both strategies are unbiased estimators for the low-frequency
components, the convergence rate of experimentally designed sampling is much
faster than that of random sampling when a graph is irregular. We validate the
proposed recovery strategies on three specific graphs: a ring graph, an
Erd\H{o}s-R\'enyi graph, and a star graph. The simulation results support the
theoretical analysis.


Adaptive Semisupervised Inference

  Semisupervised methods inevitably invoke some assumption that links the
marginal distribution of the features to the regression function of the label.
Most commonly, the cluster or manifold assumptions are used which imply that
the regression function is smooth over high-density clusters or manifolds
supporting the data. A generalization of these assumptions is that the
regression function is smooth with respect to some density sensitive distance.
This motivates the use of a density based metric for semisupervised learning.
We analyze this setting and make the following contributions - (a) we propose a
semi-supervised learner that uses a density-sensitive kernel and show that it
provides better performance than any supervised learner if the density support
set has a small condition number and (b) we show that it is possible to adapt
to the degree of semi-supervisedness using data-dependent choice of a parameter
that controls sensitivity of the distance metric to the density. This ensures
that the semisupervised learner never performs worse than a supervised learner
even if the assumptions fail to hold.


FuSSO: Functional Shrinkage and Selection Operator

  We present the FuSSO, a functional analogue to the LASSO, that efficiently
finds a sparse set of functional input covariates to regress a real-valued
response against. The FuSSO does so in a semi-parametric fashion, making no
parametric assumptions about the nature of input functional covariates and
assuming a linear form to the mapping of functional covariates to the response.
We provide a statistical backing for use of the FuSSO via proof of asymptotic
sparsistency under various conditions. Furthermore, we observe good results on
both synthetic and real-world data.


On the Decreasing Power of Kernel and Distance based Nonparametric
  Hypothesis Tests in High Dimensions

  This paper is about two related decision theoretic problems, nonparametric
two-sample testing and independence testing. There is a belief that two
recently proposed solutions, based on kernels and distances between pairs of
points, behave well in high-dimensional settings. We identify different sources
of misconception that give rise to the above belief. Specifically, we
differentiate the hardness of estimation of test statistics from the hardness
of testing whether these statistics are zero or not, and explicitly discuss a
notion of "fair" alternative hypotheses for these problems as dimension
increases. We then demonstrate that the power of these tests actually drops
polynomially with increasing dimension against fair alternatives. We end with
some theoretical insights and shed light on the \textit{median heuristic} for
kernel bandwidth selection. Our work advances the current understanding of the
power of modern nonparametric hypothesis tests in high dimensions.


Efficient Sparse Clustering of High-Dimensional Non-spherical Gaussian
  Mixtures

  We consider the problem of clustering data points in high dimensions, i.e.
when the number of data points may be much smaller than the number of
dimensions. Specifically, we consider a Gaussian mixture model (GMM) with
non-spherical Gaussian components, where the clusters are distinguished by only
a few relevant dimensions. The method we propose is a combination of a recent
approach for learning parameters of a Gaussian mixture model and sparse linear
discriminant analysis (LDA). In addition to cluster assignments, the method
returns an estimate of the set of features relevant for clustering. Our results
indicate that the sample complexity of clustering depends on the sparsity of
the relevant feature set, while only scaling logarithmically with the ambient
dimension. Additionally, we require much milder assumptions than existing work
on clustering in high dimensions. In particular, we do not require spherical
clusters nor necessitate mean separation along relevant dimensions.


Density-sensitive semisupervised inference

  Semisupervised methods are techniques for using labeled data
$(X_1,Y_1),\ldots,(X_n,Y_n)$ together with unlabeled data $X_{n+1},\ldots,X_N$
to make predictions. These methods invoke some assumptions that link the
marginal distribution $P_X$ of X to the regression function f(x). For example,
it is common to assume that f is very smooth over high density regions of
$P_X$. Many of the methods are ad-hoc and have been shown to work in specific
examples but are lacking a theoretical foundation. We provide a minimax
framework for analyzing semisupervised methods. In particular, we study methods
based on metrics that are sensitive to the distribution $P_X$. Our model
includes a parameter $\alpha$ that controls the strength of the semisupervised
assumption. We then use the data to adapt to $\alpha$.


Changepoint Detection over Graphs with the Spectral Scan Statistic

  We consider the change-point detection problem of deciding, based on noisy
measurements, whether an unknown signal over a given graph is constant or is
instead piecewise constant over two connected induced subgraphs of relatively
low cut size. We analyze the corresponding generalized likelihood ratio (GLR)
statistics and relate it to the problem of finding a sparsest cut in a graph.
We develop a tractable relaxation of the GLR statistic based on the
combinatorial Laplacian of the graph, which we call the spectral scan
statistic, and analyze its properties. We show how its performance as a testing
procedure depends directly on the spectrum of the graph, and use this result to
explicitly derive its asymptotic properties on few significant graph
topologies. Finally, we demonstrate both theoretically and by simulations that
the spectral scan statistic can outperform naive testing procedures based on
edge thresholding and $\chi^2$ testing.


Detecting Activations over Graphs using Spanning Tree Wavelet Bases

  We consider the detection of activations over graphs under Gaussian noise,
where signals are piece-wise constant over the graph. Despite the wide
applicability of such a detection algorithm, there has been little success in
the development of computationally feasible methods with proveable theoretical
guarantees for general graph topologies. We cast this as a hypothesis testing
problem, and first provide a universal necessary condition for asymptotic
distinguishability of the null and alternative hypotheses. We then introduce
the spanning tree wavelet basis over graphs, a localized basis that reflects
the topology of the graph, and prove that for any spanning tree, this approach
can distinguish null from alternative in a low signal-to-noise regime. Lastly,
we improve on this result and show that using the uniform spanning tree in the
basis construction yields a randomized test with stronger theoretical
guarantees that in many cases matches our necessary conditions. Specifically,
we obtain near-optimal performance in edge transitive graphs, $k$-nearest
neighbor graphs, and $\epsilon$-graphs.


Efficient Active Algorithms for Hierarchical Clustering

  Advances in sensing technologies and the growth of the internet have resulted
in an explosion in the size of modern datasets, while storage and processing
power continue to lag behind. This motivates the need for algorithms that are
efficient, both in terms of the number of measurements needed and running time.
To combat the challenges associated with large datasets, we propose a general
framework for active hierarchical clustering that repeatedly runs an
off-the-shelf clustering algorithm on small subsets of the data and comes with
guarantees on performance, measurement complexity and runtime complexity. We
instantiate this framework with a simple spectral clustering algorithm and
provide concrete results on its performance, showing that, under some
assumptions, this algorithm recovers all clusters of size ?(log n) using O(n
log^2 n) similarities and runs in O(n log^3 n) time for a dataset of n objects.
Through extensive experimentation we also demonstrate that this framework is
practically alluring.


On Computationally Tractable Selection of Experiments in
  Measurement-Constrained Regression Models

  We derive computationally tractable methods to select a small subset of
experiment settings from a large pool of given design points. The primary focus
is on linear regression models, while the technique extends to generalized
linear models and Delta's method (estimating functions of linear regression
models) as well. The algorithms are based on a continuous relaxation of an
otherwise intractable combinatorial optimization problem, with sampling or
greedy procedures as post-processing steps. Formal approximation guarantees are
established for both algorithms, and numerical results on both synthetic and
real-world data confirm the effectiveness of the proposed methods.


Minimax Lower Bounds for Linear Independence Testing

  Linear independence testing is a fundamental information-theoretic and
statistical problem that can be posed as follows: given $n$ points
$\{(X_i,Y_i)\}^n_{i=1}$ from a $p+q$ dimensional multivariate distribution
where $X_i \in \mathbb{R}^p$ and $Y_i \in\mathbb{R}^q$, determine whether $a^T
X$ and $b^T Y$ are uncorrelated for every $a \in \mathbb{R}^p, b\in
\mathbb{R}^q$ or not. We give minimax lower bound for this problem (when $p+q,n
\to \infty$, $(p+q)/n \leq \kappa < \infty$, without sparsity assumptions). In
summary, our results imply that $n$ must be at least as large as $\sqrt
{pq}/\|\Sigma_{XY}\|_F^2$ for any procedure (test) to have non-trivial power,
where $\Sigma_{XY}$ is the cross-covariance matrix of $X,Y$. We also provide
some evidence that the lower bound is tight, by connections to two-sample
testing and regression in specific settings.


Noise-Tolerant Interactive Learning from Pairwise Comparisons

  We study the problem of interactively learning a binary classifier using
noisy labeling and pairwise comparison oracles, where the comparison oracle
answers which one in the given two instances is more likely to be positive.
Learning from such oracles has multiple applications where obtaining direct
labels is harder but pairwise comparisons are easier, and the algorithm can
leverage both types of oracles. In this paper, we attempt to characterize how
the access to an easier comparison oracle helps in improving the label and
total query complexity. We show that the comparison oracle reduces the learning
problem to that of learning a threshold function. We then present an algorithm
that interactively queries the label and comparison oracles and we characterize
its query complexity under Tsybakov and adversarial noise conditions for the
comparison and labeling oracles. Our lower bounds show that our label and total
query complexity is almost optimal.


Minimax Rates for Homology Inference

  Often, high dimensional data lie close to a low-dimensional submanifold and
it is of interest to understand the geometry of these submanifolds. The
homology groups of a manifold are important topological invariants that provide
an algebraic summary of the manifold. These groups contain rich topological
information, for instance, about the connected components, holes, tunnels and
sometimes the dimension of the manifold. In this paper, we consider the
statistical problem of estimating the homology of a manifold from noisy samples
under several different noise models. We derive upper and lower bounds on the
minimax risk for this problem. Our upper bounds are based on estimators which
are constructed from a union of balls of appropriate radius around carefully
selected points. In each case we establish complementary lower bounds using Le
Cam's lemma.


Recovering Graph-Structured Activations using Adaptive Compressive
  Measurements

  We study the localization of a cluster of activated vertices in a graph, from
adaptively designed compressive measurements. We propose a hierarchical
partitioning of the graph that groups the activated vertices into few
partitions, so that a top-down sensing procedure can identify these partitions,
and hence the activations, using few measurements. By exploiting the cluster
structure, we are able to provide localization guarantees at weaker signal to
noise ratios than in the unstructured setting. We complement this performance
guarantee with an information theoretic lower bound, providing a necessary
signal-to-noise ratio for any algorithm to successfully localize the cluster.
We verify our analysis with some simulations, demonstrating the practicality of
our algorithm.


Near-optimal Anomaly Detection in Graphs using Lovasz Extended Scan
  Statistic

  The detection of anomalous activity in graphs is a statistical problem that
arises in many applications, such as network surveillance, disease outbreak
detection, and activity monitoring in social networks. Beyond its wide
applicability, graph structured anomaly detection serves as a case study in the
difficulty of balancing computational complexity with statistical power. In
this work, we develop from first principles the generalized likelihood ratio
test for determining if there is a well connected region of activation over the
vertices in the graph in Gaussian noise. Because this test is computationally
infeasible, we provide a relaxation, called the Lovasz extended scan statistic
(LESS) that uses submodularity to approximate the intractable generalized
likelihood ratio. We demonstrate a connection between LESS and maximum
a-posteriori inference in Markov random fields, which provides us with a
poly-time algorithm for LESS. Using electrical network theory, we are able to
control type 1 error for LESS and prove conditions under which LESS is risk
consistent. Finally, we consider specific graph models, the torus, k-nearest
neighbor graphs, and epsilon-random graphs. We show that on these graphs our
results provide near-optimal performance by matching our results to known lower
bounds.


Extreme Compressive Sampling for Covariance Estimation

  This paper studies the problem of estimating the covariance of a collection
of vectors using only highly compressed measurements of each vector. An
estimator based on back-projections of these compressive samples is proposed
and analyzed. A distribution-free analysis shows that by observing just a
single linear measurement of each vector, one can consistently estimate the
covariance matrix, in both infinity and spectral norm, and this same analysis
leads to precise rates of convergence in both norms. Via information-theoretic
techniques, lower bounds showing that this estimator is minimax-optimal for
both infinity and spectral norm estimation problems are established. These
results are also specialized to give matching upper and lower bounds for
estimating the population covariance of a collection of Gaussian vectors, again
in the compressive measurement model. The analysis conducted in this paper
shows that the effective sample complexity for this problem is scaled by a
factor of $m^2/d^2$ where $m$ is the compression dimension and $d$ is the
ambient dimension. Applications to subspace learning (Principal Components
Analysis) and learning over distributed sensor networks are also discussed.


A statistical perspective of sampling scores for linear regression

  In this paper, we consider a statistical problem of learning a linear model
from noisy samples. Existing work has focused on approximating the least
squares solution by using leverage-based scores as an importance sampling
distribution. However, no finite sample statistical guarantees and no
computationally efficient optimal sampling strategies have been proposed. To
evaluate the statistical properties of different sampling strategies, we
propose a simple yet effective estimator, which is easy for theoretical
analysis and is useful in multitask linear regression. We derive the exact mean
square error of the proposed estimator for any given sampling scores. Based on
minimizing the mean square error, we propose the optimal sampling scores for
both estimator and predictor, and show that they are influenced by the
noise-to-signal ratio. Numerical simulations match the theoretical analysis
well.


Data Poisoning Attacks on Factorization-Based Collaborative Filtering

  Recommendation and collaborative filtering systems are important in modern
information and e-commerce applications. As these systems are becoming
increasingly popular in the industry, their outputs could affect business
decision making, introducing incentives for an adversarial party to compromise
the availability or integrity of such systems. We introduce a data poisoning
attack on collaborative filtering systems. We demonstrate how a powerful
attacker with full knowledge of the learner can generate malicious data so as
to maximize his/her malicious objectives, while at the same time mimicking
normal user behavior to avoid being detected. While the complete knowledge
assumption seems extreme, it enables a robust assessment of the vulnerability
of collaborative filtering schemes to highly motivated attacks. We present
efficient solutions for two popular factorization-based collaborative filtering
algorithms: the \emph{alternative minimization} formulation and the
\emph{nuclear norm minimization} method. Finally, we test the effectiveness of
our proposed algorithms on real-world data and discuss potential defensive
strategies.


Hypothesis Transfer Learning via Transformation Functions

  We consider the Hypothesis Transfer Learning (HTL) problem where one
incorporates a hypothesis trained on the source domain into the learning
procedure of the target domain. Existing theoretical analysis either only
studies specific algorithms or only presents upper bounds on the generalization
error but not on the excess risk. In this paper, we propose a unified
algorithm-dependent framework for HTL through a novel notion of transformation
function, which characterizes the relation between the source and the target
domains. We conduct a general risk analysis of this framework and in
particular, we show for the first time, if two domains are related, HTL enjoys
faster convergence rates of excess risks for Kernel Smoothing and Kernel Ridge
Regression than those of the classical non-transfer learning settings.
Experiments on real world data demonstrate the effectiveness of our framework.


Computationally Efficient Robust Estimation of Sparse Functionals

  Many conventional statistical procedures are extremely sensitive to seemingly
minor deviations from modeling assumptions. This problem is exacerbated in
modern high-dimensional settings, where the problem dimension can grow with and
possibly exceed the sample size. We consider the problem of robust estimation
of sparse functionals, and provide a computationally and statistically
efficient algorithm in the high-dimensional setting. Our theory identifies a
unified set of deterministic conditions under which our algorithm guarantees
accurate recovery. By further establishing that these deterministic conditions
hold with high-probability for a wide range of statistical models, our theory
applies to many problems of considerable interest including sparse mean and
covariance estimation; sparse linear regression; and sparse generalized linear
models.


Gradient Descent Can Take Exponential Time to Escape Saddle Points

  Although gradient descent (GD) almost always escapes saddle points
asymptotically [Lee et al., 2016], this paper shows that even with fairly
natural random initialization schemes and non-pathological functions, GD can be
significantly slowed down by saddle points, taking exponential time to escape.
On the other hand, gradient descent with perturbations [Ge et al., 2015, Jin et
al., 2017] is not slowed down by saddle points - it can find an approximate
local minimizer in polynomial time. This result implies that GD is inherently
slower than perturbed GD, and justifies the importance of adding perturbations
for efficient non-convex optimization. While our focus is theoretical, we also
present experiments that illustrate our theoretical findings.


Near-Optimal Discrete Optimization for Experimental Design: A Regret
  Minimization Approach

  The experimental design problem concerns the selection of k points from a
potentially large design pool of p-dimensional vectors, so as to maximize the
statistical efficiency regressed on the selected k design points. Statistical
efficiency is measured by optimality criteria, including A(verage),
D(eterminant), T(race), E(igen), V(ariance) and G-optimality. Except for the
T-optimality, exact optimization is NP-hard.
  We propose a polynomial-time regret minimization framework to achieve a
$(1+\varepsilon)$ approximation with only $O(p/\varepsilon^2)$ design points,
for all the optimality criteria above.
  In contrast, to the best of our knowledge, before our work, no
polynomial-time algorithm achieves $(1+\varepsilon)$ approximations for
D/E/G-optimality, and the best poly-time algorithm achieving
$(1+\varepsilon)$-approximation for A/V-optimality requires $k =
\Omega(p^2/\varepsilon)$ design points.


Towards Understanding the Generalization Bias of Two Layer Convolutional
  Linear Classifiers with Gradient Descent

  A major challenge in understanding the generalization of deep learning is to
explain why (stochastic) gradient descent can exploit the network architecture
to find solutions that have good generalization performance when using high
capacity models. We find simple but realistic examples showing that this
phenomenon exists even when learning linear classifiers --- between two linear
networks with the same capacity, the one with a convolutional layer can
generalize better than the other when the data distribution has some underlying
spatial structure. We argue that this difference results from a combination of
the convolution architecture, data distribution and gradient descent, all of
which are necessary to be included in a meaningful analysis. We provide a
general analysis of the generalization performance as a function of data
distribution and convolutional filter size, given gradient descent as the
optimization algorithm, then interpret the results using concrete examples.
Experimental results show that our analysis is able to explain what happens in
our introduced examples.


Multiresolution Representations for Piecewise-Smooth Signals on Graphs

  What is a mathematically rigorous way to describe the taxi-pickup
distribution in Manhattan, or the profile information in online social
networks? A deep understanding of representing those data not only provides
insights to the data properties, but also benefits to many subsequent
processing procedures, such as denoising, sampling, recovery and localization.
In this paper, we model those complex and irregular data as piecewise-smooth
graph signals and propose a graph dictionary to effectively represent those
graph signals. We first propose the graph multiresolution analysis, which
provides a principle to design good representations. We then propose a
coarse-to-fine approach, which iteratively partitions a graph into two
subgraphs until we reach individual nodes. This approach efficiently implements
the graph multiresolution analysis and the induced graph dictionary promotes
sparse representations piecewise-smooth graph signals. Finally, we validate the
proposed graph dictionary on two tasks: approximation and localization. The
empirical results show that the proposed graph dictionary outperforms eight
other representation methods on six datasets, including traffic networks,
social networks and point cloud meshes.


Robust Nonparametric Regression under Huber's $ε$-contamination
  Model

  We consider the non-parametric regression problem under Huber's
$\epsilon$-contamination model, in which an $\epsilon$ fraction of observations
are subject to arbitrary adversarial noise. We first show that a simple local
binning median step can effectively remove the adversary noise and this median
estimator is minimax optimal up to absolute constants over the H\"{o}lder
function class with smoothness parameters smaller than or equal to 1.
Furthermore, when the underlying function has higher smoothness, we show that
using local binning median as pre-preprocessing step to remove the adversarial
noise, then we can apply any non-parametric estimator on top of the medians. In
particular we show local median binning followed by kernel smoothing and local
polynomial regression achieve minimaxity over H\"{o}lder and Sobolev classes
with arbitrary smoothness parameters. Our main proof technique is a decoupled
analysis of adversary noise and stochastic noise, which can be potentially
applied to other robust estimation problems. We also provide numerical results
to verify the effectiveness of our proposed methods.


PeerReview4All: Fair and Accurate Reviewer Assignment in Peer Review

  We consider the problem of automated assignment of papers to reviewers in
conference peer review, with a focus on fairness and statistical accuracy. Our
fairness objective is to maximize the review quality of the most disadvantaged
paper, in contrast to the commonly used objective of maximizing the total
quality over all papers. We design an assignment algorithm based on an
incremental max-flow procedure that we prove is near-optimally fair. Our
statistical accuracy objective is to ensure correct recovery of the papers that
should be accepted. We provide a sharp minimax analysis of the accuracy of the
peer-review process for a popular objective-score model as well as for a novel
subjective-score model that we propose in the paper. Our analysis proves that
our proposed assignment algorithm also leads to a near-optimal statistical
accuracy. Finally, we design a novel experiment that allows for an objective
comparison of various assignment algorithms, and overcomes the inherent
difficulty posed by the absence of a ground truth in experiments on
peer-review. The results of this experiment corroborate the theoretical
guarantees of our algorithm.


Gradient Descent Provably Optimizes Over-parameterized Neural Networks

  One of the mysteries in the success of neural networks is randomly
initialized first order methods like gradient descent can achieve zero training
loss even though the objective function is non-convex and non-smooth. This
paper demystifies this surprising phenomenon for two-layer fully connected ReLU
activated neural networks. For an $m$ hidden node shallow neural network with
ReLU activation and $n$ training data, we show as long as $m$ is large enough
and no two inputs are parallel, randomly initialized gradient descent converges
to a globally optimal solution at a linear convergence rate for the quadratic
loss function.
  Our analysis relies on the following observation: over-parameterization and
random initialization jointly restrict every weight vector to be close to its
initialization for all iterations, which allows us to exploit a strong
convexity-like property to show that gradient descent converges at a global
linear rate to the global optimum. We believe these insights are also useful in
analyzing deep models and other first order methods.


Provably Correct Algorithms for Matrix Column Subset Selection with
  Selectively Sampled Data

  We consider the problem of matrix column subset selection, which selects a
subset of columns from an input matrix such that the input can be well
approximated by the span of the selected columns. Column subset selection has
been applied to numerous real-world data applications such as population
genetics summarization, electronic circuits testing and recommendation systems.
In many applications the complete data matrix is unavailable and one needs to
select representative columns by inspecting only a small portion of the input
matrix. In this paper we propose the first provably correct column subset
selection algorithms for partially observed data matrices. Our proposed
algorithms exhibit different merits and limitations in terms of statistical
accuracy, computational efficiency, sample complexity and sampling schemes,
which provides a nice exploration of the tradeoff between these desired
properties for column subset selection. The proposed methods employ the idea of
feedback driven sampling and are inspired by several sampling schemes
previously introduced for low-rank matrix approximation tasks (Drineas et al.,
2008; Frieze et al., 2004; Deshpande and Vempala, 2006; Krishnamurthy and
Singh, 2014). Our analysis shows that, under the assumption that the input data
matrix has incoherent rows but possibly coherent columns, all algorithms
provably converge to the best low-rank approximation of the original data as
number of selected columns increases. Furthermore, two of the proposed
algorithms enjoy a relative error bound, which is preferred for column subset
selection and matrix approximation purposes. We also demonstrate through both
theoretical and empirical analysis the power of feedback driven sampling
compared to uniform random sampling on input matrices with highly correlated
columns.


Algorithmic Connections Between Active Learning and Stochastic Convex
  Optimization

  Interesting theoretical associations have been established by recent papers
between the fields of active learning and stochastic convex optimization due to
the common role of feedback in sequential querying mechanisms. In this paper,
we continue this thread in two parts by exploiting these relations for the
first time to yield novel algorithms in both fields, further motivating the
study of their intersection. First, inspired by a recent optimization algorithm
that was adaptive to unknown uniform convexity parameters, we present a new
active learning algorithm for one-dimensional thresholds that can yield minimax
rates by adapting to unknown noise parameters. Next, we show that one can
perform $d$-dimensional stochastic minimization of smooth uniformly convex
functions when only granted oracle access to noisy gradient signs along any
coordinate instead of real-valued gradients, by using a simple randomized
coordinate descent procedure where each line search can be solved by
$1$-dimensional active learning, provably achieving the same error convergence
rate as having the entire real-valued gradient. Combining these two parts
yields an algorithm that solves stochastic convex optimization of uniformly
convex and smooth functions using only noisy gradient signs by repeatedly
performing active learning, achieves optimal rates and is adaptive to all
unknown convexity and smoothness parameters.


Detecting Anomalous Activity on Networks with the Graph Fourier Scan
  Statistic

  We consider the problem of deciding, based on a single noisy measurement at
each vertex of a given graph, whether the underlying unknown signal is constant
over the graph or there exists a cluster of vertices with anomalous activation.
This problem is relevant to several applications such as surveillance, disease
outbreak detection, biomedical imaging, environmental monitoring, etc. Since
the activations in these problems often tend to be localized to small groups of
vertices in the graphs, we model such activity by a class of signals that are
supported over a (possibly disconnected) cluster with low cut size relative to
its size. We analyze the corresponding generalized likelihood ratio (GLR)
statistics and relate it to the problem of finding a sparsest cut in the graph.
We develop a tractable relaxation of the GLR statistic based on the
combinatorial Laplacian of the graph, which we call the graph Fourier scan
statistic, and analyze its properties. We show how its performance as a testing
procedure depends directly on the spectrum of the graph, and use this result to
explicitly derive its asymptotic properties on a few significant graph
topologies. Finally, we demonstrate theoretically and with simulations that the
graph Fourier scan statistic can outperform naive testing procedures based on
global averaging and vertex-wise thresholding. We also demonstrate the
usefulness of the GFSS by analyzing groundwater Arsenic concentrations from a
U.S. Geological Survey dataset.


