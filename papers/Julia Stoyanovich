Computational Social Choice Meets Databases

  We develop a novel framework that aims to create bridges between the
computational social choice and the database management communities. This
framework enriches the tasks currently supported in computational social choice
with relational database context, thus making it possible to formulate
sophisticated queries about voting rules, candidates, voters, issues, and
positions. At the conceptual level, we give rigorous semantics to queries in
this framework by introducing the notions of necessary answers and possible
answers to queries. At the technical level, we embark on an investigation of
the computational complexity of the necessary answers. We establish a number of
results about the complexity of the necessary answers of conjunctive queries
involving positional scoring rules that contrast sharply with earlier results
about the complexity of the necessary winners.


Zooming in on NYC taxi data with Portal

  In this paper we develop a methodology for analyzing transportation data at
different levels of temporal and geographic granularity, and apply our
methodology to the TLC Trip Record Dataset, made publicly available by the NYC
Taxi & Limousine Commission. This data is naturally represented by a set of
trajectories, annotated with time and with additional information such as
passenger count and cost. We analyze TLC data to identify hotspots, which point
to lack of convenient public transportation options, and popular routes, which
motivate ride-sharing solutions or addition of a bus route.
  Our methodology is based on using a system called Portal, which implements
efficient representations and principled analysis methods for evolving graphs.
Portal is implemented on top of Apache Spark, a popular distributed data
processing system, is inter-operable with other Spark libraries like SparkSQL,
and supports sophisticated kinds of analysis of evolving graphs efficiently.
Portal is currently under development in the Data, Responsibly Lab at Drexel.
We plan to release Portal in the open source in Fall 2017.


Transparency, Fairness, Data Protection, Neutrality: Data Management
  Challenges in the Face of New Regulation

  The data revolution continues to transform every sector of science, industry
and government. Due to the incredible impact of data-driven technology on
society, we are becoming increasingly aware of the imperative to use data and
algorithms responsibly -- in accordance with laws and ethical norms. In this
article we discuss three recent regulatory frameworks: the European Union's
General Data Protection Regulation (GDPR), the New York City Automated
Decisions Systems (ADS) Law, and the Net Neutrality principle, that aim to
protect the rights of individuals who are impacted by data collection and
analysis. These frameworks are prominent examples of a global trend:
Governments are starting to recognize the need to regulate data-driven
algorithmic technology.
  Our goal in this paper is to bring these regulatory frameworks to the
attention of the data management community, and to underscore the technical
challenges they raise and which we, as a community, are well-equipped to
address. The main take-away of this article is that legal and ethical norms
cannot be incorporated into data-driven systems as an afterthought. Rather, we
must think in terms of responsibility by design, viewing it as a systems
requirement.


Introducing Access Control in Webdamlog

  We survey recent work on the specification of an access control mechanism in
a collaborative environment. The work is presented in the context of the
WebdamLog language, an extension of datalog to a distributed context. We
discuss a fine-grained access control mechanism for intentional data based on
provenance as well as a control mechanism for delegation, i.e., for deploying
rules at remote peers.


Rule-Based Application Development using Webdamlog

  We present the WebdamLog system for managing distributed data on the Web in a
peer-to-peer manner. We demonstrate the main features of the system through an
application called Wepic for sharing pictures between attendees of the sigmod
conference. Using Wepic, the attendees will be able to share, download, rate
and annotate pictures in a highly decentralized manner. We show how WebdamLog
handles heterogeneity of the devices and services used to share data in such a
Web setting. We exhibit the simple rules that define the Wepic application and
show how to easily modify the Wepic application.


Putting Lipstick on Pig: Enabling Database-style Workflow Provenance

  Workflow provenance typically assumes that each module is a "black-box", so
that each output depends on all inputs (coarse-grained dependencies).
Furthermore, it does not model the internal state of a module, which can change
between repeated executions. In practice, however, an output may depend on only
a small subset of the inputs (fine-grained dependencies) as well as on the
internal state of the module. We present a novel provenance framework that
marries database-style and workflow-style provenance, by using Pig Latin to
expose the functionality of modules, thus capturing internal state and
fine-grained dependencies. A critical ingredient in our solution is the use of
a novel form of provenance graph that models module invocations and yields a
compact representation of fine-grained workflow provenance. It also enables a
number of novel graph transformation operations, allowing to choose the desired
level of granularity in provenance querying (ZoomIn and ZoomOut), and
supporting "what-if" workflow analytic queries. We implemented our approach in
the Lipstick system and developed a benchmark in support of a systematic
performance evaluation. Our results demonstrate the feasibility of tracking and
querying fine-grained workflow provenance.


On Obtaining Stable Rankings

  Decision making is challenging when there is more than one criterion to
consider. In such cases, it is common to assign a goodness score to each item
as a weighted sum of its attribute values and rank them accordingly. Clearly,
the ranking obtained depends on the weights used for this summation. Ideally,
one would want the ranked order not to change if the weights are changed
slightly. We call this property {\em stability} of the ranking. A consumer of a
ranked list may trust the ranking more if it has high stability. A producer of
a ranked list prefers to choose weights that result in a stable ranking, both
to earn the trust of potential consumers and because a stable ranking is
intrinsically likely to be more meaningful. In this paper, we develop a
framework that can be used to assess the stability of a provided ranking and to
obtain a stable ranking within an "acceptable" range of weight values (called
"the region of interest"). We address the case where the user cares about the
rank order of the entire set of items, and also the case where the user cares
only about the top-$k$ items. Using a geometric interpretation, we propose
algorithms that produce stable rankings. In addition to theoretical analyses,
we conduct extensive experiments on real datasets that validate our proposal.


The Webdamlog System Managing Distributed Knowledge on the Web

  We study the use of WebdamLog, a declarative high-level lan- guage in the
style of datalog, to support the distribution of both data and knowledge (i.e.,
programs) over a network of au- tonomous peers. The main novelty of WebdamLog
compared to datalog is its use of delegation, that is, the ability for a peer
to communicate a program to another peer. We present results of a user study,
showing that users can write WebdamLog programs quickly and correctly, and with
a minimal amount of training. We present an implementation of the WebdamLog
inference engine relying on the Bud dat- alog engine. We describe an
experimental evaluation of the WebdamLog engine, demonstrating that WebdamLog
can be im- plemented efficiently. We conclude with a discussion of ongoing and
future work.


Querying Evolving Graphs with Portal

  Graphs are used to represent a plethora of phenomena, from the Web and social
networks, to biological pathways, to semantic knowledge bases. Arguably the
most interesting and important questions one can ask about graphs have to do
with their evolution. Which Web pages are showing an increasing popularity
trend? How does influence propagate in social networks? How does knowledge
evolve?
  This paper proposes a logical model of an evolving graph called a TGraph,
which captures evolution of graph topology and of its vertex and edge
attributes. We present a compositional temporal graph algebra TGA, and show a
reduction of TGA to temporal relational algebra with graph-specific primitives.
We formally study the properties of TGA, and also show that it is sufficient to
concisely express a wide range of common use cases. We describe an
implementation of our model and algebra in Portal, built on top of Apache Spark
/ GraphX. We conduct extensive experiments on real datasets, and show that
Portal scales.


Search and Result Presentation in Scientific Workflow Repositories

  We study the problem of searching a repository of complex hierarchical
workflows whose component modules, both composite and atomic, have been
annotated with keywords. Since keyword search does not use the graph structure
of a workflow, we develop a model of workflows using context-free bag grammars.
We then give efficient polynomial-time algorithms that, given a workflow and a
keyword query, determine whether some execution of the workflow matches the
query. Based on these algorithms we develop a search and ranking solution that
efficiently retrieves the top-k grammars from a repository. Finally, we propose
a novel result presentation method for grammars matching a keyword query, based
on representative parse-trees. The effectiveness of our approach is validated
through an extensive experimental evaluation.


Measuring Fairness in Ranked Outputs

  Ranking and scoring are ubiquitous. We consider the setting in which an
institution, called a ranker, evaluates a set of individuals based on
demographic, behavioral or other characteristics. The final output is a ranking
that represents the relative quality of the individuals. While automatic and
therefore seemingly objective, rankers can, and often do, discriminate against
individuals and systematically disadvantage members of protected groups. This
warrants a careful study of the fairness of a ranking scheme.
  In this paper we propose fairness measures for ranked outputs. We develop a
data generation procedure that allows us to systematically control the degree
of unfairness in the output, and study the behavior of our measures on these
datasets. We then apply our proposed measures to several real datasets, and
demonstrate cases of unfairness. Finally, we show preliminary results of
incorporating our ranked fairness measures into an optimization framework, and
show potential for improving fairness of ranked outputs while maintaining
accuracy.


Research Directions for Principles of Data Management (Dagstuhl
  Perspectives Workshop 16151)

  In April 2016, a community of researchers working in the area of Principles
of Data Management (PDM) joined in a workshop at the Dagstuhl Castle in
Germany. The workshop was organized jointly by the Executive Committee of the
ACM Symposium on Principles of Database Systems (PODS) and the Council of the
International Conference on Database Theory (ICDT). The mission of this
workshop was to identify and explore some of the most important research
directions that have high relevance to society and to Computer Science today,
and where the PDM community has the potential to make significant
contributions. This report describes the family of research directions that the
workshop focused on from three perspectives: potential practical relevance,
results already obtained, and research questions that appear surmountable in
the short and medium term.


A Nutritional Label for Rankings

  Algorithmic decisions often result in scoring and ranking individuals to
determine credit worthiness, qualifications for college admissions and
employment, and compatibility as dating partners. While automatic and seemingly
objective, ranking algorithms can discriminate against individuals and
protected groups, and exhibit low diversity. Furthermore, ranked results are
often unstable --- small changes in the input data or in the ranking
methodology may lead to drastic changes in the output, making the result
uninformative and easy to manipulate. Similar concerns apply in cases where
items other than individuals are ranked, including colleges, academic
departments, or products.
  In this demonstration we present Ranking Facts, a Web-based application that
generates a "nutritional label" for rankings. Ranking Facts is made up of a
collection of visual widgets that implement our latest research results on
fairness, stability, and transparency for rankings, and that communicate
details of the ranking methodology, or of the output, to the end user. We will
showcase Ranking Facts on real datasets from different domains, including
college rankings, criminal risk assessment, and financial services.


Synthetic Data for Social Good

  Data for good implies unfettered access to data. But data owners must be
conservative about how, when, and why they share data or risk violating the
trust of the people they aim to help, losing their funding, or breaking the
law. Data sharing agreements can help prevent privacy violations, but require a
level of specificity that is premature during preliminary discussions, and can
take over a year to establish.
  We consider the generation and use of synthetic data to facilitate ad hoc
collaborations involving sensitive data. A good synthetic dataset has two
properties: it is representative of the original data, and it provides strong
guarantees about privacy.
  In this paper, we discuss important use cases for synthetic data that
challenge the state of the art in privacy-preserving data generation, and
describe DataSynthesizer, a dataset generation tool that takes a sensitive
dataset as input and generates a structurally and statistically similar
synthetic dataset, with strong privacy guarantees, as output. The data owners
need not release their data, while potential collaborators can begin developing
models and methods with some confidence that their results will work similarly
on the real dataset. The distinguishing feature of DataSynthesizer is its
usability - in most cases, the data owner need not specify any parameters to
start generating and sharing data safely and effectively.
  The code implementing DataSynthesizer is publicly available on GitHub at
https://github.com/DataResponsibly. The work on DataSynthesizer is part of the
Data, Responsibly project, where the goal is to operationalize responsibility
in data sharing, integration, analysis and use.


Designing Fair Ranking Schemes

  Items from a database are often ranked based on a combination of multiple
criteria. A user may have the flexibility to accept combinations that weigh
these criteria differently, within limits. On the other hand, this choice of
weights can greatly affect the fairness of the produced ranking. In this paper,
we develop a system that helps users choose criterion weights that lead to
greater fairness.
  We consider ranking functions that compute the score of each item as a
weighted sum of (numeric) attribute values, and then sort items on their score.
Each ranking function can be expressed as a vector of weights, or as a point in
a multi-dimensional space. For a broad range of fairness criteria, we show how
to efficiently identify regions in this space that satisfy these criteria.
Using this identification method, our system is able to tell users whether
their proposed ranking function satisfies the desired fairness criteria and, if
it does not, to suggest the smallest modification that does. We develop
user-controllable approximation that and indexing techniques that are applied
during preprocessing, and support sub-second response times during the online
phase. Our extensive experiments on real datasets demonstrate that our methods
are able to find solutions that satisfy fairness criteria effectively and
efficiently.


MobilityMirror: Bias-Adjusted Transportation Datasets

  We describe customized synthetic datasets for publishing mobility data.
Private companies are providing new transportation modalities, and their data
is of high value for integrative transportation research, policy enforcement,
and public accountability. However, these companies are disincentivized from
sharing data not only to protect the privacy of individuals (drivers and/or
passengers), but also to protect their own competitive advantage. Moreover,
demographic biases arising from how the services are delivered may be amplified
if released data is used in other contexts.
  We describe a model and algorithm for releasing origin-destination histograms
that removes selected biases in the data using causality-based methods. We
compute the origin-destination histogram of the original dataset then adjust
the counts to remove undesirable causal relationships that can lead to
discrimination or violate contractual obligations with data owners. We evaluate
the utility of the algorithm on real data from a dockless bike share program in
Seattle and taxi data in New York, and show that these adjusted transportation
datasets can retain utility while removing bias in the underlying data.


