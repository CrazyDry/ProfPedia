Cartoonish sketch-based face editing in videos using identity  deformation transfer

  We address the problem of using hand-drawn sketches to create exaggerateddeformations to faces in videos, such as enlarging the shape or modifying theposition of eyes or mouth. This task is formulated as a 3D face modelreconstruction and deformation problem. We first recover the facial identityand expressions from the video by fitting a face morphable model for eachframe. At the same time, user's editing intention is recognized from inputsketches as a set of facial modifications. Then a novel identity deformationalgorithm is proposed to transfer these facial deformations from 2D space tothe 3D facial identity directly while preserving the facial expressions. Afteran optional stage for further refining the 3D face model, these changes arepropagated to the whole video with the modified identity. Both the user studyand experimental results demonstrate that our sketching framework can helpusers effectively edit facial identities in videos, while high consistency andfidelity are ensured at the same time.

The Role of Data-driven Priors in Multi-agent Crowd Trajectory  Estimation

  Trajectory interpolation, the process of filling-in the gaps and removingnoise from observed agent trajectories, is an essential task for the motioninference in multi-agent setting. A desired trajectory interpolation methodshould be robust to noise, changes in environments or agent densities, whilealso being yielding realistic group movement behaviors. Such realisticbehaviors are, however, challenging to model as they require avoidance ofagent-agent or agent-environment collisions and, at the same time, seekcomputational efficiency. In this paper, we propose a novel framework composedof data-driven priors (local, global or combined) and an efficient optimizationstrategy for multi-agent trajectory interpolation. The data-driven priorsimplicitly encode the dependencies of movements of multiple agents and thecollision-avoiding desiderata, enabling elimination of costly pairwisecollision constraints and resulting in reduced computational complexity andoften improved estimation. Various combinations of priors and optimizationalgorithms are evaluated in comprehensive simulated experiments. Ourexperimental results reveal important insights, including the significance ofthe global flow prior and the lesser-than-expected influence of data-drivencollision priors.

Learning to Forecast and Refine Residual Motion for Image-to-Video  Generation

  We consider the problem of image-to-video translation, where an input imageis translated into an output video containing motions of a single object.Recent methods for such problems typically train transformation networks togenerate future frames conditioned on the structure sequence. Parallel work hasshown that short high-quality motions can be generated by spatiotemporalgenerative networks that leverage temporal knowledge from the training data. Wecombine the benefits of both approaches and propose a two-stage generationframework where videos are generated from structures and then refined bytemporal signals. To model motions more efficiently, we train networks to learnresidual motion between the current and future frames, which avoids learningmotion-irrelevant details. We conduct extensive experiments on twoimage-to-video translation tasks: facial expression retargeting and human poseforecasting. Superior results over the state-of-the-art methods on both tasksdemonstrate the effectiveness of our approach.

Affect-Driven Dialog Generation

  The majority of current systems for end-to-end dialog generation focus onresponse quality without an explicit control over the affective content of theresponses. In this paper, we present an affect-driven dialog system, whichgenerates emotional responses in a controlled manner using a continuousrepresentation of emotions. The system achieves this by modeling emotions at aword and sequence level using: (1) a vector representation of the desiredemotion, (2) an affect regularizer, which penalizes neutral words, and (3) anaffect sampling method, which forces the neural network to generate diversewords that are emotionally relevant. During inference, we use a rerankingprocedure that aims to extract the most emotionally relevant responses using ahuman-in-the-loop optimization process. We study the performance of our systemin terms of both quantitative (BLEU score and response diversity), andqualitative (emotional appropriateness) measures.

Topic Spotting using Hierarchical Networks with Self Attention

  Success of deep learning techniques have renewed the interest in developmentof dialogue systems. However, current systems struggle to have consistent longterm conversations with the users and fail to build rapport. Topic spotting,the task of automatically inferring the topic of a conversation, has been shownto be helpful in making a dialog system more engaging and efficient. We proposea hierarchical model with self attention for topic spotting. Experiments on theSwitchboard corpus show the superior performance of our model over previouslyproposed techniques for topic spotting and deep models for text classification.Additionally, in contrast to offline processing of dialog, we also analyze theperformance of our model in a more realistic setting i.e. in an online settingwhere the topic is identified in real time as the dialog progresses. Resultsshow that our model is able to generalize even with limited information in theonline setting.

Domain Authoring Assistant for Intelligent Virtual Agents

  Developing intelligent virtual characters has attracted a lot of attention inthe recent years. The process of creating such characters often involves a teamof creative authors who describe different aspects of the characters in naturallanguage, and planning experts that translate this description into a planningdomain. This can be quite challenging as the team of creative authors shoulddiligently define every aspect of the character especially if it containscomplex human-like behavior. Also a team of engineers has to manually translatethe natural language description of a character's personality into the planningdomain knowledge. This can be extremely time and resource demanding and can bean obstacle to author's creativity. The goal of this paper is to introduce anauthoring assistant tool to automate the process of domain generation fromnatural language description of virtual characters, thus bridging between thecreative authoring team and the planning domain experts. Moreover, the proposedtool also identifies possible missing information in the domain description anditeratively makes suggestions to the author.

Semantic Graph Convolutional Networks for 3D Human Pose Regression

  In this paper, we study the problem of learning Graph Convolutional Networks(GCNs) for regression. Current architectures of GCNs are limited to the smallreceptive field of convolution filters and shared transformation matrix foreach node. To address these limitations, we propose Semantic GraphConvolutional Networks (SemGCN), a novel neural network architecture thatoperates on regression tasks with graph-structured data. SemGCN learns tocapture semantic information such as local and global node relationships, whichis not explicitly represented in the graph. These semantic relationships can belearned through end-to-end training from the ground truth without additionalsupervision or hand-crafted rules. We further investigate applying SemGCN to 3Dhuman pose regression. Our formulation is intuitive and sufficient since both2D and 3D human poses can be represented as a structured graph encoding therelationships between joints in the skeleton of a human body. We carry outcomprehensive studies to validate our method. The results prove that SemGCNoutperforms state of the art while using 90% fewer parameters.

Generating Animations from Screenplays

  Automatically generating animation from natural language text findsapplication in a number of areas e.g. movie script writing, instructionalvideos, and public safety. However, translating natural language text intoanimation is a challenging task. Existing text-to-animation systems can handleonly very simple sentences, which limits their applications. In this paper, wedevelop a text-to-animation system which is capable of handling complexsentences. We achieve this by introducing a text simplification step into theprocess. Building on an existing animation generation system for screenwriting,we create a robust NLP pipeline to extract information from screenplays and mapthem to the system's knowledge base. We develop a set of linguistictransformation rules that simplify complex sentences. Information extractedfrom the simplified sentences is used to generate a rough storyboard and videodepicting the text. Our sentence simplification module outperforms existingsystems in terms of BLEU and SARI metrics.We further evaluated our system via auser study: 68 % participants believe that our system generates reasonableanimation from input screenplays.

Crowd Behaviour during High-Stress Evacuations in an Immersive Virtual  Environment

  Understanding the collective dynamics of crowd movements during stressfulemergency situations is central to reducing the risk of deadly crowd disasters.Yet, their systematic experimental study remains a challenging open problem dueto ethical and methodological constraints. In this paper, we demonstrate theviability of shared 3D virtual environments as an experimental platform forconducting crowd experiments with real people. In particular, we show thatcrowds of real human subjects moving and interacting in an immersive 3D virtualenvironment exhibit typical patterns of real crowds as observed in real-lifecrowded situations. These include the manifestation of social conventions andthe emergence of self-organized patterns during egress scenarios. High-stressevacuation experiments conducted in this virtual environment reveal movementscharacterized by mass herding and dangerous overcrowding as they occur in crowddisasters. We describe the behavioral mechanisms at play under such extremeconditions and identify critical zones where overcrowding may occur.Furthermore, we show that herding spontaneously emerges from a density effectwithout the need to assume an increase of the individual tendency to imitatepeers. Our experiments reveal the promise of immersive virtual environments asan ethical, cost-efficient, yet accurate platform for exploring crowd behaviourin high-risk situations with real human subjects.

Interactive Diversity Optimization of Environments

  The design of a building requires an architect to balance a wide range ofconstraints: aesthetic, geometric, usability, lighting, safety, etc. At thesame time, there are often a multiplicity of diverse designs that can meetthese constraints equally well. Architects must use their skills and artisticvision to explore these rich but highly constrained design spaces. A number ofcomputer-aided design tools use automation to provide useful analytical dataand optimal designs with respect to certain fitness criteria. However, thisautomation can come at the expense of a designer's creative control.  We propose uDOME, a user-in-the-loop system for computer-aided designexploration that balances automation and control by efficiently exploring,analyzing, and filtering the space of environment layouts to better inform anarchitect's decision-making. At each design iteration, uDOME provides a set ofdiverse designs which satisfy user-defined constraints and optimality criteriawithin a user defined parameterization of the design space. The user thenselects a design and performs a similar optimization with the same or differentparameters and objectives. This exploration process can be repeated as manytimes as the designer wishes. Our user studies indicates that \DOME, with itsdiversity-based approach, improves the efficiency and effectiveness of evennovice users with minimal training, without compromising the quality of theirdesigns.

