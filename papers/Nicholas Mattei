How Hard Is It to Control an Election by Breaking Ties?

  We study the computational complexity of controlling the result of an
election by breaking ties strategically. This problem is equivalent to the
problem of deciding the winner of an election under parallel universes
tie-breaking. When the chair of the election is only asked to break ties to
choose between one of the co-winners, the problem is trivially easy. However,
in multi-round elections, we prove that it can be NP-hard for the chair to
compute how to break ties to ensure a given result. Additionally, we show that
the form of the tie-breaking function can increase the opportunities for
control. Indeed, we prove that it can be NP-hard to control an election by
breaking ties even with a two-stage voting rule.


Lessons Learned from Development of a Software Tool to Support Academic
  Advising

  We detail some lessons learned while designing and testing a
decision-theoretic advising support tool for undergraduates at a large state
university. Between 2009 and 2011 we conducted two surveys of over 500 students
in multiple majors and colleges. These surveys asked students detailed
questions about their preferences concerning course selection, advising, and
career paths. We present data from this study which may be helpful for faculty
and staff who advise undergraduate students. We find that advising support
software tools can augment the student-advisor relationship, particularly in
terms of course planning, but cannot and should not replace in-person advising.


Computational Aspects of Multi-Winner Approval Voting

  We study computational aspects of three prominent voting rules that use
approval ballots to elect multiple winners. These rules are satisfaction
approval voting, proportional approval voting, and reweighted approval voting.
We first show that computing the winner for proportional approval voting is
NP-hard, closing a long standing open problem. As none of the rules are
strategyproof, even for dichotomous preferences, we study various strategic
aspects of the rules. In particular, we examine the computational complexity of
computing a best response for both a single agent and a group of agents. In
many settings, we show that it is NP-hard for an agent or agents to compute how
best to vote given a fixed set of approval ballots from the other agents.


Interdependent Scheduling Games

  We propose a model of interdependent scheduling games in which each player
controls a set of services that they schedule independently. A player is free
to schedule his own services at any time; however, each of these services only
begins to accrue reward for the player when all predecessor services, which may
or may not be controlled by the same player, have been activated. This model,
where players have interdependent services, is motivated by the problems faced
in planning and coordinating large-scale infrastructures, e.g., restoring
electricity and gas to residents after a natural disaster or providing medical
care in a crisis when different agencies are responsible for the delivery of
staff, equipment, and medicine. We undertake a game-theoretic analysis of this
setting and in particular consider the issues of welfare maximization,
computing best responses, Nash dynamics, and existence and computation of Nash
equilibria.


Empirical Evaluation of Real World Tournaments

  Computational Social Choice (ComSoc) is a rapidly developing field at the
intersection of computer science, economics, social choice, and political
science. The study of tournaments is fundamental to ComSoc and many results
have been published about tournament solution sets and reasoning in
tournaments. Theoretical results in ComSoc tend to be worst case and tell us
little about performance in practice. To this end we detail some experiments on
tournaments using real wold data from soccer and tennis. We make three main
contributions to the understanding of tournaments using real world data from
English Premier League, the German Bundesliga, and the ATP World Tour: (1) we
find that the NP-hard question of finding a seeding for which a given team can
win a tournament is easily solvable in real world instances, (2) using detailed
and principled methodology from statistical physics we show that our real world
data obeys a log-normal distribution; and (3) leveraging our log-normal
distribution result and using robust statistical methods, we show that the
popular Condorcet Random (CR) tournament model does not generate realistic
tournament data.


Ethical Considerations in Artificial Intelligence Courses

  The recent surge in interest in ethics in artificial intelligence may leave
many educators wondering how to address moral, ethical, and philosophical
issues in their AI courses. As instructors we want to develop curriculum that
not only prepares students to be artificial intelligence practitioners, but
also to understand the moral, ethical, and philosophical impacts that
artificial intelligence will have on society. In this article we provide
practical case studies and links to resources for use by AI educators. We also
provide concrete suggestions on how to integrate AI ethics into a general
artificial intelligence course and how to teach a stand-alone artificial
intelligence ethics course.


The Conference Paper Assignment Problem: Using Order Weighted Averages
  to Assign Indivisible Goods

  Motivated by the common academic problem of allocating papers to referees for
conference reviewing we propose a novel mechanism for solving the assignment
problem when we have a two sided matching problem with preferences from one
side (the agents/reviewers) over the other side (the objects/papers) and both
sides have capacity constraints. The assignment problem is a fundamental
problem in both computer science and economics with application in many areas
including task and resource allocation. We draw inspiration from multi-criteria
decision making and voting and use order weighted averages (OWAs) to propose a
novel and flexible class of algorithms for the assignment problem. We show an
algorithm for finding a $\Sigma$-OWA assignment in polynomial time, in contrast
to the NP-hardness of finding an egalitarian assignment. Inspired by this
setting we observe an interesting connection between our model and the classic
proportional multi-winner election problem in social choice.


Egalitarianism of Random Assignment Mechanisms

  We consider the egalitarian welfare aspects of random assignment mechanisms
when agents have unrestricted cardinal utilities over the objects. We give
bounds on how well different random assignment mechanisms approximate the
optimal egalitarian value and investigate the effect that different well-known
properties like ordinality, envy-freeness, and truthfulness have on the
achievable egalitarian value. Finally, we conduct detailed experiments
analyzing the tradeoffs between efficiency with envy-freeness or truthfulness
using two prominent random assignment mechanisms --- random serial dictatorship
and the probabilistic serial mechanism --- for different classes of utility
functions and distributions.


A Study of Proxies for Shapley Allocations of Transport Costs

  We propose and evaluate a number of solutions to the problem of calculating
the cost to serve each location in a single-vehicle transport setting. Such
cost to serve analysis has application both strategically and operationally in
transportation. The problem is formally given by the traveling salesperson game
(TSG), a cooperative total utility game in which agents correspond to locations
in a traveling salesperson problem (TSP). The cost to serve a location is an
allocated portion of the cost of an optimal tour. The Shapley value is one of
the most important normative division schemes in cooperative games, giving a
principled and fair allocation both for the TSG and more generally. We consider
a number of direct and sampling-based procedures for calculating the Shapley
value, and present the first proof that approximating the Shapley value of the
TSG within a constant factor is NP-hard. Treating the Shapley value as an ideal
baseline allocation, we then develop six proxies for that value which are
relatively easy to compute. We perform an experimental evaluation using
Synthetic Euclidean games as well as games derived from real-world tours
calculated for fast-moving consumer goods scenarios. Our experiments show that
several computationally tractable allocation techniques correspond to good
proxies for the Shapley value.


Strategyproof Peer Selection using Randomization, Partitioning, and
  Apportionment

  Peer review, evaluation, and selection is a fundamental aspect of modern
science. Funding bodies the world over employ experts to review and select the
best proposals of those submitted for funding. The problem of peer selection,
however, is much more general: a professional society may want to give a subset
of its members awards based on the opinions of all members; an instructor for a
MOOC or online course may want to crowdsource grading; or a marketing company
may select ideas from group brainstorming sessions based on peer evaluation. We
make three fundamental contributions to the study of procedures or mechanisms
for peer selection, a specific type of group decision-making problem, studied
in computer science, economics, and political science. First, we propose a
novel mechanism that is strategyproof, i.e., agents cannot benefit by reporting
insincere valuations. Second, we demonstrate the effectiveness of our mechanism
by a comprehensive simulation-based comparison with a suite of mechanisms found
in the literature. Finally, our mechanism employs a randomized rounding
technique that is of independent interest, as it solves the apportionment
problem that arises in various settings where discrete resources such as
parliamentary representation slots need to be divided proportionally.


Incorporating Behavioral Constraints in Online AI Systems

  AI systems that learn through reward feedback about the actions they take are
increasingly deployed in domains that have significant impact on our daily
life. However, in many cases the online rewards should not be the only guiding
criteria, as there are additional constraints and/or priorities imposed by
regulations, values, preferences, or ethical principles. We detail a novel
online agent that learns a set of behavioral constraints by observation and
uses these learned constraints as a guide when making decisions in an online
setting while still being reactive to reward feedback. To define this agent, we
propose to adopt a novel extension to the classical contextual multi-armed
bandit setting and we provide a new algorithm called Behavior Constrained
Thompson Sampling (BCTS) that allows for online learning while obeying
exogenous constraints. Our agent learns a constrained policy that implements
the observed behavioral constraints demonstrated by a teacher agent, and then
uses this constrained policy to guide the reward-based online exploration and
exploitation. We characterize the upper bound on the expected regret of the
contextual bandit algorithm that underlies our agent and provide a case study
with real world data in two application domains. Our experiments show that the
designed agent is able to act within the set of behavior constraints without
significantly degrading its overall reward performance.


Answering Science Exam Questions Using Query Rewriting with Background
  Knowledge

  Open-domain question answering (QA) is an important problem in AI and NLP
that is emerging as a bellwether for progress on the generalizability of AI
methods and techniques. Much of the progress in open-domain QA systems has been
realized through advances in information retrieval methods and corpus
construction. In this paper, we focus on the recently introduced ARC Challenge
dataset, which contains 2,590 multiple choice questions authored for
grade-school science exams. These questions are selected to be the most
challenging for current QA systems, and current state of the art performance is
only slightly better than random chance. We present a system that rewrites a
given question into queries that are used to retrieve supporting text from a
large corpus of science-related text. Our rewriter is able to incorporate
background knowledge from ConceptNet and -- in tandem with a generic textual
entailment system trained on SciTail that identifies support in the retrieved
results -- outperforms several strong baselines on the end-to-end QA task
despite only being trained to identify essential terms in the original source
question. We use a generalizable decision methodology over the retrieved
evidence and answer candidates to select the best answer. By combining query
rewriting, background knowledge, and textual entailment our system is able to
outperform several strong baselines on the ARC dataset.


CPDist: Deep Siamese Networks for Learning Distances Between Structured
  Preferences

  Preference are central to decision making by both machines and humans.
Representing, learning, and reasoning with preferences is an important area of
study both within computer science and across the sciences. When working with
preferences it is necessary to understand and compute the distance between sets
of objects, e.g., the preferences of a user and a the descriptions of objects
to be recommended. We present CPDist, a novel neural network to address the
problem of learning to measure the distance between structured preference
representations. We use the popular CP-net formalism to represent preferences
and then leverage deep neural networks to learn a recently proposed metric
function that is computationally hard to compute directly. CPDist is a novel
metric learning approach based on the use of deep siamese networks which learn
the Kendal Tau distance between partial orders that are induced by compact
preference representations. We find that CPDist is able to learn the distance
function with high accuracy and outperform existing approximation algorithms on
both the regression and classification task using less computation time.
Performance remains good even when CPDist is trained with only a small number
of samples compared to the dimension of the solution space, indicating the
network generalizes well.


Building Ethically Bounded AI

  The more AI agents are deployed in scenarios with possibly unexpected
situations, the more they need to be flexible, adaptive, and creative in
achieving the goal we have given them. Thus, a certain level of freedom to
choose the best path to the goal is inherent in making AI robust and flexible
enough. At the same time, however, the pervasive deployment of AI in our life,
whether AI is autonomous or collaborating with humans, raises several ethical
challenges. AI agents should be aware and follow appropriate ethical principles
and should thus exhibit properties such as fairness or other virtues. These
ethical principles should define the boundaries of AI's freedom and creativity.
However, it is still a challenge to understand how to specify and reason with
ethical boundaries in AI agents and how to combine them appropriately with
subjective preferences and goal specifications. Some initial attempts employ
either a data-driven example-based approach for both, or a symbolic rule-based
approach for both. We envision a modular approach where any AI technique can be
used for any of these essential ingredients in decision making or decision
support systems, paired with a contextual approach to define their combination
and relative weight. In a world where neither humans nor AI systems work in
isolation, but are tightly interconnected, e.g., the Internet of Things, we
also envision a compositional approach to building ethically bounded AI, where
the ethical properties of each component can be fruitfully exploited to derive
those of the overall system. In this paper we define and motivate the notion of
ethically-bounded AI, we describe two concrete examples, and we outline some
outstanding challenges.


Interpretable Multi-Objective Reinforcement Learning through Policy
  Orchestration

  Autonomous cyber-physical agents and systems play an increasingly large role
in our lives. To ensure that agents behave in ways aligned with the values of
the societies in which they operate, we must develop techniques that allow
these agents to not only maximize their reward in an environment, but also to
learn and follow the implicit constraints of society. These constraints and
norms can come from any number of sources including regulations, business
process guidelines, laws, ethical principles, social norms, and moral values.
We detail a novel approach that uses inverse reinforcement learning to learn a
set of unspecified constraints from demonstrations of the task, and
reinforcement learning to learn to maximize the environment rewards. More
precisely, we assume that an agent can observe traces of behavior of members of
the society but has no access to the explicit set of constraints that give rise
to the observed behavior. Inverse reinforcement learning is used to learn such
constraints, that are then combined with a possibly orthogonal value function
through the use of a contextual bandit-based orchestrator that picks a
contextually-appropriate choice between the two policies (constraint-based and
environment reward-based) when taking actions. The contextual bandit
orchestrator allows the agent to mix policies in novel ways, taking the best
actions from either a reward maximizing or constrained policy. In addition, the
orchestrator is transparent on which policy is being employed at each time
step. We test our algorithms using a Pac-Man domain and show that the agent is
able to learn to act optimally, act within the demonstrated constraints, and
mix these two functions in complex ways.


The Complexity of Probabilistic Lobbying

  We propose models for lobbying in a probabilistic environment, in which an
actor (called "The Lobby") seeks to influence voters' preferences of voting for
or against multiple issues when the voters' preferences are represented in
terms of probabilities. In particular, we provide two evaluation criteria and
two bribery methods to formally describe these models, and we consider the
resulting forms of lobbying with and without issue weighting. We provide a
formal analysis for these problems of lobbying in a stochastic environment, and
determine their classical and parameterized complexity depending on the given
bribery/evaluation criteria and on various natural parameterizations.
Specifically, we show that some of these problems can be solved in polynomial
time, some are NP-complete but fixed-parameter tractable, and some are
W[2]-complete. Finally, we provide approximability and inapproximability
results for these problems and several variants.


Manipulating the Probabilistic Serial Rule

  The probabilistic serial (PS) rule is one of the most prominent randomized
rules for the assignment problem. It is well-known for its superior fairness
and welfare properties. However, PS is not immune to manipulative behaviour by
the agents. We initiate the study of the computational complexity of an agent
manipulating the PS rule. We show that computing an expected utility better
response is NP- hard. On the other hand, we present a polynomial-time algorithm
to compute a lexicographic best response. For the case of two agents, we show
that even an expected utility best response can be computed in polynomial time.
Our result for the case of two agents relies on an interesting connection with
sequential allocation of discrete objects.


Equilibria Under the Probabilistic Serial Rule

  The probabilistic serial (PS) rule is a prominent randomized rule for
assigning indivisible goods to agents. Although it is well known for its good
fairness and welfare properties, it is not strategyproof. In view of this, we
address several fundamental questions regarding equilibria under PS. Firstly,
we show that Nash deviations under the PS rule can cycle. Despite the
possibilities of cycles, we prove that a pure Nash equilibrium is guaranteed to
exist under the PS rule. We then show that verifying whether a given profile is
a pure Nash equilibrium is coNP-complete, and computing a pure Nash equilibrium
is NP-hard. For two agents, we present a linear-time algorithm to compute a
pure Nash equilibrium which yields the same assignment as the truthful profile.
Finally, we conduct experiments to evaluate the quality of the equilibria that
exist under the PS rule, finding that the vast majority of pure Nash equilibria
yield social welfare that is at least that of the truthful profile.


Stable Matching with Uncertain Linear Preferences

  We consider the two-sided stable matching setting in which there may be
uncertainty about the agents' preferences due to limited information or
communication. We consider three models of uncertainty: (1) lottery model ---
in which for each agent, there is a probability distribution over linear
preferences, (2) compact indifference model --- for each agent, a weak
preference order is specified and each linear order compatible with the weak
order is equally likely and (3) joint probability model --- there is a lottery
over preference profiles. For each of the models, we study the computational
complexity of computing the stability probability of a given matching as well
as finding a matching with the highest probability of being stable. We also
examine more restricted problems such as deciding whether a certainly stable
matching exists. We find a rich complexity landscape for these problems,
indicating that the form uncertainty takes is significant.


A Cost-Effective Framework for Preference Elicitation and Aggregation

  We propose a cost-effective framework for preference elicitation and
aggregation under the Plackett-Luce model with features. Given a budget, our
framework iteratively computes the most cost-effective elicitation questions in
order to help the agents make a better group decision.
  We illustrate the viability of the framework with experiments on Amazon
Mechanical Turk, which we use to estimate the cost of answering different types
of elicitation questions. We compare the prediction accuracy of our framework
when adopting various information criteria that evaluate the expected
information gain from a question. Our experiments show carefully designed
information criteria are much more efficient, i.e., they arrive at the correct
answer using fewer queries, than randomly asking questions given the budget
constraint.


A Systematic Classification of Knowledge, Reasoning, and Context within
  the ARC Dataset

  The recent work of Clark et al. introduces the AI2 Reasoning Challenge (ARC)
and the associated ARC dataset that partitions open domain, complex science
questions into an Easy Set and a Challenge Set. That paper includes an analysis
of 100 questions with respect to the types of knowledge and reasoning required
to answer them; however, it does not include clear definitions of these types,
nor does it offer information about the quality of the labels. We propose a
comprehensive set of definitions of knowledge and reasoning types necessary for
answering the questions in the ARC dataset. Using ten annotators and a
sophisticated annotation interface, we analyze the distribution of labels
across the Challenge Set and statistics related to them. Additionally, we
demonstrate that although naive information retrieval methods return sentences
that are irrelevant to answering the query, sufficient supporting text is often
present in the (ARC) corpus. Evaluating with human-selected relevant sentences
improves the performance of a neural machine comprehension model by 42 points.


Improving Natural Language Inference Using External Knowledge in the
  Science Questions Domain

  Natural Language Inference (NLI) is fundamental to many Natural Language
Processing (NLP) applications including semantic search and question answering.
The NLI problem has gained significant attention thanks to the release of large
scale, challenging datasets. Present approaches to the problem largely focus on
learning-based methods that use only textual information in order to classify
whether a given premise entails, contradicts, or is neutral with respect to a
given hypothesis. Surprisingly, the use of methods based on structured
knowledge -- a central topic in artificial intelligence -- has not received
much attention vis-a-vis the NLI problem. While there are many open knowledge
bases that contain various types of reasoning information, their use for NLI
has not been well explored. To address this, we present a combination of
techniques that harness knowledge graphs to improve performance on the NLI
problem in the science questions domain. We present the results of applying our
techniques on text, graph, and text-to-graph based models, and discuss
implications for the use of external knowledge in solving the NLI problem. Our
model achieves the new state-of-the-art performance on the NLI problem over the
SciTail science questions dataset.


Flexible Representative Democracy: An Introduction with Binary Issues

  We introduce Flexible Representative Democracy (FRD), a novel hybrid of
Representative Democracy (RD) and direct democracy (DD), in which voters can
alter the issue-dependent weights of a set of elected representatives. In line
with the literature on Interactive Democracy, our model allows the voters to
actively determine the degree to which the system is direct versus
representative. However, unlike Liquid Democracy, FRD uses strictly
non-transitive delegations, making delegation cycles impossible, and maintains
a fixed set of accountable elected representatives. We present FRD and analyze
it using a computational approach with issues that are binary and symmetric; we
compare the outcomes of various democratic systems using Direct Democracy with
majority voting as an ideal baseline. First, we demonstrate the shortcomings of
Representative Democracy in our model. We provide NP-Hardness results for
electing an ideal set of representatives, discuss pathologies, and demonstrate
empirically that common multi-winner election rules for selecting
representatives do not perform well in expectation. To analyze the behavior of
FRD, we begin by providing theoretical results on how issue-specific
delegations determine outcomes. Finally, we provide empirical results comparing
the outcomes of RD with fixed sets of proxies across issues versus FRD with
issue-specific delegations. Our results show that variants of Proxy Voting
yield no discernible benefit over RD and reveal the potential for FRD to
improve outcomes as voter participation increases, further motivating the use
of issue-specific delegations.


