How Hard Is It to Control an Election by Breaking Ties?

  We study the computational complexity of controlling the result of anelection by breaking ties strategically. This problem is equivalent to theproblem of deciding the winner of an election under parallel universestie-breaking. When the chair of the election is only asked to break ties tochoose between one of the co-winners, the problem is trivially easy. However,in multi-round elections, we prove that it can be NP-hard for the chair tocompute how to break ties to ensure a given result. Additionally, we show thatthe form of the tie-breaking function can increase the opportunities forcontrol. Indeed, we prove that it can be NP-hard to control an election bybreaking ties even with a two-stage voting rule.

Lessons Learned from Development of a Software Tool to Support Academic  Advising

  We detail some lessons learned while designing and testing adecision-theoretic advising support tool for undergraduates at a large stateuniversity. Between 2009 and 2011 we conducted two surveys of over 500 studentsin multiple majors and colleges. These surveys asked students detailedquestions about their preferences concerning course selection, advising, andcareer paths. We present data from this study which may be helpful for facultyand staff who advise undergraduate students. We find that advising supportsoftware tools can augment the student-advisor relationship, particularly interms of course planning, but cannot and should not replace in-person advising.

Computational Aspects of Multi-Winner Approval Voting

  We study computational aspects of three prominent voting rules that useapproval ballots to elect multiple winners. These rules are satisfactionapproval voting, proportional approval voting, and reweighted approval voting.We first show that computing the winner for proportional approval voting isNP-hard, closing a long standing open problem. As none of the rules arestrategyproof, even for dichotomous preferences, we study various strategicaspects of the rules. In particular, we examine the computational complexity ofcomputing a best response for both a single agent and a group of agents. Inmany settings, we show that it is NP-hard for an agent or agents to compute howbest to vote given a fixed set of approval ballots from the other agents.

Interdependent Scheduling Games

  We propose a model of interdependent scheduling games in which each playercontrols a set of services that they schedule independently. A player is freeto schedule his own services at any time; however, each of these services onlybegins to accrue reward for the player when all predecessor services, which mayor may not be controlled by the same player, have been activated. This model,where players have interdependent services, is motivated by the problems facedin planning and coordinating large-scale infrastructures, e.g., restoringelectricity and gas to residents after a natural disaster or providing medicalcare in a crisis when different agencies are responsible for the delivery ofstaff, equipment, and medicine. We undertake a game-theoretic analysis of thissetting and in particular consider the issues of welfare maximization,computing best responses, Nash dynamics, and existence and computation of Nashequilibria.

Empirical Evaluation of Real World Tournaments

  Computational Social Choice (ComSoc) is a rapidly developing field at theintersection of computer science, economics, social choice, and politicalscience. The study of tournaments is fundamental to ComSoc and many resultshave been published about tournament solution sets and reasoning intournaments. Theoretical results in ComSoc tend to be worst case and tell uslittle about performance in practice. To this end we detail some experiments ontournaments using real wold data from soccer and tennis. We make three maincontributions to the understanding of tournaments using real world data fromEnglish Premier League, the German Bundesliga, and the ATP World Tour: (1) wefind that the NP-hard question of finding a seeding for which a given team canwin a tournament is easily solvable in real world instances, (2) using detailedand principled methodology from statistical physics we show that our real worlddata obeys a log-normal distribution; and (3) leveraging our log-normaldistribution result and using robust statistical methods, we show that thepopular Condorcet Random (CR) tournament model does not generate realistictournament data.

Ethical Considerations in Artificial Intelligence Courses

  The recent surge in interest in ethics in artificial intelligence may leavemany educators wondering how to address moral, ethical, and philosophicalissues in their AI courses. As instructors we want to develop curriculum thatnot only prepares students to be artificial intelligence practitioners, butalso to understand the moral, ethical, and philosophical impacts thatartificial intelligence will have on society. In this article we providepractical case studies and links to resources for use by AI educators. We alsoprovide concrete suggestions on how to integrate AI ethics into a generalartificial intelligence course and how to teach a stand-alone artificialintelligence ethics course.

The Conference Paper Assignment Problem: Using Order Weighted Averages  to Assign Indivisible Goods

  Motivated by the common academic problem of allocating papers to referees forconference reviewing we propose a novel mechanism for solving the assignmentproblem when we have a two sided matching problem with preferences from oneside (the agents/reviewers) over the other side (the objects/papers) and bothsides have capacity constraints. The assignment problem is a fundamentalproblem in both computer science and economics with application in many areasincluding task and resource allocation. We draw inspiration from multi-criteriadecision making and voting and use order weighted averages (OWAs) to propose anovel and flexible class of algorithms for the assignment problem. We show analgorithm for finding a $\Sigma$-OWA assignment in polynomial time, in contrastto the NP-hardness of finding an egalitarian assignment. Inspired by thissetting we observe an interesting connection between our model and the classicproportional multi-winner election problem in social choice.

Egalitarianism of Random Assignment Mechanisms

  We consider the egalitarian welfare aspects of random assignment mechanismswhen agents have unrestricted cardinal utilities over the objects. We givebounds on how well different random assignment mechanisms approximate theoptimal egalitarian value and investigate the effect that different well-knownproperties like ordinality, envy-freeness, and truthfulness have on theachievable egalitarian value. Finally, we conduct detailed experimentsanalyzing the tradeoffs between efficiency with envy-freeness or truthfulnessusing two prominent random assignment mechanisms --- random serial dictatorshipand the probabilistic serial mechanism --- for different classes of utilityfunctions and distributions.

A Study of Proxies for Shapley Allocations of Transport Costs

  We propose and evaluate a number of solutions to the problem of calculatingthe cost to serve each location in a single-vehicle transport setting. Suchcost to serve analysis has application both strategically and operationally intransportation. The problem is formally given by the traveling salesperson game(TSG), a cooperative total utility game in which agents correspond to locationsin a traveling salesperson problem (TSP). The cost to serve a location is anallocated portion of the cost of an optimal tour. The Shapley value is one ofthe most important normative division schemes in cooperative games, giving aprincipled and fair allocation both for the TSG and more generally. We considera number of direct and sampling-based procedures for calculating the Shapleyvalue, and present the first proof that approximating the Shapley value of theTSG within a constant factor is NP-hard. Treating the Shapley value as an idealbaseline allocation, we then develop six proxies for that value which arerelatively easy to compute. We perform an experimental evaluation usingSynthetic Euclidean games as well as games derived from real-world tourscalculated for fast-moving consumer goods scenarios. Our experiments show thatseveral computationally tractable allocation techniques correspond to goodproxies for the Shapley value.

Strategyproof Peer Selection using Randomization, Partitioning, and  Apportionment

  Peer review, evaluation, and selection is a fundamental aspect of modernscience. Funding bodies the world over employ experts to review and select thebest proposals of those submitted for funding. The problem of peer selection,however, is much more general: a professional society may want to give a subsetof its members awards based on the opinions of all members; an instructor for aMOOC or online course may want to crowdsource grading; or a marketing companymay select ideas from group brainstorming sessions based on peer evaluation. Wemake three fundamental contributions to the study of procedures or mechanismsfor peer selection, a specific type of group decision-making problem, studiedin computer science, economics, and political science. First, we propose anovel mechanism that is strategyproof, i.e., agents cannot benefit by reportinginsincere valuations. Second, we demonstrate the effectiveness of our mechanismby a comprehensive simulation-based comparison with a suite of mechanisms foundin the literature. Finally, our mechanism employs a randomized roundingtechnique that is of independent interest, as it solves the apportionmentproblem that arises in various settings where discrete resources such asparliamentary representation slots need to be divided proportionally.

Incorporating Behavioral Constraints in Online AI Systems

  AI systems that learn through reward feedback about the actions they take areincreasingly deployed in domains that have significant impact on our dailylife. However, in many cases the online rewards should not be the only guidingcriteria, as there are additional constraints and/or priorities imposed byregulations, values, preferences, or ethical principles. We detail a novelonline agent that learns a set of behavioral constraints by observation anduses these learned constraints as a guide when making decisions in an onlinesetting while still being reactive to reward feedback. To define this agent, wepropose to adopt a novel extension to the classical contextual multi-armedbandit setting and we provide a new algorithm called Behavior ConstrainedThompson Sampling (BCTS) that allows for online learning while obeyingexogenous constraints. Our agent learns a constrained policy that implementsthe observed behavioral constraints demonstrated by a teacher agent, and thenuses this constrained policy to guide the reward-based online exploration andexploitation. We characterize the upper bound on the expected regret of thecontextual bandit algorithm that underlies our agent and provide a case studywith real world data in two application domains. Our experiments show that thedesigned agent is able to act within the set of behavior constraints withoutsignificantly degrading its overall reward performance.

Answering Science Exam Questions Using Query Rewriting with Background  Knowledge

  Open-domain question answering (QA) is an important problem in AI and NLPthat is emerging as a bellwether for progress on the generalizability of AImethods and techniques. Much of the progress in open-domain QA systems has beenrealized through advances in information retrieval methods and corpusconstruction. In this paper, we focus on the recently introduced ARC Challengedataset, which contains 2,590 multiple choice questions authored forgrade-school science exams. These questions are selected to be the mostchallenging for current QA systems, and current state of the art performance isonly slightly better than random chance. We present a system that rewrites agiven question into queries that are used to retrieve supporting text from alarge corpus of science-related text. Our rewriter is able to incorporatebackground knowledge from ConceptNet and -- in tandem with a generic textualentailment system trained on SciTail that identifies support in the retrievedresults -- outperforms several strong baselines on the end-to-end QA taskdespite only being trained to identify essential terms in the original sourcequestion. We use a generalizable decision methodology over the retrievedevidence and answer candidates to select the best answer. By combining queryrewriting, background knowledge, and textual entailment our system is able tooutperform several strong baselines on the ARC dataset.

CPDist: Deep Siamese Networks for Learning Distances Between Structured  Preferences

  Preference are central to decision making by both machines and humans.Representing, learning, and reasoning with preferences is an important area ofstudy both within computer science and across the sciences. When working withpreferences it is necessary to understand and compute the distance between setsof objects, e.g., the preferences of a user and a the descriptions of objectsto be recommended. We present CPDist, a novel neural network to address theproblem of learning to measure the distance between structured preferencerepresentations. We use the popular CP-net formalism to represent preferencesand then leverage deep neural networks to learn a recently proposed metricfunction that is computationally hard to compute directly. CPDist is a novelmetric learning approach based on the use of deep siamese networks which learnthe Kendal Tau distance between partial orders that are induced by compactpreference representations. We find that CPDist is able to learn the distancefunction with high accuracy and outperform existing approximation algorithms onboth the regression and classification task using less computation time.Performance remains good even when CPDist is trained with only a small numberof samples compared to the dimension of the solution space, indicating thenetwork generalizes well.

Building Ethically Bounded AI

  The more AI agents are deployed in scenarios with possibly unexpectedsituations, the more they need to be flexible, adaptive, and creative inachieving the goal we have given them. Thus, a certain level of freedom tochoose the best path to the goal is inherent in making AI robust and flexibleenough. At the same time, however, the pervasive deployment of AI in our life,whether AI is autonomous or collaborating with humans, raises several ethicalchallenges. AI agents should be aware and follow appropriate ethical principlesand should thus exhibit properties such as fairness or other virtues. Theseethical principles should define the boundaries of AI's freedom and creativity.However, it is still a challenge to understand how to specify and reason withethical boundaries in AI agents and how to combine them appropriately withsubjective preferences and goal specifications. Some initial attempts employeither a data-driven example-based approach for both, or a symbolic rule-basedapproach for both. We envision a modular approach where any AI technique can beused for any of these essential ingredients in decision making or decisionsupport systems, paired with a contextual approach to define their combinationand relative weight. In a world where neither humans nor AI systems work inisolation, but are tightly interconnected, e.g., the Internet of Things, wealso envision a compositional approach to building ethically bounded AI, wherethe ethical properties of each component can be fruitfully exploited to derivethose of the overall system. In this paper we define and motivate the notion ofethically-bounded AI, we describe two concrete examples, and we outline someoutstanding challenges.

Interpretable Multi-Objective Reinforcement Learning through Policy  Orchestration

  Autonomous cyber-physical agents and systems play an increasingly large rolein our lives. To ensure that agents behave in ways aligned with the values ofthe societies in which they operate, we must develop techniques that allowthese agents to not only maximize their reward in an environment, but also tolearn and follow the implicit constraints of society. These constraints andnorms can come from any number of sources including regulations, businessprocess guidelines, laws, ethical principles, social norms, and moral values.We detail a novel approach that uses inverse reinforcement learning to learn aset of unspecified constraints from demonstrations of the task, andreinforcement learning to learn to maximize the environment rewards. Moreprecisely, we assume that an agent can observe traces of behavior of members ofthe society but has no access to the explicit set of constraints that give riseto the observed behavior. Inverse reinforcement learning is used to learn suchconstraints, that are then combined with a possibly orthogonal value functionthrough the use of a contextual bandit-based orchestrator that picks acontextually-appropriate choice between the two policies (constraint-based andenvironment reward-based) when taking actions. The contextual banditorchestrator allows the agent to mix policies in novel ways, taking the bestactions from either a reward maximizing or constrained policy. In addition, theorchestrator is transparent on which policy is being employed at each timestep. We test our algorithms using a Pac-Man domain and show that the agent isable to learn to act optimally, act within the demonstrated constraints, andmix these two functions in complex ways.

The Complexity of Probabilistic Lobbying

  We propose models for lobbying in a probabilistic environment, in which anactor (called "The Lobby") seeks to influence voters' preferences of voting foror against multiple issues when the voters' preferences are represented interms of probabilities. In particular, we provide two evaluation criteria andtwo bribery methods to formally describe these models, and we consider theresulting forms of lobbying with and without issue weighting. We provide aformal analysis for these problems of lobbying in a stochastic environment, anddetermine their classical and parameterized complexity depending on the givenbribery/evaluation criteria and on various natural parameterizations.Specifically, we show that some of these problems can be solved in polynomialtime, some are NP-complete but fixed-parameter tractable, and some areW[2]-complete. Finally, we provide approximability and inapproximabilityresults for these problems and several variants.

Manipulating the Probabilistic Serial Rule

  The probabilistic serial (PS) rule is one of the most prominent randomizedrules for the assignment problem. It is well-known for its superior fairnessand welfare properties. However, PS is not immune to manipulative behaviour bythe agents. We initiate the study of the computational complexity of an agentmanipulating the PS rule. We show that computing an expected utility betterresponse is NP- hard. On the other hand, we present a polynomial-time algorithmto compute a lexicographic best response. For the case of two agents, we showthat even an expected utility best response can be computed in polynomial time.Our result for the case of two agents relies on an interesting connection withsequential allocation of discrete objects.

Equilibria Under the Probabilistic Serial Rule

  The probabilistic serial (PS) rule is a prominent randomized rule forassigning indivisible goods to agents. Although it is well known for its goodfairness and welfare properties, it is not strategyproof. In view of this, weaddress several fundamental questions regarding equilibria under PS. Firstly,we show that Nash deviations under the PS rule can cycle. Despite thepossibilities of cycles, we prove that a pure Nash equilibrium is guaranteed toexist under the PS rule. We then show that verifying whether a given profile isa pure Nash equilibrium is coNP-complete, and computing a pure Nash equilibriumis NP-hard. For two agents, we present a linear-time algorithm to compute apure Nash equilibrium which yields the same assignment as the truthful profile.Finally, we conduct experiments to evaluate the quality of the equilibria thatexist under the PS rule, finding that the vast majority of pure Nash equilibriayield social welfare that is at least that of the truthful profile.

Stable Matching with Uncertain Linear Preferences

  We consider the two-sided stable matching setting in which there may beuncertainty about the agents' preferences due to limited information orcommunication. We consider three models of uncertainty: (1) lottery model ---in which for each agent, there is a probability distribution over linearpreferences, (2) compact indifference model --- for each agent, a weakpreference order is specified and each linear order compatible with the weakorder is equally likely and (3) joint probability model --- there is a lotteryover preference profiles. For each of the models, we study the computationalcomplexity of computing the stability probability of a given matching as wellas finding a matching with the highest probability of being stable. We alsoexamine more restricted problems such as deciding whether a certainly stablematching exists. We find a rich complexity landscape for these problems,indicating that the form uncertainty takes is significant.

A Cost-Effective Framework for Preference Elicitation and Aggregation

  We propose a cost-effective framework for preference elicitation andaggregation under the Plackett-Luce model with features. Given a budget, ourframework iteratively computes the most cost-effective elicitation questions inorder to help the agents make a better group decision.  We illustrate the viability of the framework with experiments on AmazonMechanical Turk, which we use to estimate the cost of answering different typesof elicitation questions. We compare the prediction accuracy of our frameworkwhen adopting various information criteria that evaluate the expectedinformation gain from a question. Our experiments show carefully designedinformation criteria are much more efficient, i.e., they arrive at the correctanswer using fewer queries, than randomly asking questions given the budgetconstraint.

A Systematic Classification of Knowledge, Reasoning, and Context within  the ARC Dataset

  The recent work of Clark et al. introduces the AI2 Reasoning Challenge (ARC)and the associated ARC dataset that partitions open domain, complex sciencequestions into an Easy Set and a Challenge Set. That paper includes an analysisof 100 questions with respect to the types of knowledge and reasoning requiredto answer them; however, it does not include clear definitions of these types,nor does it offer information about the quality of the labels. We propose acomprehensive set of definitions of knowledge and reasoning types necessary foranswering the questions in the ARC dataset. Using ten annotators and asophisticated annotation interface, we analyze the distribution of labelsacross the Challenge Set and statistics related to them. Additionally, wedemonstrate that although naive information retrieval methods return sentencesthat are irrelevant to answering the query, sufficient supporting text is oftenpresent in the (ARC) corpus. Evaluating with human-selected relevant sentencesimproves the performance of a neural machine comprehension model by 42 points.

Improving Natural Language Inference Using External Knowledge in the  Science Questions Domain

  Natural Language Inference (NLI) is fundamental to many Natural LanguageProcessing (NLP) applications including semantic search and question answering.The NLI problem has gained significant attention thanks to the release of largescale, challenging datasets. Present approaches to the problem largely focus onlearning-based methods that use only textual information in order to classifywhether a given premise entails, contradicts, or is neutral with respect to agiven hypothesis. Surprisingly, the use of methods based on structuredknowledge -- a central topic in artificial intelligence -- has not receivedmuch attention vis-a-vis the NLI problem. While there are many open knowledgebases that contain various types of reasoning information, their use for NLIhas not been well explored. To address this, we present a combination oftechniques that harness knowledge graphs to improve performance on the NLIproblem in the science questions domain. We present the results of applying ourtechniques on text, graph, and text-to-graph based models, and discussimplications for the use of external knowledge in solving the NLI problem. Ourmodel achieves the new state-of-the-art performance on the NLI problem over theSciTail science questions dataset.

Flexible Representative Democracy: An Introduction with Binary Issues

  We introduce Flexible Representative Democracy (FRD), a novel hybrid ofRepresentative Democracy (RD) and direct democracy (DD), in which voters canalter the issue-dependent weights of a set of elected representatives. In linewith the literature on Interactive Democracy, our model allows the voters toactively determine the degree to which the system is direct versusrepresentative. However, unlike Liquid Democracy, FRD uses strictlynon-transitive delegations, making delegation cycles impossible, and maintainsa fixed set of accountable elected representatives. We present FRD and analyzeit using a computational approach with issues that are binary and symmetric; wecompare the outcomes of various democratic systems using Direct Democracy withmajority voting as an ideal baseline. First, we demonstrate the shortcomings ofRepresentative Democracy in our model. We provide NP-Hardness results forelecting an ideal set of representatives, discuss pathologies, and demonstrateempirically that common multi-winner election rules for selectingrepresentatives do not perform well in expectation. To analyze the behavior ofFRD, we begin by providing theoretical results on how issue-specificdelegations determine outcomes. Finally, we provide empirical results comparingthe outcomes of RD with fixed sets of proxies across issues versus FRD withissue-specific delegations. Our results show that variants of Proxy Votingyield no discernible benefit over RD and reveal the potential for FRD toimprove outcomes as voter participation increases, further motivating the useof issue-specific delegations.

