Conducting Truthful Surveys, Cheaply

  We consider the problem of conducting a survey with the goal of obtaining an
unbiased estimator of some population statistic when individuals have unknown
costs (drawn from a known prior) for participating in the survey. Individuals
must be compensated for their participation and are strategic agents, and so
the payment scheme must incentivize truthful behavior. We derive optimal
truthful mechanisms for this problem for the two goals of minimizing the
variance of the estimator given a fixed budget, and minimizing the expected
cost of the survey given a fixed variance goal.


Potential Networks, Contagious Communities, and Understanding Social
  Network Structure

  In this paper we study how the network of agents adopting a particular
technology relates to the structure of the underlying network over which the
technology adoption spreads. We develop a model and show that the network of
agents adopting a particular technology may have characteristics that differ
significantly from the social network of agents over which the technology
spreads. For example, the network induced by a cascade may have a heavy-tailed
degree distribution even if the original network does not.
  This provides evidence that online social networks created by technology
adoption over an underlying social network may look fundamentally different
from social networks and indicates that using data from many online social
networks may mislead us if we try to use it to directly infer the structure of
social networks. Our results provide an alternate explanation for certain
properties repeatedly observed in data sets, for example: heavy-tailed degree
distribution, network densification, shrinking diameter, and network community
profile. These properties could be caused by a sort of `sampling bias' rather
than by attributes of the underlying social structure. By generating networks
using cascades over traditional network models that do not themselves contain
these properties, we can nevertheless reliably produce networks that contain
all these properties.
  An opportunity for interesting future research is developing new methods that
correctly infer underlying network structure from data about a network that is
generated via a cascade spread over the underlying network.


Finding Overlapping Communities in Social Networks: Toward a Rigorous
  Approach

  A "community" in a social network is usually understood to be a group of
nodes more densely connected with each other than with the rest of the network.
This is an important concept in most domains where networks arise: social,
technological, biological, etc. For many years algorithms for finding
communities implicitly assumed communities are nonoverlapping (leading to use
of clustering-based approaches) but there is increasing interest in finding
overlapping communities. A barrier to finding communities is that the solution
concept is often defined in terms of an NP-complete problem such as Clique or
Hierarchical Clustering.
  This paper seeks to initiate a rigorous approach to the problem of finding
overlapping communities, where "rigorous" means that we clearly state the
following: (a) the object sought by our algorithm (b) the assumptions about the
underlying network (c) the (worst-case) running time.
  Our assumptions about the network lie between worst-case and average-case. An
average case analysis would require a precise probabilistic model of the
network, on which there is currently no consensus. However, some plausible
assumptions about network parameters can be gleaned from a long body of work in
the sociology community spanning five decades focusing on the study of
individual communities and ego-centric networks. Thus our assumptions are
somewhat "local" in nature. Nevertheless they suffice to permit a rigorous
analysis of running time of algorithms that recover global structure.
  Our algorithms use random sampling similar to that in property testing and
algorithms for dense graphs. However, our networks are not necessarily dense
graphs, not even in local neighborhoods.
  Our algorithms explore a local-global relationship between ego-centric and
socio-centric networks that we hope will provide a fruitful framework for
future work both in computer science and sociology.


Graph Isomorphism and the Lasserre Hierarchy

  In this paper we show lower bounds for a certain large class of algorithms
solving the Graph Isomorphism problem, even on expander graph instances.
Spielman [25] shows an algorithm for isomorphism of strongly regular expander
graphs that runs in time exp(O(n^(1/3)) (this bound was recently improved to
expf O(n^(1/5) [5]). It has since been an open question to remove the
requirement that the graph be strongly regular. Recent algorithmic results show
that for many problems the Lasserre hierarchy works surprisingly well when the
underlying graph has expansion properties. Moreover, recent work of Atserias
and Maneva [3] shows that k rounds of the Lasserre hierarchy is a
generalization of the k-dimensional Weisfeiler-Lehman algorithm for Graph
Isomorphism. These two facts combined make the Lasserre hierarchy a good
candidate for solving graph isomorphism on expander graphs. Our main result
rules out this promising direction by showing that even Omega(n) rounds of the
Lasserre semidefinite program hierarchy fail to solve the Graph Isomorphism
problem even on expander graphs.


A Descending Price Auction for Matching Markets

  This work presents a descending-price-auction algorithm to obtain the maximum
market-clearing price vector (MCP) in unit-demand matching markets with m items
by exploiting the combinatorial structure. With a shrewd choice of goods for
which the prices are reduced in each step, the algorithm only uses the
combinatorial structure, which avoids solving LPs and enjoys a strongly
polynomial runtime of $O(m^4)$. Critical to the algorithm is determining the
set of under-demanded goods for which we reduce the prices simultaneously in
each step of the algorithm. This we accomplish by choosing the subset of goods
that maximize a skewness function, which makes the bipartite graph series
converges to the combinatorial structure at the maximum MCP in $O(m^2)$ steps.
A graph coloring algorithm is proposed to find the set of goods with the
maximal skewness value that yields $O(m^4)$ complexity.


Eliciting Expertise without Verification

  A central question of crowd-sourcing is how to elicit expertise from agents.
This is even more difficult when answers cannot be directly verified. A key
challenge is that sophisticated agents may strategically withhold effort or
information when they believe their payoff will be based upon comparison with
other agents whose reports will likely omit this information due to lack of
effort or expertise.
  Our work defines a natural model for this setting based on the assumption
that \emph{more sophisticated agents know the beliefs of less sophisticated
agents}.
  We then provide a mechanism design framework for this setting. From this
framework, we design several novel mechanisms, for both the single and multiple
question settings, that (1) encourage agents to invest effort and provide their
information honestly; (2) output a correct "hierarchy" of the information when
agents are rational.


Water from Two Rocks: Maximizing the Mutual Information

  We build a natural connection between the learning problem, co-training, and
forecast elicitation without verification (related to peer-prediction) and
address them simultaneously using the same information theoretic approach.
  In co-training/multiview learning, the goal is to aggregate two views of data
into a prediction for a latent label. We show how to optimally combine two
views of data by reducing the problem to an optimization problem. Our work
gives a unified and rigorous approach to the general setting.
  In forecast elicitation without verification we seek to design a mechanism
that elicits high quality forecasts from agents in the setting where the
mechanism does not have access to the ground truth. By assuming the agents'
information is independent conditioning on the outcome, we propose mechanisms
where truth-telling is a strict equilibrium for both the single-task and
multi-task settings. Our multi-task mechanism additionally has the property
that the truth-telling equilibrium pays better than any other strategy profile
and strictly better than any other "non-permutation" strategy profile when the
prior satisfies some mild conditions.


Social Learning with Questions

  This work studies sequential social learning (also known as Bayesian
observational learning), and how private communication can enable agents to
avoid herding to the wrong action/state. Starting from the seminal BHW
(Bikhchandani, Hirshleifer, and Welch, 1992) model where asymptotic learning
does not occur, we allow agents to ask private and finite questions to a
bounded subset of their predecessors. While retaining the publicly observed
history of the agents and their Bayes rationality from the BHW model, we
further assume that both the ability to ask questions and the questions
themselves are common knowledge. Then interpreting asking questions as
partitioning information sets, we study whether asymptotic learning can be
achieved with finite capacity questions. Restricting our attention to the
network where every agent is only allowed to query her immediate predecessor,
an explicit construction shows that a 1-bit question from each agent is enough
to enable asymptotic learning.


On the Complexity of Nash Equilibria of Action-Graph Games

  We consider the problem of computing Nash Equilibria of action-graph games
(AGGs). AGGs, introduced by Bhat and Leyton-Brown, is a succinct representation
of games that encapsulates both "local" dependencies as in graphical games, and
partial indifference to other agents' identities as in anonymous games, which
occur in many natural settings. This is achieved by specifying a graph on the
set of actions, so that the payoff of an agent for selecting a strategy depends
only on the number of agents playing each of the neighboring strategies in the
action graph. We present a Polynomial Time Approximation Scheme for computing
mixed Nash equilibria of AGGs with constant treewidth and a constant number of
agent types (and an arbitrary number of strategies), together with hardness
results for the cases when either the treewidth or the number of agent types is
unconstrained. In particular, we show that even if the action graph is a tree,
but the number of agent-types is unconstrained, it is NP-complete to decide the
existence of a pure-strategy Nash equilibrium and PPAD-complete to compute a
mixed Nash equilibrium (even an approximate one); similarly for symmetric AGGs
(all agents belong to a single type), if we allow arbitrary treewidth. These
hardness results suggest that, in some sense, our PTAS is as strong of a
positive result as one can expect.


General Hardness Amplification of Predicates and Puzzles

  We give new proofs for the hardness amplification of efficiently samplable
predicates and of weakly verifiable puzzles which generalize to new settings.
More concretely, in the first part of the paper, we give a new proof of Yao's
XOR-Lemma that additionally applies to related theorems in the cryptographic
setting. Our proof seems simpler than previous ones, yet immediately
generalizes to statements similar in spirit such as the extraction lemma used
to obtain pseudo-random generators from one-way functions [Hastad, Impagliazzo,
Levin, Luby, SIAM J. on Comp. 1999].
  In the second part of the paper, we give a new proof of hardness
amplification for weakly verifiable puzzles, which is more general than
previous ones in that it gives the right bound even for an arbitrary monotone
function applied to the checking circuit of the underlying puzzle.
  Both our proofs are applicable in many settings of interactive cryptographic
protocols because they satisfy a property that we call "non-rewinding". In
particular, we show that any weak cryptographic protocol whose security is
given by the unpredictability of single bits can be strengthened with a natural
information theoretic protocol. As an example, we show how these theorems solve
the main open question from [Halevi and Rabin, TCC2008] concerning bit
commitment.


Characterizing Strategic Cascades on Networks

  Transmission of disease, spread of information and rumors, adoption of new
products, and many other network phenomena can be fruitfully modeled as
cascading processes, where actions chosen by nodes influence the subsequent
behavior of neighbors in the network graph. Current literature on cascades
tends to assume nodes choose myopically based on the state of choices already
taken by other nodes. We examine the possibility of strategic choice, where
agents representing nodes anticipate the choices of others who have not yet
decided, and take into account their own influence on such choices. Our study
employs the framework of Chierichetti et al. [2012], who (under assumption of
myopic node behavior) investigate the scheduling of node decisions to promote
cascades of product adoptions preferred by the scheduler. We show that when
nodes behave strategically, outcomes can be extremely different. We exhibit
cases where in the strategic setting 100% of agents adopt, but in the myopic
setting only an arbitrarily small epsilon % do. Conversely, we present cases
where in the strategic setting 0% of agents adopt, but in the myopic setting
(100-epsilon)% do, for any constant epsilon > 0. Additionally, we prove some
properties of cascade processes with strategic agents, both in general and for
particular classes of graphs.


Buying Private Data without Verification

  We consider the problem of designing a survey to aggregate non-verifiable
information from a privacy-sensitive population: an analyst wants to compute
some aggregate statistic from the private bits held by each member of a
population, but cannot verify the correctness of the bits reported by
participants in his survey. Individuals in the population are strategic agents
with a cost for privacy, \ie, they not only account for the payments they
expect to receive from the mechanism, but also their privacy costs from any
information revealed about them by the mechanism's outcome---the computed
statistic as well as the payments---to determine their utilities. How can the
analyst design payments to obtain an accurate estimate of the population
statistic when individuals strategically decide both whether to participate and
whether to truthfully report their sensitive information?
  We design a differentially private peer-prediction mechanism that supports
accurate estimation of the population statistic as a Bayes-Nash equilibrium in
settings where agents have explicit preferences for privacy. The mechanism
requires knowledge of the marginal prior distribution on bits $b_i$, but does
not need full knowledge of the marginal distribution on the costs $c_i$,
instead requiring only an approximate upper bound. Our mechanism guarantees
$\epsilon$-differential privacy to each agent $i$ against any adversary who can
observe the statistical estimate output by the mechanism, as well as the
payments made to the $n-1$ other agents $j\neq i$. Finally, we show that with
slightly more structured assumptions on the privacy cost functions of each
agent, the cost of running the survey goes to $0$ as the number of agents
diverges.


Identifying the Major Sources of Variance in Transaction Latencies:
  Towards More Predictable Databases

  Decades of research have sought to improve transaction processing performance
and scalability in database management systems (DBMSs). However, significantly
less attention has been dedicated to the predictability of performance: how
often individual transactions exhibit execution latency far from the mean?
Performance predictability is vital when transaction processing lies on the
critical path of a complex enterprise software or an interactive web service,
as well as in emerging database-as-a-service markets where customers contract
for guaranteed levels of performance. In this paper, we take several steps
towards achieving more predictable database systems. First, we propose a
profiling framework called VProfiler that, given the source code of a DBMS, is
able to identify the dominant sources of variance in transaction latency.
VProfiler automatically instruments the DBMS source code to deconstruct the
overall variance of transaction latencies into variances and covariances of the
execution time of individual functions, which in turn provide insight into the
root causes of variance. Second, we use VProfiler to analyze MySQL and Postgres
- two of the most popular and complex open-source database systems. Our case
studies reveal that the primary causes of variance in MySQL and Postgres are
lock scheduling and centralized logging, respectively. Finally, based on
VProfiler's findings, we further focus on remedying the performance variance of
MySQL by (1) proposing a new lock scheduling algorithm, called Variance-Aware
Transaction Scheduling (VATS), (2) enhancing the buffer pool replacement
policy, and (3) identifying tuning parameters that can reduce variance
significantly. Our experimental results show that our schemes reduce overall
transaction latency variance by 37% on average (and up to 64%) without
compromising throughput or mean latency.


Social Learning in a Changing World

  We study a model of learning on social networks in dynamic environments,
describing a group of agents who are each trying to estimate an underlying
state that varies over time, given access to weak signals and the estimates of
their social network neighbors.
  We study three models of agent behavior. In the "fixed response" model,
agents use a fixed linear combination to incorporate information from their
peers into their own estimate. This can be thought of as an extension of the
DeGroot model to a dynamic setting. In the "best response" model, players
calculate minimum variance linear estimators of the underlying state.
  We show that regardless of the initial configuration, fixed response dynamics
converge to a steady state, and that the same holds for best response on the
complete graph. We show that best response dynamics can, in the long term, lead
to estimators with higher variance than is achievable using well chosen fixed
responses.
  The "penultimate prediction" model is an elaboration of the best response
model. While this model only slightly complicates the computations required of
the agents, we show that in some cases it greatly increases the efficiency of
learning, and on complete graphs is in fact optimal, in a strong sense.


Putting Peer Prediction Under the Micro(economic)scope and Making
  Truth-telling Focal

  Peer-prediction is a (meta-)mechanism which, given any proper scoring rule,
produces a mechanism to elicit privately-held, non-verifiable information from
self-interested agents. Formally, truth-telling is a strict Nash equilibrium of
the mechanism. Unfortunately, there may be other equilibria as well (including
uninformative equilibria where all players simply report the same fixed signal,
regardless of their true signal) and, typically, the truth-telling equilibrium
does not have the highest expected payoff. The main result of this paper is to
show that, in the symmetric binary setting, by tweaking peer-prediction, in
part by carefully selecting the proper scoring rule it is based on, we can make
the truth-telling equilibrium focal---that is, truth-telling has higher
expected payoff than any other equilibrium.
  Along the way, we prove the following: in the setting where agents receive
binary signals we 1) classify all equilibria of the peer-prediction mechanism;
2) introduce a new technical tool for understanding scoring rules, which allows
us to make truth-telling pay better than any other informative equilibrium; 3)
leverage this tool to provide an optimal version of the previous result; that
is, we optimize the gap between the expected payoff of truth-telling and other
informative equilibria; and 4) show that with a slight modification to the peer
prediction framework, we can, in general, make the truth-telling equilibrium
focal---that is, truth-telling pays more than any other equilibrium (including
the uninformative equilibria).


Equilibrium Selection in Information Elicitation without Verification
  via Information Monotonicity

  Peer-prediction is a mechanism which elicits privately-held, non-variable
information from self-interested agents---formally, truth-telling is a strict
Bayes Nash equilibrium of the mechanism. The original Peer-prediction mechanism
suffers from two main limitations: (1) the mechanism must know the "common
prior" of agents' signals; (2) additional undesirable and non-truthful
equilibria exist which often have a greater expected payoff than the
truth-telling equilibrium. A series of results has successfully weakened the
known common prior assumption. However, the equilibrium multiplicity issue
remains a challenge.
  In this paper, we address the above two problems. In the setting where a
common prior exists but is not known to the mechanism we show (1) a general
negative result applying to a large class of mechanisms showing truth-telling
can never pay strictly more in expectation than a particular set of equilibria
where agents collude to "relabel" the signals and tell the truth after
relabeling signals; (2) provide a mechanism that has no information about the
common prior but where truth-telling pays as much in expectation as any
relabeling equilibrium and pays strictly more than any other symmetric
equilibrium; (3) moreover in our mechanism, if the number of agents is
sufficiently large, truth-telling pays similarly to any equilibrium close to a
"relabeling" equilibrium and pays strictly more than any equilibrium that is
not close to a relabeling equilibrium.


An Information Theoretic Framework For Designing Information Elicitation
  Mechanisms That Reward Truth-telling

  In the setting where information cannot be verified, we propose a simple yet
powerful information theoretical framework---the Mutual Information
Paradigm---for information elicitation mechanisms. Our framework pays every
agent a measure of mutual information between her signal and a peer's signal.
We require that the mutual information measurement has the key property that
any "data processing" on the two random variables will decrease the mutual
information between them. We identify such information measures that generalize
Shannon mutual information.
  Our Mutual Information Paradigm overcomes the two main challenges in
information elicitation without verification: (1) how to incentivize effort and
avoid agents colluding to report random or identical responses (2) how to
motivate agents who believe they are in the minority to report truthfully.
  Aided by the information measures we found, (1) we use the paradigm to design
a family of novel mechanisms where truth-telling is a dominant strategy and any
other strategy will decrease every agent's expected payment (in the
multi-question, detail free, minimal setting where the number of questions is
large); (2) we show the versatility of our framework by providing a unified
theoretical understanding of existing mechanisms---Peer Prediction [Miller
2005], Bayesian Truth Serum [Prelec 2004], and Dasgupta and Ghosh [2013]---by
mapping them into our framework such that theoretical results of those existing
mechanisms can be reconstructed easily.
  We also give an impossibility result which illustrates, in a certain sense,
the the optimality of our framework.


Don't Be Greedy: Leveraging Community Structure to Find High Quality
  Seed Sets for Influence Maximization

  We consider the problem of maximizing the spread of influence in a social
network by choosing a fixed number of initial seeds --- a central problem in
the study of network cascades. The majority of existing work on this problem,
formally referred to as the influence maximization problem, is designed for
submodular cascades. Despite the empirical evidence that many cascades are
non-submodular, little work has been done focusing on non-submodular influence
maximization.
  We propose a new heuristic for solving the influence maximization problem and
show via simulations on real-world and synthetic networks that our algorithm
outputs more influential seed sets than the state-of-the-art greedy algorithm
in many natural cases, with average improvements of 7% for submodular cascades,
and 55% for non-submodular cascades. Our heuristic uses a dynamic programming
approach on a hierarchical decomposition of the social network to leverage the
relation between the spread of cascades and the community structure of social
networks. We verify the importance of network structure by showing the quality
of the hierarchical decomposition impacts the quality of seed set output by our
algorithm. We also present "worst-case" theoretical results proving that in
certain settings our algorithm outputs seed sets that are a factor of
$\Theta(\sqrt{n})$ more influential than those of the greedy algorithm, where
$n$ is the number of nodes in the network. Finally, we generalize our algorithm
to a message passing version that can be used to find seed sets that have at
least as much influence as the dynamic programming algorithms.


Beyond Worst-Case (In)approximability of Nonsubmodular Influence
  Maximization

  We consider the problem of maximizing the spread of influence in a social
network by choosing a fixed number of initial seeds, formally referred to as
the influence maximization problem. It admits a $(1-1/e)$-factor approximation
algorithm if the influence function is submodular. Otherwise, in the worst
case, the problem is NP-hard to approximate to within a factor of
$N^{1-\varepsilon}$. This paper studies whether this worst-case hardness result
can be circumvented by making assumptions about either the underlying network
topology or the cascade model. All of our assumptions are motivated by many
real life social network cascades.
  First, we present strong inapproximability results for a very restricted
class of networks called the (stochastic) hierarchical blockmodel, a special
case of the well-studied (stochastic) blockmodel in which relationships between
blocks admit a tree structure. We also provide a dynamic-program based
polynomial time algorithm which optimally computes a directed variant of the
influence maximization problem on hierarchical blockmodel networks. Our
algorithm indicates that the inapproximability result is due to the
bidirectionality of influence between agent-blocks.
  Second, we present strong inapproximability results for a class of influence
functions that are "almost" submodular, called 2-quasi-submodular. Our
inapproximability results hold even for any 2-quasi-submodular $f$ fixed in
advance. This result also indicates that the "threshold" between submodularity
and nonsubmodularity is sharp, regarding the approximability of influence
maximization.


Characterizing Adversarial Subspaces Using Local Intrinsic
  Dimensionality

  Deep Neural Networks (DNNs) have recently been shown to be vulnerable against
adversarial examples, which are carefully crafted instances that can mislead
DNNs to make errors during prediction. To better understand such attacks, a
characterization is needed of the properties of regions (the so-called
'adversarial subspaces') in which adversarial examples lie. We tackle this
challenge by characterizing the dimensional properties of adversarial regions,
via the use of Local Intrinsic Dimensionality (LID). LID assesses the
space-filling capability of the region surrounding a reference example, based
on the distance distribution of the example to its neighbors. We first provide
explanations about how adversarial perturbation can affect the LID
characteristic of adversarial regions, and then show empirically that LID
characteristics can facilitate the distinction of adversarial examples
generated using state-of-the-art attacks. As a proof-of-concept, we show that a
potential application of LID is to distinguish adversarial examples, and the
preliminary results show that it can outperform several state-of-the-art
detection measures by large margins for five attack strategies considered in
this paper across three benchmark datasets. Our analysis of the LID
characteristic for adversarial regions not only motivates new directions of
effective adversarial defense, but also opens up more challenges for developing
new attacks to better understand the vulnerabilities of DNNs.


Optimal Testing of Reed-Muller Codes

  We consider the problem of testing if a given function f : F_2^n -> F_2 is
close to any degree d polynomial in n variables, also known as the Reed-Muller
testing problem. The Gowers norm is based on a natural 2^{d+1}-query test for
this property. Alon et al. [AKKLR05] rediscovered this test and showed that it
accepts every degree d polynomial with probability 1, while it rejects
functions that are Omega(1)-far with probability Omega(1/(d 2^{d})). We give an
asymptotically optimal analysis of this test, and show that it rejects
functions that are (even only) Omega(2^{-d})-far with Omega(1)-probability (so
the rejection probability is a universal constant independent of d and n). This
implies a tight relationship between the (d+1)st Gowers norm of a function and
its maximal correlation with degree d polynomials, when the correlation is
close to 1. Our proof works by induction on n and yields a new analysis of even
the classical Blum-Luby-Rubinfeld [BLR93] linearity test, for the setting of
functions mapping F_2^n to F_2. The optimality follows from a tighter analysis
of counterexamples to the "inverse conjecture for the Gowers norm" constructed
by [GT09,LMS08]. Our result has several implications. First, it shows that the
Gowers norm test is tolerant, in that it also accepts close codewords. Second,
it improves the parameters of an XOR lemma for polynomials given by Viola and
Wigderson [VW07]. Third, it implies a "query hierarchy" result for property
testing of affine-invariant properties. That is, for every function q(n), it
gives an affine-invariant property that is testable with O(q(n))-queries, but
not with o(q(n))-queries, complementing an analogous result of [GKNR09] for
graph properties.


Complex Contagions in Kleinberg's Small World Model

  Complex contagions describe diffusion of behaviors in a social network in
settings where spreading requires the influence by two or more neighbors. In a
$k$-complex contagion, a cluster of nodes are initially infected, and
additional nodes become infected in the next round if they have at least $k$
already infected neighbors. It has been argued that complex contagions better
model behavioral changes such as adoption of new beliefs, fashion trends or
expensive technology innovations. This has motivated rigorous understanding of
spreading of complex contagions in social networks. Despite simple contagions
($k=1$) that spread fast in all small world graphs, how complex contagions
spread is much less understood. Previous work~\cite{Ghasemiesfeh:2013:CCW}
analyzes complex contagions in Kleinberg's small world
model~\cite{kleinberg00small} where edges are randomly added according to a
spatial distribution (with exponent $\gamma$) on top of a two dimensional grid
structure. It has been shown in~\cite{Ghasemiesfeh:2013:CCW} that the speed of
complex contagions differs exponentially when $\gamma=0$ compared to when
$\gamma=2$.
  In this paper, we fully characterize the entire parameter space of $\gamma$
except at one point, and provide upper and lower bounds for the speed of
$k$-complex contagions. We study two subtly different variants of Kleinberg's
small world model and show that, with respect to complex contagions, they
behave differently. For each model and each $k \geq 2$, we show that there is
an intermediate range of values, such that when $\gamma$ takes any of these
values, a $k$-complex contagion spreads quickly on the corresponding graph, in
a polylogarithmic number of rounds. However, if $\gamma$ is outside this range,
then a $k$-complex contagion requires a polynomial number of rounds to spread
to the entire network.


Constrained Non-Monotone Submodular Maximization: Offline and Secretary
  Algorithms

  Constrained submodular maximization problems have long been studied, with
near-optimal results known under a variety of constraints when the submodular
function is monotone. The case of non-monotone submodular maximization is less
understood: the first approximation algorithms even for the unconstrainted
setting were given by Feige et al. (FOCS '07). More recently, Lee et al. (STOC
'09, APPROX '09) show how to approximately maximize non-monotone submodular
functions when the constraints are given by the intersection of p matroid
constraints; their algorithm is based on local-search procedures that consider
p-swaps, and hence the running time may be n^Omega(p), implying their algorithm
is polynomial-time only for constantly many matroids. In this paper, we give
algorithms that work for p-independence systems (which generalize constraints
given by the intersection of p matroids), where the running time is poly(n,p).
Our algorithm essentially reduces the non-monotone maximization problem to
multiple runs of the greedy algorithm previously used in the monotone case.
  Our idea of using existing algorithms for monotone functions to solve the
non-monotone case also works for maximizing a submodular function with respect
to a knapsack constraint: we get a simple greedy-based constant-factor
approximation for this problem.
  With these simpler algorithms, we are able to adapt our approach to
constrained non-monotone submodular maximization to the (online) secretary
setting, where elements arrive one at a time in random order, and the algorithm
must make irrevocable decisions about whether or not to select each element as
it arrives. We give constant approximations in this secretary setting when the
algorithm is constrained subject to a uniform matroid or a partition matroid,
and give an O(log k) approximation when it is constrained by a general matroid
of rank k.


How Complex Contagions Spread Quickly in the Preferential Attachment
  Model and Other Time-Evolving Networks

  In this paper, we study the spreading speed of complex contagions in a social
network. A $k$-complex contagion starts from a set of initially infected seeds
such that any node with at least $k$ infected neighbors gets infected. Simple
contagions, i.e., $k=1$, quickly spread to the entire network in small world
graphs. However, fast spreading of complex contagions appears to be less likely
and more delicate; the successful cases depend crucially on the network
structure~\cite{G08,Ghasemiesfeh:2013:CCW}.
  Our main result shows that complex contagions can spread fast in a general
family of time-evolving networks that includes the preferential attachment
model~\cite{barabasi99emergence}. We prove that if the initial seeds are chosen
as the oldest nodes in a network of this family, a $k$-complex contagion covers
the entire network of $n$ nodes in $O(\log n)$ steps. We show that the choice
of the initial seeds is crucial. If the initial seeds are uniformly randomly
chosen in the PA model, even with a polynomial number of them, a complex
contagion would stop prematurely. The oldest nodes in a preferential attachment
model are likely to have high degrees. However, we remark that it is actually
not the power law degree distribution per se that facilitates fast spreading of
complex contagions, but rather the evolutionary graph structure of such models.
Some members of the said family do not even have a power-law distribution.
  We also prove that complex contagions are fast in the copy
model~\cite{KumarRaRa00}, a variant of the preferential attachment family.
  Finally, we prove that when a complex contagion starts from an arbitrary set
of initial seeds on a general graph, determining if the number of infected
vertices is above a given threshold is $\mathbf{P}$-complete. Thus, one cannot
hope to categorize all the settings in which complex contagions percolate in a
graph.


