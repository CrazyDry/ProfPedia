Abnormal Object Recognition: A Comprehensive Study

  When describing images, humans tend not to talk about the obvious, but rathermention what they find interesting. We argue that abnormalities and deviationsfrom typicalities are among the most important components that form what isworth mentioning. In this paper we introduce the abnormality detection as arecognition problem and show how to model typicalities and, consequently,meaningful deviations from prototypical properties of categories. Our model canrecognize abnormalities and report the main reasons of any recognizedabnormality. We introduce the abnormality detection dataset and showinteresting results on how to reason about abnormalities.

Unsupervised Deep Embedding for Clustering Analysis

  Clustering is central to many data-driven application domains and has beenstudied extensively in terms of distance functions and grouping algorithms.Relatively little work has focused on learning representations for clustering.In this paper, we propose Deep Embedded Clustering (DEC), a method thatsimultaneously learns feature representations and cluster assignments usingdeep neural networks. DEC learns a mapping from the data space to alower-dimensional feature space in which it iteratively optimizes a clusteringobjective. Our experimental evaluations on image and text corpora showsignificant improvement over state-of-the-art methods.

Deep3D: Fully Automatic 2D-to-3D Video Conversion with Deep  Convolutional Neural Networks

  As 3D movie viewing becomes mainstream and Virtual Reality (VR) marketemerges, the demand for 3D contents is growing rapidly. Producing 3D videos,however, remains challenging. In this paper we propose to use deep neuralnetworks for automatically converting 2D videos and images to stereoscopic 3Dformat. In contrast to previous automatic 2D-to-3D conversion algorithms, whichhave separate stages and need ground truth depth map as supervision, ourapproach is trained end-to-end directly on stereo pairs extracted from 3Dmovies. This novel training scheme makes it possible to exploit orders ofmagnitude more data and significantly increases performance. Indeed, Deep3Doutperforms baselines in both quantitative and human subject evaluations.

YOLOv3: An Incremental Improvement

  We present some updates to YOLO! We made a bunch of little design changes tomake it better. We also trained this new network that's pretty swell. It's alittle bigger than last time but more accurate. It's still fast though, don'tworry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD butthree times faster. When we look at the old .5 IOU mAP detection metric YOLOv3is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always,all the code is online at https://pjreddie.com/yolo/

What Should I Do Now? Marrying Reinforcement Learning and Symbolic  Planning

  Long-term planning poses a major difficulty to many reinforcement learningalgorithms. This problem becomes even more pronounced in dynamic visualenvironments. In this work we propose Hierarchical Planning and ReinforcementLearning (HIP-RL), a method for merging the benefits and capabilities ofSymbolic Planning with the learning abilities of Deep Reinforcement Learning.We apply HIPRL to the complex visual tasks of interactive question answeringand visual semantic planning and achieve state-of-the-art results on threechallenging datasets all while taking fewer steps at test time and training infewer iterations. Sample results can be found at youtu.be/0TtWJ_0mPfI

Semantic Understanding of Professional Soccer Commentaries

  This paper presents a novel approach to the problem of semantic parsing vialearning the correspondences between complex sentences and rich sets of events.Our main intuition is that correct correspondences tend to occur morefrequently. Our model benefits from a discriminative notion of similarity tolearn the correspondence between sentence and an event and a ranking machinerythat scores the popularity of each correspondence. Our method can discover agroup of events (called macro-events) that best describes a sentence. Weevaluate our method on our novel dataset of professional soccer commentaries.The empirical results show that our method significantly outperforms thestate-of-theart.

Image Classification and Retrieval from User-Supplied Tags

  This paper proposes direct learning of image classification fromuser-supplied tags, without filtering. Each tag is supplied by the user whoshared the image online. Enormous numbers of these tags are freely availableonline, and they give insight about the image categories important to users andto image classification. Our approach is complementary to the conventionalapproach of manual annotation, which is extremely costly. We analyze of theFlickr 100 Million Image dataset, making several useful observations about thestatistics of these tags. We introduce a large-scale robust classificationalgorithm, in order to handle the inherent noise in these tags, and acalibration procedure to better predict objective annotations. We show thatfreely available, user-supplied tags can obtain similar or superior results tolarge databases of costly manual annotations.

Segment-Phrase Table for Semantic Segmentation, Visual Entailment and  Paraphrasing

  We introduce Segment-Phrase Table (SPT), a large collection of bijectiveassociations between textual phrases and their corresponding segmentations.Leveraging recent progress in object recognition and natural languagesemantics, we show how we can successfully build a high-quality segment-phrasetable using minimal human supervision. More importantly, we demonstrate theunique value unleashed by this rich bimodal resource, for both vision as wellas natural language understanding. First, we show that fine-grained textuallabels facilitate contextual reasoning that helps in satisfying semanticconstraints across image segments. This feature enables us to achievestate-of-the-art segmentation results on benchmark datasets. Next, we show thatthe association of high-quality segmentations to textual phrases aids in richersemantic understanding and reasoning of these textual phrases. Leveraging thisfeature, we motivate the problem of visual entailment and visual paraphrasing,and demonstrate its utility on a large dataset.

VISALOGY: Answering Visual Analogy Questions

  In this paper, we study the problem of answering visual analogy questions.These questions take the form of image A is to image B as image C is to what.Answering these questions entails discovering the mapping from image A to imageB and then extending the mapping to image C and searching for the image D suchthat the relation from A to B holds for C to D. We pose this problem aslearning an embedding that encourages pairs of analogous images with similartransformations to be close together using convolutional neural networks with aquadruple Siamese architecture. We introduce a dataset of visual analogyquestions in natural images, and show first results of its kind on solvinganalogy questions on natural images.

Newtonian Image Understanding: Unfolding the Dynamics of Objects in  Static Images

  In this paper, we study the challenging problem of predicting the dynamics ofobjects in static images. Given a query object in an image, our goal is toprovide a physical understanding of the object in terms of the forces actingupon it and its long term motion as response to those forces. Direct andexplicit estimation of the forces and the motion of objects from a single imageis extremely challenging. We define intermediate physical abstractions calledNewtonian scenarios and introduce Newtonian Neural Network ($N^3$) that learnsto map a single image to a state in a Newtonian scenario. Our experimentalevaluations show that our method can reliably predict dynamics of a queryobject from a single image. In addition, our approach can provide physicalreasoning that supports the predicted dynamics in terms of velocity and forcevectors. To spur research in this direction we compiled Visual NewtonianDynamics (VIND) dataset that includes 6806 videos aligned with Newtonianscenarios represented using game engines, and 4516 still images with theirground truth dynamics.

Actions ~ Transformations

  What defines an action like "kicking ball"? We argue that the true meaning ofan action lies in the change or transformation an action brings to theenvironment. In this paper, we propose a novel representation for actions bymodeling an action as a transformation which changes the state of theenvironment before the action happens (precondition) to the state after theaction (effect). Motivated by recent advancements of video representation usingdeep learning, we design a Siamese network which models the action as atransformation on a high-level feature space. We show that our model givesimprovements on standard action recognition datasets including UCF101 andHMDB51. More importantly, our approach is able to generalize beyond learnedaction categories and shows significant performance improvement oncross-category generalization on our new ACT dataset.

Toward a Taxonomy and Computational Models of Abnormalities in Images

  The human visual system can spot an abnormal image, and reason about whatmakes it strange. This task has not received enough attention in computervision. In this paper we study various types of atypicalities in images in amore comprehensive way than has been done before. We propose a new dataset ofabnormal images showing a wide range of atypicalities. We design human subjectexperiments to discover a coarse taxonomy of the reasons for abnormality. Ourexperiments reveal three major categories of abnormality: object-centric,scene-centric, and contextual. Based on this taxonomy, we propose acomprehensive computational model that can predict all different types ofabnormality in images and outperform prior arts in abnormality recognition.

Are Elephants Bigger than Butterflies? Reasoning about Sizes of Objects

  Human vision greatly benefits from the information about sizes of objects.The role of size in several visual reasoning tasks has been thoroughly exploredin human perception and cognition. However, the impact of the information aboutsizes of objects is yet to be determined in AI. We postulate that this ismainly attributed to the lack of a comprehensive repository of sizeinformation. In this paper, we introduce a method to automatically infer objectsizes, leveraging visual and textual information from web. By maximizing thejoint likelihood of textual and visual observations, our method learns reliablerelative size estimates, with no explicit human supervision. We introduce therelative size dataset and show that our method outperforms competitive textualand visual baselines in reasoning about size comparisons.

XNOR-Net: ImageNet Classification Using Binary Convolutional Neural  Networks

  We propose two efficient approximations to standard convolutional neuralnetworks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks,the filters are approximated with binary values resulting in 32x memory saving.In XNOR-Networks, both the filters and the input to convolutional layers arebinary. XNOR-Networks approximate convolutions using primarily binaryoperations. This results in 58x faster convolutional operations and 32x memorysavings. XNOR-Nets offer the possibility of running state-of-the-art networkson CPUs (rather than GPUs) in real-time. Our binary networks are simple,accurate, efficient, and work on challenging visual tasks. We evaluate ourapproach on the ImageNet classification task. The classification accuracy witha Binary-Weight-Network version of AlexNet is only 2.9% less than thefull-precision AlexNet (in top-1 measure). We compare our method with recentnetwork binarization methods, BinaryConnect and BinaryNets, and outperformthese methods by large margins on ImageNet, more than 16% in top-1 accuracy.

A Diagram Is Worth A Dozen Images

  Diagrams are common tools for representing complex concepts, relationshipsand events, often when it would be difficult to portray the same informationwith natural images. Understanding natural images has been extensively studiedin computer vision, while diagram understanding has received little attention.In this paper, we study the problem of diagram interpretation and reasoning,the challenging task of identifying the structure of a diagram and thesemantics of its constituents and their relationships. We introduce DiagramParse Graphs (DPG) as our representation to model the structure of diagrams. Wedefine syntactic parsing of diagrams as learning to infer DPGs for diagrams andstudy semantic interpretation and reasoning of diagrams in the context ofdiagram question answering. We devise an LSTM-based method for syntacticparsing of diagrams and introduce a DPG-based attention model for diagramquestion answering. We compile a new dataset of diagrams with exhaustiveannotations of constituents and relationships for over 5,000 diagrams and15,000 questions and answers. Our results show the significance of our modelsfor syntactic parsing and question answering in diagrams using DPGs.

Query-Reduction Networks for Question Answering

  In this paper, we study the problem of question answering when reasoning overmultiple facts is required. We propose Query-Reduction Network (QRN), a variantof Recurrent Neural Network (RNN) that effectively handles both short-term(local) and long-term (global) sequential dependencies to reason over multiplefacts. QRN considers the context sentences as a sequence of state-changingtriggers, and reduces the original query to a more informed query as itobserves each trigger (context sentence) through time. Our experiments showthat QRN produces the state-of-the-art results in bAbI QA and dialog tasks, andin a real goal-oriented dialog dataset. In addition, QRN formulation allowsparallelization on RNN's time axis, saving an order of magnitude in timecomplexity for training and inference.

Bidirectional Attention Flow for Machine Comprehension

  Machine comprehension (MC), answering a query about a given contextparagraph, requires modeling complex interactions between the context and thequery. Recently, attention mechanisms have been successfully extended to MC.Typically these methods use attention to focus on a small portion of thecontext and summarize it with a fixed-size vector, couple attentionstemporally, and/or often form a uni-directional attention. In this paper weintroduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stagehierarchical process that represents the context at different levels ofgranularity and uses bi-directional attention flow mechanism to obtain aquery-aware context representation without early summarization. Ourexperimental evaluations show that our model achieves the state-of-the-artresults in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail clozetest.

LCNN: Lookup-based Convolutional Neural Network

  Porting state of the art deep learning algorithms to resource constrainedcompute platforms (e.g. VR, AR, wearables) is extremely challenging. We proposea fast, compact, and accurate model for convolutional neural networks thatenables efficient learning and inference. We introduce LCNN, a lookup-basedconvolutional neural network that encodes convolutions by few lookups to adictionary that is trained to cover the space of weights in CNNs. Training LCNNinvolves jointly learning a dictionary and a small set of linear combinations.The size of the dictionary naturally traces a spectrum of trade-offs betweenefficiency and accuracy. Our experimental results on ImageNet challenge showthat LCNN can offer 3.2x speedup while achieving 55.1% top-1 accuracy usingAlexNet architecture. Our fastest LCNN offers 37.6x speed up over AlexNet whilemaintaining 44.3% top-1 accuracy. LCNN not only offers dramatic speed ups atinference, but it also enables efficient training. In this paper, we show thebenefits of LCNN in few-shot learning and few-iteration learning, two crucialaspects of on-device training of deep learning models.

Commonly Uncommon: Semantic Sparsity in Situation Recognition

  Semantic sparsity is a common challenge in structured visual classificationproblems; when the output space is complex, the vast majority of the possiblepredictions are rarely, if ever, seen in the training set. This paper studiessemantic sparsity in situation recognition, the task of producing structuredsummaries of what is happening in images, including activities, objects and theroles objects play within the activity. For this problem, we find empiricallythat most object-role combinations are rare, and current state-of-the-artmodels significantly underperform in this sparse data regime. We avoid manysuch errors by (1) introducing a novel tensor composition function that learnsto share examples across role-noun combinations and (2) semantically augmentingour training data with automatically gathered examples of rarely observedoutputs using web data. When integrated within a complete CRF-based structuredprediction model, the tensor-based approach outperforms existing state of theart by a relative improvement of 2.11% and 4.40% on top-5 verb and noun-roleaccuracy, respectively. Adding 5 million images with our semantic augmentationtechniques gives further relative improvements of 6.23% and 9.57% on top-5 verband noun-role accuracy.

Asynchronous Temporal Fields for Action Recognition

  Actions are more than just movements and trajectories: we cook to eat and wehold a cup to drink from it. A thorough understanding of videos requires goingbeyond appearance modeling and necessitates reasoning about the sequence ofactivities, as well as the higher-level constructs such as intentions. But howdo we model and reason about these? We propose a fully-connected temporal CRFmodel for reasoning over various aspects of activities that includes objects,actions, and intentions, where the potentials are predicted by a deep network.End-to-end training of such structured models is a challenging endeavor: Forinference and learning we need to construct mini-batches consisting of wholevideos, leading to mini-batches with only a few videos. This causeshigh-correlation between data points leading to breakdown of the backpropalgorithm. To address this challenge, we present an asynchronous variationalinference method that allows efficient end-to-end training. Our method achievesa classification mAP of 22.4% on the Charades benchmark, outperforming thestate-of-the-art (17.2% mAP), and offers equal gains on the task of temporallocalization.

YOLO9000: Better, Faster, Stronger

  We introduce YOLO9000, a state-of-the-art, real-time object detection systemthat can detect over 9000 object categories. First we propose variousimprovements to the YOLO detection method, both novel and drawn from priorwork. The improved model, YOLOv2, is state-of-the-art on standard detectiontasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods likeFaster RCNN with ResNet and SSD while still running significantly faster.Finally we propose a method to jointly train on object detection andclassification. Using this method we train YOLO9000 simultaneously on the COCOdetection dataset and the ImageNet classification dataset. Our joint trainingallows YOLO9000 to predict detections for object classes that don't havelabelled detection data. We validate our approach on the ImageNet detectiontask. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despiteonly having detection data for 44 of the 200 classes. On the 156 classes not inCOCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes;it predicts detections for more than 9000 different object categories. And itstill runs in real-time.

See the Glass Half Full: Reasoning about Liquid Containers, their Volume  and Content

  Humans have rich understanding of liquid containers and their contents; forexample, we can effortlessly pour water from a pitcher to a cup. Doing sorequires estimating the volume of the cup, approximating the amount of water inthe pitcher, and predicting the behavior of water when we tilt the pitcher.Very little attention in computer vision has been made to liquids and theircontainers. In this paper, we study liquid containers and their contents, andpropose methods to estimate the volume of containers, approximate the amount ofliquid in them, and perform comparative volume estimations all from a singleRGB image. Furthermore, we show the results of the proposed model forpredicting the behavior of liquids inside containers when one tilts thecontainers. We also introduce a new dataset of Containers Of liQuid contEnt(COQE) that contains more than 5,000 images of 10,000 liquid containers incontext labelled with volume, amount of content, bounding box annotation, andcorresponding similar 3D CAD models.

SeGAN: Segmenting and Generating the Invisible

  Objects often occlude each other in scenes; Inferring their appearance beyondtheir visible parts plays an important role in scene understanding, depthestimation, object interaction and manipulation. In this paper, we study thechallenging problem of completing the appearance of occluded objects. Doing sorequires knowing which pixels to paint (segmenting the invisible parts ofobjects) and what color to paint them (generating the invisible parts). Ourproposed novel solution, SeGAN, jointly optimizes for both segmentation andgeneration of the invisible parts of objects. Our experimental results showthat: (a) SeGAN can learn to generate the appearance of the occluded parts ofobjects; (b) SeGAN outperforms state-of-the-art segmentation baselines for theinvisible parts of objects; (c) trained on synthetic photo realistic images,SeGAN can reliably segment natural images; (d) by reasoning about occluderoccludee relations, our method can infer depth layering.

Re3 : Real-Time Recurrent Regression Networks for Visual Tracking of  Generic Objects

  Robust object tracking requires knowledge and understanding of the objectbeing tracked: its appearance, its motion, and how it changes over time. Atracker must be able to modify its underlying model and adapt to newobservations. We present Re3, a real-time deep object tracker capable ofincorporating temporal information into its model. Rather than focusing on alimited set of objects or training a model at test-time to track a specificinstance, we pretrain our generic tracker on a large variety of objects andefficiently update on the fly; Re3 simultaneously tracks and updates theappearance model with a single forward pass. This lightweight model is capableof tracking objects at 150 FPS, while attaining competitive results onchallenging benchmarks. We also show that our method handles temporaryocclusion better than other comparable trackers using experiments that directlymeasure performance on sequences with occlusion.

Visual Semantic Planning using Deep Successor Representations

  A crucial capability of real-world intelligent agents is their ability toplan a sequence of actions to achieve their goals in the visual world. In thiswork, we address the problem of visual semantic planning: the task ofpredicting a sequence of actions from visual observations that transform adynamic environment from an initial state to a goal state. Doing so entailsknowledge about objects and their affordances, as well as actions and theirpreconditions and effects. We propose learning these through interacting with avisual and dynamic environment. Our proposed solution involves bootstrappingreinforcement learning with imitation learning. To ensure cross taskgeneralization, we develop a deep predictive model based on successorrepresentations. Our experimental results show near optimal results across awide range of tasks in the challenging THOR environment.

Neural Speed Reading via Skim-RNN

  Inspired by the principles of speed reading, we introduce Skim-RNN, arecurrent neural network (RNN) that dynamically decides to update only a smallfraction of the hidden state for relatively unimportant input tokens. Skim-RNNgives computational advantage over an RNN that always updates the entire hiddenstate. Skim-RNN uses the same input and output interfaces as a standard RNN andcan be easily used instead of RNNs in existing models. In our experiments, weshow that Skim-RNN can achieve significantly reduced computational cost withoutlosing accuracy compared to standard RNNs across five different naturallanguage tasks. In addition, we demonstrate that the trade-off between accuracyand speed of Skim-RNN can be dynamically controlled during inference time in astable manner. Our analysis also shows that Skim-RNN running on a single CPUoffers lower latency compared to standard RNNs on GPUs.

AI2-THOR: An Interactive 3D Environment for Visual AI

  We introduce The House Of inteRactions (THOR), a framework for visual AIresearch, available at http://ai2thor.allenai.org. AI2-THOR consists of nearphoto-realistic 3D indoor scenes, where AI agents can navigate in the scenesand interact with objects to perform tasks. AI2-THOR enables research in manydifferent domains including but not limited to deep reinforcement learning,imitation learning, learning by interaction, planning, visual questionanswering, unsupervised representation learning, object detection andsegmentation, and learning models of cognition. The goal of AI2-THOR is tofacilitate building visually intelligent models and push the research forwardin this domain.

Who Let The Dogs Out? Modeling Dog Behavior From Visual Data

  We introduce the task of directly modeling a visually intelligent agent.Computer vision typically focuses on solving various subtasks related to visualintelligence. We depart from this standard approach to computer vision; insteadwe directly model a visually intelligent agent. Our model takes visualinformation as input and directly predicts the actions of the agent. Towardthis end we introduce DECADE, a large-scale dataset of ego-centric videos froma dog's perspective as well as her corresponding movements. Using this data wemodel how the dog acts and how the dog plans her movements. We show under avariety of metrics that given just visual input we can successfully model thisintelligent agent in many situations. Moreover, the representation learned byour model encodes distinct information compared to representations trained onimage classification, and our learned representation can generalize to otherdomains. In particular, we show strong results on the task of walkable surfaceestimation by using this dog modeling task as representation learning.

DOCK: Detecting Objects by transferring Common-sense Knowledge

  We present a scalable approach for Detecting Objects by transferringCommon-sense Knowledge (DOCK) from source to target categories. In our setting,the training data for the source categories have bounding box annotations,while those for the target categories only have image-level annotations.Current state-of-the-art approaches focus on image-level visual or semanticsimilarity to adapt a detector trained on the source categories to the newtarget categories. In contrast, our key idea is to (i) use similarity not atthe image-level, but rather at the region-level, and (ii) leverage richercommon-sense (based on attribute, spatial, etc.) to guide the algorithm towardslearning the correct detections. We acquire such common-sense cuesautomatically from readily-available knowledge bases without any extra humaneffort. On the challenging MS COCO dataset, we find that common-sense knowledgecan substantially improve detection performance over existing transfer-learningbaselines.

Imagine This! Scripts to Compositions to Videos

  Imagining a scene described in natural language with realistic layout andappearance of entities is the ultimate test of spatial, visual, and semanticworld knowledge. Towards this goal, we present the Composition, Retrieval, andFusion Network (CRAFT), a model capable of learning this knowledge fromvideo-caption data and applying it while generating videos from novel captions.CRAFT explicitly predicts a temporal-layout of mentioned entities (charactersand objects), retrieves spatio-temporal entity segments from a video databaseand fuses them to generate scene videos. Our contributions include sequentialtraining of components of CRAFT while jointly modeling layout and appearances,and losses that encourage learning compositional representations for retrieval.We evaluate CRAFT on semantic fidelity to caption, composition consistency, andvisual quality. CRAFT outperforms direct pixel generation approaches andgeneralizes well to unseen captions and to unseen video databases with no textannotations. We demonstrate CRAFT on FLINTSTONES, a new richly annotatedvideo-caption dataset with over 25000 videos. For a glimpse of videos generatedby CRAFT, see https://youtu.be/688Vv86n0z8.

Phrase-Indexed Question Answering: A New Challenge for Scalable Document  Comprehension

  We formalize a new modular variant of current question answering tasks byenforcing complete independence of the document encoder from the questionencoder. This formulation addresses a key challenge in machine comprehension byrequiring a standalone representation of the document discourse. Itadditionally leads to a significant scalability advantage since the encoding ofthe answer candidate phrases in the document can be pre-computed and indexedoffline for efficient retrieval. We experiment with baseline models for the newtask, which achieve a reasonable accuracy but significantly underperformunconstrained QA models. We invite the QA research community to engage inPhrase-Indexed Question Answering (PIQA, pika) for closing the gap. Theleaderboard is at: nlp.cs.washington.edu/piqa

Charades-Ego: A Large-Scale Dataset of Paired Third and First Person  Videos

  In Actor and Observer we introduced a dataset linking the first andthird-person video understanding domains, the Charades-Ego Dataset. In thispaper we describe the egocentric aspect of the dataset and present annotationsfor Charades-Ego with 68,536 activity instances in 68.8 hours of first andthird-person video, making it one of the largest and most diverse egocentricdatasets available. Charades-Ego furthermore shares activity classes, scripts,and methodology with the Charades dataset, that consist of additional 82.3hours of third-person video with 66,500 activity instances. Charades-Ego hastemporal annotations and textual descriptions, making it suitable foregocentric video classification, localization, captioning, and new tasksutilizing the cross-modal nature of the data.

Actor and Observer: Joint Modeling of First and Third-Person Videos

  Several theories in cognitive neuroscience suggest that when people interactwith the world, or simulate interactions, they do so from a first-personegocentric perspective, and seamlessly transfer knowledge between third-person(observer) and first-person (actor). Despite this, learning such models forhuman action recognition has not been achievable due to the lack of data. Thispaper takes a step in this direction, with the introduction of Charades-Ego, alarge-scale dataset of paired first-person and third-person videos, involving112 people, with 4000 paired videos. This enables learning the link between thetwo, actor and observer perspectives. Thereby, we address one of the biggestbottlenecks facing egocentric vision research, providing a link fromfirst-person to the abundant third-person data on the web. We use this data tolearn a joint representation of first and third-person videos, with only weaksupervision, and show its effectiveness for transferring knowledge from thethird-person to the first-person domain.

Label Refinery: Improving ImageNet Classification through Label  Progression

  Among the three main components (data, labels, and models) of any supervisedlearning system, data and models have been the main subjects of activeresearch. However, studying labels and their properties has received verylittle attention. Current principles and paradigms of labeling impose severalchallenges to machine learning algorithms. Labels are often incomplete,ambiguous, and redundant. In this paper we study the effects of variousproperties of labels and introduce the Label Refinery: an iterative procedurethat updates the ground truth labels after examining the entire dataset. Weshow significant gain using refined labels across a wide range of models. Usinga Label Refinery improves the state-of-the-art top-1 accuracy of (1) AlexNetfrom 59.3 to 67.2, (2) MobileNet from 70.6 to 73.39, (3) MobileNet-0.25 from50.6 to 55.59, (4) VGG19 from 72.7 to 75.46, and (5) Darknet19 from 72.9 to74.47.

PhotoShape: Photorealistic Materials for Large-Scale Shape Collections

  Existing online 3D shape repositories contain thousands of 3D models but lackphotorealistic appearance. We present an approach to automatically assignhigh-quality, realistic appearance models to large scale 3D shape collections.The key idea is to jointly leverage three types of online data -- shapecollections, material collections, and photo collections, using the photos asreference to guide assignment of materials to shapes. By generating a largenumber of synthetic renderings, we train a convolutional neural network toclassify materials in real photos, and employ 3D-2D alignment techniques totransfer materials to different parts of each shape model. Our system producesphotorealistic, relightable, 3D shapes (PhotoShapes).

Visual Semantic Navigation using Scene Priors

  How do humans navigate to target objects in novel scenes? Do we use thesemantic/functional priors we have built over years to efficiently search andnavigate? For example, to search for mugs, we search cabinets near the coffeemachine and for fruits we try the fridge. In this work, we focus onincorporating semantic priors in the task of semantic navigation. We propose touse Graph Convolutional Networks for incorporating the prior knowledge into adeep reinforcement learning framework. The agent uses the features from theknowledge graph to predict the actions. For evaluation, we use the AI2-THORframework. Our experiments show how semantic knowledge improves performancesignificantly. More importantly, we show improvement in generalization tounseen scenes and/or objects. The supplementary video can be accessed at thefollowing link: https://youtu.be/otKjuO805dE .

ELASTIC: Improving CNNs with Dynamic Scaling Policies

  Scale variation has been a challenge from traditional to modern approaches incomputer vision. Most solutions to scale issues have a similar theme: a set ofintuitive and manually designed policies that are generic and fixed (e.g. SIFTor feature pyramid). We argue that the scaling policy should be learned fromdata. In this paper, we introduce ELASTIC, a simple, efficient and yet veryeffective approach to learn a dynamic scale policy from data. We formulate thescaling policy as a non-linear function inside the network's structure that (a)is learned from data, (b) is instance specific, (c) does not add extracomputation, and (d) can be applied on any network architecture. We appliedELASTIC to several state-of-the-art network architectures and showed consistentimprovement without extra (sometimes even lower) computation on ImageNetclassification, MSCOCO multi-label classification, and PASCAL VOC semanticsegmentation. Our results show major improvement for images with scalechallenges. Our code is available here: https://github.com/allenai/elastic

Two Body Problem: Collaborative Visual Task Completion

  Collaboration is a necessary skill to perform tasks that are beyond oneagent's capabilities. Addressed extensively in both conventional and modern AI,multi-agent collaboration has often been studied in the context of simple gridworlds. We argue that there are inherently visual aspects to collaborationwhich should be studied in visually rich environments. A key element incollaboration is communication that can be either explicit, through messages,or implicit, through perception of the other agents and the visual world.Learning to collaborate in a visual environment entails learning (1) to performthe task, (2) when and what to communicate, and (3) how to act based on thesecommunications and the perception of the visual world. In this paper we studythe problem of learning to collaborate directly from pixels in AI2-THOR anddemonstrate the benefits of explicit and implicit modes of communication toperform visual tasks. Refer to our project page for more details:https://prior.allenai.org/projects/two-body-problem

You Only Look Once: Unified, Real-Time Object Detection

  We present YOLO, a new approach to object detection. Prior work on objectdetection repurposes classifiers to perform detection. Instead, we frame objectdetection as a regression problem to spatially separated bounding boxes andassociated class probabilities. A single neural network predicts bounding boxesand class probabilities directly from full images in one evaluation. Since thewhole detection pipeline is a single network, it can be optimized end-to-enddirectly on detection performance.  Our unified architecture is extremely fast. Our base YOLO model processesimages in real-time at 45 frames per second. A smaller version of the network,Fast YOLO, processes an astounding 155 frames per second while still achievingdouble the mAP of other real-time detectors. Compared to state-of-the-artdetection systems, YOLO makes more localization errors but is far less likelyto predict false detections where nothing exists. Finally, YOLO learns verygeneral representations of objects. It outperforms all other detection methods,including DPM and R-CNN, by a wide margin when generalizing from natural imagesto artwork on both the Picasso Dataset and the People-Art Dataset.

"What happens if..." Learning to Predict the Effect of Forces in Images

  What happens if one pushes a cup sitting on a table toward the edge of thetable? How about pushing a desk against a wall? In this paper, we study theproblem of understanding the movements of objects as a result of applyingexternal forces to them. For a given force vector applied to a specificlocation in an image, our goal is to predict long-term sequential movementscaused by that force. Doing so entails reasoning about scene geometry, objects,their attributes, and the physical rules that govern the movements of objects.We design a deep neural network model that learns long-term sequentialdependencies of object movements while taking into account the geometry andappearance of the scene by combining Convolutional and Recurrent NeuralNetworks. Training our model requires a large-scale dataset of object movementscaused by external forces. To build a dataset of forces in scenes, wereconstructed all images in SUN RGB-D dataset in a physics simulator toestimate the physical movements of objects caused by external forces applied tothem. Our Forces in Scenes (ForScene) dataset contains 10,335 images in which avariety of external forces are applied to different types of objects resultingin more than 65,000 object movements represented in 3D. Our experimentalevaluations show that the challenging task of predicting long-term movements ofobjects as their reaction to external forces is possible from a single image.

Hollywood in Homes: Crowdsourcing Data Collection for Activity  Understanding

  Computer vision has a great potential to help our daily lives by searchingfor lost keys, watering flowers or reminding us to take a pill. To succeed withsuch tasks, computer vision methods need to be trained from real and diverseexamples of our daily dynamic scenes. While most of such scenes are notparticularly exciting, they typically do not appear on YouTube, in movies or TVbroadcasts. So how do we collect sufficiently many diverse but boring samplesrepresenting our lives? We propose a novel Hollywood in Homes approach tocollect such data. Instead of shooting videos in the lab, we ensure diversityby distributing and crowdsourcing the whole process of video creation fromscript writing to video recording and annotation. Following this procedure wecollect a new dataset, Charades, with hundreds of people recording videos intheir own homes, acting out casual everyday activities. The dataset is composedof 9,848 annotated videos with an average length of 30 seconds, showingactivities of 267 people from three continents. Each video is annotated bymultiple free-text descriptions, action labels, action intervals and classes ofinteracted objects. In total, Charades provides 27,847 video descriptions,66,500 temporally localized intervals for 157 action classes and 41,104 labelsfor 46 object classes. Using this rich data, we evaluate and provide baselineresults for several tasks including action recognition and automaticdescription generation. We believe that the realism, diversity, and casualnature of this dataset will present unique challenges and new opportunities forcomputer vision community.

Much Ado About Time: Exhaustive Annotation of Temporal Data

  Large-scale annotated datasets allow AI systems to learn from and build uponthe knowledge of the crowd. Many crowdsourcing techniques have been developedfor collecting image annotations. These techniques often implicitly rely on thefact that a new input image takes a negligible amount of time to perceive. Incontrast, we investigate and determine the most cost-effective way of obtaininghigh-quality multi-label annotations for temporal data such as videos. Watchingeven a short 30-second video clip requires a significant time investment from acrowd worker; thus, requesting multiple annotations following a single viewingis an important cost-saving strategy. But how many questions should we ask pervideo? We conclude that the optimal strategy is to ask as many questions aspossible in a HIT (up to 52 binary questions after watching a 30-second videoclip in our experiments). We demonstrate that while workers may not correctlyanswer all questions, the cost-benefit analysis nevertheless favors consensusfrom multiple such cheap-yet-imperfect iterations over more complexalternatives. When compared with a one-question-per-video baseline, our methodis able to achieve a 10% improvement in recall 76.7% ours versus 66.7%baseline) at comparable precision (83.8% ours versus 83.0% baseline) in abouthalf the annotation time (3.8 minutes ours compared to 7.1 minutes baseline).We demonstrate the effectiveness of our method by collecting multi-labelannotations of 157 human activities on 1,815 videos.

Target-driven Visual Navigation in Indoor Scenes using Deep  Reinforcement Learning

  Two less addressed issues of deep reinforcement learning are (1) lack ofgeneralization capability to new target goals, and (2) data inefficiency i.e.,the model requires several (and often costly) episodes of trial and error toconverge, which makes it impractical to be applied to real-world scenarios. Inthis paper, we address these two issues and apply our model to the task oftarget-driven visual navigation. To address the first issue, we propose anactor-critic model whose policy is a function of the goal as well as thecurrent state, which allows to better generalize. To address the second issue,we propose AI2-THOR framework, which provides an environment with high-quality3D scenes and physics engine. Our framework enables agents to take actions andinteract with objects. Hence, we can collect a huge number of training samplesefficiently.  We show that our proposed method (1) converges faster than thestate-of-the-art deep reinforcement learning methods, (2) generalizes acrosstargets and across scenes, (3) generalizes to a real robot scenario with asmall amount of fine-tuning (although the model is trained in simulation), (4)is end-to-end trainable and does not need feature engineering, feature matchingbetween frames or 3D reconstruction of the environment.  The supplementary video can be accessed at the following link:https://youtu.be/SmBxMDiOrvs.

AJILE Movement Prediction: Multimodal Deep Learning for Natural Human  Neural Recordings and Video

  Developing useful interfaces between brains and machines is a grand challengeof neuroengineering. An effective interface has the capacity to not onlyinterpret neural signals, but predict the intentions of the human to perform anaction in the near future; prediction is made even more challenging outsidewell-controlled laboratory experiments. This paper describes our approach todetect and to predict natural human arm movements in the future, a keychallenge in brain computer interfacing that has never before been attempted.We introduce the novel Annotated Joints in Long-term ECoG (AJILE) dataset;AJILE includes automatically annotated poses of 7 upper body joints for fourhuman subjects over 670 total hours (more than 72 million frames), along withthe corresponding simultaneously acquired intracranial neural recordings. Thesize and scope of AJILE greatly exceeds all previous datasets with movementsand electrocorticography (ECoG), making it possible to take a deep learningapproach to movement prediction. We propose a multimodal model that combinesdeep convolutional neural networks (CNN) with long short-term memory (LSTM)blocks, leveraging both ECoG and video modalities. We demonstrate that ourmodels are able to detect movements and predict future movements up to 800 msecbefore movement initiation. Further, our multimodal movement prediction modelsexhibit resilience to simulated ablation of input neural signals. We believe amultimodal approach to natural neural decoding that takes context into accountis critical in advancing bioelectronic technologies and human neuroscience.

Structured Set Matching Networks for One-Shot Part Labeling

  Diagrams often depict complex phenomena and serve as a good test bed forvisual and textual reasoning. However, understanding diagrams using naturalimage understanding approaches requires large training datasets of diagrams,which are very hard to obtain. Instead, this can be addressed as a matchingproblem either between labeled diagrams, images or both. This problem is verychallenging since the absence of significant color and texture renders localcues ambiguous and requires global reasoning. We consider the problem ofone-shot part labeling: labeling multiple parts of an object in a target imagegiven only a single source image of that category. For this set-to-set matchingproblem, we introduce the Structured Set Matching Network (SSMN), a structuredprediction model that incorporates convolutional neural networks. The SSMN istrained using global normalization to maximize local match scores betweencorresponding elements and a global consistency score among all matchedelements, while also enforcing a matching constraint between the two sets. TheSSMN significantly outperforms several strong baselines on three label transferscenarios: diagram-to-diagram, evaluated on a new diagram dataset of over 200categories; image-to-image, evaluated on a dataset built on top of the PascalPart Dataset; and image-to-diagram, evaluated on transferring labels acrossthese datasets.

IQA: Visual Question Answering in Interactive Environments

  We introduce Interactive Question Answering (IQA), the task of answeringquestions that require an autonomous agent to interact with a dynamic visualenvironment. IQA presents the agent with a scene and a question, like: "Arethere any apples in the fridge?" The agent must navigate around the scene,acquire visual understanding of scene elements, interact with objects (e.g.open refrigerators) and plan for a series of actions conditioned on thequestion. Popular reinforcement learning approaches with a single controllerperform poorly on IQA owing to the large and diverse state space. We proposethe Hierarchical Interactive Memory Network (HIMN), consisting of a factorizedset of controllers, allowing the system to operate at multiple levels oftemporal abstraction. To evaluate HIMN, we introduce IQUAD V1, a new datasetbuilt upon AI2-THOR, a simulated photo-realistic environment of configurableindoor scenes with interactive objects (code and dataset available athttps://github.com/danielgordon10/thor-iqa-cvpr-2018). IQUAD V1 has 75,000questions, each paired with a unique scene configuration. Our experiments showthat our proposed model outperforms popular single controller based methods onIQUAD V1. For sample questions and results, please view our video:https://youtu.be/pXd3C-1jr98

From Recognition to Cognition: Visual Commonsense Reasoning

  Visual understanding goes well beyond object recognition. With one glance atan image, we can effortlessly imagine the world beyond the pixels: forinstance, we can infer people's actions, goals, and mental states. While thistask is easy for humans, it is tremendously difficult for today's visionsystems, requiring higher-order cognition and commonsense reasoning about theworld. We formalize this task as Visual Commonsense Reasoning. Given achallenging question about an image, a machine must answer correctly and thenprovide a rationale justifying its answer.  Next, we introduce a new dataset, VCR, consisting of 290k multiple choice QAproblems derived from 110k movie scenes. The key recipe for generatingnon-trivial and high-quality problems at scale is Adversarial Matching, a newapproach to transform rich annotations into multiple choice questions withminimal bias. Experimental results show that while humans find VCR easy (over90% accuracy), state-of-the-art vision models struggle (~45%).  To move towards cognition-level understanding, we present a new reasoningengine, Recognition to Cognition Networks (R2C), that models the necessarylayered inferences for grounding, contextualization, and reasoning. R2C helpsnarrow the gap between humans and machines (~65%); still, the challenge is farfrom solved, and we provide analysis that suggests avenues for future work.

Learning to Learn How to Learn: Self-Adaptive Visual Navigation Using  Meta-Learning

  Learning is an inherently continuous phenomenon. When humans learn a new taskthere is no explicit distinction between training and inference. As we learn atask, we keep learning about it while performing the task. What we learn andhow we learn it varies during different stages of learning. Learning how tolearn and adapt is a key property that enables us to generalize effortlessly tonew settings. This is in contrast with conventional settings in machinelearning where a trained model is frozen during inference. In this paper westudy the problem of learning to learn at both training and test time in thecontext of visual navigation. A fundamental challenge in navigation isgeneralization to unseen scenes. In this paper we propose a self-adaptivevisual navigation method (SAVN) which learns to adapt to new environmentswithout any explicit supervision. Our solution is a meta-reinforcement learningapproach where an agent learns a self-supervised interaction loss thatencourages effective navigation. Our experiments, performed in the AI2-THORframework, show major improvements in both success rate and SPL for visualnavigation in novel scenes. Our code and data are available at:https://github.com/allenai/savn .

Video Relationship Reasoning using Gated Spatio-Temporal Energy Graph

  Visual relationship reasoning is a crucial yet challenging task forunderstanding rich interactions across visual concepts. For example, arelationship 'man, open, door' involves a complex relation 'open' betweenconcrete entities 'man, door'. While much of the existing work has studied thisproblem in the context of still images, understanding visual relationships invideos has received limited attention. Due to their temporal nature, videosenable us to model and reason about a more comprehensive set of visualrelationships, such as those requiring multiple (temporal) observations (e.g.,'man, lift up, box' vs. 'man, put down, box'), as well as relationships thatare often correlated through time (e.g., 'woman, pay, money' followed by'woman, buy, coffee'). In this paper, we construct a Conditional Random Fieldon a fully-connected spatio-temporal graph that exploits the statisticaldependency between relational entities spatially and temporally. We introduce anovel gated energy function parametrization that learns adaptive relationsconditioned on visual observations. Our model optimization is computationallyefficient, and its space computation complexity is significantly amortizedthrough our proposed parameterization. Experimental results on benchmark videodatasets (ImageNet Video and Charades) demonstrate state-of-the-art performanceacross three standard relationship reasoning tasks: Detection, Tagging, andRecognition.

