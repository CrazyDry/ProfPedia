Unsupervised Deep Embedding for Clustering Analysis

  Clustering is central to many data-driven application domains and has been
studied extensively in terms of distance functions and grouping algorithms.
Relatively little work has focused on learning representations for clustering.
In this paper, we propose Deep Embedded Clustering (DEC), a method that
simultaneously learns feature representations and cluster assignments using
deep neural networks. DEC learns a mapping from the data space to a
lower-dimensional feature space in which it iteratively optimizes a clustering
objective. Our experimental evaluations on image and text corpora show
significant improvement over state-of-the-art methods.


Abnormal Object Recognition: A Comprehensive Study

  When describing images, humans tend not to talk about the obvious, but rather
mention what they find interesting. We argue that abnormalities and deviations
from typicalities are among the most important components that form what is
worth mentioning. In this paper we introduce the abnormality detection as a
recognition problem and show how to model typicalities and, consequently,
meaningful deviations from prototypical properties of categories. Our model can
recognize abnormalities and report the main reasons of any recognized
abnormality. We introduce the abnormality detection dataset and show
interesting results on how to reason about abnormalities.


Deep3D: Fully Automatic 2D-to-3D Video Conversion with Deep
  Convolutional Neural Networks

  As 3D movie viewing becomes mainstream and Virtual Reality (VR) market
emerges, the demand for 3D contents is growing rapidly. Producing 3D videos,
however, remains challenging. In this paper we propose to use deep neural
networks for automatically converting 2D videos and images to stereoscopic 3D
format. In contrast to previous automatic 2D-to-3D conversion algorithms, which
have separate stages and need ground truth depth map as supervision, our
approach is trained end-to-end directly on stereo pairs extracted from 3D
movies. This novel training scheme makes it possible to exploit orders of
magnitude more data and significantly increases performance. Indeed, Deep3D
outperforms baselines in both quantitative and human subject evaluations.


YOLOv3: An Incremental Improvement

  We present some updates to YOLO! We made a bunch of little design changes to
make it better. We also trained this new network that's pretty swell. It's a
little bigger than last time but more accurate. It's still fast though, don't
worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but
three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3
is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5
mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always,
all the code is online at https://pjreddie.com/yolo/


What Should I Do Now? Marrying Reinforcement Learning and Symbolic
  Planning

  Long-term planning poses a major difficulty to many reinforcement learning
algorithms. This problem becomes even more pronounced in dynamic visual
environments. In this work we propose Hierarchical Planning and Reinforcement
Learning (HIP-RL), a method for merging the benefits and capabilities of
Symbolic Planning with the learning abilities of Deep Reinforcement Learning.
We apply HIPRL to the complex visual tasks of interactive question answering
and visual semantic planning and achieve state-of-the-art results on three
challenging datasets all while taking fewer steps at test time and training in
fewer iterations. Sample results can be found at youtu.be/0TtWJ_0mPfI


Semantic Understanding of Professional Soccer Commentaries

  This paper presents a novel approach to the problem of semantic parsing via
learning the correspondences between complex sentences and rich sets of events.
Our main intuition is that correct correspondences tend to occur more
frequently. Our model benefits from a discriminative notion of similarity to
learn the correspondence between sentence and an event and a ranking machinery
that scores the popularity of each correspondence. Our method can discover a
group of events (called macro-events) that best describes a sentence. We
evaluate our method on our novel dataset of professional soccer commentaries.
The empirical results show that our method significantly outperforms the
state-of-theart.


Actions ~ Transformations

  What defines an action like "kicking ball"? We argue that the true meaning of
an action lies in the change or transformation an action brings to the
environment. In this paper, we propose a novel representation for actions by
modeling an action as a transformation which changes the state of the
environment before the action happens (precondition) to the state after the
action (effect). Motivated by recent advancements of video representation using
deep learning, we design a Siamese network which models the action as a
transformation on a high-level feature space. We show that our model gives
improvements on standard action recognition datasets including UCF101 and
HMDB51. More importantly, our approach is able to generalize beyond learned
action categories and shows significant performance improvement on
cross-category generalization on our new ACT dataset.


Toward a Taxonomy and Computational Models of Abnormalities in Images

  The human visual system can spot an abnormal image, and reason about what
makes it strange. This task has not received enough attention in computer
vision. In this paper we study various types of atypicalities in images in a
more comprehensive way than has been done before. We propose a new dataset of
abnormal images showing a wide range of atypicalities. We design human subject
experiments to discover a coarse taxonomy of the reasons for abnormality. Our
experiments reveal three major categories of abnormality: object-centric,
scene-centric, and contextual. Based on this taxonomy, we propose a
comprehensive computational model that can predict all different types of
abnormality in images and outperform prior arts in abnormality recognition.


Are Elephants Bigger than Butterflies? Reasoning about Sizes of Objects

  Human vision greatly benefits from the information about sizes of objects.
The role of size in several visual reasoning tasks has been thoroughly explored
in human perception and cognition. However, the impact of the information about
sizes of objects is yet to be determined in AI. We postulate that this is
mainly attributed to the lack of a comprehensive repository of size
information. In this paper, we introduce a method to automatically infer object
sizes, leveraging visual and textual information from web. By maximizing the
joint likelihood of textual and visual observations, our method learns reliable
relative size estimates, with no explicit human supervision. We introduce the
relative size dataset and show that our method outperforms competitive textual
and visual baselines in reasoning about size comparisons.


Query-Reduction Networks for Question Answering

  In this paper, we study the problem of question answering when reasoning over
multiple facts is required. We propose Query-Reduction Network (QRN), a variant
of Recurrent Neural Network (RNN) that effectively handles both short-term
(local) and long-term (global) sequential dependencies to reason over multiple
facts. QRN considers the context sentences as a sequence of state-changing
triggers, and reduces the original query to a more informed query as it
observes each trigger (context sentence) through time. Our experiments show
that QRN produces the state-of-the-art results in bAbI QA and dialog tasks, and
in a real goal-oriented dialog dataset. In addition, QRN formulation allows
parallelization on RNN's time axis, saving an order of magnitude in time
complexity for training and inference.


Newtonian Image Understanding: Unfolding the Dynamics of Objects in
  Static Images

  In this paper, we study the challenging problem of predicting the dynamics of
objects in static images. Given a query object in an image, our goal is to
provide a physical understanding of the object in terms of the forces acting
upon it and its long term motion as response to those forces. Direct and
explicit estimation of the forces and the motion of objects from a single image
is extremely challenging. We define intermediate physical abstractions called
Newtonian scenarios and introduce Newtonian Neural Network ($N^3$) that learns
to map a single image to a state in a Newtonian scenario. Our experimental
evaluations show that our method can reliably predict dynamics of a query
object from a single image. In addition, our approach can provide physical
reasoning that supports the predicted dynamics in terms of velocity and force
vectors. To spur research in this direction we compiled Visual Newtonian
Dynamics (VIND) dataset that includes 6806 videos aligned with Newtonian
scenarios represented using game engines, and 4516 still images with their
ground truth dynamics.


XNOR-Net: ImageNet Classification Using Binary Convolutional Neural
  Networks

  We propose two efficient approximations to standard convolutional neural
networks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks,
the filters are approximated with binary values resulting in 32x memory saving.
In XNOR-Networks, both the filters and the input to convolutional layers are
binary. XNOR-Networks approximate convolutions using primarily binary
operations. This results in 58x faster convolutional operations and 32x memory
savings. XNOR-Nets offer the possibility of running state-of-the-art networks
on CPUs (rather than GPUs) in real-time. Our binary networks are simple,
accurate, efficient, and work on challenging visual tasks. We evaluate our
approach on the ImageNet classification task. The classification accuracy with
a Binary-Weight-Network version of AlexNet is only 2.9% less than the
full-precision AlexNet (in top-1 measure). We compare our method with recent
network binarization methods, BinaryConnect and BinaryNets, and outperform
these methods by large margins on ImageNet, more than 16% in top-1 accuracy.


A Diagram Is Worth A Dozen Images

  Diagrams are common tools for representing complex concepts, relationships
and events, often when it would be difficult to portray the same information
with natural images. Understanding natural images has been extensively studied
in computer vision, while diagram understanding has received little attention.
In this paper, we study the problem of diagram interpretation and reasoning,
the challenging task of identifying the structure of a diagram and the
semantics of its constituents and their relationships. We introduce Diagram
Parse Graphs (DPG) as our representation to model the structure of diagrams. We
define syntactic parsing of diagrams as learning to infer DPGs for diagrams and
study semantic interpretation and reasoning of diagrams in the context of
diagram question answering. We devise an LSTM-based method for syntactic
parsing of diagrams and introduce a DPG-based attention model for diagram
question answering. We compile a new dataset of diagrams with exhaustive
annotations of constituents and relationships for over 5,000 diagrams and
15,000 questions and answers. Our results show the significance of our models
for syntactic parsing and question answering in diagrams using DPGs.


Image Classification and Retrieval from User-Supplied Tags

  This paper proposes direct learning of image classification from
user-supplied tags, without filtering. Each tag is supplied by the user who
shared the image online. Enormous numbers of these tags are freely available
online, and they give insight about the image categories important to users and
to image classification. Our approach is complementary to the conventional
approach of manual annotation, which is extremely costly. We analyze of the
Flickr 100 Million Image dataset, making several useful observations about the
statistics of these tags. We introduce a large-scale robust classification
algorithm, in order to handle the inherent noise in these tags, and a
calibration procedure to better predict objective annotations. We show that
freely available, user-supplied tags can obtain similar or superior results to
large databases of costly manual annotations.


Segment-Phrase Table for Semantic Segmentation, Visual Entailment and
  Paraphrasing

  We introduce Segment-Phrase Table (SPT), a large collection of bijective
associations between textual phrases and their corresponding segmentations.
Leveraging recent progress in object recognition and natural language
semantics, we show how we can successfully build a high-quality segment-phrase
table using minimal human supervision. More importantly, we demonstrate the
unique value unleashed by this rich bimodal resource, for both vision as well
as natural language understanding. First, we show that fine-grained textual
labels facilitate contextual reasoning that helps in satisfying semantic
constraints across image segments. This feature enables us to achieve
state-of-the-art segmentation results on benchmark datasets. Next, we show that
the association of high-quality segmentations to textual phrases aids in richer
semantic understanding and reasoning of these textual phrases. Leveraging this
feature, we motivate the problem of visual entailment and visual paraphrasing,
and demonstrate its utility on a large dataset.


VISALOGY: Answering Visual Analogy Questions

  In this paper, we study the problem of answering visual analogy questions.
These questions take the form of image A is to image B as image C is to what.
Answering these questions entails discovering the mapping from image A to image
B and then extending the mapping to image C and searching for the image D such
that the relation from A to B holds for C to D. We pose this problem as
learning an embedding that encourages pairs of analogous images with similar
transformations to be close together using convolutional neural networks with a
quadruple Siamese architecture. We introduce a dataset of visual analogy
questions in natural images, and show first results of its kind on solving
analogy questions on natural images.


Bidirectional Attention Flow for Machine Comprehension

  Machine comprehension (MC), answering a query about a given context
paragraph, requires modeling complex interactions between the context and the
query. Recently, attention mechanisms have been successfully extended to MC.
Typically these methods use attention to focus on a small portion of the
context and summarize it with a fixed-size vector, couple attentions
temporally, and/or often form a uni-directional attention. In this paper we
introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage
hierarchical process that represents the context at different levels of
granularity and uses bi-directional attention flow mechanism to obtain a
query-aware context representation without early summarization. Our
experimental evaluations show that our model achieves the state-of-the-art
results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze
test.


LCNN: Lookup-based Convolutional Neural Network

  Porting state of the art deep learning algorithms to resource constrained
compute platforms (e.g. VR, AR, wearables) is extremely challenging. We propose
a fast, compact, and accurate model for convolutional neural networks that
enables efficient learning and inference. We introduce LCNN, a lookup-based
convolutional neural network that encodes convolutions by few lookups to a
dictionary that is trained to cover the space of weights in CNNs. Training LCNN
involves jointly learning a dictionary and a small set of linear combinations.
The size of the dictionary naturally traces a spectrum of trade-offs between
efficiency and accuracy. Our experimental results on ImageNet challenge show
that LCNN can offer 3.2x speedup while achieving 55.1% top-1 accuracy using
AlexNet architecture. Our fastest LCNN offers 37.6x speed up over AlexNet while
maintaining 44.3% top-1 accuracy. LCNN not only offers dramatic speed ups at
inference, but it also enables efficient training. In this paper, we show the
benefits of LCNN in few-shot learning and few-iteration learning, two crucial
aspects of on-device training of deep learning models.


Commonly Uncommon: Semantic Sparsity in Situation Recognition

  Semantic sparsity is a common challenge in structured visual classification
problems; when the output space is complex, the vast majority of the possible
predictions are rarely, if ever, seen in the training set. This paper studies
semantic sparsity in situation recognition, the task of producing structured
summaries of what is happening in images, including activities, objects and the
roles objects play within the activity. For this problem, we find empirically
that most object-role combinations are rare, and current state-of-the-art
models significantly underperform in this sparse data regime. We avoid many
such errors by (1) introducing a novel tensor composition function that learns
to share examples across role-noun combinations and (2) semantically augmenting
our training data with automatically gathered examples of rarely observed
outputs using web data. When integrated within a complete CRF-based structured
prediction model, the tensor-based approach outperforms existing state of the
art by a relative improvement of 2.11% and 4.40% on top-5 verb and noun-role
accuracy, respectively. Adding 5 million images with our semantic augmentation
techniques gives further relative improvements of 6.23% and 9.57% on top-5 verb
and noun-role accuracy.


Asynchronous Temporal Fields for Action Recognition

  Actions are more than just movements and trajectories: we cook to eat and we
hold a cup to drink from it. A thorough understanding of videos requires going
beyond appearance modeling and necessitates reasoning about the sequence of
activities, as well as the higher-level constructs such as intentions. But how
do we model and reason about these? We propose a fully-connected temporal CRF
model for reasoning over various aspects of activities that includes objects,
actions, and intentions, where the potentials are predicted by a deep network.
End-to-end training of such structured models is a challenging endeavor: For
inference and learning we need to construct mini-batches consisting of whole
videos, leading to mini-batches with only a few videos. This causes
high-correlation between data points leading to breakdown of the backprop
algorithm. To address this challenge, we present an asynchronous variational
inference method that allows efficient end-to-end training. Our method achieves
a classification mAP of 22.4% on the Charades benchmark, outperforming the
state-of-the-art (17.2% mAP), and offers equal gains on the task of temporal
localization.


YOLO9000: Better, Faster, Stronger

  We introduce YOLO9000, a state-of-the-art, real-time object detection system
that can detect over 9000 object categories. First we propose various
improvements to the YOLO detection method, both novel and drawn from prior
work. The improved model, YOLOv2, is state-of-the-art on standard detection
tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At
40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like
Faster RCNN with ResNet and SSD while still running significantly faster.
Finally we propose a method to jointly train on object detection and
classification. Using this method we train YOLO9000 simultaneously on the COCO
detection dataset and the ImageNet classification dataset. Our joint training
allows YOLO9000 to predict detections for object classes that don't have
labelled detection data. We validate our approach on the ImageNet detection
task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite
only having detection data for 44 of the 200 classes. On the 156 classes not in
COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes;
it predicts detections for more than 9000 different object categories. And it
still runs in real-time.


See the Glass Half Full: Reasoning about Liquid Containers, their Volume
  and Content

  Humans have rich understanding of liquid containers and their contents; for
example, we can effortlessly pour water from a pitcher to a cup. Doing so
requires estimating the volume of the cup, approximating the amount of water in
the pitcher, and predicting the behavior of water when we tilt the pitcher.
Very little attention in computer vision has been made to liquids and their
containers. In this paper, we study liquid containers and their contents, and
propose methods to estimate the volume of containers, approximate the amount of
liquid in them, and perform comparative volume estimations all from a single
RGB image. Furthermore, we show the results of the proposed model for
predicting the behavior of liquids inside containers when one tilts the
containers. We also introduce a new dataset of Containers Of liQuid contEnt
(COQE) that contains more than 5,000 images of 10,000 liquid containers in
context labelled with volume, amount of content, bounding box annotation, and
corresponding similar 3D CAD models.


SeGAN: Segmenting and Generating the Invisible

  Objects often occlude each other in scenes; Inferring their appearance beyond
their visible parts plays an important role in scene understanding, depth
estimation, object interaction and manipulation. In this paper, we study the
challenging problem of completing the appearance of occluded objects. Doing so
requires knowing which pixels to paint (segmenting the invisible parts of
objects) and what color to paint them (generating the invisible parts). Our
proposed novel solution, SeGAN, jointly optimizes for both segmentation and
generation of the invisible parts of objects. Our experimental results show
that: (a) SeGAN can learn to generate the appearance of the occluded parts of
objects; (b) SeGAN outperforms state-of-the-art segmentation baselines for the
invisible parts of objects; (c) trained on synthetic photo realistic images,
SeGAN can reliably segment natural images; (d) by reasoning about occluder
occludee relations, our method can infer depth layering.


Re3 : Real-Time Recurrent Regression Networks for Visual Tracking of
  Generic Objects

  Robust object tracking requires knowledge and understanding of the object
being tracked: its appearance, its motion, and how it changes over time. A
tracker must be able to modify its underlying model and adapt to new
observations. We present Re3, a real-time deep object tracker capable of
incorporating temporal information into its model. Rather than focusing on a
limited set of objects or training a model at test-time to track a specific
instance, we pretrain our generic tracker on a large variety of objects and
efficiently update on the fly; Re3 simultaneously tracks and updates the
appearance model with a single forward pass. This lightweight model is capable
of tracking objects at 150 FPS, while attaining competitive results on
challenging benchmarks. We also show that our method handles temporary
occlusion better than other comparable trackers using experiments that directly
measure performance on sequences with occlusion.


Visual Semantic Planning using Deep Successor Representations

  A crucial capability of real-world intelligent agents is their ability to
plan a sequence of actions to achieve their goals in the visual world. In this
work, we address the problem of visual semantic planning: the task of
predicting a sequence of actions from visual observations that transform a
dynamic environment from an initial state to a goal state. Doing so entails
knowledge about objects and their affordances, as well as actions and their
preconditions and effects. We propose learning these through interacting with a
visual and dynamic environment. Our proposed solution involves bootstrapping
reinforcement learning with imitation learning. To ensure cross task
generalization, we develop a deep predictive model based on successor
representations. Our experimental results show near optimal results across a
wide range of tasks in the challenging THOR environment.


Neural Speed Reading via Skim-RNN

  Inspired by the principles of speed reading, we introduce Skim-RNN, a
recurrent neural network (RNN) that dynamically decides to update only a small
fraction of the hidden state for relatively unimportant input tokens. Skim-RNN
gives computational advantage over an RNN that always updates the entire hidden
state. Skim-RNN uses the same input and output interfaces as a standard RNN and
can be easily used instead of RNNs in existing models. In our experiments, we
show that Skim-RNN can achieve significantly reduced computational cost without
losing accuracy compared to standard RNNs across five different natural
language tasks. In addition, we demonstrate that the trade-off between accuracy
and speed of Skim-RNN can be dynamically controlled during inference time in a
stable manner. Our analysis also shows that Skim-RNN running on a single CPU
offers lower latency compared to standard RNNs on GPUs.


AI2-THOR: An Interactive 3D Environment for Visual AI

  We introduce The House Of inteRactions (THOR), a framework for visual AI
research, available at http://ai2thor.allenai.org. AI2-THOR consists of near
photo-realistic 3D indoor scenes, where AI agents can navigate in the scenes
and interact with objects to perform tasks. AI2-THOR enables research in many
different domains including but not limited to deep reinforcement learning,
imitation learning, learning by interaction, planning, visual question
answering, unsupervised representation learning, object detection and
segmentation, and learning models of cognition. The goal of AI2-THOR is to
facilitate building visually intelligent models and push the research forward
in this domain.


Who Let The Dogs Out? Modeling Dog Behavior From Visual Data

  We introduce the task of directly modeling a visually intelligent agent.
Computer vision typically focuses on solving various subtasks related to visual
intelligence. We depart from this standard approach to computer vision; instead
we directly model a visually intelligent agent. Our model takes visual
information as input and directly predicts the actions of the agent. Toward
this end we introduce DECADE, a large-scale dataset of ego-centric videos from
a dog's perspective as well as her corresponding movements. Using this data we
model how the dog acts and how the dog plans her movements. We show under a
variety of metrics that given just visual input we can successfully model this
intelligent agent in many situations. Moreover, the representation learned by
our model encodes distinct information compared to representations trained on
image classification, and our learned representation can generalize to other
domains. In particular, we show strong results on the task of walkable surface
estimation by using this dog modeling task as representation learning.


DOCK: Detecting Objects by transferring Common-sense Knowledge

  We present a scalable approach for Detecting Objects by transferring
Common-sense Knowledge (DOCK) from source to target categories. In our setting,
the training data for the source categories have bounding box annotations,
while those for the target categories only have image-level annotations.
Current state-of-the-art approaches focus on image-level visual or semantic
similarity to adapt a detector trained on the source categories to the new
target categories. In contrast, our key idea is to (i) use similarity not at
the image-level, but rather at the region-level, and (ii) leverage richer
common-sense (based on attribute, spatial, etc.) to guide the algorithm towards
learning the correct detections. We acquire such common-sense cues
automatically from readily-available knowledge bases without any extra human
effort. On the challenging MS COCO dataset, we find that common-sense knowledge
can substantially improve detection performance over existing transfer-learning
baselines.


Imagine This! Scripts to Compositions to Videos

  Imagining a scene described in natural language with realistic layout and
appearance of entities is the ultimate test of spatial, visual, and semantic
world knowledge. Towards this goal, we present the Composition, Retrieval, and
Fusion Network (CRAFT), a model capable of learning this knowledge from
video-caption data and applying it while generating videos from novel captions.
CRAFT explicitly predicts a temporal-layout of mentioned entities (characters
and objects), retrieves spatio-temporal entity segments from a video database
and fuses them to generate scene videos. Our contributions include sequential
training of components of CRAFT while jointly modeling layout and appearances,
and losses that encourage learning compositional representations for retrieval.
We evaluate CRAFT on semantic fidelity to caption, composition consistency, and
visual quality. CRAFT outperforms direct pixel generation approaches and
generalizes well to unseen captions and to unseen video databases with no text
annotations. We demonstrate CRAFT on FLINTSTONES, a new richly annotated
video-caption dataset with over 25000 videos. For a glimpse of videos generated
by CRAFT, see https://youtu.be/688Vv86n0z8.


Phrase-Indexed Question Answering: A New Challenge for Scalable Document
  Comprehension

  We formalize a new modular variant of current question answering tasks by
enforcing complete independence of the document encoder from the question
encoder. This formulation addresses a key challenge in machine comprehension by
requiring a standalone representation of the document discourse. It
additionally leads to a significant scalability advantage since the encoding of
the answer candidate phrases in the document can be pre-computed and indexed
offline for efficient retrieval. We experiment with baseline models for the new
task, which achieve a reasonable accuracy but significantly underperform
unconstrained QA models. We invite the QA research community to engage in
Phrase-Indexed Question Answering (PIQA, pika) for closing the gap. The
leaderboard is at: nlp.cs.washington.edu/piqa


Charades-Ego: A Large-Scale Dataset of Paired Third and First Person
  Videos

  In Actor and Observer we introduced a dataset linking the first and
third-person video understanding domains, the Charades-Ego Dataset. In this
paper we describe the egocentric aspect of the dataset and present annotations
for Charades-Ego with 68,536 activity instances in 68.8 hours of first and
third-person video, making it one of the largest and most diverse egocentric
datasets available. Charades-Ego furthermore shares activity classes, scripts,
and methodology with the Charades dataset, that consist of additional 82.3
hours of third-person video with 66,500 activity instances. Charades-Ego has
temporal annotations and textual descriptions, making it suitable for
egocentric video classification, localization, captioning, and new tasks
utilizing the cross-modal nature of the data.


Actor and Observer: Joint Modeling of First and Third-Person Videos

  Several theories in cognitive neuroscience suggest that when people interact
with the world, or simulate interactions, they do so from a first-person
egocentric perspective, and seamlessly transfer knowledge between third-person
(observer) and first-person (actor). Despite this, learning such models for
human action recognition has not been achievable due to the lack of data. This
paper takes a step in this direction, with the introduction of Charades-Ego, a
large-scale dataset of paired first-person and third-person videos, involving
112 people, with 4000 paired videos. This enables learning the link between the
two, actor and observer perspectives. Thereby, we address one of the biggest
bottlenecks facing egocentric vision research, providing a link from
first-person to the abundant third-person data on the web. We use this data to
learn a joint representation of first and third-person videos, with only weak
supervision, and show its effectiveness for transferring knowledge from the
third-person to the first-person domain.


Label Refinery: Improving ImageNet Classification through Label
  Progression

  Among the three main components (data, labels, and models) of any supervised
learning system, data and models have been the main subjects of active
research. However, studying labels and their properties has received very
little attention. Current principles and paradigms of labeling impose several
challenges to machine learning algorithms. Labels are often incomplete,
ambiguous, and redundant. In this paper we study the effects of various
properties of labels and introduce the Label Refinery: an iterative procedure
that updates the ground truth labels after examining the entire dataset. We
show significant gain using refined labels across a wide range of models. Using
a Label Refinery improves the state-of-the-art top-1 accuracy of (1) AlexNet
from 59.3 to 67.2, (2) MobileNet from 70.6 to 73.39, (3) MobileNet-0.25 from
50.6 to 55.59, (4) VGG19 from 72.7 to 75.46, and (5) Darknet19 from 72.9 to
74.47.


PhotoShape: Photorealistic Materials for Large-Scale Shape Collections

  Existing online 3D shape repositories contain thousands of 3D models but lack
photorealistic appearance. We present an approach to automatically assign
high-quality, realistic appearance models to large scale 3D shape collections.
The key idea is to jointly leverage three types of online data -- shape
collections, material collections, and photo collections, using the photos as
reference to guide assignment of materials to shapes. By generating a large
number of synthetic renderings, we train a convolutional neural network to
classify materials in real photos, and employ 3D-2D alignment techniques to
transfer materials to different parts of each shape model. Our system produces
photorealistic, relightable, 3D shapes (PhotoShapes).


Visual Semantic Navigation using Scene Priors

  How do humans navigate to target objects in novel scenes? Do we use the
semantic/functional priors we have built over years to efficiently search and
navigate? For example, to search for mugs, we search cabinets near the coffee
machine and for fruits we try the fridge. In this work, we focus on
incorporating semantic priors in the task of semantic navigation. We propose to
use Graph Convolutional Networks for incorporating the prior knowledge into a
deep reinforcement learning framework. The agent uses the features from the
knowledge graph to predict the actions. For evaluation, we use the AI2-THOR
framework. Our experiments show how semantic knowledge improves performance
significantly. More importantly, we show improvement in generalization to
unseen scenes and/or objects. The supplementary video can be accessed at the
following link: https://youtu.be/otKjuO805dE .


ELASTIC: Improving CNNs with Dynamic Scaling Policies

  Scale variation has been a challenge from traditional to modern approaches in
computer vision. Most solutions to scale issues have a similar theme: a set of
intuitive and manually designed policies that are generic and fixed (e.g. SIFT
or feature pyramid). We argue that the scaling policy should be learned from
data. In this paper, we introduce ELASTIC, a simple, efficient and yet very
effective approach to learn a dynamic scale policy from data. We formulate the
scaling policy as a non-linear function inside the network's structure that (a)
is learned from data, (b) is instance specific, (c) does not add extra
computation, and (d) can be applied on any network architecture. We applied
ELASTIC to several state-of-the-art network architectures and showed consistent
improvement without extra (sometimes even lower) computation on ImageNet
classification, MSCOCO multi-label classification, and PASCAL VOC semantic
segmentation. Our results show major improvement for images with scale
challenges. Our code is available here: https://github.com/allenai/elastic


Two Body Problem: Collaborative Visual Task Completion

  Collaboration is a necessary skill to perform tasks that are beyond one
agent's capabilities. Addressed extensively in both conventional and modern AI,
multi-agent collaboration has often been studied in the context of simple grid
worlds. We argue that there are inherently visual aspects to collaboration
which should be studied in visually rich environments. A key element in
collaboration is communication that can be either explicit, through messages,
or implicit, through perception of the other agents and the visual world.
Learning to collaborate in a visual environment entails learning (1) to perform
the task, (2) when and what to communicate, and (3) how to act based on these
communications and the perception of the visual world. In this paper we study
the problem of learning to collaborate directly from pixels in AI2-THOR and
demonstrate the benefits of explicit and implicit modes of communication to
perform visual tasks. Refer to our project page for more details:
https://prior.allenai.org/projects/two-body-problem


"What happens if..." Learning to Predict the Effect of Forces in Images

  What happens if one pushes a cup sitting on a table toward the edge of the
table? How about pushing a desk against a wall? In this paper, we study the
problem of understanding the movements of objects as a result of applying
external forces to them. For a given force vector applied to a specific
location in an image, our goal is to predict long-term sequential movements
caused by that force. Doing so entails reasoning about scene geometry, objects,
their attributes, and the physical rules that govern the movements of objects.
We design a deep neural network model that learns long-term sequential
dependencies of object movements while taking into account the geometry and
appearance of the scene by combining Convolutional and Recurrent Neural
Networks. Training our model requires a large-scale dataset of object movements
caused by external forces. To build a dataset of forces in scenes, we
reconstructed all images in SUN RGB-D dataset in a physics simulator to
estimate the physical movements of objects caused by external forces applied to
them. Our Forces in Scenes (ForScene) dataset contains 10,335 images in which a
variety of external forces are applied to different types of objects resulting
in more than 65,000 object movements represented in 3D. Our experimental
evaluations show that the challenging task of predicting long-term movements of
objects as their reaction to external forces is possible from a single image.


You Only Look Once: Unified, Real-Time Object Detection

  We present YOLO, a new approach to object detection. Prior work on object
detection repurposes classifiers to perform detection. Instead, we frame object
detection as a regression problem to spatially separated bounding boxes and
associated class probabilities. A single neural network predicts bounding boxes
and class probabilities directly from full images in one evaluation. Since the
whole detection pipeline is a single network, it can be optimized end-to-end
directly on detection performance.
  Our unified architecture is extremely fast. Our base YOLO model processes
images in real-time at 45 frames per second. A smaller version of the network,
Fast YOLO, processes an astounding 155 frames per second while still achieving
double the mAP of other real-time detectors. Compared to state-of-the-art
detection systems, YOLO makes more localization errors but is far less likely
to predict false detections where nothing exists. Finally, YOLO learns very
general representations of objects. It outperforms all other detection methods,
including DPM and R-CNN, by a wide margin when generalizing from natural images
to artwork on both the Picasso Dataset and the People-Art Dataset.


Hollywood in Homes: Crowdsourcing Data Collection for Activity
  Understanding

  Computer vision has a great potential to help our daily lives by searching
for lost keys, watering flowers or reminding us to take a pill. To succeed with
such tasks, computer vision methods need to be trained from real and diverse
examples of our daily dynamic scenes. While most of such scenes are not
particularly exciting, they typically do not appear on YouTube, in movies or TV
broadcasts. So how do we collect sufficiently many diverse but boring samples
representing our lives? We propose a novel Hollywood in Homes approach to
collect such data. Instead of shooting videos in the lab, we ensure diversity
by distributing and crowdsourcing the whole process of video creation from
script writing to video recording and annotation. Following this procedure we
collect a new dataset, Charades, with hundreds of people recording videos in
their own homes, acting out casual everyday activities. The dataset is composed
of 9,848 annotated videos with an average length of 30 seconds, showing
activities of 267 people from three continents. Each video is annotated by
multiple free-text descriptions, action labels, action intervals and classes of
interacted objects. In total, Charades provides 27,847 video descriptions,
66,500 temporally localized intervals for 157 action classes and 41,104 labels
for 46 object classes. Using this rich data, we evaluate and provide baseline
results for several tasks including action recognition and automatic
description generation. We believe that the realism, diversity, and casual
nature of this dataset will present unique challenges and new opportunities for
computer vision community.


Much Ado About Time: Exhaustive Annotation of Temporal Data

  Large-scale annotated datasets allow AI systems to learn from and build upon
the knowledge of the crowd. Many crowdsourcing techniques have been developed
for collecting image annotations. These techniques often implicitly rely on the
fact that a new input image takes a negligible amount of time to perceive. In
contrast, we investigate and determine the most cost-effective way of obtaining
high-quality multi-label annotations for temporal data such as videos. Watching
even a short 30-second video clip requires a significant time investment from a
crowd worker; thus, requesting multiple annotations following a single viewing
is an important cost-saving strategy. But how many questions should we ask per
video? We conclude that the optimal strategy is to ask as many questions as
possible in a HIT (up to 52 binary questions after watching a 30-second video
clip in our experiments). We demonstrate that while workers may not correctly
answer all questions, the cost-benefit analysis nevertheless favors consensus
from multiple such cheap-yet-imperfect iterations over more complex
alternatives. When compared with a one-question-per-video baseline, our method
is able to achieve a 10% improvement in recall 76.7% ours versus 66.7%
baseline) at comparable precision (83.8% ours versus 83.0% baseline) in about
half the annotation time (3.8 minutes ours compared to 7.1 minutes baseline).
We demonstrate the effectiveness of our method by collecting multi-label
annotations of 157 human activities on 1,815 videos.


Target-driven Visual Navigation in Indoor Scenes using Deep
  Reinforcement Learning

  Two less addressed issues of deep reinforcement learning are (1) lack of
generalization capability to new target goals, and (2) data inefficiency i.e.,
the model requires several (and often costly) episodes of trial and error to
converge, which makes it impractical to be applied to real-world scenarios. In
this paper, we address these two issues and apply our model to the task of
target-driven visual navigation. To address the first issue, we propose an
actor-critic model whose policy is a function of the goal as well as the
current state, which allows to better generalize. To address the second issue,
we propose AI2-THOR framework, which provides an environment with high-quality
3D scenes and physics engine. Our framework enables agents to take actions and
interact with objects. Hence, we can collect a huge number of training samples
efficiently.
  We show that our proposed method (1) converges faster than the
state-of-the-art deep reinforcement learning methods, (2) generalizes across
targets and across scenes, (3) generalizes to a real robot scenario with a
small amount of fine-tuning (although the model is trained in simulation), (4)
is end-to-end trainable and does not need feature engineering, feature matching
between frames or 3D reconstruction of the environment.
  The supplementary video can be accessed at the following link:
https://youtu.be/SmBxMDiOrvs.


AJILE Movement Prediction: Multimodal Deep Learning for Natural Human
  Neural Recordings and Video

  Developing useful interfaces between brains and machines is a grand challenge
of neuroengineering. An effective interface has the capacity to not only
interpret neural signals, but predict the intentions of the human to perform an
action in the near future; prediction is made even more challenging outside
well-controlled laboratory experiments. This paper describes our approach to
detect and to predict natural human arm movements in the future, a key
challenge in brain computer interfacing that has never before been attempted.
We introduce the novel Annotated Joints in Long-term ECoG (AJILE) dataset;
AJILE includes automatically annotated poses of 7 upper body joints for four
human subjects over 670 total hours (more than 72 million frames), along with
the corresponding simultaneously acquired intracranial neural recordings. The
size and scope of AJILE greatly exceeds all previous datasets with movements
and electrocorticography (ECoG), making it possible to take a deep learning
approach to movement prediction. We propose a multimodal model that combines
deep convolutional neural networks (CNN) with long short-term memory (LSTM)
blocks, leveraging both ECoG and video modalities. We demonstrate that our
models are able to detect movements and predict future movements up to 800 msec
before movement initiation. Further, our multimodal movement prediction models
exhibit resilience to simulated ablation of input neural signals. We believe a
multimodal approach to natural neural decoding that takes context into account
is critical in advancing bioelectronic technologies and human neuroscience.


Structured Set Matching Networks for One-Shot Part Labeling

  Diagrams often depict complex phenomena and serve as a good test bed for
visual and textual reasoning. However, understanding diagrams using natural
image understanding approaches requires large training datasets of diagrams,
which are very hard to obtain. Instead, this can be addressed as a matching
problem either between labeled diagrams, images or both. This problem is very
challenging since the absence of significant color and texture renders local
cues ambiguous and requires global reasoning. We consider the problem of
one-shot part labeling: labeling multiple parts of an object in a target image
given only a single source image of that category. For this set-to-set matching
problem, we introduce the Structured Set Matching Network (SSMN), a structured
prediction model that incorporates convolutional neural networks. The SSMN is
trained using global normalization to maximize local match scores between
corresponding elements and a global consistency score among all matched
elements, while also enforcing a matching constraint between the two sets. The
SSMN significantly outperforms several strong baselines on three label transfer
scenarios: diagram-to-diagram, evaluated on a new diagram dataset of over 200
categories; image-to-image, evaluated on a dataset built on top of the Pascal
Part Dataset; and image-to-diagram, evaluated on transferring labels across
these datasets.


IQA: Visual Question Answering in Interactive Environments

  We introduce Interactive Question Answering (IQA), the task of answering
questions that require an autonomous agent to interact with a dynamic visual
environment. IQA presents the agent with a scene and a question, like: "Are
there any apples in the fridge?" The agent must navigate around the scene,
acquire visual understanding of scene elements, interact with objects (e.g.
open refrigerators) and plan for a series of actions conditioned on the
question. Popular reinforcement learning approaches with a single controller
perform poorly on IQA owing to the large and diverse state space. We propose
the Hierarchical Interactive Memory Network (HIMN), consisting of a factorized
set of controllers, allowing the system to operate at multiple levels of
temporal abstraction. To evaluate HIMN, we introduce IQUAD V1, a new dataset
built upon AI2-THOR, a simulated photo-realistic environment of configurable
indoor scenes with interactive objects (code and dataset available at
https://github.com/danielgordon10/thor-iqa-cvpr-2018). IQUAD V1 has 75,000
questions, each paired with a unique scene configuration. Our experiments show
that our proposed model outperforms popular single controller based methods on
IQUAD V1. For sample questions and results, please view our video:
https://youtu.be/pXd3C-1jr98


From Recognition to Cognition: Visual Commonsense Reasoning

  Visual understanding goes well beyond object recognition. With one glance at
an image, we can effortlessly imagine the world beyond the pixels: for
instance, we can infer people's actions, goals, and mental states. While this
task is easy for humans, it is tremendously difficult for today's vision
systems, requiring higher-order cognition and commonsense reasoning about the
world. We formalize this task as Visual Commonsense Reasoning. Given a
challenging question about an image, a machine must answer correctly and then
provide a rationale justifying its answer.
  Next, we introduce a new dataset, VCR, consisting of 290k multiple choice QA
problems derived from 110k movie scenes. The key recipe for generating
non-trivial and high-quality problems at scale is Adversarial Matching, a new
approach to transform rich annotations into multiple choice questions with
minimal bias. Experimental results show that while humans find VCR easy (over
90% accuracy), state-of-the-art vision models struggle (~45%).
  To move towards cognition-level understanding, we present a new reasoning
engine, Recognition to Cognition Networks (R2C), that models the necessary
layered inferences for grounding, contextualization, and reasoning. R2C helps
narrow the gap between humans and machines (~65%); still, the challenge is far
from solved, and we provide analysis that suggests avenues for future work.


Learning to Learn How to Learn: Self-Adaptive Visual Navigation Using
  Meta-Learning

  Learning is an inherently continuous phenomenon. When humans learn a new task
there is no explicit distinction between training and inference. As we learn a
task, we keep learning about it while performing the task. What we learn and
how we learn it varies during different stages of learning. Learning how to
learn and adapt is a key property that enables us to generalize effortlessly to
new settings. This is in contrast with conventional settings in machine
learning where a trained model is frozen during inference. In this paper we
study the problem of learning to learn at both training and test time in the
context of visual navigation. A fundamental challenge in navigation is
generalization to unseen scenes. In this paper we propose a self-adaptive
visual navigation method (SAVN) which learns to adapt to new environments
without any explicit supervision. Our solution is a meta-reinforcement learning
approach where an agent learns a self-supervised interaction loss that
encourages effective navigation. Our experiments, performed in the AI2-THOR
framework, show major improvements in both success rate and SPL for visual
navigation in novel scenes. Our code and data are available at:
https://github.com/allenai/savn .


Video Relationship Reasoning using Gated Spatio-Temporal Energy Graph

  Visual relationship reasoning is a crucial yet challenging task for
understanding rich interactions across visual concepts. For example, a
relationship 'man, open, door' involves a complex relation 'open' between
concrete entities 'man, door'. While much of the existing work has studied this
problem in the context of still images, understanding visual relationships in
videos has received limited attention. Due to their temporal nature, videos
enable us to model and reason about a more comprehensive set of visual
relationships, such as those requiring multiple (temporal) observations (e.g.,
'man, lift up, box' vs. 'man, put down, box'), as well as relationships that
are often correlated through time (e.g., 'woman, pay, money' followed by
'woman, buy, coffee'). In this paper, we construct a Conditional Random Field
on a fully-connected spatio-temporal graph that exploits the statistical
dependency between relational entities spatially and temporally. We introduce a
novel gated energy function parametrization that learns adaptive relations
conditioned on visual observations. Our model optimization is computationally
efficient, and its space computation complexity is significantly amortized
through our proposed parameterization. Experimental results on benchmark video
datasets (ImageNet Video and Charades) demonstrate state-of-the-art performance
across three standard relationship reasoning tasks: Detection, Tagging, and
Recognition.


