Simultaneous diagonalization: the asymmetric, low-rank, and noisy  settings

  Simultaneous matrix diagonalization is used as a subroutine in many machinelearning problems, including blind source separation and paramater estimationin latent variable models. Here, we extend algorithms for performing jointdiagonalization to low-rank and asymmetric matrices, and we also provideextensions to the perturbation analysis of these methods. Our results allowjoint diagonalization to be applied in several new settings.

Estimating Uncertainty Online Against an Adversary

  Assessing uncertainty is an important step towards ensuring the safety andreliability of machine learning systems. Existing uncertainty estimationtechniques may fail when their modeling assumptions are not met, e.g. when thedata distribution differs from the one seen at training time. Here, we proposetechniques that assess a classification algorithm's uncertainty via calibratedprobabilities (i.e. probabilities that match empirical outcome frequencies inthe long run) and which are guaranteed to be reliable (i.e. accurate andcalibrated) on out-of-distribution input, including input generated by anadversary. This represents an extension of classical online learning thathandles uncertainty in addition to guaranteeing accuracy under adversarialassumptions. We establish formal guarantees for our methods, and we validatethem on two real-world problems: question answering and medical diagnosis fromgenomic data.

Audio Super Resolution using Neural Networks

  We introduce a new audio processing technique that increases the samplingrate of signals such as speech or music using deep convolutional neuralnetworks. Our model is trained on pairs of low and high-quality audio examples;at test-time, it predicts missing samples within a low-resolution signal in aninterpolation process similar to image super-resolution. Our method is simpleand does not involve specialized audio processing techniques; in ourexperiments, it outperforms baselines on standard speech and music benchmarksat upscaling ratios of 2x, 4x, and 6x. The method has practical applications intelephony, compression, and text-to-speech generation; it demonstrates theeffectiveness of feed-forward convolutional architectures on an audiogeneration task.

Neural Variational Inference and Learning in Undirected Graphical Models

  Many problems in machine learning are naturally expressed in the language ofundirected graphical models. Here, we propose black-box learning and inferencealgorithms for undirected models that optimize a variational approximation tothe log-likelihood of the model. Central to our approach is an upper bound onthe log-partition function parametrized by a function q that we express as aflexible neural network. Our bound makes it possible to track the partitionfunction during learning, to speed-up sampling, and to train a broad class ofhybrid directed/undirected models via a unified variational inferenceframework. We empirically demonstrate the effectiveness of our method onseveral popular generative modeling datasets.

Accurate Uncertainties for Deep Learning Using Calibrated Regression

  Methods for reasoning under uncertainty are a key building block of accurateand reliable machine learning systems. Bayesian methods provide a generalframework to quantify uncertainty. However, because of model misspecificationand the use of approximate inference, Bayesian uncertainty estimates are ofteninaccurate -- for example, a 90% credible interval may not contain the trueoutcome 90% of the time. Here, we propose a simple procedure for calibratingany regression algorithm; when applied to Bayesian and probabilistic models, itis guaranteed to produce calibrated uncertainty estimates given enough data.Our procedure is inspired by Platt scaling and extends previous work onclassification. We evaluate this approach on Bayesian linear regression,feedforward, and recurrent neural networks, and find that it consistentlyoutputs well-calibrated credible intervals while improving performance on timeseries forecasting and model-based reinforcement learning tasks.

Algorithms for multi-armed bandit problems

  Although many algorithms for the multi-armed bandit problem arewell-understood theoretically, empirical confirmation of their effectiveness isgenerally scarce. This paper presents a thorough empirical study of the mostpopular multi-armed bandit algorithms. Three important observations can be madefrom our results. Firstly, simple heuristics such as epsilon-greedy andBoltzmann exploration outperform theoretically sound algorithms on mostsettings by a significant margin. Secondly, the performance of most algorithmsvaries dramatically with the parameters of the bandit problem. Our studyidentifies for each algorithm the settings where it performs well, and thesettings where it performs poorly. Thirdly, the algorithms' performancerelative each to other is affected only by the number of bandit arms and thevariance of the rewards. This finding may guide the design of subsequentempirical evaluations. In the second part of the paper, we turn our attentionto an important area of application of bandit algorithms: clinical trials.Although the design of clinical trials has been one of the principal practicalproblems motivating research on multi-armed bandits, bandit algorithms havenever been evaluated as potential treatment allocation strategies. Using datafrom a real study, we simulate the outcome that a 2001-2002 clinical trialwould have had if bandit algorithms had been used to allocate patients totreatments. We find that an adaptive trial would have successfully treated atleast 50% more patients, while significantly reducing the number of adverseeffects and increasing patient retention. At the end of the trial, the besttreatment could have still been identified with a high level of statisticalconfidence. Our findings demonstrate that bandit algorithms are attractivealternatives to current adaptive treatment allocation strategies.

Tensor Factorization via Matrix Factorization

  Tensor factorization arises in many machine learning applications, suchknowledge base modeling and parameter estimation in latent variable models.However, numerical methods for tensor factorization have not reached the levelof maturity of matrix factorization methods. In this paper, we propose a newmethod for CP tensor factorization that uses random projections to reduce theproblem to simultaneous matrix diagonalization. Our method is conceptuallysimple and also applies to non-orthogonal and asymmetric tensors of arbitraryorder. We prove that a small number random projections essentially preservesthe spectral information in the tensor, allowing us to remove the dependence onthe eigengap that plagued earlier tensor-to-matrix reductions. Experimentally,our method outperforms existing tensor factorization methods on both simulateddata and two real datasets.

Adversarial Constraint Learning for Structured Prediction

  Constraint-based learning reduces the burden of collecting labels by havingusers specify general properties of structured outputs, such as constraintsimposed by physical laws. We propose a novel framework for simultaneouslylearning these constraints and using them for supervision, bypassing thedifficulty of using domain expertise to manually specify constraints. Learningrequires a black-box simulator of structured outputs, which generates validlabels, but need not model their corresponding inputs or the input-labelrelationship. At training time, we constrain the model to produce outputs thatcannot be distinguished from simulated labels by adversarial training.Providing our framework with a small number of labeled inputs gives rise to anew semi-supervised structured prediction model; we evaluate this model onmultiple tasks --- tracking, pose estimation and time series prediction --- andfind that it achieves high accuracy with only a small number of labeled inputs.In some cases, no labels are required at all.

