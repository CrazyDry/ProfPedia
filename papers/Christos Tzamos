Mechanism Design via Optimal Transport

  Optimal mechanisms have been provided in quite general multi-item settings,as long as each bidder's type distribution is given explicitly by listing everytype in the support along with its associated probability. In the implicitsetting, e.g. when the bidders have additive valuations with independent and/orcontinuous values for the items, these results do not apply, and it wasrecently shown that exact revenue optimization is intractable, even when thereis only one bidder. Even for item distributions with special structure, optimalmechanisms have been surprisingly rare and the problem is challenging even inthe two-item case. In this paper, we provide a framework for designing optimalmechanisms using optimal transport theory and duality theory. We instantiateour framework to obtain conditions under which only pricing the grand bundle isoptimal in multi-item settings (complementing the work of [Manelli and Vincent2006], as well as to characterize optimal two-item mechanisms. We use ourresults to derive closed-form descriptions of the optimal mechanism in severaltwo-item settings, exhibiting also a setting where a continuum of lotteries isnecessary for revenue optimization but a closed-form representation of themechanism can still be found efficiently using our framework.

Optimal Pricing is Hard

  We show that computing the revenue-optimal deterministic auction inunit-demand single-buyer Bayesian settings, i.e. the optimal item-pricing, iscomputationally hard even in single-item settings where the buyer's valuedistribution is a sum of independently distributed attributes, or multi-itemsettings where the buyer's values for the items are independent. We also showthat it is intractable to optimally price the grand bundle of multiple itemsfor an additive bidder whose values for the items are independent. Thesedifficulties stem from implicit definitions of a value distribution. We providethree instances of how different properties of implicit distributions can leadto intractability: the first is a #P-hardness proof, while the remaining twoare reductions from the SQRT-SUM problem of Garey, Graham, and Johnson. Whilesimple pricing schemes can oftentimes approximate the best scheme in revenue,they can have drastically different underlying structure. We argue thereforethat either the specification of the input distribution must be highlyrestricted in format, or it is necessary for the goal to be mere approximationto the optimal scheme's revenue instead of computing properties of the schemeitself.

Strong Duality for a Multiple-Good Monopolist

  We characterize optimal mechanisms for the multiple-good monopoly problem andprovide a framework to find them. We show that a mechanism is optimal if andonly if a measure $\mu$ derived from the buyer's type distribution satisfiescertain stochastic dominance conditions. This measure expresses the marginalchange in the seller's revenue under marginal changes in the rent paid tosubsets of buyer types. As a corollary, we characterize the optimality ofgrand-bundling mechanisms, strengthening several results in the literature,where only sufficient optimality conditions have been derived. As anapplication, we show that the optimal mechanism for $n$ independent uniformitems each supported on $[c,c+1]$ is a grand-bundling mechanism, as long as $c$is sufficiently large, extending Pavlov's result for $2$ items [Pavlov'11]. Atthe same time, our characterization also implies that, for all $c$ and for allsufficiently large $n$, the optimal mechanism for $n$ independent uniform itemssupported on $[c,c+1]$ is not a grand bundling mechanism.

Strategy-Proof Facility Location for Concave Cost Functions

  We consider k-Facility Location games, where n strategic agents report theirlocations on the real line, and a mechanism maps them to k facilities. Eachagent seeks to minimize his connection cost, given by a nonnegative increasingfunction of his distance to the nearest facility. Departing from previous work,that mostly considers the identity cost function, we are interested inmechanisms without payments that are (group) strategyproof for any given costfunction, and achieve a good approximation ratio for the social cost and/or themaximum cost of the agents.  We present a randomized mechanism, called Equal Cost, which is groupstrategyproof and achieves a bounded approximation ratio for all k and n, forany given concave cost function. The approximation ratio is at most 2 for MaxCost and at most n for Social Cost. To the best of our knowledge, this is thefirst mechanism with a bounded approximation ratio for instances with k > 2facilities and any number of agents. Our result implies an interestingseparation between deterministic mechanisms, whose approximation ratio for MaxCost jumps from 2 to unbounded when k increases from 2 to 3, and randomizedmechanisms, whose approximation ratio remains at most 2 for all k. On thenegative side, we exclude the possibility of a mechanism with the properties ofEqual Cost for strictly convex cost functions. We also present a randomizedmechanism, called Pick the Loser, which applies to instances with k facilitiesand n = k+1 agents, and for any given concave cost function, is strongly groupstrategyproof and achieves an approximation ratio of 2 for Social Cost.

The Value of Knowing Your Enemy

  Many auction settings implicitly or explicitly require that bidders aretreated equally ex-ante. This may be because discrimination is philosophicallyor legally impermissible, or because it is practically difficult to implementor impossible to enforce. We study so-called {\em anonymous} auctions tounderstand the revenue tradeoffs and to develop simple anonymous auctions thatare approximately optimal.  We consider digital goods settings and show that the optimal anonymous,dominant strategy incentive compatible auction has an intuitive structure ---imagine that bidders are randomly permuted before the auction, then infer aposterior belief about bidder i's valuation from the values of other biddersand set a posted price that maximizes revenue given this posterior.  We prove that no anonymous mechanism can guarantee an approximation betterthan O(n) to the optimal revenue in the worst case (or O(log n) for regulardistributions) and that even posted price mechanisms match those guarantees.Understanding that the real power of anonymous mechanisms comes when theauctioneer can infer the bidder identities accurately, we show a tight O(k)approximation guarantee when each bidder can be confused with at most k "highertypes". Moreover, we introduce a simple mechanism based on n target prices thatis asymptotically optimal and build on this mechanism to extend our results tom-unit auctions and sponsored search.

A Size-Free CLT for Poisson Multinomials and its Applications

  An $(n,k)$-Poisson Multinomial Distribution (PMD) is the distribution of thesum of $n$ independent random vectors supported on the set ${\calB}_k=\{e_1,\ldots,e_k\}$ of standard basis vectors in $\mathbb{R}^k$. We showthat any $(n,k)$-PMD is ${\rm poly}\left({k\over \sigma}\right)$-close in totalvariation distance to the (appropriately discretized) multi-dimensionalGaussian with the same first two moments, removing the dependence on $n$ fromthe Central Limit Theorem of Valiant and Valiant. Interestingly, our CLT isobtained by bootstrapping the Valiant-Valiant CLT itself through the structuralcharacterization of PMDs shown in recent work by Daskalakis, Kamath, andTzamos. In turn, our stronger CLT can be leveraged to obtain an efficient PTASfor approximate Nash equilibria in anonymous games, significantly improving thestate of the art, and matching qualitatively the running time dependence on $n$and $1/\varepsilon$ of the best known algorithm for two-strategy anonymousgames. Our new CLT also enables the construction of covers for the set of$(n,k)$-PMDs, which are proper and whose size is shown to be essentiallyoptimal. Our cover construction combines our CLT with the Shapley-Folkmantheorem and recent sparsification results for Laplacian matrices by Batson,Spielman, and Srivastava. Our cover size lower bound is based on an algebraicgeometric construction. Finally, leveraging the structural properties of theFourier spectrum of PMDs we show that these distributions can be learned from$O_k(1/\varepsilon^2)$ samples in ${\rm poly}_k(1/\varepsilon)$-time, removingthe quasi-polynomial dependence of the running time on $1/\varepsilon$ from thealgorithm of Daskalakis, Kamath, and Tzamos.

Tight Hardness Results for Maximum Weight Rectangles

  Given $n$ weighted points (positive or negative) in $d$ dimensions, what isthe axis-aligned box which maximizes the total weight of the points itcontains?  The best known algorithm for this problem is based on a reduction to arelated problem, the Weighted Depth problem [T. M. Chan, FOCS'13], and runs intime $O(n^d)$. It was conjectured [Barbay et al., CCCG'13] that this runtime istight up to subpolynomial factors. We answer this conjecture affirmatively byproviding a matching conditional lower bound. We also provide conditional lowerbounds for the special case when points are arranged in a grid (a well studiedproblem known as Maximum Subarray problem) as well as for other relatedproblems.  All our lower bounds are based on assumptions that the best known algorithmsfor the All-Pairs Shortest Paths problem (APSP) and for the Max-Weight k-Cliqueproblem in edge-weighted graphs are essentially optimal.

Improving Viterbi is Hard: Better Runtimes Imply Faster Clique  Algorithms

  The classic algorithm of Viterbi computes the most likely path in a HiddenMarkov Model (HMM) that results in a given sequence of observations. It runs intime $O(Tn^2)$ given a sequence of $T$ observations from a HMM with $n$ states.Despite significant interest in the problem and prolonged effort by differentcommunities, no known algorithm achieves more than a polylogarithmic speedup.  In this paper, we explain this difficulty by providing matching conditionallower bounds. We show that the Viterbi algorithm runtime is optimal up tosubpolynomial factors even when the number of distinct observations is small.Our lower bounds are based on assumptions that the best known algorithms forthe All-Pairs Shortest Paths problem (APSP) and for the Max-Weight $k$-Cliqueproblem in edge-weighted graphs are essentially tight.  Finally, using a recent algorithm by Green Larsen and Williams for onlineBoolean matrix-vector multiplication, we get a $2^{\Omega(\sqrt {\log n})}$speedup for the Viterbi algorithm when there are few distinct transitionprobabilities in the HMM.

Who to Trust for Truthfully Maximizing Welfare?

  We introduce a general approach based on \emph{selective verification} andobtain approximate mechanisms without money for maximizing the social welfarein the general domain of utilitarian voting. Having a good allocation in mind,a mechanism with verification selects few critical agents and detects, using averification oracle, whether they have reported truthfully. If yes, themechanism produces the desired allocation. Otherwise, the mechanism ignores anymisreports and proceeds with the remaining agents. We obtain randomizedtruthful (or almost truthful) mechanisms without money that verify only $O(\lnm / \epsilon)$ agents, where $m$ is the number of outcomes, independently ofthe total number of agents, and are $(1-\epsilon)$-approximate for the socialwelfare. We also show that any truthful mechanism with a constant approximationratio needs to verify $\Omega(\log m)$ agents. A remarkable property of ourmechanisms is \emph{robustness}, namely that their outcome depends only on thereports of the truthful agents.

Truthful Facility Location with Additive Errors

  We address the problem of locating facilities on the $[0,1]$ interval basedon reports from strategic agents. The cost of each agent is her distance to theclosest facility, and the global objective is to minimize either the maximumcost of an agent or the social cost.  As opposed to the extensive literature on facility location which considersthe multiplicative error, we focus on minimizing the worst-case additive error.Minimizing the additive error incentivizes mechanisms to adapt to the size ofthe instance. I.e., mechanisms can sacrifice little efficiency in smallinstances (location profiles in which all agents are relatively close to oneanother), in order to gain more [absolute] efficiency in large instances. Weargue that this measure is better suited for many manifestations of thefacility location problem in various domains.  We present tight bounds for mechanisms locating a single facility in bothdeterministic and randomized cases. We further provide several extensions forlocating multiple facilities.

Combinatorial Assortment Optimization

  Assortment optimization refers to the problem of designing a slate ofproducts to offer potential customers, such as stocking the shelves in aconvenience store. The price of each product is fixed in advance, and aprobabilistic choice function describes which product a customer will choosefrom any given subset. We introduce the combinatorial assortment problem, whereeach customer may select a bundle of products. We consider a model of consumerchoice where the relative value of different bundles is described by avaluation function, while individual customers may differ in their absolutewillingness to pay, and study the complexity of the resulting optimizationproblem. We show that any sub-polynomial approximation to the problem requiresexponentially many demand queries when the valuation function is XOS, and thatno FPTAS exists even for succinctly-representable submodular valuations. On thepositive side, we show how to obtain constant approximations under a"well-priced" condition, where each product's price is sufficiently high. Wealso provide an exact algorithm for $k$-additive valuations, and show how toextend our results to a learning setting where the seller must infer thecustomers' preferences from their purchasing behavior.

Actively Avoiding Nonsense in Generative Models

  A generative model may generate utter nonsense when it is fit to maximize thelikelihood of observed data. This happens due to "model error," i.e., when thetrue data generating distribution does not fit within the class of generativemodels being learned. To address this, we propose a model of activedistribution learning using a binary invalidity oracle that identifies someexamples as clearly invalid, together with random positive examples sampledfrom the true distribution. The goal is to maximize the likelihood of thepositive examples subject to the constraint of (almost) never generatingexamples labeled invalid by the oracle. Guarantees are agnostic compared to aclass of probability distributions. We show that, while proper learning oftenrequires exponentially many queries to the invalidity oracle, improperdistribution learning can be done using polynomially many queries.

Fast Modular Subset Sum using Linear Sketching

  Given n positive integers, the Modular Subset Sum problem asks if a subsetadds up to a given target t modulo a given integer m. This is a naturalgeneralization of the Subset Sum problem (where m=+\infty) with ties toadditive combinatorics and cryptography.  Recently, in [Bringmann, SODA'17] and [Koiliaris and Xu, SODA'17], efficientalgorithms have been developed for the non-modular case, running in near-linearpseudo-polynomial time. For the modular case, however, the best known algorithmby Koiliaris and Xu [Koiliaris and Xu, SODA'17] runs in time O~(m^{5/4}).  In this paper, we present an algorithm running in time O~(m), which matches arecent conditional lower bound of [Abboud et al.'17] based on the StrongExponential Time Hypothesis. Interestingly, in contrast to most previousresults on Subset Sum, our algorithm does not use the Fast Fourier Transform.Instead, it is able to simulate the "textbook" Dynamic Programming algorithmmuch faster, using ideas from linear sketching. This is one of the firstapplications of sketching-based techniques to obtain fast algorithms forcombinatorial problems in an offline setting.

Efficient Statistics, in High Dimensions, from Truncated Samples

  We provide an efficient algorithm for the classical problem, going back toGalton, Pearson, and Fisher, of estimating, with arbitrary accuracy theparameters of a multivariate normal distribution from truncated samples.Truncated samples from a $d$-variate normal ${\calN}(\mathbf{\mu},\mathbf{\Sigma})$ means a samples is only revealed if it fallsin some subset $S \subseteq \mathbb{R}^d$; otherwise the samples are hidden andtheir count in proportion to the revealed samples is also hidden. We show thatthe mean $\mathbf{\mu}$ and covariance matrix $\mathbf{\Sigma}$ can beestimated with arbitrary accuracy in polynomial-time, as long as we have oracleaccess to $S$, and $S$ has non-trivial measure under the unknown $d$-variatenormal distribution. Additionally we show that without oracle access to $S$,any non-trivial estimation is impossible.

On the Power of Deterministic Mechanisms for Facility Location Games

  We consider K-Facility Location games, where n strategic agents report theirlocations in a metric space, and a mechanism maps them to K facilities. Ourmain result is an elegant characterization of deterministic strategyproofmechanisms with a bounded approximation ratio for 2-Facility Location on theline. In particular, we show that for instances with n \geq 5 agents, any suchmechanism either admits a unique dictator, or always places the facilities atthe leftmost and the rightmost location of the instance. As a corollary, weobtain that the best approximation ratio achievable by deterministicstrategyproof mechanisms for the problem of locating 2 facilities on the lineto minimize the total connection cost is precisely n-2. Another rathersurprising consequence is that the Two-Extremes mechanism of (Procaccia andTennenholtz, EC 2009) is the only deterministic anonymous strategyproofmechanism with a bounded approximation ratio for 2-Facility Location on theline.  The proof of the characterization employs several new ideas and technicaltools, which provide new insights into the behavior of deterministicstrategyproof mechanisms for K-Facility Location games, and may be ofindependent interest. Employing one of these tools, we show that for every K\geq 3, there do not exist any deterministic anonymous strategyproof mechanismswith a bounded approximation ratio for K-Facility Location on the line, evenfor simple instances with K+1 agents. Moreover, building on thecharacterization for the line, we show that there do not exist anydeterministic strategyproof mechanisms with a bounded approximation ratio for2-Facility Location on more general metric spaces, which is true even forsimple instances with 3 agents located in a star.

The Complexity of Optimal Mechanism Design

  Myerson's seminal work provides a computationally efficient revenue-optimalauction for selling one item to multiple bidders. Generalizing this work toselling multiple items at once has been a central question in economics andalgorithmic game theory, but its complexity has remained poorly understood. Weanswer this question by showing that a revenue-optimal auction in multi-itemsettings cannot be found and implemented computationally efficiently, unlessZPP contains P^#P. This is true even for a single additive bidder whose valuesfor the items are independently distributed on two rational numbers withrational probabilities. Our result is very general: we show that it is hard tocompute any encoding of an optimal auction of any format (direct or indirect,truthful or non-truthful) that can be implemented in expected polynomial time.In particular, under well-believed complexity-theoretic assumptions,revenue-optimization in very simple multi-item settings can only be tractablyapproximated.  We note that our hardness result applies to randomized mechanisms in a verysimple setting, and is not an artifact of introducing combinatorial structureto the problem by allowing correlation among item values, introducingcombinatorial valuations, or requiring the mechanism to be deterministic (whosestructure is readily combinatorial). Our proof is enabled by aflow-interpretation of the solutions of an exponential-size linear program forrevenue maximization with an additional supermodularity constraint.

On the Structure, Covering, and Learning of Poisson Multinomial  Distributions

  An $(n,k)$-Poisson Multinomial Distribution (PMD) is the distribution of thesum of $n$ independent random vectors supported on the set ${\calB}_k=\{e_1,\ldots,e_k\}$ of standard basis vectors in $\mathbb{R}^k$. We provea structural characterization of these distributions, showing that, for all$\varepsilon >0$, any $(n, k)$-Poisson multinomial random vector is$\varepsilon$-close, in total variation distance, to the sum of a discretizedmultidimensional Gaussian and an independent $(\text{poly}(k/\varepsilon),k)$-Poisson multinomial random vector. Our structural characterization extendsthe multi-dimensional CLT of Valiant and Valiant, by simultaneously applying toall approximation requirements $\varepsilon$. In particular, it overcomesfactors depending on $\log n$ and, importantly, the minimum eigenvalue of thePMD's covariance matrix from the distance to a multidimensional Gaussian randomvariable.  We use our structural characterization to obtain an $\varepsilon$-cover, intotal variation distance, of the set of all $(n, k)$-PMDs, significantlyimproving the cover size of Daskalakis and Papadimitriou, and obtaining thesame qualitative dependence of the cover size on $n$ and $\varepsilon$ as the$k=2$ cover of Daskalakis and Papadimitriou. We further exploit this structureto show that $(n,k)$-PMDs can be learned to within $\varepsilon$ in totalvariation distance from $\tilde{O}_k(1/\varepsilon^2)$ samples, which isnear-optimal in terms of dependence on $\varepsilon$ and independent of $n$. Inparticular, our result generalizes the single-dimensional result of Daskalakis,Diakonikolas, and Servedio for Poisson Binomials to arbitrary dimension.

Faster Sublinear Algorithms using Conditional Sampling

  A conditional sampling oracle for a probability distribution D returnssamples from the conditional distribution of D restricted to a specified subsetof the domain. A recent line of work (Chakraborty et al. 2013 and Cannone etal. 2014) has shown that having access to such a conditional sampling oraclerequires only polylogarithmic or even constant number of samples to solvedistribution testing problems like identity and uniformity. This significantlyimproves over the standard sampling model where polynomially many samples arenecessary.  Inspired by these results, we introduce a computational model based onconditional sampling to develop sublinear algorithms with exponentially fasterruntimes compared to standard sublinear algorithms. We focus on geometricoptimization problems over points in high dimensional Euclidean space. Accessto these points is provided via a conditional sampling oracle that takes asinput a succinct representation of a subset of the domain and outputs auniformly random point in that subset. We study two well studied problems:k-means clustering and estimating the weight of the minimum spanning tree. Incontrast to prior algorithms for the classic model, our algorithms have time,space and sample complexity that is polynomial in the dimension andpolylogarithmic in the number of points.  Finally, we comment on the applicability of the model and compare withexisting ones like streaming, parallel and distributed computational models.

Ten Steps of EM Suffice for Mixtures of Two Gaussians

  The Expectation-Maximization (EM) algorithm is a widely used method formaximum likelihood estimation in models with latent variables. For estimatingmixtures of Gaussians, its iteration can be viewed as a soft version of thek-means clustering algorithm. Despite its wide use and applications, there areessentially no known convergence guarantees for this method. We provide globalconvergence guarantees for mixtures of two Gaussians with known covariancematrices. We show that the population version of EM, where the algorithm isgiven access to infinitely many samples from the mixture, convergesgeometrically to the correct mean vectors, and provide simple, closed-formexpressions for the convergence rate. As a simple illustration, we show that,in one dimension, ten steps of the EM algorithm initialized at infinity resultin less than 1\% error estimation of the means. In the finite sample regime, weshow that, under a random initialization, $\tilde{O}(d/\epsilon^2)$ samplessuffice to compute the unknown vectors to within $\epsilon$ in Mahalanobisdistance, where $d$ is the dimension. In particular, the error rate of the EMbased estimator is $\tilde{O}\left(\sqrt{d \over n}\right)$ where $n$ is thenumber of samples, which is optimal up to logarithmic factors.

A Converse to Banach's Fixed Point Theorem and its CLS Completeness

  Banach's fixed point theorem for contraction maps has been widely used toanalyze the convergence of iterative methods in non-convex problems. It is acommon experience, however, that iterative maps fail to be globally contractingunder the natural metric in their domain, making the applicability of Banach'stheorem limited. We explore how generally we can apply Banach's fixed pointtheorem to establish the convergence of iterative methods when pairing it withcarefully designed metrics.  Our first result is a strong converse of Banach's theorem, showing that it isa universal analysis tool for establishing global convergence of iterativemethods to unique fixed points, and for bounding their convergence rate. Inother words, we show that, whenever an iterative map globally converges to aunique fixed point, there exists a metric under which the iterative map iscontracting and which can be used to bound the number of iterations untilconvergence. We illustrate our approach in the widely used power method,providing a new way of bounding its convergence rate through contractionarguments.  We next consider the computational complexity of Banach's fixed pointtheorem. Making the proof of our converse theorem constructive, we show thatcomputing a fixed point whose existence is guaranteed by Banach's fixed pointtheorem is CLS-complete. We thus provide the first natural complete problem forthe class CLS, which was defined in [Daskalakis, Papadimitriou 2011] to capturethe complexity of problems such as P-matrix LCP, computing KKT-points, andfinding mixed Nash equilibria in congestion and network coordination games.

Certified Computation from Unreliable Datasets

  A wide range of learning tasks require human input in labeling massive data.The collected data though are usually low quality and contain inaccuracies anderrors. As a result, modern science and business face the problem of learningfrom unreliable data sets.  In this work, we provide a generic approach that is based on\textit{verification} of only few records of the data set to guarantee highquality learning outcomes for various optimization objectives. Our method,identifies small sets of critical records and verifies their validity. We showthat many problems only need $\text{poly}(1/\varepsilon)$ verifications, toensure that the output of the computation is at most a factor of $(1 \pm\varepsilon)$ away from the truth. For any given instance, we provide an\textit{instance optimal} solution that verifies the minimum possible number ofrecords to approximately certify correctness. Then using this instance optimalformulation of the problem we prove our main result: "every function thatsatisfies some Lipschitz continuity condition can be certified with a smallnumber of verifications". We show that the required Lipschitz continuitycondition is satisfied even by some NP-complete problems, which illustrates thegenerality and importance of this theorem.  In case this certification step fails, an invalid record will be identified.Removing these records and repeating until success, guarantees that the resultwill be accurate and will depend only on the verified records. Surprisingly, aswe show, for several computation tasks more efficient methods are possible.These methods always guarantee that the produced result is not affected by theinvalid records, since any invalid record that affects the output will bedetected and verified.

Anaconda: A Non-Adaptive Conditional Sampling Algorithm for Distribution  Testing

  We investigate distribution testing with access to non-adaptive conditionalsamples. In the conditional sampling model, the algorithm is given thefollowing access to a distribution: it submits a query set $S$ to an oracle,which returns a sample from the distribution conditioned on being from $S$. Inthe non-adaptive setting, all query sets must be specified in advance ofviewing the outcomes.  Our main result is the first polylogarithmic-query algorithm for equivalencetesting, deciding whether two unknown distributions are equal to or far fromeach other. This is an exponential improvement over the previous best upperbound, and demonstrates that the complexity of the problem in this model isintermediate to the the complexity of the problem in the standard samplingmodel and the adaptive conditional sampling model. We also significantlyimprove the sample complexity for the easier problems of uniformity andidentity testing. For the former, our algorithm requires only $\tilde O(\logn)$ queries, matching the information-theoretic lower bound up to a $O(\log\log n)$-factor.  Our algorithm works by reducing the problem from $\ell_1$-testing to$\ell_\infty$-testing, which enjoys a much cheaper sample complexity.Necessitated by the limited power of the non-adaptive model, our algorithm isvery simple to state. However, there are significant challenges in theanalysis, due to the complex structure of how two arbitrary distributions maydiffer.

Reasonable multi-item mechanisms are not much better than item pricing

  Multi-item mechanisms can be very complex offering many different bundles tothe buyer that could even be randomized. Such complexity is thought to benecessary as the revenue gaps between randomized and deterministic mechanisms,or deterministic and simple mechanisms are huge even for additive valuations.  We challenge this conventional belief by showing that these large gaps canonly happen in unrealistic situations. These are situations where the mechanismovercharges a buyer for a bundle while selling individual items at much lowerprices. Arguably this is impractical as the buyer can break his order intosmaller pieces paying a much lower price overall. Our main result is that ifthe buyer is allowed to purchase as many (randomized) bundles as he pleases,the revenue of any multi-item mechanism is at most O(logn) times the revenueachievable by item pricing, where n is the number of items. This holds in themost general setting possible, with an arbitrarily correlated distribution ofbuyer types and arbitrary valuations.  We also show that this result is tight in a very strong sense. Any family ofmechanisms of subexponential description complexity cannot achieve better thanlogarithmic approximation even against the best deterministic mechanism andeven for additive valuations. In contrast, item pricing that has lineardescription complexity matches this bound against randomized mechanisms.

Capacitated Dynamic Programming: Faster Knapsack and Graph Algorithms

  One of the most fundamental problems in Computer Science is the Knapsackproblem. Given a set of n items with different weights and values, it asks topick the most valuable subset whose total weight is below a capacity thresholdT. Despite its wide applicability in various areas in Computer Science,Operations Research, and Finance, the best known running time for the problemis O(Tn). The main result of our work is an improved algorithm running in timeO(TD), where D is the number of distinct weights. Previously, faster runtimesfor Knapsack were only possible when both weights and values are bounded by Mand V respectively, running in time O(nMV) [Pisinger'99]. In comparison, ouralgorithm implies a bound of O(nM^2) without any dependence on V, or O(nV^2)without any dependence on M. Additionally, for the unbounded Knapsack problem,we provide an algorithm running in time O(M^2) or O(V^2). Both our algorithmsmatch recent conditional lower bounds shown for the Knapsack problem [Cygan etal'17, K\"unnemann et al'17].  We also initiate a systematic study of general capacitated dynamicprogramming, of which Knapsack is a core problem. This problem asks to computethe maximum weight path of length k in an edge- or node-weighted directedacyclic graph. In a graph with m edges, these problems are solvable by dynamicprogramming in time O(km), and we explore under which conditions the dependenceon k can be eliminated. We identify large classes of graphs where this ispossible and apply our results to obtain linear time algorithms for the problemof k-sparse Delta-separated sequences. The main technical innovation behind ourresults is identifying and exploiting concavity that appears in relaxations andsubproblems of the tasks we consider.

