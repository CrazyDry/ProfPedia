Extended Formulations for Polytopes of Regular Matroids

  We present a simple proof of the fact that the base (and independence)polytope of a rank $n$ regular matroid over $m$ elements has an extensioncomplexity $O(mn)$.

On the Computational Complexity of Limit Cycles in Dynamical Systems

  We study the Poincare-Bendixson theorem for two-dimensional continuousdynamical systems in compact domains from the point of view of computation,seeking algorithms for finding the limit cycle promised by this classicalresult. We start by considering a discrete analogue of this theorem and showthat both finding a point on a limit cycle, and determining if a given point ison one, are PSPACE-complete.  For the continuous version, we show that both problems are uncomputable inthe real complexity sense; i.e., their complexity is arbitrarily high.Subsequently, we introduce a notion of an "approximate cycle" and prove an"approximate" Poincar\'e-Bendixson theorem guaranteeing that some orbits comevery close to forming a cycle in the absence of approximate fixpoints;surprisingly, it holds for all dimensions. The corresponding computationalproblem defined in terms of arithmetic circuits is PSPACE-complete.

On Geodesically Convex Formulations for the Brascamp-Lieb Constant

  We consider two non-convex formulations for computing the optimal constant inthe Brascamp-Lieb inequality corresponding to a given datum, and show that theyare geodesically log-concave on the manifold of positive definite matricesendowed with the Riemannian metric corresponding to the Hessian of thelog-determinant function. The first formulation is present in the work of Lieband the second is inspired by the work of Bennett et al. Recent works of Garget al.and Allen-Zhu et al. also imply a geodesically log-concave formulation ofthe Brascamp-Lieb constant through a reduction to the operator scaling problem.However, the dimension of the arising optimization problem in their reductiondepends exponentially on the number of bits needed to describe theBrascamp-Lieb datum. The formulations presented here have dimensions that arepolynomial in the bit complexity of the input datum.

Fair Online Advertising

  Online advertising platforms are thriving due to the customizable audiencesthey offer advertisers. However, recent studies show that the audience an adgets shown to can be discriminatory with respect to sensitive attributes suchas gender or ethnicity, inadvertently crossing ethical and/or legal boundaries.To prevent this, we propose a constrained optimization framework that allowsthe platform to control of the fraction of each sensitive type an advertiser'sad gets shown to while maximizing its ad revenues. Building upon Myerson'sclassic work, we first present an optimal auction mechanism for a large classof fairness constraints. Finding the parameters of this optimal auction,however, turns out to be a non-convex problem. We show how this non-convexproblem can be reformulated as a more structured non-convex problem with nosaddle points or local-maxima; allowing us to develop a gradient-descent-basedalgorithm to solve it. Our empirical results on the A1 Yahoo! datasetdemonstrate that our algorithm can obtain uniform coverage across differentuser attributes for each advertiser at a minor loss to the revenue of theplatform, and a small change in the total number of advertisements eachadvertiser shows on the platform.

On the Optimality of a Class of LP-based Algorithms

  In this paper we will be concerned with a class of packing and coveringproblems which includes Vertex Cover and Independent Set. Typically, one canwrite an LP relaxation and then round the solution. In this paper, we explainwhy the simple LP-based rounding algorithm for the \\VC problem is optimalassuming the UGC. Complementing Raghavendra's result, our result generalizes toa class of strict, covering/packing type CSPs.

$2^{\log^{1-\eps} n}$ Hardness for Closest Vector Problem with  Preprocessing

  We prove that for an arbitrarily small constant $\eps>0,$ assuming NP$\not\subseteq$DTIME$(2^{{\log^{O(1/\eps)} n}})$, the preprocessing versions of theclosest vector problem and the nearest codeword problem are hard to approximatewithin a factor better than $2^{\log ^{1-\eps}n}.$ This improves upon theprevious hardness factor of $(\log n)^\delta$ for some $\delta > 0$ due to\cite{AKKV05}.

Matrix Inversion Is As Easy As Exponentiation

  We prove that the inverse of a positive-definite matrix can be approximatedby a weighted-sum of a small number of matrix exponentials. Combining this witha previous result [OSV12], we establish an equivalence between matrix inversionand exponentiation up to polylogarithmic factors. In particular, thisconnection justifies the use of Laplacian solvers for designing fastsemi-definite programming based algorithms for certain graph problems. Theproof relies on the Euler-Maclaurin formula and certain bounds derived from theRiemann zeta function.

A quadratically tight partition bound for classical communication  complexity and query complexity

  In this work we introduce, both for classical communication complexity andquery complexity, a modification of the 'partition bound' introduced by Jainand Klauck [2010]. We call it the 'public-coin partition bound'. We show that(the logarithm to the base two of) its communication complexity and querycomplexity versions form, for all relations, a quadratically tight lower boundon the public-coin randomized communication complexity and randomized querycomplexity respectively.

The Mixing Time of the Dikin Walk in a Polytope - A Simple Proof

  We study the mixing time of the Dikin walk in a polytope - a random walkbased on the log-barrier from the interior point method literature. This walk,and a close variant, were studied by Narayanan (2016) and Kannan-Narayanan(2012). Bounds on its mixing time are important for algorithms for sampling andoptimization over polytopes. Here, we provide a simple proof of their resultthat this random walk mixes in time O(mn) for an n-dimensional polytopedescribed using m inequalities.

On Convex Programming Relaxations for the Permanent

  In recent years, several convex programming relaxations have been proposed toestimate the permanent of a non-negative matrix, notably in the works ofGurvits and Samorodnitsky. However, the origins of these relaxations and theirrelationships to each other have remained somewhat mysterious. We present aconceptual framework, implicit in the belief propagation literature, tosystematically arrive at these convex programming relaxations for estimatingthe permanent -- as approximations to an exponential-sized max-entropy convexprogram for computing the permanent. Further, using standard convex programmingtechniques such as duality, we establish equivalence of these aforementionedrelaxations to those based on capacity-like quantities studied by Gurvits andAnari et al.

Fair Personalization

  Personalization is pervasive in the online space as, when combined withlearning, it leads to higher efficiency and revenue by allowing the mostrelevant content to be served to each user. However, recent studies suggestthat such personalization can propagate societal or systemic biases, which hasled to calls for regulatory mechanisms and algorithms to combat inequality.Here we propose a rigorous algorithmic framework that allows for thepossibility to control biased or discriminatory personalization with respect tosensitive attributes of users without losing all of the benefits ofpersonalization.

Balanced News Using Constrained Bandit-based Personalization

  We present a prototype for a news search engine that presents balancedviewpoints across liberal and conservative articles with the goal ofde-polarizing content and allowing users to escape their filter bubble. Thebalancing is done according to flexible user-defined constraints, and leveragesrecent advances in constrained bandit optimization. We showcase our balancednews feed by displaying it side-by-side with the news feed produced by atraditional (polarized) feed.

On the Number of Circuits in Regular Matroids (with Connections to  Lattices and Codes)

  We show that for any regular matroid on $m$ elements and any $\alpha \geq 1$,the number of $\alpha$-minimum circuits, or circuits whose size is at most an$\alpha$-multiple of the minimum size of a circuit in the matroid is bounded by$m^{O(\alpha^2)}$. This generalizes a result of Karger for the number of$\alpha$-minimum cuts in a graph. As a consequence, we obtain similar bounds onthe number of $\alpha$-shortest vectors in "totally unimodular" lattices and onthe number of $\alpha$-minimum weight codewords in "regular" codes.

The Unique Games Conjecture, Integrality Gap for Cut Problems and  Embeddability of Negative Type Metrics into $\ell_1$

  In this paper, we disprove a conjecture of Goemans and Linial; namely, thatevery negative type metric embeds into $\ell_1$ with constant distortion. Weshow that for an arbitrarily small constant $\delta> 0$, for all large enough$n$, there is an $n$-point negative type metric which requires distortion atleast $(\log\log n)^{1/6-\delta}$ to embed into $\ell_1.$  Surprisingly, our construction is inspired by the Unique Games Conjecture(UGC) of Khot, establishing a previously unsuspected connection betweenprobabilistically checkable proof systems (PCPs) and the theory of metricembeddings. We first prove that the UGC implies a super-constant hardnessresult for the (non-uniform) Sparsest Cut problem. Though this hardness resultrelies on the UGC, we demonstrate, nevertheless, that the corresponding PCPreduction can be used to construct an "integrality gap instance" for SparsestCut. Towards this, we first construct an integrality gap instance for a naturalSDP relaxation of Unique Games. Then we "simulate" the PCP reduction and"translate" the integrality gap instance of Unique Games to an integrality gapinstance of Sparsest Cut. This enables us to prove a $(\log \logn)^{1/6-\delta}$ integrality gap for Sparsest Cut, which is known to beequivalent to the metric embedding lower bound.

Algorithms and Hardness for Subspace Approximation

  The subspace approximation problem Subspace($k$,$p$) asks for a$k$-dimensional linear subspace that fits a given set of points optimally,where the error for fitting is a generalization of the least squares fit anduses the $\ell_{p}$ norm instead. Most of the previous work on subspaceapproximation has focused on small or constant $k$ and $p$, using coresets andsampling techniques from computational geometry.  In this paper, extending another line of work based on convex relaxation androunding, we give a polynomial time algorithm, \emph{for any $k$ and any $p\geq 2$}, with the approximation guarantee roughly $\gamma_{p} \sqrt{2 -\frac{1}{n-k}}$, where $\gamma_{p}$ is the $p$-th moment of a standard normalrandom variable N(0,1). We show that the convex relaxation we use has anintegrality gap (or "rank gap") of $\gamma_{p} (1 - \epsilon)$, for anyconstant $\epsilon > 0$. Finally, we show that assuming the Unique GamesConjecture, the subspace approximation problem is hard to approximate within afactor better than $\gamma_{p} (1 - \epsilon)$, for any constant $\epsilon >0$.

A Distributed Learning Dynamics in Social Groups

  We study a distributed learning process observed in human groups and othersocial animals. This learning process appears in settings in which eachindividual in a group is trying to decide over time, in a distributed manner,which option to select among a shared set of options. Specifically, we considera stochastic dynamics in a group in which every individual selects an option inthe following two-step process: (1) select a random individual and observe theoption that individual chose in the previous time step, and (2) adopt thatoption if its stochastic quality was good at that time step. Variousinstantiations of such distributed learning appear in nature, and have alsobeen studied in the social science literature. From the perspective of anindividual, an attractive feature of this learning process is that it is asimple heuristic that requires extremely limited computational capacities. Butwhat does it mean for the group -- could such a simple, distributed andessentially memoryless process lead the group as a whole to perform optimally?We show that the answer to this question is yes -- this distributed learning ishighly effective at identifying the best option and is close to optimal for thegroup overall. Our analysis also gives quantitative bounds that show fastconvergence of these stochastic dynamics. Prior to our work the onlytheoretical work related to such learning dynamics has been either indeterministic special cases or in the asymptotic setting. Finally, we observethat our infinite population dynamics is a stochastic variant of the classicmultiplicative weights update (MWU) method. Consequently, we arrive at thefollowing interesting converse: the learning dynamics on a finite populationconsidered here can be viewed as a novel distributed and low-memoryimplementation of the classic MWU method.

Geodesic Convex Optimization: Differentiation on Manifolds, Geodesics,  and Convexity

  Convex optimization is a vibrant and successful area due to the existence ofa variety of efficient algorithms that leverage the rich structure provided byconvexity. Convexity of a smooth set or a function in a Euclidean space isdefined by how it interacts with the standard differential structure in thisspace -- the Hessian of a convex function has to be positive semi-definiteeverywhere. However, in recent years, there is a growing demand to understandnon-convexity and develop computational methods to optimize non-convexfunctions. Intriguingly, there is a type of non-convexity that disappears onceone introduces a suitable differentiable structure and redefines convexity withrespect to the straight lines, or {\em geodesics}, with respect to thisstructure. Such convexity is referred to as {\em geodesic convexity}. Interestin studying it arises due to recent reformulations of some non-convex problemsas geodesically convex optimization problems over geodesically convex sets.Geodesics on manifolds have been extensively studied in various branches ofMathematics and Physics. However, unlike convex optimization, understandinggeodesics and geodesic convexity from a computational point of view largelyremains a mystery. The goal of this exposition is to introduce the first partof geodesic convex optimization -- geodesic convexity -- in a self-containedmanner. We first present a variety of notions from differential and Riemanniangeometry such as differentiation on manifolds, geodesics, and then introducegeodesic convexity. We conclude by showing that certain non-convex optimizationproblems such as computing the Brascamp-Lieb constant and the operator scalingproblem have geodesically convex formulations.

Entropy, Optimization and Counting

  In this paper we study the problem of computing max-entropy distributionsover a discrete set of objects subject to observed marginals. Interest in suchdistributions arises due to their applicability in areas such as statisticalphysics, economics, biology, information theory, machine learning,combinatorics and, more recently, approximation algorithms. A key difficulty incomputing max-entropy distributions has been to show that they havepolynomially-sized descriptions. We show that such descriptions exist undergeneral conditions. Subsequently, we show how algorithms for (approximately)counting the underlying discrete set can be translated into efficientalgorithms to (approximately) compute max-entropy distributions. In the reversedirection, we show how access to algorithms that compute max-entropydistributions can be used to count, which establishes an equivalence betweencounting and computing max-entropy distributions.

On a Natural Dynamics for Linear Programming

  In this paper we study dynamics inspired by Physarum polycephalum (a slimemold) for solving linear programs [NTY00, IJNT11, JZ12]. These dynamics arearrived at by a local and mechanistic interpretation of the inner workings ofthe slime mold and a global optimization perspective has been lacking even inthe simplest of instances. Our first result is an interpretation of thedynamics as an optimization process. We show that Physarum dynamics can be seenas a steepest-descent type algorithm on a certain Riemannian manifold.Moreover, we prove that the trajectories of Physarum are in fact paths ofoptimizers to a parametrized family of convex programs, in which the objectiveis a linear cost function regularized by an entropy barrier. Subsequently, werigorously establish several important properties of solution curves ofPhysarum. We prove global existence of such solutions and show that they havelimits, being optimal solutions of the underlying LP. Finally, we show that thediscretization of the Physarum dynamics is efficient for a class of linearprograms, which include unimodular constraint matrices. Thus, together, ourresults shed some light on how nature might be solving instances of perhaps themost complex problem in P: linear programming.

IRLS and Slime Mold: Equivalence and Convergence

  In this paper we present a connection between two dynamical systems arisingin entirely different contexts: one in signal processing and the other inbiology. The first is the famous Iteratively Reweighted Least Squares (IRLS)algorithm used in compressed sensing and sparse recovery while the second isthe dynamics of a slime mold (Physarum polycephalum). Both of these dynamicsare geared towards finding a minimum l1-norm solution in an affine subspace.Despite its simplicity the convergence of the IRLS method has been shown onlyfor a certain regularization of it and remains an important open problem. Ourfirst result shows that the two dynamics are projections of the same dynamicalsystem in higher dimensions. As a consequence, and building on the recent workon Physarum dynamics, we are able to prove convergence and obtain complexitybounds for a damped version of the IRLS algorithm.

Subdeterminant Maximization via Nonconvex Relaxations and  Anti-concentration

  Several fundamental problems that arise in optimization and computer sciencecan be cast as follows: Given vectors $v_1,\ldots,v_m \in \mathbb{R}^d$ and aconstraint family ${\cal B}\subseteq 2^{[m]}$, find a set $S \in \cal{B}$ thatmaximizes the squared volume of the simplex spanned by the vectors in $S$. Amotivating example is the data-summarization problem in machine learning whereone is given a collection of vectors that represent data such as documents orimages. The volume of a set of vectors is used as a measure of their diversity,and partition or matroid constraints over $[m]$ are imposed in order to ensureresource or fairness constraints. Recently, Nikolov and Singh presented aconvex program and showed how it can be used to estimate the value of the mostdiverse set when ${\cal B}$ corresponds to a partition matroid. This result wasrecently extended to regular matroids in works of Straszak and Vishnoi, andAnari and Oveis Gharan. The question of whether these estimation algorithms canbe converted into the more useful approximation algorithms -- that also outputa set -- remained open.  The main contribution of this paper is to give the first approximationalgorithms for both partition and regular matroids. We present novelformulations for the subdeterminant maximization problem for these matroids;this reduces them to the problem of finding a point that maximizes the absolutevalue of a nonconvex function over a Cartesian product of probabilitysimplices. The technical core of our results is a new anti-concentrationinequality for dependent random variables that allows us to relate the optimalvalue of these nonconvex functions to their value at a random point. Unlikeprior work on the constrained subdeterminant maximization problem, our proofsdo not rely on real-stability or convexity and could be of independent interestboth in algorithms and complexity.

Towards an SDP-based Approach to Spectral Methods: A Nearly-Linear-Time  Algorithm for Graph Partitioning and Decomposition

  In this paper, we consider the following graph partitioning problem: Theinput is an undirected graph $G=(V,E),$ a balance parameter $b \in (0,1/2]$ anda target conductance value $\gamma \in (0,1).$ The output is a cut which, ifnon-empty, is of conductance at most $O(f),$ for some function $f(G, \gamma),$and which is either balanced or well correlated with all cuts of conductance atmost $\gamma.$ Spielman and Teng gave an $\tilde{O}(|E|/\gamma^{2})$-timealgorithm for $f= \sqrt{\gamma \log^{3}|V|}$ and used it to decompose graphsinto a collection of near-expanders. We present a new spectral algorithm forthis problem which runs in time $\tilde{O}(|E|/\gamma)$ for $f=\sqrt{\gamma}.$Our result yields the first nearly-linear time algorithm for the classicBalanced Separator problem that achieves the asymptotically optimalapproximation guarantee for spectral methods. Our method has the advantage ofbeing conceptually simple and relies on a primal-dual semidefinite-programmingSDP approach. We first consider a natural SDP relaxation for the BalancedSeparator problem. While it is easy to obtain from this SDP a certificate ofthe fact that the graph has no balanced cut of conductance less than $\gamma,$somewhat surprisingly, we can obtain a certificate for the stronger correlationcondition. This is achieved via a novel separation oracle for our SDP and byappealing to Arora and Kale's framework to bound the running time. Our resultcontains technical ingredients that may be of independent interest.

A Finite Population Model of Molecular Evolution: Theory and Computation

  This paper is concerned with the evolution of haploid organisms thatreproduce asexually. In a seminal piece of work, Eigen and coauthors proposedthe quasispecies model in an attempt to understand such an evolutionaryprocess. Their work has impacted antiviral treatment and vaccine designstrategies. Yet, predictions of the quasispecies model are at best viewed as aguideline, primarily because it assumes an infinite population size, whereasrealistic population sizes can be quite small. In this paper we consider apopulation genetics-based model aimed at understanding the evolution of suchorganisms with finite population sizes and present a rigorous study of theconvergence and computational issues that arise therein. Our first result isstructural and shows that, at any time during the evolution, as the populationsize tends to infinity, the distribution of genomes predicted by our modelconverges to that predicted by the quasispecies model. This justifies thecontinued use of the quasispecies model to derive guidelines for intervention.While the stationary state in the quasispecies model is readily obtained, dueto the explosion of the state space in our model, exact computations areprohibitive. Our second set of results are computational in nature and addressthis issue. We derive conditions on the parameters of evolution under which ourstochastic model mixes rapidly. Further, for a class of widely used fitnesslandscapes we give a fast deterministic algorithm which computes the stationarydistribution of our model. These computational tools are expected to serve as aframework for the modeling of strategies for the deployment of mutagenic drugs.

How to be Fair and Diverse?

  Due to the recent cases of algorithmic bias in data-driven decision-making,machine learning methods are being put under the microscope in order tounderstand the root cause of these biases and how to correct them. Here, weconsider a basic algorithmic task that is central in machine learning:subsampling from a large data set. Subsamples are used both as an end-goal indata summarization (where fairness could either be a legal, political or moralrequirement) and to train algorithms (where biases in the samples are often asource of bias in the resulting model). Consequently, there is a growing effortto modify either the subsampling methods or the algorithms themselves in orderto ensure fairness. However, in doing so, a question that seems to beoverlooked is whether it is possible to produce fair subsamples that are alsoadequately representative of the feature space of the data set - an importantand classic requirement in machine learning. Can diversity and fairness besimultaneously ensured? We start by noting that, in some applications,guaranteeing one does not necessarily guarantee the other, and a new approachis required. Subsequently, we present an algorithmic framework which allows usto produce both fair and diverse samples. Our experimental results on an imagesummarization task show marked improvements in fairness without compromisingfeature diversity by much, giving us the best of both the worlds.

Isolating a Vertex via Lattices: Polytopes with Totally Unimodular Faces

  We present a geometric approach towards derandomizing the Isolation Lemma byMulmuley, Vazirani, and Vazirani. In particular, our approach produces aquasi-polynomial family of weights, where each weight is an integer andquasi-polynomially bounded, that can isolate a vertex in any 0/1 polytope forwhich each face lies in an affine space defined by a totally unimodular matrix.This includes the polytopes given by totally unimodular constraints andgeneralizes the recent derandomization of the Isolation Lemma for bipartiteperfect matching and matroid intersection. We prove our result by associating alattice to each face of the polytope and showing that if there is a totallyunimodular kernel matrix for this lattice, then the number of vectors of lengthwithin 3/2 of the shortest vector in it is polynomially bounded. The proof ofthis latter geometric fact is combinatorial and follows from a polynomial boundon the number of circuits of size within 3/2 of the shortest circuit in aregular matroid. This is the technical core of the paper and relies on avariant of Seymour's decomposition theorem for regular matroids. It generalizesan influential result by Karger on the number of minimum cuts in a graph toregular matroids.

Belief Propagation, Bethe Approximation and Polynomials

  Factor graphs are important models for succinctly representing probabilitydistributions in machine learning, coding theory, and statistical physics.Several computational problems, such as computing marginals and partitionfunctions, arise naturally when working with factor graphs. Belief propagationis a widely deployed iterative method for solving these problems. However,despite its significant empirical success, not much is known about thecorrectness and efficiency of belief propagation.  Bethe approximation is an optimization-based framework for approximatingpartition functions. While it is known that the stationary points of the Betheapproximation coincide with the fixed points of belief propagation, in general,the relation between the Bethe approximation and the partition function is notwell understood. It has been observed that for a few classes of factor graphs,the Bethe approximation always gives a lower bound to the partition function,which distinguishes them from the general case, where neither a lower bound,nor an upper bound holds universally. This has been rigorously proved forpermanents and for attractive graphical models.  Here we consider bipartite normal factor graphs and show that if the localconstraints satisfy a certain analytic property, the Bethe approximation is alower bound to the partition function. We arrive at this result by viewingfactor graphs through the lens of polynomials. In this process, we reformulatethe Bethe approximation as a polynomial optimization problem. Our sufficientcondition for the lower bound property to hold is inspired by recentdevelopments in the theory of real stable polynomials. We believe that this wayof viewing factor graphs and its connection to real stability might lead to abetter understanding of belief propagation and factor graphs in general.

Multiwinner Voting with Fairness Constraints

  Multiwinner voting rules are used to select a small representative subset ofcandidates or items from a larger set given the preferences of voters. However,if candidates have sensitive attributes such as gender or ethnicity (whenselecting a committee), or specified types such as political leaning (whenselecting a subset of news items), an algorithm that chooses a subset byoptimizing a multiwinner voting rule may be unbalanced in its selection -- itmay under or over represent a particular gender or political orientation in theexamples above. We introduce an algorithmic framework for multiwinner votingproblems when there is an additional requirement that the selected subsetshould be "fair" with respect to a given set of attributes. Our frameworkprovides the flexibility to (1) specify fairness with respect to multiple,non-disjoint attributes (e.g., ethnicity and gender) and (2) specify a scorefunction. We study the computational complexity of this constrained multiwinnervoting problem for monotone and submodular score functions and present severalapproximation algorithms and matching hardness of approximation results forvarious attribute group structure and types of score functions. We also presentsimulations that suggest that adding fairness constraints may not affect thescores significantly when compared to the unconstrained case.

Convex Optimization with Unbounded Nonconvex Oracles using Simulated  Annealing

  We consider the problem of minimizing a convex objective function $F$ whenone can only evaluate its noisy approximation $\hat{F}$. Unless one assumessome structure on the noise, $\hat{F}$ may be an arbitrary nonconvex function,making the task of minimizing $F$ intractable. To overcome this, prior work hasoften focused on the case when $F(x)-\hat{F}(x)$ is uniformly-bounded. In thispaper we study the more general case when the noise has magnitude $\alpha F(x)+ \beta$ for some $\alpha, \beta > 0$, and present a polynomial time algorithmthat finds an approximate minimizer of $F$ for this noise model. Previously,Markov chains, such as the stochastic gradient Langevin dynamics, have beenused to arrive at approximate solutions to these optimization problems.However, for the noise model considered in this paper, no single temperatureallows such a Markov chain to both mix quickly and concentrate near the globalminimizer. We bypass this by combining "simulated annealing" with thestochastic gradient Langevin dynamics, and gradually decreasing the temperatureof the chain in order to approach the global minimizer. As a corollary one canapproximately minimize a nonconvex function that is close to a convex function;however, the closeness can deteriorate as one moves away from the optimum.

Fair and Diverse DPP-based Data Summarization

  Sampling methods that choose a subset of the data proportional to itsdiversity in the feature space are popular for data summarization. However,recent studies have noted the occurrence of bias (under- or over-representationof a certain gender or race) in such data summarization methods. In this paperwe initiate a study of the problem of outputting a diverse and fair summary ofa given dataset. We work with a well-studied determinantal measure of diversityand corresponding distributions (DPPs) and present a framework that allows usto incorporate a general class of fairness constraints into such distributions.Coming up with efficient algorithms to sample from these constraineddeterminantal distributions, however, suffers from a complexity barrier and wepresent a fast sampler that is provably good when the input vectors satisfy anatural property. Our experimental results on a real-world and an image datasetshow that the diversity of the samples produced by adding fairness constraintsis not too far from the unconstrained case, and we also provide a theoreticalexplanation of it.

An Algorithmic Framework to Control Bias in Bandit-based Personalization

  Personalization is pervasive in the online space as it leads to higherefficiency and revenue by allowing the most relevant content to be served toeach user. However, recent studies suggest that personalization methods canpropagate societal or systemic biases and polarize opinions; this has led tocalls for regulatory mechanisms and algorithms to combat bias and inequality.Algorithmically, bandit optimization has enjoyed great success in learning userpreferences and personalizing content or feeds accordingly. We propose analgorithmic framework that allows for the possibility to control bias ordiscrimination in such bandit-based personalization. Our model allows for thespecification of general fairness constraints on the sensitive types of thecontent that can be displayed to a user. The challenge, however, is to come upwith a scalable and low regret algorithm for the constrained optimizationproblem that arises. Our main technical contribution is a provably fast andlow-regret algorithm for the fairness-constrained bandit optimization problem.Our proofs crucially leverage the special structure of our problem. Experimentson synthetic and real-world data sets show that our algorithmic framework cancontrol bias with only a minor loss to revenue.

Dimensionally Tight Bounds for Second-Order Hamiltonian Monte Carlo

  Hamiltonian Monte Carlo (HMC) is a widely deployed method to sample fromhigh-dimensional distributions in Statistics and Machine learning. HMC is knownto run very efficiently in practice and its popular second-order "leapfrog"implementation has long been conjectured to run in $d^{1/4}$ gradientevaluations. Here we show that this conjecture is true when sampling fromstrongly log-concave target distributions that satisfy a weak third-orderregularity property associated with the input data. Our regularity condition isweaker than the Lipschitz Hessian property and allows us to show fasterconvergence bounds for a much larger class of distributions than would bepossible with the usual Lipschitz Hessian constant alone. Importantdistributions that satisfy our regularity condition include posteriordistributions used in Bayesian logistic regression for which the data satisfiesan "incoherence" property. Our result compares favorably with the bestavailable bounds for the class of strongly log-concave distributions, whichgrow like $d^{{1}/{2}}$ gradient evaluations with the dimension. Moreover, oursimulations on synthetic data suggest that, when our regularity condition issatisfied, leapfrog HMC performs better than its competitors -- both in termsof accuracy and in terms of the number of gradient evaluations it requires.

Classification with Fairness Constraints: A Meta-Algorithm with Provable  Guarantees

  Developing classification algorithms that are fair with respect to sensitiveattributes of the data has become an important problem due to the growingdeployment of classification algorithms in various social contexts. Severalrecent works have focused on fairness with respect to a specific metric,modeled the corresponding fair classification problem as a constrainedoptimization problem, and developed tailored algorithms to solve them. Despitethis, there still remain important metrics for which we do not have fairclassifiers and many of the aforementioned algorithms do not come withtheoretical guarantees; perhaps because the resulting optimization problem isnon-convex. The main contribution of this paper is a new meta-algorithm forclassification that takes as input a large class of fairness constraints, withrespect to multiple non-disjoint sensitive attributes, and which comes withprovable guarantees. This is achieved by first developing a meta-algorithm fora large family of classification problems with convex constraints, and thenshowing that classification problems with general types of fairness constraintscan be reduced to those in this family. We present empirical results that showthat our algorithm can achieve near-perfect fairness with respect to variousfairness metrics, and that the loss in accuracy due to the imposed fairnessconstraints is often small. Overall, this work unifies several prior works onfair classification, presents a practical algorithm with theoreticalguarantees, and can handle fairness metrics that were previously not possible.

Dynamic Sampling from Graphical Models

  In this paper, we study the problem of sampling from a graphical model whenthe model itself is changing dynamically with time. This problem derives itsinterest from a variety of inference, learning, and sampling settings inmachine learning, computer vision, statistical physics, and theoreticalcomputer science. While the problem of sampling from a static graphical modelhas received considerable attention, theoretical works for its dynamic variantshave been largely lacking. The main contribution of this paper is an algorithmthat can sample dynamically from a broad class of graphical models overdiscrete random variables. Our algorithm is parallel and Las Vegas: it knowswhen to stop and it outputs samples from the exact distribution. We alsoprovide sufficient conditions under which this algorithm runs in timeproportional to the size of the update, on general graphical models as well aswell-studied specific spin systems. In particular we obtain, for the Isingmodel (ferromagnetic or anti-ferromagnetic) and for the hardcore model thefirst dynamic sampling algorithms that can handle both edge and vertex updates(addition, deletion, change of functions), both efficient within regimes thatare close to the respective uniqueness regimes, beyond which, even for thestatic and approximate sampling, no local algorithms were known or the problemitself is intractable. Our dynamic sampling algorithm relies on a localresampling algorithm and a new "equilibrium" property that is shown to besatisfied by our algorithm at each step, and enables us to prove itscorrectness. This equilibrium property is robust enough to guarantee thecorrectness of our algorithm, helps us improve bounds on fast convergence onspecific models, and should be of independent interest.

Stable and Fair Classification

  Fair classification has been a topic of intense study in machine learning,and several algorithms have been proposed towards this important task. However,in a recent study, Friedler et al. observed that fair classification algorithmsmay not be stable with respect to variations in the training dataset -- acrucial consideration in several real-world applications. Motivated by theirwork, we study the problem of designing classification algorithms that are bothfair and stable. We propose an extended framework based on fair classificationalgorithms that are formulated as optimization problems, by introducing astability-focused regularization term. Theoretically, we prove a stabilityguarantee, that was lacking in fair classification algorithms, and also providean accuracy guarantee for our extended framework. Our accuracy guarantee can beused to inform the selection of the regularization parameter in our framework.To the best of our knowledge, this is the first work that combines stabilityand fairness in automated decision-making tasks. We assess the benefits of ourapproach empirically by extending several fair classification algorithms thatare shown to achieve the best balance between fairness and accuracy over theAdult dataset. Our empirical results show that our framework indeed improvesthe stability at only a slight sacrifice in accuracy.

Ranking with Fairness Constraints

  Ranking algorithms are deployed widely to order a set of items inapplications such as search engines, news feeds, and recommendation systems.Recent studies, however, have shown that, left unchecked, the output of rankingalgorithms can result in decreased diversity in the type of content presented,promote stereotypes, and polarize opinions. In order to address such issues, westudy the following variant of the traditional ranking problem when, inaddition, there are fairness or diversity constraints. Given a collection ofitems along with 1) the value of placing an item in a particular position inthe ranking, 2) the collection of sensitive attributes (such as gender, race,political opinion) of each item and 3) a collection of constraints that, foreach k, bound the number of items with each attribute that are allowed toappear in the top k positions of the ranking, the goal is to output a rankingthat maximizes the value with respect to the original rank quality metric whilerespecting the constraints. This problem encapsulates various well-studiedproblems related to bipartite and hypergraph matching as special cases andturns out to be hard to approximate even with simple constraints. Our maintechnical contributions are fast exact and approximation algorithms along withcomplementary hardness results that, together, come close to settling theapproximability of this constrained ranking maximization problem. Unlike priorwork on the constrained matching problems, our algorithm runs in linear time,even when the number of constraints is large, its approximation ratio does notdepend on the number of constraints, and it produces solutions with smallconstraint violations. Our results rely on insights about the constrainedmatching problem when the objective satisfies properties that appear in commonranking metrics such as Discounted Cumulative Gain, Spearman's rho orBradley-Terry.

A Local Spectral Method for Graphs: with Applications to Improving Graph  Partitions and Exploring Data Graphs Locally

  The second eigenvalue of the Laplacian matrix and its associated eigenvectorare fundamental features of an undirected graph, and as such they have foundwidespread use in scientific computing, machine learning, and data analysis. Inmany applications, however, graphs that arise have several \emph{local} regionsof interest, and the second eigenvector will typically fail to provideinformation fine-tuned to each local region. In this paper, we introduce alocally-biased analogue of the second eigenvector, and we demonstrate itsusefulness at highlighting local properties of data graphs in a semi-supervisedmanner. To do so, we first view the second eigenvector as the solution to aconstrained optimization problem, and we incorporate the local information asan additional constraint; we then characterize the optimal solution to this newproblem and show that it can be interpreted as a generalization of aPersonalized PageRank vector; and finally, as a consequence, we show that thesolution can be computed in nearly-linear time. In addition, we show that thislocally-biased vector can be used to compute an approximation to the bestpartition \emph{near} an input seed set in a manner analogous to the way inwhich the second eigenvector of the Laplacian can be used to obtain anapproximation to the best partition in the entire input graph. Such a primitiveis useful for identifying and refining clusters locally, as it allows us tofocus on a local region of interest in a semi-supervised manner. Finally, weprovide a detailed empirical evaluation of our method by showing how it canapplied to finding locally-biased sparse cuts around an input vertex seed setin social and information networks.

Approximating the Exponential, the Lanczos Method and an  \tilde{O}(m)-Time Spectral Algorithm for Balanced Separator

  We give a novel spectral approximation algorithm for the balanced separatorproblem that, given a graph G, a constant balance b \in (0,1/2], and aparameter \gamma, either finds an \Omega(b)-balanced cut of conductanceO(\sqrt(\gamma)) in G, or outputs a certificate that all b-balanced cuts in Ghave conductance at least \gamma, and runs in time \tilde{O}(m). This settlesthe question of designing asymptotically optimal spectral algorithms forbalanced separator. Our algorithm relies on a variant of the heat kernel randomwalk and requires, as a subroutine, an algorithm to compute \exp(-L)v where Lis the Laplacian of a graph related to G and v is a vector. Algorithms forcomputing the matrix-exponential-vector product efficiently comprise our nextset of results. Our main result here is a new algorithm which computes a goodapproximation to \exp(-A)v for a class of PSD matrices A and a given vector u,in time roughly \tilde{O}(m_A), where m_A is the number of non-zero entries ofA. This uses, in a non-trivial way, the result of Spielman and Teng oninverting SDD matrices in \tilde{O}(m_A) time. Finally, we prove e^{-x} can beuniformly approximated up to a small additive error, in a non-negative interval[a,b] with a polynomial of degree roughly \sqrt{b-a}. While this result is ofindependent interest in approximation theory, we show that, via the Lanczosmethod from numerical analysis, it yields a simple algorithm to compute\exp(-A)v for PSD matrices that runs in time roughly O(t_A \sqrt{||A||}), wheret_A is the time required for computation of the vector Aw for given vector w.As an application, we obtain a simple and practical algorithm, with outputconductance O(\sqrt(\gamma)), for balanced separator that runs in time\tilde{O}(m/\sqrt(\gamma)). This latter algorithm matches the running time, butimproves on the approximation guarantee of the algorithm by Andersen and Peres.

On the Complexity of Constrained Determinantal Point Processes

  Determinantal Point Processes (DPPs) are probabilistic models that arise inquantum physics and random matrix theory and have recently found numerousapplications in computer science. DPPs define distributions over subsets of agiven ground set, they exhibit interesting properties such as negativecorrelation, and, unlike other models, have efficient algorithms for sampling.When applied to kernel methods in machine learning, DPPs favor subsets of thegiven data with more diverse features. However, many real-world applicationsrequire efficient algorithms to sample from DPPs with additional constraints onthe subset, e.g., partition or matroid constraints that are important to ensurepriors, resource or fairness constraints on the sampled subset. Whether one canefficiently sample from DPPs in such constrained settings is an importantproblem that was first raised in a survey of DPPs by \cite{KuleszaTaskar12} andstudied in some recent works in the machine learning literature.  The main contribution of our paper is the first resolution of the complexityof sampling from DPPs with constraints. We give exact efficient algorithms forsampling from constrained DPPs when their description is in unary. Furthermore,we prove that when the constraints are specified in binary, this problem is#P-hard via a reduction from the problem of computing mixed discriminantsimplying that it may be unlikely that there is an FPRAS. Our results benefitfrom viewing the constrained sampling problem via the lens of polynomials.Consequently, we obtain a few algorithms of independent interest: 1) to countover the base polytope of regular matroids when there are additional (succinct)budget constraints and, 2) to evaluate and compute the mixed characteristicpolynomials, that played a central role in the resolution of the Kadison-Singerproblem, for certain special cases.

Real Stable Polynomials and Matroids: Optimization and Counting

  A great variety of fundamental optimization and counting problems arising incomputer science, mathematics and physics can be reduced to one of thefollowing computational tasks involving polynomials and set systems: given an$m$-variate real polynomial $g$ and a family of subsets $B$ of $[m]$, (1) find$S\in B$ such that the monomial in $g$ corresponding to $S$ has the largestcoefficient in $g$, or (2) compute the sum of coefficients of monomials in $g$corresponding to all the sets in $B$. Special cases of these problems, such ascomputing permanents, sampling from DPPs and maximizing subdeterminants havebeen topics of recent interest in theoretical computer science.  In this paper we present a general convex programming framework geared tosolve both of these problems. We show that roughly, when $g$ is a real stablepolynomial with non-negative coefficients and $B$ is a matroid, the integralitygap of our relaxation is finite and depends only on $m$ (and not on thecoefficients of g).  Prior to our work, such results were known only in sporadic cases that reliedon the structure of $g$ and $B$; it was not even clear if one could formulate aconvex relaxation that has a finite integrality gap beyond these special cases.Two notable examples are a result by Gurvits on the van der Waerden conjecturefor real stable $g$ when $B$ is a single element and a result by Nikolov andSingh for multilinear real stable polynomials when $B$ is a partition matroid.Our work, which encapsulates most interesting cases of $g$ and $B$, benefitsfrom both - we were inspired by the latter in deriving the right convexprogramming relaxation and the former in establishing the integrality gap.However, proving our results requires significant extensions of both; in thatprocess we come up with new notions and connections between stable polynomialsand matroids which should be of independent interest.

Computing Maximum Entropy Distributions Everywhere

  We study the problem of computing the maximum entropy distribution with aspecified expectation over a large discrete domain. Maximum entropydistributions arise and have found numerous applications in economics, machinelearning and various sub-disciplines of mathematics and computer science. Thekey computational questions related to maximum entropy distributions arewhether they have succinct descriptions and whether they can be efficientlycomputed. Here we provide positive answers to both of these questions for verygeneral domains and, importantly, with no restriction on the expectation. Thiscompletes the picture left open by the prior work on this problem whichrequires that the expectation vector is polynomially far in the interior of theconvex hull of the domain. As a consequence we obtain a general algorithmictool and show how it can be applied to derive several old and new results in aunified manner. In particular, our results imply that certain recent continuousoptimization formulations, for instance, for discrete counting and optimizationproblems, the matrix scaling problem, and the worst case Brascamp-Liebconstants in the rank-1 regime, are efficiently computable. Attaining theseimplications requires reformulating the underlying problem as a version ofmaximum entropy computation where optimization also involves the expectationvector and, hence, cannot be assumed to be sufficiently deep in the interior.The key new technical ingredient in our work is a polynomial bound on the bitcomplexity of near-optimal dual solutions to the maximum entropy convexprogram. This result is obtained by a geometrical reasoning that involvesconvex analysis and polyhedral geometry, avoiding combinatorial arguments basedon the specific structure of the domain. We also provide a lower bound on thebit complexity of near-optimal solutions showing the tightness of our results.

Online Sampling from Log-Concave Distributions

  Given a sequence of convex functions $f_0, f_1, \ldots, f_T$, we study theproblem of sampling from the Gibbs distribution $\pi_t \propto e^{-\sum_{k=0}^tf_k}$ for each epoch $t$ in an online manner. This problem occurs inapplications to machine learning, Bayesian statistics, and optimization whereone constantly acquires new data, and must continuously update thedistribution. Our main result is an algorithm that generates independentsamples from a distribution that is a fixed $\varepsilon$ TV-distance from$\pi_t$ for every $t$ and, under mild assumptions on the functions, makespoly$\log(T)$ gradient evaluations per epoch. All previous results for thisproblem imply a bound on the number of gradient or function evaluations whichis at least linear in $T$. While we assume the functions have bounded secondmoment, we do not assume strong convexity. In particular, we show that ourassumptions hold for online Bayesian logistic regression, when the data satisfynatural regularity properties. In simulations, our algorithm achieves accuracycomparable to that of a Markov chain specialized to logistic regression. Ourmain result also implies the first algorithm to sample from a $d$-dimensionallog-concave distribution $\pi_T \propto e^{-\sum_{k=0}^T f_k}$ where the$f_k$'s are not assumed to be strongly convex and the total number of gradientevaluations is roughly $T\log(T)+\mathrm{poly}(d),$ as opposed to $T\cdot\mathrm{poly}(d)$ implied by prior works. Key to our algorithm is a novelstochastic gradient Langevin dynamics Markov chain that has a carefullydesigned variance reduction step built-in with fixed constant batch size.Technically, lack of strong convexity is a significant barrier to the analysis,and, here, our main contribution is a martingale exit time argument showing thechain is constrained to a ball of radius roughly poly$\log(T)$ for the durationof the algorithm.

Nonconvex sampling with the Metropolis-adjusted Langevin algorithm

  The Langevin Markov chain algorithms are widely deployed methods to samplefrom distributions in challenging high-dimensional and non-convex statisticsand machine learning applications. Despite this, current bounds for theLangevin algorithms are slower than those of competing algorithms in manyimportant situations, for instance when sampling from weakly log-concavedistributions, or when sampling or optimizing non-convex log-densities. In thispaper, we obtain improved bounds in many of these situations, showing that theMetropolis-adjusted Langevin algorithm (MALA) is faster than the best boundsfor its competitor algorithms when the target distribution satisfies weakthird- and fourth- order regularity properties associated with the input data.In many settings, our regularity conditions are weaker than the usual Euclideanoperator norm regularity properties, allowing us to show faster bounds for amuch larger class of distributions than would be possible with the usualEuclidean operator norm approach, including in statistics and machine learningapplications where the data satisfy a certain incoherence condition. Inparticular, we show that using our regularity conditions one can obtain fasterbounds for applications which include sampling problems in Bayesian logisticregression with weakly convex priors, and the nonconvex optimization problem oflearning linear classifiers with zero-one loss functions.  Our main technical contribution in this paper is our analysis of theMetropolis acceptance probability of MALA in terms of its "energy-conservationerror," and our bound for this error in terms of third- and fourth- orderregularity conditions. Our combination of this higher-order analysis of theenergy conservation error with the conductance method is key to obtainingbounds which have a sub-linear dependence on the dimension $d$ in thenon-strongly logconcave setting.

