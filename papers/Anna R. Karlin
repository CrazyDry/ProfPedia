Approaching Utopia: Strong Truthfulness and Externality-Resistant
  Mechanisms

  We introduce and study strongly truthful mechanisms and their applications.
We use strongly truthful mechanisms as a tool for implementation in undominated
strategies for several problems,including the design of externality resistant
auctions and a variant of multi-dimensional scheduling.


Integrality Gaps of Linear and Semi-definite Programming Relaxations for
  Knapsack

  In this paper, we study the integrality gap of the Knapsack linear program in
the Sherali- Adams and Lasserre hierarchies. First, we show that an integrality
gap of 2 - {\epsilon} persists up to a linear number of rounds of
Sherali-Adams, despite the fact that Knapsack admits a fully polynomial time
approximation scheme [27,33]. Second, we show that the Lasserre hierarchy
closes the gap quickly. Specifically, after t rounds of Lasserre, the
integrality gap decreases to t/(t - 1). To the best of our knowledge, this is
the first positive result that uses more than a small number of rounds in the
Lasserre hierarchy. Our proof uses a decomposition theorem for the Lasserre
hierarchy, which may be of independent interest.


A Simply Exponential Upper Bound on the Maximum Number of Stable
  Matchings

  Stable matching is a classical combinatorial problem that has been the
subject of intense theoretical and empirical study since its introduction in
1962 in a seminal paper by Gale and Shapley. In this paper, we provide a new
upper bound on $f(n)$, the maximum number of stable matchings that a stable
matching instance with $n$ men and $n$ women can have. It has been a
long-standing open problem to understand the asymptotic behavior of $f(n)$ as
$n\to\infty$, first posed by Donald Knuth in the 1970s. Until now the best
lower bound was approximately $2.28^n$, and the best upper bound was $2^{n\log
n- O(n)}$. In this paper, we show that for all $n$, $f(n) \leq c^n$ for some
universal constant $c$. This matches the lower bound up to the base of the
exponent. Our proof is based on a reduction to counting the number of downsets
of a family of posets that we call "mixing". The latter might be of independent
interest.


On Revenue Maximization in Second-Price Ad Auctions

  Most recent papers addressing the algorithmic problem of allocating
advertisement space for keywords in sponsored search auctions assume that
pricing is done via a first-price auction, which does not realistically model
the Generalized Second Price (GSP) auction used in practice. Towards the goal
of more realistically modeling these auctions, we introduce the Second-Price Ad
Auctions problem, in which bidders' payments are determined by the GSP
mechanism. We show that the complexity of the Second-Price Ad Auctions problem
is quite different than that of the more studied First-Price Ad Auctions
problem. First, unlike the first-price variant, for which small constant-factor
approximations are known, it is NP-hard to approximate the Second-Price Ad
Auctions problem to any non-trivial factor. Second, this discrepancy extends
even to the 0-1 special case that we call the Second-Price Matching problem
(2PM). In particular, offline 2PM is APX-hard, and for online 2PM there is no
deterministic algorithm achieving a non-trivial competitive ratio and no
randomized algorithm achieving a competitive ratio better than 2. This stands
in contrast to the results for the analogous special case in the first-price
model, the standard bipartite matching problem, which is solvable in polynomial
time and which has deterministic and randomized online algorithms achieving
better competitive ratios. On the positive side, we provide a 2-approximation
for offline 2PM and a 5.083-competitive randomized algorithm for online 2PM.
The latter result makes use of a new generalization of a classic result on the
performance of the "Ranking" algorithm for online bipartite matching.


Stability of Service under Time-of-Use Pricing

  We consider "time-of-use" pricing as a technique for matching supply and
demand of temporal resources with the goal of maximizing social welfare.
Relevant examples include energy, computing resources on a cloud computing
platform, and charging stations for electric vehicles, among many others. A
client/job in this setting has a window of time during which he needs service,
and a particular value for obtaining it. We assume a stochastic model for
demand, where each job materializes with some probability via an independent
Bernoulli trial. Given a per-time-unit pricing of resources, any realized job
will first try to get served by the cheapest available resource in its window
and, failing that, will try to find service at the next cheapest available
resource, and so on. Thus, the natural stochastic fluctuations in demand have
the potential to lead to cascading overload events. Our main result shows that
setting prices so as to optimally handle the {\em expected} demand works well:
with high probability, when the actual demand is instantiated, the system is
stable and the expected value of the jobs served is very close to that of the
optimal offline algorithm.


How to sell an app: pay-per-play or buy-it-now?

  We consider pricing in settings where a consumer discovers his value for a
good only as he uses it, and the value evolves with each use. We explore simple
and natural pricing strategies for a seller in this setting, under the
assumption that the seller knows the distribution from which the consumer's
initial value is drawn, as well as the stochastic process that governs the
evolution of the value with each use.
  We consider the differences between up-front or "buy-it-now" pricing (BIN),
and "pay-per-play" (PPP) pricing, where the consumer is charged per use. Our
results show that PPP pricing can be a very effective mechanism for price
discrimination, and thereby can increase seller revenue. But it can also be
advantageous to the buyers, as a way of mitigating risk. Indeed, this
mitigation of risk can yield a larger pool of buyers. We also show that the
practice of offering free trials is largely beneficial.
  We consider two different stochastic processes for how the buyer's value
evolves: In the first, the key random variable is how long the consumer remains
interested in the product. In the second process, the consumer's value evolves
according to a random walk or Brownian motion with reflection at 1, and
absorption at 0.


Combinatorial Auctions with Interdependent Valuations: SOS to the Rescue

  We study combinatorial auctions with interdependent valuations. In such
settings, each agent $i$ has a private signal $s_i$ that captures her private
information, and the valuation function of every agent depends on the entire
signal profile, ${\bs}=(s_1,\ldots,s_n)$. Previous results in economics
concentrated on identifying (often stringent) assumptions under which optimal
solutions can be attained. The computer science literature provided
approximation results for simple single-parameter settings (mostly single item
auctions, or matroid feasibility constraints). Both bodies of literature
considered only valuations satisfying a technical condition termed {\em single
crossing} (or variants thereof). Indeed, without imposing assumptions on the
valuations, strong impossibility results exist.
  We consider the class of {\em submodular over signals} (SOS) valuations
(without imposing any single-crossing type assumption), and provide the first
welfare approximation guarantees for multi-dimensional combinatorial auctions,
achieved by universally ex-post IC-IR mechanisms. Our main results are: $(i)$
4-approximation for any single-parameter downward-closed setting with
single-dimensional signals and SOS valuations; $(ii)$ 4-approximation for any
combinatorial auction with multi-dimensional signals and {\em separable}-SOS
valuations; and $(iii)$ $(k+3)$- and $(2\log(k)+4)$-approximation for any
combinatorial auction with single-dimensional signals, with $k$-sized signal
space, for SOS and strong-SOS valuations, respectively. All of our results
extend to a parameterized version of SOS, $d$-SOS, while losing a factor that
depends on $d$.


Thinking Twice about Second-Price Ad Auctions

  Recent work has addressed the algorithmic problem of allocating advertisement
space for keywords in sponsored search auctions so as to maximize revenue, most
of which assume that pricing is done via a first-price auction. This does not
realistically model the Generalized Second Price (GSP) auction used in
practice, in which bidders pay the next-highest bid for keywords that they are
allocated. Towards the goal of more realistically modeling these auctions, we
introduce the Second-Price Ad Auctions problem, in which bidders' payments are
determined by the GSP mechanism. We show that the complexity of the
Second-Price Ad Auctions problem is quite different than that of the more
studied First-Price Ad Auctions problem. First, unlike the first-price variant,
for which small constant-factor approximations are known, it is NP-hard to
approximate the Second-Price Ad Auctions problem to any non-trivial factor,
even when the bids are small compared to the budgets. Second, this discrepancy
extends even to the 0-1 special case that we call the Second-Price Matching
problem (2PM). Offline 2PM is APX-hard, and for online 2PM there is no
deterministic algorithm achieving a non-trivial competitive ratio and no
randomized algorithm achieving a competitive ratio better than 2. This
contrasts with the results for the analogous special case in the first-price
model, the standard bipartite matching problem, which is solvable in polynomial
time and which has deterministic and randomized online algorithms achieving
better competitive ratios. On the positive side, we provide a 2-approximation
for offline 2PM and a 5.083-competitive randomized algorithm for online 2PM.
The latter result makes use of a new generalization of a result on the
performance of the "Ranking" algorithm for online bipartite matching.


