On the Uniqueness of Group Sparse Coding

  In this technical document we present a proof on the uniqueness of groupsparse coding through the block ACS theorem. Leveraging the original ACStheorem of Hillar and Sommer for sparse coding, we demonstrate a similaruniqueness property holds for the task of group sparse coding.

Inverse Compositional Spatial Transformer Networks

  In this paper, we establish a theoretical connection between the classicalLucas & Kanade (LK) algorithm and the emerging topic of Spatial TransformerNetworks (STNs). STNs are of interest to the vision and learning communitiesdue to their natural ability to combine alignment and classification within thesame theoretical framework. Inspired by the Inverse Compositional (IC) variantof the LK algorithm, we present Inverse Compositional Spatial TransformerNetworks (IC-STNs). We demonstrate that IC-STNs can achieve better performancethan conventional STNs with less model capacity; in particular, we showsuperior performance in pure image alignment tasks as well as jointalignment/classification problems on real-world problems.

Fast, Dense Feature SDM on an iPhone

  In this paper, we present our method for enabling dense SDM to run at over 90FPS on a mobile device. Our contributions are two-fold. Drawing inspirationfrom the FFT, we propose a Sparse Compositional Regression (SCR) framework,which enables a significant speed up over classical dense regressors. Second,we propose a binary approximation to SIFT features. Binary Approximated SIFT(BASIFT) features, which are a computationally efficient approximation to SIFT,a commonly used feature with SDM. We demonstrate the performance of ouralgorithm on an iPhone 7, and show that we achieve similar accuracy to SDM.

Aligning Across Large Gaps in Time

  We present a method of temporally-invariant image registration for outdoorscenes, with invariance across time of day, across seasonal variations, andacross decade-long periods, for low- and high-texture scenes. Our method can beuseful for applications in remote sensing, GPS-denied UAV localization, 3Dreconstruction, and many others. Our method leverages a recently proposedapproach to image registration, where fully-convolutional neural networks areused to create feature maps which can be registered using theInverse-Composition Lucas-Kanade algorithm (ICLK). We show that invariance thatis learned from satellite imagery can be transferable to time-lapse datacaptured by webcams mounted on buildings near ground-level.

Deep Convolutional Compressed Sensing for LiDAR Depth Completion

  In this paper we consider the problem of estimating a dense depth map from aset of sparse LiDAR points. We use techniques from compressed sensing and therecently developed Alternating Direction Neural Networks (ADNNs) to create adeep recurrent auto-encoder for this task. Our architecture internally performsan algorithm for extracting multi-level convolutional sparse codes from theinput which are then used to make a prediction. Our results demonstrate thatwith only two layers and 1800 parameters we are able to out perform allpreviously published results, including deep networks with orders of magnitudemore parameters.

Learning Temporal Alignment Uncertainty for Efficient Event Detection

  In this paper we tackle the problem of efficient video event detection. Weargue that linear detection functions should be preferred in this regard due totheir scalability and efficiency during estimation and evaluation. A popularapproach in this regard is to represent a sequence using a bag of words (BOW)representation due to its: (i) fixed dimensionality irrespective of thesequence length, and (ii) its ability to compactly model the statistics in thesequence. A drawback to the BOW representation, however, is the intrinsicdestruction of the temporal ordering information. In this paper we propose anew representation that leverages the uncertainty in relative temporalalignments between pairs of sequences while not destroying temporal ordering.Our representation, like BOW, is of a fixed dimensionality making it easilyintegrated with a linear detection function. Extensive experiments on CK+,6DMG, and UvA-NEMO databases show significant performance improvements acrossboth isolated and continuous event detection tasks.

Joint Max Margin and Semantic Features for Continuous Event Detection in  Complex Scenes

  In this paper the problem of complex event detection in the continuous domain(i.e. events with unknown starting and ending locations) is addressed. Existingevent detection methods are limited to features that are extracted from thelocal spatial or spatio-temporal patches from the videos. However, this makesthe model vulnerable to the events with similar concepts e.g. "Open drawer" and"Open cupboard". In this work, in order to address the aforementionedlimitations we present a novel model based on the combination of semantic andtemporal features extracted from video frames. We train a max-margin classifieron top of the extracted features in an adaptive framework that is able todetect the events with unknown starting and ending locations. Our model isbased on the Bidirectional Region Neural Network and large margin StructuralOutput SVM. The generality of our model allows it to be simply applied todifferent labeled and unlabeled datasets. We finally test our algorithm onthree challenging datasets, "UCF 101-Action Recognition", "MPII CookingActivities" and "Hollywood", and we report state-of-the-art performance.

Learning detectors quickly using structured covariance matrices

  Computer vision is increasingly becoming interested in the rapid estimationof object detectors. Canonical hard negative mining strategies are slow as theyrequire multiple passes of the large negative training set. Recent work hasdemonstrated that if the distribution of negative examples is assumed to bestationary, then Linear Discriminant Analysis (LDA) can learn comparabledetectors without ever revisiting the negative set. Even with this insight,however, the time to learn a single object detector can still be on the orderof tens of seconds on a modern desktop computer. This paper proposes toleverage the resulting structured covariance matrix to obtain detectors withidentical performance in orders of magnitude less time and memory. We elucidatean important connection to the correlation filter literature, demonstratingthat these can also be trained without ever revisiting the negative set.

Correlation Filters with Limited Boundaries

  Correlation filters take advantage of specific properties in the Fourierdomain allowing them to be estimated efficiently: O(NDlogD) in the frequencydomain, versus O(D^3 + ND^2) spatially where D is signal length, and N is thenumber of signals. Recent extensions to correlation filters, such as MOSSE,have reignited interest of their use in the vision community due to theirrobustness and attractive computational properties. In this paper wedemonstrate, however, that this computational efficiency comes at a cost.Specifically, we demonstrate that only 1/D proportion of shifted examples areunaffected by boundary effects which has a dramatic effect ondetection/tracking performance. In this paper, we propose a novel approach tocorrelation filter estimation that: (i) takes advantage of inherentcomputational redundancies in the frequency domain, and (ii) dramaticallyreduces boundary effects. Impressive object tracking and detection results arepresented in terms of both accuracy and computational efficiency.

Optimization Methods for Convolutional Sparse Coding

  Sparse and convolutional constraints form a natural prior for manyoptimization problems that arise from physical processes. Detecting motifs inspeech and musical passages, super-resolving images, compressing videos, andreconstructing harmonic motions can all leverage redundancies introduced byconvolution. Solving problems involving sparse and convolutional constraintsremains a difficult computational problem, however. In this paper we present anoverview of convolutional sparse coding in a consistent framework. Theobjective involves iteratively optimizing a convolutional least-squares termfor the basis functions, followed by an L1-regularized least squares term forthe sparse coefficients. We discuss a range of optimization methods for solvingthe convolutional sparse coding objective, and the properties that make eachmethod suitable for different applications. In particular, we concentrate oncomputational complexity, speed to {\epsilon} convergence, memory usage, andthe effect of implied boundary conditions. We present a broad suite of examplescovering different signal and application domains to illustrate the generalapplicability of convolutional sparse coding, and the efficacy of the availableoptimization methods.

Why do linear SVMs trained on HOG features perform so well?

  Linear Support Vector Machines trained on HOG features are now a de factostandard across many visual perception tasks. Their popularisation can largelybe attributed to the step-change in performance they brought to pedestriandetection, and their subsequent successes in deformable parts models. Thispaper explores the interactions that make the HOG-SVM symbiosis perform sowell. By connecting the feature extraction and learning processes rather thantreating them as disparate plugins, we show that HOG features can be viewed asdoing two things: (i) inducing capacity in, and (ii) adding prior to a linearSVM trained on pixels. From this perspective, preserving second-orderstatistics and locality of interactions are key to good performance. Wedemonstrate surprising accuracy on expression recognition and pedestriandetection tasks, by assuming only the importance of preserving such localsecond-order interactions.

Regression-Based Image Alignment for General Object Categories

  Gradient-descent methods have exhibited fast and reliable performance forimage alignment in the facial domain, but have largely been ignored by thebroader vision community. They require the image function be smooth and(numerically) differentiable -- properties that hold for pixel-basedrepresentations obeying natural image statistics, but not for more generalclasses of non-linear feature transforms. We show that transforms such as DenseSIFT can be incorporated into a Lucas Kanade alignment framework by predictingdescent directions via regression. This enables robust matching of instancesfrom general object categories whilst maintaining desirable properties of LucasKanade such as the capacity to handle high-dimensional warp parametrizationsand a fast rate of convergence. We present alignment results on a number ofobjects from ImageNet, and an extension of the method to unsupervised jointalignment of objects from a corpus of images.

Dense Semantic Correspondence where Every Pixel is a Classifier

  Determining dense semantic correspondences across objects and scenes is adifficult problem that underpins many higher-level computer vision algorithms.Unlike canonical dense correspondence problems which consider images that arespatially or temporally adjacent, semantic correspondence is characterized byimages that share similar high-level structures whose exact appearance andgeometry may differ.  Motivated by object recognition literature and recent work on rapidlyestimating linear classifiers, we treat semantic correspondence as aconstrained detection problem, where an exemplar LDA classifier is learned foreach pixel. LDA classifiers have two distinct benefits: (i) they exhibit higheraverage precision than similarity metrics typically used in correspondenceproblems, and (ii) unlike exemplar SVM, can output globally interpretableposterior probabilities without calibration, whilst also being significantlyfaster to train.  We pose the correspondence problem as a graphical model, where the unarypotentials are computed via convolution with the set of exemplar classifiers,and the joint potentials enforce smoothly varying correspondence assignment.

Bit-Planes: Dense Subpixel Alignment of Binary Descriptors

  Binary descriptors have been instrumental in the recent evolution ofcomputationally efficient sparse image alignment algorithms. Increasingly,however, the vision community is interested in dense image alignment methods,which are more suitable for estimating correspondences from high frame ratecameras as they do not rely on exhaustive search. However, classic densealignment approaches are sensitive to illumination change. In this paper, wepropose an easy to implement and low complexity dense binary descriptor, whichwe refer to as bit-planes, that can be seamlessly integrated within amulti-channel Lucas & Kanade framework. This novel approach combines therobustness of binary descriptors with the speed and accuracy of dense alignmentmethods. The approach is demonstrated on a template tracking problem achievingstate-of-the-art robustness and faster than real-time performance on consumerlaptops (400+ fps on a single core Intel i7) and hand-held mobile devices (100+fps on an iPad Air 2).

Photometric Bundle Adjustment for Vision-Based SLAM

  We propose a novel algorithm for the joint refinement of structure and motionparameters from image data directly without relying on fixed and knowncorrespondences. In contrast to traditional bundle adjustment (BA) where theoptimal parameters are determined by minimizing the reprojection error usingtracked features, the proposed algorithm relies on maximizing the photometricconsistency and estimates the correspondences implicitly. Since the proposedalgorithm does not require correspondences, its application is not limited tocorner-like structure; any pixel with nonvanishing gradient could be used inthe estimation process. Furthermore, we demonstrate the feasibility of refiningthe motion and structure parameters simultaneously using the photometric inunconstrained scenes and without requiring restrictive assumptions such asplanarity. The proposed algorithm is evaluated on range of challenging outdoordatasets, and it is shown to improve upon the accuracy of the state-of-the-artVSLAM methods obtained using the minimization of the reprojection error usingtraditional BA as well as loop closure.

Learning Background-Aware Correlation Filters for Visual Tracking

  Correlation Filters (CFs) have recently demonstrated excellent performance interms of rapidly tracking objects under challenging photometric and geometricvariations. The strength of the approach comes from its ability to efficientlylearn - "on the fly" - how the object is changing over time. A fundamentaldrawback to CFs, however, is that the background of the object is not bemodelled over time which can result in suboptimal results. In this paper wepropose a Background-Aware CF that can model how both the foreground andbackground of the object varies over time. Our approach, like conventional CFs,is extremely computationally efficient - and extensive experiments overmultiple tracking benchmarks demonstrate the superior accuracy and real-timeperformance of our method compared to the state-of-the-art trackers includingthose based on a deep learning paradigm.

Deep-LK for Efficient Adaptive Object Tracking

  In this paper we present a new approach for efficient regression based objecttracking which we refer to as Deep- LK. Our approach is closely related to theGeneric Object Tracking Using Regression Networks (GOTURN) framework of Held etal. We make the following contributions. First, we demonstrate that there is atheoretical relationship between siamese regression networks like GOTURN andthe classical Inverse-Compositional Lucas & Kanade (IC-LK) algorithm. Further,we demonstrate that unlike GOTURN IC-LK adapts its regressor to the appearanceof the currently tracked frame. We argue that this missing property in GOTURNcan be attributed to its poor performance on unseen objects and/or viewpoints.Second, we propose a novel framework for object tracking - which we refer to asDeep-LK - that is inspired by the IC-LK framework. Finally, we show impressiveresults demonstrating that Deep-LK substantially outperforms GOTURN.Additionally, we demonstrate comparable tracking performance to current stateof the art deep-trackers whilst being an order of magnitude (i.e. 100 FPS)computationally efficient.

Learning Efficient Point Cloud Generation for Dense 3D Object  Reconstruction

  Conventional methods of 3D object generative modeling learn volumetricpredictions using deep networks with 3D convolutional operations, which aredirect analogies to classical 2D ones. However, these methods arecomputationally wasteful in attempt to predict 3D shapes, where information isrich only on the surfaces. In this paper, we propose a novel 3D generativemodeling framework to efficiently generate object shapes in the form of densepoint clouds. We use 2D convolutional operations to predict the 3D structurefrom multiple viewpoints and jointly apply geometric reasoning with 2Dprojection optimization. We introduce the pseudo-renderer, a differentiablemodule to approximate the true rendering operation, to synthesize novel depthmaps for optimization. Experimental results for single-image 3D objectreconstruction tasks show that we outperforms state-of-the-art methods in termsof shape similarity and prediction density.

Rethinking Reprojection: Closing the Loop for Pose-aware  ShapeReconstruction from a Single Image

  An emerging problem in computer vision is the reconstruction of 3D shape andpose of an object from a single image. Hitherto, the problem has been addressedthrough the application of canonical deep learning methods to regress from theimage directly to the 3D shape and pose labels. These approaches, however, areproblematic from two perspectives. First, they are minimizing the error between3D shapes and pose labels - with little thought about the nature of this labelerror when reprojecting the shape back onto the image. Second, they rely on theonerous and ill-posed task of hand labeling natural images with respect to 3Dshape and pose. In this paper we define the new task of pose-aware shapereconstruction from a single image, and we advocate that cheaper 2D annotationsof objects silhouettes in natural images can be utilized. We designarchitectures of pose-aware shape reconstruction which re-project the predictedshape back on to the image using the predicted pose. Our evaluation on severalobject categories demonstrates the superiority of our method for predictingpose-aware 3D shapes from natural images.

Compact Model Representation for 3D Reconstruction

  3D reconstruction from 2D images is a central problem in computer vision.Recent works have been focusing on reconstruction directly from a single image.It is well known however that only one image cannot provide enough informationfor such a reconstruction. A prior knowledge that has been entertained are 3DCAD models due to its online ubiquity. A fundamental question is how tocompactly represent millions of CAD models while allowing generalization to newunseen objects with fine-scaled geometry. We introduce an approach to compactlyrepresent a 3D mesh. Our method first selects a 3D model from a graph structureby using a novel free-form deformation FFD 3D-2D registration, and then theselected 3D model is refined to best fit the image silhouette. We perform acomprehensive quantitative and qualitative analysis that demonstratesimpressive dense and realistic 3D reconstruction from single images.

Object-Centric Photometric Bundle Adjustment with Deep Shape Prior

  Reconstructing 3D shapes from a sequence of images has long been a problem ofinterest in computer vision. Classical Structure from Motion (SfM) methods haveattempted to solve this problem through projected point displacement \& bundleadjustment. More recently, deep methods have attempted to solve this problem bydirectly learning a relationship between geometry and appearance. There is,however, a significant gap between these two strategies. SfM tackles theproblem from purely a geometric perspective, taking no account of the objectshape prior. Modern deep methods more often throw away geometric constraintsaltogether, rendering the results unreliable. In this paper we make an effortto bring these two seemingly disparate strategies together. We introducelearned shape prior in the form of deep shape generators into PhotometricBundle Adjustment (PBA) and propose to accommodate full 3D shape generated bythe shape prior within the optimization-based inference framework,demonstrating impressive results.

Semantic Photometric Bundle Adjustment on Natural Sequences

  The problem of obtaining dense reconstruction of an object in a naturalsequence of images has been long studied in computer vision. Classically thisproblem has been solved through the application of bundle adjustment (BA). Morerecently, excellent results have been attained through the application ofphotometric bundle adjustment (PBA) methods -- which directly minimize thephotometric error across frames. A fundamental drawback to BA & PBA, however,is: (i) their reliance on having to view all points on the object, and (ii) forthe object surface to be well textured. To circumvent these limitations wepropose semantic PBA which incorporates a 3D object prior, obtained throughdeep learning, within the photometric bundle adjustment problem. We demonstratestate of the art performance in comparison to leading methods for objectreconstruction across numerous natural sequences.

Learning Depth from Monocular Videos using Direct Methods

  The ability to predict depth from a single image - using recent advances inCNNs - is of increasing interest to the vision community. Unsupervisedstrategies to learning are particularly appealing as they can utilize muchlarger and varied monocular video datasets during learning without the need forground truth depth or stereo. In previous works, separate pose and depth CNNpredictors had to be determined such that their joint outputs minimized thephotometric error. Inspired by recent advances in direct visual odometry (DVO),we argue that the depth CNN predictor can be learned without a pose CNNpredictor. Further, we demonstrate empirically that incorporation of adifferentiable implementation of DVO, along with a novel depth normalizationstrategy - substantially improves performance over state of the art that usemonocular videos for training.

CNNs are Globally Optimal Given Multi-Layer Support

  Stochastic Gradient Descent (SGD) is the central workhorse for trainingmodern CNNs. Although giving impressive empirical performance it can be slow toconverge. In this paper we explore a novel strategy for training a CNN using analternation strategy that offers substantial speedups during training. We makethe following contributions: (i) replace the ReLU non-linearity within a CNNwith positive hard-thresholding, (ii) reinterpret this non-linearity as abinary state vector making the entire CNN linear if the multi-layer support isknown, and (iii) demonstrate that under certain conditions a global optima tothe CNN can be found through local descent. We then employ a novel alternationstrategy (between weights and support) for CNN training that leads tosubstantially faster convergence rates, nice theoretical properties, andachieving state of the art results across large scale datasets (e.g. ImageNet)as well as other standard benchmarks.

Take it in your stride: Do we need striding in CNNs?

  Since their inception, CNNs have utilized some type of striding operator toreduce the overlap of receptive fields and spatial dimensions. Although havingclear heuristic motivations (i.e. lowering the number of parameters to learn)the mathematical role of striding within CNN learning remains unclear. Thispaper offers a novel and mathematical rigorous perspective on the role of thestriding operator within modern CNNs. Specifically, we demonstratetheoretically that one can always represent a CNN that incorporates stridingwith an equivalent non-striding CNN which has more filters and smaller size.Through this equivalence we are then able to characterize striding as anadditional mechanism for parameter sharing among channels, thus reducingtraining complexity. Finally, the framework presented in this paper offers anew mathematical perspective on the role of striding which we hope shallfacilitate and simplify the future theoretical analysis of CNNs.

ST-GAN: Spatial Transformer Generative Adversarial Networks for Image  Compositing

  We address the problem of finding realistic geometric corrections to aforeground object such that it appears natural when composited into abackground image. To achieve this, we propose a novel Generative AdversarialNetwork (GAN) architecture that utilizes Spatial Transformer Networks (STNs) asthe generator, which we call Spatial Transformer GANs (ST-GANs). ST-GANs seekimage realism by operating in the geometric warp parameter space. Inparticular, we exploit an iterative STN warping scheme and propose a sequentialtraining strategy that achieves better results compared to naive training of asingle generator. One of the key advantages of ST-GAN is its applicability tohigh-resolution images indirectly since the predicted warp parameters aretransferable between reference frames. We demonstrate our approach in twoapplications: (1) visualizing how indoor furniture (e.g. from product images)might be perceived in a room, (2) hallucinating how accessories like glasseswould look when matched with real portraits.

Deep Component Analysis via Alternating Direction Neural Networks

  Despite a lack of theoretical understanding, deep neural networks haveachieved unparalleled performance in a wide range of applications. On the otherhand, shallow representation learning with component analysis is associatedwith rich intuition and theory, but smaller capacity often limits itsusefulness. To bridge this gap, we introduce Deep Component Analysis (DeepCA),an expressive multilayer model formulation that enforces hierarchical structurethrough constraints on latent variables in each layer. For inference, wepropose a differentiable optimization algorithm implemented using recurrentAlternating Direction Neural Networks (ADNNs) that enable parameter learningusing standard backpropagation. By interpreting feed-forward networks assingle-iteration approximations of inference in our model, we provide both anovel theoretical perspective for understanding them and a practical techniquefor constraining predictions with prior knowledge. Experimentally, wedemonstrate performance improvements on a variety of tasks, includingsingle-image depth prediction with sparse output constraints.

Deep Interpretable Non-Rigid Structure from Motion

  All current non-rigid structure from motion (NRSfM) algorithms are limitedwith respect to: (i) the number of images, and (ii) the type of shapevariability they can handle. This has hampered the practical utility of NRSfMfor many applications within vision. In this paper we propose a novel deepneural network to recover camera poses and 3D points solely from an ensemble of2D image coordinates. The proposed neural network is mathematicallyinterpretable as a multi-layer block sparse dictionary learning problem, andcan handle problems of unprecedented scale and shape complexity. Extensiveexperiments demonstrate the impressive performance of our approach where weexhibit superior precision and robustness against all availablestate-of-the-art works. The considerable model capacity of our approach affordsremarkable generalization to unseen data. We propose a quality measure (basedon the network weights) which circumvents the need for 3D ground-truth toascertain the confidence we have in the reconstruction. Once the network'sweights are estimated (for a non-rigid object) we show how our approach caneffectively recover 3D shape from a single image -- outperforming comparablemethods that rely on direct 3D supervision.

PointNetLK: Robust & Efficient Point Cloud Registration using PointNet

  PointNet has revolutionized how we think about representing point clouds. Forclassification and segmentation tasks, the approach and its subsequentextensions are state-of-the-art. To date, the successful application ofPointNet to point cloud registration has remained elusive. In this paper weargue that PointNet itself can be thought of as a learnable "imaging" function.As a consequence, classical vision algorithms for image alignment can beapplied on the problem - namely the Lucas & Kanade (LK) algorithm. Our centralinnovations stem from: (i) how to modify the LK algorithm to accommodate thePointNet imaging function, and (ii) unrolling PointNet and the LK algorithminto a single trainable recurrent deep neural network. We describe thearchitecture, and compare its performance against state-of-the-art in commonregistration scenarios. The architecture offers some remarkable propertiesincluding: generalization across shape categories and computational efficiency- opening up new paths of exploration for the application of deep learning topoint cloud registration. Code and videos are available athttps://github.com/hmgoforth/PointNetLK.

Photometric Mesh Optimization for Video-Aligned 3D Object Reconstruction

  In this paper, we address the problem of 3D object mesh reconstruction fromRGB videos. Our approach combines the best of multi-view geometric anddata-driven methods for 3D reconstruction by optimizing object meshes formulti-view photometric consistency while constraining mesh deformations with ashape prior. We pose this as a piecewise image alignment problem for each meshface projection. Our approach allows us to update shape parameters from thephotometric error without any depth or mask information. Moreover, we show howto avoid a degeneracy of zero photometric gradients via rasterizing from avirtual viewpoint. We demonstrate 3D object mesh reconstruction results fromboth synthetic and real-world videos with our photometric mesh optimization,which is unachievable with either na\"ive mesh generation networks ortraditional pipelines of surface reconstruction without heavy manualpost-processing.

The Conditional Lucas & Kanade Algorithm

  The Lucas & Kanade (LK) algorithm is the method of choice for efficient denseimage and object alignment. The approach is efficient as it attempts to modelthe connection between appearance and geometric displacement through a linearrelationship that assumes independence across pixel coordinates. A drawback ofthe approach, however, is its generative nature. Specifically, its performanceis tightly coupled with how well the linear model can synthesize appearancefrom geometric displacement, even though the alignment task itself isassociated with the inverse problem. In this paper, we present a new approach,referred to as the Conditional LK algorithm, which: (i) directly learns linearmodels that predict geometric displacement as a function of appearance, and(ii) employs a novel strategy for ensuring that the generative pixelindependence assumption can still be taken advantage of. We demonstrate thatour approach exhibits superior performance to classical generative forms of theLK algorithm. Furthermore, we demonstrate its comparable performance tostate-of-the-art methods such as the Supervised Descent Method withsubstantially less training examples, as well as the unique ability to "swap"geometric warp functions without having to retrain from scratch. Finally, froma theoretical perspective, our approach hints at possible redundancies thatexist in current state-of-the-art methods for alignment that could be leveragedin vision systems of the future.

Direct Visual Odometry using Bit-Planes

  Feature descriptors, such as SIFT and ORB, are well-known for theirrobustness to illumination changes, which has made them popular forfeature-based VSLAM\@. However, in degraded imaging conditions such as lowlight, low texture, blur and specular reflections, feature extraction is oftenunreliable. In contrast, direct VSLAM methods which estimate the camera pose byminimizing the photometric error using raw pixel intensities are often morerobust to low textured environments and blur. Nonetheless, at the core ofdirect VSLAM is the reliance on a consistent photometric appearance acrossimages, otherwise known as the brightness constancy assumption. Unfortunately,brightness constancy seldom holds in real world applications.  In this work, we overcome brightness constancy by incorporating featuredescriptors into a direct visual odometry framework. This combination resultsin an efficient algorithm that combines the strength of both feature-basedalgorithms and direct methods. Namely, we achieve robustness to arbitraryphotometric variations while operating in low-textured and poorly litenvironments. Our approach utilizes an efficient binary descriptor, which wecall Bit-Planes, and show how it can be used in the gradient-based optimizationrequired by direct methods. Moreover, we show that the squared Euclideandistance between Bit-Planes is equivalent to the Hamming distance. Hence, thedescriptor may be used in least squares optimization without sacrificing itsphotometric invariance. Finally, we present empirical results that demonstratethe robustness of the approach in poorly lit underground environments.

Need for Speed: A Benchmark for Higher Frame Rate Object Tracking

  In this paper, we propose the first higher frame rate video dataset (calledNeed for Speed - NfS) and benchmark for visual object tracking. The datasetconsists of 100 videos (380K frames) captured with now commonly availablehigher frame rate (240 FPS) cameras from real world scenarios. All frames areannotated with axis aligned bounding boxes and all sequences are manuallylabelled with nine visual attributes - such as occlusion, fast motion,background clutter, etc. Our benchmark provides an extensive evaluation of manyrecent and state-of-the-art trackers on higher frame rate sequences. We rankedeach of these trackers according to their tracking accuracy and real-timeperformance. One of our surprising conclusions is that at higher frame rates,simple trackers such as correlation filters outperform complex methods based ondeep networks. This suggests that for practical applications (such as inrobotics or embedded vision), one needs to carefully tradeoff bandwidthconstraints associated with higher frame rate acquisition, computational costsof real-time analysis, and the required application accuracy. Our dataset andbenchmark allows for the first time (to our knowledge) systematic explorationof such issues, and will be made available to allow for further research inthis space.

Proxy Templates for Inverse Compositional Photometric Bundle Adjustment

  Recent advances in 3D vision have demonstrated the strengths of photometricbundle adjustment. By directly minimizing reprojected pixel errors, instead ofgeometric reprojection errors, such methods can achieve sub-pixel alignmentaccuracy in both high and low textured regions. Typically, these problems aresolved using a forwards compositional Lucas-Kanade formulation parameterized by6-DoF rigid camera poses and a depth per point in the structure. For largeproblems the most CPU-intensive component of the pipeline is the creation andfactorization of the Hessian matrix at each iteration. For many warps, theinverse compositional formulation can offer significant speed-ups since theHessian need only be inverted once. In this paper, we show that an ordinaryinverse compositional formulation does not work for warps of this type ofparameterization due to ill-conditioning of its partial derivatives. However,we show that it is possible to overcome this limitation by introducing theconcept of a proxy template image. We show an order of magnitude improvement inspeed, with little effect on quality, going from forwards to inversecompositional in our own photometric bundle adjustment method designed forobject-centric structure from motion. This means less processing time for largesystems or denser reconstructions under the same real-time constraints. Weadditionally show that this theory can be readily applied to existing methodsby integrating it with the recently released Direct Sparse Odometry SLAMalgorithm.

Learning Policies for Adaptive Tracking with Deep Feature Cascades

  Visual object tracking is a fundamental and time-critical vision task. Recentyears have seen many shallow tracking methods based on real-time pixel-basedcorrelation filters, as well as deep methods that have top performance but needa high-end GPU. In this paper, we learn to improve the speed of deep trackerswithout losing accuracy. Our fundamental insight is to take an adaptiveapproach, where easy frames are processed with cheap features (such as pixelvalues), while challenging frames are processed with invariant but expensivedeep features. We formulate the adaptive tracking problem as a decision-makingprocess, and learn an agent to decide whether to locate objects with highconfidence on an early layer, or continue processing subsequent layers of anetwork. This significantly reduces the feed-forward cost for easy frames withdistinct or slow-moving objects. We train the agent offline in a reinforcementlearning fashion, and further demonstrate that learning all deep layers (so asto provide good features for adaptive tracking) can lead to near real-timeaverage tracking speed of 23 fps on a single CPU while achievingstate-of-the-art performance. Perhaps most tellingly, our approach provides a100X speedup for almost 50% of the time, indicating the power of an adaptiveapproach.

Image2Mesh: A Learning Framework for Single Image 3D Reconstruction

  One challenge that remains open in 3D deep learning is how to efficientlyrepresent 3D data to feed deep networks. Recent works have relied on volumetricor point cloud representations, but such approaches suffer from a number ofissues such as computational complexity, unordered data, and lack of finergeometry. This paper demonstrates that a mesh representation (i.e. vertices andfaces to form polygonal surfaces) is able to capture fine-grained geometry for3D reconstruction tasks. A mesh however is also unstructured data similar topoint clouds. We address this problem by proposing a learning framework toinfer the parameters of a compact mesh representation rather than learning fromthe mesh itself. This compact representation encodes a mesh using free-formdeformation and a sparse linear combination of models allowing us toreconstruct 3D meshes from single images. In contrast to prior work, we do notrely on silhouettes and landmarks to perform 3D reconstruction. We evaluate ourmethod on synthetic and real-world datasets with very promising results. Ourframework efficiently reconstructs 3D objects in a low-dimensional way whilepreserving its important geometrical aspects.

The relative distances to the Virgo, Fornax, and Coma clusters of  galaxies through the Dn-sigma and the Fundamental Plane relations

  We derive the relative distances to the Virgo, Fornax, and Coma clusters ofgalaxies by applying the Dn-sigma and the Fundamental Plane (FP) relations tothe data of the homogeneous samples of early--type galaxies studied by Caonetal, Lucey etal, and Jorgensen etal. The two distance indicators giveconsistent results, the relative distance moduli to Fornax and Coma withrespect to Virgo being Dmu(FV)=0.45+-0.15 and Dmu(CV)=3.55+-0.15) respectively.The formal error on D(m-M) may be as small as 0.07 mag (~3% in distance),provided that all the sources of bias are taken into account and a correctstatistical approach is used. Much of the actual uncertainty in the relativedistance of the clusters (12-15%), is due to the existence of systematicdepartures in the measurements of the velocity dispersions among the variousdatasets, and to the corrections for aperture effects. The above result for theFornax cluster is supported by the L-sigma-mu relation and, with lesseraccuracy, by the log(m)-log(re) relations. Our value of Dmu(FV) is in fairagreement with the one derived using planetary nebulae and SNe-Ia, while is inopen contrast with that coming from surface brightness fluctuations, globularclusters luminosity function, and infrared Tully-Fisher relation. In our dataComa appears slightly nearer than indicated by the other distance indicators,but now a better agreement with the Tully-Fisher relation seems to exist.Finally we found that there aren't correlations of the residuals of theDn-sigma with the effective surface brightness, the total luminosity of thegalaxies, with the ellipticity and the shape parameter a4. Instead acorrelation seems to exists with maximum Vrot, with (V/sigma) and with theexponent m of the r^1/m fit to major axis light of galaxies.

The Taipan Galaxy Survey: Scientific Goals and Observing Strategy

  Taipan is a multi-object spectroscopic galaxy survey starting in 2017 thatwill cover 2pi steradians over the southern sky, and obtain optical spectra forabout two million galaxies out to z<0.4. Taipan will use the newly-refurbished1.2m UK Schmidt Telescope at Siding Spring Observatory with the new TAIPANinstrument, which includes an innovative 'Starbugs' positioning system capableof rapidly and simultaneously deploying up to 150 spectroscopic fibres (and upto 300 with a proposed upgrade) over the 6-deg diameter focal plane, and apurpose-built spectrograph operating from 370 to 870nm with resolving powerR>2000. The main scientific goals of Taipan are: (i) to measure the distancescale of the Universe (primarily governed by the local expansion rate, H_0) to1% precision, and the structure growth rate of structure to 5%; (ii) to makethe most extensive map yet constructed of the mass distribution and motions inthe local Universe, using peculiar velocities based on improved FundamentalPlane distances, which will enable sensitive tests of gravitational physics;and (iii) to deliver a legacy sample of low-redshift galaxies as a uniquelaboratory for studying galaxy evolution as a function of mass and environment.The final survey, which will be completed within 5 years, will consist of acomplete magnitude-limited sample (i<17) of about 1.2x10^6 galaxies,supplemented by an extension to higher redshifts and fainter magnitudes(i<18.1) of a luminous red galaxy sample of about 0.8x10^6 galaxies.Observations and data processing will be carried out remotely and in afully-automated way, using a purpose-built automated 'virtual observer'software and an automated data reduction pipeline. The Taipan survey isdeliberately designed to maximise its legacy value, by complementing andenhancing current and planned surveys of the southern sky at wavelengths fromthe optical to the radio.

