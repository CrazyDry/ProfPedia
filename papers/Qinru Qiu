Efficient Recurrent Neural Networks using Structured Matrices in FPGAs

  Recurrent Neural Networks (RNNs) are becoming increasingly important for timeseries-related applications which require efficient and real-timeimplementations. The recent pruning based work ESE suffers from degradation ofperformance/energy efficiency due to the irregular network structure afterpruning. We propose block-circulant matrices for weight matrix representationin RNNs, thereby achieving simultaneous model compression and acceleration. Weaim to implement RNNs in FPGA with highest performance and energy efficiency,with certain accuracy requirement (negligible accuracy degradation).Experimental results on actual FPGA deployments shows that the proposedframework achieves a maximum energy efficiency improvement of 35.7$\times$compared with ESE.

Learning Topics using Semantic Locality

  The topic modeling discovers the latent topic probability of the given textdocuments. To generate the more meaningful topic that better represents thegiven document, we proposed a new feature extraction technique which can beused in the data preprocessing stage. The method consists of three steps.First, it generates the word/word-pair from every single document. Second, itapplies a two-way TF-IDF algorithm to word/word-pair for semantic filtering.Third, it uses the K-means algorithm to merge the word pairs that have thesimilar semantic meaning.  Experiments are carried out on the Open Movie Database (OMDb), ReutersDataset and 20NewsGroup Dataset. The mean Average Precision score is used asthe evaluation metric. Comparing our results with other state-of-the-art topicmodels, such as Latent Dirichlet allocation and traditional RestrictedBoltzmann Machines. Our proposed data preprocessing can improve the generatedtopic accuracy by up to 12.99\%.

Towards Budget-Driven Hardware Optimization for Deep Convolutional  Neural Networks using Stochastic Computing

  Recently, Deep Convolutional Neural Network (DCNN) has achieved tremendoussuccess in many machine learning applications. Nevertheless, the deep structurehas brought significant increases in computation complexity. Largescale deeplearning systems mainly operate in high-performance server clusters, thusrestricting the application extensions to personal or mobile devices. Previousworks on GPU and/or FPGA acceleration for DCNNs show increasing speedup, butignore other constraints, such as area, power, and energy. Stochastic Computing(SC), as a unique data representation and processing technique, has thepotential to enable the design of fully parallel and scalable hardwareimplementations of large-scale deep learning systems. This paper proposed anautomatic design allocation algorithm driven by budget requirement consideringoverall accuracy performance. This systematic method enables the automaticdesign of a DCNN where all design parameters are jointly optimized.Experimental results demonstrate that proposed algorithm can achieve a jointoptimization of all design parameters given the comprehensive budget of a DCNN.

Scalable NoC-based Neuromorphic Hardware Learning and Inference

  Bio-inspired neuromorphic hardware is a research direction to approachbrain's computational power and energy efficiency. Spiking neural networks(SNN) encode information as sparsely distributed spike trains and employspike-timing-dependent plasticity (STDP) mechanism for learning. Existinghardware implementations of SNN are limited in scale or do not have in-hardwarelearning capability. In this work, we propose a low-cost scalableNetwork-on-Chip (NoC) based SNN hardware architecture with fully distributedin-hardware STDP learning capability. All hardware neurons work in parallel andcommunicate through the NoC. This enables chip-level interconnection,scalability and reconfigurability necessary for deploying differentapplications. The hardware is applied to learn MNIST digits as an evaluation ofits learning capability. We explore the design space to study the trade-offsbetween speed, area and energy. How to use this procedure to find optimalarchitecture configuration is also discussed.

CircConv: A Structured Convolution with Low Complexity

  Deep neural networks (DNNs), especially deep convolutional neural networks(CNNs), have emerged as the powerful technique in various machine learningapplications. However, the large model sizes of DNNs yield high demands oncomputation resource and weight storage, thereby limiting the practicaldeployment of DNNs. To overcome these limitations, this paper proposes toimpose the circulant structure to the construction of convolutional layers, andhence leads to circulant convolutional layers (CircConvs) and circulant CNNs.The circulant structure and models can be either trained from scratch orre-trained from a pre-trained non-circulant model, thereby making it veryflexible for different training environments. Through extensive experiments,such strong structure-imposing approach is proved to be able to substantiallyreduce the number of parameters of convolutional layers and enable significantsaving of computational cost by using fast multiplication of the circulanttensor.

Towards Ultra-High Performance and Energy Efficiency of Deep Learning  Systems: An Algorithm-Hardware Co-Optimization Framework

  Hardware accelerations of deep learning systems have been extensivelyinvestigated in industry and academia. The aim of this paper is to achieveultra-high energy efficiency and performance for hardware implementations ofdeep neural networks (DNNs). An algorithm-hardware co-optimization framework isdeveloped, which is applicable to different DNN types, sizes, and applicationscenarios. The algorithm part adopts the general block-circulant matrices toachieve a fine-grained tradeoff between accuracy and compression ratio. Itapplies to both fully-connected and convolutional layers and contains amathematically rigorous proof of the effectiveness of the method. The proposedalgorithm reduces computational complexity per layer from O($n^2$) to O($n\logn$) and storage complexity from O($n^2$) to O($n$), both for training andinference. The hardware part consists of highly efficient Field ProgrammableGate Array (FPGA)-based implementations using effective reconfiguration, batchprocessing, deep pipelining, resource re-using, and hierarchical control.Experimental results demonstrate that the proposed framework achieves at least152X speedup and 71X energy efficiency gain compared with IBM TrueNorthprocessor under the same test accuracy. It achieves at least 31X energyefficiency gain compared with the reference FPGA-based work.

C-LSTM: Enabling Efficient LSTM using Structured Compression Techniques  on FPGAs

  Recently, significant accuracy improvement has been achieved for acousticrecognition systems by increasing the model size of Long Short-Term Memory(LSTM) networks. Unfortunately, the ever-increasing size of LSTM model leads toinefficient designs on FPGAs due to the limited on-chip resources. The previouswork proposes to use a pruning based compression technique to reduce the modelsize and thus speedups the inference on FPGAs. However, the random nature ofthe pruning technique transforms the dense matrices of the model to highlyunstructured sparse ones, which leads to unbalanced computation and irregularmemory accesses and thus hurts the overall performance and energy efficiency.  In contrast, we propose to use a structured compression technique which couldnot only reduce the LSTM model size but also eliminate the irregularities ofcomputation and memory accesses. This approach employs block-circulant insteadof sparse matrices to compress weight matrices and reduces the storagerequirement from $\mathcal{O}(k^2)$ to $\mathcal{O}(k)$. Fast Fourier Transformalgorithm is utilized to further accelerate the inference by reducing thecomputational complexity from $\mathcal{O}(k^2)$ to$\mathcal{O}(k\text{log}k)$. The datapath and activation functions arequantized as 16-bit to improve the resource utilization. More importantly, wepropose a comprehensive framework called C-LSTM to automatically optimize andimplement a wide range of LSTM variants on FPGAs. According to the experimentalresults, C-LSTM achieves up to 18.8X and 33.5X gains for performance and energyefficiency compared with the state-of-the-art LSTM implementation under thesame experimental setup, and the accuracy degradation is very small.

A Simulation Framework for Fast Design Space Exploration of Unmanned Air  System Traffic Management Policies

  The number of daily small Unmanned Aircraft Systems (sUAS) operations inuncontrolled low altitude airspace is expected to reach into the millions. UASTraffic Management (UTM) is an emerging concept aiming at the safe andefficient management of such very dense traffic, but few studies are addressingthe policies to accommodate such demand and the required ground infrastructurein suburban or urban environments. Searching for the optimal air trafficmanagement policy is a combinatorial optimization problem with intractablecomplexity when the number of sUAS and the constraints increases. As thedemands on the airspace increase and traffic patterns get complicated, it isdifficult to forecast the potential low altitude airspace hotspots and thecorresponding ground resource requirements. This work presents a Multi-agentAir Traffic and Resource Usage Simulation (MATRUS) framework that aims for fastevaluation of different air traffic management policies and the relationshipbetween policy, environment and resulting traffic patterns. It can also be usedas a tool to decide the resource distribution and launch site location in theplanning of a next-generation smart city. As a case study, detailed comparisonsare provided for the sUAS flight time, conflict ratio, cellular communicationresource usage, for a managed (centrally coordinated) and unmanaged (freeflight) traffic scenario.

SC-DCNN: Highly-Scalable Deep Convolutional Neural Network using  Stochastic Computing

  With recent advancing of Internet of Things (IoTs), it becomes veryattractive to implement the deep convolutional neural networks (DCNNs) ontoembedded/portable systems. Presently, executing the software-based DCNNsrequires high-performance server clusters in practice, restricting theirwidespread deployment on the mobile devices. To overcome this issue,considerable research efforts have been conducted in the context of developinghighly-parallel and specific DCNN hardware, utilizing GPGPUs, FPGAs, and ASICs.Stochastic Computing (SC), which uses bit-stream to represent a number within[-1, 1] by counting the number of ones in the bit-stream, has a high potentialfor implementing DCNNs with high scalability and ultra-low hardware footprint.Since multiplications and additions can be calculated using AND gates andmultiplexers in SC, significant reductions in power/energy and hardwarefootprint can be achieved compared to the conventional binary arithmeticimplementations. The tremendous savings in power (energy) and hardwareresources bring about immense design space for enhancing scalability androbustness for hardware DCNNs. This paper presents the first comprehensivedesign and optimization framework of SC-based DCNNs (SC-DCNNs). We firstpresent the optimal designs of function blocks that perform the basicoperations, i.e., inner product, pooling, and activation function. Then wepropose the optimal design of four types of combinations of basic functionblocks, named feature extraction blocks, which are in charge of extractingfeatures from input feature maps. Besides, weight storage methods areinvestigated to reduce the area and power/energy consumption for storingweights. Finally, the whole SC-DCNN implementation is optimized, with featureextraction blocks carefully selected, to minimize area and power/energyconsumption while maintaining a high network accuracy level.

Hardware-Driven Nonlinear Activation for Stochastic Computing Based Deep  Convolutional Neural Networks

  Recently, Deep Convolutional Neural Networks (DCNNs) have made unprecedentedprogress, achieving the accuracy close to, or even better than human-levelperception in various tasks. There is a timely need to map the latest softwareDCNNs to application-specific hardware, in order to achieve orders of magnitudeimprovement in performance, energy efficiency and compactness. StochasticComputing (SC), as a low-cost alternative to the conventional binary computingparadigm, has the potential to enable massively parallel and highly scalablehardware implementation of DCNNs. One major challenge in SC based DCNNs isdesigning accurate nonlinear activation functions, which have a significantimpact on the network-level accuracy but cannot be implemented accurately byexisting SC computing blocks. In this paper, we design and optimize SC basedneurons, and we propose highly accurate activation designs for the three mostfrequently used activation functions in software DCNNs, i.e, hyperbolictangent, logistic, and rectified linear units. Experimental results on LeNet-5using MNIST dataset demonstrate that compared with a binary ASIC hardware DCNN,the DCNN with the proposed SC neurons can achieve up to 61X, 151X, and 2Ximprovement in terms of area, power, and energy, respectively, at the cost ofsmall precision degradation.In addition, the SC approach achieves up to 21X and41X of the area, 41X and 72X of the power, and 198200X and 96443X of theenergy, compared with CPU and GPU approaches, respectively, while the error isincreased by less than 3.07%. ReLU activation is suggested for future SC basedDCNNs considering its superior performance under a small bit stream length.

A Hierarchical Framework of Cloud Resource Allocation and Power  Management Using Deep Reinforcement Learning

  Automatic decision-making approaches, such as reinforcement learning (RL),have been applied to (partially) solve the resource allocation problemadaptively in the cloud computing system. However, a complete cloud resourceallocation framework exhibits high dimensions in state and action spaces, whichprohibit the usefulness of traditional RL techniques. In addition, high powerconsumption has become one of the critical concerns in design and control ofcloud computing systems, which degrades system reliability and increasescooling cost. An effective dynamic power management (DPM) policy shouldminimize power consumption while maintaining performance degradation within anacceptable level. Thus, a joint virtual machine (VM) resource allocation andpower management framework is critical to the overall cloud computing system.Moreover, novel solution framework is necessary to address the even higherdimensions in state and action spaces. In this paper, we propose a novelhierarchical framework for solving the overall resource allocation and powermanagement problem in cloud computing systems. The proposed hierarchicalframework comprises a global tier for VM resource allocation to the servers anda local tier for distributed power management of local servers. The emergingdeep reinforcement learning (DRL) technique, which can deal with complicatedcontrol problems with large state space, is adopted to solve the global tierproblem. Furthermore, an autoencoder and a novel weight sharing structure areadopted to handle the high-dimensional state space and accelerate theconvergence speed. On the other hand, the local tier of distributed serverpower managements comprises an LSTM based workload predictor and a model-freeRL based power manager, operating in a distributed manner.

CirCNN: Accelerating and Compressing Deep Neural Networks Using  Block-CirculantWeight Matrices

  Large-scale deep neural networks (DNNs) are both compute and memoryintensive. As the size of DNNs continues to grow, it is critical to improve theenergy efficiency and performance while maintaining accuracy. For DNNs, themodel size is an important factor affecting performance, scalability and energyefficiency. Weight pruning achieves good compression ratios but suffers fromthree drawbacks: 1) the irregular network structure after pruning; 2) theincreased training complexity; and 3) the lack of rigorous guarantee ofcompression ratio and inference accuracy. To overcome these limitations, thispaper proposes CirCNN, a principled approach to represent weights and processneural networks using block-circulant matrices. CirCNN utilizes the FastFourier Transform (FFT)-based fast multiplication, simultaneously reducing thecomputational complexity (both in inference and training) from O(n2) toO(nlogn) and the storage complexity from O(n2) to O(n), with negligibleaccuracy loss. Compared to other approaches, CirCNN is distinct due to itsmathematical rigor: it can converge to the same effectiveness as DNNs withoutcompression. The CirCNN architecture, a universal DNN inference engine that canbe implemented on various hardware/software platforms with configurable networkarchitecture. To demonstrate the performance and energy efficiency, we testCirCNN in FPGA, ASIC and embedded processors. Our results show that CirCNNarchitecture achieves very high energy efficiency and performance with a smallhardware footprint. Based on the FPGA implementation and ASIC synthesisresults, CirCNN achieves 6-102X energy efficiency improvements compared withthe best state-of-the-art results.

C3PO: Database and Benchmark for Early-stage Malicious Activity  Detection in 3D Printing

  Increasing malicious users have sought practices to leverage 3D printingtechnology to produce unlawful tools in criminal activities. Currentregulations are inadequate to deal with the rapid growth of 3D printers. It isof vital importance to enable 3D printers to identify the objects to beprinted, so that the manufacturing procedure of an illegal weapon can beterminated at the early stage. Deep learning yields significant rises inperformance in the object recognition tasks. However, the lack of large-scaledatabases in 3D printing domain stalls the advancement of automatic illegalweapon recognition.  This paper presents a new 3D printing image database, namely C3PO, whichcompromises two subsets for the different system working scenarios. We extractimages from the numerical control programming code files of 22 3D models, andthen categorize the images into 10 distinct labels. The first set consists of62,200 images which represent the object projections on the three planes in aCartesian coordinate system. And the second sets consists of sequences of total671,677 images to simulate the cameras' captures of the printed objects.Importantly, we demonstrate that the weapons can be recognized in eitherscenario using deep learning based approaches using our proposed database. % Wealso use the trained deep models to build a prototype of object-aware 3Dprinter. The quantitative results are promising, and the future exploration ofthe database and the crime prevention in 3D printing are demanding tasks.

E-RNN: Design Optimization for Efficient Recurrent Neural Networks in  FPGAs

  Recurrent Neural Networks (RNNs) are becoming increasingly important for timeseries-related applications which require efficient and real-timeimplementations. The two major types are Long Short-Term Memory (LSTM) andGated Recurrent Unit (GRU) networks. It is a challenging task to havereal-time, efficient, and accurate hardware RNN implementations because of thehigh sensitivity to imprecision accumulation and the requirement of specialactivation function implementations.  A key limitation of the prior works is the lack of a systematic designoptimization framework of RNN model and hardware implementations, especiallywhen the block size (or compression ratio) should be jointly optimized with RNNtype, layer size, etc. In this paper, we adopt the block-circulant matrix-basedframework, and present the Efficient RNN (E-RNN) framework for FPGAimplementations of the Automatic Speech Recognition (ASR) application. Theoverall goal is to improve performance/energy efficiency under accuracyrequirement. We use the alternating direction method of multipliers (ADMM)technique for more accurate block-circulant training, and present two designexplorations providing guidance on block size and reducing RNN training trials.Based on the two observations, we decompose E-RNN in two phases: Phase I ondetermining RNN model to reduce computation and storage subject to accuracyrequirement, and Phase II on hardware implementations given RNN model,including processing element design/optimization, quantization, activationimplementation, etc. Experimental results on actual FPGA deployments show thatE-RNN achieves a maximum energy efficiency improvement of 37.4$\times$ comparedwith ESE, and more than 2$\times$ compared with C-LSTM, under the sameaccuracy.

