Closing the Learning-Planning Loop with Predictive State Representations

  A central problem in artificial intelligence is that of planning to maximizefuture reward under uncertainty in a partially observable environment. In thispaper we propose and demonstrate a novel algorithm which accurately learns amodel of such an environment directly from sequences of action-observationpairs. We then close the loop from observations to actions by planning in thelearned model and recovering a policy which is near-optimal in the originalenvironment. Specifically, we present an efficient and statistically consistentspectral algorithm for learning the parameters of a Predictive StateRepresentation (PSR). We demonstrate the algorithm by learning a model of asimulated high-dimensional, vision-based mobile robot planning task, and thenperform approximate point-based planning in the learned PSR. Analysis of ourresults shows that the algorithm learns a state space which efficientlycaptures the essential features of the environment. This representation allowsaccurate prediction with a small number of parameters, and enables successfuland efficient planning.

Two-Manifold Problems

  Recently, there has been much interest in spectral approaches to learningmanifolds---so-called kernel eigenmap methods. These methods have had somesuccesses, but their applicability is limited because they are not robust tonoise. To address this limitation, we look at two-manifold problems, in whichwe simultaneously reconstruct two related manifolds, each representing adifferent view of the same data. By solving these interconnected learningproblems together and allowing information to flow between them, two-manifoldalgorithms are able to succeed where a non-integrated approach would fail: eachview allows us to suppress noise in the other, reducing bias in the same waythat an instrumental variable allows us to remove bias in a {linear}dimensionality reduction problem. We propose a class of algorithms fortwo-manifold problems, based on spectral decomposition of cross-covarianceoperators in Hilbert space. Finally, we discuss situations where two-manifoldproblems are useful, and demonstrate that solving a two-manifold problem canaid in learning a nonlinear dynamical system from limited data.

Two-Manifold Problems with Applications to Nonlinear System  Identification

  Recently, there has been much interest in spectral approaches to learningmanifolds---so-called kernel eigenmap methods. These methods have had somesuccesses, but their applicability is limited because they are not robust tonoise. To address this limitation, we look at two-manifold problems, in whichwe simultaneously reconstruct two related manifolds, each representing adifferent view of the same data. By solving these interconnected learningproblems together, two-manifold algorithms are able to succeed where anon-integrated approach would fail: each view allows us to suppress noise inthe other, reducing bias. We propose a class of algorithms for two-manifoldproblems, based on spectral decomposition of cross-covariance operators inHilbert space, and discuss when two-manifold problems are useful. Finally, wedemonstrate that solving a two-manifold problem can aid in learning a nonlineardynamical system from limited data.

A Spectral Learning Approach to Range-Only SLAM

  We present a novel spectral learning algorithm for simultaneous localizationand mapping (SLAM) from range data with known correspondences. This algorithmis an instance of a general spectral system identification framework, fromwhich it inherits several desirable properties, including statisticalconsistency and no local optima. Compared with popular batch optimization ormultiple-hypothesis tracking (MHT) methods for range-only SLAM, our spectralapproach offers guaranteed low computational requirements and good trackingperformance. Compared with popular extended Kalman filter (EKF) or extendedinformation filter (EIF) approaches, and many MHT ones, our approach does notneed to linearize a transition or measurement model; such linearizations cancause severe errors in EKFs and EIFs, and to a lesser extent MHT, particularlyfor the highly non-Gaussian posteriors encountered in range-only SLAM. Weprovide a theoretical analysis of our method, including finite-sample errorbounds. Finally, we demonstrate on a real-world robotic SLAM problem that ouralgorithm is not only theoretically justified, but works well in practice: in acomparison of multiple methods, the lowest errors come from a combination ofour algorithm with batch optimization, but our method alone produces nearly asgood a result at far lower computational cost.

Hilbert Space Embeddings of Predictive State Representations

  Predictive State Representations (PSRs) are an expressive class of models forcontrolled stochastic processes. PSRs represent state as a set of predictionsof future observable events. Because PSRs are defined entirely in terms ofobservable data, statistically consistent estimates of PSR parameters can belearned efficiently by manipulating moments of observed training data. Mostlearning algorithms for PSRs have assumed that actions and observations arefinite with low cardinality. In this paper, we generalize PSRs to infinite setsof observations and actions, using the recent concept of Hilbert spaceembeddings of distributions. The essence is to represent the state as anonparametric conditional embedding operator in a Reproducing Kernel HilbertSpace (RKHS) and leverage recent work in kernel methods to estimate, predict,and update the representation. We show that these Hilbert space embeddings ofPSRs are able to gracefully handle continuous actions and observations, andthat our learned models outperform competing system identification algorithmson several prediction benchmarks.

Predictive State Temporal Difference Learning

  We propose a new approach to value function approximation which combineslinear temporal difference reinforcement learning with subspace identification.In practical applications, reinforcement learning (RL) is complicated by thefact that state is either high-dimensional or partially observable. Therefore,RL methods are designed to work with features of state rather than stateitself, and the success or failure of learning is often determined by thesuitability of the selected features. By comparison, subspace identification(SSID) methods are designed to select a feature set which preserves as muchinformation as possible about state. In this paper we connect the twoapproaches, looking at the problem of reinforcement learning with a large setof features, each of which may only be marginally useful for value functionapproximation. We introduce a new algorithm for this situation, calledPredictive State Temporal Difference (PSTD) learning. As in SSID for predictivestate representations, PSTD finds a linear compression operator that projects alarge set of features down to a small set that preserves the maximum amount ofpredictive information. As in RL, PSTD then uses a Bellman recursion toestimate a value function. We discuss the connection between PSTD and priorapproaches in RL and SSID. We prove that PSTD is statistically consistent,perform several experiments that illustrate its properties, and demonstrate itspotential on a difficult optimal stopping problem.

Incremental Sparse GP Regression for Continuous-time Trajectory  Estimation & Mapping

  Recent work on simultaneous trajectory estimation and mapping (STEAM) formobile robots has found success by representing the trajectory as a Gaussianprocess. Gaussian processes can represent a continuous-time trajectory,elegantly handle asynchronous and sparse measurements, and allow the robot toquery the trajectory to recover its estimated position at any time of interest.A major drawback of this approach is that STEAM is formulated as a batchestimation problem. In this paper we provide the critical extensions necessaryto transform the existing batch algorithm into an extremely efficientincremental algorithm. In particular, we are able to vastly speed up thesolution time through efficient variable reordering and incremental sparseupdates, which we believe will greatly increase the practicality of Gaussianprocess methods for robot mapping and localization. Finally, we demonstrate theapproach and its advantages on both synthetic and real datasets.

Learning to Filter with Predictive State Inference Machines

  Latent state space models are a fundamental and widely used tool for modelingdynamical systems. However, they are difficult to learn from data and learnedmodels often lack performance guarantees on inference tasks such as filteringand prediction. In this work, we present the PREDICTIVE STATE INFERENCE MACHINE(PSIM), a data-driven method that considers the inference procedure on adynamical system as a composition of predictors. The key idea is that ratherthan first learning a latent state space model, and then using the learnedmodel for inference, PSIM directly learns predictors for inference inpredictive state space. We provide theoretical guarantees for inference, inboth realizable and agnostic settings, and showcase practical performance on avariety of simulated and real world robotics benchmarks.

Adaptive Probabilistic Trajectory Optimization via Efficient Approximate  Inference

  Robotic systems must be able to quickly and robustly make decisions whenoperating in uncertain and dynamic environments. While Reinforcement Learning(RL) can be used to compute optimal policies with little prior knowledge aboutthe environment, it suffers from slow convergence. An alternative approach isModel Predictive Control (MPC), which optimizes policies quickly, but alsorequires accurate models of the system dynamics and environment. In this paperwe propose a new approach, adaptive probabilistic trajectory optimization, thatcombines the benefits of RL and MPC. Our method uses scalable approximateinference to learn and updates probabilistic models in an online incrementalfashion while also computing optimal control policies via successive localapproximations. We present two variations of our algorithm based on the SparseSpectrum Gaussian Process (SSGP) model, and we test our algorithm on threelearning tasks, demonstrating the effectiveness and efficiency of our approach.

4D Crop Monitoring: Spatio-Temporal Reconstruction for Agriculture

  Autonomous crop monitoring at high spatial and temporal resolution is acritical problem in precision agriculture. While Structure from Motion andMulti-View Stereo algorithms can finely reconstruct the 3D structure of a fieldwith low-cost image sensors, these algorithms fail to capture the dynamicnature of continuously growing crops. In this paper we propose a 4Dreconstruction approach to crop monitoring, which employs a spatio-temporalmodel of dynamic scenes that is useful for precision agriculture applications.Additionally, we provide a robust data association algorithm to address theproblem of large appearance changes due to scenes being viewed from differentangles at different points in time, which is critical to achieving 4Dreconstruction. Finally, we collected a high quality dataset with ground truthstatistics to evaluate the performance of our method. We demonstrate that our4D reconstruction approach provides models that are qualitatively correct withrespect to visual appearance and quantitatively accurate when measured againstthe ground truth geometric properties of the monitored crops.

Approximately Optimal Continuous-Time Motion Planning and Control via  Probabilistic Inference

  The problem of optimal motion planing and control is fundamental in robotics.However, this problem is intractable for continuous-time stochastic systems ingeneral and the solution is difficult to approximate if non-instantaneousnonlinear performance indices are present. In this work, we provide anefficient algorithm, PIPC (Probabilistic Inference for Planning and Control),that yields approximately optimal policies with arbitrary higher-ordernonlinear performance indices. Using probabilistic inference and a Gaussianprocess representation of trajectories, PIPC exploits the underlying sparsityof the problem such that its complexity scales linearly in the number ofnonlinear factors. We demonstrate the capabilities of our algorithm in areceding horizon setting with multiple systems in simulation.

Deeply AggreVaTeD: Differentiable Imitation Learning for Sequential  Prediction

  Researchers have demonstrated state-of-the-art performance in sequentialdecision making problems (e.g., robotics control, sequential prediction) withdeep neural network models. One often has access to near-optimal oracles thatachieve good performance on the task during training. We demonstrate thatAggreVaTeD --- a policy gradient extension of the Imitation Learning (IL)approach of (Ross & Bagnell, 2014) --- can leverage such an oracle to achievefaster and better solutions with less training data than a less-informedReinforcement Learning (RL) technique. Using both feedforward and recurrentneural network predictors, we present stochastic gradient procedures on asequential prediction task, dependency-parsing from raw image data, as well ason various high dimensional robotics control problems. We also provide acomprehensive theoretical study of IL that demonstrates we can expect up toexponentially lower sample complexity for learning with AggreVaTeD than with RLalgorithms, which backs our empirical findings. Our results and theory indicatethat the proposed approach can achieve superior performance with respect to theoracle when the demonstrator is sub-optimal.

Sparse Gaussian Processes for Continuous-Time Trajectory Estimation on  Matrix Lie Groups

  Continuous-time trajectory representations are a powerful tool that can beused to address several issues in many practical simultaneous localization andmapping (SLAM) scenarios, like continuously collected measurements distorted byrobot motion, or during with asynchronous sensor measurements. Sparse Gaussianprocesses (GP) allow for a probabilistic non-parametric trajectoryrepresentation that enables fast trajectory estimation by sparse GP regression.However, previous approaches are limited to dealing with vector spacerepresentations of state only. In this technical report we extend the work byBarfoot et al. [1] to general matrix Lie groups, by applying constant-velocityprior, and defining locally linear GP. This enables using sparse GP approach ina large space of practical SLAM settings. In this report we give the theory andleave the experimental evaluation in future publications.

Predictive State Recurrent Neural Networks

  We present a new model, Predictive State Recurrent Neural Networks (PSRNNs),for filtering and prediction in dynamical systems. PSRNNs draw on insights fromboth Recurrent Neural Networks (RNNs) and Predictive State Representations(PSRs), and inherit advantages from both types of models. Like many successfulRNN architectures, PSRNNs use (potentially deeply composed) bilinear transferfunctions to combine information from multiple sources. We show that suchbilinear functions arise naturally from state updates in Bayes filters likePSRs, in which observations can be viewed as gating belief states. We also showthat PSRNNs can be learned effectively by combining Backpropogation ThroughTime (BPTT) with an initialization derived from a statistically consistentlearning algorithm for PSRs called two-stage regression (2SR). Finally, we showthat PSRNNs can be factorized using tensor decomposition, reducing model sizeand suggesting interesting connections to existing multiplicative architecturessuch as LSTMs. We applied PSRNNs to 4 datasets, and showed that we outperformseveral popular alternative approaches to modeling dynamical systems in allcases.

One-Shot Learning for Semantic Segmentation

  Low-shot learning methods for image classification support learning fromsparse data. We extend these techniques to support dense semantic imagesegmentation. Specifically, we train a network that, given a small set ofannotated images, produces parameters for a Fully Convolutional Network (FCN).We use this FCN to perform dense pixel-level prediction on a test image for thenew semantic class. Our architecture shows a 25% relative meanIoU improvementcompared to the best baseline methods for one-shot segmentation on unseenclasses in the PASCAL VOC 2012 dataset and is at least 3 times faster.

Agile Off-Road Autonomous Driving Using End-to-End Deep Imitation  Learning

  We present an end-to-end imitation learning system for agile, off-roadautonomous driving using only low-cost on-board sensors. By imitating a modelpredictive controller equipped with advanced sensors, we train a deep neuralnetwork control policy to map raw, high-dimensional observations to continuoussteering and throttle commands. Compared with recent approaches to similartasks, our method requires neither state estimation nor on-the-fly planning tonavigate the vehicle. Our approach relies on, and experimentally validates,recent imitation learning theory. Empirically, we show that policies trainedwith online imitation learning overcome well-known challenges related tocovariate shift and generalize better than policies trained with batchimitation learning. Built on these insights, our autonomous driving systemdemonstrates successful high-speed off-road driving, matching thestate-of-the-art performance.

Manifold Regularization for Kernelized LSTD

  Policy evaluation or value function or Q-function approximation is a keyprocedure in reinforcement learning (RL). It is a necessary component of policyiteration and can be used for variance reduction in policy gradient methods.Therefore its quality has a significant impact on most RL algorithms. Motivatedby manifold regularized learning, we propose a novel kernelized policyevaluation method that takes advantage of the intrinsic geometry of the statespace learned from data, in order to achieve better sample efficiency andhigher accuracy in Q-function approximation. Applying the proposed method inthe Least-Squares Policy Iteration (LSPI) framework, we observe superiorperformance compared to widely used parametric basis functions on two standardbenchmarks in terms of policy quality.

Learning Hidden Quantum Markov Models

  Hidden Quantum Markov Models (HQMMs) can be thought of as quantumprobabilistic graphical models that can model sequential data. We extendprevious work on HQMMs with three contributions: (1) we show how classicalhidden Markov models (HMMs) can be simulated on a quantum circuit, (2) wereformulate HQMMs by relaxing the constraints for modeling HMMs on quantumcircuits, and (3) we present a learning algorithm to estimate the parameters ofan HQMM from data. While our algorithm requires further optimization to handlelarger datasets, we are able to evaluate our algorithm using several syntheticdatasets. We show that on HQMM generated data, our algorithm learns HQMMs withthe same number of hidden states and predictive accuracy as the true HQMMs,while HMMs learned with the Baum-Welch algorithm require more states to matchthe predictive accuracy.

Deep Forward and Inverse Perceptual Models for Tracking and Prediction

  We consider the problems of learning forward models that map state tohigh-dimensional images and inverse models that map high-dimensional images tostate in robotics. Specifically, we present a perceptual model for generatingvideo frames from state with deep networks, and provide a framework for its usein tracking and prediction tasks. We show that our proposed model greatlyoutperforms standard deconvolutional methods and GANs for image generation,producing clear, photo-realistic images. We also develop a convolutional neuralnetwork model for state estimation and compare the result to an Extended KalmanFilter to estimate robot trajectories. We validate all models on a real roboticsystem.

Variational Inference for Gaussian Process Models with Linear Complexity

  Large-scale Gaussian process inference has long faced practical challengesdue to time and space complexity that is superlinear in dataset size. Whilesparse variational Gaussian process models are capable of learning fromlarge-scale data, standard strategies for sparsifying the model can prevent theapproximation of complex functions. In this work, we propose a novelvariational Gaussian process model that decouples the representation of meanand covariance functions in reproducing kernel Hilbert space. We show that thisnew parametrization generalizes previous models. Furthermore, it yields avariational inference problem that can be solved by stochastic gradient ascentwith time and space complexity that is only linear in the number of meanfunction parameters, regardless of the choice of kernels, likelihoods, andinducing points. This strategy makes the adoption of large-scale expressiveGaussian process models possible. We run several experiments on regressiontasks and show that this decoupled approach greatly outperforms previous sparsevariational Gaussian process inference procedures.

Convergence of Value Aggregation for Imitation Learning

  Value aggregation is a general framework for solving imitation learningproblems. Based on the idea of data aggregation, it generates a policy sequenceby iteratively interleaving policy optimization and evaluation in an onlinelearning setting. While the existence of a good policy in the policy sequencecan be guaranteed non-asymptotically, little is known about the convergence ofthe sequence or the performance of the last policy. In this paper, we debunkthe common belief that value aggregation always produces a convergent policysequence with improving performance. Moreover, we identify a critical stabilitycondition for convergence and provide a tight non-asymptotic bound on theperformance of the last policy. These new theoretical insights let us stabilizeproblems with regularization, which removes the inconvenient process ofidentifying the best policy in the policy sequence in stochastic problems.

Fast Policy Learning through Imitation and Reinforcement

  Imitation learning (IL) consists of a set of tools that leverage expertdemonstrations to quickly learn policies. However, if the expert is suboptimal,IL can yield policies with inferior performance compared to reinforcementlearning (RL). In this paper, we aim to provide an algorithm that combines thebest aspects of RL and IL. We accomplish this by formulating several popular RLand IL algorithms in a common mirror descent framework, showing that thesealgorithms can be viewed as a variation on a single approach. We then proposeLOKI, a strategy for policy learning that first performs a small but randomnumber of IL iterations before switching to a policy gradient RL method. Weshow that if the switching time is properly randomized, LOKI can learn tooutperform a suboptimal expert and converge faster than running policy gradientfrom scratch. Finally, we evaluate the performance of LOKI experimentally inseveral simulated environments.

Dual Policy Iteration

  Recently, a novel class of Approximate Policy Iteration (API) algorithms havedemonstrated impressive practical performance (e.g., ExIt from [2],AlphaGo-Zero from [27]). This new family of algorithms maintains, andalternately optimizes, two policies: a fast, reactive policy (e.g., a deepneural network) deployed at test time, and a slow, non-reactive policy (e.g.,Tree Search), that can plan multiple steps ahead. The reactive policy isupdated under supervision from the non-reactive policy, while the non-reactivepolicy is improved with guidance from the reactive policy. In this work westudy this Dual Policy Iteration (DPI) strategy in an alternating optimizationframework and provide a convergence analysis that extends existing API theory.We also develop a special instance of this framework which reduces the updateof non-reactive policies to model-based optimal control using learned localmodels, and provides a theoretically sound way of unifying model-free andmodel-based RL approaches with unknown dynamics. We demonstrate the efficacy ofour approach on various continuous control Markov Decision Processes.

Truncated Horizon Policy Search: Combining Reinforcement Learning &  Imitation Learning

  In this paper, we propose to combine imitation and reinforcement learning viathe idea of reward shaping using an oracle. We study the effectiveness of thenear-optimal cost-to-go oracle on the planning horizon and demonstrate that thecost-to-go oracle shortens the learner's planning horizon as function of itsaccuracy: a globally optimal oracle can shorten the planning horizon to one,leading to a one-step greedy Markov Decision Process which is much easier tooptimize, while an oracle that is far away from the optimality requiresplanning over a longer horizon to achieve near-optimal performance. Hence ournew insight bridges the gap and interpolates between imitation learning andreinforcement learning. Motivated by the above mentioned insights, we proposeTruncated HORizon Policy Search (THOR), a method that focuses on searching forpolicies that maximize the total reshaped reward over a finite planning horizonwhen the oracle is sub-optimal. We experimentally demonstrate that agradient-based implementation of THOR can achieve superior performance comparedto RL baselines and IL baselines even when the oracle is sub-optimal.

Accelerating Imitation Learning with Predictive Models

  Sample efficiency is critical in solving real-world reinforcement learningproblems, where agent-environment interactions can be costly. Imitationlearning from expert advice has proved to be an effective strategy for reducingthe number of interactions required to train a policy. Online imitationlearning, which interleaves policy evaluation and policy optimization, is aparticularly effective technique with provable performance guarantees. In thiswork, we seek to further accelerate the convergence rate of online imitationlearning, thereby making it more sample efficient. We propose two model-basedalgorithms inspired by Follow-the-Leader (FTL) with prediction: MoBIL-VI basedon solving variational inequalities and MoBIL-Prox based on stochasticfirst-order updates. These two methods leverage a model to predict futuregradients to speed up policy learning. When the model oracle is learned online,these algorithms can provably accelerate the best known convergence rate up toan order. Our algorithms can be viewed as a generalization of stochasticMirror-Prox (Juditsky et al., 2011), and admit a simple constructive FTL-styleanalysis of performance.

Improving Image Clustering With Multiple Pretrained CNN Feature  Extractors

  For many image clustering problems, replacing raw image data with featuresextracted by a pretrained convolutional neural network (CNN), leads to betterclustering performance. However, the specific features extracted, and, byextension, the selected CNN architecture, can have a major impact on theclustering results. In practice, this crucial design choice is often decidedarbitrarily due to the impossibility of using cross-validation withunsupervised learning problems. However, information contained in the differentpretrained CNN architectures may be complementary, even when pretrained on thesame data. To improve clustering performance, we rephrase the image clusteringproblem as a multi-view clustering (MVC) problem that considers multipledifferent pretrained feature extractors as different "views" of the same data.We then propose a multi-input neural network architecture that is trainedend-to-end to solve the MVC problem effectively. Our experimental results,conducted on three different natural image datasets, show that: 1. usingmultiple pretrained CNNs jointly as feature extractors improves imageclustering; 2. using an end-to-end approach improves MVC; and 3. combining bothproduces state-of-the-art results for the problem of image clustering.

STEAP: simultaneous trajectory estimation and planning

  We present a unified probabilistic framework for simultaneous trajectoryestimation and planning (STEAP). Estimation and planning problems are usuallyconsidered separately, however, within our framework we show that solving themsimultaneously can be more accurate and efficient. The key idea is to computethe full continuous-time trajectory from start to goal at each time-step. Whilethe robot traverses the trajectory, the history portion of the trajectorysignifies the solution to the estimation problem, and the future portion of thetrajectory signifies a solution to the planning problem. Building on recentprobabilistic inference approaches to continuous-time localization and mappingand continuous-time motion planning, we solve the joint problem by iterativelyrecomputing the maximum a posteriori trajectory conditioned on all availablesensor data and cost information. Our approach can contend withhigh-degree-of-freedom (DOF) trajectory spaces, uncertainty due to limitedsensing capabilities, model inaccuracy, the stochastic effect of executingactions, and can find a solution in real-time. We evaluate our frameworkempirically in both simulation and on a mobile manipulator.

Learning Generalizable Robot Skills from Demonstrations in Cluttered  Environments

  Learning from Demonstration (LfD) is a popular approach to endowing robotswith skills without having to program them by hand. Typically, LfD relies onhuman demonstrations in clutter-free environments. This prevents thedemonstrations from being affected by irrelevant objects, whose influence canobfuscate the true intention of the human or the constraints of the desiredskill. However, it is unrealistic to assume that the robot's environment canalways be restructured to remove clutter when capturing human demonstrations.To contend with this problem, we develop an importance weighted batch andincremental skill learning approach, building on a recent inference-basedtechnique for skill representation and reproduction. Our approach reducesunwanted environmental influences on the learned skill, while still capturingthe salient human behavior. We provide both batch and incremental versions ofour approach and validate our algorithms on a 7-DOF JACO2 manipulator withreaching and placing skills.

Learning to Align Images using Weak Geometric Supervision

  Image alignment tasks require accurate pixel correspondences, which areusually recovered by matching local feature descriptors. Such descriptors areoften derived using supervised learning on existing datasets with ground truthcorrespondences. However, the cost of creating such datasets is usuallyprohibitive. In this paper, we propose a new approach to align two imagesrelated by an unknown 2D homography where the local descriptor is learned fromscratch from the images and the homography is estimated simultaneously. Our keyinsight is that a siamese convolutional neural network can be trained jointlywhile iteratively updating the homography parameters by optimizing a singleloss function. Our method is currently weakly supervised because the inputimages need to be roughly aligned.  We have used this method to align images of different modalities such as RGBand near-infra-red (NIR) without using any prior labeled data. Imagesautomatically aligned by our method were then used to train descriptors thatgeneralize to new images. We also evaluated our method on RGB images. On theHPatches benchmark, our method achieves comparable accuracy to deep localdescriptors that were trained offline in a supervised setting.

Adversarial Imitation via Variational Inverse Reinforcement Learning

  We consider a problem of learning the reward and policy from expert examplesunder unknown dynamics. Our proposed method builds on the framework ofgenerative adversarial networks and introduces the empowerment-regularizedmaximum-entropy inverse reinforcement learning to learn near-optimal rewardsand policies. Empowerment-based regularization prevents the policy fromoverfitting to expert demonstrations, which advantageously leads to moregeneralized behaviors that result in learning near-optimal rewards. Our methodsimultaneously learns empowerment through variational information maximizationalong with the reward and policy under the adversarial learning formulation. Weevaluate our approach on various high-dimensional complex control tasks. Wealso test our learned rewards in challenging transfer learning problems wheretraining and testing environments are made to be different from each other interms of dynamics or structure. The results show that our proposed method notonly learns near-optimal rewards and policies that are matching expert behaviorbut also performs significantly better than state-of-the-art inversereinforcement learning algorithms.

Predictor-Corrector Policy Optimization

  We present a predictor-corrector framework, called PicCoLO, that cantransform a first-order model-free reinforcement or imitation learningalgorithm into a new hybrid method that leverages predictive models toaccelerate policy learning. The new "PicCoLOed" algorithm optimizes a policy byrecursively repeating two steps: In the Prediction Step, the learner uses amodel to predict the unseen future gradient and then applies the predictedestimate to update the policy; in the Correction Step, the learner runs theupdated policy in the environment, receives the true gradient, and thencorrects the policy using the gradient error. Unlike previous algorithms,PicCoLO corrects for the mistakes of using imperfect predicted gradients andhence does not suffer from model bias. The development of PicCoLO is madepossible by a novel reduction from predictable online learning to adversarialonline learning, which provides a systematic way to modify existing first-orderalgorithms to achieve the optimal regret with respect to predictableinformation. We show, in both theory and simulation, that the convergence rateof several first-order model-free algorithms can be improved by PicCoLO.

Truncated Back-propagation for Bilevel Optimization

  Bilevel optimization has been recently revisited for designing and analyzingalgorithms in hyperparameter tuning and meta learning tasks. However, due toits nested structure, evaluating exact gradients for high-dimensional problemsis computationally challenging. One heuristic to circumvent this difficulty isto use the approximate gradient given by performing truncated back-propagationthrough the iterative optimization procedure that solves the lower-levelproblem. Although promising empirical performance has been reported, itstheoretical properties are still unclear. In this paper, we analyze theproperties of this family of approximate gradients and establish sufficientconditions for convergence. We validate this on several hyperparameter tuningand meta learning tasks. We find that optimization with the approximategradient computed using few-step back-propagation often performs comparably tooptimization with the exact gradient, while requiring far less memory and halfthe computation time.

Learning and Inference in Hilbert Space with Quantum Graphical Models

  Quantum Graphical Models (QGMs) generalize classical graphical models byadopting the formalism for reasoning about uncertainty from quantum mechanics.Unlike classical graphical models, QGMs represent uncertainty with densitymatrices in complex Hilbert spaces. Hilbert space embeddings (HSEs) alsogeneralize Bayesian inference in Hilbert spaces. We investigate the linkbetween QGMs and HSEs and show that the sum rule and Bayes rule for QGMs areequivalent to the kernel sum rule in HSEs and a special case of Nadaraya-Watsonkernel regression, respectively. We show that these operations can bekernelized, and use these insights to propose a Hilbert Space Embedding ofHidden Quantum Markov Models (HSE-HQMM) to model dynamics. We presentexperimental results showing that HSE-HQMMs are competitive withstate-of-the-art models like LSTMs and PSRNNs on several datasets, while alsoproviding a nonparametric method for maintaining a probability distributionover continuous-valued features.

Differentiable MPC for End-to-end Planning and Control

  We present foundations for using Model Predictive Control (MPC) as adifferentiable policy class for reinforcement learning in continuous state andaction spaces. This provides one way of leveraging and combining the advantagesof model-free and model-based approaches. Specifically, we differentiatethrough MPC by using the KKT conditions of the convex approximation at a fixedpoint of the controller. Using this strategy, we are able to learn the cost anddynamics of a controller via end-to-end learning. Our experiments focus onimitation learning in the pendulum and cartpole domains, where we learn thecost and dynamics terms of an MPC policy class. We show that our MPC policiesare significantly more data-efficient than a generic neural network and thatour method is superior to traditional system identification in a setting wherethe expert is unrealizable.

RMPflow: A Computational Graph for Automatic Motion Policy Generation

  We develop a novel policy synthesis algorithm, RMPflow, based ongeometrically consistent transformations of Riemannian Motion Policies (RMPs).RMPs are a class of reactive motion policies designed to parameterizenon-Euclidean behaviors as dynamical systems in intrinsically nonlinear taskspaces. Given a set of RMPs designed for individual tasks, RMPflow canconsistently combine these local policies to generate an expressive globalpolicy, while simultaneously exploiting sparse structure for computationalefficiency. We study the geometric properties of RMPflow and provide sufficientconditions for stability. Finally, we experimentally demonstrate thataccounting for the geometry of task policies can simplify classically difficultproblems, such as planning through clutter on high-DOF manipulation systems.

Multi-Objective Policy Generation for Multi-Robot Systems Using  Riemannian Motion Policies

  In the multi-robot systems literature, control policies are typicallyobtained through descent rules for a potential function which encodes a singleteam-level objective. However, for multi-objective tasks, it can be hard todesign a single control policy that fulfills all the objectives. In this paper,we exploit the idea of decomposing the multi-objective task into a set ofsimple subtasks. We associate each subtask with a potentially lower-dimensionalmanifold, and design Riemannian Motion Policies (RMPs) on these manifolds.Centralized and decentralized algorithms are proposed to combine these policiesinto a final control policy on the configuration space that the robots canexecute. We propose a collection of RMPs for simple multi-robot tasks that canbe used for building controllers for more complicated tasks. In particular, weprove that many existing multi-robot controllers can be closely approximated bycombining the proposed RMPs. Theoretical analysis shows that the multi-robotsystem under the generated control policy is stable. The proposed framework isvalidated through both simulated tasks and robotic implementations.

Online Learning with Continuous Variations: Dynamic Regret and  Reductions

  We study the dynamic regret of a new class of online learning problems, inwhich the gradient of the loss function changes continuously across rounds withrespect to the learner's decisions. This setup is motivated by the use ofonline learning as a tool to analyze the performance of iterative algorithms.Our goal is to identify interpretable dynamic regret rates that explicitlyconsider the loss variations as consequences of the learner's decisions asopposed to external constraints. We show that achieving sublinear dynamicregret in general is equivalent to solving certain variational inequalities,equilibrium problems, and fixed-point problems. Leveraging this identification,we present necessary and sufficient conditions for the existence of efficientalgorithms that achieve sublinear dynamic regret. Furthermore, we show areduction from dynamic regret to both static regret and convergence rate toequilibriums in the aforementioned problems, which allows us to analyze thedynamic regret of many existing learning algorithms in few steps.

An Online Learning Approach to Model Predictive Control

  Model predictive control (MPC) is a powerful technique for solving dynamiccontrol tasks. In this paper, we show that there exists a close connectionbetween MPC and online learning, an abstract theoretical framework foranalyzing online decision making in the optimization literature. This newperspective provides a foundation for leveraging powerful online learningalgorithms to design MPC algorithms. Specifically, we propose a new algorithmbased on dynamic mirror descent (DMD), an online learning algorithm that isdesigned for non-stationary setups. Our algorithm, Dynamic Mirror Decent ModelPredictive Control (DMD-MPC), represents a general family of MPC algorithmsthat includes many existing techniques as special instances. DMD-MPC alsoprovides a fresh perspective on previous heuristics used in MPC and suggests aprincipled way to design new MPC algorithms. In the experimental section ofthis paper, we demonstrate the flexibility of DMD-MPC, presenting a set of newMPC algorithms on a simple simulated cartpole and a simulated and real-worldaggressive driving task.

Learning Quantum Graphical Models using Constrained Gradient Descent on  the Stiefel Manifold

  Quantum graphical models (QGMs) extend the classical framework for reasoningabout uncertainty by incorporating the quantum mechanical view of probability.Prior work on QGMs has focused on hidden quantum Markov models (HQMMs), whichcan be formulated using quantum analogues of the sum rule and Bayes rule usedin classical graphical models. Despite the focus on developing the QGMframework, there has been little progress in learning these models from data.The existing state-of-the-art approach randomly initializes parameters anditeratively finds unitary transformations that increase the likelihood of thedata. While this algorithm demonstrated theoretical strengths of HQMMs overHMMs, it is slow and can only handle a small number of hidden states. In thispaper, we tackle the learning problem by solving a constrained optimizationproblem on the Stiefel manifold using a well-known retraction-based algorithm.We demonstrate that this approach is not only faster and yields bettersolutions on several datasets, but also scales to larger models that wereprohibitively slow to train via the earlier method.

Stable, Concurrent Controller Composition for Multi-Objective Robotic  Tasks

  Robotic systems often need to consider multiple tasks concurrently. Thischallenge calls for control synthesis algorithms that are capable of fulfillingmultiple control specifications simultaneously while maintaining the stabilityof the overall system. In this paper, we decompose complex, multi-objectivetasks into subtasks, where individual subtask controllers are designedindependently and then combined to generate the overall control policy. Inparticular, we adopt Riemannian Motion Policies (RMPs), a recently proposedcontroller structure in robotics, and, RMPflow, its associated computationalframework for combining RMP controllers. We re-establish and extend thestability results of RMPflow through a rigorous Control Lyapunov Function (CLF)treatment. We then show that RMPflow can stably combine individually designedsubtask controllers that satisfy certain CLF constraints. This new insightleads to an efficient CLF-based computational framework to generate stablecontrollers that consider all the subtasks simultaneously. Compared with theoriginal usage of RMPflow, our framework provides users the flexibility toincorporate design heuristics through nominal controllers for the subtasks. Wevalidate the proposed computational framework through numerical simulation androbotic implementation.

Reduced-Rank Hidden Markov Models

  We introduce the Reduced-Rank Hidden Markov Model (RR-HMM), a generalizationof HMMs that can model smooth state evolution as in Linear Dynamical Systems(LDSs) as well as non-log-concave predictive distributions as incontinuous-observation HMMs. RR-HMMs assume an m-dimensional latent state and ndiscrete observations, with a transition matrix of rank k <= m. This impliesthe dynamics evolve in a k-dimensional subspace, while the shape of the set ofpredictive distributions is determined by m. Latent state belief is representedwith a k-dimensional state vector and inference is carried out entirely in R^k,making RR-HMMs as computationally efficient as k-state HMMs yet moreexpressive. To learn RR-HMMs, we relax the assumptions of a recently proposedspectral learning algorithm for HMMs (Hsu, Kakade and Zhang 2009) and apply itto learn k-dimensional observable representations of rank-k RR-HMMs. Thealgorithm is consistent and free of local optima, and we extend its performanceguarantees to cover the RR-HMM case. We show how this algorithm can be used inconjunction with a kernel density estimator to efficiently modelhigh-dimensional multivariate continuous data. We also relax the assumptionthat single observations are sufficient to disambiguate state, and extend thealgorithm accordingly. Experiments on synthetic data and a toy video, as wellas on a difficult robot vision modeling problem, yield accurate models thatcompare favorably with standard alternatives in simulation quality andprediction capability.

Functional Gradient Motion Planning in Reproducing Kernel Hilbert Spaces

  We introduce a functional gradient descent trajectory optimization algorithmfor robot motion planning in Reproducing Kernel Hilbert Spaces (RKHSs).Functional gradient algorithms are a popular choice for motion planning incomplex many-degree-of-freedom robots, since they (in theory) work by directlyoptimizing within a space of continuous trajectories to avoid obstacles whilemaintaining geometric properties such as smoothness. However, in practice,functional gradient algorithms typically commit to a fixed, finiteparameterization of trajectories, often as a list of waypoints. Such aparameterization can lose much of the benefit of reasoning in a continuoustrajectory space: e.g., it can require taking an inconveniently small step sizeand large number of iterations to maintain smoothness. Our work generalizesfunctional gradient trajectory optimization by formulating it as minimizationof a cost functional in an RKHS. This generalization lets us representtrajectories as linear combinations of kernel functions, without any need forwaypoints. As a result, we are able to take larger steps and achieve a locallyoptimal trajectory in just a few iterations. Depending on the selection ofkernel, we can directly optimize in spaces of trajectories that are inherentlysmooth in velocity, jerk, curvature, etc., and that have a low-dimensional,adaptively chosen parameterization. Our experiments illustrate theeffectiveness of the planner for different kernels, including Gaussian RBFs,Laplacian RBFs, and B-splines, as compared to the standard discretized waypointrepresentation.

Learning from Conditional Distributions via Dual Embeddings

  Many machine learning tasks, such as learning with invariance and policyevaluation in reinforcement learning, can be characterized as problems oflearning from conditional distributions. In such problems, each sample $x$itself is associated with a conditional distribution $p(z|x)$ represented bysamples $\{z_i\}_{i=1}^M$, and the goal is to learn a function $f$ that linksthese conditional distributions to target values $y$. These learning problemsbecome very challenging when we only have limited samples or in the extremecase only one sample from each conditional distribution. Commonly usedapproaches either assume that $z$ is independent of $x$, or require anoverwhelmingly large samples from each conditional distribution.  To address these challenges, we propose a novel approach which employs a newmin-max reformulation of the learning from conditional distribution problem.With such new reformulation, we only need to deal with the joint distribution$p(z,x)$. We also design an efficient learning algorithm, Embedding-SGD, andestablish theoretical sample complexity for such problems. Finally, ournumerical experiments on both synthetic and real-world datasets show that theproposed approach can significantly improve over the existing algorithms.

Continuous-Time Gaussian Process Motion Planning via Probabilistic  Inference

  We introduce a novel formulation of motion planning, for continuous-timetrajectories, as probabilistic inference. We first show how smoothcontinuous-time trajectories can be represented by a small number of statesusing sparse Gaussian process (GP) models. We next develop an efficientgradient-based optimization algorithm that exploits this sparsity and GPinterpolation. We call this algorithm the Gaussian Process Motion Planner(GPMP). We then detail how motion planning problems can be formulated asprobabilistic inference on a factor graph. This forms the basis for GPMP2, avery efficient algorithm that combines GP representations of trajectories withfast, structure-exploiting inference via numerical optimization. Finally, weextend GPMP2 to an incremental algorithm, iGPMP2, that can efficiently replanwhen conditions change. We benchmark our algorithms against severalsampling-based and trajectory optimization-based motion planning algorithms onplanning problems in multiple environments. Our evaluation reveals that GPMP2is several times faster than previous algorithms while retaining robustness. Wealso benchmark iGPMP2 on replanning problems, and show that it can findsuccessful solutions in a fraction of the time required by GPMP2 to replan fromscratch.

Predictive-State Decoders: Encoding the Future into Recurrent Networks

  Recurrent neural networks (RNNs) are a vital modeling technique that rely oninternal states learned indirectly by optimization of a supervised,unsupervised, or reinforcement training loss. RNNs are used to model dynamicprocesses that are characterized by underlying latent states whose form isoften unknown, precluding its analytic representation inside an RNN. In thePredictive-State Representation (PSR) literature, latent state processes aremodeled by an internal state representation that directly models thedistribution of future observations, and most recent work in this area hasrelied on explicitly representing and targeting sufficient statistics of thisprobability distribution. We seek to combine the advantages of RNNs and PSRs byaugmenting existing state-of-the-art recurrent neural networks withPredictive-State Decoders (PSDs), which add supervision to the network'sinternal state representation to target predicting future observations.Predictive-State Decoders are simple to implement and easily incorporated intoexisting training pipelines via additional loss regularization. We demonstratethe effectiveness of PSDs with experimental results in three different domains:probabilistic filtering, Imitation Learning, and Reinforcement Learning. Ineach, our method improves statistical performance of state-of-the-art recurrentbaselines and does so with fewer iterations and less data.

Semantically Meaningful View Selection

  An understanding of the nature of objects could help robots to solve bothhigh-level abstract tasks and improve performance at lower-level concretetasks. Although deep learning has facilitated progress in image understanding,a robot's performance in problems like object recognition often depends on theangle from which the object is observed. Traditionally, robot sorting tasksrely on a fixed top-down view of an object. By changing its viewing angle, arobot can select a more semantically informative view leading to betterperformance for object recognition. In this paper, we introduce the problem ofsemantic view selection, which seeks to find good camera poses to gain semanticknowledge about an observed object. We propose a conceptual formulation of theproblem, together with a solvable relaxation based on clustering. We thenpresent a new image dataset consisting of around 10k images representingvarious views of 144 objects under different poses. Finally we use this datasetto propose a first solution to the problem by training a neural network topredict a "semantic score" from a top view image and camera pose. The viewspredicted to have higher scores are then shown to provide better clusteringresults than fixed top-down views.

Orthogonally Decoupled Variational Gaussian Processes

  Gaussian processes (GPs) provide a powerful non-parametric framework forreasoning over functions. Despite appealing theory, its superlinearcomputational and memory complexities have presented a long-standing challenge.State-of-the-art sparse variational inference methods trade modeling accuracyagainst complexity. However, the complexities of these methods still scalesuperlinearly in the number of basis functions, implying that that sparse GPmethods are able to learn from large datasets only when a small model is used.Recently, a decoupled approach was proposed that removes the unnecessarycoupling between the complexities of modeling the mean and the covariancefunctions of a GP. It achieves a linear complexity in the number of meanparameters, so an expressive posterior mean function can be modeled. Whilepromising, this approach suffers from optimization difficulties due toill-conditioning and non-convexity. In this work, we propose an alternativedecoupled parametrization. It adopts an orthogonal basis in the mean functionto model the residues that cannot be learned by the standard coupled approach.Therefore, our method extends, rather than replaces, the coupled approach toachieve strictly better performance. This construction admits a straightforwardnatural gradient update rule, so the structure of the information manifold thatis lost during decoupling can be leveraged to speed up learning. Empirically,our algorithm demonstrates significantly faster convergence in multipleexperiments.

Robust Learning of Tactile Force Estimation through Robot Interaction

  Current methods for estimating force from tactile sensor signals are eitherinaccurate analytic models or task-specific learned models. In this paper, weexplore learning a robust model that maps tactile sensor signals to force. Wespecifically explore learning a mapping for the SynTouch BioTac sensor vianeural networks. We propose a voxelized input feature layer for spatial signalsand leverage information about the sensor surface to regularize the lossfunction. To learn a robust tactile force model that transfers across tasks, wegenerate ground truth data from three different sources: (1) the BioTac rigidlymounted to a force torque~(FT) sensor, (2) a robot interacting with a ballrigidly attached to the same FT sensor, and (3) through force inference on aplanar pushing task by formalizing the mechanics as a system of particles andoptimizing over the object motion. A total of 140k samples were collected fromthe three sources. We achieve a median angular accuracy of 3.5 degrees inpredicting force direction (66% improvement over the current state of the art)and a median magnitude accuracy of 0.06 N (93% improvement) on a test dataset.Additionally, we evaluate the learned force model in a force feedback graspcontroller performing object lifting and gentle placement. Our results can befound on https://sites.google.com/view/tactile-force.

Joint Inference of Kinematic and Force Trajectories with Visuo-Tactile  Sensing

  To perform complex tasks, robots must be able to interact with and manipulatetheir surroundings. One of the key challenges in accomplishing this is robuststate estimation during physical interactions, where the state involves notonly the robot and the object being manipulated, but also the state of thecontact itself. In this work, within the context of planar pushing, we extendprevious inference-based approaches to state estimation in several ways. Weestimate the robot, object, and the contact state on multiple manipulationplatforms configured with a vision-based articulated model tracker, and eithera biomimetic tactile sensor or a force-torque sensor. We show how to fuse rawmeasurements from the tracker and tactile sensors to jointly estimate thetrajectory of the kinematic states and the forces in the system viaprobabilistic inference on factor graphs, in both batch and incrementalsettings. We perform several benchmarks with our framework and show howperformance is affected by incorporating various geometric and physics basedconstraints, occluding vision sensors, or injecting noise in tactile sensors.We also compare with prior work on multiple datasets and demonstrate that ourapproach can effectively optimize over multi-modal sensor data and reduceuncertainty to find better state estimates.

