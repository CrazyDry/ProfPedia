On the Spread of Random Interleaver

  For a given blocklength we determine the number of interleavers which have
spread equal to two. Using this, we find out the probability that a randomly
chosen interleaver has spread two. We show that as blocklength increases, this
probability increases but very quickly converges to the value $1-e^{-2} \approx
0.8647$. Subsequently, we determine a lower bound on the probability of an
interleaver having spread at least $s$. We show that this lower bound converges
to the value $e^{-2(s-2)^{2}}$, as the blocklength increases.


Construction of Turbo Code Interleavers from 3-Regular Hamiltonian
  Graphs

  In this letter we present a new construction of interleavers for turbo codes
from 3-regular Hamiltonian graphs. The interleavers can be generated using a
few parameters, which can be selected in such a way that the girth of the
interleaver graph (IG) becomes large, inducing a high summary distance. The
size of the search space for these parameters is derived. The proposed
interleavers themselves work as their de-interleavers.


On a Duality Between Recoverable Distributed Storage and Index Coding

  In this paper, we introduce a model of a single-failure locally recoverable
distributed storage system. This model appears to give rise to a problem
seemingly dual of the well-studied index coding problem. The relation between
the dimensions of an optimal index code and optimal distributed storage code of
our model has been established in this paper. We also show some extensions to
vector codes.


Capacity of Locally Recoverable Codes

  Motivated by applications in distributed storage, the notion of a locally
recoverable code (LRC) was introduced a few years back. In an LRC, any
coordinate of a codeword is recoverable by accessing only a small number of
other coordinates. While different properties of LRCs have been well-studied,
their performance on channels with random erasures or errors has been mostly
unexplored. In this note, we analyze the performance of LRCs over such
stochastic channels. In particular, for input-symmetric discrete memoryless
channels, we give a tight characterization of the gap to Shannon capacity when
LRCs are used over the channel.


Codes in Permutations and Error Correction for Rank Modulation

  Codes for rank modulation have been recently proposed as a means of
protecting flash memory devices from errors. We study basic coding theoretic
problems for such codes, representing them as subsets of the set of
permutations of $n$ elements equipped with the Kendall tau distance. We derive
several lower and upper bounds on the size of codes. These bounds enable us to
establish the exact scaling of the size of optimal codes for large values of
$n$. We also show the existence of codes whose size is within a constant factor
of the sphere packing bound for any fixed number of errors.


On the Number of Errors Correctable with Codes on Graphs

  We study ensembles of codes on graphs (generalized low-density parity-check,
or LDPC codes) constructed from random graphs and fixed local constrained
codes, and their extension to codes on hypergraphs. It is known that the
average minimum distance of codes in these ensembles grows linearly with the
code length. We show that these codes can correct a linearly growing number of
errors under simple iterative decoding algorithms. In particular, we show that
this property extends to codes constructed by parallel concatenation of Hamming
codes and other codes with small minimum distance. Previously known results
that proved this property for graph codes relied on graph expansion and
required the choice of local codes with large distance relative to their
length.


Coding for High-Density Recording on a 1-D Granular Magnetic Medium

  In terabit-density magnetic recording, several bits of data can be replaced
by the values of their neighbors in the storage medium. As a result, errors in
the medium are dependent on each other and also on the data written. We
consider a simple one-dimensional combinatorial model of this medium. In our
model, we assume a setting where binary data is sequentially written on the
medium and a bit can erroneously change to the immediately preceding value. We
derive several properties of codes that correct this type of errors, focusing
on bounds on their cardinality.
  We also define a probabilistic finite-state channel model of the storage
medium, and derive lower and upper estimates of its capacity. A lower bound is
derived by evaluating the symmetric capacity of the channel, i.e., the maximum
transmission rate under the assumption of the uniform input distribution of the
channel. An upper bound is found by showing that the original channel is a
stochastic degradation of another, related channel model whose capacity we can
compute explicitly.


An Upper Bound On the Size of Locally Recoverable Codes

  In a {\em locally recoverable} or {\em repairable} code, any symbol of a
codeword can be recovered by reading only a small (constant) number of other
symbols. The notion of local recoverability is important in the area of
distributed storage where a most frequent error-event is a single storage node
failure (erasure). A common objective is to repair the node by downloading data
from as few other storage node as possible. In this paper, we bound the minimum
distance of a code in terms of its length, size and locality. Unlike previous
bounds, our bound follows from a significantly simple analysis and depends on
the size of the alphabet being used. It turns out that the binary Simplex codes
satisfy our bound with equality; hence the Simplex codes are the first example
of a optimal binary locally repairable code family. We also provide
achievability results based on random coding and concatenated codes that are
numerically verified to be close to our bounds.


Storage Capacity of Repairable Networks

  In this paper, we introduce a model of a distributed storage system that is
locally recoverable from any single server failure. Unlike the usual local
recovery model of codes for distributed storage, this model accounts for the
fact that each server or storage node in a network is connectible to only some,
and not all other, nodes. This may happen for reasons such as physical
separation, inhomogeneity in storage platforms etc. We estimate the storage
capacity of both undirected and directed networks under this model and propose
some constructive schemes. From a coding theory point of view, we show that
this model is approximately dual of the well-studied index coding problem.
  Further in this paper, we extend the above model to handle multiple server
failures. Among other results, we provide an upper bound on the minimum
pairwise distance of a set of words that can be stored in a graph with the
local repair guarantee. The well-known impossibility bounds on the distance of
locally recoverable codes follow from our result.


On the Capacity of Memoryless Adversary

  In this paper, we study a model of communication under adversarial noise. In
this model, the adversary makes online decisions on whether to corrupt a
transmitted bit based on only the value of that bit. Like the usual binary
symmetric channel of information theory or the fully adversarial channel of
combinatorial coding theory, the adversary can, with high probability,
introduce at most a given fraction of error.
  It is shown that, the capacity (maximum rate of reliable information
transfer) of such memoryless adversary is strictly below that of the binary
symmetric channel. We give new upper bound on the capacity of such channel --
the tightness of this upper bound remains an open question. The main component
of our proof is the careful examination of error-correcting properties of a
code with skewed distance distribution.


Restricted isometry property of random subdictionaries

  We study statistical restricted isometry, a property closely related to
sparse signal recovery, of deterministic sensing matrices of size $m \times N$.
A matrix is said to have a statistical restricted isometry property (StRIP) of
order $k$ if most submatrices with $k$ columns define a near-isometric map of
${\mathbb R}^k$ into ${\mathbb R}^m$. As our main result, we establish
sufficient conditions for the StRIP property of a matrix in terms of the mutual
coherence and mean square coherence. We show that for many existing
deterministic families of sampling matrices, $m=O(k)$ rows suffice for
$k$-StRIP, which is an improvement over the known estimates of either $m =
\Theta(k \log N)$ or $m = \Theta(k\log k)$. We also give examples of matrix
families that are shown to have the StRIP property using our sufficient
conditions.


Group testing schemes from codes and designs

  In group testing, simple binary-output tests are designed to identify a small
number $t$ of defective items that are present in a large population of $N$
items. Each test takes as input a group of items and produces a binary output
indicating whether the group is free of the defective items or contains one or
more of them. In this paper we study a relaxation of the combinatorial group
testing problem. A matrix is called $(t,\epsilon)$-disjunct if it gives rise to
a nonadaptive group testing scheme with the property of identifying a uniformly
random $t$-set of defective subjects out of a population of size $N$ with false
positive probability of an item at most $\epsilon$. We establish a new
connection between $(t,\epsilon)$-disjunct matrices and error correcting codes
based on the dual distance of the codes and derive estimates of the parameters
of codes that give rise to such schemes. Our methods rely on the moments of the
distance distribution of codes and inequalities for moments of sums of
independent random variables. We also provide a new connection between group
testing schemes and combinatorial designs.


A Theoretical Analysis of First Heuristics of Crowdsourced Entity
  Resolution

  Entity resolution (ER) is the task of identifying all records in a database
that refer to the same underlying entity, and are therefore duplicates of each
other. Due to inherent ambiguity of data representation and poor data quality,
ER is a challenging task for any automated process. As a remedy, human-powered
ER via crowdsourcing has become popular in recent years. Using crowd to answer
queries is costly and time consuming. Furthermore, crowd-answers can often be
faulty. Therefore, crowd-based ER methods aim to minimize human participation
without sacrificing the quality and use a computer generated similarity matrix
actively. While, some of these methods perform well in practice, no theoretical
analysis exists for them, and further their worst case performances do not
reflect the experimental findings. This creates a disparity in the
understanding of the popular heuristics for this problem. In this paper, we
make the first attempt to close this gap. We provide a thorough analysis of the
prominent heuristic algorithms for crowd-based ER. We justify experimental
observations with our analysis and information theoretic lower bounds.


Combinatorial Alphabet-Dependent Bounds for Locally Recoverable Codes

  Locally recoverable (LRC) codes have recently been a focus point of research
in coding theory due to their theoretical appeal and applications in
distributed storage systems. In an LRC code, any erased symbol of a codeword
can be recovered by accessing only a small number of other symbols. For LRC
codes over a small alphabet (such as binary), the optimal rate-distance
trade-off is unknown. We present several new combinatorial bounds on LRC codes
including the locality-aware sphere packing and Plotkin bounds. We also develop
an approach to linear programming (LP) bounds on LRC codes. The resulting LP
bound gives better estimates in examples than the other upper bounds known in
the literature. Further, we provide the tightest known upper bound on the rate
of linear LRC codes with a given relative distance, an improvement over the
previous best known bounds.


Robust Gradient Descent via Moment Encoding with LDPC Codes

  This paper considers the problem of implementing large-scale gradient descent
algorithms in a distributed computing setting in the presence of {\em
straggling} processors. To mitigate the effect of the stragglers, it has been
previously proposed to encode the data with an erasure-correcting code and
decode at the master server at the end of the computation. We, instead, propose
to encode the second-moment of the data with a low density parity-check (LDPC)
code. The iterative decoding algorithms for LDPC codes have very low
computational overhead and the number of decoding iterations can be made to
automatically adjust with the number of stragglers in the system. We show that
for a random model for stragglers, the proposed moment encoding based gradient
descent method can be viewed as the stochastic gradient descent method. This
allows us to obtain convergence guarantees for the proposed solution.
Furthermore, the proposed moment encoding based method is shown to outperform
the existing schemes in a real distributed computing setup.


Bounds on the Rate of Linear Locally Repairable Codes over Small
  Alphabets

  Locally repairable codes (LRC) have recently been a subject of intense
research due to theoretical appeal and their application in distributed storage
systems. In an LRC, any coordinate of a codeword can be recovered by accessing
only few other coordinates. For LRCs over small alphabet (such as binary), the
optimal rate-distance trade-off is unknown. In this paper we provide the
tightest known upper bound on the rate of linear LRCs of a given relative
distance, an improvement over any previous result, in particular
\cite{cadambe2013upper}.


Constructions of Rank Modulation Codes

  Rank modulation is a way of encoding information to correct errors in flash
memory devices as well as impulse noise in transmission lines. Modeling rank
modulation involves construction of packings of the space of permutations
equipped with the Kendall tau distance.
  We present several general constructions of codes in permutations that cover
a broad range of code parameters. In particular, we show a number of ways in
which conventional error-correcting codes can be modified to correct errors in
the Kendall space. Codes that we construct afford simple encoding and decoding
algorithms of essentially the same complexity as required to correct errors in
the Hamming metric. For instance, from binary BCH codes we obtain codes
correcting $t$ Kendall errors in $n$ memory cells that support the order of
$n!/(\log_2n!)^t$ messages, for any constant $t= 1,2,...$ We also construct
families of codes that correct a number of errors that grows with $n$ at
varying rates, from $\Theta(n)$ to $\Theta(n^{2})$. One of our constructions
gives rise to a family of rank modulation codes for which the trade-off between
the number of messages and the number of correctable Kendall errors approaches
the optimal scaling rate. Finally, we list a number of possibilities for
constructing codes of finite length, and give examples of rank modulation codes
with specific parameters.


Construction of Almost Disjunct Matrices for Group Testing

  In a \emph{group testing} scheme, a set of tests is designed to identify a
small number $t$ of defective items among a large set (of size $N$) of items.
In the non-adaptive scenario the set of tests has to be designed in one-shot.
In this setting, designing a testing scheme is equivalent to the construction
of a \emph{disjunct matrix}, an $M \times N$ matrix where the union of supports
of any $t$ columns does not contain the support of any other column. In
principle, one wants to have such a matrix with minimum possible number $M$ of
rows (tests). One of the main ways of constructing disjunct matrices relies on
\emph{constant weight error-correcting codes} and their \emph{minimum
distance}. In this paper, we consider a relaxed definition of a disjunct matrix
known as \emph{almost disjunct matrix}. This concept is also studied under the
name of \emph{weakly separated design} in the literature. The relaxed
definition allows one to come up with group testing schemes where a
close-to-one fraction of all possible sets of defective items are identifiable.
Our main contribution is twofold. First, we go beyond the minimum distance
analysis and connect the \emph{average distance} of a constant weight code to
the parameters of an almost disjunct matrix constructed from it. Our second
contribution is to explicitly construct almost disjunct matrices based on our
average distance analysis, that have much smaller number of rows than any
previous explicit construction of disjunct matrices. The parameters of our
construction can be varied to cover a large range of relations for $t$ and $N$.


Update-Efficiency and Local Repairability Limits for Capacity
  Approaching Codes

  Motivated by distributed storage applications, we investigate the degree to
which capacity achieving encodings can be efficiently updated when a single
information bit changes, and the degree to which such encodings can be
efficiently (i.e., locally) repaired when single encoded bit is lost.
  Specifically, we first develop conditions under which optimum
error-correction and update-efficiency are possible, and establish that the
number of encoded bits that must change in response to a change in a single
information bit must scale logarithmically in the block-length of the code if
we are to achieve any nontrivial rate with vanishing probability of error over
the binary erasure or binary symmetric channels. Moreover, we show there exist
capacity-achieving codes with this scaling.
  With respect to local repairability, we develop tight upper and lower bounds
on the number of remaining encoded bits that are needed to recover a single
lost bit of the encoding. In particular, we show that if the code-rate is
$\epsilon$ less than the capacity, then for optimal codes, the maximum number
of codeword symbols required to recover one lost symbol must scale as
$\log1/\epsilon$.
  Several variations on---and extensions of---these results are also developed.


Nonadaptive group testing with random set of defectives

  In a group testing scheme, a set of tests is designed to identify a small
number $t$ of defective items that are present among a large number $N$ of
items. Each test takes as input a group of items and produces a binary output
indicating whether any defective item is present in the group. In a
non-adaptive scheme designing a testing scheme is equivalent to the
construction of a disjunct matrix, an $M \times N$ binary matrix where the
union of supports of any $t$ columns does not contain the support of any other
column. In this paper we consider the scenario where defective items are random
and follow simple probability distributions. In particular we consider the
cases where 1) each item can be defective independently with probability
$\frac{t}{N}$ and 2) each $t$-set of items can be defective with uniform
probability. In both cases our aim is to design a testing matrix that
successfully identifies the set of defectives with high probability. Both of
these models have been studied in the literature before and it is known that
$O(t\log N)$ tests are necessary as well as sufficient (via random coding) in
both cases. Our main focus is explicit deterministic construction of the test
matrices amenable to above scenarios. One of the most popular ways of
constructing test matrices relies on \emph{constant-weight error-correcting
codes} and their minimum distance. We go beyond the minimum distance analysis
and connect the average distance of a constant weight code to the parameters of
the resulting test matrix. With our relaxed requirements, we show that using
explicit constant-weight codes (e.g., based on algebraic geometry codes) we may
achieve a number of tests equal to $O(t \frac{\log^2 N}{ \log t})$ for both the
first and the second cases.


Local Partial Clique Covers for Index Coding

  Index coding, or broadcasting with side information, is a network coding
problem of most fundamental importance. In this problem, given a directed
graph, each vertex represents a user with a need of information, and the
neighborhood of each vertex represents the side information availability to
that user. The aim is to find an encoding to minimum number of bits (optimal
rate) that, when broadcasted, will be sufficient to the need of every user. Not
only the optimal rate is intractable, but it is also very hard to characterize
with some other well-studied graph parameter or with a simpler formulation,
such as a linear program. Recently there have been a series of works that
address this question and provide explicit schemes for index coding as the
optimal value of a linear program with rate given by well-studied properties
such as local chromatic number or partial clique-covering number. There has
been a recent attempt to combine these existing notions of local chromatic
number and partial clique covering into a unified notion denoted as the local
partial clique cover (Arbabjolfaei and Kim, 2014).
  We present a generalized novel upper-bound (encoding scheme) - in the form of
the minimum value of a linear program - for optimal index coding. Our bound
also combines the notions of local chromatic number and partial clique covering
into a new definition of the local partial clique cover, which outperforms both
the previous bounds, as well as beats the previous attempt to combination.
  Further, we look at the upper bound derived recently by Thapa et al., 2015,
and extend their $n$-$\mathsf{GIC}$ (Generalized Interlinked Cycle)
construction to $(k,n)$-$\mathsf{GIC}$ graphs, which are a generalization of
$k$-partial cliques.


Clustering with Noisy Queries

  In this paper, we initiate a rigorous theoretical study of clustering with
noisy queries (or a faulty oracle). Given a set of $n$ elements, our goal is to
recover the true clustering by asking minimum number of pairwise queries to an
oracle. Oracle can answer queries of the form : "do elements $u$ and $v$ belong
to the same cluster?" -- the queries can be asked interactively (adaptive
queries), or non-adaptively up-front, but its answer can be erroneous with
probability $p$. In this paper, we provide the first information theoretic
lower bound on the number of queries for clustering with noisy oracle in both
situations. We design novel algorithms that closely match this query complexity
lower bound, even when the number of clusters is unknown. Moreover, we design
computationally efficient algorithms both for the adaptive and non-adaptive
settings. The problem captures/generalizes multiple application scenarios. It
is directly motivated by the growing body of work that use crowdsourcing for
{\em entity resolution}, a fundamental and challenging data mining task aimed
to identify all records in a database referring to the same entity. Here crowd
represents the noisy oracle, and the number of queries directly relates to the
cost of crowdsourcing. Another application comes from the problem of {\em sign
edge prediction} in social network, where social interactions can be both
positive and negative, and one must identify the sign of all pair-wise
interactions by querying a few pairs. Furthermore, clustering with noisy oracle
is intimately connected to correlation clustering, leading to improvement
therein. Finally, it introduces a new direction of study in the popular {\em
stochastic block model} where one has an incomplete stochastic block model
matrix to recover the clusters.


Query Complexity of Clustering with Side Information

  Suppose, we are given a set of $n$ elements to be clustered into $k$
(unknown) clusters, and an oracle/expert labeler that can interactively answer
pair-wise queries of the form, "do two elements $u$ and $v$ belong to the same
cluster?". The goal is to recover the optimum clustering by asking the minimum
number of queries. In this paper, we initiate a rigorous theoretical study of
this basic problem of query complexity of interactive clustering, and provide
strong information theoretic lower bounds, as well as nearly matching upper
bounds. Most clustering problems come with a similarity matrix, which is used
by an automated process to cluster similar points together. Our main
contribution in this paper is to show the dramatic power of side information
aka similarity matrix on reducing the query complexity of clustering. A
similarity matrix represents noisy pair-wise relationships such as one computed
by some function on attributes of the elements. A natural noisy model is where
similarity values are drawn independently from some arbitrary probability
distribution $f_+$ when the underlying pair of elements belong to the same
cluster, and from some $f_-$ otherwise. We show that given such a similarity
matrix, the query complexity reduces drastically from $\Theta(nk)$ (no
similarity matrix) to $O(\frac{k^2\log{n}}{\cH^2(f_+\|f_-)})$ where $\cH^2$
denotes the squared Hellinger divergence. Moreover, this is also
information-theoretic optimal within an $O(\log{n})$ factor. Our algorithms are
all efficient, and parameter free, i.e., they work without any knowledge of $k,
f_+$ and $f_-$, and only depend logarithmically with $n$. Along the way, our
work also reveals intriguing connection to popular community detection models
such as the {\em stochastic block model}, significantly generalizes them, and
opens up many venues for interesting future research.


Storage Capacity as an Information-Theoretic Vertex Cover and the Index
  Coding Rate

  Motivated by applications in distributed storage, the storage capacity of a
graph was recently defined to be the maximum amount of information that can be
stored across the vertices of a graph such that the information at any vertex
can be recovered from the information stored at the neighboring vertices.
Computing the storage capacity is a fundamental problem in network coding and
is related, or equivalent, to some well-studied problems such as index coding
with side information and generalized guessing games. In this paper, we
consider storage capacity as a natural information-theoretic analogue of the
minimum vertex cover of a graph. Indeed, while it was known that storage
capacity is upper bounded by minimum vertex cover, we show that by treating it
as such we can get a 3/2 approximation for planar graphs, and a 4/3
approximation for triangle-free planar graphs. Since the storage capacity is
intimately related to the index coding rate, we get a 2 approximation of index
coding rate for planar graphs and 3/2 approximation for triangle-free planar
graphs. We also show a polynomial time approximation scheme for the index
coding rate when the alphabet size is constant. We then develop a general
method of "gadget covering" to upper bound the storage capacity in terms of the
average of a set of vertex covers. This method is intuitive and leads to the
exact characterization of storage capacity for various families of graphs. As
an illustrative example, we use this approach to derive the exact storage
capacity of cycles-with-chords, a family of graphs related to outerplanar
graphs. Finally, we generalize the storage capacity notion to include recovery
from partial node failures in distributed storage. We show tight upper and
lower bounds on this partial recovery capacity that scales nicely with the
fraction of failures in a vertex.


High Dimensional Discrete Integration by Hashing and Optimization

  Recently Ermon et al. (2013) pioneered a way to practically compute
approximations to large scale counting or discrete integration problems by
using random hashes. The hashes are used to reduce the counting problem into
many separate discrete optimization problems. The optimization problems then
can be solved by an NP-oracle such as commercial SAT solvers or integer linear
programming (ILP) solvers. In particular, Ermon et al. showed that if the
domain of integration is $\{0,1\}^n$ then it is possible to obtain a solution
within a factor of $16$ of the optimal (a 16-approximation) by this technique.
  In many crucial counting tasks, such as computation of partition function of
ferromagnetic Potts model, the domain of integration is naturally $\{0,1,\dots,
q-1\}^n, q>2$, the hypergrid. The straightforward extension of Ermon et al.'s
method allows a $q^2$-approximation for this problem. For large values of $q$,
this is undesirable. In this paper, we show an improved technique to obtain an
approximation factor of $4+O(1/q^2)$ to this problem. We are able to achieve
this by using an idea of optimization over multiple bins of the hash functions,
that can be easily implemented by inequality constraints, or even in
unconstrained way. Also the burden on the NP-oracle is not increased by our
method (an ILP solver can still be used). Our method extends to the case when
the domain of integration is the symmetric group, and as a result we can obtain
a $(4+o(1))$-approximation of the {\em permanent of} a matrix. All these
results hold assuming the existence of an NP-oracle. We provide experimental
simulation results to support the theoretical guarantees of our algorithms,
including comparison to the popular Markov-Chain-Monte-Carlo (MCMC) methods.


Linear Programming Approximations for Index Coding

  Index coding, a source coding problem over broadcast channels, has been a
subject of both theoretical and practical interest since its introduction (by
Birk and Kol, 1998). In short, the problem can be defined as follows: there is
an input $\textbf{x} \triangleq (\textbf{x}_1, \dots, \textbf{x}_n)$, a set of
$n$ clients who each desire a single symbol $\textbf{x}_i$ of the input, and a
broadcaster whose goal is to send as few messages as possible to all clients so
that each one can recover its desired symbol. Additionally, each client has
some predetermined "side information," corresponding to certain symbols of the
input $\textbf{x}$, which we represent as the "side information graph"
$\mathcal{G}$. The graph $\mathcal{G}$ has a vertex $v_i$ for each client and a
directed edge $(v_i, v_j)$ indicating that client $i$ knows the $j$th symbol of
the input. Given a fixed side information graph $\mathcal{G}$, we are
interested in determining or approximating the "broadcast rate" of index coding
on the graph, i.e. the fewest number of messages the broadcaster can transmit
so that every client gets their desired information.
  Using index coding schemes based on linear programs (LPs), we take a
two-pronged approach to approximating the broadcast rate. First, extending
earlier work on planar graphs, we focus on approximating the broadcast rate for
special graph families such as graphs with small chromatic number and disk
graphs. In certain cases, we are able to show that simple LP-based schemes give
constant-factor approximations of the broadcast rate, which seem extremely
difficult to obtain in the general case. Second, we provide several LP-based
schemes for the general case which are not constant-factor approximations, but
which strictly improve on the prior best-known schemes.


Codes on hypergraphs

  Codes on hypergraphs are an extension of the well-studied family of codes on
bipartite graphs. Bilu and Hoory (2004) constructed an explicit family of codes
on regular t-partite hypergraphs whose minimum distance improves earlier
estimates of the distance of bipartite-graph codes. They also suggested a
decoding algorithm for such codes and estimated its error-correcting
capability.
  In this paper we study two aspects of hypergraph codes. First, we compute the
weight enumerators of several ensembles of such codes, establishing conditions
under which they attain the Gilbert-Varshamov bound and deriving estimates of
their distance. In particular, we show that this bound is attained by codes
constructed on a fixed bipartite graph with a large spectral gap.
  We also suggest a new decoding algorithm of hypergraph codes that corrects a
constant fraction of errors, improving upon the algorithm of Bilu and Hoory.


On linear balancing sets

  Let n be an even positive integer and F be the field \GF(2). A word in F^n is
called balanced if its Hamming weight is n/2. A subset C \subseteq F^n$ is
called a balancing set if for every word y \in F^n there is a word x \in C such
that y + x is balanced. It is shown that most linear subspaces of F^n of
dimension slightly larger than 3/2\log_2(n) are balancing sets. A
generalization of this result to linear subspaces that are "almost balancing"
is also presented. On the other hand, it is shown that the problem of deciding
whether a given set of vectors in F^n spans a balancing set, is NP-hard. An
application of linear balancing sets is presented for designing efficient
error-correcting coding schemes in which the codewords are balanced.


Random Subdictionaries and Coherence Conditions for Sparse Signal
  Recovery

  The most frequently used condition for sampling matrices employed in
compressive sampling is the restricted isometry (RIP) property of the matrix
when restricted to sparse signals. At the same time, imposing this condition
makes it difficult to find explicit matrices that support recovery of signals
from sketches of the optimal (smallest possible)dimension. A number of attempts
have been made to relax or replace the RIP property in sparse recovery
algorithms. We focus on the relaxation under which the near-isometry property
holds for most rather than for all submatrices of the sampling matrix, known as
statistical RIP or StRIP condition. We show that sampling matrices of
dimensions $m\times N$ with maximum coherence $\mu=O((k\log^3 N)^{-1/4})$ and
mean square coherence $\bar \mu^2=O(1/(k\log N))$ support stable recovery of
$k$-sparse signals using Basis Pursuit. These assumptions are satisfied in many
examples. As a result, we are able to construct sampling matrices that support
recovery with low error for sparsity $k$ higher than $\sqrt m,$ which exceeds
the range of parameters of the known classes of RIP matrices.


Cooperative Local Repair in Distributed Storage

  Erasure-correcting codes, that support local repair of codeword symbols, have
attracted substantial attention recently for their application in distributed
storage systems. This paper investigates a generalization of the usual locally
repairable codes. In particular, this paper studies a class of codes with the
following property: any small set of codeword symbols can be reconstructed
(repaired) from a small number of other symbols. This is referred to as
cooperative local repair. The main contribution of this paper is bounds on the
trade-off of the minimum distance and the dimension of such codes, as well as
explicit constructions of families of codes that enable cooperative local
repair. Some other results regarding cooperative local repair are also
presented, including an analysis for the well-known Hadamard/Simplex codes.


Compression in the Space of Permutations

  We investigate lossy compression (source coding) of data in the form of
permutations. This problem has direct applications in the storage of ordinal
data or rankings, and in the analysis of sorting algorithms. We analyze the
rate-distortion characteristic for the permutation space under the uniform
distribution, and the minimum achievable rate of compression that allows a
bounded distortion after recovery. Our analysis is with respect to different
practical and useful distortion measures, including Kendall-tau distance,
Spearman's footrule, Chebyshev distance and inversion-$\ell_1$ distance. We
establish equivalence of source code designs under certain distortions and show
simple explicit code designs that incur low encoding/decoding complexities and
are asymptotically optimal. Finally, we show that for the Mallows model, a
popular nonuniform ranking model on the permutation space, both the entropy and
the maximum distortion at zero rate are much lower than the uniform
counterparts, which motivates the future design of efficient compression
schemes for this model.


Security in Locally Repairable Storage

  In this paper we extend the notion of {\em locally repairable} codes to {\em
secret sharing} schemes. The main problem that we consider is to find optimal
ways to distribute shares of a secret among a set of storage-nodes
(participants) such that the content of each node (share) can be recovered by
using contents of only few other nodes, and at the same time the secret can be
reconstructed by only some allowable subsets of nodes. As a special case, an
eavesdropper observing some set of specific nodes (such as less than certain
number of nodes) does not get any information. In other words, we propose to
study a locally repairable distributed storage system that is secure against a
{\em passive eavesdropper} that can observe some subsets of nodes.
  We provide a number of results related to such systems including upper-bounds
and achievability results on the number of bits that can be securely stored
with these constraints.


Efficient Rank Aggregation via Lehmer Codes

  We propose a novel rank aggregation method based on converting permutations
into their corresponding Lehmer codes or other subdiagonal images. Lehmer
codes, also known as inversion vectors, are vector representations of
permutations in which each coordinate can take values not restricted by the
values of other coordinates. This transformation allows for decoupling of the
coordinates and for performing aggregation via simple scalar median or mode
computations. We present simulation results illustrating the performance of
this completely parallelizable approach and analytically prove that both the
mode and median aggregation procedure recover the correct centroid aggregate
with small sample complexity when the permutations are drawn according to the
well-known Mallows models. The proposed Lehmer code approach may also be used
on partial rankings, with similar performance guarantees.


Estimation of Sparsity via Simple Measurements

  We consider several related problems of estimating the 'sparsity' or number
of nonzero elements $d$ in a length $n$ vector $\mathbf{x}$ by observing only
$\mathbf{b} = M \odot \mathbf{x}$, where $M$ is a predesigned test matrix
independent of $\mathbf{x}$, and the operation $\odot$ varies between problems.
We aim to provide a $\Delta$-approximation of sparsity for some constant
$\Delta$ with a minimal number of measurements (rows of $M$). This framework
generalizes multiple problems, such as estimation of sparsity in group testing
and compressed sensing. We use techniques from coding theory as well as
probabilistic methods to show that $O(D \log D \log n)$ rows are sufficient
when the operation $\odot$ is logical OR (i.e., group testing), and nearly this
many are necessary, where $D$ is a known upper bound on $d$. When instead the
operation $\odot$ is multiplication over $\mathbb{R}$ or a finite field
$\mathbb{F}_q$, we show that respectively $\Theta(D)$ and $\Theta(D \log_q
\frac{n}{D})$ measurements are necessary and sufficient.


Semisupervised Clustering by Queries and Locally Encodable Source Coding

  Source coding is the canonical problem of data compression in information
theory. In a {\em locally encodable} source coding, each compressed bit depends
on only few bits of the input. In this paper, we show that a recently popular
model of semisupervised clustering is equivalent to locally encodable source
coding. In this model, the task is to perform multiclass labeling of unlabeled
elements. At the beginning, we can ask in parallel a set of simple queries to
an oracle who provides (possibly erroneous) binary answers to the queries. The
queries cannot involve more than two (or a fixed constant number $\Delta$ of)
elements. Now the labeling of all the elements (or clustering) must be
performed based on the (noisy) query answers. The goal is to recover all the
correct labelings while minimizing the number of such queries. The equivalence
to locally encodable source codes leads us to find lower bounds on the number
of queries required in variety of scenarios. We are also able to show
fundamental limitations of pairwise `same cluster' queries - and propose
pairwise AND queries, that provably performs better in many situations.


Associative Memory using Dictionary Learning and Expander Decoding

  An associative memory is a framework of content-addressable memory that
stores a collection of message vectors (or a dataset) over a neural network
while enabling a neurally feasible mechanism to recover any message in the
dataset from its noisy version. Designing an associative memory requires
addressing two main tasks: 1) learning phase: given a dataset, learn a concise
representation of the dataset in the form of a graphical model (or a neural
network), 2) recall phase: given a noisy version of a message vector from the
dataset, output the correct message vector via a neurally feasible algorithm
over the network learnt during the learning phase. This paper studies the
problem of designing a class of neural associative memories which learns a
network representation for a large dataset that ensures correction against a
large number of adversarial errors during the recall phase. Specifically, the
associative memories designed in this paper can store dataset containing
$\exp(n)$ $n$-length message vectors over a network with $O(n)$ nodes and can
tolerate $\Omega(\frac{n}{{\rm polylog} n})$ adversarial errors. This paper
carries out this memory design by mapping the learning phase and recall phase
to the tasks of dictionary learning with a square dictionary and iterative
error correction in an expander code, respectively.


The Geometric Block Model

  To capture the inherent geometric features of many community detection
problems, we propose to use a new random graph model of communities that we
call a Geometric Block Model. The geometric block model generalizes the random
geometric graphs in the same way that the well-studied stochastic block model
generalizes the Erdos-Renyi random graphs. It is also a natural extension of
random community models inspired by the recent theoretical and practical
advancement in community detection. While being a topic of fundamental
theoretical interest, our main contribution is to show that many practical
community structures are better explained by the geometric block model. We also
show that a simple triangle-counting algorithm to detect communities in the
geometric block model is near-optimal. Indeed, even in the regime where the
average degree of the graph grows only logarithmically with the number of
vertices (sparse-graph), we show that this algorithm performs extremely well,
both theoretically and practically. In contrast, the triangle-counting
algorithm is far from being optimum for the stochastic block model. We simulate
our results on both real and synthetic datasets to show superior performance of
both the new model as well as our algorithm.


Novel Impossibility Results for Group-Testing

  In this work we prove non-trivial impossibility results for perhaps the
simplest non-linear estimation problem, that of {\it Group Testing} (GT), via
the recently developed Madiman-Tetali inequalities. Group Testing concerns
itself with identifying a hidden set of $d$ defective items from a set of $n$
items via $t$ {disjunctive/pooled} measurements ("group tests"). We consider
the linear sparsity regime, i.e. $d = \delta n$ for any constant $\delta >0$, a
hitherto little-explored (though natural) regime. In a standard
information-theoretic setting, where the tests are required to be non-adaptive
and a small probability of reconstruction error is allowed, our lower bounds on
$t$ are the {\it first} that improve over the classical counting lower bound,
$t/n \geq H(\delta)$, where $H(\cdot)$ is the binary entropy function. As
corollaries of our result, we show that (i) for $\delta \gtrsim 0.347$,
individual testing is essentially optimal, i.e., $t \geq n(1-o(1))$; and (ii)
there is an {adaptivity gap}, since for $\delta \in (0.3471,0.3819)$ known
{adaptive} GT algorithms require fewer than $n$ tests to reconstruct ${\cal
D}$, whereas our bounds imply that the best nonadaptive algorithm must
essentially be individual testing of each element. Perhaps most importantly,
our work provides a framework for combining combinatorial and
information-theoretic methods for deriving non-trivial lower bounds for a
variety of non-linear estimation problems.


Representation Learning and Recovery in the ReLU Model

  Rectified linear units, or ReLUs, have become the preferred activation
function for artificial neural networks. In this paper we consider two basic
learning problems assuming that the underlying data follow a generative model
based on a ReLU-network -- a neural network with ReLU activations. As a
primarily theoretical study, we limit ourselves to a single-layer network. The
first problem we study corresponds to dictionary-learning in the presence of
nonlinearity (modeled by the ReLU functions). Given a set of observation
vectors $\mathbf{y}^i \in \mathbb{R}^d, i =1, 2, \dots , n$, we aim to recover
$d\times k$ matrix $A$ and the latent vectors $\{\mathbf{c}^i\} \subset
\mathbb{R}^k$ under the model $\mathbf{y}^i = \mathrm{ReLU}(A\mathbf{c}^i
+\mathbf{b})$, where $\mathbf{b}\in \mathbb{R}^d$ is a random bias. We show
that it is possible to recover the column space of $A$ within an error of
$O(d)$ (in Frobenius norm) under certain conditions on the probability
distribution of $\mathbf{b}$.
  The second problem we consider is that of robust recovery of the signal in
the presence of outliers, i.e., large but sparse noise. In this setting we are
interested in recovering the latent vector $\mathbf{c}$ from its noisy
nonlinear sketches of the form $\mathbf{v} = \mathrm{ReLU}(A\mathbf{c}) +
\mathbf{e}+\mathbf{w}$, where $\mathbf{e} \in \mathbb{R}^d$ denotes the
outliers with sparsity $s$ and $\mathbf{w} \in \mathbb{R}^d$ denote the dense
but small noise. This line of work has recently been studied (Soltanolkotabi,
2017) without the presence of outliers. For this problem, we show that a
generalized LASSO algorithm is able to recover the signal $\mathbf{c} \in
\mathbb{R}^k$ within an $\ell_2$ error of $O(\sqrt{\frac{(k+s)\log d}{d}})$
when $A$ is a random Gaussian matrix.


Connectivity in Random Annulus Graphs and the Geometric Block Model

  We provide new connectivity results for {\em vertex-random graphs} or {\em
random annulus graphs} which are significant generalizations of random
geometric graphs. Random geometric graphs (RGG) are one of the most basic
models of random graphs for spatial networks proposed by Gilbert in 1961,
shortly after the introduction of the Erd\H{o}s-R\'{en}yi random graphs. They
resemble social networks in many ways (e.g. by spontaneously creating cluster
of nodes with high modularity). The connectivity properties of RGG have been
studied since its introduction, and analyzing them has been significantly
harder than their Erd\H{o}s-R\'{en}yi counterparts due to correlated edge
formation.
  Our next contribution is in using the connectivity of random annulus graphs
to provide necessary and sufficient conditions for efficient recovery of
communities for {\em the geometric block model} (GBM). The GBM is a
probabilistic model for community detection defined over an RGG in a similar
spirit as the popular {\em stochastic block model}, which is defined over an
Erd\H{o}s-R\'{en}yi random graph. The geometric block model inherits the
transitivity properties of RGGs and thus models communities better than a
stochastic block model. However, analyzing them requires fresh perspectives as
all prior tools fail due to correlation in edge formation. We provide a simple
and efficient algorithm that can recover communities in GBM exactly with high
probability in the regime of connectivity.


Low rank approximation and decomposition of large matrices using error
  correcting codes

  Low rank approximation is an important tool used in many applications of
signal processing and machine learning. Recently, randomized sketching
algorithms were proposed to effectively construct low rank approximations and
obtain approximate singular value decompositions of large matrices. Similar
ideas were used to solve least squares regression problems. In this paper, we
show how matrices from error correcting codes can be used to find such low rank
approximations and matrix decompositions, and extend the framework to linear
least squares regression problems. The benefits of using these code matrices
are the following: (i) They are easy to generate and they reduce randomness
significantly. (ii) Code matrices with mild properties satisfy the subspace
embedding property, and have a better chance of preserving the geometry of an
entire subspace of vectors. (iii) For parallel and distributed applications,
code matrices have significant advantages over structured random matrices and
Gaussian random matrices. (iv) Unlike Fourier or Hadamard transform matrices,
which require sampling $O(k\log k)$ columns for a rank-$k$ approximation, the
log factor is not necessary for certain types of code matrices. That is,
$(1+\epsilon)$ optimal Frobenius norm error can be achieved for a rank-$k$
approximation with $O(k/\epsilon)$ samples. (v) Fast multiplication is possible
with structured code matrices, so fast approximations can be achieved for
general dense input matrices. (vi) For least squares regression problem
$\min\|Ax-b\|_2$ where $A\in \mathbb{R}^{n\times d}$, the $(1+\epsilon)$
relative error approximation can be achieved with $O(d/\epsilon)$ samples, with
high probability, when certain code matrices are used.


Clustering Via Crowdsourcing

  In recent years, crowdsourcing, aka human aided computation has emerged as an
effective platform for solving problems that are considered complex for
machines alone. Using human is time-consuming and costly due to monetary
compensations. Therefore, a crowd based algorithm must judiciously use any
information computed through an automated process, and ask minimum number of
questions to the crowd adaptively.
  One such problem which has received significant attention is {\em entity
resolution}. Formally, we are given a graph $G=(V,E)$ with unknown edge set $E$
where $G$ is a union of $k$ (again unknown, but typically large $O(n^\alpha)$,
for $\alpha>0$) disjoint cliques $G_i(V_i, E_i)$, $i =1, \dots, k$. The goal is
to retrieve the sets $V_i$s by making minimum number of pair-wise queries $V
\times V\to\{\pm1\}$ to an oracle (the crowd). When the answer to each query is
correct, e.g. via resampling, then this reduces to finding connected components
in a graph. On the other hand, when crowd answers may be incorrect, it
corresponds to clustering over minimum number of noisy inputs. Even, with
perfect answers, a simple lower and upper bound of $\Theta(nk)$ on query
complexity can be shown. A major contribution of this paper is to reduce the
query complexity to linear or even sublinear in $n$ when mild side information
is provided by a machine, and even in presence of crowd errors which are not
correctable via resampling. We develop new information theoretic lower bounds
on the query complexity of clustering with side information and errors, and our
upper bounds closely match with them. Our algorithms are naturally
parallelizable, and also give near-optimal bounds on the number of adaptive
rounds required to match the query complexity.


