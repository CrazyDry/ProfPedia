Correlated Non-Parametric Latent Feature Models

  We are often interested in explaining data through a set of hidden factors orfeatures. When the number of hidden features is unknown, the Indian BuffetProcess (IBP) is a nonparametric latent feature model that does not bound thenumber of active features in dataset. However, the IBP assumes that all latentfeatures are uncorrelated, making it inadequate for many realworld problems. Weintroduce a framework for correlated nonparametric feature models, generalisingthe IBP. We use this framework to generate several specific models anddemonstrate applications on realworld datasets.

Graph-Sparse LDA: A Topic Model with Structured Sparsity

  Originally designed to model text, topic modeling has become a powerful toolfor uncovering latent structure in domains including medicine, finance, andvision. The goals for the model vary depending on the application: in somecases, the discovered topics may be used for prediction or some otherdownstream task. In other cases, the content of the topic itself may be ofintrinsic scientific interest.  Unfortunately, even using modern sparse techniques, the discovered topics areoften difficult to interpret due to the high dimensionality of the underlyingspace. To improve topic interpretability, we introduce Graph-Sparse LDA, ahierarchical topic model that leverages knowledge of relationships betweenwords (e.g., as encoded by an ontology). In our model, topics are summarized bya few latent concept-words from the underlying graph that explain the observedwords. Graph-Sparse LDA recovers sparse, interpretable summaries on tworeal-world biomedical datasets while matching state-of-the-art predictionperformance.

How do Humans Understand Explanations from Machine Learning Systems? An  Evaluation of the Human-Interpretability of Explanation

  Recent years have seen a boom in interest in machine learning systems thatcan provide a human-understandable rationale for their predictions ordecisions. However, exactly what kinds of explanation are trulyhuman-interpretable remains poorly understood. This work advances ourunderstanding of what makes explanations interpretable in the specific contextof verification. Suppose we have a machine learning system that predicts X, andwe provide rationale for this prediction X. Given an input, an explanation, andan output, is the output consistent with the input and the supposed rationale?Via a series of user-studies, we identify what kinds of increases in complexityhave the greatest effect on the time it takes for humans to verify therationale, and which seem relatively insensitive.

Accountability of AI Under the Law: The Role of Explanation

  The ubiquity of systems using artificial intelligence or "AI" has broughtincreasing attention to how those systems should be regulated. The choice ofhow to regulate AI systems will require care. AI systems have the potential tosynthesize large amounts of data, allowing for greater levels ofpersonalization and precision than ever before---applications range fromclinical decision support to autonomous driving and predictive policing. Thatsaid, there exist legitimate concerns about the intentional and unintentionalnegative consequences of AI systems. There are many ways to hold AI systemsaccountable. In this work, we focus on one: explanation. Questions about alegal right to explanation from AI systems was recently debated in the EUGeneral Data Protection Regulation, and thus thinking carefully about when andhow explanation from AI systems might improve accountability is timely. In thiswork, we review contexts in which explanation is currently required under thelaw, and then list the technical considerations that must be considered if wedesired AI systems that could provide kinds of explanations that are currentlyrequired of humans.

