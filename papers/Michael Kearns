A Clustering Coefficient Network Formation Game

  For the most up-to-date version please visithttp://www.cis.upenn.edu/~brautbar/ccgame.pdf

Efficient Nash Computation in Large Population Games with Bounded  Influence

  We introduce a general representation of large-population games in which eachplayer s influence ON the others IS centralized AND limited, but may otherwisebe arbitrary.This representation significantly generalizes the class known AScongestion games IN a natural way.Our main results are provably correct ANDefficient algorithms FOR computing AND learning approximate Nash equilibria INthis general framework.

Exact Inference of Hidden Structure from Sample Data in Noisy-OR  Networks

  In the literature on graphical models, there has been increased attentionpaid to the problems of learning hidden structure (see Heckerman [H96] forsurvey) and causal mechanisms from sample data [H96, P88, S93, P95, F98]. Inmost settings we should expect the former to be difficult, and the latterpotentially impossible without experimental intervention. In this work, weexamine some restricted settings in which perfectly reconstruct the hiddenstructure solely on the basis of observed sample data.

An Empirical Study of Rich Subgroup Fairness for Machine Learning

  Kearns et al. [2018] recently proposed a notion of rich subgroup fairnessintended to bridge the gap between statistical and individual notions offairness. Rich subgroup fairness picks a statistical fairness constraint (say,equalizing false positive rates across protected groups), but then asks thatthis constraint hold over an exponentially or infinitely large collection ofsubgroups defined by a class of functions with bounded VC dimension. They givean algorithm guaranteed to learn subject to this constraint, under thecondition that it has access to oracles for perfectly learning absent afairness constraint. In this paper, we undertake an extensive empiricalevaluation of the algorithm of Kearns et al. On four real datasets for whichfairness is a concern, we investigate the basic convergence of the algorithmwhen instantiated with fast heuristics in place of learning oracles, measurethe tradeoffs between fairness and accuracy, and compare this approach with therecent algorithm of Agarwal et al. [2018], which implements weaker and moretraditional marginal fairness constraints defined by individual protectedattributes. We find that in general, the Kearns et al. algorithm convergesquickly, large gains in fairness can be obtained with mild costs to accuracy,and that optimizing accuracy subject only to marginal fairness leads toclassifiers with substantial subgroup unfairness. We also provide a number ofanalyses and visualizations of the dynamics and behavior of the Kearns et al.algorithm. Overall we find this algorithm to be effective on real data, andrich subgroup fairness to be a viable notion in practice.

Fast Planning in Stochastic Games

  Stochastic games generalize Markov decision processes (MDPs) to a multiagentsetting by allowing the state transitions to depend jointly on all playeractions, and having rewards determined by multiplayer matrix games at eachstate. We consider the problem of computing Nash equilibria in stochasticgames, the analogue of planning in MDPs. We begin by providing a generalizationof finite-horizon value iteration that computes a Nash strategy for each playerin generalsum stochastic games. The algorithm takes an arbitrary Nash selectionfunction as input, which allows the translation of local choices betweenmultiple Nash equilibria into the selection of a single global Nashequilibrium.  Our main technical result is an algorithm for computing near-Nash equilibriain large or infinite state spaces. This algorithm builds on our finite-horizonvalue iteration algorithm, and adapts the sparse sampling methods of Kearns,Mansour and Ng (1999) to stochastic games. We conclude by descrbing acounterexample showing that infinite-horizon discounted value iteration, whichwas shown by shaplely to converge in the zero-sum case (a result we give extendslightly here), does not converge in the general-sum case.

Empirical Limitations on High Frequency Trading Profitability

  Addressing the ongoing examination of high-frequency trading practices infinancial markets, we report the results of an extensive empirical studyestimating the maximum possible profitability of the most aggressive suchpractices, and arrive at figures that are surprisingly modest. By "aggressive"we mean any trading strategy exclusively employing market orders and relativelyshort holding periods. Our findings highlight the tension between executioncosts and trading horizon confronted by high-frequency traders, and provide acontrolled and large-scale empirical perspective on the high-frequency debatethat has heretofore been absent. Our study employs a number of novel empiricalmethods, including the simulation of an "omniscient" high-frequency trader whocan see the future and act accordingly.

Large Deviation Methods for Approximate Probabilistic Inference

  We study two-layer belief networks of binary random variables in which theconditional probabilities Pr[childlparents] depend monotonically on weightedsums of the parents. In large networks where exact probabilistic inference isintractable, we show how to compute upper and lower bounds on manyprobabilities of interest. In particular, using methods from large deviationtheory, we derive rigorous bounds on marginal probabilities such asPr[children] and prove rates of convergence for the accuracy of our bounds as afunction of network size. Our results apply to networks with generic transferfunction parameterizations of the conditional probability tables, such assigmoid and noisy-OR. They also explicitly illustrate the types of averagingbehavior that can simplify the problem of inference in large networks.

Graphical Models for Bandit Problems

  We introduce a rich class of graphical models for multi-armed bandit problemsthat permit both the state or context space and the action space to be verylarge, yet succinctly specify the payoffs for any context-action pair. Our mainresult is an algorithm for such models whose regret is bounded by the number ofparameters and whose running time depends only on the treewidth of the graphsubstructure induced by the action space.

Budget Optimization for Sponsored Search: Censored Learning in MDPs

  We consider the budget optimization problem faced by an advertiserparticipating in repeated sponsored search auctions, seeking to maximize thenumber of clicks attained under that budget. We cast the budget optimizationproblem as a Markov Decision Process (MDP) with censored observations, andpropose a learning algorithm based on the wellknown Kaplan-Meier orproduct-limit estimator. We validate the performance of this algorithm bycomparing it to several others on a large set of search auction data fromMicrosoft adCenter, demonstrating fast convergence to optimal performance.

Dynamic Actuation of Single-Crystal Diamond Nanobeams

  We show the dielectrophoretic actuation of single-crystal diamondnanomechanical devices. Gradient radio-frequency electromagnetic forces areused to achieve actuation of both cantilever and doubly clamped beamstructures, with operation frequencies ranging from a few MHz to ~50MHz.Frequency tuning and parametric actuation are also studied.

Predicting with Distributions

  We consider a new learning model in which a joint distribution over vectorpairs $(x,y)$ is determined by an unknown function $c(x)$ that maps inputvectors $x$ not to individual outputs, but to entire {\em distributions\/} overoutput vectors $y$. Our main results take the form of rather general reductionsfrom our model to algorithms for PAC learning the function class and thedistribution class separately, and show that virtually every such combinationyields an efficient algorithm in our model. Our methods include a randomizedreduction to classification noise and an application of Le Cam's method toobtain robust learning algorithms.

Competitive Contagion in Networks

  We develop a game-theoretic framework for the study of competition betweenfirms who have budgets to "seed" the initial adoption of their products byconsumers located in a social network. The payoffs to the firms are theeventual number of adoptions of their product through a competitive stochasticdiffusion process in the network. This framework yields a rich class ofcompetitive strategies, which depend in subtle ways on the stochastic dynamicsof adoption, the relative budgets of the players, and the underlying structureof the social network.  We identify a general property of the adoption dynamics --- namely,decreasing returns to local adoption --- for which the inefficiency of resourceuse at equilibrium (the Price of Anarchy) is uniformly bounded above, acrossall networks. We also show that if this property is violated the Price ofAnarchy can be unbounded, thus yielding sharp threshold behavior for a broadclass of dynamics.  We also introduce a new notion, the Budget Multiplier, that measures theextent that imbalances in player budgets can be amplified at equilibrium. Weagain identify a general property of the adoption dynamics --- namely,proportional local adoption between competitors --- for which the (purestrategy) Budget Multiplier is uniformly bounded above, across all networks. Weshow that a violation of this property can lead to unbounded Budget Multiplier,again yielding sharp threshold behavior for a broad class of dynamics.

An Information-Theoretic Analysis of Hard and Soft Assignment Methods  for Clustering

  Assignment methods are at the heart of many algorithms for unsupervisedlearning and clustering - in particular, the well-known K-means andExpectation-Maximization (EM) algorithms. In this work, we study severaldifferent methods of assignment, including the "hard" assignments used byK-means and the ?soft' assignments used by EM. While it is known that K-meansminimizes the distortion on the data and EM maximizes the likelihood, little isknown about the systematic differences of behavior between the two algorithms.Here we shed light on these differences via an information-theoretic analysis.The cornerstone of our results is a simple decomposition of the expecteddistortion, showing that K-means (and its extension for inferring generalparametric densities from unlabeled sample data) must implicitly manage atrade-off between how similar the data assigned to each cluster are, and howthe data are balanced among the clusters. How well the data are balanced ismeasured by the entropy of the partition defined by the hard assignments. Inaddition to letting us predict and verify systematic differences betweenK-means and EM on specific examples, the decomposition allows us to give arather general argument showing that K ?means will consistently find densitieswith less "overlap" than EM. We also study a third natural assignment methodthat we call posterior assignment, that is close in spirit to the softassignments of EM, but leads to a surprisingly different algorithm.

Graphical Models for Game Theory

  In this work, we introduce graphical modelsfor multi-player game theory, andgive powerful algorithms for computing their Nash equilibria in certain cases.An n-player game is given by an undirected graph on n nodes and a set of nlocal matrices. The interpretation is that the payoff to player i is determinedentirely by the actions of player i and his neighbors in the graph, and thusthe payoff matrix to player i is indexed only by these players. We thus viewthe global n-player game as being composed of interacting local games, eachinvolving many fewer players. Each player's action may have global impact, butit occurs through the propagation of local influences.Our main technical resultis an efficient algorithm for computing Nash equilibria when the underlyinggraph is a tree (or can be turned into a tree with few node mergings). Thealgorithm runs in time polynomial in the size of the representation (the graphand theassociated local game matrices), and comes in two related but distinctflavors. The first version involves an approximation step, and computes arepresentation of all approximate Nash equilibria (of which there may be anexponential number in general). The second version allows the exact computationof Nash equilibria at the expense of weakened complexity bounds. The algorithmrequires only local message-passing between nodes (and thus can be implementedby the players themselves in a distributed manner). Despite an analogy toinference in Bayes nets that we develop, the analysis of our algorithm is moreinvolved than that for the polytree algorithm in, owing partially to the factthat we must either compute, or select from, an exponential number of potentialsolutions. We discuss a number of extensions, such as the computation ofequilibria with desirable global properties (e.g. maximizing global return),and directions for further research.

Synaptic Transmission: An Information-Theoretic Perspective

  Here we analyze synaptic transmission from an information-theoreticperspective. We derive closed-form expressions for the lower-bounds on thecapacity of a simple model of a cortical synapse under two explicit codingparadigms. Under the ``signal estimation'' paradigm, we assume the signal to beencoded in the mean firing rate of a Poisson neuron. The performance of anoptimal linear estimator of the signal then provides a lower bound on thecapacity for signal estimation. Under the ``signal detection'' paradigm, thepresence or absence of the signal has to be detected. Performance of theoptimal spike detector allows us to compute a lower bound on the capacity forsignal detection. We find that single synapses (for empirically measuredparameter values) transmit information poorly but significant improvement canbe achieved with a small amount of redundancy.

Triplet Lifetime in Gaseous Argon

  MiniCLEAN is a single-phase liquid argon dark matter experiment. During theinitial cooling phase, impurities within the cold gas ($<$140 K) were monitoredby measuring the scintillation light triplet lifetime, and ultimately a tripletlifetime of 3.480 $\pm$ 0.001 (stat.) $\pm$ 0.064 (sys.) $\mu$s was obtained,indicating ultra-pure argon. This is the longest argon triplet time constantever reported. The effect of quenching of separate components of thescintillation light is also investigated.

Censored Exploration and the Dark Pool Problem

  We introduce and analyze a natural algorithm for multi-venue exploration fromcensored data, which is motivated by the Dark Pool Problem of modernquantitative finance. We prove that our algorithm converges in polynomial timeto a near-optimal allocation policy; prior results for similar problems instochastic inventory control guaranteed only asymptotic convergence andexamined variants in which each venue could be treated independently. Ouranalysis bears a strong resemblance to that of efficient exploration/exploitation schemes in the reinforcement learning literature. We describe anextensive experimental evaluation of our algorithm on the Dark Pool Problemusing real trading data.

Nash Convergence of Gradient Dynamics in Iterated General-Sum Games

  Multi-agent games are becoming an increasing prevalent formalism for thestudy of electronic commerce and auctions. The speed at which transactions cantake place and the growing complexity of electronic marketplaces makes thestudy of computationally simple agents an appealing direction. In this work, weanalyze the behavior of agents that incrementally adapt their strategy throughgradient ascent on expected payoff, in the simple setting of two-player,two-action, iterated general-sum games, and present a surprising result. Weshow that either the agents will converge to Nash equilibrium, or if thestrategies themselves do not converge, then their average payoffs willnevertheless converge to the payoffs of a Nash equilibrium.

Planning the Future of U.S. Particle Physics (Snowmass 2013): Chapter 2:  Intensity Frontier

  These reports present the results of the 2013 Community Summer Study of theAPS Division of Particles and Fields ("Snowmass 2013") on the future program ofparticle physics in the U.S. Chapter 2, on the Intensity Frontier, discussesthe program of research with high-intensity beams and rare processes. This areaincludes experiments on neutrinos, proton decay, charged-lepton and quark weakinteractions, atomic and nuclear probes of fundamental symmetries, and searchesfor new, light, weakly-interacting particles.

A Computational Study of Feasible Repackings in the FCC Incentive  Auctions

  We report the results of a computational study of repacking in the FCCIncentive Auctions. Our interest lies in the structure and constraints of thesolution space of feasible repackings. Our analyses are "mechanism-free", inthe sense that they identify constraints that must hold regardless of thereverse auction mechanism chosen or the prices offered for broadcasterclearing. We examine topics such as the amount of spectrum that can be clearednationwide, the geographic distribution of broadcaster clearings required toreach a clearing target, and the likelihood of reaching clearing targets undervarious models for broadcaster participation. Our study uses FCC interferencedata and a satisfiability-checking approach, and elucidates both theunavoidable mathematical constraints on solutions imposed by interference, aswell as additional constraints imposed by assumptions on the participationdecisions of broadcasters.

Online Learning and Profit Maximization from Revealed Preferences

  We consider the problem of learning from revealed preferences in an onlinesetting. In our framework, each period a consumer buys an optimal bundle ofgoods from a merchant according to her (linear) utility function and currentprices, subject to a budget constraint. The merchant observes only thepurchased goods, and seeks to adapt prices to optimize his profits. We give anefficient algorithm for the merchant's problem that consists of a learningphase in which the consumer's utility function is (perhaps partially) inferred,followed by a price optimization step. We also consider an alternative onlinelearning algorithm for the setting where prices are set exogenously, but themerchant would still like to predict the bundle that will be bought by theconsumer for purposes of inventory or supply chain management. In contrast withmost prior work on the revealed preferences problem, we demonstrate that bymaking stronger assumptions on the form of utility functions, efficientalgorithms for both learning and profit maximization are possible, even inadaptive, online settings.

Privacy for the Protected (Only)

  Motivated by tensions between data privacy for individual citizens, andsocietal priorities such as counterterrorism and the containment of infectiousdisease, we introduce a computational model that distinguishes between partiesfor whom privacy is explicitly protected, and those for whom it is not (thetargeted subpopulation). The goal is the development of algorithms that caneffectively identify and take action upon members of the targeted subpopulationin a way that minimally compromises the privacy of the protected, whilesimultaneously limiting the expense of distinguishing members of the two groupsvia costly mechanisms such as surveillance, background checks, or medicaltesting. Within this framework, we provide provably privacy-preservingalgorithms for targeted search in social networks. These algorithms are naturalvariants of common graph search methods, and ensure privacy for the protectedby the careful injection of noise in the prioritization of potential targets.We validate the utility of our algorithms with extensive computationalexperiments on two large-scale social network datasets.

Robust Mediators in Large Games

  A mediator is a mechanism that can only suggest actions to players, as afunction of all agents' reported types, in a given game of incompleteinformation. We study what is achievable by two kinds of mediators, "strong"and "weak." Players can choose to opt-out of using a strong mediator but cannotmisrepresent their type if they opt-in. Such a mediator is "strong" because wecan view it as having the ability to verify player types. Weak mediators lackthis ability--- players are free to misrepresent their type to a weak mediator.We show a striking result---in a prior-free setting, assuming only that thegame is large and players have private types, strong mediators can implementapproximate equilibria of the complete-information game. If the game is acongestion game, then the same result holds using only weak mediators. Ourresult follows from a novel application of differential privacy, in particular,a variant we propose called joint differential privacy.

Fair Algorithms for Infinite and Contextual Bandits

  We study fairness in linear bandit problems. Starting from the notion ofmeritocratic fairness introduced in Joseph et al. [2016], we carry out a morerefined analysis of a more general problem, achieving better performanceguarantees with fewer modelling assumptions on the number and structure ofavailable choices as well as the number selected. We also analyze thepreviously-unstudied question of fairness in infinite linear bandit problems,obtaining instance-dependent regret upper bounds as well as lower boundsdemonstrating that this instance-dependence is necessary. The result is aframework for meritocratic fairness in an online linear setting that issubstantially more powerful, general, and realistic than the current state ofthe art.

Fairness in Reinforcement Learning

  We initiate the study of fairness in reinforcement learning, where theactions of a learning algorithm may affect its environment and future rewards.Our fairness constraint requires that an algorithm never prefers one actionover another if the long-term (discounted) reward of choosing the latter actionis higher. Our first result is negative: despite the fact that fairness isconsistent with the optimal policy, any learning algorithm satisfying fairnessmust take time exponential in the number of states to achieve non-trivialapproximation to the optimal policy. We then provide a provably fair polynomialtime algorithm under an approximate notion of fairness, thus establishing anexponential gap between exact and approximate fairness

Fairness in Criminal Justice Risk Assessments: The State of the Art

  Objectives: Discussions of fairness in criminal justice risk assessmentstypically lack conceptual precision. Rhetoric too often substitutes for carefulanalysis. In this paper, we seek to clarify the tradeoffs between differentkinds of fairness and between fairness and accuracy.  Methods: We draw on the existing literatures in criminology, computer scienceand statistics to provide an integrated examination of fairness and accuracy incriminal justice risk assessments. We also provide an empirical illustrationusing data from arraignments.  Results: We show that there are at least six kinds of fairness, some of whichare incompatible with one another and with accuracy.  Conclusions: Except in trivial cases, it is impossible to maximize accuracyand fairness at the same time, and impossible simultaneously to satisfy allkinds of fairness. In practice, a major complication is different base ratesacross different legally protected groups. There is a need to considerchallenging tradeoffs.

A Convex Framework for Fair Regression

  We introduce a flexible family of fairness regularizers for (linear andlogistic) regression problems. These regularizers all enjoy convexity,permitting fast optimization, and they span the rang from notions of groupfairness to strong individual fairness. By varying the weight on the fairnessregularizer, we can compute the efficient frontier of the accuracy-fairnesstrade-off on any given dataset, and we measure the severity of this trade-offvia a numerical quantity we call the Price of Fairness (PoF). The centerpieceof our results is an extensive comparative study of the PoF across sixdifferent datasets in which fairness is a primary consideration.

Online Learning with an Unknown Fairness Metric

  We consider the problem of online learning in the linear contextual banditssetting, but in which there are also strong individual fairness constraintsgoverned by an unknown similarity metric. These constraints demand that weselect similar actions or individuals with approximately equal probability(arXiv:1104.3913), which may be at odds with optimizing reward, thus modelingsettings where profit and social policy are in tension. We assume we learnabout an unknown Mahalanobis similarity metric from only weak feedback thatidentifies fairness violations, but does not quantify their extent. This isintended to represent the interventions of a regulator who "knows unfairnesswhen he sees it" but nevertheless cannot enunciate a quantitative fairnessmetric over individuals. Our main result is an algorithm in the adversarialcontext setting that has a number of fairness violations that depends onlylogarithmically on $T$, while obtaining an optimal $O(\sqrt{T})$ regret boundto the best fair policy.

PEGASUS: A Policy Search Method for Large MDPs and POMDPs

  We propose a new approach to the problem of searching a space of policies fora Markov decision process (MDP) or a partially observable Markov decisionprocess (POMDP), given a model. Our approach is based on the followingobservation: Any (PO)MDP can be transformed into an "equivalent" POMDP in whichall state transitions (given the current state and action) are deterministic.This reduces the general problem of policy search to one in which we need onlyconsider POMDPs with deterministic transitions. We give a natural way ofestimating the value of all policies in these transformed POMDPs. Policy searchis then simply performed by searching for a policy with high estimated value.We also establish conditions under which our value estimates will be good,recovering theoretical results similar to those of Kearns, Mansour and Ng(1999), but with "sample complexity" bounds that have only a polynomial ratherthan exponential dependence on the horizon time. Our method applies toarbitrary POMDPs, including ones with infinite state and action spaces. We alsopresent empirical results for our approach on a small discrete problem, and ona complex continuous state/continuous action problem involving learning to ridea bicycle.

On Sparse Discretization for Graphical Games

  This short paper concerns discretization schemes for representing andcomputing approximate Nash equilibria, with emphasis on graphical games, butbriefly touching on normal-form and poly-matrix games. The main technicalcontribution is a representation theorem that informally states that to accountfor every exact Nash equilibrium using a nearby approximate Nash equilibrium ona grid over mixed strategies, a uniform discretization size linear on theinverse of the approximation quality and natural game-representation parameterssuffices. For graphical games, under natural conditions, the discretization islogarithmic in the game-representation size, a substantial improvement over thelinear dependency previously required. The paper has five other objectives: (1)given the venue, to highlight the important, but often ignored, role that workon constraint networks in AI has in simplifying the derivation and analysis ofalgorithms for computing approximate Nash equilibria; (2) to summarize thestate-of-the-art on computing approximate Nash equilibria, with emphasis onrelevance to graphical games; (3) to help clarify the distinction betweensparse-discretization and sparse-support techniques; (4) to illustrate andadvocate for the deliberate mathematical simplicity of the formal proof of therepresentation theorem; and (5) to list and discuss important open problems,emphasizing graphical-game generalizations, which the AI community is mostsuitable to solve.

Fairness in Learning: Classic and Contextual Bandits

  We introduce the study of fairness in multi-armed bandit problems. Ourfairness definition can be interpreted as demanding that given a pool ofapplicants (say, for college admission or mortgages), a worse applicant isnever favored over a better one, despite a learning algorithm's uncertaintyover the true payoffs. We prove results of two types.  First, in the important special case of the classic stochastic banditsproblem (i.e., in which there are no contexts), we provide a provably fairalgorithm based on "chained" confidence intervals, and provide a cumulativeregret bound with a cubic dependence on the number of arms. We further showthat any fair algorithm must have such a dependence. When combined with regretbounds for standard non-fair algorithms such as UCB, this proves a strongseparation between fair and unfair learning, which extends to the generalcontextual case.  In the general contextual case, we prove a tight connection between fairnessand the KWIK (Knows What It Knows) learning model: a KWIK algorithm for a classof functions can be transformed into a provably fair contextual banditalgorithm, and conversely any fair contextual bandit algorithm can betransformed into a KWIK learning algorithm. This tight connection allows us toprovide a provably fair algorithm for the linear contextual bandit problem witha polynomial dependence on the dimension, and to show (for a different class offunctions) a worst-case exponential gap in regret between fair and non-fairlearning algorithms

Fairness Incentives for Myopic Agents

  We consider settings in which we wish to incentivize myopic agents (such asAirbnb landlords, who may emphasize short-term profits and property safety) totreat arriving clients fairly, in order to prevent overall discriminationagainst individuals or groups. We model such settings in both classical andcontextual bandit models in which the myopic agents maximize rewards accordingto current empirical averages, but are also amenable to exogenous payments thatmay cause them to alter their choices. Our notion of fairness asks that morequalified individuals are never (probabilistically) preferred over lessqualified ones [Joseph et al].  We investigate whether it is possible to design inexpensive {subsidy} orpayment schemes for a principal to motivate myopic agents to play fairly in allor almost all rounds. When the principal has full information about the stateof the myopic agents, we show it is possible to induce fair play on every roundwith a subsidy scheme of total cost $o(T)$ (for the classic setting with $k$arms, $\tilde{O}(\sqrt{k^3T})$, and for the $d$-dimensional linear contextualsetting $\tilde{O}(d\sqrt{k^3 T})$). If the principal has much more limitedinformation (as might often be the case for an external regulator or watchdog),and only observes the number of rounds in which members from each of the $k$groups were selected, but not the empirical estimates maintained by the myopicagent, the design of such a scheme becomes more complex. We show both positiveand negative results in the classic and linear bandit settings by upper andlower bounding the cost of fair subsidy schemes.

Differentially Private Fair Learning

  Motivated by settings in which predictive models may be required to benon-discriminatory with respect to certain attributes (such as race), but evencollecting the sensitive attribute may be forbidden or restricted, we initiatethe study of fair learning under the constraint of differential privacy. Wedesign two learning algorithms that simultaneously promise differential privacyand equalized odds, a 'fairness' condition that corresponds to equalizing falsepositive and negative rates across protected groups. Our first algorithm is aprivate implementation of the equalized odds post-processing approach of [Hardtet al., 2016]. This algorithm is appealingly simple, but must be able to useprotected group membership explicitly at test time, which can be viewed as aform of 'disparate treatment'. Our second algorithm is a differentially privateversion of the oracle-efficient in-processing approach of [Agarwal et al.,2018] that can be used to find the optimal fair classifier, given access to asubroutine that can solve the original (not necessarily fair) learning problem.This algorithm is more complex but need not have access to protected groupmembership at test time. We identify new tradeoffs between fairness, accuracy,and privacy that emerge only when requiring all three properties, and show thatthese tradeoffs can be milder if group membership may be used at test time. Weconclude with a brief experimental evaluation.

Report on the Depth Requirements for a Massive Detector at Homestake

  This report provides the technical justification for locating a largedetector underground in a US based Deep Underground Science and EngineeringLaboratory. A large detector with a fiducial mass in the mega-ton scale willmost likely be a multipurpose facility. The main physics justification for sucha device is detection of accelerator generated neutrinos, nucleon decay, andnatural sources of neutrinos such as solar, atmospheric and supernovaneutrinos. In addition to the physics justification there are practical issuesregarding the existing infrastructure at Homestake, and the stresscharacteristics of the Homestake rock formations.  The depth requirements associated with the various physics processes arereported for water Cherenkov and liquid argon detector technologies. While someof these physics processes can be adequately studied at shallower depths, noneof them require a depth greater than 4300 mwe which corresponds to the 4850 ftlevel at Homestake. It is very important to note that the scale of the planneddetector is such that even for accelerator neutrino detection (which allows oneto use the accelerator duty factor to eliminate cosmics) a minimum depth isneeded to reduce risk of contamination from cosmic rays. After consideration ofthe science and the practical issues regarding the Homestake site, we stronglyrecommend that the geotechnical studies be commenced at the 4850ft level in atimely manner.

Mechanism Design in Large Games: Incentives and Privacy

  We study the problem of implementing equilibria of complete information gamesin settings of incomplete information, and address this problem using"recommender mechanisms." A recommender mechanism is one that does not have thepower to enforce outcomes or to force participation, rather it only has thepower to suggestion outcomes on the basis of voluntary participation. We showthat despite these restrictions, recommender mechanisms can implementequilibria of complete information games in settings of incomplete informationunder the condition that the game is large---i.e. that there are a large numberof players, and any player's action affects any other's payoff by at most asmall amount.  Our result follows from a novel application of differential privacy. We showthat any algorithm that computes a correlated equilibrium of a completeinformation game while satisfying a variant of differential privacy---which wecall joint differential privacy---can be used as a recommender mechanism whilesatisfying our desired incentive properties. Our main technical result is analgorithm for computing a correlated equilibrium of a large game whilesatisfying joint differential privacy.  Although our recommender mechanisms are designed to satisfy game-theoreticproperties, our solution ends up satisfying a strong privacy property as well.No group of players can learn "much" about the type of any player outside thegroup from the recommendations of the mechanism, even if these players colludein an arbitrary way. As such, our algorithm is able to implement equilibria ofcomplete information games, without revealing information about the realizedtypes.

Privacy and Truthful Equilibrium Selection for Aggregative Games

  We study a very general class of games --- multi-dimensional aggregativegames --- which in particular generalize both anonymous games and weightedcongestion games. For any such game that is also large, we solve theequilibrium selection problem in a strong sense. In particular, we give anefficient weak mediator: a mechanism which has only the power to listen toreported types and provide non-binding suggested actions, such that (a) it isan asymptotic Nash equilibrium for every player to truthfully report their typeto the mediator, and then follow its suggested action; and (b) that whenplayers do so, they end up coordinating on a particular asymptotic purestrategy Nash equilibrium of the induced complete information game. In fact,truthful reporting is an ex-post Nash equilibrium of the mediated game, so oursolution applies even in settings of incomplete information, and even whenplayer types are arbitrary or worst-case (i.e. not drawn from a common prior).We achieve this by giving an efficient differentially private algorithm forcomputing a Nash equilibrium in such games. The rates of convergence toequilibrium in all of our results are inverse polynomial in the number ofplayers $n$. We also apply our main results to a multi-dimensional market game.  Our results can be viewed as giving, for a rich class of games, a more robustversion of the Revelation Principle, in that we work with weaker informationalassumptions (no common prior), yet provide a stronger solution concept (ex-postNash versus Bayes Nash equilibrium). In comparison to previous work, our mainconceptual contribution is showing that weak mediators are a game theoreticobject that exist in a wide variety of games -- previously, they were onlyknown to exist in traffic routing games.

Strategic Network Formation with Attack and Immunization

  Strategic network formation arises where agents receive benefit fromconnections to other agents, but also incur costs for forming links. Weconsider a new network formation game that incorporates an adversarial attack,as well as immunization against attack. An agent's benefit is the expected sizeof her connected component post-attack, and agents may also choose to immunizethemselves from attack at some additional cost. Our framework is a stylizedmodel of settings where reachability rather than centrality is the primaryconcern and vertices vulnerable to attacks may reduce risk via costly measures.  In the reachability benefit model without attack or immunization, the set ofequilibria is the empty graph and any tree. The introduction of attack andimmunization changes the game dramatically; new equilibrium topologies emerge,some more sparse and some more dense than trees. We show that, under a mildassumption on the adversary, every equilibrium network with $n$ agents containsat most $2n-4$ edges for $n\geq 4$. So despite permitting topologies denserthan trees, the amount of overbuilding is limited. We also show that attack andimmunization don't significantly erode social welfare: every non-trivialequilibrium with respect to several adversaries has welfare at least as that ofany equilibrium in the attack-free model.  We complement our theory with simulations demonstrating fast convergence of anew bounded rationality dynamic which generalizes linkstable best response butis considerably more powerful in our game. The simulations further elucidatethe wide variety of asymmetric equilibria and demonstrate topologicalconsequences of the dynamics e.g. heavy-tailed degree distributions. Finally,we report on a behavioral experiment on our game with over 100 participants,where despite the complexity of the game, the resulting network wassurprisingly close to equilibrium.

Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup  Fairness

  The most prevalent notions of fairness in machine learning are statisticaldefinitions: they fix a small collection of pre-defined groups, and then askfor parity of some statistic of the classifier across these groups. Constraintsof this form are susceptible to intentional or inadvertent "fairnessgerrymandering", in which a classifier appears to be fair on each individualgroup, but badly violates the fairness constraint on one or more structuredsubgroups defined over the protected attributes. We propose instead to demandstatistical notions of fairness across exponentially (or infinitely) manysubgroups, defined by a structured class of functions over the protectedattributes. This interpolates between statistical definitions of fairness andrecently proposed individual notions of fairness, but raises severalcomputational challenges. It is no longer clear how to audit a fixed classifierto see if it satisfies such a strong definition of fairness. We prove that thecomputational problem of auditing subgroup fairness for both equality of falsepositive rates and statistical parity is equivalent to the problem of weakagnostic learning, which means it is computationally hard in the worst case,even for simple structured subclasses.  We then derive two algorithms that provably converge to the best fairclassifier, given access to oracles which can solve the agnostic learningproblem. The algorithms are based on a formulation of subgroup fairness as atwo-player zero-sum game between a Learner and an Auditor. Our first algorithmprovably converges in a polynomial number of steps. Our second algorithm enjoysonly provably asymptotic convergence, but has the merit of simplicity andfaster per-step computation. We implement the simpler algorithm using linearregression as a heuristic oracle, and show that we can effectively both auditand learn fair classifiers on real datasets.

Fair Algorithms for Learning in Allocation Problems

  Settings such as lending and policing can be modeled by a centralized agentallocating a resource (loans or police officers) amongst several groups, inorder to maximize some objective (loans given that are repaid or criminals thatare apprehended). Often in such problems fairness is also a concern. A naturalnotion of fairness, based on general principles of equality of opportunity,asks that conditional on an individual being a candidate for the resource, theprobability of actually receiving it is approximately independent of theindividual's group. In lending this means that equally creditworthy individualsin different racial groups have roughly equal chances of receiving a loan. Inpolicing it means that two individuals committing the same crime in differentdistricts would have roughly equal chances of being arrested.  We formalize this fairness notion for allocation problems and investigate itsalgorithmic consequences. Our main technical results include an efficientlearning algorithm that converges to an optimal fair allocation even when thefrequency of candidates (creditworthy individuals or criminals) in each groupis unknown. The algorithm operates in a censored feedback model in which onlythe number of candidates who received the resource in a given allocation can beobserved, rather than the true number of candidates. This models the fact thatwe do not learn the creditworthiness of individuals we do not give loans to norlearn about crimes committed if the police presence in a district is low.  As an application of our framework, we consider the predictive policingproblem. The learning algorithm is trained on arrest data gathered from its owndeployments on previous days, resulting in a potential feedback loop that ouralgorithm provably overcomes. We empirically investigate the performance of ouralgorithm on the Philadelphia Crime Incidents dataset.

Solar Neutrino Measurements in Super-Kamiokande-IV

  Upgraded electronics, improved water system dynamics, better calibration andanalysis techniques allowed Super-Kamiokande-IV to clearly observe verylow-energy 8B solar neutrino interactions, with recoil electron kineticenergies as low as 3.49 MeV. Super-Kamiokande-IV data-taking began in Septemberof 2008; this paper includes data until February 2014, a total livetime of 1664days. The measured solar neutrino flux is (2.308+-0.020(stat.) +0.039-0.040(syst.)) x 106/(cm2sec) assuming no oscillations. The observedrecoil electron energy spectrum is consistent with no distortions due toneutrino oscillations. An extended maximum likelihood fit to the amplitude ofthe expected solar zenith angle variation of the neutrino-electron elasticscattering rate in SK-IV results in a day/night asymmetry of(-3.6+-1.6(stat.)+-0.6(syst.))%. The SK-IV solar neutrino data determine thesolar mixing angle as sin2 theta_12 = 0.327+0.026-0.031, all SK solar data(SK-I, SK-II, SK III and SKIV) measures this angle to be sin2 theta_12 =0.334+0.027-0.023, the determined mass-squared splitting is Delta m2_21 =4.8+1.5-0.8 x10-5 eV2.

The Long-Baseline Neutrino Experiment: Exploring Fundamental Symmetries  of the Universe

  The preponderance of matter over antimatter in the early Universe, thedynamics of the supernova bursts that produced the heavy elements necessary forlife and whether protons eventually decay --- these mysteries at the forefrontof particle physics and astrophysics are key to understanding the earlyevolution of our Universe, its current state and its eventual fate. TheLong-Baseline Neutrino Experiment (LBNE) represents an extensively developedplan for a world-class experiment dedicated to addressing these questions. LBNEis conceived around three central components: (1) a new, high-intensityneutrino source generated from a megawatt-class proton accelerator at FermiNational Accelerator Laboratory, (2) a near neutrino detector just downstreamof the source, and (3) a massive liquid argon time-projection chamber deployedas a far detector deep underground at the Sanford Underground ResearchFacility. This facility, located at the site of the former Homestake Mine inLead, South Dakota, is approximately 1,300 km from the neutrino source atFermilab -- a distance (baseline) that delivers optimal sensitivity to neutrinocharge-parity symmetry violation and mass ordering effects. This ambitious yetcost-effective design incorporates scalability and flexibility and canaccommodate a variety of upgrades and contributions. With its exceptionalcombination of experimental configuration, technical capabilities, andpotential for transformative discoveries, LBNE promises to be a vital facilityfor the field of particle physics worldwide, providing physicists from aroundthe globe with opportunities to collaborate in a twenty to thirty year programof exciting science. In this document we provide a comprehensive overview ofLBNE's scientific objectives, its place in the landscape of neutrino physicsworldwide, the technologies it will incorporate and the capabilities it willpossess.

