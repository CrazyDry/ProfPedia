Probabilistic Motion Estimation Based on Temporal Coherence

  We develop a theory for the temporal integration of visual motion motivated
by psychophysical experiments. The theory proposes that input data are
temporally grouped and used to predict and estimate the motion flows in the
image sequence. This temporal grouping can be considered a generalization of
the data association techniques used by engineers to study motion sequences.
Our temporal-grouping theory is expressed in terms of the Bayesian
generalization of standard Kalman filtering. To implement the theory we derive
a parallel network which shares some properties of cortical networks. Computer
simulations of this network demonstrate that our theory qualitatively accounts
for psychophysical experiments on motion occlusion and motion outliers.


Complexity of Representation and Inference in Compositional Models with
  Part Sharing

  This paper describes serial and parallel compositional models of multiple
objects with part sharing. Objects are built by part-subpart compositions and
expressed in terms of a hierarchical dictionary of object parts. These parts
are represented on lattices of decreasing sizes which yield an executive
summary description. We describe inference and learning algorithms for these
models. We analyze the complexity of this model in terms of computation time
(for serial computers) and numbers of nodes (e.g., "neurons") for parallel
computers. In particular, we compute the complexity gains by part sharing and
its dependence on how the dictionary scales with the level of the hierarchy. We
explore three regimes of scaling behavior where the dictionary size (i)
increases exponentially with the level, (ii) is determined by an unsupervised
compositional learning algorithm applied to real data, (iii) decreases
exponentially with scale. This analysis shows that in some regimes the use of
shared parts enables algorithms which can perform inference in time linear in
the number of levels for an exponential number of objects. In other regimes
part sharing has little advantage for serial computers but can give linear
processing on parallel computers.


Explain Images with Multimodal Recurrent Neural Networks

  In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model
for generating novel sentence descriptions to explain the content of images. It
directly models the probability distribution of generating a word given
previous words and the image. Image descriptions are generated by sampling from
this distribution. The model consists of two sub-networks: a deep recurrent
neural network for sentences and a deep convolutional network for images. These
two sub-networks interact with each other in a multimodal layer to form the
whole m-RNN model. The effectiveness of our model is validated on three
benchmark datasets (IAPR TC-12, Flickr 8K, and Flickr 30K). Our model
outperforms the state-of-the-art generative method. In addition, the m-RNN
model can be applied to retrieval tasks for retrieving images or sentences, and
achieves significant performance improvement over the state-of-the-art methods
which directly optimize the ranking objective function for retrieval.


Robust Estimation of 3D Human Poses from a Single Image

  Human pose estimation is a key step to action recognition. We propose a
method of estimating 3D human poses from a single image, which works in
conjunction with an existing 2D pose/joint detector. 3D pose estimation is
challenging because multiple 3D poses may correspond to the same 2D pose after
projection due to the lack of depth information. Moreover, current 2D pose
estimators are usually inaccurate which may cause errors in the 3D estimation.
We address the challenges in three ways: (i) We represent a 3D pose as a linear
combination of a sparse set of bases learned from 3D human skeletons. (ii) We
enforce limb length constraints to eliminate anthropomorphically implausible
skeletons. (iii) We estimate a 3D pose by minimizing the $L_1$-norm error
between the projection of the 3D pose and the corresponding 2D detection. The
$L_1$-norm loss term is robust to inaccurate 2D joint estimations. We use the
alternating direction method (ADM) to solve the optimization problem
efficiently. Our approach outperforms the state-of-the-arts on three benchmark
datasets.


The Secrets of Salient Object Segmentation

  In this paper we provide an extensive evaluation of fixation prediction and
salient object segmentation algorithms as well as statistics of major datasets.
Our analysis identifies serious design flaws of existing salient object
benchmarks, called the dataset design bias, by over emphasizing the
stereotypical concepts of saliency. The dataset design bias does not only
create the discomforting disconnection between fixations and salient object
segmentation, but also misleads the algorithm designing. Based on our analysis,
we propose a new high quality dataset that offers both fixation and salient
object segmentation ground-truth. With fixations and salient object being
presented simultaneously, we are able to bridge the gap between fixations and
salient objects, and propose a novel method for salient object segmentation.
Finally, we report significant benchmark progress on three existing datasets of
segmenting salient objects


Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs
  and a Discriminatively Trained Domain Transform

  Deep convolutional neural networks (CNNs) are the backbone of state-of-art
semantic image segmentation systems. Recent work has shown that complementing
CNNs with fully-connected conditional random fields (CRFs) can significantly
enhance their object localization accuracy, yet dense CRF inference is
computationally expensive. We propose replacing the fully-connected CRF with
domain transform (DT), a modern edge-preserving filtering method in which the
amount of smoothing is controlled by a reference edge map. Domain transform
filtering is several times faster than dense CRF inference and we show that it
yields comparable semantic segmentation results, accurately capturing object
boundaries. Importantly, our formulation allows learning the reference edge map
from intermediate CNN features instead of using the image gradient magnitude as
in standard DT filtering. This produces task-specific edges in an end-to-end
trainable system optimizing the target semantic segmentation quality.


Attention to Scale: Scale-aware Semantic Image Segmentation

  Incorporating multi-scale features in fully convolutional neural networks
(FCNs) has been a key element to achieving state-of-the-art performance on
semantic image segmentation. One common way to extract multi-scale features is
to feed multiple resized input images to a shared deep network and then merge
the resulting features for pixelwise classification. In this work, we propose
an attention mechanism that learns to softly weight the multi-scale features at
each pixel location. We adapt a state-of-the-art semantic image segmentation
model, which we jointly train with multi-scale input images and the attention
model. The proposed attention model not only outperforms average- and
max-pooling, but allows us to diagnostically visualize the importance of
features at different positions and scales. Moreover, we show that adding extra
supervision to the output at each scale is essential to achieving excellent
performance when merging multi-scale features. We demonstrate the effectiveness
of our model with extensive experiments on three challenging datasets,
including PASCAL-Person-Part, PASCAL VOC 2012 and a subset of MS-COCO 2014.


PASCAL Boundaries: A Class-Agnostic Semantic Boundary Dataset

  In this paper, we address the boundary detection task motivated by the
ambiguities in current definition of edge detection. To this end, we generate a
large database consisting of more than 10k images (which is 20x bigger than
existing edge detection databases) along with ground truth boundaries between
459 semantic classes including both foreground objects and different types of
background, and call it the PASCAL Boundaries dataset, which will be released
to the community. In addition, we propose a novel deep network-based
multi-scale semantic boundary detector and name it Multi-scale Deep Semantic
Boundary Detector (M-DSBD). We provide baselines using models that were trained
on edge detection and show that they transfer reasonably to the task of
boundary detection. Finally, we point to various important research problems
that this dataset can be used for.


NormFace: L2 Hypersphere Embedding for Face Verification

  Thanks to the recent developments of Convolutional Neural Networks, the
performance of face verification methods has increased rapidly. In a typical
face verification method, feature normalization is a critical step for boosting
performance. This motivates us to introduce and study the effect of
normalization during training. But we find this is non-trivial, despite
normalization being differentiable. We identify and study four issues related
to normalization through mathematical analysis, which yields understanding and
helps with parameter settings. Based on this analysis we propose two strategies
for training using normalized features. The first is a modification of softmax
loss, which optimizes cosine similarity instead of inner-product. The second is
a reformulation of metric learning by introducing an agent vector for each
class. We show that both strategies, and small variants, consistently improve
performance by between 0.2% to 0.4% on the LFW dataset based on two models.
This is significant because the performance of the two models on LFW dataset is
close to saturation at over 98%. Codes and models are released on
https://github.com/happynear/NormFace


Learning Deep Structured Models

  Many problems in real-world applications involve predicting several random
variables which are statistically related. Markov random fields (MRFs) are a
great mathematical tool to encode such relationships. The goal of this paper is
to combine MRFs with deep learning algorithms to estimate complex
representations while taking into account the dependencies between the output
random variables. Towards this goal, we propose a training algorithm that is
able to learn structured models jointly with deep features that form the MRF
potentials. Our approach is efficient as it blends learning and inference and
makes use of GPU acceleration. We demonstrate the effectiveness of our
algorithm in the tasks of predicting words from noisy images, as well as
multi-class classification of Flickr photographs. We show that joint learning
of the deep features and the MRF parameters results in significant performance
gains.


Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image
  Segmentation

  Deep convolutional neural networks (DCNNs) trained on a large number of
images with strong pixel-level annotations have recently significantly pushed
the state-of-art in semantic image segmentation. We study the more challenging
problem of learning DCNNs for semantic image segmentation from either (1)
weakly annotated training data such as bounding boxes or image-level labels or
(2) a combination of few strongly labeled and many weakly labeled images,
sourced from one or multiple datasets. We develop Expectation-Maximization (EM)
methods for semantic image segmentation model training under these weakly
supervised and semi-supervised settings. Extensive experimental evaluation
shows that the proposed techniques can learn models delivering competitive
results on the challenging PASCAL VOC 2012 image segmentation benchmark, while
requiring significantly less annotation effort. We share source code
implementing the proposed system at
https://bitbucket.org/deeplab/deeplab-public.


Exploiting Symmetry and/or Manhattan Properties for 3D Object Structure
  Estimation from Single and Multiple Images

  Many man-made objects have intrinsic symmetries and Manhattan structure. By
assuming an orthographic projection model, this paper addresses the estimation
of 3D structures and camera projection using symmetry and/or Manhattan
structure cues, which occur when the input is single- or multiple-image from
the same category, e.g., multiple different cars. Specifically, analysis on the
single image case implies that Manhattan alone is sufficient to recover the
camera projection, and then the 3D structure can be reconstructed uniquely
exploiting symmetry. However, Manhattan structure can be difficult to observe
from a single image due to occlusion. To this end, we extend to the
multiple-image case which can also exploit symmetry but does not require
Manhattan axes. We propose a novel rigid structure from motion method,
exploiting symmetry and using multiple images from the same category as input.
Experimental results on the Pascal3D+ dataset show that our method
significantly outperforms baseline methods.


Object Recognition with and without Objects

  While recent deep neural networks have achieved a promising performance on
object recognition, they rely implicitly on the visual contents of the whole
image. In this paper, we train deep neural net- works on the foreground
(object) and background (context) regions of images respectively. Consider- ing
human recognition in the same situations, net- works trained on the pure
background without ob- jects achieves highly reasonable recognition performance
that beats humans by a large margin if only given context. However, humans
still outperform networks with pure object available, which indicates networks
and human beings have different mechanisms in understanding an image.
Furthermore, we straightforwardly combine multiple trained networks to explore
different visual cues learned by different networks. Experiments show that
useful visual hints can be explicitly learned separately and then combined to
achieve higher performance, which verifies the advantages of the proposed
framework.


Generating Multiple Diverse Hypotheses for Human 3D Pose Consistent with
  2D Joint Detections

  We propose a method to generate multiple diverse and valid human pose
hypotheses in 3D all consistent with the 2D detection of joints in a monocular
RGB image. We use a novel generative model uniform (unbiased) in the space of
anatomically plausible 3D poses. Our model is compositional (produces a pose by
combining parts) and since it is restricted only by anatomical constraints it
can generalize to every plausible human 3D pose. Removing the model bias
intrinsically helps to generate more diverse 3D pose hypotheses. We argue that
generating multiple pose hypotheses is more reasonable than generating only a
single 3D pose based on the 2D joint detection given the depth ambiguity and
the uncertainty due to occlusion and imperfect 2D joint detection. We hope that
the idea of generating multiple consistent pose hypotheses can give rise to a
new line of future work that has not received much attention in the literature.
We used the Human3.6M dataset for empirical evaluation.


Deep Supervision for Pancreatic Cyst Segmentation in Abdominal CT Scans

  Automatic segmentation of an organ and its cystic region is a prerequisite of
computer-aided diagnosis. In this paper, we focus on pancreatic cyst
segmentation in abdominal CT scan. This task is important and very useful in
clinical practice yet challenging due to the low contrast in boundary, the
variability in location, shape and the different stages of the pancreatic
cancer. Inspired by the high relevance between the location of a pancreas and
its cystic region, we introduce extra deep supervision into the segmentation
network, so that cyst segmentation can be improved with the help of relatively
easier pancreas segmentation. Under a reasonable transformation function, our
approach can be factorized into two stages, and each stage can be efficiently
optimized via gradient back-propagation throughout the deep networks. We
collect a new dataset with 131 pathological samples, which, to the best of our
knowledge, is the largest set for pancreatic cyst segmentation. Without human
assistance, our approach reports a 63.44% average accuracy, measured by the
Dice-S{\o}rensen coefficient (DSC), which is higher than the number (60.46%)
without deep supervision.


A 3D Coarse-to-Fine Framework for Volumetric Medical Image Segmentation

  In this paper, we adopt 3D Convolutional Neural Networks to segment
volumetric medical images. Although deep neural networks have been proven to be
very effective on many 2D vision tasks, it is still challenging to apply them
to 3D tasks due to the limited amount of annotated 3D data and limited
computational resources. We propose a novel 3D-based coarse-to-fine framework
to effectively and efficiently tackle these challenges. The proposed 3D-based
framework outperforms the 2D counterpart to a large margin since it can
leverage the rich spatial infor- mation along all three axes. We conduct
experiments on two datasets which include healthy and pathological pancreases
respectively, and achieve the current state-of-the-art in terms of
Dice-S{\o}rensen Coefficient (DSC). On the NIH pancreas segmentation dataset,
we outperform the previous best by an average of over 2%, and the worst case is
improved by 7% to reach almost 70%, which indicates the reliability of our
framework in clinical applications.


Single-Shot Object Detection with Enriched Semantics

  We propose a novel single shot object detection network named Detection with
Enriched Semantics (DES). Our motivation is to enrich the semantics of object
detection features within a typical deep detector, by a semantic segmentation
branch and a global activation module. The segmentation branch is supervised by
weak segmentation ground-truth, i.e., no extra annotation is required. In
conjunction with that, we employ a global activation module which learns
relationship between channels and object classes in a self-supervised manner.
Comprehensive experimental results on both PASCAL VOC and MS COCO detection
datasets demonstrate the effectiveness of the proposed method. In particular,
with a VGG16 based DES, we achieve an mAP of 81.7 on VOC2007 test and an mAP of
32.8 on COCO test-dev with an inference speed of 31.5 milliseconds per image on
a Titan Xp GPU. With a lower resolution version, we achieve an mAP of 79.7 on
VOC2007 with an inference speed of 13.0 milliseconds per image.


Bridging the Gap Between 2D and 3D Organ Segmentation with Volumetric
  Fusion Net

  There has been a debate on whether to use 2D or 3D deep neural networks for
volumetric organ segmentation. Both 2D and 3D models have their advantages and
disadvantages. In this paper, we present an alternative framework, which trains
2D networks on different viewpoints for segmentation, and builds a 3D
Volumetric Fusion Net (VFN) to fuse the 2D segmentation results. VFN is
relatively shallow and contains much fewer parameters than most 3D networks,
making our framework more efficient at integrating 3D information for
segmentation. We train and test the segmentation and fusion modules
individually, and propose a novel strategy, named cross-cross-augmentation, to
make full use of the limited training data. We evaluate our framework on
several challenging abdominal organs, and verify its superiority in
segmentation accuracy and stability over existing 2D and 3D approaches.


Multi-Scale Spatially-Asymmetric Recalibration for Image Classification

  Convolution is spatially-symmetric, i.e., the visual features are independent
of its position in the image, which limits its ability to utilize contextual
cues for visual recognition. This paper addresses this issue by introducing a
recalibration process, which refers to the surrounding region of each neuron,
computes an importance value and multiplies it to the original neural response.
Our approach is named multi-scale spatially-asymmetric recalibration (MS-SAR),
which extracts visual cues from surrounding regions at multiple scales, and
designs a weighting scheme which is asymmetric in the spatial domain. MS-SAR is
implemented in an efficient way, so that only small fractions of extra
parameters and computations are required. We apply MS-SAR to several popular
building blocks, including the residual block and the densely-connected block,
and demonstrate its superior performance in both CIFAR and ILSVRC2012
classification tasks.


Training Multi-organ Segmentation Networks with Sample Selection by
  Relaxed Upper Confident Bound

  Deep convolutional neural networks (CNNs), especially fully convolutional
networks, have been widely applied to automatic medical image segmentation
problems, e.g., multi-organ segmentation. Existing CNN-based segmentation
methods mainly focus on looking for increasingly powerful network
architectures, but pay less attention to data sampling strategies for training
networks more effectively. In this paper, we present a simple but effective
sample selection method for training multi-organ segmentation networks. Sample
selection exhibits an exploitation-exploration strategy, i.e., exploiting hard
samples and exploring less frequently visited samples. Based on the fact that
very hard samples might have annotation errors, we propose a new sample
selection policy, named Relaxed Upper Confident Bound (RUCB). Compared with
other sample selection policies, e.g., Upper Confident Bound (UCB), it exploits
a range of hard samples rather than being stuck with a small set of very hard
ones, which mitigates the influence of annotation errors during training. We
apply this new sample selection policy to training a multi-organ segmentation
network on a dataset containing 120 abdominal CT scans and show that it boosts
segmentation performance significantly.


Joint Shape Representation and Classification for Detecting PDAC

  We aim to detect pancreatic ductal adenocarcinoma (PDAC) in abdominal CT
scans, which sheds light on early diagnosis of pancreatic cancer. This is a 3D
volume classification task with little training data. We propose a two-stage
framework, which first segments the pancreas into a binary mask, then
compresses the mask into a shape vector and performs abnormality
classification. Shape representation and classification are performed in a {\em
joint} manner, both to exploit the knowledge that PDAC often changes the {\bf
shape} of the pancreas and to prevent over-fitting. Experiments are performed
on $300$ normal scans and $156$ PDAC cases. We achieve a specificity of
$90.2\%$ (false alarm occurs on less than $1/10$ normal cases) at a sensitivity
of $80.2\%$ (less than $1/5$ PDAC cases are not detected), which show promise
for clinical applications.


Deep Nets: What have they ever done for Vision?

  This is an opinion paper about the strengths and weaknesses of Deep Nets for
vision. They are at the center of recent progress on artificial intelligence
and are of growing importance in cognitive science and neuroscience. They have
enormous successes but also clear limitations. There is also only partial
understanding of their inner workings. It seems unlikely that Deep Nets in
their current form will be the best long-term solution either for building
general purpose intelligent machines or for understanding the mind/brain, but
it is likely that many aspects of them will remain. At present Deep Nets do
very well on specific types of visual tasks and on specific benchmarked
datasets. But Deep Nets are much less general purpose, flexible, and adaptive
than the human visual system. Moreover, methods like Deep Nets may run into
fundamental difficulties when faced with the enormous complexity of natural
images which can lead to a combinatorial explosion. To illustrate our main
points, while keeping the references small, this paper is slightly biased
towards work from our group.


Multi-Scale Coarse-to-Fine Segmentation for Screening Pancreatic Ductal
  Adenocarcinoma

  This paper proposes an intuitive approach to finding pancreatic ductal
adenocarcinoma (PDAC), the most common type of pancreatic cancer, by checking
abdominal CT scans. Our idea is named segmentation-for-classification (S4C),
which classifies a volume by checking if at least a sufficient number of voxels
is segmented as the tumor. In order to deal with tumors with different scales,
we train volumetric segmentation networks with multi-scale inputs, and test
them in a coarse-to-fine flowchart. A post-processing module is used to filter
out outliers and reduce false alarms. We perform a case study on our dataset
containing 439 CT scans, in which 136 cases were diagnosed with PDAC and 303
cases are normal. Our approach reports a sensitivity of 94.1% at a specificity
of 98.5%, with an average tumor segmentation accuracy of 56.46% over all PDAC
cases.


PreCo: A Large-scale Dataset in Preschool Vocabulary for Coreference
  Resolution

  We introduce PreCo, a large-scale English dataset for coreference resolution.
The dataset is designed to embody the core challenges in coreference, such as
entity representation, by alleviating the challenge of low overlap between
training and test sets and enabling separated analysis of mention detection and
mention clustering. To strengthen the training-test overlap, we collect a large
corpus of about 38K documents and 12.4M words which are mostly from the
vocabulary of English-speaking preschoolers. Experiments show that with higher
training-test overlap, error analysis on PreCo is more efficient than the one
on OntoNotes, a popular existing dataset. Furthermore, we annotate singleton
mentions making it possible for the first time to quantify the influence that a
mention detector makes on coreference resolution performance. The dataset is
freely available at https://preschool-lab.github.io/PreCo/.


Phase Collaborative Network for Multi-Phase Medical Imaging Segmentation

  Integrating multi-phase information is an effective way of boosting visual
recognition. In this paper, we investigate this problem from the perspective of
medical imaging analysis, in which two phases in CT scans known as arterial and
venous are combined towards higher segmentation accuracy. To this end, we
propose Phase Collaborative Network (PCN), an end-to-end network which contains
both generative and discriminative modules to formulate phase-to-phase
relations and data-to-label relations, respectively. Experiments are performed
on several CT image segmentation datasets. PCN achieves superior performance
with either two phases or only one phase available. Moreover, we empirically
verify that the accuracy gain comes from the collaboration between phases.


Snapshot Distillation: Teacher-Student Optimization in One Generation

  Optimizing a deep neural network is a fundamental task in computer vision,
yet direct training methods often suffer from over-fitting. Teacher-student
optimization aims at providing complementary cues from a model trained
previously, but these approaches are often considerably slow due to the
pipeline of training a few generations in sequence, i.e., time complexity is
increased by several times.
  This paper presents snapshot distillation (SD), the first framework which
enables teacher-student optimization in one generation. The idea of SD is very
simple: instead of borrowing supervision signals from previous generations, we
extract such information from earlier epochs in the same generation, meanwhile
make sure that the difference between teacher and student is sufficiently large
so as to prevent under-fitting. To achieve this goal, we implement SD in a
cyclic learning rate policy, in which the last snapshot of each cycle is used
as the teacher for all iterations in the next cycle, and the teacher signal is
smoothed to provide richer information. In standard image classification
benchmarks such as CIFAR100 and ILSVRC2012, SD achieves consistent accuracy
gain without heavy computational overheads. We also verify that models
pre-trained with SD transfers well to object detection and semantic
segmentation in the PascalVOC dataset.


Thickened 2D Networks for 3D Medical Image Segmentation

  There has been a debate in medical image segmentation on whether to use 2D or
3D networks, where both pipelines have advantages and disadvantages. This paper
presents a novel approach which thickens the input of a 2D network, so that the
model is expected to enjoy both the stability and efficiency of 2D networks as
well as the ability of 3D networks in modeling volumetric contexts. A major
information loss happens when a large number of 2D slices are fused at the
first convolutional layer, resulting in a relatively weak ability of the
network in distinguishing the difference among slices. To alleviate this
drawback, we propose an effective framework which (i) postpones slice fusion
and (ii) adds highway connections from the pre-fusion layer so that the
prediction layer receives slice-sensitive auxiliary cues. Experiments on
segmenting a few abdominal targets in particular blood vessels which require
strong 3D contexts demonstrate the effectiveness of our approach.


Representing Data by a Mixture of Activated Simplices

  We present a new model which represents data as a mixture of simplices.
Simplices are geometric structures that generalize triangles. We give a simple
geometric understanding that allows us to learn a simplicial structure
efficiently. Our method requires that the data are unit normalized (and thus
lie on the unit sphere). We show that under this restriction, building a model
with simplices amounts to constructing a convex hull inside the sphere whose
boundary facets is close to the data. We call the boundary facets of the convex
hull that are close to the data Activated Simplices. While the total number of
bases used to build the simplices is a parameter of the model, the dimensions
of the individual activated simplices are learned from the data. Simplices can
have different dimensions, which facilitates modeling of inhomogeneous data
sources. The simplicial structure is bounded --- this is appropriate for
modeling data with constraints, such as human elbows can not bend more than 180
degrees. The simplices are easy to interpret and extremes within the data can
be discovered among the vertices. The method provides good reconstruction and
regularization. It supports good nearest neighbor classification and it allows
realistic generative models to be constructed. It achieves state-of-the-art
results on benchmark datasets, including 3D poses and digits.


Semantic Image Segmentation with Deep Convolutional Nets and Fully
  Connected CRFs

  Deep Convolutional Neural Networks (DCNNs) have recently shown state of the
art performance in high level vision tasks, such as image classification and
object detection. This work brings together methods from DCNNs and
probabilistic graphical models for addressing the task of pixel-level
classification (also called "semantic image segmentation"). We show that
responses at the final layer of DCNNs are not sufficiently localized for
accurate object segmentation. This is due to the very invariance properties
that make DCNNs good for high level tasks. We overcome this poor localization
property of deep networks by combining the responses at the final DCNN layer
with a fully connected Conditional Random Field (CRF). Qualitatively, our
"DeepLab" system is able to localize segment boundaries at a level of accuracy
which is beyond previous methods. Quantitatively, our method sets the new
state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching
71.6% IOU accuracy in the test set. We show how these results can be obtained
efficiently: Careful network re-purposing and a novel application of the 'hole'
algorithm from the wavelet community allow dense computation of neural net
responses at 8 frames per second on a modern GPU.


DeepLab: Semantic Image Segmentation with Deep Convolutional Nets,
  Atrous Convolution, and Fully Connected CRFs

  In this work we address the task of semantic image segmentation with Deep
Learning and make three main contributions that are experimentally shown to
have substantial practical merit. First, we highlight convolution with
upsampled filters, or 'atrous convolution', as a powerful tool in dense
prediction tasks. Atrous convolution allows us to explicitly control the
resolution at which feature responses are computed within Deep Convolutional
Neural Networks. It also allows us to effectively enlarge the field of view of
filters to incorporate larger context without increasing the number of
parameters or the amount of computation. Second, we propose atrous spatial
pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP
probes an incoming convolutional feature layer with filters at multiple
sampling rates and effective fields-of-views, thus capturing objects as well as
image context at multiple scales. Third, we improve the localization of object
boundaries by combining methods from DCNNs and probabilistic graphical models.
The commonly deployed combination of max-pooling and downsampling in DCNNs
achieves invariance but has a toll on localization accuracy. We overcome this
by combining the responses at the final DCNN layer with a fully connected
Conditional Random Field (CRF), which is shown both qualitatively and
quantitatively to improve localization performance. Our proposed "DeepLab"
system sets the new state-of-art at the PASCAL VOC-2012 semantic image
segmentation task, reaching 79.7% mIOU in the test set, and advances the
results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and
Cityscapes. All of our code is made publicly available online.


DeePM: A Deep Part-Based Model for Object Detection and Semantic Part
  Localization

  In this paper, we propose a deep part-based model (DeePM) for symbiotic
object detection and semantic part localization. For this purpose, we annotate
semantic parts for all 20 object categories on the PASCAL VOC 2012 dataset,
which provides information on object pose, occlusion, viewpoint and
functionality. DeePM is a latent graphical model based on the state-of-the-art
R-CNN framework, which learns an explicit representation of the object-part
configuration with flexible type sharing (e.g., a sideview horse head can be
shared by a fully-visible sideview horse and a highly truncated sideview horse
with head and neck only). For comparison, we also present an end-to-end
Object-Part (OP) R-CNN which learns an implicit feature representation for
jointly mapping an image ROI to the object and part bounding boxes. We evaluate
the proposed methods for both the object and part detection performance on
PASCAL VOC 2012, and show that DeePM consistently outperforms OP R-CNN in
detecting objects and parts. In addition, it obtains superior performance to
Fast and Faster R-CNNs in object detection.


Semi-Supervised Sparse Representation Based Classification for Face
  Recognition with Insufficient Labeled Samples

  This paper addresses the problem of face recognition when there is only few,
or even only a single, labeled examples of the face that we wish to recognize.
Moreover, these examples are typically corrupted by nuisance variables, both
linear (i.e., additive nuisance variables such as bad lighting, wearing of
glasses) and non-linear (i.e., non-additive pixel-wise nuisance variables such
as expression changes). The small number of labeled examples means that it is
hard to remove these nuisance variables between the training and testing faces
to obtain good recognition performance. To address the problem we propose a
method called Semi-Supervised Sparse Representation based Classification
(S$^3$RC). This is based on recent work on sparsity where faces are represented
in terms of two dictionaries: a gallery dictionary consisting of one or more
examples of each person, and a variation dictionary representing linear
nuisance variables (e.g., different lighting conditions, different glasses).
The main idea is that (i) we use the variation dictionary to characterize the
linear nuisance variables via the sparsity framework, then (ii) prototype face
images are estimated as a gallery dictionary via a Gaussian Mixture Model
(GMM), with mixed labeled and unlabeled samples in a semi-supervised manner, to
deal with the non-linear nuisance variations between labeled and unlabeled
samples. We have done experiments with insufficient labeled samples, even when
there is only a single labeled sample per person. Our results on the AR,
Multi-PIE, CAS-PEAL, and LFW databases demonstrate that the proposed method is
able to deliver significantly improved performance over existing methods.


A Fixed-Point Model for Pancreas Segmentation in Abdominal CT Scans

  Deep neural networks have been widely adopted for automatic organ
segmentation from abdominal CT scans. However, the segmentation accuracy of
some small organs (e.g., the pancreas) is sometimes below satisfaction,
arguably because deep networks are easily disrupted by the complex and variable
background regions which occupies a large fraction of the input volume. In this
paper, we formulate this problem into a fixed-point model which uses a
predicted segmentation mask to shrink the input region. This is motivated by
the fact that a smaller input region often leads to more accurate segmentation.
In the training process, we use the ground-truth annotation to generate
accurate input regions and optimize network weights. On the testing stage, we
fix the network parameters and update the segmentation results in an iterative
manner. We evaluate our approach on the NIH pancreas segmentation dataset, and
outperform the state-of-the-art by more than 4%, measured by the average
Dice-S{\o}rensen Coefficient (DSC). In addition, we report 62.43% DSC in the
worst case, which guarantees the reliability of our approach in clinical
applications.


Regularizing Face Verification Nets For Pain Intensity Regression

  Limited labeled data are available for the research of estimating facial
expression intensities. For instance, the ability to train deep networks for
automated pain assessment is limited by small datasets with labels of
patient-reported pain intensities. Fortunately, fine-tuning from a
data-extensive pre-trained domain, such as face verification, can alleviate
this problem. In this paper, we propose a network that fine-tunes a
state-of-the-art face verification network using a regularized regression loss
and additional data with expression labels. In this way, the expression
intensity regression task can benefit from the rich feature representations
trained on a huge amount of data for face verification. The proposed
regularized deep regressor is applied to estimate the pain expression intensity
and verified on the widely-used UNBC-McMaster Shoulder-Pain dataset, achieving
the state-of-the-art performance. A weighted evaluation metric is also proposed
to address the imbalance issue of different pain intensities.


Multi-Context Attention for Human Pose Estimation

  In this paper, we propose to incorporate convolutional neural networks with a
multi-context attention mechanism into an end-to-end framework for human pose
estimation. We adopt stacked hourglass networks to generate attention maps from
features at multiple resolutions with various semantics. The Conditional Random
Field (CRF) is utilized to model the correlations among neighboring regions in
the attention map. We further combine the holistic attention model, which
focuses on the global consistency of the full human body, and the body part
attention model, which focuses on the detailed description for different body
parts. Hence our model has the ability to focus on different granularity from
local salient regions to global semantic-consistent spaces. Additionally, we
design novel Hourglass Residual Units (HRUs) to increase the receptive field of
the network. These units are extensions of residual units with a side branch
incorporating filters with larger receptive fields, hence features with various
scales are learned and combined within the HRUs. The effectiveness of the
proposed multi-context attention mechanism and the hourglass residual units is
evaluated on two widely used human pose estimation benchmarks. Our approach
outperforms all existing methods on both benchmarks over all the body parts.


NDDR-CNN: Layerwise Feature Fusing in Multi-Task CNNs by Neural
  Discriminative Dimensionality Reduction

  In this paper, we propose a novel Convolutional Neural Network (CNN)
structure for general-purpose multi-task learning (MTL), which enables
automatic feature fusing at every layer from different tasks. This is in
contrast with the most widely used MTL CNN structures which empirically or
heuristically share features on some specific layers (e.g., share all the
features except the last convolutional layer). The proposed layerwise feature
fusing scheme is formulated by combining existing CNN components in a novel
way, with clear mathematical interpretability as discriminative dimensionality
reduction, which is referred to as Neural Discriminative Dimensionality
Reduction (NDDR). Specifically, we first concatenate features with the same
spatial resolution from different tasks according to their channel dimension.
Then, we show that the discriminative dimensionality reduction can be fulfilled
by 1x1 Convolution, Batch Normalization, and Weight Decay in one CNN. The use
of existing CNN components ensures the end-to-end training and the
extensibility of the proposed NDDR layer to various state-of-the-art CNN
architectures in a "plug-and-play" manner. The detailed ablation analysis shows
that the proposed NDDR layer is easy to train and also robust to different
hyperparameters. Experiments on different task sets with various base network
architectures demonstrate the promising performance and desirable
generalizability of our proposed method. The code of our paper is available at
https://github.com/ethanygao/NDDR-CNN.


Semi-Supervised Multi-Organ Segmentation via Deep Multi-Planar
  Co-Training

  In multi-organ segmentation of abdominal CT scans, most existing fully
supervised deep learning algorithms require lots of voxel-wise annotations,
which are usually difficult, expensive, and slow to obtain. In comparison,
massive unlabeled 3D CT volumes are usually easily accessible. Current
mainstream works to address the semi-supervised biomedical image segmentation
problem are mostly graph-based. By contrast, deep network based semi-supervised
learning methods have not drawn much attention in this field. In this work, we
propose Deep Multi-Planar Co-Training (DMPCT), whose contributions can be
divided into two folds: 1) The deep model is learned in a co-training style
which can mine consensus information from multiple planes like the sagittal,
coronal, and axial planes; 2) Multi-planar fusion is applied to generate more
reliable pseudo-labels, which alleviates the errors occurring in the
pseudo-labels and thus can help to train better segmentation networks.
Experiments are done on our newly collected large dataset with 100 unlabeled
cases as well as 210 labeled cases where 16 anatomical structures are manually
annotated by four radiologists and confirmed by a senior expert. The results
suggest that DMPCT significantly outperforms the fully supervised method by
more than 4% especially when only a small set of annotations is used.


Progressive Recurrent Learning for Visual Recognition

  Computer vision is difficult, partly because the mathematical function
connecting input and output data is often complex, fuzzy and thus hard to
learn. A currently popular solution is to design a deep neural network and
optimize it on a large-scale dataset. However, as the number of parameters
increases, the generalization ability is often not guaranteed, e.g., the model
can over-fit due to the limited amount of training data, or fail to converge
because the desired function is too difficult to learn. This paper presents an
effective framework named progressive recurrent learning (PRL). The core idea
is similar to curriculum learning which gradually increases the difficulty of
training data. We generalize it to a wide range of vision problems that were
previously considered less proper to apply curriculum learning. PRL starts with
inserting a recurrent prediction scheme, based on the motivation of feeding the
prediction of a vision model to the same model iteratively, so that the
auxiliary cues contained in it can be exploited to improve the quality of
itself. In order to better optimize this framework, we start with providing
perfect prediction, i.e., ground-truth, to the second stage, but gradually
replace it with the prediction of the first stage. In the final status, the
ground-truth information is not needed any more, so that the entire model works
on the real data distribution as in the testing process. We apply PRL to two
challenging visual recognition tasks, namely, object localization and semantic
segmentation, and demonstrate consistent accuracy gain compared to the baseline
training strategy, especially in the scenarios of more difficult vision tasks.


Elastic Boundary Projection for 3D Medical Imaging Segmentation

  We focus on an important yet challenging problem: using a 2D deep network to
deal with 3D segmentation for medical imaging analysis. Existing approaches
either applied multi-view planar (2D) networks or directly used volumetric (3D)
networks for this purpose, but both of them are not ideal: 2D networks cannot
capture 3D contexts effectively, and 3D networks are both memory-consuming and
less stable arguably due to the lack of pre-trained models.
  In this paper, we bridge the gap between 2D and 3D using a novel approach
named Elastic Boundary Projection (EBP). The key observation is that, although
the object is a 3D volume, what we really need in segmentation is to find its
boundary which is a 2D surface. Therefore, we place a number of pivot points in
the 3D space, and for each pivot, we determine its distance to the object
boundary along a dense set of directions. This creates an elastic shell around
each pivot which is initialized as a perfect sphere. We train a 2D deep network
to determine whether each ending point falls within the object, and gradually
adjust the shell so that it gradually converges to the actual shape of the
boundary and thus achieves the goal of segmentation. EBP allows 3D segmentation
without cutting the volume into slices or small patches, which stands out from
conventional 2D and 3D approaches. EBP achieves promising accuracy in
segmenting several abdominal organs from CT scans.


Towards Accurate Task Accomplishment with Low-Cost Robotic Arms

  Training a robotic arm to accomplish real-world tasks has been attracting
increasing attention in both academia and industry. This work discusses the
role of computer vision algorithms in this field. We focus on low-cost arms on
which no sensors are equipped and thus all decisions are made upon visual
recognition, e.g., real-time 3D pose estimation. This requires annotating a lot
of training data, which is not only time-consuming but also laborious.
  In this paper, we present an alternative solution, which uses a 3D model to
create a large number of synthetic data, trains a vision model in this virtual
domain, and applies it to real-world images after domain adaptation. To this
end, we design a semi-supervised approach, which fully leverages the geometric
constraints among keypoints. We apply an iterative algorithm for optimization.
Without any annotations on real images, our algorithm generalizes well and
produces satisfying results on 3D pose estimation, which is evaluated on two
real-world datasets. We also construct a vision-based control system for task
accomplishment, for which we train a reinforcement learning agent in a virtual
environment and apply it to the real-world. Moreover, our approach, with merely
a 3D model being required, has the potential to generalize to other types of
multi-rigid-body dynamic systems.


Regional Homogeneity: Towards Learning Transferable Universal
  Adversarial Perturbations Against Defenses

  This paper focuses on learning transferable adversarial examples specifically
against defense models (models to defense adversarial attacks). In particular,
we show that a simple universal perturbation can fool a series of
state-of-the-art defenses.
  Adversarial examples generated by existing attacks are generally hard to
transfer to defense models. We observe the property of regional homogeneity in
adversarial perturbations and suggest that the defenses are less robust to
regionally homogeneous perturbations. Therefore, we propose an effective
transforming paradigm and a customized gradient transformer module to transform
existing perturbations into regionally homogeneous ones. Without explicitly
forcing the perturbations to be universal, we observe that a well-trained
gradient transformer module tends to output input-independent gradients (hence
universal) benefiting from the under-fitting phenomenon. Thorough experiments
demonstrate that our work significantly outperforms the prior art attacking
algorithms (either image-dependent or universal ones) by an average improvement
of 14.0% when attacking 9 defenses in the black-box setting. In addition to the
cross-model transferability, we also verify that regionally homogeneous
perturbations can well transfer across different vision tasks (attacking with
the semantic segmentation task and testing on the object detection task).


Zoom Better to See Clearer: Human and Object Parsing with Hierarchical
  Auto-Zoom Net

  Parsing articulated objects, e.g. humans and animals, into semantic parts
(e.g. body, head and arms, etc.) from natural images is a challenging and
fundamental problem for computer vision. A big difficulty is the large
variability of scale and location for objects and their corresponding parts.
Even limited mistakes in estimating scale and location will degrade the parsing
output and cause errors in boundary details. To tackle these difficulties, we
propose a "Hierarchical Auto-Zoom Net" (HAZN) for object part parsing which
adapts to the local scales of objects and parts. HAZN is a sequence of two
"Auto-Zoom Net" (AZNs), each employing fully convolutional networks that
perform two tasks: (1) predict the locations and scales of object instances
(the first AZN) or their parts (the second AZN); (2) estimate the part scores
for predicted object instance or part regions. Our model can adaptively "zoom"
(resize) predicted image regions into their proper scales to refine the
parsing.
  We conduct extensive experiments over the PASCAL part datasets on humans,
horses, and cows. For humans, our approach significantly outperforms the
state-of-the-arts by 5% mIOU and is especially better at segmenting small
instances and small parts. We obtain similar improvements for parsing cows and
horses over alternative methods. In summary, our strategy of first zooming into
objects and then zooming into parts is very effective. It also enables us to
process different regions of the image at different scales adaptively so that,
for example, we do not need to waste computational resources scaling the entire
image.


Recurrent Saliency Transformation Network: Incorporating Multi-Stage
  Visual Cues for Small Organ Segmentation

  We aim at segmenting small organs (e.g., the pancreas) from abdominal CT
scans. As the target often occupies a relatively small region in the input
image, deep neural networks can be easily confused by the complex and variable
background. To alleviate this, researchers proposed a coarse-to-fine approach,
which used prediction from the first (coarse) stage to indicate a smaller input
region for the second (fine) stage. Despite its effectiveness, this algorithm
dealt with two stages individually, which lacked optimizing a global energy
function, and limited its ability to incorporate multi-stage visual cues.
Missing contextual information led to unsatisfying convergence in iterations,
and that the fine stage sometimes produced even lower segmentation accuracy
than the coarse stage.
  This paper presents a Recurrent Saliency Transformation Network. The key
innovation is a saliency transformation module, which repeatedly converts the
segmentation probability map from the previous iteration as spatial weights and
applies these weights to the current iteration. This brings us two-fold
benefits. In training, it allows joint optimization over the deep networks
dealing with different input scales. In testing, it propagates multi-stage
visual information throughout iterations to improve segmentation accuracy.
Experiments in the NIH pancreas segmentation dataset demonstrate the
state-of-the-art accuracy, which outperforms the previous best by an average of
over 2%. Much higher accuracies are also reported on several small organs in a
larger dataset collected by ourselves. In addition, our approach enjoys better
convergence properties, making it more efficient and reliable in practice.


DeepVoting: A Robust and Explainable Deep Network for Semantic Part
  Detection under Partial Occlusion

  In this paper, we study the task of detecting semantic parts of an object,
e.g., a wheel of a car, under partial occlusion. We propose that all models
should be trained without seeing occlusions while being able to transfer the
learned knowledge to deal with occlusions. This setting alleviates the
difficulty in collecting an exponentially large dataset to cover occlusion
patterns and is more essential. In this scenario, the proposal-based deep
networks, like RCNN-series, often produce unsatisfactory results, because both
the proposal extraction and classification stages may be confused by the
irrelevant occluders. To address this, [25] proposed a voting mechanism that
combines multiple local visual cues to detect semantic parts. The semantic
parts can still be detected even though some visual cues are missing due to
occlusions. However, this method is manually-designed, thus is hard to be
optimized in an end-to-end manner.
  In this paper, we present DeepVoting, which incorporates the robustness shown
by [25] into a deep network, so that the whole pipeline can be jointly
optimized. Specifically, it adds two layers after the intermediate features of
a deep network, e.g., the pool-4 layer of VGGNet. The first layer extracts the
evidence of local visual cues, and the second layer performs a voting mechanism
by utilizing the spatial relationship between visual cues and semantic parts.
We also propose an improved version DeepVoting+ by learning visual cues from
context outside objects. In experiments, DeepVoting achieves significantly
better performance than several baseline methods, including Faster-RCNN, for
semantic part detection under occlusion. In addition, DeepVoting enjoys
explainability as the detection results can be diagnosed via looking up the
voting cues.


Adversarial Attacks Beyond the Image Space

  Generating adversarial examples is an intriguing problem and an important way
of understanding the working mechanism of deep neural networks. Most existing
approaches generated perturbations in the image space, i.e., each pixel can be
modified independently. However, in this paper we pay special attention to the
subset of adversarial examples that correspond to meaningful changes in 3D
physical properties (like rotation and translation, illumination condition,
etc.). These adversaries arguably pose a more serious concern, as they
demonstrate the possibility of causing neural network failure by easy
perturbations of real-world 3D objects and scenes.
  In the contexts of object classification and visual question answering, we
augment state-of-the-art deep neural networks that receive 2D input images with
a rendering module (either differentiable or not) in front, so that a 3D scene
(in the physical space) is rendered into a 2D image (in the image space), and
then mapped to a prediction (in the output space). The adversarial
perturbations can now go beyond the image space, and have clear meanings in the
3D physical world. Though image-space adversaries can be interpreted as
per-pixel albedo change, we verify that they cannot be well explained along
these physically meaningful dimensions, which often have a non-local effect.
But it is still possible to successfully attack beyond the image space on the
physical space, though this is more difficult than image-space attacks,
reflected in lower success rates and heavier perturbations required.


Abdominal multi-organ segmentation with organ-attention networks and
  statistical fusion

  Accurate and robust segmentation of abdominal organs on CT is essential for
many clinical applications such as computer-aided diagnosis and computer-aided
surgery. But this task is challenging due to the weak boundaries of organs, the
complexity of the background, and the variable sizes of different organs. To
address these challenges, we introduce a novel framework for multi-organ
segmentation by using organ-attention networks with reverse connections
(OAN-RCs) which are applied to 2D views, of the 3D CT volume, and output
estimates which are combined by statistical fusion exploiting structural
similarity. OAN is a two-stage deep convolutional network, where deep network
features from the first stage are combined with the original image, in a second
stage, to reduce the complex background and enhance the discriminative
information for the target organs. RCs are added to the first stage to give the
lower layers semantic information thereby enabling them to adapt to the sizes
of different organs. Our networks are trained on 2D views enabling us to use
holistic information and allowing efficient computation. To compensate for the
limited cross-sectional information of the original 3D volumetric CT,
multi-sectional images are reconstructed from the three different 2D view
directions. Then we combine the segmentation results from the different views
using statistical fusion, with a novel term relating the structural similarity
of the 2D views to the original 3D structure. To train the network and evaluate
results, 13 structures were manually annotated by four human raters and
confirmed by a senior expert on 236 normal cases. We tested our algorithm and
computed Dice-Sorensen similarity coefficients and surface distances for
evaluating our estimates of the 13 structures. Our experiments show that the
proposed approach outperforms 2D- and 3D-patch based state-of-the-art methods.


Iterative Reorganization with Weak Spatial Constraints: Solving
  Arbitrary Jigsaw Puzzles for Unsupervised Representation Learning

  Learning visual features from unlabeled image data is an important yet
challenging task, which is often achieved by training a model on some
annotation-free information. We consider spatial contexts, for which we solve
so-called jigsaw puzzles, i.e., each image is cut into grids and then
disordered, and the goal is to recover the correct configuration. Existing
approaches formulated it as a classification task by defining a fixed mapping
from a small subset of configurations to a class set, but these approaches
ignore the underlying relationship between different configurations and also
limit their application to more complex scenarios. This paper presents a novel
approach which applies to jigsaw puzzles with an arbitrary grid size and
dimensionality. We provide a fundamental and generalized principle, that weaker
cues are easier to be learned in an unsupervised manner and also transfer
better. In the context of puzzle recognition, we use an iterative manner which,
instead of solving the puzzle all at once, adjusts the order of the patches in
each step until convergence. In each step, we combine both unary and binary
features on each patch into a cost function judging the correctness of the
current configuration. Our approach, by taking similarity between puzzles into
consideration, enjoys a more reasonable way of learning visual knowledge. We
verify the effectiveness of our approach in two aspects. First, it is able to
solve arbitrarily complex puzzles, including high-dimensional puzzles, that
prior methods are difficult to handle. Second, it serves as a reliable way of
network initialization, which leads to better transfer performance in a few
visual recognition tasks including image classification, object detection, and
semantic segmentation.


