TweeTime: A Minimally Supervised Method for Recognizing and Normalizing
  Time Expressions in Twitter

  We describe TweeTIME, a temporal tagger for recognizing and normalizing time
expressions in Twitter. Most previous work in social media analysis has to rely
on temporal resolvers that are designed for well-edited text, and therefore
suffer from the reduced performance due to domain mismatch. We present a
minimally supervised method that learns from large quantities of unlabeled data
and requires no hand-engineered rules or hand-annotated training corpora.
TweeTIME achieves 0.68 F1 score on the end-to-end task of resolving date
expressions, outperforming a broad range of state-of-the-art systems.


An Annotated Corpus for Machine Reading of Instructions in Wet Lab
  Protocols

  We describe an effort to annotate a corpus of natural language instructions
consisting of 622 wet lab protocols to facilitate automatic or semi-automatic
conversion of protocols into a machine-readable format and benefit biological
research. Experimental results demonstrate the utility of our corpus for
developing machine learning approaches to shallow semantic parsing of
instructional texts. We make our annotated Wet Lab Protocol Corpus available to
the research community.


Structured Minimally Supervised Learning for Neural Relation Extraction

  We present an approach to minimally supervised relation extraction that
combines the benefits of learned representations and structured learning, and
accurately predicts sentence-level relation mentions given only
proposition-level supervision from a KB. By explicitly reasoning about missing
data during learning, our approach enables large-scale training of 1D
convolutional neural networks while mitigating the issue of label noise
inherent in distant supervision. Our approach achieves state-of-the-art results
on minimally supervised sentential relation extraction, outperforming a number
of baselines, including a competitive approach that uses the attention layer of
a purely neural model.


Metal Transport and Chemical Heterogeneity in Early Star Forming Systems

  To constrain the properties of the first stars with the chemical abundance
patterns observed in metal-poor stars, one must identify any non-trivial
effects that the hydrodynamics of metal dispersal can imprint on the
abundances. We use realistic cosmological hydrodynamic simulations to quantify
the distribution of metals resulting from one Population III supernova and from
a small number of such supernovae exploding in close succession. Overall,
supernova ejecta are highly inhomogeneously dispersed throughout the
simulations. When the supernova bubbles collapse, quasi-virialized
metal-enriched clouds, fed by fallback from the bubbles and by streaming of
metal-free gas from the cosmic web, grow in the centers of the dark matter
halos. Partial turbulent homogenization on scales resolved in the simulation is
observed only in the densest clouds where the vortical time scales are short
enough to ensure true homogenization on subgrid scales. However, the abundances
in the clouds differ from the gross yields of the supernovae. Continuing the
simulations until the cloud have gone into gravitational collapse, we predict
that the abundances in second-generation stars will be deficient in the
innermost mass shells of the supernova (if only one has exploded) or in the
ejecta of the latest supernovae (when multiple have exploded). This indicates
that hydrodynamics gives rise to biases complicating the identification of
nucleosynthetic sources in the chemical abundance spaces of the surviving
stars.


Abundance anomalies in metal-poor stars from Population III supernova
  ejecta hydrodynamics

  We present a simulation of the long-term evolution of a Population III
supernova remnant in a cosmological minihalo. Employing passive Lagrangian
tracer particles, we investigate how chemical stratification and anisotropy in
the explosion can affect the abundances of the first low-mass, metal-enriched
stars. We find that reverse shock heating can leave the inner mass shells at
entropies too high to cool, leading to carbon-enhancement in the re-collapsing
gas. This hydrodynamic selection effect could explain the observed incidence of
carbon-enhanced metal-poor (CEMP) stars at low metallicity. We further explore
how anisotropic ejecta distributions, recently seen in direct numerical
simulations of core-collapse explosions, may translate to abundances in
metal-poor stars. We find that some of the observed scatter in the Population
II abundance ratios can be explained by an incomplete mixing of supernova
ejecta, even in the case of only one contributing enrichment event. We
demonstrate that the customary hypothesis of fully-mixed ejecta clearly fails
if post-explosion hydrodynamics prefers the recycling of some nucleosynthetic
products over others. Furthermore, to fully exploit the stellar-archaeological
program of constraining the Pop III initial mass function from the observed Pop
II abundances, considering these hydrodynamical transport effects is crucial.
We discuss applications to the rich chemical structure of ultra-faint dwarf
satellite galaxies, to be probed in unprecedented detail with upcoming
spectroscopic surveys.


Deep Reinforcement Learning for Dialogue Generation

  Recent neural models of dialogue generation offer great promise for
generating responses for conversational agents, but tend to be shortsighted,
predicting utterances one at a time while ignoring their influence on future
outcomes. Modeling the future direction of a dialogue is crucial to generating
coherent, interesting dialogues, a need which led traditional NLP models of
dialogue to draw on reinforcement learning. In this paper, we show how to
integrate these goals, applying deep reinforcement learning to model future
reward in chatbot dialogue. The model simulates dialogues between two virtual
agents, using policy gradient methods to reward sequences that display three
useful conversational properties: informativity (non-repetitive turns),
coherence, and ease of answering (related to forward-looking function). We
evaluate our model on diversity, length as well as with human judges, showing
that the proposed algorithm generates more interactive responses and manages to
foster a more sustained conversation in dialogue simulation. This work marks a
first step towards learning a neural conversational model based on the
long-term success of dialogues.


Inferring User Preferences by Probabilistic Logical Reasoning over
  Social Networks

  We propose a framework for inferring the latent attitudes or preferences of
users by performing probabilistic first-order logical reasoning over the social
network graph. Our method answers questions about Twitter users like {\em Does
this user like sushi?} or {\em Is this user a New York Knicks fan?} by building
a probabilistic model that reasons over user attributes (the user's location or
gender) and the social network (the user's friends and spouse), via inferences
like homophily (I am more likely to like sushi if spouse or friends like sushi,
I am more likely to like the Knicks if I live in New York). The algorithm uses
distant supervision, semi-supervised data harvesting and vector space models to
extract user attributes (e.g. spouse, education, location) and preferences
(likes and dislikes) from text. The extracted propositions are then fed into a
probabilistic reasoner (we investigate both Markov Logic and Probabilistic Soft
Logic). Our experiments show that probabilistic logical reasoning significantly
improves the performance on attribute and relation extraction, and also
achieves an F-score of 0.791 at predicting a users likes or dislikes,
significantly better than two strong baselines.


Learning multi-faceted representations of individuals from heterogeneous
  evidence using neural networks

  Inferring latent attributes of people online is an important social computing
task, but requires integrating the many heterogeneous sources of information
available on the web. We propose learning individual representations of people
using neural nets to integrate rich linguistic and network evidence gathered
from social media. The algorithm is able to combine diverse cues, such as the
text a person writes, their attributes (e.g. gender, employer, education,
location) and social relations to other people. We show that by integrating
both textual and network evidence, these representations offer improved
performance at four important tasks in social media inference on Twitter:
predicting (1) gender, (2) occupation, (3) location, and (4) friendships for
users. Our approach scales to large datasets and the learned representations
can be used as general features in and have the potential to benefit a large
number of downstream tasks including link prediction, community detection, or
probabilistic reasoning over social networks.


Adversarial Learning for Neural Dialogue Generation

  In this paper, drawing intuition from the Turing test, we propose using
adversarial training for open-domain dialogue generation: the system is trained
to produce sequences that are indistinguishable from human-generated dialogue
utterances. We cast the task as a reinforcement learning (RL) problem where we
jointly train two systems, a generative model to produce response sequences,
and a discriminator---analagous to the human evaluator in the Turing test--- to
distinguish between the human-generated dialogues and the machine-generated
ones. The outputs from the discriminator are then used as rewards for the
generative model, pushing the system to generate dialogues that mostly resemble
human dialogues.
  In addition to adversarial training we describe a model for adversarial {\em
evaluation} that uses success in fooling an adversary as a dialogue evaluation
metric, while avoiding a number of potential pitfalls. Experimental results on
several metrics, including adversarial evaluation, demonstrate that the
adversarially-trained system generates higher-quality responses than previous
baselines.


"i have a feeling trump will win..................": Forecasting Winners
  and Losers from User Predictions on Twitter

  Social media users often make explicit predictions about upcoming events.
Such statements vary in the degree of certainty the author expresses toward the
outcome:"Leonardo DiCaprio will win Best Actor" vs. "Leonardo DiCaprio may win"
or "No way Leonardo wins!". Can popular beliefs on social media predict who
will win? To answer this question, we build a corpus of tweets annotated for
veridicality on which we train a log-linear classifier that detects positive
veridicality with high precision. We then forecast uncertain outcomes using the
wisdom of crowds, by aggregating users' explicit predictions. Our method for
forecasting winners is fully automated, relying only on a set of contenders as
input. It requires no training data of past outcomes and outperforms sentiment
and tweet volume baselines on a broad range of contest prediction tasks. We
further demonstrate how our approach can be used to measure the reliability of
individual accounts' predictions and retrospectively identify surprise
outcomes.


Generating More Interesting Responses in Neural Conversation Models with
  Distributional Constraints

  Neural conversation models tend to generate safe, generic responses for most
inputs. This is due to the limitations of likelihood-based decoding objectives
in generation tasks with diverse outputs, such as conversation. To address this
challenge, we propose a simple yet effective approach for incorporating side
information in the form of distributional constraints over the generated
responses. We propose two constraints that help generate more content rich
responses that are based on a model of syntax and topics (Griffiths et al.,
2005) and semantic similarity (Arora et al., 2016). We evaluate our approach
against a variety of competitive baselines, using both automatic metrics and
human judgments, showing that our proposed approach generates responses that
are much less generic without sacrificing plausibility. A working demo of our
code can be found at https://github.com/abaheti95/DC-NeuralConversation.


Analyzing the Perceived Severity of Cybersecurity Threats Reported on
  Social Media

  Breaking cybersecurity events are shared across a range of websites,
including security blogs (FireEye, Kaspersky, etc.), in addition to social
media platforms such as Facebook and Twitter. In this paper, we investigate
methods to analyze the severity of cybersecurity threats based on the language
that is used to describe them online. A corpus of 6,000 tweets describing
software vulnerabilities is annotated with authors' opinions toward their
severity. We show that our corpus supports the development of automatic
classifiers with high precision for this task. Furthermore, we demonstrate the
value of analyzing users' opinions about the severity of threats reported
online as an early indicator of important software vulnerabilities. We present
a simple, yet effective method for linking software vulnerabilities reported in
tweets to Common Vulnerabilities and Exposures (CVEs) in the National
Vulnerability Database (NVD). Using our predicted severity scores, we show that
it is possible to achieve a Precision@50 of 0.86 when forecasting high severity
vulnerabilities, significantly outperforming a baseline that is based on tweet
volume. Finally we show how reports of severe vulnerabilities online are
predictive of real-world exploits.


Multicomponent Gas-Particle Flow and Heat/Mass Transfer Induced by a
  Localized Laser Irradiation on a Urethane-Coated Stainless Steel Substrate

  A three-dimensional numerical simulation is conducted for a complex process
in a laser-material system, which involves heat and mass transfer in a
compressible gaseous phase and chemical reaction during laser irradiation on a
urethane paint coated on a stainless steel substrate. A finite volume method
(FVM) with a co-located grid mesh that discretizes the entire computational
domain is employed to simulate the heating process. The results show that when
the top surface of the paint reaches a threshold temperature of 560 K, the
polyurethane starts to decompose through chemical reaction. As a result,
combustion products CO2, H2O and NO2 are produced and chromium (III) oxide,
which serves as pigment in the paint, is ejected as solid parcels from the
paint into the gaseous domain. Variations of temperature, density and velocity
at the center of the laser irradiation spot, and the concentrations of reaction
reactant/products in the gaseous phase are presented and discussed, by
comparing six scenarios with different laser powers ranging from 2.5 kW to 15
kW with an increment of 2.5 kW.


Measurement of the Wrong-Sign Decay D0 -> K+ pi- pi+ pi-

  A measurement of the rate for the "wrong-sign" decay D0 -> K+ pi- pi+ pi-
relative to that for the "right-sign" decay D0 -> K- pi+ pi+ pi- is presented.
Using 791 fb-1 of data collected with the Belle detector, we obtain a branching
fraction ratio of R_WS = [0.324 +- 0.008 (stat) +- 0.007 (sys)]%. Multiplying
this ratio by the world average value for the branching fraction B(D0 -> K- pi+
pi+ pi-) gives a branching fraction B(D0 -> K+ pi- pi+ pi-) = (2.61 +- 0.06
+0.09 -0.08) x 10-4.


Search for the decay B0 -> phi gamma

  We have searched for the decay B0 -> phi gamma using the full Belle data set
of 772 x 10^6 BBbar pairs collected at the Upsilon(4S) resonance with the Belle
detector at the KEKB e+e- collider. No signal is observed, and we set an upper
limit on the branching fraction of B(B0 -> phi gamma) < 1.0 x 10^{-7} at 90%
confidence level. This is the most stringent limit on this decay mode to date.


