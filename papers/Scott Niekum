Towards Online Learning from Corrective Demonstrations

  Robots operating in real-world human environments will likely encounter taskexecution failures. To address this, we would like to allow co-present humansto refine the robot's task model as errors are encountered. Existing approachesto task model modification require reasoning over the entire dataset and model,limiting the rate of corrective updates. We introduce the State-Indexed TaskUpdates (SITU) algorithm to efficiently incorporate corrective demonstrationsinto an existing task model by iteratively making local updates that onlyrequire reasoning over a small subset of the model. In future work, we willevaluate this approach with a user study.

Data-Efficient Policy Evaluation Through Behavior Policy Search

  We consider the task of evaluating a policy for a Markov decision process(MDP). The standard unbiased technique for evaluating a policy is to deploy thepolicy and observe its performance. We show that the data collected fromdeploying a different policy, commonly called the behavior policy, can be usedto produce unbiased estimates with lower mean squared error than this standardtechnique. We derive an analytic expression for the optimal behavior policy ---the behavior policy that minimizes the mean squared error of the resultingestimates. Because this expression depends on terms that are unknown inpractice, we propose a novel policy evaluation sub-problem, behavior policysearch: searching for a behavior policy that reduces mean squared error. Wepresent a behavior policy search algorithm and empirically demonstrate itseffectiveness in lowering the mean squared error of policy performanceestimates.

Safe Reinforcement Learning via Shielding

  Reinforcement learning algorithms discover policies that maximize reward, butdo not necessarily guarantee safety during learning or execution phases. Weintroduce a new approach to learn optimal policies while enforcing propertiesexpressed in temporal logic. To this end, given the temporal logicspecification that is to be obeyed by the learning system, we propose tosynthesize a reactive system called a shield. The shield is introduced in thetraditional learning process in two alternative ways, depending on the locationat which the shield is implemented. In the first one, the shield acts each timethe learning agent is about to make a decision and provides a list of safeactions. In the second way, the shield is introduced after the learning agent.The shield monitors the actions from the learner and corrects them only if thechosen action causes a violation of the specification. We discuss whichrequirements a shield must meet to preserve the convergence guarantees of thelearner. Finally, we demonstrate the versatility of our approach on severalchallenging reinforcement learning scenarios.

Importance Sampling Policy Evaluation with an Estimated Behavior Policy

  We consider the problem of off-policy evaluation in Markov decisionprocesses. Off-policy evaluation is the task of evaluating the expected returnof one policy with data generated by a different, behavior policy. Importancesampling is a technique for off-policy evaluation that re-weights off-policyreturns to account for differences in the likelihood of the returns between thetwo policies. In this paper, we study importance sampling with an estimatedbehavior policy where the behavior policy estimate comes from the same set ofdata used to compute the importance sampling estimate. We find that thisestimator often lowers the mean squared error of off-policy evaluation comparedto importance sampling with the true behavior policy or using a behavior policythat is estimated from a separate data set. Our empirical results also extendto other popular variants of importance sampling and show that estimating anon-Markovian behavior policy can further lower mean squared error even whenthe true behavior policy is Markovian.

Risk-Aware Active Inverse Reinforcement Learning

  Active learning from demonstration allows a robot to query a human forspecific types of input to achieve efficient learning. Existing work hasexplored a variety of active query strategies; however, to our knowledge, noneof these strategies directly minimize the performance risk of the policy therobot is learning. Utilizing recent advances in performance bounds for inversereinforcement learning, we propose a risk-aware active inverse reinforcementlearning algorithm that focuses active queries on areas of the state space withthe potential for large generalization error. We show that risk-aware activelearning outperforms standard active IRL approaches on gridworld, simulateddriving, and table setting tasks, while also providing a performance-basedstopping criterion that allows a robot to know when it has received enoughdemonstrations to safely perform a task.

Using Natural Language for Reward Shaping in Reinforcement Learning

  Recent reinforcement learning (RL) approaches have shown strong performancein complex domains such as Atari games, but are often highly sampleinefficient. A common approach to reduce interaction time with the environmentis to use reward shaping, which involves carefully designing reward functionsthat provide the agent intermediate rewards for progress towards the goal.However, designing appropriate shaping rewards is known to be difficult as wellas time-consuming. In this work, we address this problem by using naturallanguage instructions to perform reward shaping. We propose the LanguagE-ActionReward Network (LEARN), a framework that maps free-form natural languageinstructions to intermediate rewards based on actions taken by the agent. Theseintermediate language-based rewards can seamlessly be integrated into anystandard reinforcement learning algorithm. We experiment with Montezuma'sRevenge from the Atari Learning Environment, a popular benchmark in RL. Ourexperiments on a diverse set of 15 tasks demonstrate that, for the same numberof interactions with the environment, language-based rewards lead to successfulcompletion of the task 60% more often on average, compared to learning withoutlanguage.

Bootstrapping with Models: Confidence Intervals for Off-Policy  Evaluation

  For an autonomous agent, executing a poor policy may be costly or evendangerous. For such agents, it is desirable to determine confidence intervallower bounds on the performance of any given policy without executing saidpolicy. Current methods for exact high confidence off-policy evaluation thatuse importance sampling require a substantial amount of data to achieve a tightlower bound. Existing model-based methods only address the problem in discretestate spaces. Since exact bounds are intractable for many domains we trade offstrict guarantees of safety for more data-efficient approximate bounds. In thiscontext, we propose two bootstrapping off-policy evaluation methods which uselearned MDP transition models in order to estimate lower confidence bounds onpolicy performance with limited data in both continuous and discrete statespaces. Since direct use of a model may introduce bias, we derive a theoreticalupper bound on model bias for when the model transition function is estimatedwith i.i.d. trajectories. This bound broadens our understanding of theconditions under which model-based methods have high bias. Finally, weempirically evaluate our proposed methods and analyze the settings in whichdifferent bootstrapping off-policy confidence interval methods succeed andfail.

Efficient Probabilistic Performance Bounds for Inverse Reinforcement  Learning

  In the field of reinforcement learning there has been recent progress towardssafety and high-confidence bounds on policy performance. However, to ourknowledge, no practical methods exist for determining high-confidence policyperformance bounds in the inverse reinforcement learning setting---where thetrue reward function is unknown and only samples of expert behavior are given.We propose a sampling method based on Bayesian inverse reinforcement learningthat uses demonstrations to determine practical high-confidence upper bounds onthe $\alpha$-worst-case difference in expected return between any evaluationpolicy and the optimal policy under the expert's unknown reward function. Weevaluate our proposed bound on both a standard grid navigation task and asimulated driving task and achieve tighter and more accurate bounds than afeature count-based baseline. We also give examples of how our proposed boundcan be utilized to perform risk-aware policy selection and risk-aware policyimprovement. Because our proposed bound requires several orders of magnitudefewer demonstrations than existing high-confidence bounds, it is the firstpractical method that allows agents that learn from demonstration to expressconfidence in the quality of their learned policy.

Efficient Hierarchical Robot Motion Planning Under Uncertainty and  Hybrid Dynamics

  Noisy observations coupled with nonlinear dynamics pose one of the biggestchallenges in robot motion planning. By decomposing nonlinear dynamics into adiscrete set of local dynamics models, hybrid dynamics provide a natural way tomodel nonlinear dynamics, especially in systems with sudden discontinuities indynamics due to factors such as contacts. We propose a hierarchical POMDPplanner that develops cost-optimized motion plans for hybrid dynamics models.The hierarchical planner first develops a high-level motion plan to sequencethe local dynamics models to be visited and then converts it into a detailedcontinuous state plan. This hierarchical planning approach results in adecomposition of the POMDP planning problem into smaller sub-parts that can besolved with significantly lower computational costs. The ability to sequencethe visitation of local dynamics models also provides a powerful way toleverage the hybrid dynamics to reduce state uncertainty. We evaluate theproposed planner on a navigation task in the simulated domain and on anassembly task with a robotic manipulator, showing that our approach can solvetasks having high observation noise and nonlinear dynamics effectively withsignificantly lower computational costs compared to direct planning approaches.

Machine Teaching for Inverse Reinforcement Learning: Algorithms and  Applications

  Inverse reinforcement learning (IRL) infers a reward function fromdemonstrations, allowing for policy improvement and generalization. However,despite much recent interest in IRL, little work has been done to understandthe minimum set of demonstrations needed to teach a specific sequentialdecision-making task. We formalize the problem of finding maximally informativedemonstrations for IRL as a machine teaching problem where the goal is to findthe minimum number of demonstrations needed to specify the reward equivalenceclass of the demonstrator. We extend previous work on algorithmic teaching forsequential decision-making tasks by showing a reduction to the set coverproblem which enables an efficient approximation algorithm for determining theset of maximally-informative demonstrations. We apply our proposed machineteaching algorithm to two novel applications: providing a lower bound on thenumber of queries needed to learn a policy using active IRL and developing anovel IRL algorithm that can learn more efficiently from informativedemonstrations than a standard IRL approach.

One-Shot Learning of Multi-Step Tasks from Observation via Activity  Localization in Auxiliary Video

  Due to burdensome data requirements, learning from demonstration often fallsshort of its promise to allow users to quickly and naturally program robots.Demonstrations are inherently ambiguous and incomplete, making correctgeneralization to unseen situations difficult without a large number ofdemonstrations in varying conditions. By contrast, humans are often able tolearn complex tasks from a single demonstration (typically observations withoutaction labels) by leveraging context learned over a lifetime. Inspired by thiscapability, our goal is to enable robots to perform one-shot learning ofmulti-step tasks from observation by leveraging auxiliary video data ascontext. Our primary contribution is a novel system that achieves this goal by:(1) using a single user-segmented demonstration to define the primitive actionsthat comprise a task, (2) localizing additional examples of these actions inunsegmented auxiliary videos via a metalearning-based approach, (3) using theseadditional examples to learn a reward function for each action, and (4)performing reinforcement learning on top of the inferred reward functions tolearn action policies that can be combined to accomplish the task. Weempirically demonstrate that a robot can learn multi-step tasks moreeffectively when provided auxiliary video, and that performance greatlyimproves when localizing individual actions, compared to learning fromunsegmented videos.

LAAIR: A Layered Architecture for Autonomous Interactive Robots

  When developing general purpose robots, the overarching software architecturecan greatly affect the ease of accomplishing various tasks. Initial efforts tocreate unified robot systems in the 1990s led to hybrid architectures,emphasizing a hierarchy in which deliberative plans direct the use of reactiveskills. However, since that time there has been significant progress in thelow-level skills available to robots, including manipulation and perception,making it newly feasible to accomplish many more tasks in real-world domains.There is thus renewed optimism that robots will be able to perform a wide arrayof tasks while maintaining responsiveness to human operators. However, the toplayer in traditional hybrid architectures, designed to achieve long-term goals,can make it difficult to react quickly to human interactions during goal-drivenexecution. To mitigate this difficulty, we propose a novel architecture thatsupports such transitions by adding a top-level reactive module which hasflexible access to both reactive skills and a deliberative control module. Tovalidate this architecture, we present a case study of its application on adomestic service robot platform.

