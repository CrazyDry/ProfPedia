Towards Online Learning from Corrective Demonstrations

  Robots operating in real-world human environments will likely encounter task
execution failures. To address this, we would like to allow co-present humans
to refine the robot's task model as errors are encountered. Existing approaches
to task model modification require reasoning over the entire dataset and model,
limiting the rate of corrective updates. We introduce the State-Indexed Task
Updates (SITU) algorithm to efficiently incorporate corrective demonstrations
into an existing task model by iteratively making local updates that only
require reasoning over a small subset of the model. In future work, we will
evaluate this approach with a user study.


Data-Efficient Policy Evaluation Through Behavior Policy Search

  We consider the task of evaluating a policy for a Markov decision process
(MDP). The standard unbiased technique for evaluating a policy is to deploy the
policy and observe its performance. We show that the data collected from
deploying a different policy, commonly called the behavior policy, can be used
to produce unbiased estimates with lower mean squared error than this standard
technique. We derive an analytic expression for the optimal behavior policy ---
the behavior policy that minimizes the mean squared error of the resulting
estimates. Because this expression depends on terms that are unknown in
practice, we propose a novel policy evaluation sub-problem, behavior policy
search: searching for a behavior policy that reduces mean squared error. We
present a behavior policy search algorithm and empirically demonstrate its
effectiveness in lowering the mean squared error of policy performance
estimates.


Safe Reinforcement Learning via Shielding

  Reinforcement learning algorithms discover policies that maximize reward, but
do not necessarily guarantee safety during learning or execution phases. We
introduce a new approach to learn optimal policies while enforcing properties
expressed in temporal logic. To this end, given the temporal logic
specification that is to be obeyed by the learning system, we propose to
synthesize a reactive system called a shield. The shield is introduced in the
traditional learning process in two alternative ways, depending on the location
at which the shield is implemented. In the first one, the shield acts each time
the learning agent is about to make a decision and provides a list of safe
actions. In the second way, the shield is introduced after the learning agent.
The shield monitors the actions from the learner and corrects them only if the
chosen action causes a violation of the specification. We discuss which
requirements a shield must meet to preserve the convergence guarantees of the
learner. Finally, we demonstrate the versatility of our approach on several
challenging reinforcement learning scenarios.


Importance Sampling Policy Evaluation with an Estimated Behavior Policy

  We consider the problem of off-policy evaluation in Markov decision
processes. Off-policy evaluation is the task of evaluating the expected return
of one policy with data generated by a different, behavior policy. Importance
sampling is a technique for off-policy evaluation that re-weights off-policy
returns to account for differences in the likelihood of the returns between the
two policies. In this paper, we study importance sampling with an estimated
behavior policy where the behavior policy estimate comes from the same set of
data used to compute the importance sampling estimate. We find that this
estimator often lowers the mean squared error of off-policy evaluation compared
to importance sampling with the true behavior policy or using a behavior policy
that is estimated from a separate data set. Our empirical results also extend
to other popular variants of importance sampling and show that estimating a
non-Markovian behavior policy can further lower mean squared error even when
the true behavior policy is Markovian.


Risk-Aware Active Inverse Reinforcement Learning

  Active learning from demonstration allows a robot to query a human for
specific types of input to achieve efficient learning. Existing work has
explored a variety of active query strategies; however, to our knowledge, none
of these strategies directly minimize the performance risk of the policy the
robot is learning. Utilizing recent advances in performance bounds for inverse
reinforcement learning, we propose a risk-aware active inverse reinforcement
learning algorithm that focuses active queries on areas of the state space with
the potential for large generalization error. We show that risk-aware active
learning outperforms standard active IRL approaches on gridworld, simulated
driving, and table setting tasks, while also providing a performance-based
stopping criterion that allows a robot to know when it has received enough
demonstrations to safely perform a task.


Using Natural Language for Reward Shaping in Reinforcement Learning

  Recent reinforcement learning (RL) approaches have shown strong performance
in complex domains such as Atari games, but are often highly sample
inefficient. A common approach to reduce interaction time with the environment
is to use reward shaping, which involves carefully designing reward functions
that provide the agent intermediate rewards for progress towards the goal.
However, designing appropriate shaping rewards is known to be difficult as well
as time-consuming. In this work, we address this problem by using natural
language instructions to perform reward shaping. We propose the LanguagE-Action
Reward Network (LEARN), a framework that maps free-form natural language
instructions to intermediate rewards based on actions taken by the agent. These
intermediate language-based rewards can seamlessly be integrated into any
standard reinforcement learning algorithm. We experiment with Montezuma's
Revenge from the Atari Learning Environment, a popular benchmark in RL. Our
experiments on a diverse set of 15 tasks demonstrate that, for the same number
of interactions with the environment, language-based rewards lead to successful
completion of the task 60% more often on average, compared to learning without
language.


Bootstrapping with Models: Confidence Intervals for Off-Policy
  Evaluation

  For an autonomous agent, executing a poor policy may be costly or even
dangerous. For such agents, it is desirable to determine confidence interval
lower bounds on the performance of any given policy without executing said
policy. Current methods for exact high confidence off-policy evaluation that
use importance sampling require a substantial amount of data to achieve a tight
lower bound. Existing model-based methods only address the problem in discrete
state spaces. Since exact bounds are intractable for many domains we trade off
strict guarantees of safety for more data-efficient approximate bounds. In this
context, we propose two bootstrapping off-policy evaluation methods which use
learned MDP transition models in order to estimate lower confidence bounds on
policy performance with limited data in both continuous and discrete state
spaces. Since direct use of a model may introduce bias, we derive a theoretical
upper bound on model bias for when the model transition function is estimated
with i.i.d. trajectories. This bound broadens our understanding of the
conditions under which model-based methods have high bias. Finally, we
empirically evaluate our proposed methods and analyze the settings in which
different bootstrapping off-policy confidence interval methods succeed and
fail.


Efficient Probabilistic Performance Bounds for Inverse Reinforcement
  Learning

  In the field of reinforcement learning there has been recent progress towards
safety and high-confidence bounds on policy performance. However, to our
knowledge, no practical methods exist for determining high-confidence policy
performance bounds in the inverse reinforcement learning setting---where the
true reward function is unknown and only samples of expert behavior are given.
We propose a sampling method based on Bayesian inverse reinforcement learning
that uses demonstrations to determine practical high-confidence upper bounds on
the $\alpha$-worst-case difference in expected return between any evaluation
policy and the optimal policy under the expert's unknown reward function. We
evaluate our proposed bound on both a standard grid navigation task and a
simulated driving task and achieve tighter and more accurate bounds than a
feature count-based baseline. We also give examples of how our proposed bound
can be utilized to perform risk-aware policy selection and risk-aware policy
improvement. Because our proposed bound requires several orders of magnitude
fewer demonstrations than existing high-confidence bounds, it is the first
practical method that allows agents that learn from demonstration to express
confidence in the quality of their learned policy.


Efficient Hierarchical Robot Motion Planning Under Uncertainty and
  Hybrid Dynamics

  Noisy observations coupled with nonlinear dynamics pose one of the biggest
challenges in robot motion planning. By decomposing nonlinear dynamics into a
discrete set of local dynamics models, hybrid dynamics provide a natural way to
model nonlinear dynamics, especially in systems with sudden discontinuities in
dynamics due to factors such as contacts. We propose a hierarchical POMDP
planner that develops cost-optimized motion plans for hybrid dynamics models.
The hierarchical planner first develops a high-level motion plan to sequence
the local dynamics models to be visited and then converts it into a detailed
continuous state plan. This hierarchical planning approach results in a
decomposition of the POMDP planning problem into smaller sub-parts that can be
solved with significantly lower computational costs. The ability to sequence
the visitation of local dynamics models also provides a powerful way to
leverage the hybrid dynamics to reduce state uncertainty. We evaluate the
proposed planner on a navigation task in the simulated domain and on an
assembly task with a robotic manipulator, showing that our approach can solve
tasks having high observation noise and nonlinear dynamics effectively with
significantly lower computational costs compared to direct planning approaches.


Machine Teaching for Inverse Reinforcement Learning: Algorithms and
  Applications

  Inverse reinforcement learning (IRL) infers a reward function from
demonstrations, allowing for policy improvement and generalization. However,
despite much recent interest in IRL, little work has been done to understand
the minimum set of demonstrations needed to teach a specific sequential
decision-making task. We formalize the problem of finding maximally informative
demonstrations for IRL as a machine teaching problem where the goal is to find
the minimum number of demonstrations needed to specify the reward equivalence
class of the demonstrator. We extend previous work on algorithmic teaching for
sequential decision-making tasks by showing a reduction to the set cover
problem which enables an efficient approximation algorithm for determining the
set of maximally-informative demonstrations. We apply our proposed machine
teaching algorithm to two novel applications: providing a lower bound on the
number of queries needed to learn a policy using active IRL and developing a
novel IRL algorithm that can learn more efficiently from informative
demonstrations than a standard IRL approach.


One-Shot Learning of Multi-Step Tasks from Observation via Activity
  Localization in Auxiliary Video

  Due to burdensome data requirements, learning from demonstration often falls
short of its promise to allow users to quickly and naturally program robots.
Demonstrations are inherently ambiguous and incomplete, making correct
generalization to unseen situations difficult without a large number of
demonstrations in varying conditions. By contrast, humans are often able to
learn complex tasks from a single demonstration (typically observations without
action labels) by leveraging context learned over a lifetime. Inspired by this
capability, our goal is to enable robots to perform one-shot learning of
multi-step tasks from observation by leveraging auxiliary video data as
context. Our primary contribution is a novel system that achieves this goal by:
(1) using a single user-segmented demonstration to define the primitive actions
that comprise a task, (2) localizing additional examples of these actions in
unsegmented auxiliary videos via a metalearning-based approach, (3) using these
additional examples to learn a reward function for each action, and (4)
performing reinforcement learning on top of the inferred reward functions to
learn action policies that can be combined to accomplish the task. We
empirically demonstrate that a robot can learn multi-step tasks more
effectively when provided auxiliary video, and that performance greatly
improves when localizing individual actions, compared to learning from
unsegmented videos.


LAAIR: A Layered Architecture for Autonomous Interactive Robots

  When developing general purpose robots, the overarching software architecture
can greatly affect the ease of accomplishing various tasks. Initial efforts to
create unified robot systems in the 1990s led to hybrid architectures,
emphasizing a hierarchy in which deliberative plans direct the use of reactive
skills. However, since that time there has been significant progress in the
low-level skills available to robots, including manipulation and perception,
making it newly feasible to accomplish many more tasks in real-world domains.
There is thus renewed optimism that robots will be able to perform a wide array
of tasks while maintaining responsiveness to human operators. However, the top
layer in traditional hybrid architectures, designed to achieve long-term goals,
can make it difficult to react quickly to human interactions during goal-driven
execution. To mitigate this difficulty, we propose a novel architecture that
supports such transitions by adding a top-level reactive module which has
flexible access to both reactive skills and a deliberative control module. To
validate this architecture, we present a case study of its application on a
domestic service robot platform.


