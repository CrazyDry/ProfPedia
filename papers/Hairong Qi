Feature Encoding in Band-limited Distributed Surveillance Systems

  Distributed surveillance systems have become popular in recent years due to
security concerns. However, transmitting high dimensional data in
bandwidth-limited distributed systems becomes a major challenge. In this paper,
we address this issue by proposing a novel probabilistic algorithm based on the
divergence between the probability distributions of the visual features in
order to reduce their dimensionality and thus save the network bandwidth in
distributed wireless smart camera networks. We demonstrate the effectiveness of
the proposed approach through extensive experiments on two surveillance
recognition tasks.


End-to-end Binary Representation Learning via Direct Binary Embedding

  Learning binary representation is essential to large-scale computer vision
tasks. Most existing algorithms require a separate quantization constraint to
learn effective hashing functions. In this work, we present Direct Binary
Embedding (DBE), a simple yet very effective algorithm to learn binary
representation in an end-to-end fashion. By appending an ingeniously designed
DBE layer to the deep convolutional neural network (DCNN), DBE learns binary
code directly from the continuous DBE layer activation without quantization
error. By employing the deep residual network (ResNet) as DCNN component, DBE
captures rich semantics from images. Furthermore, in the effort of handling
multilabel images, we design a joint cross entropy loss that includes both
softmax cross entropy and weighted binary cross entropy in consideration of the
correlation and independence of labels, respectively. Extensive experiments
demonstrate the significant superiority of DBE over state-of-the-art methods on
tasks of natural object recognition, image retrieval and image annotation.


Multi-View Task-Driven Recognition in Visual Sensor Networks

  Nowadays, distributed smart cameras are deployed for a wide set of tasks in
several application scenarios, ranging from object recognition, image
retrieval, and forensic applications. Due to limited bandwidth in distributed
systems, efficient coding of local visual features has in fact been an active
topic of research. In this paper, we propose a novel approach to obtain a
compact representation of high-dimensional visual data using sensor fusion
techniques. We convert the problem of visual analysis in resource-limited
scenarios to a multi-view representation learning, and we show that the key to
finding properly compressed representation is to exploit the position of
cameras with respect to each other as a norm-based regularization in the
particular signal representation of sparse coding. Learning the representation
of each camera is viewed as an individual task and a multi-task learning with
joint sparsity for all nodes is employed. The proposed representation learning
scheme is referred to as the multi-view task-driven learning for visual sensor
network (MT-VSN). We demonstrate that MT-VSN outperforms state-of-the-art in
various surveillance recognition tasks.


Addressing Ambiguity in Multi-target Tracking by Hierarchical Strategy

  This paper presents a novel hierarchical approach for the simultaneous
tracking of multiple targets in a video. We use a network flow approach to link
detections in low-level and tracklets in high-level. At each step of the
hierarchy, the confidence of candidates is measured by using a new scoring
system, ConfRank, that considers the quality and the quantity of its
neighborhood. The output of the first stage is a collection of safe tracklets
and unlinked high-confidence detections. For each individual detection, we
determine if it belongs to an existing or is a new tracklet. We show the effect
of our framework to recover missed detections and reduce switch identity. The
proposed tracker is referred to as TVOD for multi-target tracking using the
visual tracker and generic object detector. We achieve competitive results with
lower identity switches on several datasets comparing to state-of-the-art.


Person Re-identification Using Visual Attention

  Despite recent attempts for solving the person re-identification problem, it
remains a challenging task since a person's appearance can vary significantly
when large variations in view angle, human pose and illumination are involved.
The concept of attention is one of the most interesting recent architectural
innovations in neural networks. Inspired by that, in this paper we propose a
novel approach based on using a gradient-based attention mechanism in deep
convolution neural network for solving the person re-identification problem.
Our model learns to focus selectively on parts of the input image for which the
networks' output is most sensitive to. Extensive comparative evaluations
demonstrate that the proposed method outperforms state-of-the-art approaches,
including both traditional and deep neural network-based methods on the
challenging CUHK01, CUHK03, and Market1501 datasets.


Decoupled Learning for Conditional Adversarial Networks

  Incorporating encoding-decoding nets with adversarial nets has been widely
adopted in image generation tasks. We observe that the state-of-the-art
achievements were obtained by carefully balancing the reconstruction loss and
adversarial loss, and such balance shifts with different network structures,
datasets, and training strategies. Empirical studies have demonstrated that an
inappropriate weight between the two losses may cause instability, and it is
tricky to search for the optimal setting, especially when lacking prior
knowledge on the data and network.
  This paper gives the first attempt to relax the need of manual balancing by
proposing the concept of \textit{decoupled learning}, where a novel network
structure is designed that explicitly disentangles the backpropagation paths of
the two losses.
  Experimental results demonstrate the effectiveness, robustness, and
generality of the proposed method. The other contribution of the paper is the
design of a new evaluation metric to measure the image quality of generative
models. We propose the so-called \textit{normalized relative discriminative
score} (NRDS), which introduces the idea of relative comparison, rather than
providing absolute estimates like existing metrics.


Fast-converging Conditional Generative Adversarial Networks for Image
  Synthesis

  Building on top of the success of generative adversarial networks (GANs),
conditional GANs attempt to better direct the data generation process by
conditioning with certain additional information. Inspired by the most recent
AC-GAN, in this paper we propose a fast-converging conditional GAN (FC-GAN). In
addition to the real/fake classifier used in vanilla GANs, our discriminator
has an advanced auxiliary classifier which distinguishes each real class from
an extra `fake' class. The `fake' class avoids mixing generated data with real
data, which can potentially confuse the classification of real data as AC-GAN
does, and makes the advanced auxiliary classifier behave as another real/fake
classifier. As a result, FC-GAN can accelerate the process of differentiation
of all classes, thus boost the convergence speed. Experimental results on image
synthesis demonstrate our model is competitive in the quality of images
generated while achieving a faster convergence rate.


Single-shot Channel Pruning Based on Alternating Direction Method of
  Multipliers

  Channel pruning has been identified as an effective approach to constructing
efficient network structures. Its typical pipeline requires iterative pruning
and fine-tuning. In this work, we propose a novel single-shot channel pruning
approach based on alternating direction methods of multipliers (ADMM), which
can eliminate the need for complex iterative pruning and fine-tuning procedure
and achieve a target compression ratio with only one run of pruning and
fine-tuning. To the best of our knowledge, this is the first study of
single-shot channel pruning. The proposed method introduces filter-level
sparsity during training and can achieve competitive performance with a simple
heuristic pruning criterion (L1-norm). Extensive evaluations have been
conducted with various widely-used benchmark architectures and image datasets
for object classification purpose. The experimental results on classification
accuracy show that the proposed method can outperform state-of-the-art network
pruning works under various scenarios.


Image color transfer to evoke different emotions based on color
  combinations

  In this paper, a color transfer framework to evoke different emotions for
images based on color combinations is proposed. The purpose of this color
transfer is to change the "look and feel" of images, i.e., evoking different
emotions. Colors are confirmed as the most attractive factor in images. In
addition, various studies in both art and science areas have concluded that
other than single color, color combinations are necessary to evoke specific
emotions. Therefore, we propose a novel framework to transfer color of images
based on color combinations, using a predefined color emotion model. The
contribution of this new framework is three-fold. First, users do not need to
provide reference images as used in traditional color transfer algorithms. In
most situations, users may not have enough aesthetic knowledge or path to
choose desired reference images. Second, because of the usage of color
combinations instead of single color for emotions, a new color transfer
algorithm that does not require an image library is proposed. Third, again
because of the usage of color combinations, artifacts that are normally seen in
traditional frameworks using single color are avoided. We present encouraging
results generated from this new framework and its potential in several possible
applications including color transfer of photos and paintings.


Non-Intrusive Energy Disaggregation Using Non-negative Matrix
  Factorization with Sum-to-k Constraint

  Energy disaggregation or Non-Intrusive Load Monitoring (NILM) addresses the
issue of extracting device-level energy consumption information by monitoring
the aggregated signal at one single measurement point without installing meters
on each individual device. Energy disaggregation can be formulated as a source
separation problem where the aggregated signal is expressed as linear
combination of basis vectors in a matrix factorization framework. In this
paper, an approach based on Sum-to-k constrained Non-negative Matrix
Factorization (S2K-NMF) is proposed. By imposing the sum-to-k constraint and
the non-negative constraint, S2K-NMF is able to effectively extract
perceptually meaningful sources from complex mixtures. The strength of the
proposed algorithm is demonstrated through two sets of experiments: Energy
disaggregation in a residential smart home, and HVAC components energy
monitoring in an industrial building testbed maintained at the Oak Ridge
National Laboratory (ORNL). Extensive experimental results demonstrate the
superior performance of S2K-NMF as compared to state-of-the-art
decomposition-based disaggregation algorithms. The source code and our
collected data (HVORUT) for studying NILM for HVAC units can be found at
https://bitbucket.org/aicip/nonintrusive-load-monitoring.


Derivative Delay Embedding: Online Modeling of Streaming Time Series

  The staggering amount of streaming time series coming from the real world
calls for more efficient and effective online modeling solution. For time
series modeling, most existing works make some unrealistic assumptions such as
the input data is of fixed length or well aligned, which requires extra effort
on segmentation or normalization of the raw streaming data. Although some
literature claim their approaches to be invariant to data length and
misalignment, they are too time-consuming to model a streaming time series in
an online manner. We propose a novel and more practical online modeling and
classification scheme, DDE-MGM, which does not make any assumptions on the time
series while maintaining high efficiency and state-of-the-art performance. The
derivative delay embedding (DDE) is developed to incrementally transform time
series to the embedding space, where the intrinsic characteristics of data is
preserved as recursive patterns regardless of the stream length and
misalignment. Then, a non-parametric Markov geographic model (MGM) is proposed
to both model and classify the pattern in an online manner. Experimental
results demonstrate the effectiveness and superior classification accuracy of
the proposed DDE-MGM in an online setting as compared to the state-of-the-art.


Age Progression/Regression by Conditional Adversarial Autoencoder

  "If I provide you a face image of mine (without telling you the actual age
when I took the picture) and a large amount of face images that I crawled
(containing labeled faces of different ages but not necessarily paired), can
you show me what I would look like when I am 80 or what I was like when I was
5?" The answer is probably a "No." Most existing face aging works attempt to
learn the transformation between age groups and thus would require the paired
samples as well as the labeled query image. In this paper, we look at the
problem from a generative modeling perspective such that no paired samples is
required. In addition, given an unlabeled image, the generative model can
directly produce the image with desired age attribute. We propose a conditional
adversarial autoencoder (CAAE) that learns a face manifold, traversing on which
smooth age progression and regression can be realized simultaneously. In CAAE,
the face is first mapped to a latent vector through a convolutional encoder,
and then the vector is projected to the face manifold conditional on age
through a deconvolutional generator. The latent vector preserves personalized
face features (i.e., personality) and the age condition controls progression
vs. regression. Two adversarial networks are imposed on the encoder and
generator, respectively, forcing to generate more photo-realistic faces.
Experimental results demonstrate the appealing performance and flexibility of
the proposed framework by comparing with the state-of-the-art and ground truth.


r-BTN: Cross-domain Face Composite and Synthesis from Limited Facial
  Patches

  We start by asking an interesting yet challenging question, "If an eyewitness
can only recall the eye features of the suspect, such that the forensic artist
can only produce a sketch of the eyes (e.g., the top-left sketch shown in Fig.
1), can advanced computer vision techniques help generate the whole face
image?" A more generalized question is that if a large proportion (e.g., more
than 50%) of the face/sketch is missing, can a realistic whole face
sketch/image still be estimated. Existing face completion and generation
methods either do not conduct domain transfer learning or can not handle large
missing area. For example, the inpainting approach tends to blur the generated
region when the missing area is large (i.e., more than 50%). In this paper, we
exploit the potential of deep learning networks in filling large missing region
(e.g., as high as 95% missing) and generating realistic faces with
high-fidelity in cross domains. We propose the recursive generation by
bidirectional transformation networks (r-BTN) that recursively generates a
whole face/sketch from a small sketch/face patch. The large missing area and
the cross domain challenge make it difficult to generate satisfactory results
using a unidirectional cross-domain learning structure. On the other hand, a
forward and backward bidirectional learning between the face and sketch domains
would enable recursive estimation of the missing region in an incremental
manner (Fig. 1) and yield appealing results. r-BTN also adopts an adversarial
constraint to encourage the generation of realistic faces/sketches. Extensive
experiments have been conducted to demonstrate the superior performance from
r-BTN as compared to existing potential solutions.


Discriminative Cross-View Binary Representation Learning

  Learning compact representation is vital and challenging for large scale
multimedia data. Cross-view/cross-modal hashing for effective binary
representation learning has received significant attention with exponentially
growing availability of multimedia content. Most existing cross-view hashing
algorithms emphasize the similarities in individual views, which are then
connected via cross-view similarities. In this work, we focus on the
exploitation of the discriminative information from different views, and
propose an end-to-end method to learn semantic-preserving and discriminative
binary representation, dubbed Discriminative Cross-View Hashing (DCVH), in
light of learning multitasking binary representation for various tasks
including cross-view retrieval, image-to-image retrieval, and image
annotation/tagging. The proposed DCVH has the following key components. First,
it uses convolutional neural network (CNN) based nonlinear hashing functions
and multilabel classification for both images and texts simultaneously. Such
hashing functions achieve effective continuous relaxation during training
without explicit quantization loss by using Direct Binary Embedding (DBE)
layers. Second, we propose an effective view alignment via Hamming distance
minimization, which is efficiently accomplished by bit-wise XOR operation.
Extensive experiments on two image-text benchmark datasets demonstrate that
DCVH outperforms state-of-the-art cross-view hashing algorithms as well as
single-view image hashing algorithms. In addition, DCVH can provide competitive
performance for image annotation/tagging.


Reference-Conditioned Super-Resolution by Neural Texture Transfer

  With the recent advancement in deep learning, we have witnessed a great
progress in single image super-resolution. However, due to the significant
information loss of the image downscaling process, it has become extremely
challenging to further advance the state-of-the-art, especially for large
upscaling factors. This paper explores a new research direction in super
resolution, called reference-conditioned super-resolution, in which a reference
image containing desired high-resolution texture details is provided besides
the low-resolution image. We focus on transferring the high-resolution texture
from reference images to the super-resolution process without the constraint of
content similarity between reference and target images, which is a key
difference from previous example-based methods. Inspired by recent work on
image stylization, we address the problem via neural texture transfer. We
design an end-to-end trainable deep model which generates detail enriched
results by adaptively fusing the content from the low-resolution image with the
texture patterns from the reference image. We create a benchmark dataset for
the general research of reference-based super-resolution, which contains
reference images paired with low-resolution inputs with varying degrees of
similarity. Both objective and subjective evaluations demonstrate the great
potential of using reference images as well as the superiority of our results
over other state-of-the-art methods.


Talking Face Generation by Conditional Recurrent Adversarial Network

  Given an arbitrary face image and an arbitrary speech clip, the proposed work
attempts to generating the talking face video with accurate lip synchronization
while maintaining smooth transition of both lip and facial movement over the
entire video clip. Existing works either do not consider temporal dependency on
face images across different video frames thus easily yielding
noticeable/abrupt facial and lip movement or are only limited to the generation
of talking face video for a specific person thus lacking generalization
capacity. We propose a novel conditional video generation network where the
audio input is treated as a condition for the recurrent adversarial network
such that temporal dependency is incorporated to realize smooth transition for
the lip and facial movement. In addition, we deploy a multi-task adversarial
training scheme in the context of video generation to improve both
photo-realism and the accuracy for lip synchronization. Finally, based on the
phoneme distribution information extracted from the audio clip, we develop a
sample selection method that effectively reduces the size of the training
dataset without sacrificing the quality of the generated video. Extensive
experiments on both controlled and uncontrolled datasets demonstrate the
superiority of the proposed approach in terms of visual quality, lip sync
accuracy, and smooth transition of lip and facial movement, as compared to the
state-of-the-art.


Unsupervised Sparse Dirichlet-Net for Hyperspectral Image
  Super-Resolution

  In many computer vision applications, obtaining images of high resolution in
both the spatial and spectral domains are equally important. However, due to
hardware limitations, one can only expect to acquire images of high resolution
in either the spatial or spectral domains. This paper focuses on hyperspectral
image super-resolution (HSI-SR), where a hyperspectral image (HSI) with low
spatial resolution (LR) but high spectral resolution is fused with a
multispectral image (MSI) with high spatial resolution (HR) but low spectral
resolution to obtain HR HSI. Existing deep learning-based solutions are all
supervised that would need a large training set and the availability of HR HSI,
which is unrealistic. Here, we make the first attempt to solving the HSI-SR
problem using an unsupervised encoder-decoder architecture that carries the
following uniquenesses. First, it is composed of two encoder-decoder networks,
coupled through a shared decoder, in order to preserve the rich spectral
information from the HSI network. Second, the network encourages the
representations from both modalities to follow a sparse Dirichlet distribution
which naturally incorporates the two physical constraints of HSI and MSI.
Third, the angular difference between representations are minimized in order to
reduce the spectral distortion. We refer to the proposed architecture as
unsupervised Sparse Dirichlet-Net, or uSDN. Extensive experimental results
demonstrate the superior performance of uSDN as compared to the
state-of-the-art.


Attention-based Few-Shot Person Re-identification Using Meta Learning

  In this paper, we investigate the challenging task of person
re-identification from a new perspective and propose an end-to-end
attention-based architecture for few-shot re-identification through
meta-learning. The motivation for this task lies in the fact that humans, can
usually identify another person after just seeing that given person a few times
(or even once) by attending to their memory. On the other hand, the unique
nature of the person re-identification problem, i.e., only few examples exist
per identity and new identities always appearing during testing, calls for a
few shot learning architecture with the capacity of handling new identities.
Hence, we frame the problem within a meta-learning setting, where a neural
network based meta-learner is trained to optimize a learner i.e., an
attention-based matching function. Another challenge of the person
re-identification problem is the small inter-class difference between different
identities and large intra-class difference of the same identity. In order to
increase the discriminative power of the model, we propose a new
attention-based feature encoding scheme that takes into account the critical
intra-view and cross-view relationship of images. We refer to the proposed
Attention-based Re-identification Metalearning model as ARM. Extensive
evaluations demonstrate the advantages of the ARM as compared to the
state-of-the-art on the challenging PRID2011, CUHK01, CUHK03 and Market1501
datasets.


Beyond Inferring Class Representatives: User-Level Privacy Leakage From
  Federated Learning

  Federated learning, i.e., a mobile edge computing framework for deep
learning, is a recent advance in privacy-preserving machine learning, where the
model is trained in a decentralized manner by the clients, i.e., data curators,
preventing the server from directly accessing those private data from the
clients. This learning mechanism significantly challenges the attack from the
server side. Although the state-of-the-art attacking techniques that
incorporated the advance of Generative adversarial networks (GANs) could
construct class representatives of the global data distribution among all
clients, it is still challenging to distinguishably attack a specific client
(i.e., user-level privacy leakage), which is a stronger privacy threat to
precisely recover the private data from a specific client. This paper gives the
first attempt to explore user-level privacy leakage against the federated
learning by the attack from a malicious server. We propose a framework
incorporating GAN with a multi-task discriminator, which simultaneously
discriminates category, reality, and client identity of input samples. The
novel discrimination on client identity enables the generator to recover user
specified private data. Unlike existing works that tend to interfere the
training process of the federated learning, the proposed method works
"invisibly" on the server side. The experimental results demonstrate the
effectiveness of the proposed attacking approach and the superior to the
state-of-the-art.


Speeding up convolutional networks pruning with coarse ranking

  Channel-based pruning has achieved significant successes in accelerating deep
convolutional neural network, whose pipeline is an iterative three-step
procedure: ranking, pruning and fine-tuning. However, this iterative procedure
is computationally expensive. In this study, we present a novel computationally
efficient channel pruning approach based on the coarse ranking that utilizes
the intermediate results during fine-tuning to rank the importance of filters,
built upon state-of-the-art works with data-driven ranking criteria. The goal
of this work is not to propose a single improved approach built upon a specific
channel pruning method, but to introduce a new general framework that works for
a series of channel pruning methods. Various benchmark image datasets
(CIFAR-10, ImageNet, Birds-200, and Flowers-102) and network architectures
(AlexNet and VGG-16) are utilized to evaluate the proposed approach for object
classification purpose. Experimental results show that the proposed method can
achieve almost identical performance with the corresponding state-of-the-art
works (baseline) while our ranking time is negligibly short. In specific, with
the proposed method, 75% and 54% of the total computation time for the whole
pruning procedure can be reduced for AlexNet on CIFAR-10, and for VGG-16 on
ImageNet, respectively. Our approach would significantly facilitate pruning
practice, especially on resource-constrained platforms.


Image Super-Resolution by Neural Texture Transfer

  Due to the significant information loss in low-resolution (LR) images, it has
become extremely challenging to further advance the state-of-the-art of single
image super-resolution (SISR). Reference-based super-resolution (RefSR), on the
other hand, has proven to be promising in recovering high-resolution (HR)
details when a reference (Ref) image with similar content as that of the LR
input is given. However, the quality of RefSR can degrade severely when Ref is
less similar. This paper aims to unleash the potential of RefSR by leveraging
more texture details from Ref images with stronger robustness even when
irrelevant Ref images are provided. Inspired by the recent work on image
stylization, we formulate the RefSR problem as neural texture transfer. We
design an end-to-end deep model which enriches HR details by adaptively
transferring the texture from Ref images according to their textural
similarity. Instead of matching content in the raw pixel space as done by
previous methods, our key contribution is a multi-level matching conducted in
the neural space. This matching scheme facilitates multi-scale neural transfer
that allows the model to benefit more from those semantically related Ref
patches, and gracefully degrade to SISR performance on the least relevant Ref
inputs. We build a benchmark dataset for the general research of RefSR, which
contains Ref images paired with LR inputs with varying levels of similarity.
Both quantitative and qualitative evaluations demonstrate the superiority of
our method over state-of-the-art.


