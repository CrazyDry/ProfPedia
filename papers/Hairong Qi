Feature Encoding in Band-limited Distributed Surveillance Systems

  Distributed surveillance systems have become popular in recent years due tosecurity concerns. However, transmitting high dimensional data inbandwidth-limited distributed systems becomes a major challenge. In this paper,we address this issue by proposing a novel probabilistic algorithm based on thedivergence between the probability distributions of the visual features inorder to reduce their dimensionality and thus save the network bandwidth indistributed wireless smart camera networks. We demonstrate the effectiveness ofthe proposed approach through extensive experiments on two surveillancerecognition tasks.

End-to-end Binary Representation Learning via Direct Binary Embedding

  Learning binary representation is essential to large-scale computer visiontasks. Most existing algorithms require a separate quantization constraint tolearn effective hashing functions. In this work, we present Direct BinaryEmbedding (DBE), a simple yet very effective algorithm to learn binaryrepresentation in an end-to-end fashion. By appending an ingeniously designedDBE layer to the deep convolutional neural network (DCNN), DBE learns binarycode directly from the continuous DBE layer activation without quantizationerror. By employing the deep residual network (ResNet) as DCNN component, DBEcaptures rich semantics from images. Furthermore, in the effort of handlingmultilabel images, we design a joint cross entropy loss that includes bothsoftmax cross entropy and weighted binary cross entropy in consideration of thecorrelation and independence of labels, respectively. Extensive experimentsdemonstrate the significant superiority of DBE over state-of-the-art methods ontasks of natural object recognition, image retrieval and image annotation.

Multi-View Task-Driven Recognition in Visual Sensor Networks

  Nowadays, distributed smart cameras are deployed for a wide set of tasks inseveral application scenarios, ranging from object recognition, imageretrieval, and forensic applications. Due to limited bandwidth in distributedsystems, efficient coding of local visual features has in fact been an activetopic of research. In this paper, we propose a novel approach to obtain acompact representation of high-dimensional visual data using sensor fusiontechniques. We convert the problem of visual analysis in resource-limitedscenarios to a multi-view representation learning, and we show that the key tofinding properly compressed representation is to exploit the position ofcameras with respect to each other as a norm-based regularization in theparticular signal representation of sparse coding. Learning the representationof each camera is viewed as an individual task and a multi-task learning withjoint sparsity for all nodes is employed. The proposed representation learningscheme is referred to as the multi-view task-driven learning for visual sensornetwork (MT-VSN). We demonstrate that MT-VSN outperforms state-of-the-art invarious surveillance recognition tasks.

Addressing Ambiguity in Multi-target Tracking by Hierarchical Strategy

  This paper presents a novel hierarchical approach for the simultaneoustracking of multiple targets in a video. We use a network flow approach to linkdetections in low-level and tracklets in high-level. At each step of thehierarchy, the confidence of candidates is measured by using a new scoringsystem, ConfRank, that considers the quality and the quantity of itsneighborhood. The output of the first stage is a collection of safe trackletsand unlinked high-confidence detections. For each individual detection, wedetermine if it belongs to an existing or is a new tracklet. We show the effectof our framework to recover missed detections and reduce switch identity. Theproposed tracker is referred to as TVOD for multi-target tracking using thevisual tracker and generic object detector. We achieve competitive results withlower identity switches on several datasets comparing to state-of-the-art.

Person Re-identification Using Visual Attention

  Despite recent attempts for solving the person re-identification problem, itremains a challenging task since a person's appearance can vary significantlywhen large variations in view angle, human pose and illumination are involved.The concept of attention is one of the most interesting recent architecturalinnovations in neural networks. Inspired by that, in this paper we propose anovel approach based on using a gradient-based attention mechanism in deepconvolution neural network for solving the person re-identification problem.Our model learns to focus selectively on parts of the input image for which thenetworks' output is most sensitive to. Extensive comparative evaluationsdemonstrate that the proposed method outperforms state-of-the-art approaches,including both traditional and deep neural network-based methods on thechallenging CUHK01, CUHK03, and Market1501 datasets.

Decoupled Learning for Conditional Adversarial Networks

  Incorporating encoding-decoding nets with adversarial nets has been widelyadopted in image generation tasks. We observe that the state-of-the-artachievements were obtained by carefully balancing the reconstruction loss andadversarial loss, and such balance shifts with different network structures,datasets, and training strategies. Empirical studies have demonstrated that aninappropriate weight between the two losses may cause instability, and it istricky to search for the optimal setting, especially when lacking priorknowledge on the data and network.  This paper gives the first attempt to relax the need of manual balancing byproposing the concept of \textit{decoupled learning}, where a novel networkstructure is designed that explicitly disentangles the backpropagation paths ofthe two losses.  Experimental results demonstrate the effectiveness, robustness, andgenerality of the proposed method. The other contribution of the paper is thedesign of a new evaluation metric to measure the image quality of generativemodels. We propose the so-called \textit{normalized relative discriminativescore} (NRDS), which introduces the idea of relative comparison, rather thanproviding absolute estimates like existing metrics.

Fast-converging Conditional Generative Adversarial Networks for Image  Synthesis

  Building on top of the success of generative adversarial networks (GANs),conditional GANs attempt to better direct the data generation process byconditioning with certain additional information. Inspired by the most recentAC-GAN, in this paper we propose a fast-converging conditional GAN (FC-GAN). Inaddition to the real/fake classifier used in vanilla GANs, our discriminatorhas an advanced auxiliary classifier which distinguishes each real class froman extra `fake' class. The `fake' class avoids mixing generated data with realdata, which can potentially confuse the classification of real data as AC-GANdoes, and makes the advanced auxiliary classifier behave as another real/fakeclassifier. As a result, FC-GAN can accelerate the process of differentiationof all classes, thus boost the convergence speed. Experimental results on imagesynthesis demonstrate our model is competitive in the quality of imagesgenerated while achieving a faster convergence rate.

Single-shot Channel Pruning Based on Alternating Direction Method of  Multipliers

  Channel pruning has been identified as an effective approach to constructingefficient network structures. Its typical pipeline requires iterative pruningand fine-tuning. In this work, we propose a novel single-shot channel pruningapproach based on alternating direction methods of multipliers (ADMM), whichcan eliminate the need for complex iterative pruning and fine-tuning procedureand achieve a target compression ratio with only one run of pruning andfine-tuning. To the best of our knowledge, this is the first study ofsingle-shot channel pruning. The proposed method introduces filter-levelsparsity during training and can achieve competitive performance with a simpleheuristic pruning criterion (L1-norm). Extensive evaluations have beenconducted with various widely-used benchmark architectures and image datasetsfor object classification purpose. The experimental results on classificationaccuracy show that the proposed method can outperform state-of-the-art networkpruning works under various scenarios.

Image color transfer to evoke different emotions based on color  combinations

  In this paper, a color transfer framework to evoke different emotions forimages based on color combinations is proposed. The purpose of this colortransfer is to change the "look and feel" of images, i.e., evoking differentemotions. Colors are confirmed as the most attractive factor in images. Inaddition, various studies in both art and science areas have concluded thatother than single color, color combinations are necessary to evoke specificemotions. Therefore, we propose a novel framework to transfer color of imagesbased on color combinations, using a predefined color emotion model. Thecontribution of this new framework is three-fold. First, users do not need toprovide reference images as used in traditional color transfer algorithms. Inmost situations, users may not have enough aesthetic knowledge or path tochoose desired reference images. Second, because of the usage of colorcombinations instead of single color for emotions, a new color transferalgorithm that does not require an image library is proposed. Third, againbecause of the usage of color combinations, artifacts that are normally seen intraditional frameworks using single color are avoided. We present encouragingresults generated from this new framework and its potential in several possibleapplications including color transfer of photos and paintings.

Derivative Delay Embedding: Online Modeling of Streaming Time Series

  The staggering amount of streaming time series coming from the real worldcalls for more efficient and effective online modeling solution. For timeseries modeling, most existing works make some unrealistic assumptions such asthe input data is of fixed length or well aligned, which requires extra efforton segmentation or normalization of the raw streaming data. Although someliterature claim their approaches to be invariant to data length andmisalignment, they are too time-consuming to model a streaming time series inan online manner. We propose a novel and more practical online modeling andclassification scheme, DDE-MGM, which does not make any assumptions on the timeseries while maintaining high efficiency and state-of-the-art performance. Thederivative delay embedding (DDE) is developed to incrementally transform timeseries to the embedding space, where the intrinsic characteristics of data ispreserved as recursive patterns regardless of the stream length andmisalignment. Then, a non-parametric Markov geographic model (MGM) is proposedto both model and classify the pattern in an online manner. Experimentalresults demonstrate the effectiveness and superior classification accuracy ofthe proposed DDE-MGM in an online setting as compared to the state-of-the-art.

Age Progression/Regression by Conditional Adversarial Autoencoder

  "If I provide you a face image of mine (without telling you the actual agewhen I took the picture) and a large amount of face images that I crawled(containing labeled faces of different ages but not necessarily paired), canyou show me what I would look like when I am 80 or what I was like when I was5?" The answer is probably a "No." Most existing face aging works attempt tolearn the transformation between age groups and thus would require the pairedsamples as well as the labeled query image. In this paper, we look at theproblem from a generative modeling perspective such that no paired samples isrequired. In addition, given an unlabeled image, the generative model candirectly produce the image with desired age attribute. We propose a conditionaladversarial autoencoder (CAAE) that learns a face manifold, traversing on whichsmooth age progression and regression can be realized simultaneously. In CAAE,the face is first mapped to a latent vector through a convolutional encoder,and then the vector is projected to the face manifold conditional on agethrough a deconvolutional generator. The latent vector preserves personalizedface features (i.e., personality) and the age condition controls progressionvs. regression. Two adversarial networks are imposed on the encoder andgenerator, respectively, forcing to generate more photo-realistic faces.Experimental results demonstrate the appealing performance and flexibility ofthe proposed framework by comparing with the state-of-the-art and ground truth.

Non-Intrusive Energy Disaggregation Using Non-negative Matrix  Factorization with Sum-to-k Constraint

  Energy disaggregation or Non-Intrusive Load Monitoring (NILM) addresses theissue of extracting device-level energy consumption information by monitoringthe aggregated signal at one single measurement point without installing meterson each individual device. Energy disaggregation can be formulated as a sourceseparation problem where the aggregated signal is expressed as linearcombination of basis vectors in a matrix factorization framework. In thispaper, an approach based on Sum-to-k constrained Non-negative MatrixFactorization (S2K-NMF) is proposed. By imposing the sum-to-k constraint andthe non-negative constraint, S2K-NMF is able to effectively extractperceptually meaningful sources from complex mixtures. The strength of theproposed algorithm is demonstrated through two sets of experiments: Energydisaggregation in a residential smart home, and HVAC components energymonitoring in an industrial building testbed maintained at the Oak RidgeNational Laboratory (ORNL). Extensive experimental results demonstrate thesuperior performance of S2K-NMF as compared to state-of-the-artdecomposition-based disaggregation algorithms. The source code and ourcollected data (HVORUT) for studying NILM for HVAC units can be found athttps://bitbucket.org/aicip/nonintrusive-load-monitoring.

r-BTN: Cross-domain Face Composite and Synthesis from Limited Facial  Patches

  We start by asking an interesting yet challenging question, "If an eyewitnesscan only recall the eye features of the suspect, such that the forensic artistcan only produce a sketch of the eyes (e.g., the top-left sketch shown in Fig.1), can advanced computer vision techniques help generate the whole faceimage?" A more generalized question is that if a large proportion (e.g., morethan 50%) of the face/sketch is missing, can a realistic whole facesketch/image still be estimated. Existing face completion and generationmethods either do not conduct domain transfer learning or can not handle largemissing area. For example, the inpainting approach tends to blur the generatedregion when the missing area is large (i.e., more than 50%). In this paper, weexploit the potential of deep learning networks in filling large missing region(e.g., as high as 95% missing) and generating realistic faces withhigh-fidelity in cross domains. We propose the recursive generation bybidirectional transformation networks (r-BTN) that recursively generates awhole face/sketch from a small sketch/face patch. The large missing area andthe cross domain challenge make it difficult to generate satisfactory resultsusing a unidirectional cross-domain learning structure. On the other hand, aforward and backward bidirectional learning between the face and sketch domainswould enable recursive estimation of the missing region in an incrementalmanner (Fig. 1) and yield appealing results. r-BTN also adopts an adversarialconstraint to encourage the generation of realistic faces/sketches. Extensiveexperiments have been conducted to demonstrate the superior performance fromr-BTN as compared to existing potential solutions.

Discriminative Cross-View Binary Representation Learning

  Learning compact representation is vital and challenging for large scalemultimedia data. Cross-view/cross-modal hashing for effective binaryrepresentation learning has received significant attention with exponentiallygrowing availability of multimedia content. Most existing cross-view hashingalgorithms emphasize the similarities in individual views, which are thenconnected via cross-view similarities. In this work, we focus on theexploitation of the discriminative information from different views, andpropose an end-to-end method to learn semantic-preserving and discriminativebinary representation, dubbed Discriminative Cross-View Hashing (DCVH), inlight of learning multitasking binary representation for various tasksincluding cross-view retrieval, image-to-image retrieval, and imageannotation/tagging. The proposed DCVH has the following key components. First,it uses convolutional neural network (CNN) based nonlinear hashing functionsand multilabel classification for both images and texts simultaneously. Suchhashing functions achieve effective continuous relaxation during trainingwithout explicit quantization loss by using Direct Binary Embedding (DBE)layers. Second, we propose an effective view alignment via Hamming distanceminimization, which is efficiently accomplished by bit-wise XOR operation.Extensive experiments on two image-text benchmark datasets demonstrate thatDCVH outperforms state-of-the-art cross-view hashing algorithms as well assingle-view image hashing algorithms. In addition, DCVH can provide competitiveperformance for image annotation/tagging.

Reference-Conditioned Super-Resolution by Neural Texture Transfer

  With the recent advancement in deep learning, we have witnessed a greatprogress in single image super-resolution. However, due to the significantinformation loss of the image downscaling process, it has become extremelychallenging to further advance the state-of-the-art, especially for largeupscaling factors. This paper explores a new research direction in superresolution, called reference-conditioned super-resolution, in which a referenceimage containing desired high-resolution texture details is provided besidesthe low-resolution image. We focus on transferring the high-resolution texturefrom reference images to the super-resolution process without the constraint ofcontent similarity between reference and target images, which is a keydifference from previous example-based methods. Inspired by recent work onimage stylization, we address the problem via neural texture transfer. Wedesign an end-to-end trainable deep model which generates detail enrichedresults by adaptively fusing the content from the low-resolution image with thetexture patterns from the reference image. We create a benchmark dataset forthe general research of reference-based super-resolution, which containsreference images paired with low-resolution inputs with varying degrees ofsimilarity. Both objective and subjective evaluations demonstrate the greatpotential of using reference images as well as the superiority of our resultsover other state-of-the-art methods.

Talking Face Generation by Conditional Recurrent Adversarial Network

  Given an arbitrary face image and an arbitrary speech clip, the proposed workattempts to generating the talking face video with accurate lip synchronizationwhile maintaining smooth transition of both lip and facial movement over theentire video clip. Existing works either do not consider temporal dependency onface images across different video frames thus easily yieldingnoticeable/abrupt facial and lip movement or are only limited to the generationof talking face video for a specific person thus lacking generalizationcapacity. We propose a novel conditional video generation network where theaudio input is treated as a condition for the recurrent adversarial networksuch that temporal dependency is incorporated to realize smooth transition forthe lip and facial movement. In addition, we deploy a multi-task adversarialtraining scheme in the context of video generation to improve bothphoto-realism and the accuracy for lip synchronization. Finally, based on thephoneme distribution information extracted from the audio clip, we develop asample selection method that effectively reduces the size of the trainingdataset without sacrificing the quality of the generated video. Extensiveexperiments on both controlled and uncontrolled datasets demonstrate thesuperiority of the proposed approach in terms of visual quality, lip syncaccuracy, and smooth transition of lip and facial movement, as compared to thestate-of-the-art.

Unsupervised Sparse Dirichlet-Net for Hyperspectral Image  Super-Resolution

  In many computer vision applications, obtaining images of high resolution inboth the spatial and spectral domains are equally important. However, due tohardware limitations, one can only expect to acquire images of high resolutionin either the spatial or spectral domains. This paper focuses on hyperspectralimage super-resolution (HSI-SR), where a hyperspectral image (HSI) with lowspatial resolution (LR) but high spectral resolution is fused with amultispectral image (MSI) with high spatial resolution (HR) but low spectralresolution to obtain HR HSI. Existing deep learning-based solutions are allsupervised that would need a large training set and the availability of HR HSI,which is unrealistic. Here, we make the first attempt to solving the HSI-SRproblem using an unsupervised encoder-decoder architecture that carries thefollowing uniquenesses. First, it is composed of two encoder-decoder networks,coupled through a shared decoder, in order to preserve the rich spectralinformation from the HSI network. Second, the network encourages therepresentations from both modalities to follow a sparse Dirichlet distributionwhich naturally incorporates the two physical constraints of HSI and MSI.Third, the angular difference between representations are minimized in order toreduce the spectral distortion. We refer to the proposed architecture asunsupervised Sparse Dirichlet-Net, or uSDN. Extensive experimental resultsdemonstrate the superior performance of uSDN as compared to thestate-of-the-art.

Attention-based Few-Shot Person Re-identification Using Meta Learning

  In this paper, we investigate the challenging task of personre-identification from a new perspective and propose an end-to-endattention-based architecture for few-shot re-identification throughmeta-learning. The motivation for this task lies in the fact that humans, canusually identify another person after just seeing that given person a few times(or even once) by attending to their memory. On the other hand, the uniquenature of the person re-identification problem, i.e., only few examples existper identity and new identities always appearing during testing, calls for afew shot learning architecture with the capacity of handling new identities.Hence, we frame the problem within a meta-learning setting, where a neuralnetwork based meta-learner is trained to optimize a learner i.e., anattention-based matching function. Another challenge of the personre-identification problem is the small inter-class difference between differentidentities and large intra-class difference of the same identity. In order toincrease the discriminative power of the model, we propose a newattention-based feature encoding scheme that takes into account the criticalintra-view and cross-view relationship of images. We refer to the proposedAttention-based Re-identification Metalearning model as ARM. Extensiveevaluations demonstrate the advantages of the ARM as compared to thestate-of-the-art on the challenging PRID2011, CUHK01, CUHK03 and Market1501datasets.

Beyond Inferring Class Representatives: User-Level Privacy Leakage From  Federated Learning

  Federated learning, i.e., a mobile edge computing framework for deeplearning, is a recent advance in privacy-preserving machine learning, where themodel is trained in a decentralized manner by the clients, i.e., data curators,preventing the server from directly accessing those private data from theclients. This learning mechanism significantly challenges the attack from theserver side. Although the state-of-the-art attacking techniques thatincorporated the advance of Generative adversarial networks (GANs) couldconstruct class representatives of the global data distribution among allclients, it is still challenging to distinguishably attack a specific client(i.e., user-level privacy leakage), which is a stronger privacy threat toprecisely recover the private data from a specific client. This paper gives thefirst attempt to explore user-level privacy leakage against the federatedlearning by the attack from a malicious server. We propose a frameworkincorporating GAN with a multi-task discriminator, which simultaneouslydiscriminates category, reality, and client identity of input samples. Thenovel discrimination on client identity enables the generator to recover userspecified private data. Unlike existing works that tend to interfere thetraining process of the federated learning, the proposed method works"invisibly" on the server side. The experimental results demonstrate theeffectiveness of the proposed attacking approach and the superior to thestate-of-the-art.

Speeding up convolutional networks pruning with coarse ranking

  Channel-based pruning has achieved significant successes in accelerating deepconvolutional neural network, whose pipeline is an iterative three-stepprocedure: ranking, pruning and fine-tuning. However, this iterative procedureis computationally expensive. In this study, we present a novel computationallyefficient channel pruning approach based on the coarse ranking that utilizesthe intermediate results during fine-tuning to rank the importance of filters,built upon state-of-the-art works with data-driven ranking criteria. The goalof this work is not to propose a single improved approach built upon a specificchannel pruning method, but to introduce a new general framework that works fora series of channel pruning methods. Various benchmark image datasets(CIFAR-10, ImageNet, Birds-200, and Flowers-102) and network architectures(AlexNet and VGG-16) are utilized to evaluate the proposed approach for objectclassification purpose. Experimental results show that the proposed method canachieve almost identical performance with the corresponding state-of-the-artworks (baseline) while our ranking time is negligibly short. In specific, withthe proposed method, 75% and 54% of the total computation time for the wholepruning procedure can be reduced for AlexNet on CIFAR-10, and for VGG-16 onImageNet, respectively. Our approach would significantly facilitate pruningpractice, especially on resource-constrained platforms.

Image Super-Resolution by Neural Texture Transfer

  Due to the significant information loss in low-resolution (LR) images, it hasbecome extremely challenging to further advance the state-of-the-art of singleimage super-resolution (SISR). Reference-based super-resolution (RefSR), on theother hand, has proven to be promising in recovering high-resolution (HR)details when a reference (Ref) image with similar content as that of the LRinput is given. However, the quality of RefSR can degrade severely when Ref isless similar. This paper aims to unleash the potential of RefSR by leveragingmore texture details from Ref images with stronger robustness even whenirrelevant Ref images are provided. Inspired by the recent work on imagestylization, we formulate the RefSR problem as neural texture transfer. Wedesign an end-to-end deep model which enriches HR details by adaptivelytransferring the texture from Ref images according to their texturalsimilarity. Instead of matching content in the raw pixel space as done byprevious methods, our key contribution is a multi-level matching conducted inthe neural space. This matching scheme facilitates multi-scale neural transferthat allows the model to benefit more from those semantically related Refpatches, and gracefully degrade to SISR performance on the least relevant Refinputs. We build a benchmark dataset for the general research of RefSR, whichcontains Ref images paired with LR inputs with varying levels of similarity.Both quantitative and qualitative evaluations demonstrate the superiority ofour method over state-of-the-art.

