OpenRoACH: A Durable Open-Source Hexapedal Platform with Onboard Robot  Operating System (ROS)

  OpenRoACH is a 15-cm 200-gram self-contained hexapedal robot with an onboardsingle-board computer. To our knowledge, it is the smallest legged robot withthe capability of running the Robot Operating System (ROS) onboard. The robotis fully open sourced, uses accessible materials and off-the-shelf electroniccomponents, can be fabricated with benchtop fast-prototyping machines such as alaser cutter and a 3D printer, and can be assembled by one person within twohours. Its sensory capacity has been tested with gyroscopes, accelerometers,Beacon sensors, color vision sensors, linescan sensors and cameras. It islow-cost within \$150 including structure materials, motors, electronics, and abattery. The capabilities of OpenRoACH are demonstrated with multi-surfacewalking and running, 24-hour continuous walking burn-ins, carrying 200-gramdynamic payloads and 800-gram static payloads, and ROS control of steeringbased on camera feedback. Information and files related to mechanical design,fabrication, assembly, electronics, and control algorithms are all publiclyavailable on https://wiki.eecs.berkeley.edu/biomimetics/Main/OpenRoACH.

Neural Network Dynamics for Model-Based Deep Reinforcement Learning with  Model-Free Fine-Tuning

  Model-free deep reinforcement learning algorithms have been shown to becapable of learning a wide range of robotic skills, but typically require avery large number of samples to achieve good performance. Model-basedalgorithms, in principle, can provide for much more efficient learning, buthave proven difficult to extend to expressive, high-capacity models such asdeep neural networks. In this work, we demonstrate that medium-sized neuralnetwork models can in fact be combined with model predictive control (MPC) toachieve excellent sample complexity in a model-based reinforcement learningalgorithm, producing stable and plausible gaits to accomplish various complexlocomotion tasks. We also propose using deep neural network dynamics models toinitialize a model-free learner, in order to combine the sample efficiency ofmodel-based approaches with the high task-specific performance of model-freemethods. We empirically demonstrate on MuJoCo locomotion tasks that our puremodel-based approach trained on just random action data can follow arbitrarytrajectories with excellent sample efficiency, and that our hybrid algorithmcan accelerate model-free learning on high-speed benchmark tasks, achievingsample efficiency gains of 3-5x on swimmer, cheetah, hopper, and ant agents.Videos can be found at https://sites.google.com/view/mbmf

Learning Image-Conditioned Dynamics Models for Control of Under-actuated  Legged Millirobots

  Millirobots are a promising robotic platform for many applications due totheir small size and low manufacturing costs. Legged millirobots, inparticular, can provide increased mobility in complex environments and improvedscaling of obstacles. However, controlling these small, highly dynamic, andunderactuated legged systems is difficult. Hand-engineered controllers cansometimes control these legged millirobots, but they have difficulties withdynamic maneuvers and complex terrains. We present an approach for controllinga real-world legged millirobot that is based on learned neural network models.Using less than 17 minutes of data, our method can learn a predictive model ofthe robot's dynamics that can enable effective gaits to be synthesized on thefly for following user-specified waypoints on a given terrain. Furthermore, byleveraging expressive, high-capacity neural network models, our approach allowsfor these predictions to be directly conditioned on camera images, endowing therobot with the ability to predict how different terrains might affect itsdynamics. This enables sample-efficient and effective learning for locomotionof a dynamic legged millirobot on various terrains, including gravel, turf,carpet, and styrofoam. Experiment videos can be found athttps://sites.google.com/view/imageconddyn

Learning to Adapt in Dynamic, Real-World Environments Through  Meta-Reinforcement Learning

  Although reinforcement learning methods can achieve impressive results insimulation, the real world presents two major challenges: generating samples isexceedingly expensive, and unexpected perturbations or unseen situations causeproficient but specialized policies to fail at test time. Given that it isimpractical to train separate policies to accommodate all situations the agentmay see in the real world, this work proposes to learn how to quickly andeffectively adapt online to new tasks. To enable sample-efficient learning, weconsider learning online adaptation in the context of model-based reinforcementlearning. Our approach uses meta-learning to train a dynamics model prior suchthat, when combined with recent data, this prior can be rapidly adapted to thelocal context. Our experiments demonstrate online adaptation for continuouscontrol tasks on both simulated and real-world agents. We first show simulatedagents adapting their behavior online to novel terrains, crippled body parts,and highly-dynamic environments. We also illustrate the importance ofincorporating online adaptation into autonomous agents that operate in the realworld by applying our method to a real dynamic legged millirobot. Wedemonstrate the agent's learned ability to quickly adapt online to a missingleg, adjust to novel terrains and slopes, account for miscalibration or errorsin pose estimation, and compensate for pulling payloads.

