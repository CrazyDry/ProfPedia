Proceedings of NIPS 2016 Workshop on Interpretable Machine Learning for  Complex Systems

  This is the Proceedings of NIPS 2016 Workshop on Interpretable MachineLearning for Complex Systems, held in Barcelona, Spain on December 9, 2016

Proceedings of NIPS 2017 Symposium on Interpretable Machine Learning

  This is the Proceedings of NIPS 2017 Symposium on Interpretable MachineLearning, held in Long Beach, California, USA on December 7, 2017

Thoughts on Massively Scalable Gaussian Processes

  We introduce a framework and early results for massively scalable Gaussianprocesses (MSGP), significantly extending the KISS-GP approach of Wilson andNickisch (2015). The MSGP framework enables the use of Gaussian processes (GPs)on billions of datapoints, without requiring distributed inference, or severeassumptions. In particular, MSGP reduces the standard $O(n^3)$ complexity of GPlearning and inference to $O(n)$, and the standard $O(n^2)$ complexity per testpoint prediction to $O(1)$. MSGP involves 1) decomposing covariance matrices asKronecker products of Toeplitz matrices approximated by circulant matrices.This multi-level circulant approximation allows one to unify the orthogonalcomputational benefits of fast Kronecker and Toeplitz approaches, and issignificantly faster than either approach in isolation; 2) local kernelinterpolation and inducing points to allow for arbitrarily located data inputs,and $O(1)$ test time predictions; 3) exploiting block-Toeplitz Toeplitz-blockstructure (BTTB), which enables fast inference and learning whenmultidimensional Kronecker structure is not present; and 4) projections of theinput space to flexibly model correlated inputs and high dimensional data. Theability to handle many ($m \approx n$) inducing points allows for near-exactaccuracy and large scale kernel learning.

Bayesian Inference for NMR Spectroscopy with Applications to Chemical  Quantification

  Nuclear magnetic resonance (NMR) spectroscopy exploits the magneticproperties of atomic nuclei to discover the structure, reaction state andchemical environment of molecules. We propose a probabilistic generative modeland inference procedures for NMR spectroscopy. Specifically, we use a weightedsum of trigonometric functions undergoing exponential decay to model freeinduction decay (FID) signals. We discuss the challenges in estimating thecomponents of this general model -- amplitudes, phase shifts, frequencies,decay rates, and noise variances -- and offer practical solutions. We comparewith conventional Fourier transform spectroscopy for estimating the relativeconcentrations of chemicals in a mixture, using synthetic and experimentallyacquired FID signals. We find the proposed model is particularly robust to lowsignal to noise ratios (SNR), and overlapping peaks in the Fourier transform ofthe FID, enabling accurate predictions (e.g., 1% sensitivity at low SNR) whichare not possible with conventional spectroscopy (5% sensitivity).

Rotating Wilson loops and open strings in AdS3

  The AdS/CFT correspondence relates N=4 super Yang-Mills on S3 to type IIBstring theory on AdS5xS5. In this context, a quark/anti-quark pair moving on anS1 inside S3 following prescribed trajectories is dual to an open string endingon the boundary of AdS3. In this paper we study the corresponding classicalstring solutions. The Pohlmeyer reduction reduces the equations of motion to ageneralized sinh-Gordon equation. This equation includes, as particular cases,the Liouville equation as well as the sinh/cosh-Gordon equations. We studygeneric solutions of the Liouville equation and finite gap solutions of thesinh/cosh-Gordon ones. The latter ones are written in terms of Riemann thetafunctions. The corresponding string solutions are reconstructed giving newsolutions and a broad understanding of open strings moving in AdS3. As afurther example, the simple case of a rigidly rotating string is shown toexhibit these three behaviors depending on a simple relation between itsangular velocity and the angular separation between its endpoints.

Copula Processes

  We define a copula process which describes the dependencies betweenarbitrarily many random variables independently of their marginaldistributions. As an example, we develop a stochastic volatility model,Gaussian Copula Process Volatility (GCPV), to predict the latent standarddeviations of a sequence of random variables. To make predictions we useBayesian inference, with the Laplace approximation, and with Markov chain MonteCarlo as an alternative. We find both methods comparable. We also find ourmodel can outperform GARCH on simulated and financial data. And unlike GARCH,GCPV can easily handle missing data, incorporate covariates other than time,and model a rich class of covariance structures.

Generalised Wishart Processes

  We introduce a stochastic process with Wishart marginals: the generalisedWishart process (GWP). It is a collection of positive semi-definite randommatrices indexed by any arbitrary dependent variable. We use it to modeldynamic (e.g. time varying) covariance matrices. Unlike existing models, it cancapture a diverse class of covariance structures, it can easily handle missingdata, the dependent variable can readily include covariates other than time,and it scales well with dimension; there is no need for free parameters, andoptional parameters are easy to interpret. We describe how to construct theGWP, introduce general procedures for inference and predictions, and show thatit outperforms its main competitor, multivariate GARCH, even on financial datathat especially suits GARCH. We also show how to predict the mean of amultivariate process while accounting for dynamic correlations.

Gaussian Process Kernels for Pattern Discovery and Extrapolation

  Gaussian processes are rich distributions over functions, which provide aBayesian nonparametric approach to smoothing and interpolation. We introducesimple closed form kernels that can be used with Gaussian processes to discoverpatterns and enable extrapolation. These kernels are derived by modelling aspectral density -- the Fourier transform of a kernel -- with a Gaussianmixture. The proposed kernels support a broad class of stationary covariances,but Gaussian process inference remains simple and analytic. We demonstrate theproposed kernels by discovering patterns and performing long rangeextrapolation on synthetic examples, as well as atmospheric CO2 trends andairline passenger data. We also show that we can reconstruct standardcovariances within our framework.

Gaussian Process Regression Networks

  We introduce a new regression framework, Gaussian process regression networks(GPRN), which combines the structural properties of Bayesian neural networkswith the non-parametric flexibility of Gaussian processes. This modelaccommodates input dependent signal and noise correlations between multipleresponse variables, input dependent length-scales and amplitudes, andheavy-tailed predictive distributions. We derive both efficient Markov chainMonte Carlo and variational Bayes inference procedures for this model. We applyGPRN as a multiple output regression and multivariate volatility model,demonstrating substantially improved performance over eight popular multipleoutput (multi-task) Gaussian process models and three multivariate volatilitymodels on benchmark datasets, including a 1000 dimensional gene expressiondataset.

The Human Kernel

  Bayesian nonparametric models, such as Gaussian processes, provide acompelling framework for automatic statistical modelling: these models have ahigh degree of flexibility, and automatically calibrated complexity. However,automating human expertise remains elusive; for example, Gaussian processeswith standard kernels struggle on function extrapolation problems that aretrivial for human learners. In this paper, we create function extrapolationproblems and acquire human responses, and then design a kernel learningframework to reverse engineer the inductive biases of human learners across aset of behavioral experiments. We use the learned kernels to gain psychologicalinsights and to extrapolate in human-like ways that go beyond traditionalstationary and polynomial kernels. Finally, we investigate Occam's razor inhuman and Gaussian process based function learning.

GPatt: Fast Multidimensional Pattern Extrapolation with Gaussian  Processes

  Gaussian processes are typically used for smoothing and interpolation onsmall datasets. We introduce a new Bayesian nonparametric framework -- GPatt --enabling automatic pattern extrapolation with Gaussian processes on largemultidimensional datasets. GPatt unifies and extends highly expressive kernelsand fast exact inference techniques. Without human intervention -- no handcrafting of kernel features, and no sophisticated initialisation procedures --we show that GPatt can solve large scale pattern extrapolation, inpainting, andkernel discovery problems, including a problem with 383400 training points. Wefind that GPatt significantly outperforms popular alternative scalable Gaussianprocess methods in speed and accuracy. Moreover, we discover profounddifferences between each of these methods, suggesting expressive kernels,nonparametric representations, and exact inference are useful for modellinglarge scale multidimensional patterns.

Kernel Interpolation for Scalable Structured Gaussian Processes  (KISS-GP)

  We introduce a new structured kernel interpolation (SKI) framework, whichgeneralises and unifies inducing point methods for scalable Gaussian processes(GPs). SKI methods produce kernel approximations for fast computations throughkernel interpolation. The SKI framework clarifies how the quality of aninducing point approach depends on the number of inducing (aka interpolation)points, interpolation strategy, and GP covariance kernel. SKI also provides amechanism to create new scalable kernel methods, through choosing differentkernel interpolation strategies. Using SKI, with local cubic kernelinterpolation, we introduce KISS-GP, which is 1) more scalable than inducingpoint alternatives, 2) naturally enables Kronecker and Toeplitz algebra forsubstantial additional gains in scalability, without requiring any grid data,and 3) can be used for fast and expressive kernel learning. KISS-GP costs O(n)time and storage for GP inference. We evaluate KISS-GP for kernel matrixapproximation, kernel learning, and natural sound modelling.

Deep Kernel Learning

  We introduce scalable deep kernels, which combine the structural propertiesof deep learning architectures with the non-parametric flexibility of kernelmethods. Specifically, we transform the inputs of a spectral mixture basekernel with a deep architecture, using local kernel interpolation, inducingpoints, and structure exploiting (Kronecker and Toeplitz) algebra for ascalable kernel representation. These closed-form kernels can be used asdrop-in replacements for standard kernels, with benefits in expressive powerand scalability. We jointly learn the properties of these kernels through themarginal likelihood of a Gaussian process. Inference and learning cost $O(n)$for $n$ training points, and predictions cost $O(1)$ per test point. On a largeand diverse collection of applications, including a dataset with 2 millionexamples, we show improved performance over scalable Gaussian processes withflexible kernel learning models, and stand-alone deep architectures.

Stochastic Variational Deep Kernel Learning

  Deep kernel learning combines the non-parametric flexibility of kernelmethods with the inductive biases of deep learning architectures. We propose anovel deep kernel learning model and stochastic variational inference procedurewhich generalizes deep kernel learning approaches to enable classification,multi-task learning, additive covariance structures, and stochastic gradienttraining. Specifically, we apply additive base kernels to subsets of outputfeatures from deep neural architectures, and jointly learn the parameters ofthe base kernels and deep network through a Gaussian process marginallikelihood objective. Within this framework, we derive an efficient form ofstochastic variational inference which leverages local kernel interpolation,inducing points, and structure exploiting algebra. We show improved performanceover stand alone deep networks, SVMs, and state of the art scalable Gaussianprocesses on several classification benchmarks, including an airline delaydataset containing 6 million training points, CIFAR, and ImageNet.

Bayesian GAN

  Generative adversarial networks (GANs) can implicitly learn richdistributions over images, audio, and data which are hard to model with anexplicit likelihood. We present a practical Bayesian formulation forunsupervised and semi-supervised learning with GANs. Within this framework, weuse stochastic gradient Hamiltonian Monte Carlo to marginalize the weights ofthe generator and discriminator networks. The resulting approach isstraightforward and obtains good performance without any standard interventionssuch as feature matching, or mini-batch discrimination. By exploring anexpressive posterior over the parameters of the generator, the Bayesian GANavoids mode-collapse, produces interpretable and diverse candidate samples, andprovides state-of-the-art quantitative results for semi-supervised learning onbenchmarks including SVHN, CelebA, and CIFAR-10, outperforming DCGAN,Wasserstein GANs, and DCGAN ensembles.

Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs

  The loss functions of deep neural networks are complex and their geometricproperties are not well understood. We show that the optima of these complexloss functions are in fact connected by simple curves over which training andtest accuracy are nearly constant. We introduce a training procedure todiscover these high-accuracy pathways between modes. Inspired by this newgeometric insight, we also propose a new ensembling method entitled FastGeometric Ensembling (FGE). Using FGE we can train high-performing ensembles inthe time required to train a single model. We achieve improved performancecompared to the recent state-of-the-art Snapshot Ensembles, on CIFAR-10,CIFAR-100, and ImageNet.

Averaging Weights Leads to Wider Optima and Better Generalization

  Deep neural networks are typically trained by optimizing a loss function withan SGD variant, in conjunction with a decaying learning rate, untilconvergence. We show that simple averaging of multiple points along thetrajectory of SGD, with a cyclical or constant learning rate, leads to bettergeneralization than conventional training. We also show that this StochasticWeight Averaging (SWA) procedure finds much flatter solutions than SGD, andapproximates the recent Fast Geometric Ensembling (FGE) approach with a singlemodel. Using SWA we achieve notable improvement in test accuracy overconventional SGD training on a range of state-of-the-art residual networks,PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, andImageNet. In short, SWA is extremely easy to implement, improvesgeneralization, and has almost no computational overhead.

Change Surfaces for Expressive Multidimensional Changepoints and  Counterfactual Prediction

  Identifying changes in model parameters is fundamental in machine learningand statistics. However, standard changepoint models are limited inexpressiveness, often addressing unidimensional problems and assuminginstantaneous changes. We introduce change surfaces as a multidimensional andhighly expressive generalization of changepoints. We provide a model-agnosticformalization of change surfaces, illustrating how they can provide variable,heterogeneous, and non-monotonic rates of change across multiple dimensions.Additionally, we show how change surfaces can be used for counterfactualprediction. As a concrete instantiation of the change surface framework, wedevelop Gaussian Process Change Surfaces (GPCS). We demonstrate counterfactualprediction with Bayesian posterior mean and credible sets, as well as massivescalability by introducing novel methods for additive non-separable kernels.Using two large spatio-temporal datasets we employ GPCS to discover andcharacterize complex changes that can provide scientific and policy relevantinsights. Specifically, we analyze twentieth century measles incidence acrossthe United States and discover previously unknown heterogeneous changes afterthe introduction of the measles vaccine. Additionally, we apply the model torequests for lead testing kits in New York City, discovering distinct spatialand demographic patterns.

A Simple Baseline for Bayesian Uncertainty in Deep Learning

  We propose SWA-Gaussian (SWAG), a simple, scalable, and general purposeapproach for uncertainty representation and calibration in deep learning.Stochastic Weight Averaging (SWA), which computes the first moment ofstochastic gradient descent (SGD) iterates with a modified learning rateschedule, has recently been shown to improve generalization in deep learning.With SWAG, we fit a Gaussian using the SWA solution as the first moment and alow rank plus diagonal covariance also derived from the SGD iterates, formingan approximate posterior distribution over neural network weights; we thensample from this Gaussian distribution to perform Bayesian model averaging. Weempirically find that SWAG approximates the shape of the true posterior, inaccordance with results describing the stationary distribution of SGD iterates.Moreover, we demonstrate that SWAG performs well on a wide variety of computervision tasks, including out of sample detection, calibration, and transferlearning, in comparison to many popular alternatives including MC dropout, KFACLaplace, and temperature scaling.

Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning

  The posteriors over neural network weights are high dimensional andmultimodal. Each mode typically characterizes a meaningfully differentrepresentation of the data. We develop Cyclical Stochastic Gradient MCMC(SG-MCMC) to automatically explore such distributions. In particular, wepropose a cyclical stepsize schedule, where larger steps discover new modes,and smaller steps characterize each mode. We prove that our proposed learningrate schedule provides faster convergence to samples from a stationarydistribution than SG-MCMC with standard decaying schedules. Moreover, weprovide extensive experimental results to demonstrate the effectiveness ofcyclical SG-MCMC in learning complex multimodal distributions, especially forfully Bayesian inference with modern deep neural networks.

A la Carte - Learning Fast Kernels

  Kernel methods have great promise for learning rich statisticalrepresentations of large modern datasets. However, compared to neural networks,kernel methods have been perceived as lacking in scalability and flexibility.We introduce a family of fast, flexible, lightly parametrized and generalpurpose kernel learning methods, derived from Fastfood basis functionexpansions. We provide mechanisms to learn the properties of groups of spectralfrequencies in these expansions, which require only O(mlogd) time and O(m)memory, for m basis functions and d input dimensions. We show that the proposedmethods can learn a wide class of kernels, outperforming the alternatives inaccuracy, speed, and memory consumption.

Multimodal Word Distributions

  Word embeddings provide point representations of words containing usefulsemantic information. We introduce multimodal word distributions formed fromGaussian mixtures, for multiple word meanings, entailment, and rich uncertaintyinformation. To learn these distributions, we propose an energy-basedmax-margin objective. We show that the resulting approach captures uniquelyexpressive semantic information, and outperforms alternatives, such as word2vecskip-grams, and Gaussian embeddings, on benchmark datasets such as wordsimilarity and entailment.

Product Kernel Interpolation for Scalable Gaussian Processes

  Recent work shows that inference for Gaussian processes can be performedefficiently using iterative methods that rely only on matrix-vectormultiplications (MVMs). Structured Kernel Interpolation (SKI) exploits thesetechniques by deriving approximate kernels with very fast MVMs. Unfortunately,such strategies suffer badly from the curse of dimensionality. We develop a newtechnique for MVM based learning that exploits product kernel structure. Wedemonstrate that this technique is broadly applicable, resulting in linearrather than exponential runtime with dimension for SKI, as well asstate-of-the-art asymptotic complexity for multi-task GPs.

Quasi-particles in the principal picture of  $\widehat{\mathfrak{sl}}_{2}$ and Rogers-Ramanujan-type identities

  In their seminal work J. Lepowsky and R. L. Wilson gave a vertex-operatortheoretic interpretation of Gordon-Andrews-Bressoud's generalization ofRogers-Ramanujan combinatorial identities, by constructing bases of vacuumspaces for the principal Heisenberg subalgebra of standard$\widehat{\mathfrak{sl}}_{2}$-modules, parametrized with partitions satisfyingcertain difference 2 conditions. In this paper we define quasi-particles in theprincipal picture of $\widehat{\mathfrak{sl}}_{2}$ and construct quasi-particlemonomial bases of standard $\widehat{\mathfrak{sl}}_{2}$-modules for whichprincipally specialized characters are given as products of sum sides of thecorresponding analytic Rogers-Ramanujan-type identities with the character ofthe Fock space for the principal Heisenberg subalgebra.

Learning Scalable Deep Kernels with Recurrent Structure

  Many applications in speech, robotics, finance, and biology deal withsequential data, where ordering matters and recurrent structures are common.However, this structure cannot be easily captured by standard kernel functions.To model such structure, we propose expressive closed-form kernel functions forGaussian processes. The resulting model, GP-LSTM, fully encapsulates theinductive biases of long short-term memory (LSTM) recurrent networks, whileretaining the non-parametric probabilistic advantages of Gaussian processes. Welearn the properties of the proposed kernels by optimizing the Gaussian processmarginal likelihood using a new provably convergent semi-stochastic gradientprocedure and exploit the structure of these kernels for scalable training andprediction. This approach provides a practical representation for BayesianLSTMs. We demonstrate state-of-the-art performance on several benchmarks, andthoroughly investigate a consequential autonomous driving application, wherethe predictive uncertainties provided by GP-LSTM are uniquely valuable.

Scalable Log Determinants for Gaussian Process Kernel Learning

  For applications as varied as Bayesian neural networks, determinantal pointprocesses, elliptical graphical models, and kernel learning for Gaussianprocesses (GPs), one must compute a log determinant of an $n \times n$ positivedefinite matrix, and its derivatives - leading to prohibitive$\mathcal{O}(n^3)$ computations. We propose novel $\mathcal{O}(n)$ approachesto estimating these quantities from only fast matrix vector multiplications(MVMs). These stochastic approximations are based on Chebyshev, Lanczos, andsurrogate models, and converge quickly even for kernel matrices that havechallenging spectra. We leverage these approximations to develop a scalableGaussian process approach to kernel learning. We find that Lanczos is generallysuperior to Chebyshev for kernel learning, and that a surrogate approach can behighly efficient and accurate with popular kernels.

Constant-Time Predictive Distributions for Gaussian Processes

  One of the most compelling features of Gaussian process (GP) regression isits ability to provide well-calibrated posterior distributions. Recent advancesin inducing point methods have sped up GP marginal likelihood and posteriormean computations, leaving posterior covariance estimation and sampling as theremaining computational bottlenecks. In this paper we address theseshortcomings by using the Lanczos algorithm to rapidly approximate thepredictive covariance matrix. Our approach, which we refer to as LOVE (LanczOsVariance Estimates), substantially improves time and space complexity. In ourexperiments, LOVE computes covariances up to 2,000 times faster and drawssamples 18,000 times faster than existing methods, all without sacrificingaccuracy.

Gaussian Process Subset Scanning for Anomalous Pattern Detection in  Non-iid Data

  Identifying anomalous patterns in real-world data is essential forunderstanding where, when, and how systems deviate from their expecteddynamics. Yet methods that separately consider the anomalousness of eachindividual data point have low detection power for subtle, emergingirregularities. Additionally, recent detection techniques based on subsetscanning make strong independence assumptions and suffer degraded performancein correlated data. We introduce methods for identifying anomalous patterns innon-iid data by combining Gaussian processes with novel log-likelihood ratiostatistic and subset scanning techniques. Our approaches are powerful,interpretable, and can integrate information across multiple data streams. Weillustrate their performance on numeric simulations and three open sourcespatiotemporal datasets of opioid overdose deaths, 311 calls, and stormreports.

Hierarchical Density Order Embeddings

  By representing words with probability densities rather than point vectors,probabilistic word embeddings can capture rich and interpretable semanticinformation and uncertainty. The uncertainty information can be particularlymeaningful in capturing entailment relationships -- whereby general words suchas "entity" correspond to broad distributions that encompass more specificwords such as "animal" or "instrument". We introduce density order embeddings,which learn hierarchical representations through encapsulation of probabilitydensities. In particular, we propose simple yet effective loss functions anddistance metrics, as well as graph-based schemes to select negative samples tobetter learn hierarchical density representations. Our approach providesstate-of-the-art performance on the WordNet hypernym relationship predictiontask and the challenging HyperLex lexical entailment dataset -- while retaininga rich and interpretable density representation.

Probabilistic FastText for Multi-Sense Word Embeddings

  We introduce Probabilistic FastText, a new model for word embeddings that cancapture multiple word senses, sub-word structure, and uncertainty information.In particular, we represent each word with a Gaussian mixture density, wherethe mean of a mixture component is given by the sum of n-grams. Thisrepresentation allows the model to share statistical strength across sub-wordstructures (e.g. Latin roots), producing accurate representations of rare,misspelt, or even unseen words. Moreover, each component of the mixture cancapture a different word sense. Probabilistic FastText outperforms bothFastText, which has no probabilistic model, and dictionary-level probabilisticembeddings, which do not incorporate subword structures, on severalword-similarity benchmarks, including English RareWord and foreign languagedatasets. We also achieve state-of-art performance on benchmarks that measureability to discern different meanings. Thus, the proposed model is the first toachieve multi-sense representations while having enriched semantics on rarewords.

GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU  Acceleration

  Despite advances in scalable models, the inference tools used for Gaussianprocesses (GPs) have yet to fully capitalize on developments in computinghardware. We present an efficient and general approach to GP inference based onBlackbox Matrix-Matrix multiplication (BBMM). BBMM inference uses a modifiedbatched version of the conjugate gradients algorithm to derive all terms fortraining and inference in a single call. BBMM reduces the asymptotic complexityof exact GP inference from $O(n^3)$ to $O(n^2)$. Adapting this algorithm toscalable approximations and complex GP models simply requires a routine forefficient matrix-matrix multiplication with the kernel and its derivative. Inaddition, BBMM uses a specialized preconditioner to substantially speed upconvergence. In experiments we show that BBMM effectively uses GPU hardware todramatically accelerate both exact GP inference and scalable approximations.Additionally, we provide GPyTorch, a software platform for scalable GPinference via BBMM, built on PyTorch.

Scaling Gaussian Process Regression with Derivatives

  Gaussian processes (GPs) with derivatives are useful in many applications,including Bayesian optimization, implicit surface reconstruction, and terrainreconstruction. Fitting a GP to function values and derivatives at $n$ pointsin $d$ dimensions requires linear solves and log determinants with an ${n(d+1)\times n(d+1)}$ positive definite matrix -- leading to prohibitive$\mathcal{O}(n^3d^3)$ computations for standard direct methods. We proposeiterative solvers using fast $\mathcal{O}(nd)$ matrix-vector multiplications(MVMs), together with pivoted Cholesky preconditioning that cuts the iterationsto convergence by several orders of magnitude, allowing for fast kernellearning and prediction. Our approaches, together with dimensionalityreduction, enables Bayesian optimization with derivatives to scale tohigh-dimensional problems and large evaluation budgets.

Practical Multi-fidelity Bayesian Optimization for Hyperparameter Tuning

  Bayesian optimization is popular for optimizing time-consuming black-boxobjectives. Nonetheless, for hyperparameter tuning in deep neural networks, thetime required to evaluate the validation error for even a few hyperparametersettings remains a bottleneck. Multi-fidelity optimization promises reliefusing cheaper proxies to such objectives --- for example, validation error fora network trained using a subset of the training points or fewer iterationsthan required for convergence. We propose a highly flexible and practicalapproach to multi-fidelity Bayesian optimization, focused on efficientlyoptimizing hyperparameters for iteratively trained supervised learning models.We introduce a new acquisition function, the trace-aware knowledge-gradient,which efficiently leverages both multiple continuous fidelity controls andtrace observations --- values of the objective at a sequence of fidelities,available when varying fidelity using training iterations. We provide aprovably convergent method for optimizing our acquisition function and show itoutperforms state-of-the-art alternatives for hyperparameter tuning of deepneural networks and large-scale kernel learning.

Exact Gaussian Processes on a Million Data Points

  Gaussian processes (GPs) are flexible models with state-of-the-artperformance on many impactful applications. However, computational constraintswith standard inference procedures have limited exact GPs to problems withfewer than about ten thousand training points, necessitating approximations forlarger datasets. In this paper, we develop a scalable approach for exact GPsthat leverages multi-GPU parallelization and methods like linear conjugategradients, accessing the kernel matrix only through matrix multiplication. Bypartitioning and distributing kernel matrix multiplies, we demonstrate that anexact GP can be trained on over a million points in 3 days using 8 GPUs and cancompute predictive means and variances in under a second using 1 GPU at testtime. Moreover, we perform the first-ever comparison of exact GPs againststate-of-the-art scalable approximations on large-scale regression datasetswith $10^4-10^6$ data points, showing dramatic performance improvements.

Scalable Lévy Process Priors for Spectral Kernel Learning

  Gaussian processes are rich distributions over functions, with generalizationproperties determined by a kernel function. When used for long-rangeextrapolation, predictions are particularly sensitive to the choice of kernelparameters. It is therefore critical to account for kernel uncertainty in ourpredictive distributions. We propose a distribution over kernels formed bymodelling a spectral mixture density with a L\'evy process. The resultingdistribution has support for all stationary covariances--including the popularRBF, periodic, and Mat\'ern kernels--combined with inductive biases whichenable automatic and data efficient learning, long-range extrapolation, andstate of the art predictive performance. The proposed model also presents anapproach to spectral regularization, as the L\'evy process introduces asparsity-inducing prior over mixture components, allowing automatic selectionover model order and pruning of extraneous components. We exploit the algebraicstructure of the proposed process for $\mathcal{O}(n)$ training and$\mathcal{O}(1)$ predictions. We perform extrapolations having reasonableuncertainty estimates on several benchmarks, show that the proposed model canrecover flexible ground truth covariances and that it is robust to errors ininitialization.

Student-t Processes as Alternatives to Gaussian Processes

  We investigate the Student-t process as an alternative to the Gaussianprocess as a nonparametric prior over functions. We derive closed formexpressions for the marginal likelihood and predictive distribution of aStudent-t process, by integrating away an inverse Wishart process prior overthe covariance kernel of a Gaussian process model. We show surprisingequivalences between different hierarchical Gaussian process models leading toStudent-t processes, and derive a new sampling scheme for the inverse Wishartprocess, which helps elucidate these equivalences. Overall, we show that aStudent-t process can retain the attractive properties of a Gaussian process --a nonparametric representation, analytic marginal and predictive distributions,and easy model selection through covariance kernels -- but has enhancedflexibility, and predictive covariances that, unlike a Gaussian process,explicitly depend on the values of training observations. We verify empiricallythat a Student-t process is especially useful in situations where there arechanges in covariance structure, or in applications like Bayesian optimization,where accurate predictive covariances are critical for good performance. Theseadvantages come at no additional computational cost over Gaussian processes.

Bayesian Optimization with Gradients

  Bayesian optimization has been successful at global optimization ofexpensive-to-evaluate multimodal objective functions. However, unlike mostoptimization methods, Bayesian optimization typically does not use derivativeinformation. In this paper we show how Bayesian optimization can exploitderivative information to decrease the number of objective function evaluationsrequired for good performance. In particular, we develop a novel Bayesianoptimization algorithm, the derivative-enabled knowledge-gradient (dKG), forwhich we show one-step Bayes-optimality, asymptotic consistency, and greaterone-step value of information than is possible in the derivative-free setting.Our procedure accommodates noisy and incomplete derivative information, comesin both sequential and batch forms, and can optionally reduce the computationalcost of inference through automatically selected retention of a singledirectional derivative. We also compute the d-KG acquisition function and itsgradient using a novel fast discretization-free technique. We show d-KGprovides state-of-the-art performance compared to a wide range of optimizationprocedures with and without gradients, on benchmarks including logisticregression, deep learning, kernel learning, and k-nearest neighbors.

There Are Many Consistent Explanations of Unlabeled Data: Why You Should  Average

  Presently the most successful approaches to semi-supervised learning arebased on consistency regularization, whereby a model is trained to be robust tosmall perturbations of its inputs and parameters. To understand consistencyregularization, we conceptually explore how loss geometry interacts withtraining procedures. The consistency loss dramatically improves generalizationperformance over supervised-only training; however, we show that SGD strugglesto converge on the consistency loss and continues to make large steps that leadto changes in predictions on the test data. Motivated by these observations, wepropose to train consistency-based methods with Stochastic Weight Averaging(SWA), a recent approach which averages weights along the trajectory of SGDwith a modified learning rate schedule. We also propose fast-SWA, which furtheraccelerates convergence by averaging multiple points within each cycle of acyclical learning rate schedule. With weight averaging, we achieve the bestknown semi-supervised results on CIFAR-10 and CIFAR-100, over many differentquantities of labeled training data. For example, we achieve 5.0% error onCIFAR-10 with only 4000 labels, compared to the previous best result in theliterature of 6.3%.

The Space Infrared Interferometric Telescope (SPIRIT): High-resolution  imaging and spectroscopy in the far-infrared

  We report results of a recently-completed pre-Formulation Phase study ofSPIRIT, a candidate NASA Origins Probe mission. SPIRIT is a spatial andspectral interferometer with an operating wavelength range 25 - 400 microns.SPIRIT will provide sub-arcsecond resolution images and spectra with resolutionR = 3000 in a 1 arcmin field of view to accomplish three primary scientificobjectives: (1) Learn how planetary systems form from protostellar disks, andhow they acquire their inhomogeneous composition; (2) characterize the familyof extrasolar planetary systems by imaging the structure in debris disks tounderstand how and where planets of different types form; and (3) learn howhigh-redshift galaxies formed and merged to form the present-day population ofgalaxies. Observations with SPIRIT will be complementary to those of the JamesWebb Space Telescope and the ground-based Atacama Large Millimeter Array. Allthree observatories could be operational contemporaneously.

SysML: The New Frontier of Machine Learning Systems

  Machine learning (ML) techniques are enjoying rapidly increasing adoption.However, designing and implementing the systems that support ML models inreal-world deployments remains a significant obstacle, in large part due to theradically different development and deployment profile of modern ML methods,and the range of practical concerns that come with broader adoption. We proposeto foster a new systems machine learning research community at the intersectionof the traditional systems and ML communities, focused on topics such ashardware systems for ML, software systems for ML, and ML optimized for metricsbeyond predictive accuracy. To do this, we describe a new conference, SysML,that explicitly targets research at the intersection of systems and machinelearning with a program committee split evenly between experts in systems andML, and an explicit focus on topics at the intersection of the two.

The Detailed Science Case for the Maunakea Spectroscopic Explorer, 2019  edition

  (Abridged) The Maunakea Spectroscopic Explorer (MSE) is an end-to-end scienceplatform for the design, execution and scientific exploitation of spectroscopicsurveys. It will unveil the composition and dynamics of the faint Universe andimpact nearly every field of astrophysics across all spatial scales, fromindividual stars to the largest scale structures in the Universe. Major pillarsin the science program for MSE include (i) the ultimate Gaia follow-up facilityfor understanding the chemistry and dynamics of the distant Milky Way,including the outer disk and faint stellar halo at high spectral resolution(ii) galaxy formation and evolution at cosmic noon, via the type ofrevolutionary surveys that have occurred in the nearby Universe, but nowconducted at the peak of the star formation history of the Universe (iii)derivation of the mass of the neutrino and insights into inflationary physicsthrough a cosmological redshift survey that probes a large volume of theUniverse with a high galaxy density. MSE is positioned to become a critical hubin the emerging international network of front-line astronomical facilities,with scientific capabilities that naturally complement and extend thescientific power of Gaia, the Large Synoptic Survey Telescope, the SquareKilometer Array, Euclid, WFIRST, the 30m telescopes and many more.

The Ninth Data Release of the Sloan Digital Sky Survey: First  Spectroscopic Data from the SDSS-III Baryon Oscillation Spectroscopic Survey

  The Sloan Digital Sky Survey III (SDSS-III) presents the first spectroscopicdata from the Baryon Oscillation Spectroscopic Survey (BOSS). This ninth datarelease (DR9) of the SDSS project includes 535,995 new galaxy spectra (medianz=0.52), 102,100 new quasar spectra (median z=2.32), and 90,897 new stellarspectra, along with the data presented in previous data releases. These spectrawere obtained with the new BOSS spectrograph and were taken between 2009December and 2011 July. In addition, the stellar parameters pipeline, whichdetermines radial velocities, surface temperatures, surface gravities, andmetallicities of stars, has been updated and refined with improvements intemperature estimates for stars with T_eff<5000 K and in metallicity estimatesfor stars with [Fe/H]>-0.5. DR9 includes new stellar parameters for all starspresented in DR8, including stars from SDSS-I and II, as well as those observedas part of the SDSS-III Sloan Extension for Galactic Understanding andExploration-2 (SEGUE-2).  The astrometry error introduced in the DR8 imaging catalogs has beencorrected in the DR9 data products. The next data release for SDSS-III will bein Summer 2013, which will present the first data from the Apache PointObservatory Galactic Evolution Experiment (APOGEE) along with another year ofdata from BOSS, followed by the final SDSS-III data release in December 2014.

SDSS-III: Massive Spectroscopic Surveys of the Distant Universe, the  Milky Way Galaxy, and Extra-Solar Planetary Systems

  Building on the legacy of the Sloan Digital Sky Survey (SDSS-I and II),SDSS-III is a program of four spectroscopic surveys on three scientific themes:dark energy and cosmological parameters, the history and structure of the MilkyWay, and the population of giant planets around other stars. In keeping withSDSS tradition, SDSS-III will provide regular public releases of all its data,beginning with SDSS DR8 (which occurred in Jan 2011). This paper presents anoverview of the four SDSS-III surveys. BOSS will measure redshifts of 1.5million massive galaxies and Lya forest spectra of 150,000 quasars, using theBAO feature of large scale structure to obtain percent-level determinations ofthe distance scale and Hubble expansion rate at z<0.7 and at z~2.5. SEGUE-2,which is now completed, measured medium-resolution (R=1800) optical spectra of118,000 stars in a variety of target categories, probing chemical evolution,stellar kinematics and substructure, and the mass profile of the dark matterhalo from the solar neighborhood to distances of 100 kpc. APOGEE will obtainhigh-resolution (R~30,000), high signal-to-noise (S/N>100 per resolutionelement), H-band (1.51-1.70 micron) spectra of 10^5 evolved, late-type stars,measuring separate abundances for ~15 elements per star and creating the firsthigh-precision spectroscopic survey of all Galactic stellar populations (bulge,bar, disks, halo) with a uniform set of stellar tracers and spectraldiagnostics. MARVELS will monitor radial velocities of more than 8000 FGK starswith the sensitivity and cadence (10-40 m/s, ~24 visits per star) needed todetect giant planets with periods up to two years, providing an unprecedenteddata set for understanding the formation and dynamical evolution of giantplanet systems. (Abridged)

The Fifteenth Data Release of the Sloan Digital Sky Surveys: First  Release of MaNGA Derived Quantities, Data Visualization Tools and Stellar  Library

  Twenty years have passed since first light for the Sloan Digital Sky Survey(SDSS). Here, we release data taken by the fourth phase of SDSS (SDSS-IV)across its first three years of operation (July 2014-July 2017). This is thethird data release for SDSS-IV, and the fifteenth from SDSS (Data ReleaseFifteen; DR15). New data come from MaNGA - we release 4824 datacubes, as wellas the first stellar spectra in the MaNGA Stellar Library (MaStar), the firstset of survey-supported analysis products (e.g. stellar and gas kinematics,emission line, and other maps) from the MaNGA Data Analysis Pipeline (DAP), anda new data visualisation and access tool we call "Marvin". The next datarelease, DR16, will include new data from both APOGEE-2 and eBOSS; thosesurveys release no new data here, but we document updates and corrections totheir data processing pipelines. The release is cumulative; it also includesthe most recent reductions and calibrations of all data taken by SDSS sincefirst light. In this paper we describe the location and format of the data andtools and cite technical references describing how it was obtained andprocessed. The SDSS website (www.sdss.org) has also been updated, providinglinks to data downloads, tutorials and examples of data use. While SDSS-IV willcontinue to collect astronomical data until 2020, and will be followed bySDSS-V (2020-2025), we end this paper by describing plans to ensure thesustainability of the SDSS data archive for many years beyond the collection ofdata.

The Long-Baseline Neutrino Experiment: Exploring Fundamental Symmetries  of the Universe

  The preponderance of matter over antimatter in the early Universe, thedynamics of the supernova bursts that produced the heavy elements necessary forlife and whether protons eventually decay --- these mysteries at the forefrontof particle physics and astrophysics are key to understanding the earlyevolution of our Universe, its current state and its eventual fate. TheLong-Baseline Neutrino Experiment (LBNE) represents an extensively developedplan for a world-class experiment dedicated to addressing these questions. LBNEis conceived around three central components: (1) a new, high-intensityneutrino source generated from a megawatt-class proton accelerator at FermiNational Accelerator Laboratory, (2) a near neutrino detector just downstreamof the source, and (3) a massive liquid argon time-projection chamber deployedas a far detector deep underground at the Sanford Underground ResearchFacility. This facility, located at the site of the former Homestake Mine inLead, South Dakota, is approximately 1,300 km from the neutrino source atFermilab -- a distance (baseline) that delivers optimal sensitivity to neutrinocharge-parity symmetry violation and mass ordering effects. This ambitious yetcost-effective design incorporates scalability and flexibility and canaccommodate a variety of upgrades and contributions. With its exceptionalcombination of experimental configuration, technical capabilities, andpotential for transformative discoveries, LBNE promises to be a vital facilityfor the field of particle physics worldwide, providing physicists from aroundthe globe with opportunities to collaborate in a twenty to thirty year programof exciting science. In this document we provide a comprehensive overview ofLBNE's scientific objectives, its place in the landscape of neutrino physicsworldwide, the technologies it will incorporate and the capabilities it willpossess.

The Fourteenth Data Release of the Sloan Digital Sky Survey: First  Spectroscopic Data from the extended Baryon Oscillation Spectroscopic Survey  and from the second phase of the Apache Point Observatory Galactic Evolution  Experiment

  The fourth generation of the Sloan Digital Sky Survey (SDSS-IV) has been inoperation since July 2014. This paper describes the second data release fromthis phase, and the fourteenth from SDSS overall (making this, Data ReleaseFourteen or DR14). This release makes public data taken by SDSS-IV in its firsttwo years of operation (July 2014-2016). Like all previous SDSS releases, DR14is cumulative, including the most recent reductions and calibrations of alldata taken by SDSS since the first phase began operations in 2000. New in DR14is the first public release of data from the extended Baryon OscillationSpectroscopic Survey (eBOSS); the first data from the second phase of theApache Point Observatory (APO) Galactic Evolution Experiment (APOGEE-2),including stellar parameter estimates from an innovative data driven machinelearning algorithm known as "The Cannon"; and almost twice as many data cubesfrom the Mapping Nearby Galaxies at APO (MaNGA) survey as were in the previousrelease (N = 2812 in total). This paper describes the location and format ofthe publicly available data from SDSS-IV surveys. We provide references to theimportant technical papers describing how these data have been taken (bothtargeting and observation details) and processed for scientific use. The SDSSwebsite (www.sdss.org) has been updated for this release, and provides links todata downloads, as well as tutorials and examples of data use. SDSS-IV isplanning to continue to collect astronomical data until 2020, and will befollowed by SDSS-V.

Observation of $B^0_s\rightarrowχ_{c1}φ$ decay and study of  $B^0\rightarrowχ_{c1,2}K^{*0}$ decays

  The first observation of the decay $B^0_s\rightarrow\chi_{c1}\phi$ and astudy of $B^0\rightarrow\chi_{c1,2}K^{*0}$ decays are presented. The analysisis performed using a dataset, corresponding to an integrated luminosity of 1.0fb$^{-1}$, collected by the LHCb experiment in pp collisions at acentre-of-mass energy of 7 TeV. The following ratios of branching fractions aremeasured: \begin{equation*} \begin{array}{lll}\dfrac{\cal{B}(B^0_s\rightarrow\chi_{c1}\phi)}{\cal{B}(B^0_s\rightarrowJ/\psi\phi)} &=& (18.9 \pm1.8\,(stat)\pm1.3\,(syst)\pm0.8\,(\cal{B})) \times10^{-2}, \nonumber \\ \noalign{\vskip 3pt}\dfrac{\cal{B}(B^0\rightarrow\chi_{c1}K^{*0})}{\cal{B}(B^0\rightarrowJ/\psiK^{*0})} &=& (19.8 \pm1.1\,(stat)\pm1.2\,(syst)\pm0.9\,(\cal{B})) \times10^{-2}, \nonumber \\ \noalign{\vskip 3pt}\dfrac{\cal{B}(B^0\rightarrow\chi_{c2}K^{*0})}{\cal{B}(B^0\rightarrow\chi_{c1}K^{*0})} &=& (17.1 \pm5.0\,(stat)\pm1.7\,(syst)\pm1.1\,(\cal{B})) \times10^{-2}, \nonumber \\ \noalign{\vskip 3pt} \end{array} \end{equation*}\noindent where the third uncertainty is due to the limited knowledge of thebranching fractions of ${\chi_{c}\rightarrow J/\psi \gamma}$ modes.

Measurement of CP violation in the phase space of $B^{\pm} \to K^{\pm}  π^{+} π^{-}$ and $B^{\pm} \to K^{\pm} K^{+} K^{-}$ decays

  The charmless decays $B^{\pm}\to K^{\pm}\pi^+\pi^-$ and $B^{\pm}\toK^{\pm}K^+K^-$ are reconstructed using data, corresponding to an integratedluminosity of 1.0 fb$^{-1}$, collected by LHCb in 2011. The inclusive chargeasymmetries of these modes are measured as $A_{C\!P}(B^{\pm}\toK^{\pm}\pi^+\pi^-) = 0.032 \pm 0.008 \stat \pm 0.004 \syst \pm 0.007 (J\psiK^{\pm})$ and $A_{C\!P}(B^{\pm}\to K^{\pm}K^+K^-) = -0.043 \pm 0.009 \stat \pm0.003 \syst \pm 0.007 (J\psi K^{\pm})$, where the third uncertainty is due tothe $C\!P$ asymmetry of the $B^{\pm}\to J\psi K^{\pm}$ reference mode. Thesignificance of $A_{C\!P}(B^{\pm}\to K^{\pm}K^+K^-)$ exceeds three standarddeviations and is the first evidence of an inclusive $C\!P$ asymmetry incharmless three-body $B$ decays. In addition to the inclusive $C\!P$asymmetries, larger asymmetries are observed in localised regions of phasespace.

Measurement of the differential branching fraction of the decay  $Λ_b^0\rightarrowΛμ^+μ^-$

  The differential branching fraction of the decay$\Lambda_b^0\rightarrow\Lambda\mu^+\mu^-$ is measured as a function of thesquare of the dimuon invariant mass, $q^2$. A yield of $78\pm12$$\Lambda_b^0\rightarrow\Lambda\mu^+\mu^-$ decays is observed using data,corresponding to an integrated luminosity of 1.0\,fb$^{-1}$, collected by theLHCb experiment at a centre-of-mass energy of 7\,TeV. A significant signal isfound in the $q^2$ region above the square of the $J/\psi$ mass, while atlower-$q^2$ values upper limits are set on the differential branching fraction.Integrating the differential branching fraction over $q^2$, while excluding the$J/\psi$ and $\psi(2S)$ regions, gives a branching fraction of$\BF($\Lambda_b^0\rightarrow\Lambda\mu^+\mu^-$)=(0.96\pm 0.16\stat\pm0.13\syst\pm 0.21 (\mathrm{norm}))\times 10^{-6}$, where the uncertainties arestatistical, systematic and due to the normalisation mode,$$\Lambda_b^0\rightarrow J/psi\Lambda$, respectively.

Searches for $B^{0}_{(s)} \rightarrow J/ψ p \overline{p}$ and  $B^{+} \to J/ψ p \overline{p} π^{+}$ decays

  The results of searches for $B^{0}_{(s)} \rightarrow J/{\psi} p \overline{p}$and $B^{+} \to J/{\psi} p \overline{p} {\pi}^{+}$ decays are reported. Theanalysis is based on a data sample, corresponding to an integrated luminosityof 1.0 fb$^{-1}$ of $pp$ collisions, collected with the LHCb detector. Anexcess with 2.8$\,\sigma$ significance is seen for the decay $B^{0}_{s} \toJ/{\psi} p \overline{p}$ and an upper limit on the branching fraction is set atthe 90 % confidence level: ${\cal B}(B^{0}_{s} \to J/{\psi} p \overline{p}) <4.8 \times 10^{-6}$, which is the first such limit. No significant signals areseen for $B^{0} \to J/{\psi} p \overline{p}$ and $B^{+} \to J/{\psi} p\overline{p} {\pi}^{+}$ decays, for which the corresponding limits are set:${\cal{B}}(B^{0} \to J/{\psi} p \overline{p}) < 5.2 \times 10^{-7}$, whichsignificantly improves the existing limit; and ${\cal{B}}(B^{+} \to J/{\psi} p\overline{p} {\pi}^{+}) < 5.0 \times 10^{-7}$, which is the first limit on thisbranching fraction.

