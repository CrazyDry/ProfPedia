Response to "Visual Dialogue without Vision or Dialogue" (Massiceti et  al., 2018)

  In a recent workshop paper, Massiceti et al. presented a baseline model andsubsequent critique of Visual Dialog (Das et al., CVPR 2017) that raises whatwe believe to be unfounded concerns about the dataset and evaluation. Thisarticle intends to rebut the critique and clarify potential confusions forpractitioners and future participants in the Visual Dialog challenge.

An Efficient Message-Passing Algorithm for the M-Best MAP Problem

  Much effort has been directed at algorithms for obtaining the highestprobability configuration in a probabilistic random field model known as themaximum a posteriori (MAP) inference problem. In many situations, one couldbenefit from having not just a single solution, but the top M most probablesolutions known as the M-Best MAP problem. In this paper, we propose anefficient message-passing based algorithm for solving the M-Best MAP problem.Specifically, our algorithm solves the recently proposed Linear Programming(LP) formulation of M-Best MAP [7], while being orders of magnitude faster thana generic LP-solver. Our approach relies on studying a particular partialLagrangian relaxation of the M-Best MAP LP which exposes a naturalcombinatorial structure of the problem that we exploit.

Graph R-CNN for Scene Graph Generation

  We propose a novel scene graph generation model called Graph R-CNN, that isboth effective and efficient at detecting objects and their relations inimages. Our model contains a Relation Proposal Network (RePN) that efficientlydeals with the quadratic number of potential relations between objects in animage. We also propose an attentional Graph Convolutional Network (aGCN) thateffectively captures contextual information between objects and relations.Finally, we introduce a new evaluation metric that is more holistic andrealistic than existing metrics. We report state-of-the-art performance onscene graph generation as evaluated using both existing and our proposedmetrics.

Fabrik: An Online Collaborative Neural Network Editor

  We present Fabrik, an online neural network editor that provides tools tovisualize, edit, and share neural networks from within a browser. Fabrikprovides a simple and intuitive GUI to import neural networks written inpopular deep learning frameworks such as Caffe, Keras, and TensorFlow, andallows users to interact with, build, and edit models via simple drag and drop.Fabrik is designed to be framework agnostic and support high interoperability,and can be used to export models back to any supported framework. Finally, itprovides powerful collaborative features to enable users to iterate over modeldesign remotely and at scale.

Submodular meets Structured: Finding Diverse Subsets in  Exponentially-Large Structured Item Sets

  To cope with the high level of ambiguity faced in domains such as ComputerVision or Natural Language processing, robust prediction methods often searchfor a diverse set of high-quality candidate solutions or proposals. Instructured prediction problems, this becomes a daunting task, as the solutionspace (image labelings, sentence parses, etc.) is exponentially large. We studygreedy algorithms for finding a diverse subset of solutions instructured-output spaces by drawing new connections between submodularfunctions over combinatorial item sets and High-Order Potentials (HOPs) studiedfor graphical models. Specifically, we show via examples that when marginalgains of submodular diversity functions allow structured representations, thisenables efficient (sub-linear time) approximate maximization by reducing thegreedy augmentation step to inference in a factor graph with appropriatelyconstructed HOPs. We discuss benefits, tradeoffs, and show that ourconstructions lead to significantly better proposals.

Candidate Constrained CRFs for Loss-Aware Structured Prediction

  When evaluating computer vision systems, we are often concerned withperformance on a task-specific evaluation measure such as theIntersection-Over-Union score used in the PASCAL VOC image segmentationchallenge. Ideally, our systems would be tuned specifically to these evaluationmeasures. However, despite much work on loss-aware structured prediction, topperforming systems do not use these techniques. In this work, we seek toaddress this problem, incorporating loss-aware prediction in a manner that isamenable to the approaches taken by top performing systems. Our main idea is tosimultaneously leverage two systems: a highly tuned pipeline system as is foundon top of leaderboards, and a traditional CRF. We show how to combine highquality candidate solutions from the pipeline with the probabilistic approachof the CRF that is amenable to loss-aware prediction. The result is that we canuse loss-aware prediction methodology to improve performance of the highlytuned pipeline system.

Combining the Best of Graphical Models and ConvNets for Semantic  Segmentation

  We present a two-module approach to semantic segmentation that incorporatesConvolutional Networks (CNNs) and Graphical Models. Graphical models are usedto generate a small (5-30) set of diverse segmentations proposals, such thatthis set has high recall. Since the number of required proposals is so low, wecan extract fairly complex features to rank them. Our complex feature of choiceis a novel CNN called SegNet, which directly outputs a (coarse) semanticsegmentation. Importantly, SegNet is specifically trained to optimize thecorpus-level PASCAL IOU loss function. To the best of our knowledge, this isthe first CNN specifically designed for semantic segmentation. This two-moduleapproach achieves $52.5\%$ on the PASCAL 2012 segmentation challenge.

VIP: Finding Important People in Images

  People preserve memories of events such as birthdays, weddings, or vacationsby capturing photos, often depicting groups of people. Invariably, someindividuals in the image are more important than others given the context ofthe event. This paper analyzes the concept of the importance of individuals ingroup photographs. We address two specific questions -- Given an image, who arethe most important individuals in it? Given multiple images of a person, whichimage depicts the person in the most important role? We introduce a measure ofimportance of people in images and investigate the correlation betweenimportance and visual saliency. We find that not only can we automaticallypredict the importance of people from purely visual cues, incorporating thispredicted importance results in significant improvement in applications such asim2text (generating sentences that describe images of groups of people).

CloudCV: Large Scale Distributed Computer Vision as a Cloud Service

  We are witnessing a proliferation of massive visual data. Unfortunatelyscaling existing computer vision algorithms to large datasets leavesresearchers repeatedly solving the same algorithmic, logistical, andinfrastructural problems. Our goal is to democratize computer vision; oneshould not have to be a computer vision, big data and distributed computingexpert to have access to state-of-the-art distributed computer visionalgorithms. We present CloudCV, a comprehensive system to provide access tostate-of-the-art distributed computer vision algorithms as a cloud servicethrough a Web Interface and APIs.

Reducing Overfitting in Deep Networks by Decorrelating Representations

  One major challenge in training Deep Neural Networks is preventingoverfitting. Many techniques such as data augmentation and novel regularizerssuch as Dropout have been proposed to prevent overfitting without requiring amassive amount of training data. In this work, we propose a new regularizercalled DeCov which leads to significantly reduced overfitting (as indicated bythe difference between train and val performance), and better generalization.Our regularizer encourages diverse or non-redundant representations in DeepNeural Networks by minimizing the cross-covariance of hidden activations. Thissimple intuition has been explored in a number of past works but surprisinglyhas never been applied as a regularizer in supervised learning. Experimentsacross a range of datasets and network architectures show that this loss alwaysreduces overfitting while almost always maintaining or increasinggeneralization performance and often improving performance over Dropout.

Why M Heads are Better than One: Training a Diverse Ensemble of Deep  Networks

  Convolutional Neural Networks have achieved state-of-the-art performance on awide range of tasks. Most benchmarks are led by ensembles of these powerfullearners, but ensembling is typically treated as a post-hoc procedureimplemented by averaging independently trained models with model variationinduced by bagging or random initialization. In this paper, we rigorously treatensembling as a first-class problem to explicitly address the question: whatare the best strategies to create an ensemble? We first compare a large numberof ensembling strategies, and then propose and evaluate novel strategies, suchas parameter sharing (through a new family of models we call TreeNets) as wellas training under ensemble-aware and diversity-encouraging losses. Wedemonstrate that TreeNets can improve ensemble performance and that diverseensembles can be trained end-to-end under a unified loss, achievingsignificantly higher "oracle" accuracies than classical ensembles.

Counting Everyday Objects in Everyday Scenes

  We are interested in counting the number of instances of object classes innatural, everyday images. Previous counting approaches tackle the problem inrestricted domains such as counting pedestrians in surveillance videos. Countscan also be estimated from outputs of other vision tasks like object detection.In this work, we build dedicated models for counting designed to tackle thelarge variance in counts, appearances, and scales of objects found in naturalscenes. Our approach is inspired by the phenomenon of subitizing - the abilityof humans to make quick assessments of counts given a perceptual signal, forsmall count values. Given a natural scene, we employ a divide and conquerstrategy while incorporating context across the scene to adapt the subitizingidea to counting. Our approach offers consistent improvements over numerousbaseline approaches for counting on the PASCAL VOC 2007 and COCO datasets.Subsequently, we study how counting can be used to improve object detection. Wethen show a proof of concept application of our counting methods to the task ofVisual Question Answering, by studying the `how many?' questions in the VQA andCOCO-QA datasets.

Joint Unsupervised Learning of Deep Representations and Image Clusters

  In this paper, we propose a recurrent framework for Joint UnsupervisedLEarning (JULE) of deep representations and image clusters. In our framework,successive operations in a clustering algorithm are expressed as steps in arecurrent process, stacked on top of representations output by a ConvolutionalNeural Network (CNN). During training, image clusters and representations areupdated jointly: image clustering is conducted in the forward pass, whilerepresentation learning in the backward pass. Our key idea behind thisframework is that good representations are beneficial to image clustering andclustering results provide supervisory signals to representation learning. Byintegrating two processes into a single model with a unified weighted tripletloss and optimizing it end-to-end, we can obtain not only more powerfulrepresentations, but also more precise image clusters. Extensive experimentsshow that our method outperforms the state-of-the-art on image clusteringacross a variety of image datasets. Moreover, the learned representationsgeneralize well when transferred to other tasks.

Visual Storytelling

  We introduce the first dataset for sequential vision-to-language, and explorehow this data may be used for the task of visual storytelling. The firstrelease of this dataset, SIND v.1, includes 81,743 unique photos in 20,211sequences, aligned to both descriptive (caption) and story language. Weestablish several strong baselines for the storytelling task, and motivate anautomatic metric to benchmark progress. Modelling concrete description as wellas figurative and social language, as provided in this dataset and thestorytelling task, has the potential to move artificial intelligence from basicunderstandings of typical visual scenes towards more and more human-likeunderstanding of grounded event structure and subjective expression.

Radio Transformer Networks: Attention Models for Learning to Synchronize  in Wireless Systems

  We introduce learned attention models into the radio machine learning domainfor the task of modulation recognition by leveraging spatial transformernetworks and introducing new radio domain appropriate transformations. Thisattention model allows the network to learn a localization network capable ofsynchronizing and normalizing a radio signal blindly with zero knowledge of thesignals structure based on optimization of the network for classificationaccuracy, sparse representation, and regularization. Using this architecture weare able to outperform our prior results in accuracy vs signal to noise ratioagainst an identical system without attention, however we believe such anattention model has implication far beyond the task of modulation recognition.

Hierarchical Question-Image Co-Attention for Visual Question Answering

  A number of recent works have proposed attention models for Visual QuestionAnswering (VQA) that generate spatial maps highlighting image regions relevantto answering the question. In this paper, we argue that in addition to modeling"where to look" or visual attention, it is equally important to model "whatwords to listen to" or question attention. We present a novel co-attentionmodel for VQA that jointly reasons about image and question attention. Inaddition, our model reasons about the question (and consequently the image viathe co-attention mechanism) in a hierarchical fashion via a novel 1-dimensionalconvolution neural networks (CNN). Our model improves the state-of-the-art onthe VQA dataset from 60.3% to 60.5%, and from 61.6% to 63.3% on the COCO-QAdataset. By using ResNet, the performance is further improved to 62.1% for VQAand 65.4% for COCO-QA.

Human Attention in Visual Question Answering: Do Humans and Deep  Networks Look at the Same Regions?

  We conduct large-scale studies on `human attention' in Visual QuestionAnswering (VQA) to understand where humans choose to look to answer questionsabout images. We design and test multiple game-inspired novelattention-annotation interfaces that require the subject to sharpen regions ofa blurred image to answer a question. Thus, we introduce the VQA-HAT (HumanATtention) dataset. We evaluate attention maps generated by state-of-the-artVQA models against human attention both qualitatively (via visualizations) andquantitatively (via rank-order correlation). Overall, our experiments show thatcurrent attention models in VQA do not seem to be looking at the same regionsas humans.

Human Attention in Visual Question Answering: Do Humans and Deep  Networks Look at the Same Regions?

  We conduct large-scale studies on `human attention' in Visual QuestionAnswering (VQA) to understand where humans choose to look to answer questionsabout images. We design and test multiple game-inspired novelattention-annotation interfaces that require the subject to sharpen regions ofa blurred image to answer a question. Thus, we introduce the VQA-HAT (HumanATtention) dataset. We evaluate attention maps generated by state-of-the-artVQA models against human attention both qualitatively (via visualizations) andquantitatively (via rank-order correlation). Overall, our experiments show thatcurrent attention models in VQA do not seem to be looking at the same regionsas humans.

Question Relevance in VQA: Identifying Non-Visual And False-Premise  Questions

  Visual Question Answering (VQA) is the task of answering natural-languagequestions about images. We introduce the novel problem of determining therelevance of questions to images in VQA. Current VQA models do not reason aboutwhether a question is even related to the given image (e.g. What is the capitalof Argentina?) or if it requires information from external resources to answercorrectly. This can break the continuity of a dialogue in human-machineinteraction. Our approaches for determining relevance are composed of twostages. Given an image and a question, (1) we first determine whether thequestion is visual or not, (2) if visual, we determine whether the question isrelevant to the given image or not. Our approaches, based on LSTM-RNNs, VQAmodel uncertainty, and caption-question similarity, are able to outperformstrong baselines on both relevance tasks. We also present human studies showingthat VQA models augmented with such question relevance reasoning are perceivedas more intelligent, reasonable, and human-like.

Analyzing the Behavior of Visual Question Answering Models

  Recently, a number of deep-learning based models have been proposed for thetask of Visual Question Answering (VQA). The performance of most models isclustered around 60-70%. In this paper we propose systematic methods to analyzethe behavior of these models as a first step towards recognizing theirstrengths and weaknesses, and identifying the most fruitful directions forprogress. We analyze two models, one each from two major classes of VQA models-- with-attention and without-attention and show the similarities anddifferences in the behavior of these models. We also analyze the winning entryof the VQA Challenge 2016.  Our behavior analysis reveals that despite recent progress, today's VQAmodels are "myopic" (tend to fail on sufficiently novel instances), often "jumpto conclusions" (converge on a predicted answer after 'listening' to just halfthe question), and are "stubborn" (do not change their answers across images).

Sort Story: Sorting Jumbled Images and Captions into Stories

  Temporal common sense has applications in AI tasks such as QA, multi-documentsummarization, and human-AI communication. We propose the task of sequencing --given a jumbled set of aligned image-caption pairs that belong to a story, thetask is to sort them such that the output sequence forms a coherent story. Wepresent multiple approaches, via unary (position) and pairwise (order)predictions, and their ensemble-based combinations, achieving strong results onthis task. We use both text-based and image-based features, which depictcomplementary improvements. Using qualitative examples, we demonstrate that ourmodels have learnt interesting aspects of temporal common sense.

Stochastic Multiple Choice Learning for Training Diverse Deep Ensembles

  Many practical perception systems exist within larger processes that includeinteractions with users or additional components capable of evaluating thequality of predicted solutions. In these contexts, it is beneficial to providethese oracle mechanisms with multiple highly likely hypotheses rather than asingle prediction. In this work, we pose the task of producing multiple outputsas a learning problem over an ensemble of deep networks -- introducing a novelstochastic gradient descent based approach to minimize the loss with respect toan oracle. Our method is simple to implement, agnostic to both architecture andloss function, and parameter-free. Our approach achieves lower oracle errorcompared to existing methods on a wide range of tasks and deep architectures.We also show qualitatively that the diverse solutions produced often provideinterpretable representations of task ambiguity.

Measuring Machine Intelligence Through Visual Question Answering

  As machines have become more intelligent, there has been a renewed interestin methods for measuring their intelligence. A common approach is to proposetasks for which a human excels, but one which machines find difficult. However,an ideal task should also be easy to evaluate and not be easily gameable. Webegin with a case study exploring the recently popular task of image captioningand its limitations as a task for measuring machine intelligence. Analternative and more promising task is Visual Question Answering that tests amachine's ability to reason about language and vision. We describe a datasetunprecedented in size created for the task that contains over 760,000 humangenerated questions about images. Using around 10 million human generatedanswers, machines may be easily evaluated.

Towards Transparent AI Systems: Interpreting Visual Question Answering  Models

  Deep neural networks have shown striking progress and obtainedstate-of-the-art results in many AI research fields in the recent years.However, it is often unsatisfying to not know why they predict what they do. Inthis paper, we address the problem of interpreting Visual Question Answering(VQA) models. Specifically, we are interested in finding what part of the input(pixels in images or words in questions) the VQA model focuses on whileanswering the question. To tackle this problem, we use two visualizationtechniques -- guided backpropagation and occlusion -- to find important wordsin the question and important regions in the image. We then present qualitativeand quantitative analyses of these importance maps. We found that even withoutexplicit attention mechanisms, VQA models may sometimes be implicitly attendingto relevant regions in the image, and often to appropriate words in thequestion.

Grad-CAM: Why did you say that?

  We propose a technique for making Convolutional Neural Network (CNN)-basedmodels more transparent by visualizing input regions that are 'important' forpredictions -- or visual explanations. Our approach, called Gradient-weightedClass Activation Mapping (Grad-CAM), uses class-specific gradient informationto localize important regions. These localizations are combined with existingpixel-space visualizations to create a novel high-resolution andclass-discriminative visualization called Guided Grad-CAM. These methods helpbetter understand CNN-based models, including image captioning and visualquestion answering (VQA) models. We evaluate our visual explanations bymeasuring their ability to discriminate between classes, to inspire trust inhumans, and their correlation with occlusion maps. Grad-CAM provides a new wayto understand CNN-based models.  We have released code, an online demo hosted on CloudCV, and a full versionof this extended abstract.

LR-GAN: Layered Recursive Generative Adversarial Networks for Image  Generation

  We present LR-GAN: an adversarial image generation model which takes scenestructure and context into account. Unlike previous generative adversarialnetworks (GANs), the proposed GAN learns to generate image background andforegrounds separately and recursively, and stitch the foregrounds on thebackground in a contextually relevant manner to produce a complete naturalimage. For each foreground, the model learns to generate its appearance, shapeand pose. The whole model is unsupervised, and is trained in an end-to-endmanner with gradient descent methods. The experiments demonstrate that LR-GANcan generate more natural images with objects that are more human recognizablethan DCGAN.

C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0  Dataset

  Visual Question Answering (VQA) has received a lot of attention over the pastcouple of years. A number of deep learning models have been proposed for thistask. However, it has been shown that these models are heavily driven bysuperficial correlations in the training data and lack compositionality -- theability to answer questions about unseen compositions of seen concepts. Thiscompositionality is desirable and central to intelligence. In this paper, wepropose a new setting for Visual Question Answering where the testquestion-answer pairs are compositionally novel compared to trainingquestion-answer pairs. To facilitate developing models under this setting, wepresent a new compositional split of the VQA v1.0 dataset, which we callCompositional VQA (C-VQA). We analyze the distribution of questions and answersin the C-VQA splits. Finally, we evaluate several existing VQA models underthis new setting and show that the performances of these models degrade by asignificant amount compared to the original VQA setting.

The Promise of Premise: Harnessing Question Premises in Visual Question  Answering

  In this paper, we make a simple observation that questions about images oftencontain premises - objects and relationships implied by the question - and thatreasoning about premises can help Visual Question Answering (VQA) modelsrespond more intelligently to irrelevant or previously unseen questions. Whenpresented with a question that is irrelevant to an image, state-of-the-art VQAmodels will still answer purely based on learned language biases, resulting innon-sensical or even misleading answers. We note that a visual question isirrelevant to an image if at least one of its premises is false (i.e. notdepicted in the image). We leverage this observation to construct a dataset forQuestion Relevance Prediction and Explanation (QRPE) by searching for falsepremises. We train novel question relevance detection models and show thatmodels that reason about premises consistently outperform models that do not.We also find that forcing standard VQA models to reason about premises duringtraining can lead to improvements on tasks requiring compositional reasoning.

ParlAI: A Dialog Research Software Platform

  We introduce ParlAI (pronounced "par-lay"), an open-source software platformfor dialog research implemented in Python, available at http://parl.ai. Itsgoal is to provide a unified framework for sharing, training and testing ofdialog models, integration of Amazon Mechanical Turk for data collection, humanevaluation, and online/reinforcement learning; and a repository of machinelearning models for comparing with others' models, and improving upon existingarchitectures. Over 20 tasks are supported in the first release, includingpopular datasets such as SQuAD, bAbI tasks, MCTest, WikiQA, QACNN, QADailyMail,CBT, bAbI Dialog, Ubuntu, OpenSubtitles and VQA. Several models are integrated,including neural models such as memory networks, seq2seq and attentive LSTMs.

Bidirectional Beam Search: Forward-Backward Inference in Neural Sequence  Models for Fill-in-the-Blank Image Captioning

  We develop the first approximate inference algorithm for 1-Best (and M-Best)decoding in bidirectional neural sequence models by extending Beam Search (BS)to reason about both forward and backward time dependencies. Beam Search (BS)is a widely used approximate inference algorithm for decoding sequences fromunidirectional neural sequence models. Interestingly, approximate inference inbidirectional models remains an open problem, despite their significantadvantage in modeling information from both the past and future. To enable theuse of bidirectional models, we present Bidirectional Beam Search (BiBS), anefficient algorithm for approximate bidirectional inference.To evaluate ourmethod and as an interesting problem in its own right, we introduce a novelFill-in-the-Blank Image Captioning task which requires reasoning about bothpast and future sentence structure to reconstruct sensible image descriptions.We use this task as well as the Visual Madlibs dataset to demonstrate theeffectiveness of our approach, consistently outperforming all baseline methods.

Deal or No Deal? End-to-End Learning for Negotiation Dialogues

  Much of human dialogue occurs in semi-cooperative settings, where agents withdifferent goals attempt to agree on common decisions. Negotiations requirecomplex communication and reasoning skills, but success is easy to measure,making this an interesting task for AI. We gather a large dataset ofhuman-human negotiations on a multi-issue bargaining task, where agents whocannot observe each other's reward functions must reach an agreement (or adeal) via natural language dialogue. For the first time, we show it is possibleto train end-to-end models for negotiation, which must learn both linguisticand reasoning skills with no annotated dialogue states. We also introducedialogue rollouts, in which the model plans ahead by simulating possiblecomplete continuations of the conversation, and find that this techniquedramatically improves performance. Our code and dataset are publicly available(https://github.com/facebookresearch/end-to-end-negotiator).

Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog

  A number of recent works have proposed techniques for end-to-end learning ofcommunication protocols among cooperative multi-agent populations, and havesimultaneously found the emergence of grounded human-interpretable language inthe protocols developed by the agents, all learned without any humansupervision!  In this paper, using a Task and Tell reference game between two agents as atestbed, we present a sequence of 'negative' results culminating in a'positive' one -- showing that while most agent-invented languages areeffective (i.e. achieve near-perfect task rewards), they are decidedly notinterpretable or compositional.  In essence, we find that natural language does not emerge 'naturally',despite the semblance of ease of natural-language-emergence that one may gatherfrom recent literature. We discuss how it is possible to coax the inventedlanguages to become more and more human-like and compositional by increasingrestrictions on how two agents may communicate.

Embodied Question Answering

  We present a new AI task -- Embodied Question Answering (EmbodiedQA) -- wherean agent is spawned at a random location in a 3D environment and asked aquestion ("What color is the car?"). In order to answer, the agent must firstintelligently navigate to explore the environment, gather information throughfirst-person (egocentric) vision, and then answer the question ("orange").  This challenging task requires a range of AI skills -- active perception,language understanding, goal-driven navigation, commonsense reasoning, andgrounding of language into actions. In this work, we develop the environments,end-to-end-trained reinforcement learning agents, and evaluation protocols forEmbodiedQA.

Neural Baby Talk

  We introduce a novel framework for image captioning that can produce naturallanguage explicitly grounded in entities that object detectors find in theimage. Our approach reconciles classical slot filling approaches (that aregenerally better grounded in images) with modern neural captioning approaches(that are generally more natural sounding and accurate). Our approach firstgenerates a sentence `template' with slot locations explicitly tied to specificimage regions. These slots are then filled in by visual concepts identified inthe regions by object detectors. The entire architecture (sentence templategeneration and slot filling with object detectors) is end-to-enddifferentiable. We verify the effectiveness of our proposed model on differentimage captioning tasks. On standard image captioning and novel objectcaptioning, our model reaches state-of-the-art on both COCO and Flickr30kdatasets. We also demonstrate that our model has unique advantages when thetrain and test distributions of scene compositions -- and hence language priorsof associated captions -- are different. Code has been made available at:https://github.com/jiasenlu/NeuralBabyTalk

Audio Visual Scene-Aware Dialog (AVSD) Challenge at DSTC7

  Scene-aware dialog systems will be able to have conversations with usersabout the objects and events around them. Progress on such systems can be madeby integrating state-of-the-art technologies from multiple research areasincluding end-to-end dialog systems visual dialog, and video description. Weintroduce the Audio Visual Scene Aware Dialog (AVSD) challenge and dataset. Inthis challenge, which is one track of the 7th Dialog System TechnologyChallenges (DSTC7) workshop1, the task is to build a system that generatesresponses in a dialog about an input video

Learn from Your Neighbor: Learning Multi-modal Mappings from Sparse  Annotations

  Many structured prediction problems (particularly in vision and languagedomains) are ambiguous, with multiple outputs being correct for an input - e.g.there are many ways of describing an image, multiple ways of translating asentence; however, exhaustively annotating the applicability of all possibleoutputs is intractable due to exponentially large output spaces (e.g. allEnglish sentences). In practice, these problems are cast as multi-classprediction, with the likelihood of only a sparse set of annotations beingmaximized - unfortunately penalizing for placing beliefs on plausible butunannotated outputs. We make and test the following hypothesis - for a giveninput, the annotations of its neighbors may serve as an additional supervisorysignal. Specifically, we propose an objective that transfers supervision fromneighboring examples. We first study the properties of our developed method ina controlled toy setup before reporting results on multi-label classificationand two image-grounded sequence modeling tasks - captioning and questiongeneration. We evaluate using standard task-specific metrics and measures ofoutput diversity, finding consistent improvements over standard maximumlikelihood training and other baselines.

Talk the Walk: Navigating New York City through Grounded Dialogue

  We introduce "Talk The Walk", the first large-scale dialogue dataset groundedin action and perception. The task involves two agents (a "guide" and a"tourist") that communicate via natural language in order to achieve a commongoal: having the tourist navigate to a given target location. The task anddataset, which are described in detail, are challenging and their full solutionis an open problem that we pose to the community. We (i) focus on the task oftourist localization and develop the novel Masked Attention for SpatialConvolutions (MASC) mechanism that allows for grounding tourist utterances intothe guide's map, (ii) show it yields significant improvements for both emergentand natural language communication, and (iii) using this method, we establishnon-trivial baselines on the full task.

Pythia v0.1: the Winning Entry to the VQA Challenge 2018

  This document describes Pythia v0.1, the winning entry from Facebook AIResearch (FAIR)'s A-STAR team to the VQA Challenge 2018.  Our starting point is a modular re-implementation of the bottom-up top-down(up-down) model. We demonstrate that by making subtle but important changes tothe model architecture and the learning rate schedule, fine-tuning imagefeatures, and adding data augmentation, we can significantly improve theperformance of the up-down model on VQA v2.0 dataset -- from 65.67% to 70.22%.  Furthermore, by using a diverse ensemble of models trained with differentfeatures and on different datasets, we are able to significantly improve overthe 'standard' way of ensembling (i.e. same model with different random seeds)by 1.31%. Overall, we achieve 72.27% on the test-std split of the VQA v2.0dataset. Our code in its entirety (training, evaluation, data-augmentation,ensembling) and pre-trained models are publicly available at:https://github.com/facebookresearch/pythia

Choose Your Neuron: Incorporating Domain Knowledge through  Neuron-Importance

  Individual neurons in convolutional neural networks supervised forimage-level classification tasks have been shown to implicitly learnsemantically meaningful concepts ranging from simple textures and shapes towhole or partial objects - forming a "dictionary" of concepts acquired throughthe learning process. In this work we introduce a simple, efficient zero-shotlearning approach based on this observation. Our approach, which we call NeuronImportance-AwareWeight Transfer (NIWT), learns to map domain knowledge aboutnovel "unseen" classes onto this dictionary of learned concepts and thenoptimizes for network parameters that can effectively combine these concepts -essentially learning classifiers by discovering and composing learned semanticconcepts in deep networks. Our approach shows improvements over previousapproaches on the CUBirds and AWA2 generalized zero-shot learning benchmarks.We demonstrate our approach on a diverse set of semantic inputs as externaldomain knowledge including attributes and natural language captions. Moreoverby learning inverse mappings, NIWT can provide visual and textual explanationsfor the predictions made by the newly learned classifiers and provide neuronnames. Our code is available athttps://github.com/ramprs/neuron-importance-zsl.

Neural Modular Control for Embodied Question Answering

  We present a modular approach for learning policies for navigation over longplanning horizons from language input. Our hierarchical policy operates atmultiple timescales, where the higher-level master policy proposes subgoals tobe executed by specialized sub-policies. Our choice of subgoals iscompositional and semantic, i.e. they can be sequentially combined in arbitraryorderings, and assume human-interpretable descriptions (e.g. 'exit room', 'findkitchen', 'find refrigerator', etc.).  We use imitation learning to warm-start policies at each level of thehierarchy, dramatically increasing sample efficiency, followed by reinforcementlearning. Independent reinforcement learning at each level of hierarchy enablessub-policies to adapt to consequences of their actions and recover from errors.Subsequent joint hierarchical training enables the master policy to adapt tothe sub-policies.

TarMAC: Targeted Multi-Agent Communication

  We explore a collaborative multi-agent reinforcement learning setting where ateam of agents attempts to solve cooperative tasks in partially-observableenvironments. In this scenario, learning an effective communication protocol iskey. We propose a communication architecture that allows for targetedcommunication, where agents learn both what messages to send and who to sendthem to, solely from downstream task-specific reward without any communicationsupervision. Additionally, we introduce a multi-stage communication approachwhere the agents co-ordinate via multiple rounds of communication before takingactions in the environment. We evaluate our approach on a diverse set ofcooperative multi-agent tasks, of varying difficulties, with varying number ofagents, in a variety of environments ranging from 2D grid layouts of shapes andsimulated traffic junctions to complex 3D indoor environments. We demonstratethe benefits of targeted as well as multi-stage communication. Moreover, weshow that the targeted communication strategies learned by agents are bothinterpretable and intuitive.

Dialog System Technology Challenge 7

  This paper introduces the Seventh Dialog System Technology Challenges (DSTC),which use shared datasets to explore the problem of building dialog systems.Recently, end-to-end dialog modeling approaches have been applied to variousdialog tasks. The seventh DSTC (DSTC7) focuses on developing technologiesrelated to end-to-end dialog systems for (1) sentence selection, (2) sentencegeneration and (3) audio visual scene aware dialog. This paper summarizes theoverall setup and results of DSTC7, including detailed descriptions of thedifferent tracks and provided datasets. We also describe overall trends in thesubmitted systems and the key results. Each track introduced new datasets andparticipants achieved impressive results using state-of-the-art end-to-endtechnologies.

Audio-Visual Scene-Aware Dialog

  We introduce the task of scene-aware dialog. Given a follow-up question in anongoing dialog about a video, our goal is to generate a complete and naturalresponse to a question given (a) an input video, and (b) the history ofprevious turns in the dialog. To succeed, agents must ground the semantics inthe video and leverage contextual cues from the history of the dialog to answerthe question. To benchmark this task, we introduce the Audio Visual Scene-AwareDialog (AVSD) dataset. For each of more than 11,000 videos of human actions forthe Charades dataset. Our dataset contains a dialog about the video, plus afinal summary of the video by one of the dialog participants. We train severalbaseline systems for this task and evaluate the performance of the trainedmodels using several qualitative and quantitative metrics. Our results indicatethat the models must comprehend all the available inputs (video, audio,question and dialog history) to perform well on this dataset.

Embodied Multimodal Multitask Learning

  Recent efforts on training visual navigation agents conditioned on languageusing deep reinforcement learning have been successful in learning policies fordifferent multimodal tasks, such as semantic goal navigation and embodiedquestion answering. In this paper, we propose a multitask model capable ofjointly learning these multimodal tasks, and transferring knowledge of wordsand their grounding in visual objects across the tasks. The proposed model usesa novel Dual-Attention unit to disentangle the knowledge of words in thetextual representations and visual concepts in the visual representations, andalign them with each other. This disentangled task-invariant alignment ofrepresentations facilitates grounding and knowledge transfer across both tasks.We show that the proposed model outperforms a range of baselines on both tasksin simulated 3D environments. We also show that this disentanglement ofrepresentations makes our model modular, interpretable, and allows for transferto instructions containing new words by leveraging object detectors.

EvalAI: Towards Better Evaluation Systems for AI Agents

  We introduce EvalAI, an open source platform for evaluating and comparingmachine learning (ML) and artificial intelligence algorithms (AI) at scale.EvalAI is built to provide a scalable solution to the research community tofulfill the critical need of evaluating machine learning models and agentsacting in an environment against annotations or with a human-in-the-loop. Thiswill help researchers, students, and data scientists to create, collaborate,and participate in AI challenges organized around the globe. By simplifying andstandardizing the process of benchmarking these models, EvalAI seeks to lowerthe barrier to entry for participating in the global scientific effort to pushthe frontiers of machine learning and artificial intelligence, therebyincreasing the rate of measurable progress in this domain.

Taking a HINT: Leveraging Explanations to Make Vision and Language  Models More Grounded

  Many vision and language models suffer from poor visual grounding - oftenfalling back on easy-to-learn language priors rather than associating languagewith visual concepts. In this work, we propose a generic framework which wecall Human Importance-aware Network Tuning (HINT) that effectively leverageshuman supervision to improve visual grounding. HINT constrains deep networks tobe sensitive to the same input regions as humans. Crucially, our approachoptimizes the alignment between human attention maps and gradient-based networkimportances - ensuring that models learn not just to look at but rather rely onvisual concepts that humans found relevant for a task when making predictions.We demonstrate our approach on Visual Question Answering and Image Captioningtasks, achieving state of-the-art for the VQA-CP dataset which penalizesover-reliance on language priors.

Probabilistic Neural-symbolic Models for Interpretable Visual Question  Answering

  We propose a new class of probabilistic neural-symbolic models, that havesymbolic functional programs as a latent, stochastic variable. Instantiated inthe context of visual question answering, our probabilistic formulation offerstwo key conceptual advantages over prior neural-symbolic models for VQA.Firstly, the programs generated by our model are more understandable whilerequiring lesser number of teaching examples. Secondly, we show that one canpose counterfactual scenarios to the model, to probe its beliefs on theprograms that could lead to a specified answer given an image. Our results onthe CLEVR and SHAPES datasets verify our hypotheses, showing that the modelgets better program (and answer) prediction accuracy even in the low dataregime, and allows one to probe the coherence and consistency of reasoningperformed.

Learning Dynamics Model in Reinforcement Learning by Incorporating the  Long Term Future

  In model-based reinforcement learning, the agent interleaves between modellearning and planning. These two components are inextricably intertwined. Ifthe model is not able to provide sensible long-term prediction, the executedplanner would exploit model flaws, which can yield catastrophic failures. Thispaper focuses on building a model that reasons about the long-term future anddemonstrates how to use this for efficient planning and exploration. To thisend, we build a latent-variable autoregressive model by leveraging recent ideasin variational inference. We argue that forcing latent variables to carryfuture information through an auxiliary task substantially improves long-termpredictions. Moreover, by planning in the latent space, the planner's solutionis ensured to be within regions where the model is valid. An explorationstrategy can be devised by searching for unlikely trajectories under the model.Our method achieves higher reward faster compared to baselines on a variety oftasks and environments in both the imitation learning and model-basedreinforcement learning settings.

Embodied Question Answering in Photorealistic Environments with Point  Cloud Perception

  To help bridge the gap between internet vision-style problems and the goal ofvision for embodied perception we instantiate a large-scale navigation task --Embodied Question Answering [1] in photo-realistic environments (Matterport3D). We thoroughly study navigation policies that utilize 3D point clouds, RGBimages, or their combination. Our analysis of these models reveals several keyfindings. We find that two seemingly naive navigation baselines, forward-onlyand random, are strong navigators and challenging to outperform, due to thespecific choice of the evaluation setting presented by [1]. We find a novelloss-weighting scheme we call Inflection Weighting to be important whentraining recurrent models for navigation with behavior cloning and are able toout perform the baselines with this technique. We find that point cloudsprovide a richer signal than RGB images for learning obstacle avoidance,motivating the use (and continued study) of 3D deep learning models forembodied navigation.

Embodied Visual Recognition

  Passive visual systems typically fail to recognize objects in the amodalsetting where they are heavily occluded. In contrast, humans and other embodiedagents have the ability to move in the environment, and actively control theviewing angle to better understand object shapes and semantics. In this work,we introduce the task of Embodied Visual Recognition (EVR): An agent isinstantiated in a 3D environment close to an occluded target object, and isfree to move in the environment to perform object classification, amodal objectlocalization, and amodal object segmentation. To address this, we develop a newmodel called Embodied Mask R-CNN, for agents to learn to move strategically toimprove their visual recognition abilities. We conduct experiments using theHouse3D environment. Experimental results show that: 1) agents with embodiment(movement) achieve better visual recognition performance than passive ones; 2)in order to improve visual recognition abilities, agents can learn strategicalmoving paths that are different from shortest paths.

