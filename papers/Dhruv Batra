Response to "Visual Dialogue without Vision or Dialogue" (Massiceti et
  al., 2018)

  In a recent workshop paper, Massiceti et al. presented a baseline model and
subsequent critique of Visual Dialog (Das et al., CVPR 2017) that raises what
we believe to be unfounded concerns about the dataset and evaluation. This
article intends to rebut the critique and clarify potential confusions for
practitioners and future participants in the Visual Dialog challenge.


An Efficient Message-Passing Algorithm for the M-Best MAP Problem

  Much effort has been directed at algorithms for obtaining the highest
probability configuration in a probabilistic random field model known as the
maximum a posteriori (MAP) inference problem. In many situations, one could
benefit from having not just a single solution, but the top M most probable
solutions known as the M-Best MAP problem. In this paper, we propose an
efficient message-passing based algorithm for solving the M-Best MAP problem.
Specifically, our algorithm solves the recently proposed Linear Programming
(LP) formulation of M-Best MAP [7], while being orders of magnitude faster than
a generic LP-solver. Our approach relies on studying a particular partial
Lagrangian relaxation of the M-Best MAP LP which exposes a natural
combinatorial structure of the problem that we exploit.


Graph R-CNN for Scene Graph Generation

  We propose a novel scene graph generation model called Graph R-CNN, that is
both effective and efficient at detecting objects and their relations in
images. Our model contains a Relation Proposal Network (RePN) that efficiently
deals with the quadratic number of potential relations between objects in an
image. We also propose an attentional Graph Convolutional Network (aGCN) that
effectively captures contextual information between objects and relations.
Finally, we introduce a new evaluation metric that is more holistic and
realistic than existing metrics. We report state-of-the-art performance on
scene graph generation as evaluated using both existing and our proposed
metrics.


Fabrik: An Online Collaborative Neural Network Editor

  We present Fabrik, an online neural network editor that provides tools to
visualize, edit, and share neural networks from within a browser. Fabrik
provides a simple and intuitive GUI to import neural networks written in
popular deep learning frameworks such as Caffe, Keras, and TensorFlow, and
allows users to interact with, build, and edit models via simple drag and drop.
Fabrik is designed to be framework agnostic and support high interoperability,
and can be used to export models back to any supported framework. Finally, it
provides powerful collaborative features to enable users to iterate over model
design remotely and at scale.


Candidate Constrained CRFs for Loss-Aware Structured Prediction

  When evaluating computer vision systems, we are often concerned with
performance on a task-specific evaluation measure such as the
Intersection-Over-Union score used in the PASCAL VOC image segmentation
challenge. Ideally, our systems would be tuned specifically to these evaluation
measures. However, despite much work on loss-aware structured prediction, top
performing systems do not use these techniques. In this work, we seek to
address this problem, incorporating loss-aware prediction in a manner that is
amenable to the approaches taken by top performing systems. Our main idea is to
simultaneously leverage two systems: a highly tuned pipeline system as is found
on top of leaderboards, and a traditional CRF. We show how to combine high
quality candidate solutions from the pipeline with the probabilistic approach
of the CRF that is amenable to loss-aware prediction. The result is that we can
use loss-aware prediction methodology to improve performance of the highly
tuned pipeline system.


Combining the Best of Graphical Models and ConvNets for Semantic
  Segmentation

  We present a two-module approach to semantic segmentation that incorporates
Convolutional Networks (CNNs) and Graphical Models. Graphical models are used
to generate a small (5-30) set of diverse segmentations proposals, such that
this set has high recall. Since the number of required proposals is so low, we
can extract fairly complex features to rank them. Our complex feature of choice
is a novel CNN called SegNet, which directly outputs a (coarse) semantic
segmentation. Importantly, SegNet is specifically trained to optimize the
corpus-level PASCAL IOU loss function. To the best of our knowledge, this is
the first CNN specifically designed for semantic segmentation. This two-module
approach achieves $52.5\%$ on the PASCAL 2012 segmentation challenge.


Hierarchical Question-Image Co-Attention for Visual Question Answering

  A number of recent works have proposed attention models for Visual Question
Answering (VQA) that generate spatial maps highlighting image regions relevant
to answering the question. In this paper, we argue that in addition to modeling
"where to look" or visual attention, it is equally important to model "what
words to listen to" or question attention. We present a novel co-attention
model for VQA that jointly reasons about image and question attention. In
addition, our model reasons about the question (and consequently the image via
the co-attention mechanism) in a hierarchical fashion via a novel 1-dimensional
convolution neural networks (CNN). Our model improves the state-of-the-art on
the VQA dataset from 60.3% to 60.5%, and from 61.6% to 63.3% on the COCO-QA
dataset. By using ResNet, the performance is further improved to 62.1% for VQA
and 65.4% for COCO-QA.


Human Attention in Visual Question Answering: Do Humans and Deep
  Networks Look at the Same Regions?

  We conduct large-scale studies on `human attention' in Visual Question
Answering (VQA) to understand where humans choose to look to answer questions
about images. We design and test multiple game-inspired novel
attention-annotation interfaces that require the subject to sharpen regions of
a blurred image to answer a question. Thus, we introduce the VQA-HAT (Human
ATtention) dataset. We evaluate attention maps generated by state-of-the-art
VQA models against human attention both qualitatively (via visualizations) and
quantitatively (via rank-order correlation). Overall, our experiments show that
current attention models in VQA do not seem to be looking at the same regions
as humans.


Human Attention in Visual Question Answering: Do Humans and Deep
  Networks Look at the Same Regions?

  We conduct large-scale studies on `human attention' in Visual Question
Answering (VQA) to understand where humans choose to look to answer questions
about images. We design and test multiple game-inspired novel
attention-annotation interfaces that require the subject to sharpen regions of
a blurred image to answer a question. Thus, we introduce the VQA-HAT (Human
ATtention) dataset. We evaluate attention maps generated by state-of-the-art
VQA models against human attention both qualitatively (via visualizations) and
quantitatively (via rank-order correlation). Overall, our experiments show that
current attention models in VQA do not seem to be looking at the same regions
as humans.


Question Relevance in VQA: Identifying Non-Visual And False-Premise
  Questions

  Visual Question Answering (VQA) is the task of answering natural-language
questions about images. We introduce the novel problem of determining the
relevance of questions to images in VQA. Current VQA models do not reason about
whether a question is even related to the given image (e.g. What is the capital
of Argentina?) or if it requires information from external resources to answer
correctly. This can break the continuity of a dialogue in human-machine
interaction. Our approaches for determining relevance are composed of two
stages. Given an image and a question, (1) we first determine whether the
question is visual or not, (2) if visual, we determine whether the question is
relevant to the given image or not. Our approaches, based on LSTM-RNNs, VQA
model uncertainty, and caption-question similarity, are able to outperform
strong baselines on both relevance tasks. We also present human studies showing
that VQA models augmented with such question relevance reasoning are perceived
as more intelligent, reasonable, and human-like.


Analyzing the Behavior of Visual Question Answering Models

  Recently, a number of deep-learning based models have been proposed for the
task of Visual Question Answering (VQA). The performance of most models is
clustered around 60-70%. In this paper we propose systematic methods to analyze
the behavior of these models as a first step towards recognizing their
strengths and weaknesses, and identifying the most fruitful directions for
progress. We analyze two models, one each from two major classes of VQA models
-- with-attention and without-attention and show the similarities and
differences in the behavior of these models. We also analyze the winning entry
of the VQA Challenge 2016.
  Our behavior analysis reveals that despite recent progress, today's VQA
models are "myopic" (tend to fail on sufficiently novel instances), often "jump
to conclusions" (converge on a predicted answer after 'listening' to just half
the question), and are "stubborn" (do not change their answers across images).


Sort Story: Sorting Jumbled Images and Captions into Stories

  Temporal common sense has applications in AI tasks such as QA, multi-document
summarization, and human-AI communication. We propose the task of sequencing --
given a jumbled set of aligned image-caption pairs that belong to a story, the
task is to sort them such that the output sequence forms a coherent story. We
present multiple approaches, via unary (position) and pairwise (order)
predictions, and their ensemble-based combinations, achieving strong results on
this task. We use both text-based and image-based features, which depict
complementary improvements. Using qualitative examples, we demonstrate that our
models have learnt interesting aspects of temporal common sense.


Stochastic Multiple Choice Learning for Training Diverse Deep Ensembles

  Many practical perception systems exist within larger processes that include
interactions with users or additional components capable of evaluating the
quality of predicted solutions. In these contexts, it is beneficial to provide
these oracle mechanisms with multiple highly likely hypotheses rather than a
single prediction. In this work, we pose the task of producing multiple outputs
as a learning problem over an ensemble of deep networks -- introducing a novel
stochastic gradient descent based approach to minimize the loss with respect to
an oracle. Our method is simple to implement, agnostic to both architecture and
loss function, and parameter-free. Our approach achieves lower oracle error
compared to existing methods on a wide range of tasks and deep architectures.
We also show qualitatively that the diverse solutions produced often provide
interpretable representations of task ambiguity.


Reducing Overfitting in Deep Networks by Decorrelating Representations

  One major challenge in training Deep Neural Networks is preventing
overfitting. Many techniques such as data augmentation and novel regularizers
such as Dropout have been proposed to prevent overfitting without requiring a
massive amount of training data. In this work, we propose a new regularizer
called DeCov which leads to significantly reduced overfitting (as indicated by
the difference between train and val performance), and better generalization.
Our regularizer encourages diverse or non-redundant representations in Deep
Neural Networks by minimizing the cross-covariance of hidden activations. This
simple intuition has been explored in a number of past works but surprisingly
has never been applied as a regularizer in supervised learning. Experiments
across a range of datasets and network architectures show that this loss always
reduces overfitting while almost always maintaining or increasing
generalization performance and often improving performance over Dropout.


Why M Heads are Better than One: Training a Diverse Ensemble of Deep
  Networks

  Convolutional Neural Networks have achieved state-of-the-art performance on a
wide range of tasks. Most benchmarks are led by ensembles of these powerful
learners, but ensembling is typically treated as a post-hoc procedure
implemented by averaging independently trained models with model variation
induced by bagging or random initialization. In this paper, we rigorously treat
ensembling as a first-class problem to explicitly address the question: what
are the best strategies to create an ensemble? We first compare a large number
of ensembling strategies, and then propose and evaluate novel strategies, such
as parameter sharing (through a new family of models we call TreeNets) as well
as training under ensemble-aware and diversity-encouraging losses. We
demonstrate that TreeNets can improve ensemble performance and that diverse
ensembles can be trained end-to-end under a unified loss, achieving
significantly higher "oracle" accuracies than classical ensembles.


C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0
  Dataset

  Visual Question Answering (VQA) has received a lot of attention over the past
couple of years. A number of deep learning models have been proposed for this
task. However, it has been shown that these models are heavily driven by
superficial correlations in the training data and lack compositionality -- the
ability to answer questions about unseen compositions of seen concepts. This
compositionality is desirable and central to intelligence. In this paper, we
propose a new setting for Visual Question Answering where the test
question-answer pairs are compositionally novel compared to training
question-answer pairs. To facilitate developing models under this setting, we
present a new compositional split of the VQA v1.0 dataset, which we call
Compositional VQA (C-VQA). We analyze the distribution of questions and answers
in the C-VQA splits. Finally, we evaluate several existing VQA models under
this new setting and show that the performances of these models degrade by a
significant amount compared to the original VQA setting.


Submodular meets Structured: Finding Diverse Subsets in
  Exponentially-Large Structured Item Sets

  To cope with the high level of ambiguity faced in domains such as Computer
Vision or Natural Language processing, robust prediction methods often search
for a diverse set of high-quality candidate solutions or proposals. In
structured prediction problems, this becomes a daunting task, as the solution
space (image labelings, sentence parses, etc.) is exponentially large. We study
greedy algorithms for finding a diverse subset of solutions in
structured-output spaces by drawing new connections between submodular
functions over combinatorial item sets and High-Order Potentials (HOPs) studied
for graphical models. Specifically, we show via examples that when marginal
gains of submodular diversity functions allow structured representations, this
enables efficient (sub-linear time) approximate maximization by reducing the
greedy augmentation step to inference in a factor graph with appropriately
constructed HOPs. We discuss benefits, tradeoffs, and show that our
constructions lead to significantly better proposals.


VIP: Finding Important People in Images

  People preserve memories of events such as birthdays, weddings, or vacations
by capturing photos, often depicting groups of people. Invariably, some
individuals in the image are more important than others given the context of
the event. This paper analyzes the concept of the importance of individuals in
group photographs. We address two specific questions -- Given an image, who are
the most important individuals in it? Given multiple images of a person, which
image depicts the person in the most important role? We introduce a measure of
importance of people in images and investigate the correlation between
importance and visual saliency. We find that not only can we automatically
predict the importance of people from purely visual cues, incorporating this
predicted importance results in significant improvement in applications such as
im2text (generating sentences that describe images of groups of people).


CloudCV: Large Scale Distributed Computer Vision as a Cloud Service

  We are witnessing a proliferation of massive visual data. Unfortunately
scaling existing computer vision algorithms to large datasets leaves
researchers repeatedly solving the same algorithmic, logistical, and
infrastructural problems. Our goal is to democratize computer vision; one
should not have to be a computer vision, big data and distributed computing
expert to have access to state-of-the-art distributed computer vision
algorithms. We present CloudCV, a comprehensive system to provide access to
state-of-the-art distributed computer vision algorithms as a cloud service
through a Web Interface and APIs.


Counting Everyday Objects in Everyday Scenes

  We are interested in counting the number of instances of object classes in
natural, everyday images. Previous counting approaches tackle the problem in
restricted domains such as counting pedestrians in surveillance videos. Counts
can also be estimated from outputs of other vision tasks like object detection.
In this work, we build dedicated models for counting designed to tackle the
large variance in counts, appearances, and scales of objects found in natural
scenes. Our approach is inspired by the phenomenon of subitizing - the ability
of humans to make quick assessments of counts given a perceptual signal, for
small count values. Given a natural scene, we employ a divide and conquer
strategy while incorporating context across the scene to adapt the subitizing
idea to counting. Our approach offers consistent improvements over numerous
baseline approaches for counting on the PASCAL VOC 2007 and COCO datasets.
Subsequently, we study how counting can be used to improve object detection. We
then show a proof of concept application of our counting methods to the task of
Visual Question Answering, by studying the `how many?' questions in the VQA and
COCO-QA datasets.


Joint Unsupervised Learning of Deep Representations and Image Clusters

  In this paper, we propose a recurrent framework for Joint Unsupervised
LEarning (JULE) of deep representations and image clusters. In our framework,
successive operations in a clustering algorithm are expressed as steps in a
recurrent process, stacked on top of representations output by a Convolutional
Neural Network (CNN). During training, image clusters and representations are
updated jointly: image clustering is conducted in the forward pass, while
representation learning in the backward pass. Our key idea behind this
framework is that good representations are beneficial to image clustering and
clustering results provide supervisory signals to representation learning. By
integrating two processes into a single model with a unified weighted triplet
loss and optimizing it end-to-end, we can obtain not only more powerful
representations, but also more precise image clusters. Extensive experiments
show that our method outperforms the state-of-the-art on image clustering
across a variety of image datasets. Moreover, the learned representations
generalize well when transferred to other tasks.


Visual Storytelling

  We introduce the first dataset for sequential vision-to-language, and explore
how this data may be used for the task of visual storytelling. The first
release of this dataset, SIND v.1, includes 81,743 unique photos in 20,211
sequences, aligned to both descriptive (caption) and story language. We
establish several strong baselines for the storytelling task, and motivate an
automatic metric to benchmark progress. Modelling concrete description as well
as figurative and social language, as provided in this dataset and the
storytelling task, has the potential to move artificial intelligence from basic
understandings of typical visual scenes towards more and more human-like
understanding of grounded event structure and subjective expression.


Radio Transformer Networks: Attention Models for Learning to Synchronize
  in Wireless Systems

  We introduce learned attention models into the radio machine learning domain
for the task of modulation recognition by leveraging spatial transformer
networks and introducing new radio domain appropriate transformations. This
attention model allows the network to learn a localization network capable of
synchronizing and normalizing a radio signal blindly with zero knowledge of the
signals structure based on optimization of the network for classification
accuracy, sparse representation, and regularization. Using this architecture we
are able to outperform our prior results in accuracy vs signal to noise ratio
against an identical system without attention, however we believe such an
attention model has implication far beyond the task of modulation recognition.


Measuring Machine Intelligence Through Visual Question Answering

  As machines have become more intelligent, there has been a renewed interest
in methods for measuring their intelligence. A common approach is to propose
tasks for which a human excels, but one which machines find difficult. However,
an ideal task should also be easy to evaluate and not be easily gameable. We
begin with a case study exploring the recently popular task of image captioning
and its limitations as a task for measuring machine intelligence. An
alternative and more promising task is Visual Question Answering that tests a
machine's ability to reason about language and vision. We describe a dataset
unprecedented in size created for the task that contains over 760,000 human
generated questions about images. Using around 10 million human generated
answers, machines may be easily evaluated.


Towards Transparent AI Systems: Interpreting Visual Question Answering
  Models

  Deep neural networks have shown striking progress and obtained
state-of-the-art results in many AI research fields in the recent years.
However, it is often unsatisfying to not know why they predict what they do. In
this paper, we address the problem of interpreting Visual Question Answering
(VQA) models. Specifically, we are interested in finding what part of the input
(pixels in images or words in questions) the VQA model focuses on while
answering the question. To tackle this problem, we use two visualization
techniques -- guided backpropagation and occlusion -- to find important words
in the question and important regions in the image. We then present qualitative
and quantitative analyses of these importance maps. We found that even without
explicit attention mechanisms, VQA models may sometimes be implicitly attending
to relevant regions in the image, and often to appropriate words in the
question.


Grad-CAM: Why did you say that?

  We propose a technique for making Convolutional Neural Network (CNN)-based
models more transparent by visualizing input regions that are 'important' for
predictions -- or visual explanations. Our approach, called Gradient-weighted
Class Activation Mapping (Grad-CAM), uses class-specific gradient information
to localize important regions. These localizations are combined with existing
pixel-space visualizations to create a novel high-resolution and
class-discriminative visualization called Guided Grad-CAM. These methods help
better understand CNN-based models, including image captioning and visual
question answering (VQA) models. We evaluate our visual explanations by
measuring their ability to discriminate between classes, to inspire trust in
humans, and their correlation with occlusion maps. Grad-CAM provides a new way
to understand CNN-based models.
  We have released code, an online demo hosted on CloudCV, and a full version
of this extended abstract.


LR-GAN: Layered Recursive Generative Adversarial Networks for Image
  Generation

  We present LR-GAN: an adversarial image generation model which takes scene
structure and context into account. Unlike previous generative adversarial
networks (GANs), the proposed GAN learns to generate image background and
foregrounds separately and recursively, and stitch the foregrounds on the
background in a contextually relevant manner to produce a complete natural
image. For each foreground, the model learns to generate its appearance, shape
and pose. The whole model is unsupervised, and is trained in an end-to-end
manner with gradient descent methods. The experiments demonstrate that LR-GAN
can generate more natural images with objects that are more human recognizable
than DCGAN.


The Promise of Premise: Harnessing Question Premises in Visual Question
  Answering

  In this paper, we make a simple observation that questions about images often
contain premises - objects and relationships implied by the question - and that
reasoning about premises can help Visual Question Answering (VQA) models
respond more intelligently to irrelevant or previously unseen questions. When
presented with a question that is irrelevant to an image, state-of-the-art VQA
models will still answer purely based on learned language biases, resulting in
non-sensical or even misleading answers. We note that a visual question is
irrelevant to an image if at least one of its premises is false (i.e. not
depicted in the image). We leverage this observation to construct a dataset for
Question Relevance Prediction and Explanation (QRPE) by searching for false
premises. We train novel question relevance detection models and show that
models that reason about premises consistently outperform models that do not.
We also find that forcing standard VQA models to reason about premises during
training can lead to improvements on tasks requiring compositional reasoning.


ParlAI: A Dialog Research Software Platform

  We introduce ParlAI (pronounced "par-lay"), an open-source software platform
for dialog research implemented in Python, available at http://parl.ai. Its
goal is to provide a unified framework for sharing, training and testing of
dialog models, integration of Amazon Mechanical Turk for data collection, human
evaluation, and online/reinforcement learning; and a repository of machine
learning models for comparing with others' models, and improving upon existing
architectures. Over 20 tasks are supported in the first release, including
popular datasets such as SQuAD, bAbI tasks, MCTest, WikiQA, QACNN, QADailyMail,
CBT, bAbI Dialog, Ubuntu, OpenSubtitles and VQA. Several models are integrated,
including neural models such as memory networks, seq2seq and attentive LSTMs.


Bidirectional Beam Search: Forward-Backward Inference in Neural Sequence
  Models for Fill-in-the-Blank Image Captioning

  We develop the first approximate inference algorithm for 1-Best (and M-Best)
decoding in bidirectional neural sequence models by extending Beam Search (BS)
to reason about both forward and backward time dependencies. Beam Search (BS)
is a widely used approximate inference algorithm for decoding sequences from
unidirectional neural sequence models. Interestingly, approximate inference in
bidirectional models remains an open problem, despite their significant
advantage in modeling information from both the past and future. To enable the
use of bidirectional models, we present Bidirectional Beam Search (BiBS), an
efficient algorithm for approximate bidirectional inference.To evaluate our
method and as an interesting problem in its own right, we introduce a novel
Fill-in-the-Blank Image Captioning task which requires reasoning about both
past and future sentence structure to reconstruct sensible image descriptions.
We use this task as well as the Visual Madlibs dataset to demonstrate the
effectiveness of our approach, consistently outperforming all baseline methods.


Deal or No Deal? End-to-End Learning for Negotiation Dialogues

  Much of human dialogue occurs in semi-cooperative settings, where agents with
different goals attempt to agree on common decisions. Negotiations require
complex communication and reasoning skills, but success is easy to measure,
making this an interesting task for AI. We gather a large dataset of
human-human negotiations on a multi-issue bargaining task, where agents who
cannot observe each other's reward functions must reach an agreement (or a
deal) via natural language dialogue. For the first time, we show it is possible
to train end-to-end models for negotiation, which must learn both linguistic
and reasoning skills with no annotated dialogue states. We also introduce
dialogue rollouts, in which the model plans ahead by simulating possible
complete continuations of the conversation, and find that this technique
dramatically improves performance. Our code and dataset are publicly available
(https://github.com/facebookresearch/end-to-end-negotiator).


Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog

  A number of recent works have proposed techniques for end-to-end learning of
communication protocols among cooperative multi-agent populations, and have
simultaneously found the emergence of grounded human-interpretable language in
the protocols developed by the agents, all learned without any human
supervision!
  In this paper, using a Task and Tell reference game between two agents as a
testbed, we present a sequence of 'negative' results culminating in a
'positive' one -- showing that while most agent-invented languages are
effective (i.e. achieve near-perfect task rewards), they are decidedly not
interpretable or compositional.
  In essence, we find that natural language does not emerge 'naturally',
despite the semblance of ease of natural-language-emergence that one may gather
from recent literature. We discuss how it is possible to coax the invented
languages to become more and more human-like and compositional by increasing
restrictions on how two agents may communicate.


Embodied Question Answering

  We present a new AI task -- Embodied Question Answering (EmbodiedQA) -- where
an agent is spawned at a random location in a 3D environment and asked a
question ("What color is the car?"). In order to answer, the agent must first
intelligently navigate to explore the environment, gather information through
first-person (egocentric) vision, and then answer the question ("orange").
  This challenging task requires a range of AI skills -- active perception,
language understanding, goal-driven navigation, commonsense reasoning, and
grounding of language into actions. In this work, we develop the environments,
end-to-end-trained reinforcement learning agents, and evaluation protocols for
EmbodiedQA.


Neural Baby Talk

  We introduce a novel framework for image captioning that can produce natural
language explicitly grounded in entities that object detectors find in the
image. Our approach reconciles classical slot filling approaches (that are
generally better grounded in images) with modern neural captioning approaches
(that are generally more natural sounding and accurate). Our approach first
generates a sentence `template' with slot locations explicitly tied to specific
image regions. These slots are then filled in by visual concepts identified in
the regions by object detectors. The entire architecture (sentence template
generation and slot filling with object detectors) is end-to-end
differentiable. We verify the effectiveness of our proposed model on different
image captioning tasks. On standard image captioning and novel object
captioning, our model reaches state-of-the-art on both COCO and Flickr30k
datasets. We also demonstrate that our model has unique advantages when the
train and test distributions of scene compositions -- and hence language priors
of associated captions -- are different. Code has been made available at:
https://github.com/jiasenlu/NeuralBabyTalk


Audio Visual Scene-Aware Dialog (AVSD) Challenge at DSTC7

  Scene-aware dialog systems will be able to have conversations with users
about the objects and events around them. Progress on such systems can be made
by integrating state-of-the-art technologies from multiple research areas
including end-to-end dialog systems visual dialog, and video description. We
introduce the Audio Visual Scene Aware Dialog (AVSD) challenge and dataset. In
this challenge, which is one track of the 7th Dialog System Technology
Challenges (DSTC7) workshop1, the task is to build a system that generates
responses in a dialog about an input video


Learn from Your Neighbor: Learning Multi-modal Mappings from Sparse
  Annotations

  Many structured prediction problems (particularly in vision and language
domains) are ambiguous, with multiple outputs being correct for an input - e.g.
there are many ways of describing an image, multiple ways of translating a
sentence; however, exhaustively annotating the applicability of all possible
outputs is intractable due to exponentially large output spaces (e.g. all
English sentences). In practice, these problems are cast as multi-class
prediction, with the likelihood of only a sparse set of annotations being
maximized - unfortunately penalizing for placing beliefs on plausible but
unannotated outputs. We make and test the following hypothesis - for a given
input, the annotations of its neighbors may serve as an additional supervisory
signal. Specifically, we propose an objective that transfers supervision from
neighboring examples. We first study the properties of our developed method in
a controlled toy setup before reporting results on multi-label classification
and two image-grounded sequence modeling tasks - captioning and question
generation. We evaluate using standard task-specific metrics and measures of
output diversity, finding consistent improvements over standard maximum
likelihood training and other baselines.


Talk the Walk: Navigating New York City through Grounded Dialogue

  We introduce "Talk The Walk", the first large-scale dialogue dataset grounded
in action and perception. The task involves two agents (a "guide" and a
"tourist") that communicate via natural language in order to achieve a common
goal: having the tourist navigate to a given target location. The task and
dataset, which are described in detail, are challenging and their full solution
is an open problem that we pose to the community. We (i) focus on the task of
tourist localization and develop the novel Masked Attention for Spatial
Convolutions (MASC) mechanism that allows for grounding tourist utterances into
the guide's map, (ii) show it yields significant improvements for both emergent
and natural language communication, and (iii) using this method, we establish
non-trivial baselines on the full task.


Pythia v0.1: the Winning Entry to the VQA Challenge 2018

  This document describes Pythia v0.1, the winning entry from Facebook AI
Research (FAIR)'s A-STAR team to the VQA Challenge 2018.
  Our starting point is a modular re-implementation of the bottom-up top-down
(up-down) model. We demonstrate that by making subtle but important changes to
the model architecture and the learning rate schedule, fine-tuning image
features, and adding data augmentation, we can significantly improve the
performance of the up-down model on VQA v2.0 dataset -- from 65.67% to 70.22%.
  Furthermore, by using a diverse ensemble of models trained with different
features and on different datasets, we are able to significantly improve over
the 'standard' way of ensembling (i.e. same model with different random seeds)
by 1.31%. Overall, we achieve 72.27% on the test-std split of the VQA v2.0
dataset. Our code in its entirety (training, evaluation, data-augmentation,
ensembling) and pre-trained models are publicly available at:
https://github.com/facebookresearch/pythia


Choose Your Neuron: Incorporating Domain Knowledge through
  Neuron-Importance

  Individual neurons in convolutional neural networks supervised for
image-level classification tasks have been shown to implicitly learn
semantically meaningful concepts ranging from simple textures and shapes to
whole or partial objects - forming a "dictionary" of concepts acquired through
the learning process. In this work we introduce a simple, efficient zero-shot
learning approach based on this observation. Our approach, which we call Neuron
Importance-AwareWeight Transfer (NIWT), learns to map domain knowledge about
novel "unseen" classes onto this dictionary of learned concepts and then
optimizes for network parameters that can effectively combine these concepts -
essentially learning classifiers by discovering and composing learned semantic
concepts in deep networks. Our approach shows improvements over previous
approaches on the CUBirds and AWA2 generalized zero-shot learning benchmarks.
We demonstrate our approach on a diverse set of semantic inputs as external
domain knowledge including attributes and natural language captions. Moreover
by learning inverse mappings, NIWT can provide visual and textual explanations
for the predictions made by the newly learned classifiers and provide neuron
names. Our code is available at
https://github.com/ramprs/neuron-importance-zsl.


Neural Modular Control for Embodied Question Answering

  We present a modular approach for learning policies for navigation over long
planning horizons from language input. Our hierarchical policy operates at
multiple timescales, where the higher-level master policy proposes subgoals to
be executed by specialized sub-policies. Our choice of subgoals is
compositional and semantic, i.e. they can be sequentially combined in arbitrary
orderings, and assume human-interpretable descriptions (e.g. 'exit room', 'find
kitchen', 'find refrigerator', etc.).
  We use imitation learning to warm-start policies at each level of the
hierarchy, dramatically increasing sample efficiency, followed by reinforcement
learning. Independent reinforcement learning at each level of hierarchy enables
sub-policies to adapt to consequences of their actions and recover from errors.
Subsequent joint hierarchical training enables the master policy to adapt to
the sub-policies.


TarMAC: Targeted Multi-Agent Communication

  We explore a collaborative multi-agent reinforcement learning setting where a
team of agents attempts to solve cooperative tasks in partially-observable
environments. In this scenario, learning an effective communication protocol is
key. We propose a communication architecture that allows for targeted
communication, where agents learn both what messages to send and who to send
them to, solely from downstream task-specific reward without any communication
supervision. Additionally, we introduce a multi-stage communication approach
where the agents co-ordinate via multiple rounds of communication before taking
actions in the environment. We evaluate our approach on a diverse set of
cooperative multi-agent tasks, of varying difficulties, with varying number of
agents, in a variety of environments ranging from 2D grid layouts of shapes and
simulated traffic junctions to complex 3D indoor environments. We demonstrate
the benefits of targeted as well as multi-stage communication. Moreover, we
show that the targeted communication strategies learned by agents are both
interpretable and intuitive.


Dialog System Technology Challenge 7

  This paper introduces the Seventh Dialog System Technology Challenges (DSTC),
which use shared datasets to explore the problem of building dialog systems.
Recently, end-to-end dialog modeling approaches have been applied to various
dialog tasks. The seventh DSTC (DSTC7) focuses on developing technologies
related to end-to-end dialog systems for (1) sentence selection, (2) sentence
generation and (3) audio visual scene aware dialog. This paper summarizes the
overall setup and results of DSTC7, including detailed descriptions of the
different tracks and provided datasets. We also describe overall trends in the
submitted systems and the key results. Each track introduced new datasets and
participants achieved impressive results using state-of-the-art end-to-end
technologies.


Audio-Visual Scene-Aware Dialog

  We introduce the task of scene-aware dialog. Given a follow-up question in an
ongoing dialog about a video, our goal is to generate a complete and natural
response to a question given (a) an input video, and (b) the history of
previous turns in the dialog. To succeed, agents must ground the semantics in
the video and leverage contextual cues from the history of the dialog to answer
the question. To benchmark this task, we introduce the Audio Visual Scene-Aware
Dialog (AVSD) dataset. For each of more than 11,000 videos of human actions for
the Charades dataset. Our dataset contains a dialog about the video, plus a
final summary of the video by one of the dialog participants. We train several
baseline systems for this task and evaluate the performance of the trained
models using several qualitative and quantitative metrics. Our results indicate
that the models must comprehend all the available inputs (video, audio,
question and dialog history) to perform well on this dataset.


Embodied Multimodal Multitask Learning

  Recent efforts on training visual navigation agents conditioned on language
using deep reinforcement learning have been successful in learning policies for
different multimodal tasks, such as semantic goal navigation and embodied
question answering. In this paper, we propose a multitask model capable of
jointly learning these multimodal tasks, and transferring knowledge of words
and their grounding in visual objects across the tasks. The proposed model uses
a novel Dual-Attention unit to disentangle the knowledge of words in the
textual representations and visual concepts in the visual representations, and
align them with each other. This disentangled task-invariant alignment of
representations facilitates grounding and knowledge transfer across both tasks.
We show that the proposed model outperforms a range of baselines on both tasks
in simulated 3D environments. We also show that this disentanglement of
representations makes our model modular, interpretable, and allows for transfer
to instructions containing new words by leveraging object detectors.


EvalAI: Towards Better Evaluation Systems for AI Agents

  We introduce EvalAI, an open source platform for evaluating and comparing
machine learning (ML) and artificial intelligence algorithms (AI) at scale.
EvalAI is built to provide a scalable solution to the research community to
fulfill the critical need of evaluating machine learning models and agents
acting in an environment against annotations or with a human-in-the-loop. This
will help researchers, students, and data scientists to create, collaborate,
and participate in AI challenges organized around the globe. By simplifying and
standardizing the process of benchmarking these models, EvalAI seeks to lower
the barrier to entry for participating in the global scientific effort to push
the frontiers of machine learning and artificial intelligence, thereby
increasing the rate of measurable progress in this domain.


Taking a HINT: Leveraging Explanations to Make Vision and Language
  Models More Grounded

  Many vision and language models suffer from poor visual grounding - often
falling back on easy-to-learn language priors rather than associating language
with visual concepts. In this work, we propose a generic framework which we
call Human Importance-aware Network Tuning (HINT) that effectively leverages
human supervision to improve visual grounding. HINT constrains deep networks to
be sensitive to the same input regions as humans. Crucially, our approach
optimizes the alignment between human attention maps and gradient-based network
importances - ensuring that models learn not just to look at but rather rely on
visual concepts that humans found relevant for a task when making predictions.
We demonstrate our approach on Visual Question Answering and Image Captioning
tasks, achieving state of-the-art for the VQA-CP dataset which penalizes
over-reliance on language priors.


Probabilistic Neural-symbolic Models for Interpretable Visual Question
  Answering

  We propose a new class of probabilistic neural-symbolic models, that have
symbolic functional programs as a latent, stochastic variable. Instantiated in
the context of visual question answering, our probabilistic formulation offers
two key conceptual advantages over prior neural-symbolic models for VQA.
Firstly, the programs generated by our model are more understandable while
requiring lesser number of teaching examples. Secondly, we show that one can
pose counterfactual scenarios to the model, to probe its beliefs on the
programs that could lead to a specified answer given an image. Our results on
the CLEVR and SHAPES datasets verify our hypotheses, showing that the model
gets better program (and answer) prediction accuracy even in the low data
regime, and allows one to probe the coherence and consistency of reasoning
performed.


Learning Dynamics Model in Reinforcement Learning by Incorporating the
  Long Term Future

  In model-based reinforcement learning, the agent interleaves between model
learning and planning. These two components are inextricably intertwined. If
the model is not able to provide sensible long-term prediction, the executed
planner would exploit model flaws, which can yield catastrophic failures. This
paper focuses on building a model that reasons about the long-term future and
demonstrates how to use this for efficient planning and exploration. To this
end, we build a latent-variable autoregressive model by leveraging recent ideas
in variational inference. We argue that forcing latent variables to carry
future information through an auxiliary task substantially improves long-term
predictions. Moreover, by planning in the latent space, the planner's solution
is ensured to be within regions where the model is valid. An exploration
strategy can be devised by searching for unlikely trajectories under the model.
Our method achieves higher reward faster compared to baselines on a variety of
tasks and environments in both the imitation learning and model-based
reinforcement learning settings.


Embodied Question Answering in Photorealistic Environments with Point
  Cloud Perception

  To help bridge the gap between internet vision-style problems and the goal of
vision for embodied perception we instantiate a large-scale navigation task --
Embodied Question Answering [1] in photo-realistic environments (Matterport
3D). We thoroughly study navigation policies that utilize 3D point clouds, RGB
images, or their combination. Our analysis of these models reveals several key
findings. We find that two seemingly naive navigation baselines, forward-only
and random, are strong navigators and challenging to outperform, due to the
specific choice of the evaluation setting presented by [1]. We find a novel
loss-weighting scheme we call Inflection Weighting to be important when
training recurrent models for navigation with behavior cloning and are able to
out perform the baselines with this technique. We find that point clouds
provide a richer signal than RGB images for learning obstacle avoidance,
motivating the use (and continued study) of 3D deep learning models for
embodied navigation.


Embodied Visual Recognition

  Passive visual systems typically fail to recognize objects in the amodal
setting where they are heavily occluded. In contrast, humans and other embodied
agents have the ability to move in the environment, and actively control the
viewing angle to better understand object shapes and semantics. In this work,
we introduce the task of Embodied Visual Recognition (EVR): An agent is
instantiated in a 3D environment close to an occluded target object, and is
free to move in the environment to perform object classification, amodal object
localization, and amodal object segmentation. To address this, we develop a new
model called Embodied Mask R-CNN, for agents to learn to move strategically to
improve their visual recognition abilities. We conduct experiments using the
House3D environment. Experimental results show that: 1) agents with embodiment
(movement) achieve better visual recognition performance than passive ones; 2)
in order to improve visual recognition abilities, agents can learn strategical
moving paths that are different from shortest paths.


