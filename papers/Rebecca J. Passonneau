Combining Multiple Knowledge Sources for Discourse Segmentation

  We predict discourse segment boundaries from linguistic features ofutterances, using a corpus of spoken narratives as data. We present two methodsfor developing segmentation algorithms from training data: hand tuning andmachine learning. When multiple types of features are used, results approachhuman performance on an independent test set (both methods), and usingcross-validation (machine learning).

OmniGraph: Rich Representation and Graph Kernel Learning

  OmniGraph, a novel representation to support a range of NLP classificationtasks, integrates lexical items, syntactic dependencies and frame semanticparses into graphs. Feature engineering is folded into the learning throughconvolution graph kernel learning to explore different extents of the graph. Ahigh-dimensional space of features includes individual nodes as well as complexsubgraphs. In experiments on a text-forecasting problem that predicts stockprice change from news for company mentions, OmniGraph beats several benchmarksbased on bag-of-words, syntactic dependencies, and semantic trees. The highlyexpressive features OmniGraph discovers provide insights into the semanticsacross distinct market sectors. To demonstrate the method's generality, we alsoreport its high performance results on a fine-grained sentiment corpus.

Integrating Gricean and Attentional Constraints

  This paper concerns how to generate and understand discourse anaphoric nounphrases. I present the results of an analysis of all discourse anaphoric nounphrases (N=1,233) in a corpus of ten narrative monologues, where the choicebetween a definite pronoun or phrasal NP conforms largely to Griceanconstraints on informativeness. I discuss Dale and Reiter's [To appear] recentmodel and show how it can be augmented for understanding as well as generatingthe range of data presented here. I argue that integrating centering [Grosz etal., 1983] [Kameyama, 1985] with this model can be applied uniformly todiscourse anaphoric pronouns and phrasal NPs. I conclude with a hypothesis foraddressing the interaction between local and global discourse processing.

Applying Reliability Metrics to Co-Reference Annotation

  Studies of the contextual and linguistic factors that constrain discoursephenomena such as reference are coming to depend increasingly on annotatedlanguage corpora. In preparing the corpora, it is important to evaluate thereliability of the annotation, but methods for doing so have not been readilyavailable. In this report, I present a method for computing reliability ofcoreference annotation. First I review a method for applying the informationretrieval metrics of recall and precision to coreference annotation proposed byMarc Vilain and his collaborators. I show how this method makes it possible toconstruct contingency tables for computing Cohen's Kappa, a familiarreliability metric. By comparing recall and precision to reliability on thesame data sets, I also show that recall and precision can be misleadingly high.Because Kappa factors out chance agreement among coders, it is a preferablemeasure for developing annotated corpora where no pre-existing targetannotation exists.

Intention-based Segmentation: Human Reliability and Correlation with  Linguistic Cues

  Certain spans of utterances in a discourse, referred to here as segments, arewidely assumed to form coherent units. Further, the segmental structure ofdiscourse has been claimed to constrain and be constrained by many phenomena.However, there is weak consensus on the nature of segments and the criteria forrecognizing or generating them. We present quantitative results of a two partstudy using a corpus of spontaneous, narrative monologues. The first partevaluates the statistical reliability of human segmentation of our corpus,where speaker intention is the segmentation criterion. We then use thesubjects' segmentations to evaluate the correlation of discourse segmentationwith three linguistic cues (referential noun phrases, cue words, and pauses),using information retrieval metrics.

Abstractive Multi-Document Summarization via Phrase Selection and  Merging

  We propose an abstraction-based multi-document summarization framework thatcan construct new sentences by exploring more fine-grained syntactic units thansentences, namely, noun/verb phrases. Different from existing abstraction-basedapproaches, our method first constructs a pool of concepts and factsrepresented by phrases from the input documents. Then new sentences aregenerated by selecting and merging informative phrases to maximize the salienceof phrases and meanwhile satisfy the sentence construction constraints. Weemploy integer linear optimization for conducting phrase selection and mergingsimultaneously in order to achieve the global optimal solution for a summary.Experimental results on the benchmark data set TAC 2011 show that our frameworkoutperforms the state-of-the-art models under automated pyramid evaluationmetric, and achieves reasonably well results on manual linguistic qualityevaluation.

