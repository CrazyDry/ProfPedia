P=NP

  We claim to resolve the P=?NP problem via a formal argument for P=NP.

Is there a duality in the classical acceptance of non-constructive,  foundational, concepts as axiomatic?

  We consider a philosophical question that is implicit in Selmer Bringsjord'spaper, "The narrational case against Church's Thesis": If, as Mendelson argues,the classically accepted definitions of foundational concepts such as "partialrecursive function", "function", "(Tarskian) truth", "set" etc. are vague andimprecise - hence possibly non-constructive and intuitionisticallyobjectionable - then replacing one non-constructive concept by another may bepsychologically unappealing, but it should be meta-mathematically valid andacceptable.

Toward Formalizing Teleportation of Pedagogical Artificial Agents

  Our paradigm for the use of artificial agents to teach requires among otherthings that they persist through time in their interaction with human students,in such a way that they "teleport" or "migrate" from an embodiment at one timet to a different embodiment at later time t'. In this short paper, we report oninitial steps toward the formalization of such teleportation, in order toenable an overseeing AI system to establish, mechanically, and verifiably, thatthe human students in question will likely believe that the very sameartificial agent has persisted across such times despite the differentembodiments.

Tentacular Artificial Intelligence, and the Architecture Thereof,  Introduced

  We briefly introduce herein a new form of distributed, multi-agent artificialintelligence, which we refer to as "tentacular." Tentacular AI is distinguishedby six attributes, which among other things entail a capacity for reasoning andplanning based in highly expressive calculi (logics), and which enlistssubsidiary agents across distances circumscribed only by the reach of one ormore given networks.

Proof Verification Can Be Hard!

  The generally accepted wisdom in computational circles is that pure proofverification is a solved problem and that the computationally hard elements andfertile areas of study lie in proof discovery. This wisdom presumably does holdfor conventional proof systems such as first-order logic with a standard proofcalculus such as natural deduction or resolution. But this folk belief breaksdown when we consider more user-friendly/powerful inference rules. One suchrule is the restricted {\omega}-rule, which is not even semi-decidable whenadded to a standard proof calculus of a nice theory. While presumably not anovel result, we feel that the hardness of proof verification isunder-appreciated in most communities that deal with proofs. A proof-sketchfollows.

Counterfactual Conditionals in Quantified Modal Logic

  We present a novel formalization of counterfactual conditionals in aquantified modal logic. Counterfactual conditionals play a vital role inethical and moral reasoning. Prior work has shown that moral reasoning systems(and more generally, theory-of-mind reasoning systems) should be at least asexpressive as first-order (quantified) modal logic (QML) to be well-behaved.While existing work on moral reasoning has focused on counterfactual-free QMLmoral reasoning, we present a fully specified and implemented formal systemthat includes counterfactual conditionals. We validate our model with twoprojects. In the first project, we demonstrate that our system can be used tomodel a complex moral principle, the doctrine of double effect. In the secondproject, we use the system to build a data-set with true and falsecounterfactuals as licensed by our theory, which we believe can be useful forother researchers. This project also shows that our model can becomputationally feasible.

Toward the Engineering of Virtuous Machines

  While various traditions under the 'virtue ethics' umbrella have been studiedextensively and advocated by ethicists, it has not been clear that there existsa version of virtue ethics rigorous enough to be a target for machine ethics(which we take to include the engineering of an ethical sensibility in amachine or robot itself, not only the study of ethics in the humans who mightcreate artificial agents). We begin to address this by presenting an embryonicformalization of a key part of any virtue-ethics theory: namely, the learningof virtue by a focus on exemplars of moral virtue. Our work is based in part ona computational formal logic previously used to formally model other ethicaltheories and principles therein, and to implement these models in artificialagents.

Learning $\textit{Ex Nihilo}$

  This paper introduces, philosophically and to a degree formally, the novelconcept of learning $\textit{ex nihilo}$, intended (obviously) to be analogousto the concept of creation $\textit{ex nihilo}$. Learning  $\textit{ex nihilo}$ is an agent's learning "from nothing," by the suitableemployment of schemata for deductive and inductive reasoning. This reasoningmust be in machine-verifiable accord with a formal proof/argument theory in a$\textit{cognitive calculus}$ (i.e., roughly, an intensional higher-ordermulti-operator quantified logic), and this reasoning is applied to perceptsreceived by the agent, in the context of both some prior knowledge, and someprior and current interests. Learning $\textit{ex nihilo}$ is a challenge tocontemporary forms of ML, indeed a severe one, but the challenge is offered inthe spirt of seeking to stimulate attempts, on the part of non-logicist MLresearchers and engineers, to collaborate with those in possession oflearning-$\textit{ex nihilo}$ frameworks, and eventually attempts to integratedirectly with such frameworks at the implementation level. Such integrationwill require, among other things, the symbiotic interoperation ofstate-of-the-art automated reasoners and high-expressivity planners, withstatistical/connectionist ML technology.

Strength Factors: An Uncertainty System for a Quantified Modal Logic

  We present a new system S for handling uncertainty in a quantified modallogic (first-order modal logic). The system is based on both probability theoryand proof theory. The system is derived from Chisholm's epistemology. Weconcretize Chisholm's system by grounding his undefined and primitive (i.e.foundational) concept of reasonablenes in probability and proof theory. S canbe useful in systems that have to interact with humans and providejustifications for their uncertainty. As a demonstration of the system, weapply the system to provide a solution to the lottery paradox. Anotheradvantage of the system is that it can be used to provide uncertainty valuesfor counterfactual statements. Counterfactuals are statements that an agentknows for sure are false. Among other cases, counterfactuals are useful whensystems have to explain their actions to users. Uncertainties forcounterfactuals fall out naturally from our system.  Efficient reasoning in just simple first-order logic is a hard problem.Resolution-based first-order reasoning systems have made significant progressover the last several decades in building systems that have solved non-trivialtasks (even unsolved conjectures in mathematics). We present a sketch of anovel algorithm for reasoning that extends first-order resolution.  Finally, while there have been many systems of uncertainty for propositionallogics, first-order logics and propositional modal logics, there has been verylittle work in building systems of uncertainty for first-order modal logics.The work described below is in progress; and once finished will address thislack.

On Automating the Doctrine of Double Effect

  The doctrine of double effect ($\mathcal{DDE}$) is a long-studied ethicalprinciple that governs when actions that have both positive and negativeeffects are to be allowed. The goal in this paper is to automate$\mathcal{DDE}$. We briefly present $\mathcal{DDE}$, and use a first-ordermodal logic, the deontic cognitive event calculus, as our framework toformalize the doctrine. We present formalizations of increasingly strongerversions of the principle, including what is known as the doctrine of tripleeffect. We then use our framework to simulate successfully scenarios that havebeen used to test for the presence of the principle in human subjects. Ourframework can be used in two different modes: One can use it to build$\mathcal{DDE}$-compliant autonomous systems from scratch, or one can use it toverify that a given AI system is $\mathcal{DDE}$-compliant, by applying a$\mathcal{DDE}$ layer on an existing system or model. For the latter mode, theunderlying AI system can be built using any architecture (planners, deep neuralnetworks, bayesian networks, knowledge-representation systems, or a hybrid); aslong as the system exposes a few parameters in its model, such verification ispossible. The role of the $\mathcal{DDE}$ layer here is akin to a (dynamic orstatic) software verifier that examines existing software modules. Finally, weend by presenting initial work on how one can apply our $\mathcal{DDE}$ layerto the STRIPS-style planning model, and to a modified POMDP model.This ispreliminary work to illustrate the feasibility of the second mode, and we hopethat our initial sketches can be useful for other researchers in incorporatingDDE in their own frameworks.

Toward Cognitive and Immersive Systems: Experiments in a Cognitive  Microworld

  As computational power has continued to increase, and sensors have becomemore accurate, the corresponding advent of systems that are at once cognitiveand immersive has arrived. These \textit{cognitive and immersive systems}(CAISs) fall squarely into the intersection of AI with HCI/HRI: such systemsinteract with and assist the human agents that enter them, in no small partbecause such systems are infused with AI able to understand and reason aboutthese humans and their knowledge, beliefs, goals, communications, plans, etc.We herein explain our approach to engineering CAISs. We emphasize the capacityof a CAIS to develop and reason over a `theory of the mind' of its humanpartners. This capacity entails that the AI in question has a sophisticatedmodel of the beliefs, knowledge, goals, desires, emotions, etc.\ of thesehumans. To accomplish this engineering, a formal framework of very highexpressivity is needed. In our case, this framework is a \textit{cognitiveevent calculus}, a particular kind of quantified multi-operator modal logic,and a matching high-expressivity automated reasoner and planner. To explain,advance, and to a degree validate our approach, we show that a calculus of thistype satisfies a set of formal requirements, and can enable a CAIS tounderstand a psychologically tricky scenario couched in what we call the\textit{cognitive polysolid framework} (CPF). We also formally show that a roomthat satisfies these requirements can have a useful property we term\emph{expectation of usefulness}. CPF, a sub-class of \textit{cognitivemicroworlds}, includes machinery able to represent and plan over not merelyblocks and actions (such as seen in the primitive `blocks worlds' of old), butalso over agents and their mental attitudes about both other agents andinanimate objects.

