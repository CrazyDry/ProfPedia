Least Squares Policy Iteration with Instrumental Variables vs. Direct  Policy Search: Comparison Against Optimal Benchmarks Using Energy Storage

  This paper studies approximate policy iteration (API) methods which useleast-squares Bellman error minimization for policy evaluation. We addressseveral of its enhancements, namely, Bellman error minimization usinginstrumental variables, least-squares projected Bellman error minimization, andprojected Bellman error minimization using instrumental variables. We provethat for a general discrete-time stochastic control problem, Bellman errorminimization using instrumental variables is equivalent to both variants ofprojected Bellman error minimization. An alternative to these API methods isdirect policy search based on knowledge gradient. The practical performance ofthese three approximate dynamic programming methods are then investigated inthe context of an application in energy storage, integrated with anintermittent wind energy supply to fully serve a stochastic time-varyingelectricity demand. We create a library of test problems using real-world dataand apply value iteration to find their optimal policies. These benchmarks arethen used to compare the developed policies. Our analysis indicates that APIwith instrumental variables Bellman error minimization prominently outperformsAPI with least-squares Bellman error minimization. However, these approachesunderperform our direct policy search implementation.

Dirichlet Process Mixtures of Generalized Linear Models

  We propose Dirichlet Process mixtures of Generalized Linear Models (DP-GLM),a new method of nonparametric regression that accommodates continuous andcategorical inputs, and responses that can be modeled by a generalized linearmodel. We prove conditions for the asymptotic unbiasedness of the DP-GLMregression mean function estimate. We also give examples for when thoseconditions hold, including models for compactly supported continuousdistributions and a model with continuous covariates and categorical response.We empirically analyze the properties of the DP-GLM and why it provides betterresults than existing Dirichlet process mixture regression models. We evaluateDP-GLM on several data sets, comparing it to modern methods of nonparametricregression like CART, Bayesian trees and Gaussian processes. Compared toexisting techniques, the DP-GLM provides a single model (and correspondinginference algorithms) that performs well in many regression settings.

Stochastic Search with an Observable State Variable

  In this paper we study convex stochastic search problems where a noisyobjective function value is observed after a decision is made. There are manystochastic search problems whose behavior depends on an exogenous statevariable which affects the shape of the objective function. Currently, there isno general purpose algorithm to solve this class of problems. We usenonparametric density estimation to take observations from the jointstate-outcome distribution and use them to infer the optimal decision for agiven query state. We propose two solution methods that depend on the problemcharacteristics: function-based and gradient-based optimization. We examine twoweighting schemes, kernel-based weights and Dirichlet process-based weights,for use with the solution methods. The weights and solution methods are testedon a synthetic multi-product newsvendor problem and the hour-ahead windcommitment problem. Our results show that in some cases Dirichlet processweights offer substantial benefits over kernel based weights and more generallythat nonparametric estimation methods provide good solutions to otherwiseintractable problems.

A New Optimal Stepsize For Approximate Dynamic Programming

  Approximate dynamic programming (ADP) has proven itself in a wide range ofapplications spanning large-scale transportation problems, health care, revenuemanagement, and energy systems. The design of effective ADP algorithms has manydimensions, but one crucial factor is the stepsize rule used to update a valuefunction approximation. Many operations research applications arecomputationally intensive, and it is important to obtain good results quickly.Furthermore, the most popular stepsize formulas use tunable parameters and canproduce very poor results if tuned improperly. We derive a new stepsize rulethat optimizes the prediction error in order to improve the short-termperformance of an ADP algorithm. With only one, relatively insensitive tunableparameter, the new rule adapts to the level of noise in the problem andproduces faster convergence in numerical experiments.

Regularized Decomposition of High-Dimensional Multistage Stochastic  Programs with Markov Uncertainty

  We develop a quadratic regularization approach for the solution ofhigh-dimensional multistage stochastic optimization problems characterized by apotentially large number of time periods/stages (e.g. hundreds), ahigh-dimensional resource state variable, and a Markov information process. Theresulting algorithms are shown to converge to an optimal policy after a finitenumber of iterations under mild technical assumptions. Computationalexperiments are conducted using the setting of optimizing energy storage over alarge transmission grid, which motivates both the spatial and temporaldimensions of our problem. Our numerical results indicate that the proposedmethods exhibit significantly faster convergence than their classicalcounterparts, with greater gains observed for higher-dimensional problems.

Risk-Averse Approximate Dynamic Programming with Quantile-Based Risk  Measures

  In this paper, we consider a finite-horizon Markov decision process (MDP) forwhich the objective at each stage is to minimize a quantile-based risk measure(QBRM) of the sequence of future costs; we call the overall objective a dynamicquantile-based risk measure (DQBRM). In particular, we consider optimizingdynamic risk measures where the one-step risk measures are QBRMs, a class ofrisk measures that includes the popular value at risk (VaR) and the conditionalvalue at risk (CVaR). Although there is considerable theoretical development ofrisk-averse MDPs in the literature, the computational challenges have not beenexplored as thoroughly. We propose data-driven and simulation-based approximatedynamic programming (ADP) algorithms to solve the risk-averse sequentialdecision problem. We address the issue of inefficient sampling for riskapplications in simulated settings and present a procedure, based on importancesampling, to direct samples toward the "risky region" as the ADP algorithmprogresses. Finally, we show numerical results of our algorithms in the contextof an application involving risk-averse bidding for energy storage.

SDDP vs. ADP: The Effect of Dimensionality in Multistage Stochastic  Optimization for Grid Level Energy Storage

  There has been widespread interest in the use of grid-level storage to handlethe variability from increasing penetrations of wind and solar energy. Thisproblem setting requires optimizing energy storage and release decisions foranywhere from a half-dozen, to potentially hundreds of storage devices spreadaround the grid as new technologies evolve. We approach this problem using twocompeting algorithmic strategies. The first, developed within the stochasticprogramming literature, is stochastic dual dynamic programming (SDDP) whichuses Benders decomposition to create a multidimensional value functionapproximations, which have been widely used to manage hydro reservoirs. Thesecond approach, which has evolved using the language of approximate dynamicprogramming, uses separable, piecewise linear value function approximations, amethod which has been successfully applied to high-dimensional fleet managementproblems. This paper brings these two approaches together using a commonnotational system, and contrasts the algorithmic strategies (which are both aform of approximate dynamic programming) used by each approach. The methods arethen subjected to rigorous testing using the context of optimizing grid levelstorage.

Optimal Learning for Stochastic Optimization with Nonlinear Parametric  Belief Models

  We consider the problem of estimating the expected value of information (theknowledge gradient) for Bayesian learning problems where the belief model isnonlinear in the parameters. Our goal is to maximize some metric, whilesimultaneously learning the unknown parameters of the nonlinear belief model,by guiding a sequential experimentation process which is expensive. We overcomethe problem of computing the expected value of an experiment, which iscomputationally intractable, by using a sampled approximation, which helps toguide experiments but does not provide an accurate estimate of the unknownparameters. We then introduce a resampling process which allows the sampledmodel to adapt to new information, exploiting past experiments. We showtheoretically that the method converges asymptotically to the true parameters,while simultaneously maximizing our metric. We show empirically that theprocess exhibits rapid convergence, yielding good results with a very smallnumber of experiments.

Stochastic Optimization with Parametric Cost Function Approximations

  A widely used heuristic for solving stochastic optimization problems is touse a deterministic rolling horizon procedure, which has been modified tohandle uncertainty (e.g. buffer stocks, schedule slack). This approach has beencriticized for its use of a deterministic approximation of a stochasticproblem, which is the major motivation for stochastic programming. We recastthis debate by identifying both deterministic and stochastic approaches aspolicies for solving a stochastic base model, which may be a simulator or thereal world. Stochastic lookahead models (stochastic programming) require arange of approximations to keep the problem tractable. By contrast, so-calleddeterministic models are actually parametrically modified cost functionapproximations which use parametric adjustments to the objective functionand/or the constraints. These parameters are then optimized in a stochasticbase model which does not require making any of the types of simplificationsrequired by stochastic programming. We formalize this strategy and describe agradient-based stochastic search strategy to optimize the parameters.

Backward Approximate Dynamic Programming with Hidden Semi-Markov  Stochastic Models in Energy Storage Optimization

  We consider an energy storage problem involving a wind farm with a forecastedpower output, a stochastic load, an energy storage device, and a connection tothe larger power grid with stochastic prices. Electricity prices and wind powerforecast errors are modeled using a novel hidden semi-Markov model thataccurately replicates not just the distribution of the errors, but alsocrossing times, capturing the amount of time each process stays above or belowsome benchmark such as the forecast. This is an important property ofstochastic processes involved in storage problems. We show that we achieve morerobust solutions using this model than when more common stochastic models areconsidered. The new model introduces some additional complexity to the problemas its information states are partially hidden, forming a partially observableMarkov decision process. We derive a near-optimal time-dependent policy usingbackward approximate dynamic programming, which overcomes the computationalhurdles of classical (exact) backward dynamic programming, with higher qualitysolutions than the more familiar forward approximate dynamic programmingmethods.

An Approximate Dynamic Programming Algorithm for Monotone Value  Functions

  Many sequential decision problems can be formulated as Markov DecisionProcesses (MDPs) where the optimal value function (or cost-to-go function) canbe shown to satisfy a monotone structure in some or all of its dimensions. Whenthe state space becomes large, traditional techniques, such as the backwarddynamic programming algorithm (i.e., backward induction or value iteration),may no longer be effective in finding a solution within a reasonable timeframe, and thus we are forced to consider other approaches, such as approximatedynamic programming (ADP). We propose a provably convergent ADP algorithmcalled Monotone-ADP that exploits the monotonicity of the value functions inorder to increase the rate of convergence. In this paper, we describe a generalfinite-horizon problem setting where the optimal value function is monotone,present a convergence proof for Monotone-ADP under various technicalassumptions, and show numerical results for three application domains: optimalstopping, energy storage/allocation, and glycemic control for diabetespatients. The empirical results indicate that by taking advantage ofmonotonicity, we can attain high quality solutions within a relatively smallnumber of iterations, using up to two orders of magnitude less computation thanis needed to compute the optimal solution exactly.

Optimal Hour-Ahead Bidding in the Real-Time Electricity Market with  Battery Storage using Approximate Dynamic Programming

  There is growing interest in the use of grid-level storage to smoothvariations in supply that are likely to arise with increased use of wind andsolar energy. Energy arbitrage, the process of buying, storing, and sellingelectricity to exploit variations in electricity spot prices, is becoming animportant way of paying for expensive investments into grid-level storage.Independent system operators such as the NYISO (New York Independent SystemOperator) require that battery storage operators place bids into an hour-aheadmarket (although settlements may occur in increments as small as 5 minutes,which is considered near "real-time"). The operator has to place these bidswithout knowing the energy level in the battery at the beginning of the hour,while simultaneously accounting for the value of leftover energy at the end ofthe hour. The problem is formulated as a dynamic program. We describe andemploy a convergent approximate dynamic programming (ADP) algorithm thatexploits monotonicity of the value function to find a revenue-generatingbidding policy; using optimal benchmarks, we empirically show the computationalbenefits of the algorithm. Furthermore, we propose a distribution-free variantof the ADP algorithm that does not require any knowledge of the distribution ofthe price process (and makes no assumptions regarding a specific real-timeprice model). We demonstrate that a policy trained on historical real-timeprice data from the NYISO using this distribution-free approach is indeedeffective.

A Knowledge Gradient Policy for Sequencing Experiments to Identify the  Structure of RNA Molecules Using a Sparse Additive Belief Model

  We present a sparse knowledge gradient (SpKG) algorithm for adaptivelyselecting the targeted regions within a large RNA molecule to identify whichregions are most amenable to interactions with other molecules. Experimentally,such regions can be inferred from fluorescence measurements obtained by bindinga complementary probe with fluorescence markers to the targeted regions. We usea biophysical model which shows that the fluorescence ratio under the log scalehas a sparse linear relationship with the coefficients describing theaccessibility of each nucleotide, since not all sites are accessible (due tothe folding of the molecule). The SpKG algorithm uniquely combines the Bayesianranking and selection problem with the frequentist $\ell_1$ regularizedregression approach Lasso. We use this algorithm to identify the sparsitypattern of the linear model as well as sequentially decide the best regions totest before experimental budget is exhausted. Besides, we also develop twoother new algorithms: batch SpKG algorithm, which generates more suggestionssequentially to run parallel experiments; and batch SpKG with a procedure whichwe call length mutagenesis. It dynamically adds in new alternatives, in theform of types of probes, are created by inserting, deleting or mutatingnucleotides within existing probes. In simulation, we demonstrate thesealgorithms on the Group I intron (a mid-size RNA molecule), showing that theyefficiently learn the correct sparsity pattern, identify the most accessibleregion, and outperform several other policies.

The Information-Collecting Vehicle Routing Problem: Stochastic  Optimization for Emergency Storm Response

  Utilities face the challenge of responding to power outages due to storms andice damage, but most power grids are not equipped with sensors to pinpoint theprecise location of the faults causing the outage. Instead, utilities have todepend primarily on phone calls (trouble calls) from customers who have lostpower to guide the dispatching of utility trucks. In this paper, we develop apolicy that routes a utility truck to restore outages in the power grid asquickly as possible, using phone calls to create beliefs about outages, butalso using utility trucks as a mechanism for collecting additional information.This means that routing decisions change not only the physical state of thetruck (as it moves from one location to another) and the grid (as the truckperforms repairs), but also our belief about the network, creating the firststochastic vehicle routing problem that explicitly models informationcollection and belief modeling. We address the problem of managing a singleutility truck, which we start by formulating as a sequential stochasticoptimization model which captures our belief about the state of the grid. Wepropose a stochastic lookahead policy, and use Monte Carlo tree search (MCTS)to produce a practical policy that is asymptotically optimal. Simulationresults show that the developed policy restores the power grid much fastercompared to standard industry heuristics.

Monte Carlo Tree Search with Sampled Information Relaxation Dual Bounds

  Monte Carlo Tree Search (MCTS), most famously used in game-play artificialintelligence (e.g., the game of Go), is a well-known strategy for constructingapproximate solutions to sequential decision problems. Its primary innovationis the use of a heuristic, known as a default policy, to obtain Monte Carloestimates of downstream values for states in a decision tree. This informationis used to iteratively expand the tree towards regions of states and actionsthat an optimal policy might visit. However, to guarantee convergence to theoptimal action, MCTS requires the entire tree to be expanded asymptotically. Inthis paper, we propose a new technique called Primal-Dual MCTS that utilizessampled information relaxation upper bounds on potential actions, creating thepossibility of "ignoring" parts of the tree that stem from highly suboptimalchoices. This allows us to prove that despite converging to a partial decisiontree in the limit, the recommended action from Primal-Dual MCTS is optimal. Thenew approach shows significant promise when used to optimize the behavior of asingle driver navigating a graph while operating on a ride-sharing platform.Numerical experiments on a real dataset of 7,000 trips in New Jersey suggestthat Primal-Dual MCTS improves upon standard MCTS by producing deeper decisiontrees and exhibits a reduced sensitivity to the size of the action space.

Approximate Dynamic Programming for Planning a Ride-Sharing System using  Autonomous Fleets of Electric Vehicles

  Within a decade, almost every major auto company, along with fleet operatorssuch as Uber, have announced plans to put autonomous vehicles on the road. Atthe same time, electric vehicles are quickly emerging as a next-generationtechnology that is cost effective, in addition to offering the benefits ofreducing the carbon footprint. The combination of a centrally managed fleet ofdriverless vehicles, along with the operating characteristics of electricvehicles, is creating a transformative new technology that offers significantcost savings with high service levels. This problem involves a dispatch problemfor assigning riders to cars, a surge pricing problem for deciding on the priceper trip and a planning problem for deciding on the fleet size. We useapproximate dynamic programming to develop high-quality operational dispatchstrategies to determine which car is best for a particular trip, when a carshould be recharged, and when it should be re-positioned to a different zonewhich offers a higher density of trips. We prove that the value functions aremonotone in the battery and time dimensions and use hierarchical aggregation toget better estimates of the value functions with a small number ofobservations. Then, surge pricing is discussed using an adaptive learningapproach to decide on the price for each trip. Finally, we discuss the fleetsize problem which depends on the previous two problems.

Practicality of Nested Risk Measures for Dynamic Electric Vehicle  Charging

  We consider the sequential decision problem faced by the manager of anelectric vehicle (EV) charging station, who aims to satisfy the charging demandof the customer while minimizing cost. Since the total time needed to chargethe EV up to capacity is often less than the amount of time that the customeris away, there are opportunities to exploit electricity spot price variationswithin some reservation window. We formulate the problem as a finite horizonMarkov decision process (MDP) and consider a risk-averse objective function byoptimizing under a dynamic risk measure constructed using a convex combinationof expected value and conditional value at risk (CVaR). It has been recognizedthat the objective function of a risk-averse MDP lacks a practicalinterpretation. Therefore, in both academic and industry practice, the dynamicrisk measure objective is often not of primary interest; instead, therisk-averse MDP is used as a computational tool for solving problems withpredefined "practical" risk and reward objectives (termed the base model). Inthis paper, we study the extent to which the two sides of this framework arecompatible with each other for the EV setting -- roughly speaking, does a "morerisk-averse" MDP provide lower risk in the practical sense as well? In order toanswer such a question, the effect of the degree of dynamic risk-aversion onthe optimal MDP policy is analyzed. Based on these results, we also propose aprincipled approximation approach to finding an instance of the risk-averse MDPwhose optimal policy behaves well under the practical objectives of the basemodel. Our numerical experiments suggest that EV charging stations can beoperated at a significantly higher level of profitability if dynamic chargingis adopted and a small amount of risk is tolerated.

Recursive Optimization of Convex Risk Measures: Mean-Semideviation  Models

  We develop recursive, data-driven, stochastic subgradient methods foroptimizing a new, versatile, and application-driven class of convex riskmeasures, termed here as mean-semideviations, strictly generalizing thewell-known and popular mean-upper-semideviation. We introduce the MESSAGEpalgorithm, which is an efficient compositional subgradient procedure foriteratively solving convex mean-semideviation risk-averse problems tooptimality. We analyze the asymptotic behavior of the MESSAGEp algorithm undera flexible and structure-exploiting set of problem assumptions. In particular:1) Under appropriate stepsize rules, we establish pathwise convergence of theMESSAGEp algorithm in a strong technical sense, confirming its asymptoticconsistency. 2) Assuming a strongly convex cost, we show that, for fixedsemideviation order $p>1$ and for $\epsilon\in\left[0,1\right)$, the MESSAGEpalgorithm achieves a squared-${\cal L}_{2}$ solution suboptimality rate of theorder of ${\cal O}(n^{-\left(1-\epsilon\right)/2})$ iterations, where, for$\epsilon>0$, pathwise convergence is simultaneously guaranteed. This resultestablishes a rate of order arbitrarily close to ${\cal O}(n^{-1/2})$, whileensuring strongly stable pathwise operation. For $p\equiv1$, the rate orderimproves to ${\cal O}(n^{-2/3})$, which also suffices for pathwise convergence,and matches previous results. 3) Likewise, in the general case of a convexcost, we show that, for any $\epsilon\in\left[0,1\right)$, the MESSAGEpalgorithm with iterate smoothing achieves an ${\cal L}_{1}$ objectivesuboptimality rate of the order of ${\calO}(n^{-\left(1-\epsilon\right)/\left(4\bf{1}_{\left\{ p>1\right\} }+4\right)})$iterations. This result provides maximal rates of ${\cal O}(n^{-1/4})$, if$p\equiv1$, and ${\cal O}(n^{-1/8})$, if $p>1$, matching the state of the art,as well.

