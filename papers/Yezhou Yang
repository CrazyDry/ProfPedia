Learning the Semantics of Manipulation Action

  In this paper we present a formal computational framework for modeling
manipulation actions. The introduced formalism leads to semantics of
manipulation action and has applications to both observing and understanding
human manipulation actions as well as executing them with a robotic mechanism
(e.g. a humanoid robot). It is based on a Combinatory Categorial Grammar. The
goal of the introduced framework is to: (1) represent manipulation actions with
both syntax and semantic parts, where the semantic part employs
$\lambda$-calculus; (2) enable a probabilistic semantic parsing schema to learn
the $\lambda$-calculus representation of manipulation action from an annotated
action corpus of videos; (3) use (1) and (2) to develop a system that visually
observes manipulation actions and understands their meaning while it can reason
beyond observations using propositional logic and axiom schemata. The
experiments conducted on a public available large manipulation action dataset
validate the theoretical framework and our implementation.


Neural Self Talk: Image Understanding via Continuous Questioning and
  Answering

  In this paper we consider the problem of continuously discovering image
contents by actively asking image based questions and subsequently answering
the questions being asked. The key components include a Visual Question
Generation (VQG) module and a Visual Question Answering module, in which
Recurrent Neural Networks (RNN) and Convolutional Neural Network (CNN) are
used. Given a dataset that contains images, questions and their answers, both
modules are trained at the same time, with the difference being VQG uses the
images as input and the corresponding questions as output, while VQA uses
images and questions as input and the corresponding answers as output. We
evaluate the self talk process subjectively using Amazon Mechanical Turk, which
show effectiveness of the proposed method.


What Can I Do Around Here? Deep Functional Scene Understanding for
  Cognitive Robots

  For robots that have the capability to interact with the physical environment
through their end effectors, understanding the surrounding scenes is not merely
a task of image classification or object recognition. To perform actual tasks,
it is critical for the robot to have a functional understanding of the visual
scene. Here, we address the problem of localizing and recognition of functional
areas from an arbitrary indoor scene, formulated as a two-stage deep learning
based detection pipeline. A new scene functionality testing-bed, which is
complied from two publicly available indoor scene datasets, is used for
evaluation. Our method is evaluated quantitatively on the new dataset,
demonstrating the ability to perform efficient recognition of functional areas
from arbitrary indoor scenes. We also demonstrate that our detection model can
be generalized onto novel indoor scenes by cross validating it with the images
from two different datasets.


From Images to Sentences through Scene Description Graphs using
  Commonsense Reasoning and Knowledge

  In this paper we propose the construction of linguistic descriptions of
images. This is achieved through the extraction of scene description graphs
(SDGs) from visual scenes using an automatically constructed knowledge base.
SDGs are constructed using both vision and reasoning. Specifically, commonsense
reasoning is applied on (a) detections obtained from existing perception
methods on given images, (b) a "commonsense" knowledge base constructed using
natural language processing of image annotations and (c) lexical ontological
knowledge from resources such as WordNet. Amazon Mechanical Turk(AMT)-based
evaluations on Flickr8k, Flickr30k and MS-COCO datasets show that in most
cases, sentences auto-constructed from SDGs obtained by our method give a more
relevant and thorough description of an image than a recent state-of-the-art
image caption based approach. Our Image-Sentence Alignment Evaluation results
are also comparable to that of the recent state-of-the art approaches.


TripletGAN: Training Generative Model with Triplet Loss

  As an effective way of metric learning, triplet loss has been widely used in
many deep learning tasks, including face recognition and person-ReID, leading
to many states of the arts. The main innovation of triplet loss is using
feature map to replace softmax in the classification task. Inspired by this
concept, we propose here a new adversarial modeling method by substituting the
classification loss of discriminator with triplet loss. Theoretical proof based
on IPM (Integral probability metric) demonstrates that such setting will help
the generator converge to the given distribution theoretically under some
conditions. Moreover, since triplet loss requires the generator to maximize
distance within a class, we justify tripletGAN is also helpful to prevent mode
collapse through both theory and experiment.


Stroke Controllable Fast Style Transfer with Adaptive Receptive Fields

  The Fast Style Transfer methods have been recently proposed to transfer a
photograph to an artistic style in real-time. This task involves controlling
the stroke size in the stylized results, which remains an open challenge. In
this paper, we present a stroke controllable style transfer network that can
achieve continuous and spatial stroke size control. By analyzing the factors
that influence the stroke size, we propose to explicitly account for the
receptive field and the style image scales. We propose a StrokePyramid module
to endow the network with adaptive receptive fields, and two training
strategies to achieve faster convergence and augment new stroke sizes upon a
trained model respectively. By combining the proposed runtime control
strategies, our network can achieve continuous changes in stroke sizes and
produce distinct stroke sizes in different spatial regions within the same
output image.


Transductive Unbiased Embedding for Zero-Shot Learning

  Most existing Zero-Shot Learning (ZSL) methods have the strong bias problem,
in which instances of unseen (target) classes tend to be categorized as one of
the seen (source) classes. So they yield poor performance after being deployed
in the generalized ZSL settings. In this paper, we propose a straightforward
yet effective method named Quasi-Fully Supervised Learning (QFSL) to alleviate
the bias problem. Our method follows the way of transductive learning, which
assumes that both the labeled source images and unlabeled target images are
available for training. In the semantic embedding space, the labeled source
images are mapped to several fixed points specified by the source categories,
and the unlabeled target images are forced to be mapped to other points
specified by the target categories. Experiments conducted on AwA2, CUB and SUN
datasets demonstrate that our method outperforms existing state-of-the-art
approaches by a huge margin of 9.3~24.5% following generalized ZSL settings,
and by a large margin of 0.2~16.2% following conventional ZSL settings.


LightNet: A Versatile, Standalone Matlab-based Environment for Deep
  Learning

  LightNet is a lightweight, versatile and purely Matlab-based deep learning
framework. The idea underlying its design is to provide an easy-to-understand,
easy-to-use and efficient computational platform for deep learning research.
The implemented framework supports major deep learning architectures such as
Multilayer Perceptron Networks (MLP), Convolutional Neural Networks (CNN) and
Recurrent Neural Networks (RNN). The framework also supports both CPU and GPU
computation, and the switch between them is straightforward. Different
applications in computer vision, natural language processing and robotics are
demonstrated as experiments.


Reliable Attribute-Based Object Recognition Using High Predictive Value
  Classifiers

  We consider the problem of object recognition in 3D using an ensemble of
attribute-based classifiers. We propose two new concepts to improve
classification in practical situations, and show their implementation in an
approach implemented for recognition from point-cloud data. First, the viewing
conditions can have a strong influence on classification performance. We study
the impact of the distance between the camera and the object and propose an
approach to fuse multiple attribute classifiers, which incorporates distance
into the decision making. Second, lack of representative training samples often
makes it difficult to learn the optimal threshold value for best positive and
negative detection rate. We address this issue, by setting in our attribute
classifiers instead of just one threshold value, two threshold values to
distinguish a positive, a negative and an uncertainty class, and we prove the
theoretical correctness of this approach. Empirical studies demonstrate the
effectiveness and feasibility of the proposed concepts.


Co-active Learning to Adapt Humanoid Movement for Manipulation

  In this paper we address the problem of robot movement adaptation under
various environmental constraints interactively. Motion primitives are
generally adopted to generate target motion from demonstrations. However, their
generalization capability is weak while facing novel environments.
Additionally, traditional motion generation methods do not consider the
versatile constraints from various users, tasks, and environments. In this
work, we propose a co-active learning framework for learning to adapt robot
end-effector's movement for manipulation tasks. It is designed to adapt the
original imitation trajectories, which are learned from demonstrations, to
novel situations with various constraints. The framework also considers user's
feedback towards the adapted trajectories, and it learns to adapt movement
through human-in-the-loop interactions. The implemented system generalizes
trained motion primitives to various situations with different constraints
considering user preferences. Experiments on a humanoid platform validate the
effectiveness of our approach.


Fast Task-Specific Target Detection via Graph Based Constraints
  Representation and Checking

  In this work, we present a fast target detection framework for real-world
robotics applications. Considering that an intelligent agent attends to a
task-specific object target during execution, our goal is to detect the object
efficiently. We propose the concept of early recognition, which influences the
candidate proposal process to achieve fast and reliable detection performance.
To check the target constraints efficiently, we put forward a novel policy to
generate a sub-optimal checking order, and prove that it has bounded time cost
compared to the optimal checking sequence, which is not achievable in
polynomial time. Experiments on two different scenarios: 1) rigid object and 2)
non-rigid body part detection validate our pipeline. To show that our method is
widely applicable, we further present a human-robot interaction system based on
our non-rigid body part detection.


Answering Image Riddles using Vision and Reasoning through Probabilistic
  Soft Logic

  In this work, we explore a genre of puzzles ("image riddles") which involves
a set of images and a question. Answering these puzzles require both
capabilities involving visual detection (including object, activity
recognition) and, knowledge-based or commonsense reasoning. We compile a
dataset of over 3k riddles where each riddle consists of 4 images and a
groundtruth answer. The annotations are validated using crowd-sourced
evaluation. We also define an automatic evaluation metric to track future
progress. Our task bears similarity with the commonly known IQ tasks such as
analogy solving, sequence filling that are often used to test intelligence.
  We develop a Probabilistic Reasoning-based approach that utilizes
probabilistic commonsense knowledge to answer these riddles with a reasonable
accuracy. We demonstrate the results of our approach using both automatic and
human evaluations. Our approach achieves some promising results for these
riddles and provides a strong baseline for future attempts. We make the entire
dataset and related materials publicly available to the community in
ImageRiddle Website (http://bit.ly/22f9Ala).


On the Importance of Consistency in Training Deep Neural Networks

  We explain that the difficulties of training deep neural networks come from a
syndrome of three consistency issues. This paper describes our efforts in their
analysis and treatment. The first issue is the training speed inconsistency in
different layers. We propose to address it with an intuitive,
simple-to-implement, low footprint second-order method. The second issue is the
scale inconsistency between the layer inputs and the layer residuals. We
explain how second-order information provides favorable convenience in removing
this roadblock. The third and most challenging issue is the inconsistency in
residual propagation. Based on the fundamental theorem of linear algebra, we
provide a mathematical characterization of the famous vanishing gradient
problem. Thus, an important design principle for future optimization and neural
network design is derived. We conclude this paper with the construction of a
novel contractive neural network.


DeepSIC: Deep Semantic Image Compression

  Incorporating semantic information into the codecs during image compression
can significantly reduce the repetitive computation of fundamental semantic
analysis (such as object recognition) in client-side applications. The same
practice also enable the compressed code to carry the image semantic
information during storage and transmission. In this paper, we propose a
concept called Deep Semantic Image Compression (DeepSIC) and put forward two
novel architectures that aim to reconstruct the compressed image and generate
corresponding semantic representations at the same time. The first architecture
performs semantic analysis in the encoding process by reserving a portion of
the bits from the compressed code to store the semantic representations. The
second performs semantic analysis in the decoding step with the feature maps
that are embedded in the compressed code. In both architectures, the feature
maps are shared by the compression and the semantic analytics modules. To
validate our approaches, we conduct experiments on the publicly available
benchmarking datasets and achieve promising results. We also provide a thorough
analysis of the advantages and disadvantages of the proposed technique.


Explicit Reasoning over End-to-End Neural Architectures for Visual
  Question Answering

  Many vision and language tasks require commonsense reasoning beyond
data-driven image and natural language processing. Here we adopt Visual
Question Answering (VQA) as an example task, where a system is expected to
answer a question in natural language about an image. Current state-of-the-art
systems attempted to solve the task using deep neural architectures and
achieved promising performance. However, the resulting systems are generally
opaque and they struggle in understanding questions for which extra knowledge
is required. In this paper, we present an explicit reasoning layer on top of a
set of penultimate neural network based systems. The reasoning layer enables
reasoning and answering questions where additional knowledge is required, and
at the same time provides an interpretable interface to the end users.
Specifically, the reasoning layer adopts a Probabilistic Soft Logic (PSL) based
engine to reason over a basket of inputs: visual relations, the semantic parse
of the question, and background ontological knowledge from word2vec and
ConceptNet. Experimental analysis of the answers and the key evidential
predicates generated on the VQA dataset validate our approach.


Weakly Supervised Attention Learning for Textual Phrases Grounding

  Grounding textual phrases in visual content is a meaningful yet challenging
problem with various potential applications such as image-text inference or
text-driven multimedia interaction. Most of the current existing methods adopt
the supervised learning mechanism which requires ground-truth at pixel level
during training. However, fine-grained level ground-truth annotation is quite
time-consuming and severely narrows the scope for more general applications. In
this extended abstract, we explore methods to localize flexibly image regions
from the top-down signal (in a form of one-hot label or natural languages) with
a weakly supervised attention learning mechanism. In our model, two types of
modules are utilized: a backbone module for visual feature capturing, and an
attentive module generating maps based on regularized bilinear pooling. We
construct the model in an end-to-end fashion which is trained by encouraging
the spatial attentive map to shift and focus on the region that consists of the
best matched visual features with the top-down signal. We demonstrate the
preliminary yet promising results on a testbed that is synthesized with
multi-label MNIST data.


Active Object Perceiver: Recognition-guided Policy Learning for Object
  Searching on Mobile Robots

  We study the problem of learning a navigation policy for a robot to actively
search for an object of interest in an indoor environment solely from its
visual inputs. While scene-driven visual navigation has been widely studied,
prior efforts on learning navigation policies for robots to find objects are
limited. The problem is often more challenging than target scene finding as the
target objects can be very small in the view and can be in an arbitrary pose.
We approach the problem from an active perceiver perspective, and propose a
novel framework that integrates a deep neural network based object recognition
module and a deep reinforcement learning based action prediction mechanism. To
validate our method, we conduct experiments on both a simulation dataset
(AI2-THOR) and a real-world environment with a physical robot. We further
propose a new decaying reward function to learn the control policy specific to
the object searching task. Experimental results validate the efficacy of our
method, which outperforms competing methods in both average trajectory length
and success rate.


GAPLE: Generalizable Approaching Policy LEarning for Robotic Object
  Searching in Indoor Environment

  We study the problem of learning a generalizable action policy for an
intelligent agent to actively approach an object of interest in an indoor
environment solely from its visual inputs. While scene-driven or
recognition-driven visual navigation has been widely studied, prior efforts
suffer severely from the limited generalization capability. In this paper, we
first argue the object searching task is environment dependent while the
approaching ability is general. To learn a generalizable approaching policy, we
present a novel solution dubbed as GAPLE which adopts two channels of visual
features: depth and semantic segmentation, as the inputs to the policy learning
module. The empirical studies conducted on the House3D dataset as well as on a
physical platform in a real world scenario validate our hypothesis, and we
further provide in-depth qualitative analysis.


How Shall I Drive? Interaction Modeling and Motion Planning towards
  Empathetic and Socially-Graceful Driving

  While intelligence of autonomous vehicles (AVs) has significantly advanced in
recent years, accidents involving AVs suggest that these autonomous systems
lack gracefulness in driving when interacting with human drivers. In the
setting of a two-player game, we propose model predictive control based on
social gracefulness, which is measured by the discrepancy between the actions
taken by the AV and those that could have been taken in favor of the human
driver. We define social awareness as the ability of an agent to infer such
favorable actions based on knowledge about the other agent's intent, and
further show that empathy, i.e., the ability to understand others' intent by
simultaneously inferring others' understanding of the agent's self intent, is
critical to successful intent inference. Lastly, through an intersection case,
we show that the proposed gracefulness objective allows an AV to learn more
sophisticated behavior, such as passive-aggressive motions that gently force
the other agent to yield.


Improving Model Robustness with Transformation-Invariant Attacks

  Vulnerability of neural networks under adversarial attacks has raised serious
concerns and extensive research. Recent studies suggested that model robustness
relies on the use of robust features, i.e., features with strong correlation
with labels, and that data dimensionality and distribution affect the learning
of robust features. On the other hand, experiments showed that human vision,
which is robust against adversarial attacks, is invariant to natural input
transformations. Drawing on these findings, this paper investigates whether
constraints on transformation invariance, including image cropping, rotation,
and zooming, will force image classifiers to learn and use robust features and
in turn acquire better robustness. Experiments on MNIST and CIFAR10 show that
transformation invariance alone has limited effect. Nonetheless, models
adversarially trained on cropping-invariant attacks, in particular, can (1)
extract more robust features, (2) have significantly better robustness than the
state-of-the-art models from adversarial training, and (3) require less
training data.


Image Decomposition and Classification through a Generative Model

  We demonstrate in this paper that a generative model can be designed to
perform classification tasks under challenging settings, including adversarial
attacks and input distribution shifts. Specifically, we propose a conditional
variational autoencoder that learns both the decomposition of inputs and the
distributions of the resulting components. During test, we jointly optimize the
latent variables of the generator and the relaxed component labels to find the
best match between the given input and the output of the generator. The model
demonstrates promising performance at recognizing overlapping components from
the multiMNIST dataset, and novel component combinations from a traffic sign
dataset. Experiments also show that the proposed model achieves high robustness
on MNIST and NORB datasets, in particular for high-strength gradient attacks
and non-gradient attacks.


TKD: Temporal Knowledge Distillation for Active Perception

  Deep neural networks based methods have been proved to achieve outstanding
performance on object detection and classification tasks. Despite significant
performance improvement, due to the deep structures, they still require
prohibitive runtime to process images and maintain the highest possible
performance for real-time applications. Observing the phenomenon that human
vision system (HVS) relies heavily on the temporal dependencies among frames
from the visual input to conduct recognition efficiently, we propose a novel
framework dubbed as TKD: temporal knowledge distillation. This framework
distills the temporal knowledge from a heavy neural networks based model over
selected video frames (the perception of the moments) to a light-weight model.
To enable the distillation, we put forward two novel procedures: 1) an
Long-short Term Memory (LSTM) based key frame selection method; and 2) a novel
teacher-bounded loss design. To validate, we conduct comprehensive empirical
evaluations using different object detection methods over multiple datasets
including Youtube-Objects and Hollywood scene dataset. Our results show
consistent improvement in accuracy-speed trad-offs for object detection over
the frames of the dynamic scene, compare to other modern object recognition
methods.


Modularized Textual Grounding for Counterfactual Resilience

  Computer Vision applications often require a textual grounding module with
precision, interpretability, and resilience to counterfactual inputs/queries.
To achieve high grounding precision, current textual grounding methods heavily
rely on large-scale training data with manual annotations at the pixel level.
Such annotations are expensive to obtain and thus severely narrow the model's
scope of real-world applications. Moreover, most of these methods sacrifice
interpretability, generalizability, and they neglect the importance of being
resilient to counterfactual inputs. To address these issues, we propose a
visual grounding system which is 1) end-to-end trainable in a weakly supervised
fashion with only image-level annotations, and 2) counterfactually resilient
owing to the modular design. Specifically, we decompose textual descriptions
into three levels: entity, semantic attribute, color information, and perform
compositional grounding progressively. We validate our model through a series
of experiments and demonstrate its improvement over the state-of-the-art
methods. In particular, our model's performance not only surpasses other
weakly/un-supervised methods and even approaches the strongly supervised ones,
but also is interpretable for decision making and performs much better in face
of counterfactual classes than all the others.


Prediction of Manipulation Actions

  Looking at a person's hands one often can tell what the person is going to do
next, how his/her hands are moving and where they will be, because an actor's
intentions shape his/her movement kinematics during action execution.
Similarly, active systems with real-time constraints must not simply rely on
passive video-segment classification, but they have to continuously update
their estimates and predict future actions. In this paper, we study the
prediction of dexterous actions. We recorded from subjects performing different
manipulation actions on the same object, such as "squeezing", "flipping",
"washing", "wiping" and "scratching" with a sponge. In psychophysical
experiments, we evaluated human observers' skills in predicting actions from
video sequences of different length, depicting the hand movement in the
preparation and execution of actions before and after contact with the object.
We then developed a recurrent neural network based method for action prediction
using as input patches around the hand. We also used the same formalism to
predict the forces on the finger tips using for training synchronized video and
force data streams. Evaluations on two new datasets showed that our system
closely matches human performance in the recognition task, and demonstrate the
ability of our algorithm to predict what and how a dexterous action is
performed.


Neural Style Transfer: A Review

  The seminal work of Gatys et al. demonstrated the power of Convolutional
Neural Networks (CNNs) in creating artistic imagery by separating and
recombining image content and style. This process of using CNNs to render a
content image in different styles is referred to as Neural Style Transfer
(NST). Since then, NST has become a trending topic both in academic literature
and industrial applications. It is receiving increasing attention and a variety
of approaches are proposed to either improve or extend the original NST
algorithm. In this paper, we aim to provide a comprehensive overview of the
current progress towards NST. We first propose a taxonomy of current algorithms
in the field of NST. Then, we present several evaluation methods and compare
different NST algorithms both qualitatively and quantitatively. The review
concludes with a discussion of various applications of NST and open problems
for future research. A list of papers discussed in this review, corresponding
codes, pre-trained models and more comparison results are publicly available at
https://github.com/ycjing/Neural-Style-Transfer-Papers.


Improving utility of brain tumor confocal laser endomicroscopy:
  objective value assessment and diagnostic frame detection with convolutional
  neural networks

  Confocal laser endomicroscopy (CLE), although capable of obtaining images at
cellular resolution during surgery of brain tumors in real time, creates as
many non-diagnostic as diagnostic images. Non-useful images are often distorted
due to relative motion between probe and brain or blood artifacts. Many images,
however, simply lack diagnostic features immediately informative to the
physician. Examining all the hundreds or thousands of images from a single case
to discriminate diagnostic images from nondiagnostic ones can be tedious.
Providing a real-time diagnostic value assessment of images (fast enough to be
used during the surgical acquisition process and accurate enough for the
pathologist to rely on) to automatically detect diagnostic frames would
streamline the analysis of images and filter useful images for the
pathologist/surgeon. We sought to automatically classify images as diagnostic
or non-diagnostic. AlexNet, a deep-learning architecture, was used in a 4-fold
cross validation manner. Our dataset includes 16,795 images (8572 nondiagnostic
and 8223 diagnostic) from 74 CLE-aided brain tumor surgery patients. The ground
truth for all the images is provided by the pathologist. Average model accuracy
on test data was 91% overall (90.79 % accuracy, 90.94 % sensitivity and 90.87 %
specificity). To evaluate the model reliability we also performed receiver
operating characteristic (ROC) analysis yielding 0.958 average for the area
under ROC curve (AUC). These results demonstrate that a deeply trained AlexNet
network can achieve a model that reliably and quickly recognizes diagnostic CLE
images.


Prospects for Theranostics in Neurosurgical Imaging: Empowering Confocal
  Laser Endomicroscopy Diagnostics via Deep Learning

  Confocal laser endomicroscopy (CLE) is an advanced optical fluorescence
imaging technology that has the potential to increase intraoperative precision,
extend resection, and tailor surgery for malignant invasive brain tumors
because of its subcellular dimension resolution. Despite its promising
diagnostic potential, interpreting the gray tone fluorescence images can be
difficult for untrained users. In this review, we provide a detailed
description of bioinformatical analysis methodology of CLE images that begins
to assist the neurosurgeon and pathologist to rapidly connect on-the-fly
intraoperative imaging, pathology, and surgical observation into a
conclusionary system within the concept of theranostics. We present an overview
and discuss deep learning models for automatic detection of the diagnostic CLE
images and discuss various training regimes and ensemble modeling effect on the
power of deep learning predictive models. Two major approaches reviewed in this
paper include the models that can automatically classify CLE images into
diagnostic/nondiagnostic, glioma/nonglioma, tumor/injury/normal categories and
models that can localize histological features on the CLE images using weakly
supervised methods. We also briefly review advances in the deep learning
approaches used for CLE image analysis in other organs. Significant advances in
speed and precision of automated diagnostic frame selection would augment the
diagnostic potential of CLE, improve operative workflow and integration into
brain tumor surgery. Such technology and bioinformatics analytics lend
themselves to improved precision, personalization, and theranostics in brain
tumor treatment.


Interpretable Partitioned Embedding for Customized Fashion Outfit
  Composition

  Intelligent fashion outfit composition becomes more and more popular in these
years. Some deep learning based approaches reveal competitive composition
recently. However, the unexplainable characteristic makes such deep learning
based approach cannot meet the the designer, businesses and consumers' urge to
comprehend the importance of different attributes in an outfit composition. To
realize interpretable and customized fashion outfit compositions, we propose a
partitioned embedding network to learn interpretable representations from
clothing items. The overall network architecture consists of three components:
an auto-encoder module, a supervised attributes module and a multi-independent
module. The auto-encoder module serves to encode all useful information into
the embedding. In the supervised attributes module, multiple attributes labels
are adopted to ensure that different parts of the overall embedding correspond
to different attributes. In the multi-independent module, adversarial operation
are adopted to fulfill the mutually independent constraint. With the
interpretable and partitioned embedding, we then construct an outfit
composition graph and an attribute matching map. Given specified attributes
description, our model can recommend a ranked list of outfit composition with
interpretable matching scores. Extensive experiments demonstrate that 1) the
partitioned embedding have unmingled parts which corresponding to different
attributes and 2) outfits recommended by our model are more desirable in
comparison with the existing methods.


Spatial Knowledge Distillation to aid Visual Reasoning

  For tasks involving language and vision, the current state-of-the-art methods
tend not to leverage any additional information that might be present to gather
relevant (commonsense) knowledge. A representative task is Visual Question
Answering where large diagnostic datasets have been proposed to test a system's
capability of answering questions about images. The training data is often
accompanied by annotations of individual object properties and spatial
locations. In this work, we take a step towards integrating this additional
privileged information in the form of spatial knowledge to aid in visual
reasoning. We propose a framework that combines recent advances in knowledge
distillation (teacher-student framework), relational reasoning and
probabilistic logical languages to incorporate such knowledge in existing
neural networks for the task of Visual Question Answering. Specifically, for a
question posed against an image, we use a probabilistic logical language to
encode the spatial knowledge and the spatial understanding about the question
in the form of a mask that is directly provided to the teacher network. The
student network learns from the ground-truth information as well as the
teachers prediction via distillation. We also demonstrate the impact of
predicting such a mask inside the teachers network using attention.
Empirically, we show that both the methods improve the test accuracy over a
state-of-the-art approach on a publicly available dataset.


Convolutional Neural Networks: Ensemble Modeling, Fine-Tuning and
  Unsupervised Semantic Localization for Intraoperative CLE Images

  Confocal laser endomicroscopy (CLE) is an advanced optical fluorescence
technology undergoing assessment for applications in brain tumor surgery.
Despite its promising potential, interpreting the unfamiliar gray tone images
of fluorescent stains can be difficult. Many of the CLE images can be distorted
by motion, extremely low or high fluorescence signal, or obscured by red blood
cell accumulation, and these can be interpreted as nondiagnostic. However, just
one neat CLE image might suffice for intraoperative diagnosis of the tumor.
While manual examination of thousands of nondiagnostic images during surgery
would be impractical, this creates an opportunity for a model to select
diagnostic images for the pathologists or surgeon's review. In this study, we
sought to develop a deep learning model to automatically detect the diagnostic
images using a manually annotated dataset, and we employed a patient-based
nested cross-validation approach to explore generalizability of the model. We
explored various training regimes: deep training, shallow fine-tuning, and deep
fine-tuning. Further, we investigated the effect of ensemble modeling by
combining the top-5 single models crafted in the development phase. We
localized histological features from diagnostic CLE images by visualization of
shallow and deep neural activations. Our inter-rater experiment results
confirmed that our ensemble of deeply fine-tuned models achieved higher
agreement with the ground truth than the other observers. With the speed and
precision of the proposed method (110 images/second; 85% on the gold standard
test subset), it has potential to be integrated into the operative workflow in
the brain tumor surgery.


Weakly-Supervised Learning-Based Feature Localization in Confocal Laser
  Endomicroscopy Glioma Images

  Confocal Laser Endomicroscope (CLE) is a novel handheld fluorescence imaging
device that has shown promise for rapid intraoperative diagnosis of brain tumor
tissue. Currently CLE is capable of image display only and lacks an automatic
system to aid the surgeon in analyzing the images. The goal of this project was
to develop a computer-aided diagnostic approach for CLE imaging of human glioma
with feature localization function. Despite the tremendous progress in object
detection and image segmentation methods in recent years, most of such methods
require large annotated datasets for training. However, manual annotation of
thousands of histopathological images by physicians is costly and time
consuming. To overcome this problem, we propose a Weakly-Supervised Learning
(WSL)-based model for feature localization that trains on image-level
annotations, and then localizes incidences of a class-of-interest in the test
image. We developed a novel convolutional neural network for diagnostic
features localization from CLE images by employing a novel multiscale
activation map that is laterally inhibited and collaterally integrated. To
validate our method, we compared proposed model's output to the manual
annotation performed by four neurosurgeons on test images. Proposed model
achieved 88% mean accuracy and 86% mean intersection over union on intermediate
features and 87% mean accuracy and 88% mean intersection over union on
restrictive fine features, while outperforming other state of the art methods
tested. This system can improve accuracy and efficiency in characterization of
CLE images of glioma tissue during surgery, augment intraoperative
decision-making process regarding tumor margin and affect resection rates.


