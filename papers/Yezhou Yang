From Images to Sentences through Scene Description Graphs using  Commonsense Reasoning and Knowledge

  In this paper we propose the construction of linguistic descriptions ofimages. This is achieved through the extraction of scene description graphs(SDGs) from visual scenes using an automatically constructed knowledge base.SDGs are constructed using both vision and reasoning. Specifically, commonsensereasoning is applied on (a) detections obtained from existing perceptionmethods on given images, (b) a "commonsense" knowledge base constructed usingnatural language processing of image annotations and (c) lexical ontologicalknowledge from resources such as WordNet. Amazon Mechanical Turk(AMT)-basedevaluations on Flickr8k, Flickr30k and MS-COCO datasets show that in mostcases, sentences auto-constructed from SDGs obtained by our method give a morerelevant and thorough description of an image than a recent state-of-the-artimage caption based approach. Our Image-Sentence Alignment Evaluation resultsare also comparable to that of the recent state-of-the art approaches.

Learning the Semantics of Manipulation Action

  In this paper we present a formal computational framework for modelingmanipulation actions. The introduced formalism leads to semantics ofmanipulation action and has applications to both observing and understandinghuman manipulation actions as well as executing them with a robotic mechanism(e.g. a humanoid robot). It is based on a Combinatory Categorial Grammar. Thegoal of the introduced framework is to: (1) represent manipulation actions withboth syntax and semantic parts, where the semantic part employs$\lambda$-calculus; (2) enable a probabilistic semantic parsing schema to learnthe $\lambda$-calculus representation of manipulation action from an annotatedaction corpus of videos; (3) use (1) and (2) to develop a system that visuallyobserves manipulation actions and understands their meaning while it can reasonbeyond observations using propositional logic and axiom schemata. Theexperiments conducted on a public available large manipulation action datasetvalidate the theoretical framework and our implementation.

Neural Self Talk: Image Understanding via Continuous Questioning and  Answering

  In this paper we consider the problem of continuously discovering imagecontents by actively asking image based questions and subsequently answeringthe questions being asked. The key components include a Visual QuestionGeneration (VQG) module and a Visual Question Answering module, in whichRecurrent Neural Networks (RNN) and Convolutional Neural Network (CNN) areused. Given a dataset that contains images, questions and their answers, bothmodules are trained at the same time, with the difference being VQG uses theimages as input and the corresponding questions as output, while VQA usesimages and questions as input and the corresponding answers as output. Weevaluate the self talk process subjectively using Amazon Mechanical Turk, whichshow effectiveness of the proposed method.

What Can I Do Around Here? Deep Functional Scene Understanding for  Cognitive Robots

  For robots that have the capability to interact with the physical environmentthrough their end effectors, understanding the surrounding scenes is not merelya task of image classification or object recognition. To perform actual tasks,it is critical for the robot to have a functional understanding of the visualscene. Here, we address the problem of localizing and recognition of functionalareas from an arbitrary indoor scene, formulated as a two-stage deep learningbased detection pipeline. A new scene functionality testing-bed, which iscomplied from two publicly available indoor scene datasets, is used forevaluation. Our method is evaluated quantitatively on the new dataset,demonstrating the ability to perform efficient recognition of functional areasfrom arbitrary indoor scenes. We also demonstrate that our detection model canbe generalized onto novel indoor scenes by cross validating it with the imagesfrom two different datasets.

TripletGAN: Training Generative Model with Triplet Loss

  As an effective way of metric learning, triplet loss has been widely used inmany deep learning tasks, including face recognition and person-ReID, leadingto many states of the arts. The main innovation of triplet loss is usingfeature map to replace softmax in the classification task. Inspired by thisconcept, we propose here a new adversarial modeling method by substituting theclassification loss of discriminator with triplet loss. Theoretical proof basedon IPM (Integral probability metric) demonstrates that such setting will helpthe generator converge to the given distribution theoretically under someconditions. Moreover, since triplet loss requires the generator to maximizedistance within a class, we justify tripletGAN is also helpful to prevent modecollapse through both theory and experiment.

Stroke Controllable Fast Style Transfer with Adaptive Receptive Fields

  The Fast Style Transfer methods have been recently proposed to transfer aphotograph to an artistic style in real-time. This task involves controllingthe stroke size in the stylized results, which remains an open challenge. Inthis paper, we present a stroke controllable style transfer network that canachieve continuous and spatial stroke size control. By analyzing the factorsthat influence the stroke size, we propose to explicitly account for thereceptive field and the style image scales. We propose a StrokePyramid moduleto endow the network with adaptive receptive fields, and two trainingstrategies to achieve faster convergence and augment new stroke sizes upon atrained model respectively. By combining the proposed runtime controlstrategies, our network can achieve continuous changes in stroke sizes andproduce distinct stroke sizes in different spatial regions within the sameoutput image.

Transductive Unbiased Embedding for Zero-Shot Learning

  Most existing Zero-Shot Learning (ZSL) methods have the strong bias problem,in which instances of unseen (target) classes tend to be categorized as one ofthe seen (source) classes. So they yield poor performance after being deployedin the generalized ZSL settings. In this paper, we propose a straightforwardyet effective method named Quasi-Fully Supervised Learning (QFSL) to alleviatethe bias problem. Our method follows the way of transductive learning, whichassumes that both the labeled source images and unlabeled target images areavailable for training. In the semantic embedding space, the labeled sourceimages are mapped to several fixed points specified by the source categories,and the unlabeled target images are forced to be mapped to other pointsspecified by the target categories. Experiments conducted on AwA2, CUB and SUNdatasets demonstrate that our method outperforms existing state-of-the-artapproaches by a huge margin of 9.3~24.5% following generalized ZSL settings,and by a large margin of 0.2~16.2% following conventional ZSL settings.

LightNet: A Versatile, Standalone Matlab-based Environment for Deep  Learning

  LightNet is a lightweight, versatile and purely Matlab-based deep learningframework. The idea underlying its design is to provide an easy-to-understand,easy-to-use and efficient computational platform for deep learning research.The implemented framework supports major deep learning architectures such asMultilayer Perceptron Networks (MLP), Convolutional Neural Networks (CNN) andRecurrent Neural Networks (RNN). The framework also supports both CPU and GPUcomputation, and the switch between them is straightforward. Differentapplications in computer vision, natural language processing and robotics aredemonstrated as experiments.

Reliable Attribute-Based Object Recognition Using High Predictive Value  Classifiers

  We consider the problem of object recognition in 3D using an ensemble ofattribute-based classifiers. We propose two new concepts to improveclassification in practical situations, and show their implementation in anapproach implemented for recognition from point-cloud data. First, the viewingconditions can have a strong influence on classification performance. We studythe impact of the distance between the camera and the object and propose anapproach to fuse multiple attribute classifiers, which incorporates distanceinto the decision making. Second, lack of representative training samples oftenmakes it difficult to learn the optimal threshold value for best positive andnegative detection rate. We address this issue, by setting in our attributeclassifiers instead of just one threshold value, two threshold values todistinguish a positive, a negative and an uncertainty class, and we prove thetheoretical correctness of this approach. Empirical studies demonstrate theeffectiveness and feasibility of the proposed concepts.

Co-active Learning to Adapt Humanoid Movement for Manipulation

  In this paper we address the problem of robot movement adaptation undervarious environmental constraints interactively. Motion primitives aregenerally adopted to generate target motion from demonstrations. However, theirgeneralization capability is weak while facing novel environments.Additionally, traditional motion generation methods do not consider theversatile constraints from various users, tasks, and environments. In thiswork, we propose a co-active learning framework for learning to adapt robotend-effector's movement for manipulation tasks. It is designed to adapt theoriginal imitation trajectories, which are learned from demonstrations, tonovel situations with various constraints. The framework also considers user'sfeedback towards the adapted trajectories, and it learns to adapt movementthrough human-in-the-loop interactions. The implemented system generalizestrained motion primitives to various situations with different constraintsconsidering user preferences. Experiments on a humanoid platform validate theeffectiveness of our approach.

Fast Task-Specific Target Detection via Graph Based Constraints  Representation and Checking

  In this work, we present a fast target detection framework for real-worldrobotics applications. Considering that an intelligent agent attends to atask-specific object target during execution, our goal is to detect the objectefficiently. We propose the concept of early recognition, which influences thecandidate proposal process to achieve fast and reliable detection performance.To check the target constraints efficiently, we put forward a novel policy togenerate a sub-optimal checking order, and prove that it has bounded time costcompared to the optimal checking sequence, which is not achievable inpolynomial time. Experiments on two different scenarios: 1) rigid object and 2)non-rigid body part detection validate our pipeline. To show that our method iswidely applicable, we further present a human-robot interaction system based onour non-rigid body part detection.

Answering Image Riddles using Vision and Reasoning through Probabilistic  Soft Logic

  In this work, we explore a genre of puzzles ("image riddles") which involvesa set of images and a question. Answering these puzzles require bothcapabilities involving visual detection (including object, activityrecognition) and, knowledge-based or commonsense reasoning. We compile adataset of over 3k riddles where each riddle consists of 4 images and agroundtruth answer. The annotations are validated using crowd-sourcedevaluation. We also define an automatic evaluation metric to track futureprogress. Our task bears similarity with the commonly known IQ tasks such asanalogy solving, sequence filling that are often used to test intelligence.  We develop a Probabilistic Reasoning-based approach that utilizesprobabilistic commonsense knowledge to answer these riddles with a reasonableaccuracy. We demonstrate the results of our approach using both automatic andhuman evaluations. Our approach achieves some promising results for theseriddles and provides a strong baseline for future attempts. We make the entiredataset and related materials publicly available to the community inImageRiddle Website (http://bit.ly/22f9Ala).

On the Importance of Consistency in Training Deep Neural Networks

  We explain that the difficulties of training deep neural networks come from asyndrome of three consistency issues. This paper describes our efforts in theiranalysis and treatment. The first issue is the training speed inconsistency indifferent layers. We propose to address it with an intuitive,simple-to-implement, low footprint second-order method. The second issue is thescale inconsistency between the layer inputs and the layer residuals. Weexplain how second-order information provides favorable convenience in removingthis roadblock. The third and most challenging issue is the inconsistency inresidual propagation. Based on the fundamental theorem of linear algebra, weprovide a mathematical characterization of the famous vanishing gradientproblem. Thus, an important design principle for future optimization and neuralnetwork design is derived. We conclude this paper with the construction of anovel contractive neural network.

DeepSIC: Deep Semantic Image Compression

  Incorporating semantic information into the codecs during image compressioncan significantly reduce the repetitive computation of fundamental semanticanalysis (such as object recognition) in client-side applications. The samepractice also enable the compressed code to carry the image semanticinformation during storage and transmission. In this paper, we propose aconcept called Deep Semantic Image Compression (DeepSIC) and put forward twonovel architectures that aim to reconstruct the compressed image and generatecorresponding semantic representations at the same time. The first architectureperforms semantic analysis in the encoding process by reserving a portion ofthe bits from the compressed code to store the semantic representations. Thesecond performs semantic analysis in the decoding step with the feature mapsthat are embedded in the compressed code. In both architectures, the featuremaps are shared by the compression and the semantic analytics modules. Tovalidate our approaches, we conduct experiments on the publicly availablebenchmarking datasets and achieve promising results. We also provide a thoroughanalysis of the advantages and disadvantages of the proposed technique.

Explicit Reasoning over End-to-End Neural Architectures for Visual  Question Answering

  Many vision and language tasks require commonsense reasoning beyonddata-driven image and natural language processing. Here we adopt VisualQuestion Answering (VQA) as an example task, where a system is expected toanswer a question in natural language about an image. Current state-of-the-artsystems attempted to solve the task using deep neural architectures andachieved promising performance. However, the resulting systems are generallyopaque and they struggle in understanding questions for which extra knowledgeis required. In this paper, we present an explicit reasoning layer on top of aset of penultimate neural network based systems. The reasoning layer enablesreasoning and answering questions where additional knowledge is required, andat the same time provides an interpretable interface to the end users.Specifically, the reasoning layer adopts a Probabilistic Soft Logic (PSL) basedengine to reason over a basket of inputs: visual relations, the semantic parseof the question, and background ontological knowledge from word2vec andConceptNet. Experimental analysis of the answers and the key evidentialpredicates generated on the VQA dataset validate our approach.

Weakly Supervised Attention Learning for Textual Phrases Grounding

  Grounding textual phrases in visual content is a meaningful yet challengingproblem with various potential applications such as image-text inference ortext-driven multimedia interaction. Most of the current existing methods adoptthe supervised learning mechanism which requires ground-truth at pixel levelduring training. However, fine-grained level ground-truth annotation is quitetime-consuming and severely narrows the scope for more general applications. Inthis extended abstract, we explore methods to localize flexibly image regionsfrom the top-down signal (in a form of one-hot label or natural languages) witha weakly supervised attention learning mechanism. In our model, two types ofmodules are utilized: a backbone module for visual feature capturing, and anattentive module generating maps based on regularized bilinear pooling. Weconstruct the model in an end-to-end fashion which is trained by encouragingthe spatial attentive map to shift and focus on the region that consists of thebest matched visual features with the top-down signal. We demonstrate thepreliminary yet promising results on a testbed that is synthesized withmulti-label MNIST data.

Active Object Perceiver: Recognition-guided Policy Learning for Object  Searching on Mobile Robots

  We study the problem of learning a navigation policy for a robot to activelysearch for an object of interest in an indoor environment solely from itsvisual inputs. While scene-driven visual navigation has been widely studied,prior efforts on learning navigation policies for robots to find objects arelimited. The problem is often more challenging than target scene finding as thetarget objects can be very small in the view and can be in an arbitrary pose.We approach the problem from an active perceiver perspective, and propose anovel framework that integrates a deep neural network based object recognitionmodule and a deep reinforcement learning based action prediction mechanism. Tovalidate our method, we conduct experiments on both a simulation dataset(AI2-THOR) and a real-world environment with a physical robot. We furtherpropose a new decaying reward function to learn the control policy specific tothe object searching task. Experimental results validate the efficacy of ourmethod, which outperforms competing methods in both average trajectory lengthand success rate.

GAPLE: Generalizable Approaching Policy LEarning for Robotic Object  Searching in Indoor Environment

  We study the problem of learning a generalizable action policy for anintelligent agent to actively approach an object of interest in an indoorenvironment solely from its visual inputs. While scene-driven orrecognition-driven visual navigation has been widely studied, prior effortssuffer severely from the limited generalization capability. In this paper, wefirst argue the object searching task is environment dependent while theapproaching ability is general. To learn a generalizable approaching policy, wepresent a novel solution dubbed as GAPLE which adopts two channels of visualfeatures: depth and semantic segmentation, as the inputs to the policy learningmodule. The empirical studies conducted on the House3D dataset as well as on aphysical platform in a real world scenario validate our hypothesis, and wefurther provide in-depth qualitative analysis.

How Shall I Drive? Interaction Modeling and Motion Planning towards  Empathetic and Socially-Graceful Driving

  While intelligence of autonomous vehicles (AVs) has significantly advanced inrecent years, accidents involving AVs suggest that these autonomous systemslack gracefulness in driving when interacting with human drivers. In thesetting of a two-player game, we propose model predictive control based onsocial gracefulness, which is measured by the discrepancy between the actionstaken by the AV and those that could have been taken in favor of the humandriver. We define social awareness as the ability of an agent to infer suchfavorable actions based on knowledge about the other agent's intent, andfurther show that empathy, i.e., the ability to understand others' intent bysimultaneously inferring others' understanding of the agent's self intent, iscritical to successful intent inference. Lastly, through an intersection case,we show that the proposed gracefulness objective allows an AV to learn moresophisticated behavior, such as passive-aggressive motions that gently forcethe other agent to yield.

Improving Model Robustness with Transformation-Invariant Attacks

  Vulnerability of neural networks under adversarial attacks has raised seriousconcerns and extensive research. Recent studies suggested that model robustnessrelies on the use of robust features, i.e., features with strong correlationwith labels, and that data dimensionality and distribution affect the learningof robust features. On the other hand, experiments showed that human vision,which is robust against adversarial attacks, is invariant to natural inputtransformations. Drawing on these findings, this paper investigates whetherconstraints on transformation invariance, including image cropping, rotation,and zooming, will force image classifiers to learn and use robust features andin turn acquire better robustness. Experiments on MNIST and CIFAR10 show thattransformation invariance alone has limited effect. Nonetheless, modelsadversarially trained on cropping-invariant attacks, in particular, can (1)extract more robust features, (2) have significantly better robustness than thestate-of-the-art models from adversarial training, and (3) require lesstraining data.

Image Decomposition and Classification through a Generative Model

  We demonstrate in this paper that a generative model can be designed toperform classification tasks under challenging settings, including adversarialattacks and input distribution shifts. Specifically, we propose a conditionalvariational autoencoder that learns both the decomposition of inputs and thedistributions of the resulting components. During test, we jointly optimize thelatent variables of the generator and the relaxed component labels to find thebest match between the given input and the output of the generator. The modeldemonstrates promising performance at recognizing overlapping components fromthe multiMNIST dataset, and novel component combinations from a traffic signdataset. Experiments also show that the proposed model achieves high robustnesson MNIST and NORB datasets, in particular for high-strength gradient attacksand non-gradient attacks.

TKD: Temporal Knowledge Distillation for Active Perception

  Deep neural networks based methods have been proved to achieve outstandingperformance on object detection and classification tasks. Despite significantperformance improvement, due to the deep structures, they still requireprohibitive runtime to process images and maintain the highest possibleperformance for real-time applications. Observing the phenomenon that humanvision system (HVS) relies heavily on the temporal dependencies among framesfrom the visual input to conduct recognition efficiently, we propose a novelframework dubbed as TKD: temporal knowledge distillation. This frameworkdistills the temporal knowledge from a heavy neural networks based model overselected video frames (the perception of the moments) to a light-weight model.To enable the distillation, we put forward two novel procedures: 1) anLong-short Term Memory (LSTM) based key frame selection method; and 2) a novelteacher-bounded loss design. To validate, we conduct comprehensive empiricalevaluations using different object detection methods over multiple datasetsincluding Youtube-Objects and Hollywood scene dataset. Our results showconsistent improvement in accuracy-speed trad-offs for object detection overthe frames of the dynamic scene, compare to other modern object recognitionmethods.

Modularized Textual Grounding for Counterfactual Resilience

  Computer Vision applications often require a textual grounding module withprecision, interpretability, and resilience to counterfactual inputs/queries.To achieve high grounding precision, current textual grounding methods heavilyrely on large-scale training data with manual annotations at the pixel level.Such annotations are expensive to obtain and thus severely narrow the model'sscope of real-world applications. Moreover, most of these methods sacrificeinterpretability, generalizability, and they neglect the importance of beingresilient to counterfactual inputs. To address these issues, we propose avisual grounding system which is 1) end-to-end trainable in a weakly supervisedfashion with only image-level annotations, and 2) counterfactually resilientowing to the modular design. Specifically, we decompose textual descriptionsinto three levels: entity, semantic attribute, color information, and performcompositional grounding progressively. We validate our model through a seriesof experiments and demonstrate its improvement over the state-of-the-artmethods. In particular, our model's performance not only surpasses otherweakly/un-supervised methods and even approaches the strongly supervised ones,but also is interpretable for decision making and performs much better in faceof counterfactual classes than all the others.

Prediction of Manipulation Actions

  Looking at a person's hands one often can tell what the person is going to donext, how his/her hands are moving and where they will be, because an actor'sintentions shape his/her movement kinematics during action execution.Similarly, active systems with real-time constraints must not simply rely onpassive video-segment classification, but they have to continuously updatetheir estimates and predict future actions. In this paper, we study theprediction of dexterous actions. We recorded from subjects performing differentmanipulation actions on the same object, such as "squeezing", "flipping","washing", "wiping" and "scratching" with a sponge. In psychophysicalexperiments, we evaluated human observers' skills in predicting actions fromvideo sequences of different length, depicting the hand movement in thepreparation and execution of actions before and after contact with the object.We then developed a recurrent neural network based method for action predictionusing as input patches around the hand. We also used the same formalism topredict the forces on the finger tips using for training synchronized video andforce data streams. Evaluations on two new datasets showed that our systemclosely matches human performance in the recognition task, and demonstrate theability of our algorithm to predict what and how a dexterous action isperformed.

Neural Style Transfer: A Review

  The seminal work of Gatys et al. demonstrated the power of ConvolutionalNeural Networks (CNNs) in creating artistic imagery by separating andrecombining image content and style. This process of using CNNs to render acontent image in different styles is referred to as Neural Style Transfer(NST). Since then, NST has become a trending topic both in academic literatureand industrial applications. It is receiving increasing attention and a varietyof approaches are proposed to either improve or extend the original NSTalgorithm. In this paper, we aim to provide a comprehensive overview of thecurrent progress towards NST. We first propose a taxonomy of current algorithmsin the field of NST. Then, we present several evaluation methods and comparedifferent NST algorithms both qualitatively and quantitatively. The reviewconcludes with a discussion of various applications of NST and open problemsfor future research. A list of papers discussed in this review, correspondingcodes, pre-trained models and more comparison results are publicly available athttps://github.com/ycjing/Neural-Style-Transfer-Papers.

Improving utility of brain tumor confocal laser endomicroscopy:  objective value assessment and diagnostic frame detection with convolutional  neural networks

  Confocal laser endomicroscopy (CLE), although capable of obtaining images atcellular resolution during surgery of brain tumors in real time, creates asmany non-diagnostic as diagnostic images. Non-useful images are often distorteddue to relative motion between probe and brain or blood artifacts. Many images,however, simply lack diagnostic features immediately informative to thephysician. Examining all the hundreds or thousands of images from a single caseto discriminate diagnostic images from nondiagnostic ones can be tedious.Providing a real-time diagnostic value assessment of images (fast enough to beused during the surgical acquisition process and accurate enough for thepathologist to rely on) to automatically detect diagnostic frames wouldstreamline the analysis of images and filter useful images for thepathologist/surgeon. We sought to automatically classify images as diagnosticor non-diagnostic. AlexNet, a deep-learning architecture, was used in a 4-foldcross validation manner. Our dataset includes 16,795 images (8572 nondiagnosticand 8223 diagnostic) from 74 CLE-aided brain tumor surgery patients. The groundtruth for all the images is provided by the pathologist. Average model accuracyon test data was 91% overall (90.79 % accuracy, 90.94 % sensitivity and 90.87 %specificity). To evaluate the model reliability we also performed receiveroperating characteristic (ROC) analysis yielding 0.958 average for the areaunder ROC curve (AUC). These results demonstrate that a deeply trained AlexNetnetwork can achieve a model that reliably and quickly recognizes diagnostic CLEimages.

Prospects for Theranostics in Neurosurgical Imaging: Empowering Confocal  Laser Endomicroscopy Diagnostics via Deep Learning

  Confocal laser endomicroscopy (CLE) is an advanced optical fluorescenceimaging technology that has the potential to increase intraoperative precision,extend resection, and tailor surgery for malignant invasive brain tumorsbecause of its subcellular dimension resolution. Despite its promisingdiagnostic potential, interpreting the gray tone fluorescence images can bedifficult for untrained users. In this review, we provide a detaileddescription of bioinformatical analysis methodology of CLE images that beginsto assist the neurosurgeon and pathologist to rapidly connect on-the-flyintraoperative imaging, pathology, and surgical observation into aconclusionary system within the concept of theranostics. We present an overviewand discuss deep learning models for automatic detection of the diagnostic CLEimages and discuss various training regimes and ensemble modeling effect on thepower of deep learning predictive models. Two major approaches reviewed in thispaper include the models that can automatically classify CLE images intodiagnostic/nondiagnostic, glioma/nonglioma, tumor/injury/normal categories andmodels that can localize histological features on the CLE images using weaklysupervised methods. We also briefly review advances in the deep learningapproaches used for CLE image analysis in other organs. Significant advances inspeed and precision of automated diagnostic frame selection would augment thediagnostic potential of CLE, improve operative workflow and integration intobrain tumor surgery. Such technology and bioinformatics analytics lendthemselves to improved precision, personalization, and theranostics in braintumor treatment.

Interpretable Partitioned Embedding for Customized Fashion Outfit  Composition

  Intelligent fashion outfit composition becomes more and more popular in theseyears. Some deep learning based approaches reveal competitive compositionrecently. However, the unexplainable characteristic makes such deep learningbased approach cannot meet the the designer, businesses and consumers' urge tocomprehend the importance of different attributes in an outfit composition. Torealize interpretable and customized fashion outfit compositions, we propose apartitioned embedding network to learn interpretable representations fromclothing items. The overall network architecture consists of three components:an auto-encoder module, a supervised attributes module and a multi-independentmodule. The auto-encoder module serves to encode all useful information intothe embedding. In the supervised attributes module, multiple attributes labelsare adopted to ensure that different parts of the overall embedding correspondto different attributes. In the multi-independent module, adversarial operationare adopted to fulfill the mutually independent constraint. With theinterpretable and partitioned embedding, we then construct an outfitcomposition graph and an attribute matching map. Given specified attributesdescription, our model can recommend a ranked list of outfit composition withinterpretable matching scores. Extensive experiments demonstrate that 1) thepartitioned embedding have unmingled parts which corresponding to differentattributes and 2) outfits recommended by our model are more desirable incomparison with the existing methods.

Spatial Knowledge Distillation to aid Visual Reasoning

  For tasks involving language and vision, the current state-of-the-art methodstend not to leverage any additional information that might be present to gatherrelevant (commonsense) knowledge. A representative task is Visual QuestionAnswering where large diagnostic datasets have been proposed to test a system'scapability of answering questions about images. The training data is oftenaccompanied by annotations of individual object properties and spatiallocations. In this work, we take a step towards integrating this additionalprivileged information in the form of spatial knowledge to aid in visualreasoning. We propose a framework that combines recent advances in knowledgedistillation (teacher-student framework), relational reasoning andprobabilistic logical languages to incorporate such knowledge in existingneural networks for the task of Visual Question Answering. Specifically, for aquestion posed against an image, we use a probabilistic logical language toencode the spatial knowledge and the spatial understanding about the questionin the form of a mask that is directly provided to the teacher network. Thestudent network learns from the ground-truth information as well as theteachers prediction via distillation. We also demonstrate the impact ofpredicting such a mask inside the teachers network using attention.Empirically, we show that both the methods improve the test accuracy over astate-of-the-art approach on a publicly available dataset.

Convolutional Neural Networks: Ensemble Modeling, Fine-Tuning and  Unsupervised Semantic Localization for Intraoperative CLE Images

  Confocal laser endomicroscopy (CLE) is an advanced optical fluorescencetechnology undergoing assessment for applications in brain tumor surgery.Despite its promising potential, interpreting the unfamiliar gray tone imagesof fluorescent stains can be difficult. Many of the CLE images can be distortedby motion, extremely low or high fluorescence signal, or obscured by red bloodcell accumulation, and these can be interpreted as nondiagnostic. However, justone neat CLE image might suffice for intraoperative diagnosis of the tumor.While manual examination of thousands of nondiagnostic images during surgerywould be impractical, this creates an opportunity for a model to selectdiagnostic images for the pathologists or surgeon's review. In this study, wesought to develop a deep learning model to automatically detect the diagnosticimages using a manually annotated dataset, and we employed a patient-basednested cross-validation approach to explore generalizability of the model. Weexplored various training regimes: deep training, shallow fine-tuning, and deepfine-tuning. Further, we investigated the effect of ensemble modeling bycombining the top-5 single models crafted in the development phase. Welocalized histological features from diagnostic CLE images by visualization ofshallow and deep neural activations. Our inter-rater experiment resultsconfirmed that our ensemble of deeply fine-tuned models achieved higheragreement with the ground truth than the other observers. With the speed andprecision of the proposed method (110 images/second; 85% on the gold standardtest subset), it has potential to be integrated into the operative workflow inthe brain tumor surgery.

Weakly-Supervised Learning-Based Feature Localization in Confocal Laser  Endomicroscopy Glioma Images

  Confocal Laser Endomicroscope (CLE) is a novel handheld fluorescence imagingdevice that has shown promise for rapid intraoperative diagnosis of brain tumortissue. Currently CLE is capable of image display only and lacks an automaticsystem to aid the surgeon in analyzing the images. The goal of this project wasto develop a computer-aided diagnostic approach for CLE imaging of human gliomawith feature localization function. Despite the tremendous progress in objectdetection and image segmentation methods in recent years, most of such methodsrequire large annotated datasets for training. However, manual annotation ofthousands of histopathological images by physicians is costly and timeconsuming. To overcome this problem, we propose a Weakly-Supervised Learning(WSL)-based model for feature localization that trains on image-levelannotations, and then localizes incidences of a class-of-interest in the testimage. We developed a novel convolutional neural network for diagnosticfeatures localization from CLE images by employing a novel multiscaleactivation map that is laterally inhibited and collaterally integrated. Tovalidate our method, we compared proposed model's output to the manualannotation performed by four neurosurgeons on test images. Proposed modelachieved 88% mean accuracy and 86% mean intersection over union on intermediatefeatures and 87% mean accuracy and 88% mean intersection over union onrestrictive fine features, while outperforming other state of the art methodstested. This system can improve accuracy and efficiency in characterization ofCLE images of glioma tissue during surgery, augment intraoperativedecision-making process regarding tumor margin and affect resection rates.

