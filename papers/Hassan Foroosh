A Closed-Form Model for Image-Based Distant Lighting

  In this paper, we present a new mathematical foundation for image-basedlighting. Using a simple manipulation of the local coordinate system, we derivea closed-form solution to the light integral equation under distant environmentillumination. We derive our solution for different BRDF's such as lambertianand Phong-like. The method is free of noise, and provides the possibility ofusing the full spectrum of frequencies captured by images taken from theenvironment. This allows for the color of the rendered object to be tonedaccording to the color of the light in the environment. Experimental resultsalso show that one can gain an order of magnitude or higher in rendering timecompared to Monte Carlo quadrature methods and spherical harmonics.

View-Invariant Recognition of Action Style Self-Dissimilarity

  Self-similarity was recently introduced as a measure of inter-classcongruence for classification of actions. Herein, we investigate the dualproblem of intra-class dissimilarity for classification of action styles. Weintroduce self-dissimilarity matrices that discriminate between same actionsperformed by different subjects regardless of viewing direction and cameraparameters. We investigate two frameworks using these invariant styledissimilarity measures based on Principal Component Analysis (PCA) and FisherDiscriminant Analysis (FDA). Extensive experiments performed on IXMAS datasetindicate remarkably good discriminant characteristics for the proposedinvariant measures for gender recognition from video data.

Sub-Pixel Registration of Wavelet-Encoded Images

  Sub-pixel registration is a crucial step for applications such assuper-resolution in remote sensing, motion compensation in magnetic resonanceimaging, and non-destructive testing in manufacturing, to name a few. Recently,these technologies have been trending towards wavelet encoded imaging andsparse/compressive sensing. The former plays a crucial role in reducing imagingartifacts, while the latter significantly increases the acquisition speed. Inview of these new emerging needs for applications of wavelet encoded imaging,we propose a sub-pixel registration method that can achieve direct waveletdomain registration from a sparse set of coefficients. We make the followingcontributions: (i) We devise a method of decoupling scale, rotation, andtranslation parameters in the Haar wavelet domain, (ii) We derive explicitmathematical expressions that define in-band sub-pixel registration in terms ofwavelet coefficients, (iii) Using the derived expressions, we propose anapproach to achieve in-band subpixel registration, avoiding back and forthtransformations. (iv) Our solution remains highly accurate even when a sparseset of coefficients are used, which is due to localization of signals in asparse set of wavelet coefficients. We demonstrate the accuracy of our method,and show that it outperforms the state-of-the-art on simulated and real data,even when the data is sparse.

View-Invariant Template Matching Using Homography Constraints

  Change in viewpoint is one of the major factors for variation in objectappearance across different images. Thus, view-invariant object recognition isa challenging and important image understanding task. In this paper, we proposea method that can match objects in images taken under different viewpoints.Unlike most methods in the literature, no restriction on camera orientations orinternal camera parameters are imposed and no prior knowledge of 3D structureof the object is required. We prove that when two cameras take pictures of thesame object from two different viewing angels, the relationship between everyquadruple of points reduces to the special case of homography with two equaleigenvalues. Based on this property, we formulate the problem as an errorfunction that indicates how likely two sets of 2D points are projections of thesame set of 3D points under two different cameras. Comprehensive set ofexperiments were conducted to prove the robustness of the method to noise, andevaluate its performance on real-world applications, such as face and objectrecognition.

Motion-Compensated Temporal Filtering for Critically-Sampled  Wavelet-Encoded Images

  We propose a novel motion estimation/compensation (ME/MC) method forwavelet-based (in-band) motion compensated temporal filtering (MCTF), withapplication to low-bitrate video coding. Unlike the conventional in-band MCTFalgorithms, which require redundancy to overcome the shift-variance problem ofcritically sampled (complete) discrete wavelet transforms (DWT), we performME/MC steps directly on DWT coefficients by avoiding the need ofshift-invariance. We omit upsampling, the inverse-DWT (IDWT), and thecalculation of redundant DWT coefficients, while achieving arbitrary subpixelaccuracy without interpolation, and high video quality even at verylow-bitrates, by deriving the exact relationships between DWT subbands of inputimage sequences. Experimental results demonstrate the accuracy of the proposedmethod, confirming that our model for ME/MC effectively improves video codingquality.

Volumetric Super-Resolution of Multispectral Data

  Most multispectral remote sensors (e.g. QuickBird, IKONOS, and Landsat 7ETM+) provide low-spatial high-spectral resolution multispectral (MS) orhigh-spatial low-spectral resolution panchromatic (PAN) images, separately. Inorder to reconstruct a high-spatial/high-spectral resolution multispectralimage volume, either the information in MS and PAN images are fused (i.e.pansharpening) or super-resolution reconstruction (SRR) is used with only MSimages captured on different dates. Existing methods do not utilize temporalinformation of MS and high spatial resolution of PAN images together to improvethe resolution. In this paper, we propose a multiframe SRR algorithm usingpansharpened MS images, taking advantage of both temporal and spatialinformation available in multispectral imagery, in order to exceed spatialresolution of given PAN images. We first apply pansharpening to a set ofmultispectral images and their corresponding PAN images captured on differentdates. Then, we use the pansharpened multispectral images as input to theproposed wavelet-based multiframe SRR method to yield full volumetric SRR. Theproposed SRR method is obtained by deriving the subband relations betweenmultitemporal MS volumes. We demonstrate the results on Landsat 7 ETM+ imagescomparing our method to conventional techniques.

Phase-Shifting Separable Haar Wavelets and Applications

  This paper presents a new approach for tackling the shift-invariance problemin the discrete Haar domain, without trading off any of its desirableproperties, such as compression, separability, orthogonality, and symmetry. Thepaper presents several key theoretical contributions. First, we derive closedform expressions for phase shifting in the Haar domain both in partiallydecimated and fully decimated transforms. Second, it is shown that the waveletcoefficients of the shifted signal can be computed solely by using thecoefficients of the original transformed signal. Third, we derive closed-formexpressions for non-integer shifts, which have not been previously reported inthe literature. Fourth, we establish the complexity of the proposed phaseshifting approach using the derived analytic expressions. As an applicationexample of these results, we apply the new formulae to image rotation andinterpolation, and evaluate its performance against standard methods.

An Invariant Model of the Significance of Different Body Parts in  Recognizing Different Actions

  In this paper, we show that different body parts do not play equallyimportant roles in recognizing a human action in video data. We investigate towhat extent a body part plays a role in recognition of different actions andhence propose a generic method of assigning weights to different body points.The approach is inspired by the strong evidence in the applied perceptioncommunity that humans perform recognition in a foveated manner, that is theyrecognize events or objects by only focusing on visually significant aspects.An important contribution of our method is that the computation of the weightsassigned to body parts is invariant to viewing directions and camera parametersin the input data. We have performed extensive experiments to validate theproposed approach and demonstrate its significance. In particular, results showthat considerable improvement in performance is gained by taking into accountthe relative importance of different body parts as defined by our approach.

Image Annotation using Multi-Layer Sparse Coding

  Automatic annotation of images with descriptive words is a challengingproblem with vast applications in the areas of image search and retrieval. Thisproblem can be viewed as a label-assignment problem by a classifier dealingwith a very large set of labels, i.e., the vocabulary set. We propose a novelannotation method that employs two layers of sparse coding and performscoarse-to-fine labeling. Themes extracted from the training data are treated ascoarse labels. Each theme is a set of training images that share a commonsubject in their visual and textual contents. Our system extracts coarse labelsfor training and test images without requiring any prior knowledge. Vocabularywords are the fine labels to be associated with images. Most of the annotationmethods achieve low recall due to the large number of available fine labels,i.e., vocabulary words. These systems also tend to achieve high precision forhighly frequent words only while relatively rare words are more important forsearch and retrieval purposes. Our system not only outperforms variouspreviously proposed annotation systems, but also achieves symmetric response interms of precision and recall. Our system scores and maintains high precisionfor words with a wide range of frequencies. Such behavior is achieved byintelligently reducing the number of available fine labels or words for eachimage based on coarse labels assigned to it.

Single Image Action Recognition by Predicting Space-Time Saliency

  We propose a novel approach based on deep Convolutional Neural Networks (CNN)to recognize human actions in still images by predicting the future motion, anddetecting the shape and location of the salient parts of the image. We make thefollowing major contributions to this important area of research: (i) We usethe predicted future motion in the static image (Walker et al., 2015) as ameans of compensating for the missing temporal information, while using thesaliency map to represent the the spatial information in the form of locationand shape of what is predicted as significant. (ii) We cast actionclassification in static images as a domain adaptation problem by transferlearning. We first map the input static image to a new domain that we refer toas the Predicted Optical Flow-Saliency Map domain (POF-SM), and then fine-tunethe layers of a deep CNN model trained on classifying the ImageNet dataset toperform action classification in the POF-SM domain. (iii) We tested our methodon the popular Willow dataset. But unlike existing methods, we also tested on amore realistic and challenging dataset of over 2M still images that wecollected and labeled by taking random frames from the UCF-101 video dataset.We call our dataset the UCF Still Image dataset or UCFSI-101 in short. Ourresults outperform the state of the art.

Learning Semantics for Image Annotation

  Image search and retrieval engines rely heavily on textual annotation inorder to match word queries to a set of candidate images. A system that canautomatically annotate images with meaningful text can be highly beneficial forsuch engines. Currently, the approaches to develop such systems try toestablish relationships between keywords and visual features of images. In thispaper, We make three main contributions to this area: (i) We transform thisproblem from the low-level keyword space to the high-level semantics space thatwe refer to as the "{\em image theme}", (ii) Instead of treating each possiblekeyword independently, we use latent Dirichlet allocation to learn image themesfrom the associated texts in a training phase. Images are then annotated withimage themes rather than keywords, using a modified continuous relevance model,which takes into account the spatial coherence and the visual continuity amongimages of common theme. (iii) To achieve more coherent annotations among imagesof common theme, we have integrated ConceptNet in learning the semantics ofimages, and hence augment image descriptions beyond annotations provided byhumans. Images are thus further annotated by a few most significant words ofthe prominent image theme. Our extensive experiments show that a coherenttheme-based image annotation using high-level semantics results in improvedprecision and recall as compared with equivalent classical keyword annotationsystems.

Non-Linear Phase-Shifting of Haar Wavelets for Run-Time All-Frequency  Lighting

  This paper focuses on real-time all-frequency image-based rendering using aninnovative solution for run-time computation of light transport. The approachis based on new results derived for non-linear phase shifting in the Haarwavelet domain. Although image-based methods for real-time rendering of dynamicglossy objects have been proposed, they do not truly scale to all possiblefrequencies and high sampling rates without trading storage, glossiness, orcomputational time, while varying both lighting and viewpoint. This is due tothe fact that current approaches are limited to precomputed radiance transfer(PRT), which is prohibitively expensive in terms of memory requirements andreal-time rendering when both varying light and viewpoint changes are requiredtogether with high sampling rates for high frequency lighting of glossymaterial. On the other hand, current methods cannot handle object rotation,which is one of the paramount issues for all PRT methods using wavelets. Thislatter problem arises because the precomputed data are defined in a globalcoordinate system and encoded in the wavelet domain, while the object isrotated in a local coordinate system. At the root of all the above problems isthe lack of efficient run-time solution to the nontrivial problem of rotatingwavelets (a non-linear phase-shift), which we solve in this paper.

Super-Resolution of Wavelet-Encoded Images

  Multiview super-resolution image reconstruction (SRIR) is often cast as aresampling problem by merging non-redundant data from multiple low-resolution(LR) images on a finer high-resolution (HR) grid, while inverting the effect ofthe camera point spread function (PSF). One main problem with multiview methodsis that resampling from nonuniform samples (provided by LR images) and theinversion of the PSF are highly nonlinear and ill-posed problems. Non-linearityand ill-posedness are typically overcome by linearization and regularization,often through an iterative optimization process, which essentially trade offthe very same information (i.e. high frequency) that we want to recover. Wepropose a novel point of view for multiview SRIR: Unlike existing multiviewmethods that reconstruct the entire spectrum of the HR image from the multiplegiven LR images, we derive explicit expressions that show how thehigh-frequency spectra of the unknown HR image are related to the spectra ofthe LR images. Therefore, by taking any of the LR images as the reference torepresent the low-frequency spectra of the HR image, one can reconstruct thesuper-resolution image by focusing only on the reconstruction of thehigh-frequency spectra. This is very much like single-image methods, whichextrapolate the spectrum of one image, except that we rely on informationprovided by all other views, rather than by prior constraints as insingle-image methods (which may not be an accurate source of information). Thisis made possible by deriving and applying explicit closed-form expressions thatdefine how the local high frequency information that we aim to recover for thereference high resolution image is related to the local low frequencyinformation in the sequence of views. Results and comparisons with recentlypublished state-of-the-art methods show the superiority of the proposedsolution.

Design of Efficient Convolutional Layers using Single Intra-channel  Convolution, Topological Subdivisioning and Spatial "Bottleneck" Structure

  Deep convolutional neural networks achieve remarkable visual recognitionperformance, at the cost of high computational complexity. In this paper, wehave a new design of efficient convolutional layers based on three schemes. The3D convolution operation in a convolutional layer can be considered asperforming spatial convolution in each channel and linear projection acrosschannels simultaneously. By unravelling them and arranging the spatialconvolution sequentially, the proposed layer is composed of a singleintra-channel convolution, of which the computation is negligible, and a linearchannel projection. A topological subdivisioning is adopted to reduce theconnection between the input channels and output channels. Additionally, wealso introduce a spatial "bottleneck" structure that utilizes aconvolution-projection-deconvolution pipeline to take advantage of thecorrelation between adjacent pixels in the input. Our experiments demonstratethat the proposed layers remarkably outperform the standard convolutionallayers with regard to accuracy/complexity ratio. Our models achieve similaraccuracy to VGG, ResNet-50, ResNet-101 while requiring 42, 4.5, 6.5 times lesscomputation respectively.

Simultaneous Detection and Quantification of Retinal Fluid with Deep  Learning

  We propose a new deep learning approach for automatic detection andsegmentation of fluid within retinal OCT images. The proposed frameworkutilizes both ResNet and Encoder-Decoder neural network architectures. Whentraining the network, we apply a novel data augmentation method called myopicwarping together with standard rotation-based augmentation to increase thetraining set size to 45 times the original amount. Finally, the network outputis post-processed with an energy minimization algorithm (graph cut) along witha few other knowledge guided morphological operations to finalize thesegmentation process. Based on OCT imaging data and its ground truth from theRETOUCH challenge, the proposed system achieves dice indices of 0.522, 0.682,and 0.612, and average absolute volume differences of 0.285, 0.115, and 0.156mm$^3$ for intaretinal fluid, subretinal fluid, and pigment epithelialdetachment respectively.

Probabilistic Sparse Subspace Clustering Using Delayed Association

  Discovering and clustering subspaces in high-dimensional data is afundamental problem of machine learning with a wide range of applications indata mining, computer vision, and pattern recognition. Earlier methods dividedthe problem into two separate stages of finding the similarity matrix andfinding clusters. Similar to some recent works, we integrate these two stepsusing a joint optimization approach. We make the following contributions: (i)we estimate the reliability of the cluster assignment for each point beforeassigning a point to a subspace. We group the data points into two groups of"certain" and "uncertain", with the assignment of latter group delayed untiltheir subspace association certainty improves. (ii) We demonstrate that delayedassociation is better suited for clustering subspaces that have ambiguities,i.e. when subspaces intersect or data are contaminated with outliers/noise.(iii) We demonstrate experimentally that such delayed probabilistic associationleads to a more accurate self-representation and final clusters. The proposedmethod has higher accuracy both for points that exclusively lie in onesubspace, and those that are on the intersection of subspaces. (iv) We showthat delayed association leads to huge reduction of computational cost, sinceit allows for incremental spectral clustering.

Rediscovering Deep Neural Networks in Finite-State Distributions

  We propose a new way of thinking about deep neural networks, in which thelinear and non-linear components of the network are naturally derived andjustified in terms of principles in probability theory. In particular, themodels constructed in our framework assign probabilities to uncertainrealizations, leading to Kullback-Leibler Divergence (KLD) as the linear layer.In our model construction, we also arrive at a structure similar to ReLUactivation supported with Bayes' theorem. The non-linearities in our frameworkare normalization layers with ReLU and Sigmoid as element-wise approximations.Additionally, the pooling function is derived as a marginalization of spatialrandom variables according to the mechanics of the framework. As such, MaxPooling is an approximation to the aforementioned marginalization process.Since our models are comprised of finite state distributions (FSD) as variablesand parameters, exact computation of information-theoretic quantities such asentropy and KLD is possible, thereby providing more objective measures toanalyze networks. Unlike existing designs that rely on heuristics, the proposedframework restricts subjective interpretations of CNNs and sheds light on thefunctionality of neural networks from a completely new perspective.

ComDefend: An Efficient Image Compression Model to Defend Adversarial  Examples

  Deep neural networks (DNNs) have been demonstrated to be vulnerable toadversarial examples. Specifically, adding imperceptible perturbations to cleanimages can fool the well trained deep neural networks. In this paper, wepropose an end-to-end image compression model to defend adversarial examples:\textbf{ComDefend}. The proposed model consists of a compression convolutionalneural network (ComCNN) and a reconstruction convolutional neural network(ResCNN). The ComCNN is used to maintain the structure information of theoriginal image and purify adversarial perturbations. And the ResCNN is used toreconstruct the original image with high quality. In other words, ComDefend cantransform the adversarial image to its clean version, which is then fed to thetrained classifier. Our method is a pre-processing module, and does not modifythe classifier's structure during the whole process. Therefore, it can becombined with other model-specific defense models to jointly improve theclassifier's robustness. A series of experiments conducted on MNIST, CIFAR10and ImageNet show that the proposed method outperforms the state-of-the-artdefense methods, and is consistently effective to protect classifiers againstadversarial attacks.

A Curriculum Domain Adaptation Approach to the Semantic Segmentation of  Urban Scenes

  During the last half decade, convolutional neural networks (CNNs) havetriumphed over semantic segmentation, which is one of the core tasks in manyapplications such as autonomous driving and augmented reality. However, totrain CNNs requires a considerable amount of data, which is difficult tocollect and laborious to annotate. Recent advances in computer graphics make itpossible to train CNNs on photo-realistic synthetic imagery withcomputer-generated annotations. Despite this, the domain mismatch between thereal images and the synthetic data hinders the models' performance. Hence, wepropose a curriculum-style learning approach to minimizing the domain gap inurban scene semantic segmentation. The curriculum domain adaptation solves easytasks first to infer necessary properties about the target domain; inparticular, the first task is to learn global label distributions over imagesand local distributions over landmark superpixels. These are easy to estimatebecause images of urban scenes have strong idiosyncrasies (e.g., the size andspatial relations of buildings, streets, cars, etc.). We then train asegmentation network, while regularizing its predictions in the target domainto follow those inferred properties. In experiments, our method outperforms thebaselines on two datasets and two backbone networks. We also report extensiveablation studies about our approach.

Sparse One-Time Grab Sampling of Inliers

  Estimating structures in "big data" and clustering them are among the mostfundamental problems in computer vision, pattern recognition, data mining, andmany other other research fields. Over the past few decades, many studies havebeen conducted focusing on different aspects of these problems. One of the mainapproaches that is explored in the literature to tackle the problems of sizeand dimensionality is sampling subsets of the data in order to estimate thecharacteristics of the whole population, e.g. estimating the underlyingclusters or structures in the data. In this paper, we propose a `one-time-grab'sampling algorithm\cite{jaberi2015swift,jaberi2018sparse}. This method can beused as the front end to any supervised or unsupervised clustering method.Rather than focusing on the strategy of maximizing the probability of samplinginliers, our goal is to minimize the number of samples needed to instantiateall underlying model instances. More specifically, our goal is to answer thefollowing question: {\em `Given a very large population of points with $C$embedded structures and gross outliers, what is the minimum number of points$r$ to be selected randomly in one grab in order to make sure with probability$P$ that at least $\varepsilon$ points are selected on each structure, where$\varepsilon$ is the number of degrees of freedom of each structure.'}

