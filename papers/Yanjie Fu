Provably Good Early Detection of Diseases using Non-Sparse
  Covariance-Regularized Linear Discriminant Analysis

  To improve the performance of Linear Discriminant Analysis (LDA) for early
detection of diseases using Electronic Health Records (EHR) data, we propose
\TheName{} -- a novel framework for \emph{\underline{E}HR based
\underline{E}arly \underline{D}etection of \underline{D}iseases} on top of
\emph{Covariance-Regularized} LDA models. Specifically, \TheName\ employs a
\emph{non-sparse} inverse covariance matrix (or namely precision matrix)
estimator derived from graphical lasso and incorporates the estimator into LDA
classifiers to improve classification accuracy. Theoretical analysis on
\TheName\ shows that it can bound the expected error rate of LDA
classification, under certain assumptions. Finally, we conducted extensive
experiments using a large-scale real-world EHR dataset -- CHSN. We compared our
solution with other regularized LDA and downstream classifiers. The result
shows \TheName\ outperforms all baselines and backups our theoretical analysis.


CSWA: Aggregation-Free Spatial-Temporal Community Sensing

  In this paper, we present a novel community sensing paradigm -- {C}ommunity
{S}ensing {W}ithout {A}ggregation}. CSWA is designed to obtain the environment
information (e.g., air pollution or temperature) in each subarea of the target
area, without aggregating sensor and location data collected by community
members. CSWA operates on top of a secured peer-to-peer network over the
community members and proposes a novel \emph{Decentralized Spatial-Temporal
Compressive Sensing} framework based on \emph{Parallelized Stochastic Gradient
Descent}. Through learning the \emph{low-rank structure} via distributed
optimization, CSWA approximates the value of the sensor data in each subarea
(both covered and uncovered) for each sensing cycle using the sensor data
locally stored in each member's mobile device. Simulation experiments based on
real-world datasets demonstrate that CSWA exhibits low approximation error
(i.e., less than $0.2 ^\circ$C in city-wide temperature sensing task and $10$
units of PM2.5 index in urban air pollution sensing) and performs comparably to
(sometimes better than) state-of-the-art algorithms based on the data
aggregation and centralized computation.


Heterogeneous Metric Learning with Content-based Regularization for
  Software Artifact Retrieval

  The problem of software artifact retrieval has the goal to effectively locate
software artifacts, such as a piece of source code, in a large code repository.
This problem has been traditionally addressed through the textual query. In
other words, information retrieval techniques will be exploited based on the
textual similarity between queries and textual representation of software
artifacts, which is generated by collecting words from comments, identifiers,
and descriptions of programs. However, in addition to these semantic
information, there are rich information embedded in source codes themselves.
These source codes, if analyzed properly, can be a rich source for enhancing
the efforts of software artifact retrieval. To this end, in this paper, we
develop a feature extraction method on source codes. Specifically, this method
can capture both the inherent information in the source codes and the semantic
information hidden in the comments, descriptions, and identifiers of the source
codes. Moreover, we design a heterogeneous metric learning approach, which
allows to integrate code features and text features into the same latent
semantic space. This, in turn, can help to measure the artifact similarity by
exploiting the joint power of both code and text features. Finally, extensive
experiments on real-world data show that the proposed method can help to
improve the performances of software artifact retrieval with a significant
margin.


REMIX: Automated Exploration for Interactive Outlier Detection

  Outlier detection is the identification of points in a dataset that do not
conform to the norm. Outlier detection is highly sensitive to the choice of the
detection algorithm and the feature subspace used by the algorithm. Extracting
domain-relevant insights from outliers needs systematic exploration of these
choices since diverse outlier sets could lead to complementary insights. This
challenge is especially acute in an interactive setting, where the choices must
be explored in a time-constrained manner. In this work, we present REMIX, the
first system to address the problem of outlier detection in an interactive
setting. REMIX uses a novel mixed integer programming (MIP) formulation for
automatically selecting and executing a diverse set of outlier detectors within
a time limit. This formulation incorporates multiple aspects such as (i) an
upper limit on the total execution time of detectors (ii) diversity in the
space of algorithms and features, and (iii) meta-learning for evaluating the
cost and utility of detectors. REMIX provides two distinct ways for the analyst
to consume its results: (i) a partitioning of the detectors explored by REMIX
into perspectives through low-rank non-negative matrix factorization; each
perspective can be easily visualized as an intuitive heatmap of experiments
versus outliers, and (ii) an ensembled set of outliers which combines outlier
scores from all detectors. We demonstrate the benefits of REMIX through
extensive empirical validation on real-world data.


LATTE: Application Oriented Social Network Embedding

  In recent years, many research works propose to embed the network structured
data into a low-dimensional feature space, where each node is represented as a
feature vector. However, due to the detachment of embedding process with
external tasks, the learned embedding results by most existing embedding models
can be ineffective for application tasks with specific objectives, e.g.,
community detection or information diffusion. In this paper, we propose study
the application oriented heterogeneous social network embedding problem.
Significantly different from the existing works, besides the network structure
preservation, the problem should also incorporate the objectives of external
applications in the objective function. To resolve the problem, in this paper,
we propose a novel network embedding framework, namely the "appLicAtion
orienTed neTwork Embedding" (Latte) model. In Latte, the heterogeneous network
structure can be applied to compute the node "diffusive proximity" scores,
which capture both local and global network structures. Based on these computed
scores, Latte learns the network representation feature vectors by extending
the autoencoder model model to the heterogeneous network scenario, which can
also effectively unite the objectives of network embedding and external
application tasks. Extensive experiments have been done on real-world
heterogeneous social network datasets, and the experimental results have
demonstrated the outstanding performance of Latte in learning the
representation vectors for specific application tasks.


Fake News Detection with Deep Diffusive Network Model

  In recent years, due to the booming development of online social networks,
fake news for various commercial and political purposes has been appearing in
large numbers and widespread in the online world. With deceptive words, online
social network users can get infected by these online fake news easily, which
has brought about tremendous effects on the offline society already. An
important goal in improving the trustworthiness of information in online social
networks is to identify the fake news timely. This paper aims at investigating
the principles, methodologies and algorithms for detecting fake news articles,
creators and subjects from online social networks and evaluating the
corresponding performance. This paper addresses the challenges introduced by
the unknown characteristics of fake news and diverse connections among news
articles, creators and subjects. Based on a detailed data analysis, this paper
introduces a novel automatic fake news credibility inference model, namely
FakeDetector. Based on a set of explicit and latent features extracted from the
textual information, FakeDetector builds a deep diffusive network model to
learn the representations of news articles, creators and subjects
simultaneously. Extensive experiments have been done on a real-world fake news
dataset to compare FakeDetector with several state-of-the-art models, and the
experimental results have demonstrated the effectiveness of the proposed model.


Explainable Social Contextual Image Recommendation with Hierarchical
  Attention

  Image based social networks are among the most popular social networking
services in recent years. With tremendous images uploaded everyday,
understanding users' preferences to the user-generated images and recommending
them to users have become an urgent need. However, this is a challenging task.
On one hand, we have to overcome the extremely data sparsity issue in image
recommendation. On the other hand, we have to model the complex aspects that
influence users' preferences to these highly subjective content from the
heterogeneous data. In this paper, we develop an explainable social contextual
image recommendation model to simultaneously explain and predict users'
preferences to images. Specifically, in addition to user interest modeling in
the standard recommendation, we identify three key aspects that affect each
user's preference on the social platform, where each aspect summarizes a
contextual representation from the complex relationships between users and
images. We design a hierarchical attention model in recommendation process
given the three contextual aspects. Particularly, the bottom layered attention
networks learn to select informative elements of each aspect from heterogeneous
data, and the top layered attention network learns to score the aspect
importance of the three identified aspects for each user. In this way, we could
overcome the data sparsity issue by leveraging the social contextual aspects
from heterogeneous data, and explain the underlying reasons for each user's
behavior with the learned hierarchial attention scores. Extensive experimental
results on real-world datasets clearly show the superiority of our proposed
model.


SocialGCN: An Efficient Graph Convolutional Network based Model for
  Social Recommendation

  Collaborative Filtering (CF) is one of the most successful approaches for
recommender systems. With the emergence of online social networks, social
recommendation has become a popular research direction. Most of these social
recommendation models utilized each user's local neighbors' preferences to
alleviate the data sparsity issue in CF. However, they only considered the
local neighbors of each user and neglected the process that users' preferences
are influenced as information diffuses in the social network. Recently, Graph
Convolutional Networks~(GCN) have shown promising results by modeling the
information diffusion process in graphs that leverage both graph structure and
node feature information. To this end, in this paper, we propose an effective
graph convolutional neural network based model for social recommendation. Based
on a classical CF model, the key idea of our proposed model is that we borrow
the strengths of GCNs to capture how users' preferences are influenced by the
social diffusion process in social networks. The diffusion of users'
preferences is built on a layer-wise diffusion manner, with the initial user
embedding as a function of the current user's features and a free base user
latent vector that is not contained in the user feature. Similarly, each item's
latent vector is also a combination of the item's free latent vector, as well
as its feature representation. Furthermore, we show that our proposed model is
flexible when user and item features are not available. Finally, extensive
experimental results on two real-world datasets clearly show the effectiveness
of our proposed model.


BL-MNE: Emerging Heterogeneous Social Network Embedding through Broad
  Learning with Aligned Autoencoder

  Network embedding aims at projecting the network data into a low-dimensional
feature space, where the nodes are represented as a unique feature vector and
network structure can be effectively preserved. In recent years, more and more
online application service sites can be represented as massive and complex
networks, which are extremely challenging for traditional machine learning
algorithms to deal with. Effective embedding of the complex network data into
low-dimension feature representation can both save data storage space and
enable traditional machine learning algorithms applicable to handle the network
data. Network embedding performance will degrade greatly if the networks are of
a sparse structure, like the emerging networks with few connections. In this
paper, we propose to learn the embedding representation for a target emerging
network based on the broad learning setting, where the emerging network is
aligned with other external mature networks at the same time. To solve the
problem, a new embedding framework, namely "Deep alIgned autoencoder based
eMbEdding" (DIME), is introduced in this paper. DIME handles the diverse link
and attribute in a unified analytic based on broad learning, and introduces the
multiple aligned attributed heterogeneous social network concept to model the
network structure. A set of meta paths are introduced in the paper, which
define various kinds of connections among users via the heterogeneous link and
attribute information. The closeness among users in the networks are defined as
the meta proximity scores, which will be fed into DIME to learn the embedding
vectors of users in the emerging network. Extensive experiments have been done
on real-world aligned social networks, which have demonstrated the
effectiveness of DIME in learning the emerging network embedding vectors.


Insight-HXMT observations of the first binary neutron star merger
  GW170817

  Finding the electromagnetic (EM) counterpart of binary compact star merger,
especially the binary neutron star (BNS) merger, is critically important for
gravitational wave (GW) astronomy, cosmology and fundamental physics. On Aug.
17, 2017, Advanced LIGO and \textit{Fermi}/GBM independently triggered the
first BNS merger, GW170817, and its high energy EM counterpart, GRB 170817A,
respectively, resulting in a global observation campaign covering gamma-ray,
X-ray, UV, optical, IR, radio as well as neutrinos. The High Energy X-ray
telescope (HE) onboard \textit{Insight}-HXMT (Hard X-ray Modulation Telescope)
is the unique high-energy gamma-ray telescope that monitored the entire GW
localization area and especially the optical counterpart (SSS17a/AT2017gfo)
with very large collection area ($\sim$1000 cm$^2$) and microsecond time
resolution in 0.2-5 MeV. In addition, \textit{Insight}-HXMT quickly implemented
a Target of Opportunity (ToO) observation to scan the GW localization area for
potential X-ray emission from the GW source. Although it did not detect any
significant high energy (0.2-5 MeV) radiation from GW170817, its observation
helped to confirm the unexpected weak and soft nature of GRB 170817A.
Meanwhile, \textit{Insight}-HXMT/HE provides one of the most stringent
constraints (~10$^{-7}$ to 10$^{-6}$ erg/cm$^2$/s) for both GRB170817A and any
other possible precursor or extended emissions in 0.2-5 MeV, which help us to
better understand the properties of EM radiation from this BNS merger.
Therefore the observation of \textit{Insight}-HXMT constitutes an important
chapter in the full context of multi-wavelength and multi-messenger observation
of this historical GW event.


