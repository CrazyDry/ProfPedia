Proceedings of the Twenty-Third Conference on Uncertainty in Artificial
  Intelligence (2007)

  This is the Proceedings of the Twenty-Third Conference on Uncertainty in
Artificial Intelligence, which was held in Vancouver, British Columbia, July 19
- 22 2007.


Revisiting the Softmax Bellman Operator: Theoretical Properties and
  Practical Benefits

  The softmax function has been primarily employed in reinforcement learning
(RL) to improve exploration and provide a differentiable approximation to the
max function, as also observed in the mellowmax paper by Asadi and Littman.
This paper instead focuses on using the softmax function in the Bellman
updates, independent of the exploration strategy. Our main theory provides a
performance bound for the softmax Bellman operator, and shows it converges to
the standard Bellman operator exponentially fast in the inverse temperature
parameter. We also prove that under certain conditions, the softmax operator
can reduce the overestimation error and the gradient noise. A detailed
comparison among different Bellman operators is then presented to show the
trade-off when selecting them. We apply the softmax operator to deep RL by
combining it with the deep Q-network (DQN) and double DQN algorithms in an
off-policy fashion, and demonstrate that these variants can often achieve
better performance in several Atari games, and compare favorably to their
mellowmax counterpart.


Greedy Algorithms for Sparse Reinforcement Learning

  Feature selection and regularization are becoming increasingly prominent
tools in the efforts of the reinforcement learning (RL) community to expand the
reach and applicability of RL. One approach to the problem of feature selection
is to impose a sparsity-inducing form of regularization on the learning method.
Recent work on $L_1$ regularization has adapted techniques from the supervised
learning literature for use with RL. Another approach that has received renewed
attention in the supervised learning community is that of using a simple
algorithm that greedily adds new features. Such algorithms have many of the
good properties of the $L_1$ regularization methods, while also being extremely
efficient and, in some cases, allowing theoretical guarantees on recovery of
the true form of a sparse target function from sampled data. This paper
considers variants of orthogonal matching pursuit (OMP) applied to
reinforcement learning. The resulting algorithms are analyzed and compared
experimentally with existing $L_1$ regularized approaches. We demonstrate that
perhaps the most natural scenario in which one might hope to achieve sparse
recovery fails; however, one variant, OMP-BRM, provides promising theoretical
guarantees under certain assumptions on the feature dictionary. Another
variant, OMP-TD, empirically outperforms prior methods both in approximation
accuracy and efficiency on several benchmark problems.


