Proceedings of the Twenty-Third Conference on Uncertainty in Artificial  Intelligence (2007)

  This is the Proceedings of the Twenty-Third Conference on Uncertainty inArtificial Intelligence, which was held in Vancouver, British Columbia, July 19- 22 2007.

Revisiting the Softmax Bellman Operator: Theoretical Properties and  Practical Benefits

  The softmax function has been primarily employed in reinforcement learning(RL) to improve exploration and provide a differentiable approximation to themax function, as also observed in the mellowmax paper by Asadi and Littman.This paper instead focuses on using the softmax function in the Bellmanupdates, independent of the exploration strategy. Our main theory provides aperformance bound for the softmax Bellman operator, and shows it converges tothe standard Bellman operator exponentially fast in the inverse temperatureparameter. We also prove that under certain conditions, the softmax operatorcan reduce the overestimation error and the gradient noise. A detailedcomparison among different Bellman operators is then presented to show thetrade-off when selecting them. We apply the softmax operator to deep RL bycombining it with the deep Q-network (DQN) and double DQN algorithms in anoff-policy fashion, and demonstrate that these variants can often achievebetter performance in several Atari games, and compare favorably to theirmellowmax counterpart.

Greedy Algorithms for Sparse Reinforcement Learning

  Feature selection and regularization are becoming increasingly prominenttools in the efforts of the reinforcement learning (RL) community to expand thereach and applicability of RL. One approach to the problem of feature selectionis to impose a sparsity-inducing form of regularization on the learning method.Recent work on $L_1$ regularization has adapted techniques from the supervisedlearning literature for use with RL. Another approach that has received renewedattention in the supervised learning community is that of using a simplealgorithm that greedily adds new features. Such algorithms have many of thegood properties of the $L_1$ regularization methods, while also being extremelyefficient and, in some cases, allowing theoretical guarantees on recovery ofthe true form of a sparse target function from sampled data. This paperconsiders variants of orthogonal matching pursuit (OMP) applied toreinforcement learning. The resulting algorithms are analyzed and comparedexperimentally with existing $L_1$ regularized approaches. We demonstrate thatperhaps the most natural scenario in which one might hope to achieve sparserecovery fails; however, one variant, OMP-BRM, provides promising theoreticalguarantees under certain assumptions on the feature dictionary. Anothervariant, OMP-TD, empirically outperforms prior methods both in approximationaccuracy and efficiency on several benchmark problems.

