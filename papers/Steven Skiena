Analysis of airplane boarding via space-time geometry and random matrix
  theory

  We show that airplane boarding can be asymptotically modeled by 2-dimensional
Lorentzian geometry. Boarding time is given by the maximal proper time among
curves in the model. Discrepancies between the model and simulation results are
closely related to random matrix theory. We then show how such models can be
used to explain why some commonly practiced airline boarding policies are
ineffective and even detrimental.


SpeedRead: A Fast Named Entity Recognition Pipeline

  Online content analysis employs algorithmic methods to identify entities in
unstructured text. Both machine learning and knowledge-base approaches lie at
the foundation of contemporary named entities extraction systems. However, the
progress in deploying these approaches on web-scale has been been hampered by
the computational cost of NLP over massive text corpora. We present SpeedRead
(SR), a named entity recognition pipeline that runs at least 10 times faster
than Stanford NLP pipeline. This pipeline consists of a high performance Penn
Treebank- compliant tokenizer, close to state-of-art part-of-speech (POS)
tagger and knowledge-based named entity recognizer.


On the Convergent Properties of Word Embedding Methods

  Do word embeddings converge to learn similar things over different
initializations? How repeatable are experiments with word embeddings? Are all
word embedding techniques equally reliable? In this paper we propose evaluating
methods for learning word representations by their consistency across
initializations. We propose a measure to quantify the similarity of the learned
word representations under this setting (where they are subject to different
random initializations). Our preliminary results illustrate that our metric not
only measures a intrinsic property of word embedding methods but also
correlates well with other evaluation metrics on downstream tasks. We believe
our methods are is useful in characterizing robustness -- an important property
to consider when developing new word embedding methods.


The Lazy Bureaucrat Scheduling Problem

  We introduce a new class of scheduling problems in which the optimization is
performed by the worker (single ``machine'') who performs the tasks. A typical
worker's objective is to minimize the amount of work he does (he is ``lazy''),
or more generally, to schedule as inefficiently (in some sense) as possible.
The worker is subject to the constraint that he must be busy when there is work
that he can do; we make this notion precise both in the preemptive and
nonpreemptive settings. The resulting class of ``perverse'' scheduling
problems, which we denote ``Lazy Bureaucrat Problems,'' gives rise to a rich
set of new questions that explore the distinction between maximization and
minimization in computing optimal schedules.


Call Admission Control Algorithm for pre-stored VBR video streams

  We examine the problem of accepting a new request for a pre-stored VBR video
stream that has been smoothed using any of the smoothing algorithms found in
the literature. The output of these algorithms is a piecewise constant-rate
schedule for a Variable Bit-Rate (VBR) stream. The schedule guarantees that the
decoder buffer does not overflow or underflow. The problem addressed in this
paper is the determination of the minimal time displacement of each new
requested VBR stream so that it can be accomodated by the network and/or the
video server without overbooking the committed traffic. We prove that this
call-admission control problem for multiple requested VBR streams is
NP-complete and inapproximable within a constant factor, by reducing it from
the VERTEX COLOR problem. We also present a deterministic morphology-sensitive
algorithm that calculates the minimal time displacement of a VBR stream
request. The complexity of the proposed algorithm make it suitable for
real-time determination of the time displacement parameter during the call
admission phase.


Towards a Taxonomical Consensus: Diversity and Richness Inference from
  Large Scale rRNA gene Analysis

  Population analysis is persistently challenging but important, leading to the
determination of diversity and function prediction of microbial community
members. Here we detail our bioinformatics methods for analyzing population
distribution and diversity in large microbial communities. This was achieved
via (i) a homology based method for robust phylotype determination, equaling
the classification accuracy of the Ribosomal Database Project (RDP) classifier,
but providing improved associations of closely related sequences; (ii) a
comparison of different clustering methods for achieving more accurate richness
estimations. Our methodology, which we developed using the RDP vetted 16S rRNA
gene sequence set, was validated by testing it on a large 16S rRNA gene dataset
of approximately 2300 sequences, which we obtained from a soil microbial
community study. We concluded that the best approach to obtain accurate
phylogenetics profile of large microbial communities, based on 16S rRNA gene
sequence information, is to apply an optimized blast classifier. This approach
is complemented by the grouping of closely related sequences, using complete
linkage clustering, in order to calculate richness and evenness indices for the
communities.


The Expressive Power of Word Embeddings

  We seek to better understand the difference in quality of the several
publicly released embeddings. We propose several tasks that help to distinguish
the characteristics of different embeddings. Our evaluation of sentiment
polarity and synonym/antonym relations shows that embeddings are able to
capture surprisingly nuanced semantics even in the absence of sentence
structure. Moreover, benchmarking the embeddings shows great variance in
quality and characteristics of the semantics captured by the tested embeddings.
Finally, we show the impact of varying the number of dimensions and the
resolution of each dimension on the effective useful features captured by the
embedding space. Our contributions highlight the importance of embeddings for
NLP tasks and the effect of their quality on the final results.


Polyglot: Distributed Word Representations for Multilingual NLP

  Distributed word representations (word embeddings) have recently contributed
to competitive performance in language modeling and several NLP tasks. In this
work, we train word embeddings for more than 100 languages using their
corresponding Wikipedias. We quantitatively demonstrate the utility of our word
embeddings by using them as the sole features for training a part of speech
tagger for a subset of these languages. We find their performance to be
competitive with near state-of-art methods in English, Danish and Swedish.
Moreover, we investigate the semantic features captured by these embeddings
through the proximity of word groupings. We will release these embeddings
publicly to help researchers in the development and enhancement of multilingual
applications.


News-Based Group Modeling and Forecasting

  In this paper, we study news group modeling and forecasting methods using
quantitative data generated by our large-scale natural language processing
(NLP) text analysis system. A news group is a set of news entities, like top
U.S. cities, governors, senators, golfers, or movie actors. Our fame
distribution analysis of news groups shows that log-normal and power-law
distributions generally could describe news groups in many aspects. We use
several real news groups including cities, politicians, and CS professors, to
evaluate our news group models in terms of time series data distribution
analysis, group-fame probability analysis, and fame-changing analysis over long
time. We also build a practical news generation model using a HMM (Hidden
Markov Model) based approach. Most importantly, our analysis shows the future
entity fame distribution has a power-law tail. That is, only a small number of
news entities in a group could become famous in the future. Based on these
analysis we are able to answer some interesting forecasting problems - for
example, what is the future average fame (or maximum fame) of a specific news
group? And what is the probability that some news entity become very famous
within a certain future time range? We also give concrete examples to
illustrate our forecasting approaches.


POLYGLOT-NER: Massive Multilingual Named Entity Recognition

  The increasing diversity of languages used on the web introduces a new level
of complexity to Information Retrieval (IR) systems. We can no longer assume
that textual content is written in one language or even the same language
family. In this paper, we demonstrate how to build massive multilingual
annotators with minimal human expertise and intervention. We describe a system
that builds Named Entity Recognition (NER) annotators for 40 major languages
using Wikipedia and Freebase. Our approach does not require NER human annotated
datasets or language specific resources like treebanks, parallel corpora, and
orthographic rules. The novelty of approach lies therein - using only language
agnostic techniques, while achieving competitive performance.
  Our method learns distributed word representations (word embeddings) which
encode semantic and syntactic features of words in each language. Then, we
automatically generate datasets from Wikipedia link structure and Freebase
attributes. Finally, we apply two preprocessing stages (oversampling and exact
surface form matching) which do not require any linguistic expertise.
  Our evaluation is two fold: First, we demonstrate the system performance on
human annotated datasets. Second, for languages where no gold-standard
benchmarks are available, we propose a new method, distant evaluation, based on
statistical machine translation.


Inducing Language Networks from Continuous Space Word Representations

  Recent advancements in unsupervised feature learning have developed powerful
latent representations of words. However, it is still not clear what makes one
representation better than another and how we can learn the ideal
representation. Understanding the structure of latent spaces attained is key to
any future advancement in unsupervised learning. In this work, we introduce a
new view of continuous space word representations as language networks. We
explore two techniques to create language networks from learned features by
inducing them for two popular word representation methods and examining the
properties of their resulting networks. We find that the induced networks
differ from other methods of creating language networks, and that they contain
meaningful community structure.


DeepWalk: Online Learning of Social Representations

  We present DeepWalk, a novel approach for learning latent representations of
vertices in a network. These latent representations encode social relations in
a continuous vector space, which is easily exploited by statistical models.
DeepWalk generalizes recent advancements in language modeling and unsupervised
feature learning (or deep learning) from sequences of words to graphs. DeepWalk
uses local information obtained from truncated random walks to learn latent
representations by treating walks as the equivalent of sentences. We
demonstrate DeepWalk's latent representations on several multi-label network
classification tasks for social networks such as BlogCatalog, Flickr, and
YouTube. Our results show that DeepWalk outperforms challenging baselines which
are allowed a global view of the network, especially in the presence of missing
information. DeepWalk's representations can provide $F_1$ scores up to 10%
higher than competing methods when labeled data is sparse. In some experiments,
DeepWalk's representations are able to outperform all baseline methods while
using 60% less training data. DeepWalk is also scalable. It is an online
learning algorithm which builds useful incremental results, and is trivially
parallelizable. These qualities make it suitable for a broad class of real
world applications such as network classification, and anomaly detection.


Marking Streets to Improve Parking Density

  Street parking spots for automobiles are a scarce commodity in most urban
environments. The heterogeneity of car sizes makes it inefficient to rigidly
define fixed-sized spots. Instead, unmarked streets in cities like New York
leave placement decisions to individual drivers, who have no direct incentive
to maximize street utilization.
  In this paper, we explore the effectiveness of two different behavioral
interventions designed to encourage better parking, namely (1) educational
campaigns to encourage parkers to "kiss the bumper" and reduce the distance
between themselves and their neighbors, or (2) painting appropriately-spaced
markings on the street and urging drivers to "hit the line". Through analysis
and simulation, we establish that the greatest densities are achieved when
lines are painted to create spots roughly twice the length of average-sized
cars. Kiss-the-bumper campaigns are in principle more effective than
hit-the-line for equal degrees of compliance, although we believe that the
visual cues of painted lines induce better parking behavior.


Recognizing Descriptive Wikipedia Categories for Historical Figures

  Wikipedia is a useful knowledge source that benefits many applications in
language processing and knowledge representation. An important feature of
Wikipedia is that of categories. Wikipedia pages are assigned different
categories according to their contents as human-annotated labels which can be
used in information retrieval, ad hoc search improvements, entity ranking and
tag recommendations. However, important pages are usually assigned too many
categories, which makes it difficult to recognize the most important ones that
give the best descriptions.
  In this paper, we propose an approach to recognize the most descriptive
Wikipedia categories. We observe that historical figures in a precise category
presumably are mutually similar and such categorical coherence could be
evaluated via texts or Wikipedia links of corresponding members in the
category. We rank descriptive level of Wikipedia categories according to their
coherence and our ranking yield an overall agreement of 88.27% compared with
human wisdom.


Statistically Significant Detection of Linguistic Change

  We propose a new computational approach for tracking and detecting
statistically significant linguistic shifts in the meaning and usage of words.
Such linguistic shifts are especially prevalent on the Internet, where the
rapid exchange of ideas can quickly change a word's meaning. Our meta-analysis
approach constructs property time series of word usage, and then uses
statistically sound change point detection algorithms to identify significant
linguistic shifts.
  We consider and analyze three approaches of increasing complexity to generate
such linguistic property time series, the culmination of which uses
distributional characteristics inferred from word co-occurrences. Using
recently proposed deep neural language models, we first train vector
representations of words for each time period. Second, we warp the vector
spaces into one unified coordinate system. Finally, we construct a
distance-based distributional time series for each word to track it's
linguistic displacement over time.
  We demonstrate that our approach is scalable by tracking linguistic change
across years of micro-blogging using Twitter, a decade of product reviews using
a corpus of movie reviews from Amazon, and a century of written books using the
Google Book-ngrams. Our analysis reveals interesting patterns of language usage
change commensurate with each medium.


Don't Walk, Skip! Online Learning of Multi-scale Network Embeddings

  We present Walklets, a novel approach for learning multiscale representations
of vertices in a network. In contrast to previous works, these representations
explicitly encode multiscale vertex relationships in a way that is analytically
derivable.
  Walklets generates these multiscale relationships by subsampling short random
walks on the vertices of a graph. By `skipping' over steps in each random walk,
our method generates a corpus of vertex pairs which are reachable via paths of
a fixed length. This corpus can then be used to learn a series of latent
representations, each of which captures successively higher order relationships
from the adjacency matrix.
  We demonstrate the efficacy of Walklets's latent representations on several
multi-label network classification tasks for social networks such as
BlogCatalog, DBLP, Flickr, and YouTube. Our results show that Walklets
outperforms new methods based on neural matrix factorization. Specifically, we
outperform DeepWalk by up to 10% and LINE by 58% Micro-F1 on challenging
multi-label classification tasks. Finally, Walklets is an online algorithm, and
can easily scale to graphs with millions of vertices and edges.


False-Friend Detection and Entity Matching via Unsupervised
  Transliteration

  Transliterations play an important role in multilingual entity reference
resolution, because proper names increasingly travel between languages in news
and social media. Previous work associated with machine translation targets
transliteration only single between language pairs, focuses on specific classes
of entities (such as cities and celebrities) and relies on manual curation,
which limits the expression power of transliteration in multilingual
environment.
  By contrast, we present an unsupervised transliteration model covering 69
major languages that can generate good transliterations for arbitrary strings
between any language pair. Our model yields top-(1, 20, 100) averages of
(32.85%, 60.44%, 83.20%) in matching gold standard transliteration compared to
results from a recently-published system of (26.71%, 50.27%, 72.79%). We also
show the quality of our model in detecting true and false friends from
Wikipedia high frequency lexicons. Our method indicates a strong signal of
pronunciation similarity and boosts the probability of finding true friends in
68 out of 69 languages.


Citation histories of papers: sometimes the rich get richer, sometimes
  they don't

  We describe a simple model of how a publication's citations change over time,
based on pure-birth stochastic processes with a linear cumulative advantage
effect. The model is applied to citation data from the Physical Review corpus
provided by APS. Our model reveals that papers fall into three different
clusters: papers that have rapid initial citations and ultimately high impact
(fast-hi), fast to rise but quick to plateau (fast-flat), or late bloomers
(slow-late), which may either never achieve many citations, or do so many years
after publication. In "fast-hi" and "slow-late", there is a rich-get-richer
effect: papers that have many citations accumulate additional citations more
rapidly while the "fast-flat" papers do not display this effect. We conclude by
showing that only a few years of post-publication statistics are needed to
identify high impact ("fast-hi") papers.


Latent Human Traits in the Language of Social Media: An Open-Vocabulary
  Approach

  Over the past century, personality theory and research has successfully
identified core sets of characteristics that consistently describe and explain
fundamental differences in the way people think, feel and behave. Such
characteristics were derived through theory, dictionary analyses, and survey
research using explicit self-reports. The availability of social media data
spanning millions of users now makes it possible to automatically derive
characteristics from language use -- at large scale. Taking advantage of
linguistic information available through Facebook, we study the process of
inferring a new set of potential human traits based on unprompted language use.
We subject these new traits to a comprehensive set of evaluations and compare
them with a popular five factor model of personality. We find that our
language-based trait construct is often more generalizable in that it often
predicts non-questionnaire-based outcomes better than questionnaire-based
traits (e.g. entities someone likes, income and intelligence quotient), while
the factors remain nearly as stable as traditional factors. Our approach
suggests a value in new constructs of personality derived from everyday human
language use.


HARP: Hierarchical Representation Learning for Networks

  We present HARP, a novel method for learning low dimensional embeddings of a
graph's nodes which preserves higher-order structural features. Our proposed
method achieves this by compressing the input graph prior to embedding it,
effectively avoiding troublesome embedding configurations (i.e. local minima)
which can pose problems to non-convex optimization. HARP works by finding a
smaller graph which approximates the global structure of its input. This
simplified graph is used to learn a set of initial representations, which serve
as good initializations for learning representations in the original, detailed
graph. We inductively extend this idea, by decomposing a graph in a series of
levels, and then embed the hierarchy of graphs from the coarsest one to the
original graph. HARP is a general meta-strategy to improve all of the
state-of-the-art neural algorithms for embedding graphs, including DeepWalk,
LINE, and Node2vec. Indeed, we demonstrate that applying HARP's hierarchical
paradigm yields improved implementations for all three of these methods, as
evaluated on both classification tasks on real-world graphs such as DBLP,
BlogCatalog, CiteSeer, and Arxiv, where we achieve a performance gain over the
original implementations by up to 14% Macro F1.


Syntax-Directed Variational Autoencoder for Structured Data

  Deep generative models have been enjoying success in modeling continuous
data. However it remains challenging to capture the representations for
discrete structures with formal grammars and semantics, e.g., computer programs
and molecular structures. How to generate both syntactically and semantically
correct data still remains largely an open problem. Inspired by the theory of
compiler where the syntax and semantics check is done via syntax-directed
translation (SDT), we propose a novel syntax-directed variational autoencoder
(SD-VAE) by introducing stochastic lazy attributes. This approach converts the
offline SDT check into on-the-fly generated guidance for constraining the
decoder. Comparing to the state-of-the-art methods, our approach enforces
constraints on the output space so that the output will be not only
syntactically valid, but also semantically reasonable. We evaluate the proposed
model with applications in programming language and molecules, including
reconstruction and program/molecule optimization. The results demonstrate the
effectiveness in incorporating syntactic and semantic constraints in discrete
generative models, which is significantly better than current state-of-the-art
approaches.


Co-training Embeddings of Knowledge Graphs and Entity Descriptions for
  Cross-lingual Entity Alignment

  Multilingual knowledge graph (KG) embeddings provide latent semantic
representations of entities and structured knowledge with cross-lingual
inferences, which benefit various knowledge-driven cross-lingual NLP tasks.
However, precisely learning such cross-lingual inferences is usually hindered
by the low coverage of entity alignment in many KGs. Since many multilingual
KGs also provide literal descriptions of entities, in this paper, we introduce
an embedding-based approach which leverages a weakly aligned multilingual KG
for semi-supervised cross-lingual learning using entity descriptions. Our
approach performs co-training of two embedding models, i.e. a multilingual KG
embedding model and a multilingual literal description embedding model. The
models are trained on a large Wikipedia-based trilingual dataset where most
entity alignment is unknown to training. Experimental results show that the
performance of the proposed approach on the entity alignment task improves at
each iteration of co-training, and eventually reaches a stage at which it
significantly surpasses previous approaches. We also show that our approach has
promising abilities for zero-shot entity alignment, and cross-lingual KG
completion.


A Tutorial on Network Embeddings

  Network embedding methods aim at learning low-dimensional latent
representation of nodes in a network. These representations can be used as
features for a wide range of tasks on graphs such as classification,
clustering, link prediction, and visualization. In this survey, we give an
overview of network embeddings by summarizing and categorizing recent
advancements in this research field. We first discuss the desirable properties
of network embeddings and briefly introduce the history of network embedding
algorithms. Then, we discuss network embedding methods under different
scenarios, such as supervised versus unsupervised learning, learning embeddings
for homogeneous networks versus for heterogeneous networks, etc. We further
demonstrate the applications of network embeddings, and conclude the survey
with future work in this area.


Learning to Represent Bilingual Dictionaries

  Bilingual word embeddings have been widely used to capture the similarity of
lexical semantics in different human languages. However, many applications,
such as cross-lingual semantic search and question answering, can be largely
benefited from the cross-lingual correspondence between sentences and lexicons.
To bridge this gap, we propose a neural embedding model that leverages
bilingual dictionaries. The proposed model is trained to map the literal word
definitions to the cross-lingual target words, for which we explore with
different sentence encoding techniques. To enhance the learning process on
limited resources, our model adopts several critical learning strategies,
including multi-task learning on different bridges of languages, and joint
learning of the dictionary model with a bilingual word embedding model.
Experimental evaluation focuses on two applications. The results of the
cross-lingual reverse dictionary retrieval task show our model's promising
ability of comprehending bilingual concepts based on descriptions, and
highlight the effectiveness of proposed learning strategies in improving
performance. Meanwhile, our model effectively addresses the bilingual
paraphrase identification problem and significantly outperforms previous
approaches.


Multi-view Models for Political Ideology Detection of News Articles

  A news article's title, content and link structure often reveal its political
ideology. However, most existing works on automatic political ideology
detection only leverage textual cues. Drawing inspiration from recent advances
in neural inference, we propose a novel attention based multi-view model to
leverage cues from all of the above views to identify the ideology evinced by a
news article. Our model draws on advances in representation learning in natural
language processing and network science to capture cues from both textual
content and the network structure of news articles. We empirically evaluate our
model against a battery of baselines and show that our model outperforms state
of the art by 10 percentage points F1 score.


Enhanced Network Embeddings via Exploiting Edge Labels

  Network embedding methods aim at learning low-dimensional latent
representation of nodes in a network. While achieving competitive performance
on a variety of network inference tasks such as node classification and link
prediction, these methods treat the relations between nodes as a binary
variable and ignore the rich semantics of edges. In this work, we attempt to
learn network embeddings which simultaneously preserve network structure and
relations between nodes. Experiments on several real-world networks illustrate
that by considering different relations between different node pairs, our
method is capable of producing node embeddings of higher quality than a number
of state-of-the-art network embedding methods, as evaluated on a challenging
multi-label node classification task.


Exploring the power of GPU's for training Polyglot language models

  One of the major research trends currently is the evolution of heterogeneous
parallel computing. GP-GPU computing is being widely used and several
applications have been designed to exploit the massive parallelism that
GP-GPU's have to offer. While GPU's have always been widely used in areas of
computer vision for image processing, little has been done to investigate
whether the massive parallelism provided by GP-GPU's can be utilized
effectively for Natural Language Processing(NLP) tasks. In this work, we
investigate and explore the power of GP-GPU's in the task of learning language
models. More specifically, we investigate the performance of training Polyglot
language models using deep belief neural networks. We evaluate the performance
of training the model on the GPU and present optimizations that boost the
performance on the GPU.One of the key optimizations, we propose increases the
performance of a function involved in calculating and updating the gradient by
approximately 50 times on the GPU for sufficiently large batch sizes. We show
that with the above optimizations, the GP-GPU's performance on the task
increases by factor of approximately 3-4. The optimizations we made are generic
Theano optimizations and hence potentially boost the performance of other
models which rely on these operations.We also show that these optimizations
result in the GPU's performance at this task being now comparable to that on
the CPU. We conclude by presenting a thorough evaluation of the applicability
of GP-GPU's for this task and highlight the factors limiting the performance of
training a Polyglot model on the GPU.


Freshman or Fresher? Quantifying the Geographic Variation of Internet
  Language

  We present a new computational technique to detect and analyze statistically
significant geographic variation in language. Our meta-analysis approach
captures statistical properties of word usage across geographical regions and
uses statistical methods to identify significant changes specific to regions.
While previous approaches have primarily focused on lexical variation between
regions, our method identifies words that demonstrate semantic and syntactic
variation as well.
  We extend recently developed techniques for neural language models to learn
word representations which capture differing semantics across geographical
regions. In order to quantify this variation and ensure robust detection of
true regional differences, we formulate a null model to determine whether
observed changes are statistically significant. Our method is the first such
approach to explicitly account for random variation due to chance while
detecting regional variation in word meaning.
  To validate our model, we study and analyze two different massive online data
sets: millions of tweets from Twitter spanning not only four different
countries but also fifty states, as well as millions of phrases contained in
the Google Book Ngrams. Our analysis reveals interesting facets of language
change at multiple scales of geographic resolution -- from neighboring states
to distant continents.
  Finally, using our model, we propose a measure of semantic distance between
languages. Our analysis of British and American English over a period of 100
years reveals that semantic variation between these dialects is shrinking.


Nationality Classification Using Name Embeddings

  Nationality identification unlocks important demographic information, with
many applications in biomedical and sociological research. Existing name-based
nationality classifiers use name substrings as features and are trained on
small, unrepresentative sets of labeled names, typically extracted from
Wikipedia. As a result, these methods achieve limited performance and cannot
support fine-grained classification.
  We exploit the phenomena of homophily in communication patterns to learn name
embeddings, a new representation that encodes gender, ethnicity, and
nationality which is readily applicable to building classifiers and other
systems. Through our analysis of 57M contact lists from a major Internet
company, we are able to design a fine-grained nationality classifier covering
39 groups representing over 90% of the world population. In an evaluation
against other published systems over 13 common classes, our F1 score (0.795) is
substantial better than our closest competitor Ethnea (0.580). To the best of
our knowledge, this is the most accurate, fine-grained nationality classifier
available.
  As a social media application, we apply our classifiers to the followers of
major Twitter celebrities over six different domains. We demonstrate stark
differences in the ethnicities of the followers of Trump and Obama, and in the
sports and entertainments favored by different groups. Finally, we identify an
anomalous political figure whose presumably inflated following appears largely
incapable of reading the language he posts in.


MediaRank: Computational Ranking of Online News Sources

  In the recent political climate, the topic of news quality has drawn
attention both from the public and the academic communities. The growing
distrust of traditional news media makes it harder to find a common base of
accepted truth. In this work, we design and build MediaRank
(www.media-rank.com), a fully automated system to rank over 50,000 online news
sources around the world. MediaRank collects and analyzes one million news
webpages and two million related tweets everyday. We base our algorithmic
analysis on four properties journalists have established to be associated with
reporting quality: peer reputation, reporting bias / breadth, bottomline
financial pressure, and popularity.
  Our major contributions of this paper include: (i) Open, interpretable
quality rankings for over 50,000 of the world's major news sources. Our
rankings are validated against 35 published news rankings, including French,
German, Russian, and Spanish language sources. MediaRank scores correlate
positively with 34 of 35 of these expert rankings. (ii) New computational
methods for measuring influence and bottomline pressure. To the best of our
knowledge, we are the first to study the large-scale news reporting citation
graph in-depth. We also propose new ways to measure the aggressiveness of
advertisements and identify social bots, establishing a connection between both
types of bad behavior. (iii) Analyzing the effect of media source bias and
significance. We prove that news sources cite others despite different
political views in accord with quality measures. However, in four
English-speaking countries (US, UK, Canada, and Australia), the highest ranking
sources all disproportionately favor left-wing parties, even when the majority
of news sources exhibited conservative slants.


When Can You Fold a Map?

  We explore the following problem: given a collection of creases on a piece of
paper, each assigned a folding direction of mountain or valley, is there a flat
folding by a sequence of simple folds? There are several models of simple
folds; the simplest one-layer simple fold rotates a portion of paper about a
crease in the paper by +-180 degrees. We first consider the analogous questions
in one dimension lower -- bending a segment into a flat object -- which lead to
interesting problems on strings. We develop efficient algorithms for the
recognition of simply foldable 1D crease patterns, and reconstruction of a
sequence of simple folds. Indeed, we prove that a 1D crease pattern is
flat-foldable by any means precisely if it is by a sequence of one-layer simple
folds.
  Next we explore simple foldability in two dimensions, and find a surprising
contrast: ``map'' folding and variants are polynomial, but slight
generalizations are NP-complete. Specifically, we develop a linear-time
algorithm for deciding foldability of an orthogonal crease pattern on a
rectangular piece of paper, and prove that it is (weakly) NP-complete to decide
foldability of (1) an orthogonal crease pattern on a orthogonal piece of paper,
(2) a crease pattern of axis-parallel and diagonal (45-degree) creases on a
square piece of paper, and (3) crease patterns without a mountain/valley
assignment.


