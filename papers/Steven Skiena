Analysis of airplane boarding via space-time geometry and random matrix  theory

  We show that airplane boarding can be asymptotically modeled by 2-dimensionalLorentzian geometry. Boarding time is given by the maximal proper time amongcurves in the model. Discrepancies between the model and simulation results areclosely related to random matrix theory. We then show how such models can beused to explain why some commonly practiced airline boarding policies areineffective and even detrimental.

SpeedRead: A Fast Named Entity Recognition Pipeline

  Online content analysis employs algorithmic methods to identify entities inunstructured text. Both machine learning and knowledge-base approaches lie atthe foundation of contemporary named entities extraction systems. However, theprogress in deploying these approaches on web-scale has been been hampered bythe computational cost of NLP over massive text corpora. We present SpeedRead(SR), a named entity recognition pipeline that runs at least 10 times fasterthan Stanford NLP pipeline. This pipeline consists of a high performance PennTreebank- compliant tokenizer, close to state-of-art part-of-speech (POS)tagger and knowledge-based named entity recognizer.

On the Convergent Properties of Word Embedding Methods

  Do word embeddings converge to learn similar things over differentinitializations? How repeatable are experiments with word embeddings? Are allword embedding techniques equally reliable? In this paper we propose evaluatingmethods for learning word representations by their consistency acrossinitializations. We propose a measure to quantify the similarity of the learnedword representations under this setting (where they are subject to differentrandom initializations). Our preliminary results illustrate that our metric notonly measures a intrinsic property of word embedding methods but alsocorrelates well with other evaluation metrics on downstream tasks. We believeour methods are is useful in characterizing robustness -- an important propertyto consider when developing new word embedding methods.

The Lazy Bureaucrat Scheduling Problem

  We introduce a new class of scheduling problems in which the optimization isperformed by the worker (single ``machine'') who performs the tasks. A typicalworker's objective is to minimize the amount of work he does (he is ``lazy''),or more generally, to schedule as inefficiently (in some sense) as possible.The worker is subject to the constraint that he must be busy when there is workthat he can do; we make this notion precise both in the preemptive andnonpreemptive settings. The resulting class of ``perverse'' schedulingproblems, which we denote ``Lazy Bureaucrat Problems,'' gives rise to a richset of new questions that explore the distinction between maximization andminimization in computing optimal schedules.

Call Admission Control Algorithm for pre-stored VBR video streams

  We examine the problem of accepting a new request for a pre-stored VBR videostream that has been smoothed using any of the smoothing algorithms found inthe literature. The output of these algorithms is a piecewise constant-rateschedule for a Variable Bit-Rate (VBR) stream. The schedule guarantees that thedecoder buffer does not overflow or underflow. The problem addressed in thispaper is the determination of the minimal time displacement of each newrequested VBR stream so that it can be accomodated by the network and/or thevideo server without overbooking the committed traffic. We prove that thiscall-admission control problem for multiple requested VBR streams isNP-complete and inapproximable within a constant factor, by reducing it fromthe VERTEX COLOR problem. We also present a deterministic morphology-sensitivealgorithm that calculates the minimal time displacement of a VBR streamrequest. The complexity of the proposed algorithm make it suitable forreal-time determination of the time displacement parameter during the calladmission phase.

Towards a Taxonomical Consensus: Diversity and Richness Inference from  Large Scale rRNA gene Analysis

  Population analysis is persistently challenging but important, leading to thedetermination of diversity and function prediction of microbial communitymembers. Here we detail our bioinformatics methods for analyzing populationdistribution and diversity in large microbial communities. This was achievedvia (i) a homology based method for robust phylotype determination, equalingthe classification accuracy of the Ribosomal Database Project (RDP) classifier,but providing improved associations of closely related sequences; (ii) acomparison of different clustering methods for achieving more accurate richnessestimations. Our methodology, which we developed using the RDP vetted 16S rRNAgene sequence set, was validated by testing it on a large 16S rRNA gene datasetof approximately 2300 sequences, which we obtained from a soil microbialcommunity study. We concluded that the best approach to obtain accuratephylogenetics profile of large microbial communities, based on 16S rRNA genesequence information, is to apply an optimized blast classifier. This approachis complemented by the grouping of closely related sequences, using completelinkage clustering, in order to calculate richness and evenness indices for thecommunities.

The Expressive Power of Word Embeddings

  We seek to better understand the difference in quality of the severalpublicly released embeddings. We propose several tasks that help to distinguishthe characteristics of different embeddings. Our evaluation of sentimentpolarity and synonym/antonym relations shows that embeddings are able tocapture surprisingly nuanced semantics even in the absence of sentencestructure. Moreover, benchmarking the embeddings shows great variance inquality and characteristics of the semantics captured by the tested embeddings.Finally, we show the impact of varying the number of dimensions and theresolution of each dimension on the effective useful features captured by theembedding space. Our contributions highlight the importance of embeddings forNLP tasks and the effect of their quality on the final results.

Polyglot: Distributed Word Representations for Multilingual NLP

  Distributed word representations (word embeddings) have recently contributedto competitive performance in language modeling and several NLP tasks. In thiswork, we train word embeddings for more than 100 languages using theircorresponding Wikipedias. We quantitatively demonstrate the utility of our wordembeddings by using them as the sole features for training a part of speechtagger for a subset of these languages. We find their performance to becompetitive with near state-of-art methods in English, Danish and Swedish.Moreover, we investigate the semantic features captured by these embeddingsthrough the proximity of word groupings. We will release these embeddingspublicly to help researchers in the development and enhancement of multilingualapplications.

Inducing Language Networks from Continuous Space Word Representations

  Recent advancements in unsupervised feature learning have developed powerfullatent representations of words. However, it is still not clear what makes onerepresentation better than another and how we can learn the idealrepresentation. Understanding the structure of latent spaces attained is key toany future advancement in unsupervised learning. In this work, we introduce anew view of continuous space word representations as language networks. Weexplore two techniques to create language networks from learned features byinducing them for two popular word representation methods and examining theproperties of their resulting networks. We find that the induced networksdiffer from other methods of creating language networks, and that they containmeaningful community structure.

DeepWalk: Online Learning of Social Representations

  We present DeepWalk, a novel approach for learning latent representations ofvertices in a network. These latent representations encode social relations ina continuous vector space, which is easily exploited by statistical models.DeepWalk generalizes recent advancements in language modeling and unsupervisedfeature learning (or deep learning) from sequences of words to graphs. DeepWalkuses local information obtained from truncated random walks to learn latentrepresentations by treating walks as the equivalent of sentences. Wedemonstrate DeepWalk's latent representations on several multi-label networkclassification tasks for social networks such as BlogCatalog, Flickr, andYouTube. Our results show that DeepWalk outperforms challenging baselines whichare allowed a global view of the network, especially in the presence of missinginformation. DeepWalk's representations can provide $F_1$ scores up to 10%higher than competing methods when labeled data is sparse. In some experiments,DeepWalk's representations are able to outperform all baseline methods whileusing 60% less training data. DeepWalk is also scalable. It is an onlinelearning algorithm which builds useful incremental results, and is triviallyparallelizable. These qualities make it suitable for a broad class of realworld applications such as network classification, and anomaly detection.

News-Based Group Modeling and Forecasting

  In this paper, we study news group modeling and forecasting methods usingquantitative data generated by our large-scale natural language processing(NLP) text analysis system. A news group is a set of news entities, like topU.S. cities, governors, senators, golfers, or movie actors. Our famedistribution analysis of news groups shows that log-normal and power-lawdistributions generally could describe news groups in many aspects. We useseveral real news groups including cities, politicians, and CS professors, toevaluate our news group models in terms of time series data distributionanalysis, group-fame probability analysis, and fame-changing analysis over longtime. We also build a practical news generation model using a HMM (HiddenMarkov Model) based approach. Most importantly, our analysis shows the futureentity fame distribution has a power-law tail. That is, only a small number ofnews entities in a group could become famous in the future. Based on theseanalysis we are able to answer some interesting forecasting problems - forexample, what is the future average fame (or maximum fame) of a specific newsgroup? And what is the probability that some news entity become very famouswithin a certain future time range? We also give concrete examples toillustrate our forecasting approaches.

POLYGLOT-NER: Massive Multilingual Named Entity Recognition

  The increasing diversity of languages used on the web introduces a new levelof complexity to Information Retrieval (IR) systems. We can no longer assumethat textual content is written in one language or even the same languagefamily. In this paper, we demonstrate how to build massive multilingualannotators with minimal human expertise and intervention. We describe a systemthat builds Named Entity Recognition (NER) annotators for 40 major languagesusing Wikipedia and Freebase. Our approach does not require NER human annotateddatasets or language specific resources like treebanks, parallel corpora, andorthographic rules. The novelty of approach lies therein - using only languageagnostic techniques, while achieving competitive performance.  Our method learns distributed word representations (word embeddings) whichencode semantic and syntactic features of words in each language. Then, weautomatically generate datasets from Wikipedia link structure and Freebaseattributes. Finally, we apply two preprocessing stages (oversampling and exactsurface form matching) which do not require any linguistic expertise.  Our evaluation is two fold: First, we demonstrate the system performance onhuman annotated datasets. Second, for languages where no gold-standardbenchmarks are available, we propose a new method, distant evaluation, based onstatistical machine translation.

Statistically Significant Detection of Linguistic Change

  We propose a new computational approach for tracking and detectingstatistically significant linguistic shifts in the meaning and usage of words.Such linguistic shifts are especially prevalent on the Internet, where therapid exchange of ideas can quickly change a word's meaning. Our meta-analysisapproach constructs property time series of word usage, and then usesstatistically sound change point detection algorithms to identify significantlinguistic shifts.  We consider and analyze three approaches of increasing complexity to generatesuch linguistic property time series, the culmination of which usesdistributional characteristics inferred from word co-occurrences. Usingrecently proposed deep neural language models, we first train vectorrepresentations of words for each time period. Second, we warp the vectorspaces into one unified coordinate system. Finally, we construct adistance-based distributional time series for each word to track it'slinguistic displacement over time.  We demonstrate that our approach is scalable by tracking linguistic changeacross years of micro-blogging using Twitter, a decade of product reviews usinga corpus of movie reviews from Amazon, and a century of written books using theGoogle Book-ngrams. Our analysis reveals interesting patterns of language usagechange commensurate with each medium.

Marking Streets to Improve Parking Density

  Street parking spots for automobiles are a scarce commodity in most urbanenvironments. The heterogeneity of car sizes makes it inefficient to rigidlydefine fixed-sized spots. Instead, unmarked streets in cities like New Yorkleave placement decisions to individual drivers, who have no direct incentiveto maximize street utilization.  In this paper, we explore the effectiveness of two different behavioralinterventions designed to encourage better parking, namely (1) educationalcampaigns to encourage parkers to "kiss the bumper" and reduce the distancebetween themselves and their neighbors, or (2) painting appropriately-spacedmarkings on the street and urging drivers to "hit the line". Through analysisand simulation, we establish that the greatest densities are achieved whenlines are painted to create spots roughly twice the length of average-sizedcars. Kiss-the-bumper campaigns are in principle more effective thanhit-the-line for equal degrees of compliance, although we believe that thevisual cues of painted lines induce better parking behavior.

Don't Walk, Skip! Online Learning of Multi-scale Network Embeddings

  We present Walklets, a novel approach for learning multiscale representationsof vertices in a network. In contrast to previous works, these representationsexplicitly encode multiscale vertex relationships in a way that is analyticallyderivable.  Walklets generates these multiscale relationships by subsampling short randomwalks on the vertices of a graph. By `skipping' over steps in each random walk,our method generates a corpus of vertex pairs which are reachable via paths ofa fixed length. This corpus can then be used to learn a series of latentrepresentations, each of which captures successively higher order relationshipsfrom the adjacency matrix.  We demonstrate the efficacy of Walklets's latent representations on severalmulti-label network classification tasks for social networks such asBlogCatalog, DBLP, Flickr, and YouTube. Our results show that Walkletsoutperforms new methods based on neural matrix factorization. Specifically, weoutperform DeepWalk by up to 10% and LINE by 58% Micro-F1 on challengingmulti-label classification tasks. Finally, Walklets is an online algorithm, andcan easily scale to graphs with millions of vertices and edges.

False-Friend Detection and Entity Matching via Unsupervised  Transliteration

  Transliterations play an important role in multilingual entity referenceresolution, because proper names increasingly travel between languages in newsand social media. Previous work associated with machine translation targetstransliteration only single between language pairs, focuses on specific classesof entities (such as cities and celebrities) and relies on manual curation,which limits the expression power of transliteration in multilingualenvironment.  By contrast, we present an unsupervised transliteration model covering 69major languages that can generate good transliterations for arbitrary stringsbetween any language pair. Our model yields top-(1, 20, 100) averages of(32.85%, 60.44%, 83.20%) in matching gold standard transliteration compared toresults from a recently-published system of (26.71%, 50.27%, 72.79%). We alsoshow the quality of our model in detecting true and false friends fromWikipedia high frequency lexicons. Our method indicates a strong signal ofpronunciation similarity and boosts the probability of finding true friends in68 out of 69 languages.

Citation histories of papers: sometimes the rich get richer, sometimes  they don't

  We describe a simple model of how a publication's citations change over time,based on pure-birth stochastic processes with a linear cumulative advantageeffect. The model is applied to citation data from the Physical Review corpusprovided by APS. Our model reveals that papers fall into three differentclusters: papers that have rapid initial citations and ultimately high impact(fast-hi), fast to rise but quick to plateau (fast-flat), or late bloomers(slow-late), which may either never achieve many citations, or do so many yearsafter publication. In "fast-hi" and "slow-late", there is a rich-get-richereffect: papers that have many citations accumulate additional citations morerapidly while the "fast-flat" papers do not display this effect. We conclude byshowing that only a few years of post-publication statistics are needed toidentify high impact ("fast-hi") papers.

Recognizing Descriptive Wikipedia Categories for Historical Figures

  Wikipedia is a useful knowledge source that benefits many applications inlanguage processing and knowledge representation. An important feature ofWikipedia is that of categories. Wikipedia pages are assigned differentcategories according to their contents as human-annotated labels which can beused in information retrieval, ad hoc search improvements, entity ranking andtag recommendations. However, important pages are usually assigned too manycategories, which makes it difficult to recognize the most important ones thatgive the best descriptions.  In this paper, we propose an approach to recognize the most descriptiveWikipedia categories. We observe that historical figures in a precise categorypresumably are mutually similar and such categorical coherence could beevaluated via texts or Wikipedia links of corresponding members in thecategory. We rank descriptive level of Wikipedia categories according to theircoherence and our ranking yield an overall agreement of 88.27% compared withhuman wisdom.

Latent Human Traits in the Language of Social Media: An Open-Vocabulary  Approach

  Over the past century, personality theory and research has successfullyidentified core sets of characteristics that consistently describe and explainfundamental differences in the way people think, feel and behave. Suchcharacteristics were derived through theory, dictionary analyses, and surveyresearch using explicit self-reports. The availability of social media dataspanning millions of users now makes it possible to automatically derivecharacteristics from language use -- at large scale. Taking advantage oflinguistic information available through Facebook, we study the process ofinferring a new set of potential human traits based on unprompted language use.We subject these new traits to a comprehensive set of evaluations and comparethem with a popular five factor model of personality. We find that ourlanguage-based trait construct is often more generalizable in that it oftenpredicts non-questionnaire-based outcomes better than questionnaire-basedtraits (e.g. entities someone likes, income and intelligence quotient), whilethe factors remain nearly as stable as traditional factors. Our approachsuggests a value in new constructs of personality derived from everyday humanlanguage use.

HARP: Hierarchical Representation Learning for Networks

  We present HARP, a novel method for learning low dimensional embeddings of agraph's nodes which preserves higher-order structural features. Our proposedmethod achieves this by compressing the input graph prior to embedding it,effectively avoiding troublesome embedding configurations (i.e. local minima)which can pose problems to non-convex optimization. HARP works by finding asmaller graph which approximates the global structure of its input. Thissimplified graph is used to learn a set of initial representations, which serveas good initializations for learning representations in the original, detailedgraph. We inductively extend this idea, by decomposing a graph in a series oflevels, and then embed the hierarchy of graphs from the coarsest one to theoriginal graph. HARP is a general meta-strategy to improve all of thestate-of-the-art neural algorithms for embedding graphs, including DeepWalk,LINE, and Node2vec. Indeed, we demonstrate that applying HARP's hierarchicalparadigm yields improved implementations for all three of these methods, asevaluated on both classification tasks on real-world graphs such as DBLP,BlogCatalog, CiteSeer, and Arxiv, where we achieve a performance gain over theoriginal implementations by up to 14% Macro F1.

Syntax-Directed Variational Autoencoder for Structured Data

  Deep generative models have been enjoying success in modeling continuousdata. However it remains challenging to capture the representations fordiscrete structures with formal grammars and semantics, e.g., computer programsand molecular structures. How to generate both syntactically and semanticallycorrect data still remains largely an open problem. Inspired by the theory ofcompiler where the syntax and semantics check is done via syntax-directedtranslation (SDT), we propose a novel syntax-directed variational autoencoder(SD-VAE) by introducing stochastic lazy attributes. This approach converts theoffline SDT check into on-the-fly generated guidance for constraining thedecoder. Comparing to the state-of-the-art methods, our approach enforcesconstraints on the output space so that the output will be not onlysyntactically valid, but also semantically reasonable. We evaluate the proposedmodel with applications in programming language and molecules, includingreconstruction and program/molecule optimization. The results demonstrate theeffectiveness in incorporating syntactic and semantic constraints in discretegenerative models, which is significantly better than current state-of-the-artapproaches.

Co-training Embeddings of Knowledge Graphs and Entity Descriptions for  Cross-lingual Entity Alignment

  Multilingual knowledge graph (KG) embeddings provide latent semanticrepresentations of entities and structured knowledge with cross-lingualinferences, which benefit various knowledge-driven cross-lingual NLP tasks.However, precisely learning such cross-lingual inferences is usually hinderedby the low coverage of entity alignment in many KGs. Since many multilingualKGs also provide literal descriptions of entities, in this paper, we introducean embedding-based approach which leverages a weakly aligned multilingual KGfor semi-supervised cross-lingual learning using entity descriptions. Ourapproach performs co-training of two embedding models, i.e. a multilingual KGembedding model and a multilingual literal description embedding model. Themodels are trained on a large Wikipedia-based trilingual dataset where mostentity alignment is unknown to training. Experimental results show that theperformance of the proposed approach on the entity alignment task improves ateach iteration of co-training, and eventually reaches a stage at which itsignificantly surpasses previous approaches. We also show that our approach haspromising abilities for zero-shot entity alignment, and cross-lingual KGcompletion.

A Tutorial on Network Embeddings

  Network embedding methods aim at learning low-dimensional latentrepresentation of nodes in a network. These representations can be used asfeatures for a wide range of tasks on graphs such as classification,clustering, link prediction, and visualization. In this survey, we give anoverview of network embeddings by summarizing and categorizing recentadvancements in this research field. We first discuss the desirable propertiesof network embeddings and briefly introduce the history of network embeddingalgorithms. Then, we discuss network embedding methods under differentscenarios, such as supervised versus unsupervised learning, learning embeddingsfor homogeneous networks versus for heterogeneous networks, etc. We furtherdemonstrate the applications of network embeddings, and conclude the surveywith future work in this area.

Learning to Represent Bilingual Dictionaries

  Bilingual word embeddings have been widely used to capture the similarity oflexical semantics in different human languages. However, many applications,such as cross-lingual semantic search and question answering, can be largelybenefited from the cross-lingual correspondence between sentences and lexicons.To bridge this gap, we propose a neural embedding model that leveragesbilingual dictionaries. The proposed model is trained to map the literal worddefinitions to the cross-lingual target words, for which we explore withdifferent sentence encoding techniques. To enhance the learning process onlimited resources, our model adopts several critical learning strategies,including multi-task learning on different bridges of languages, and jointlearning of the dictionary model with a bilingual word embedding model.Experimental evaluation focuses on two applications. The results of thecross-lingual reverse dictionary retrieval task show our model's promisingability of comprehending bilingual concepts based on descriptions, andhighlight the effectiveness of proposed learning strategies in improvingperformance. Meanwhile, our model effectively addresses the bilingualparaphrase identification problem and significantly outperforms previousapproaches.

Multi-view Models for Political Ideology Detection of News Articles

  A news article's title, content and link structure often reveal its politicalideology. However, most existing works on automatic political ideologydetection only leverage textual cues. Drawing inspiration from recent advancesin neural inference, we propose a novel attention based multi-view model toleverage cues from all of the above views to identify the ideology evinced by anews article. Our model draws on advances in representation learning in naturallanguage processing and network science to capture cues from both textualcontent and the network structure of news articles. We empirically evaluate ourmodel against a battery of baselines and show that our model outperforms stateof the art by 10 percentage points F1 score.

Enhanced Network Embeddings via Exploiting Edge Labels

  Network embedding methods aim at learning low-dimensional latentrepresentation of nodes in a network. While achieving competitive performanceon a variety of network inference tasks such as node classification and linkprediction, these methods treat the relations between nodes as a binaryvariable and ignore the rich semantics of edges. In this work, we attempt tolearn network embeddings which simultaneously preserve network structure andrelations between nodes. Experiments on several real-world networks illustratethat by considering different relations between different node pairs, ourmethod is capable of producing node embeddings of higher quality than a numberof state-of-the-art network embedding methods, as evaluated on a challengingmulti-label node classification task.

Exploring the power of GPU's for training Polyglot language models

  One of the major research trends currently is the evolution of heterogeneousparallel computing. GP-GPU computing is being widely used and severalapplications have been designed to exploit the massive parallelism thatGP-GPU's have to offer. While GPU's have always been widely used in areas ofcomputer vision for image processing, little has been done to investigatewhether the massive parallelism provided by GP-GPU's can be utilizedeffectively for Natural Language Processing(NLP) tasks. In this work, weinvestigate and explore the power of GP-GPU's in the task of learning languagemodels. More specifically, we investigate the performance of training Polyglotlanguage models using deep belief neural networks. We evaluate the performanceof training the model on the GPU and present optimizations that boost theperformance on the GPU.One of the key optimizations, we propose increases theperformance of a function involved in calculating and updating the gradient byapproximately 50 times on the GPU for sufficiently large batch sizes. We showthat with the above optimizations, the GP-GPU's performance on the taskincreases by factor of approximately 3-4. The optimizations we made are genericTheano optimizations and hence potentially boost the performance of othermodels which rely on these operations.We also show that these optimizationsresult in the GPU's performance at this task being now comparable to that onthe CPU. We conclude by presenting a thorough evaluation of the applicabilityof GP-GPU's for this task and highlight the factors limiting the performance oftraining a Polyglot model on the GPU.

Freshman or Fresher? Quantifying the Geographic Variation of Internet  Language

  We present a new computational technique to detect and analyze statisticallysignificant geographic variation in language. Our meta-analysis approachcaptures statistical properties of word usage across geographical regions anduses statistical methods to identify significant changes specific to regions.While previous approaches have primarily focused on lexical variation betweenregions, our method identifies words that demonstrate semantic and syntacticvariation as well.  We extend recently developed techniques for neural language models to learnword representations which capture differing semantics across geographicalregions. In order to quantify this variation and ensure robust detection oftrue regional differences, we formulate a null model to determine whetherobserved changes are statistically significant. Our method is the first suchapproach to explicitly account for random variation due to chance whiledetecting regional variation in word meaning.  To validate our model, we study and analyze two different massive online datasets: millions of tweets from Twitter spanning not only four differentcountries but also fifty states, as well as millions of phrases contained inthe Google Book Ngrams. Our analysis reveals interesting facets of languagechange at multiple scales of geographic resolution -- from neighboring statesto distant continents.  Finally, using our model, we propose a measure of semantic distance betweenlanguages. Our analysis of British and American English over a period of 100years reveals that semantic variation between these dialects is shrinking.

Nationality Classification Using Name Embeddings

  Nationality identification unlocks important demographic information, withmany applications in biomedical and sociological research. Existing name-basednationality classifiers use name substrings as features and are trained onsmall, unrepresentative sets of labeled names, typically extracted fromWikipedia. As a result, these methods achieve limited performance and cannotsupport fine-grained classification.  We exploit the phenomena of homophily in communication patterns to learn nameembeddings, a new representation that encodes gender, ethnicity, andnationality which is readily applicable to building classifiers and othersystems. Through our analysis of 57M contact lists from a major Internetcompany, we are able to design a fine-grained nationality classifier covering39 groups representing over 90% of the world population. In an evaluationagainst other published systems over 13 common classes, our F1 score (0.795) issubstantial better than our closest competitor Ethnea (0.580). To the best ofour knowledge, this is the most accurate, fine-grained nationality classifieravailable.  As a social media application, we apply our classifiers to the followers ofmajor Twitter celebrities over six different domains. We demonstrate starkdifferences in the ethnicities of the followers of Trump and Obama, and in thesports and entertainments favored by different groups. Finally, we identify ananomalous political figure whose presumably inflated following appears largelyincapable of reading the language he posts in.

MediaRank: Computational Ranking of Online News Sources

  In the recent political climate, the topic of news quality has drawnattention both from the public and the academic communities. The growingdistrust of traditional news media makes it harder to find a common base ofaccepted truth. In this work, we design and build MediaRank(www.media-rank.com), a fully automated system to rank over 50,000 online newssources around the world. MediaRank collects and analyzes one million newswebpages and two million related tweets everyday. We base our algorithmicanalysis on four properties journalists have established to be associated withreporting quality: peer reputation, reporting bias / breadth, bottomlinefinancial pressure, and popularity.  Our major contributions of this paper include: (i) Open, interpretablequality rankings for over 50,000 of the world's major news sources. Ourrankings are validated against 35 published news rankings, including French,German, Russian, and Spanish language sources. MediaRank scores correlatepositively with 34 of 35 of these expert rankings. (ii) New computationalmethods for measuring influence and bottomline pressure. To the best of ourknowledge, we are the first to study the large-scale news reporting citationgraph in-depth. We also propose new ways to measure the aggressiveness ofadvertisements and identify social bots, establishing a connection between bothtypes of bad behavior. (iii) Analyzing the effect of media source bias andsignificance. We prove that news sources cite others despite differentpolitical views in accord with quality measures. However, in fourEnglish-speaking countries (US, UK, Canada, and Australia), the highest rankingsources all disproportionately favor left-wing parties, even when the majorityof news sources exhibited conservative slants.

When Can You Fold a Map?

  We explore the following problem: given a collection of creases on a piece ofpaper, each assigned a folding direction of mountain or valley, is there a flatfolding by a sequence of simple folds? There are several models of simplefolds; the simplest one-layer simple fold rotates a portion of paper about acrease in the paper by +-180 degrees. We first consider the analogous questionsin one dimension lower -- bending a segment into a flat object -- which lead tointeresting problems on strings. We develop efficient algorithms for therecognition of simply foldable 1D crease patterns, and reconstruction of asequence of simple folds. Indeed, we prove that a 1D crease pattern isflat-foldable by any means precisely if it is by a sequence of one-layer simplefolds.  Next we explore simple foldability in two dimensions, and find a surprisingcontrast: ``map'' folding and variants are polynomial, but slightgeneralizations are NP-complete. Specifically, we develop a linear-timealgorithm for deciding foldability of an orthogonal crease pattern on arectangular piece of paper, and prove that it is (weakly) NP-complete to decidefoldability of (1) an orthogonal crease pattern on a orthogonal piece of paper,(2) a crease pattern of axis-parallel and diagonal (45-degree) creases on asquare piece of paper, and (3) crease patterns without a mountain/valleyassignment.

