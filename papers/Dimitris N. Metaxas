Derivative expansion and gauge independence of the false vacuum decay  rate in various gauges

  In theories with radiative symmetry breaking, the calculation of the falsevacuum decay rate requires the inclusion of higher-order terms in thederivative expansion of the effective action. I show here that, in the case ofcovariant gauges, the presence of infrared singularities forbids the consistentcalculation by keeping the lowest-order terms. The situation is remedied,however, in the case of $R_{\xi}$ gauges. Using the Nielsen identities I showthat the final result is gauge independent for generic values of the gaugeparameter $v$ that are not anomalously small.

CU-Net: Coupled U-Nets

  We design a new connectivity pattern for the U-Net architecture. Givenseveral stacked U-Nets, we couple each U-Net pair through the connections oftheir semantic blocks, resulting in the coupled U-Nets (CU-Net). The couplingconnections could make the information flow more efficiently across U-Nets. Thefeature reuse across U-Nets makes each U-Net very parameter efficient. Weevaluate the coupled U-Nets on two benchmark datasets of human pose estimation.Both the accuracy and model parameter number are compared. The CU-Net obtainscomparable accuracy as state-of-the-art methods. However, it only has at least60% fewer parameters than other approaches.

Visual Tracking via Reliable Memories

  In this paper, we propose a novel visual tracking framework thatintelligently discovers reliable patterns from a wide range of video to resistdrift error for long-term tracking tasks. First, we design a Discrete FourierTransform (DFT) based tracker which is able to exploit a large number oftracked samples while still ensures real-time performance. Second, we propose aclustering method with temporal constraints to explore and memorize consistentpatterns from previous frames, named as reliable memories. By virtue of thismethod, our tracker can utilize uncontaminated information to alleviatedrifting issues. Experimental results show that our tracker performs favorablyagainst other state of-the-art methods on benchmark datasets. Furthermore, itis significantly competent in handling drifts and able to robustly trackchallenging long videos over 4000 frames, while most of others lose track atearly frames.

A Recurrent Encoder-Decoder Network for Sequential Face Alignment

  We propose a novel recurrent encoder-decoder network model for real-timevideo-based face alignment. Our proposed model predicts 2D facial point mapsregularized by a regression loss, while uniquely exploiting recurrent learningat both spatial and temporal dimensions. At the spatial level, we add afeedback loop connection between the combined output response map and theinput, in order to enable iterative coarse-to-fine face alignment using asingle network model. At the temporal level, we first decouple the features inthe bottleneck of the network into temporal-variant factors, such as pose andexpression, and temporal-invariant factors, such as identity information.Temporal recurrent learning is then applied to the decoupled temporal-variantfeatures, yielding better generalization and significantly more accurateresults at test time. We perform a comprehensive experimental analysis, showingthe importance of each component of our proposed model, as well as superiorresults over the state-of-the-art in standard datasets.

Track Facial Points in Unconstrained Videos

  Tracking Facial Points in unconstrained videos is challenging due to thenon-rigid deformation that changes over time. In this paper, we propose toexploit incremental learning for person-specific alignment in wild conditions.Our approach takes advantage of part-based representation and cascaderegression for robust and efficient alignment on each frame. Unlike existingmethods that usually rely on models trained offline, we incrementally updatethe representation subspace and the cascade of regressors in a unifiedframework to achieve personalized modeling on the fly. To alleviate thedrifting issue, the fitting results are evaluated using a deep neural network,where well-aligned faces are picked out to incrementally update therepresentation and fitting models. Both image and video datasets are employedto valid the proposed method. The results demonstrate the superior performanceof our approach compared with existing approaches in terms of fitting accuracyand efficiency.

Multispectral Deep Neural Networks for Pedestrian Detection

  Multispectral pedestrian detection is essential for around-the-clockapplications, e.g., surveillance and autonomous driving. We deeply analyzeFaster R-CNN for multispectral pedestrian detection task and then model it intoa convolutional network (ConvNet) fusion problem. Further, we discover thatConvNet-based pedestrian detectors trained by color or thermal imagesseparately provide complementary information in discriminating human instances.Thus there is a large potential to improve pedestrian detection by using colorand thermal images in DNNs simultaneously. We carefully design four ConvNetfusion architectures that integrate two-branch ConvNets on different DNNsstages, all of which yield better performance compared with the baselinedetector. Our experimental results on KAIST pedestrian benchmark show that theHalfway Fusion model that performs fusion on the middle-level convolutionalfeatures outperforms the baseline method by 11% and yields a missing rate 3.5%lower than the other proposed architectures.

Cartoonish sketch-based face editing in videos using identity  deformation transfer

  We address the problem of using hand-drawn sketches to create exaggerateddeformations to faces in videos, such as enlarging the shape or modifying theposition of eyes or mouth. This task is formulated as a 3D face modelreconstruction and deformation problem. We first recover the facial identityand expressions from the video by fitting a face morphable model for eachframe. At the same time, user's editing intention is recognized from inputsketches as a set of facial modifications. Then a novel identity deformationalgorithm is proposed to transfer these facial deformations from 2D space tothe 3D facial identity directly while preserving the facial expressions. Afteran optional stage for further refining the 3D face model, these changes arepropagated to the whole video with the modified identity. Both the user studyand experimental results demonstrate that our sketching framework can helpusers effectively edit facial identities in videos, while high consistency andfidelity are ensured at the same time.

RED-Net: A Recurrent Encoder-Decoder Network for Video-based Face  Alignment

  We propose a novel method for real-time face alignment in videos based on arecurrent encoder-decoder network model. Our proposed model predicts 2D facialpoint heat maps regularized by both detection and regression loss, whileuniquely exploiting recurrent learning at both spatial and temporal dimensions.At the spatial level, we add a feedback loop connection between the combinedoutput response map and the input, in order to enable iterative coarse-to-fineface alignment using a single network model, instead of relying on traditionalcascaded model ensembles. At the temporal level, we first decouple the featuresin the bottleneck of the network into temporal-variant factors, such as poseand expression, and temporal-invariant factors, such as identity information.Temporal recurrent learning is then applied to the decoupled temporal-variantfeatures. We show that such feature disentangling yields better generalizationand significantly more accurate results at test time. We perform acomprehensive experimental analysis, showing the importance of each componentof our proposed model, as well as superior results over the state of the artand several variations of our method in standard datasets.

CR-GAN: Learning Complete Representations for Multi-view Generation

  Generating multi-view images from a single-view input is an essential yetchallenging problem. It has broad applications in vision, graphics, androbotics. Our study indicates that the widely-used generative adversarialnetwork (GAN) may learn "incomplete" representations due to the single-pathwayframework: an encoder-decoder network followed by a discriminator network. Wepropose CR-GAN to address this problem. In addition to the singlereconstruction path, we introduce a generation sideway to maintain thecompleteness of the learned embedding space. The two learning pathwayscollaborate and compete in a parameter-sharing manner, yielding considerablyimproved generalization ability to "unseen" dataset. More importantly, thetwo-pathway framework makes it possible to combine both labeled and unlabeleddata for self-supervised learning, which further enriches the embedding spacefor realistic generations. The experimental results prove that CR-GANsignificantly outperforms state-of-the-art methods, especially when generatingfrom "unseen" inputs in wild conditions.

Semantic Graph Convolutional Networks for 3D Human Pose Regression

  In this paper, we study the problem of learning Graph Convolutional Networks(GCNs) for regression. Current architectures of GCNs are limited to the smallreceptive field of convolution filters and shared transformation matrix foreach node. To address these limitations, we propose Semantic GraphConvolutional Networks (SemGCN), a novel neural network architecture thatoperates on regression tasks with graph-structured data. SemGCN learns tocapture semantic information such as local and global node relationships, whichis not explicitly represented in the graph. These semantic relationships can belearned through end-to-end training from the ground truth without additionalsupervision or hand-crafted rules. We further investigate applying SemGCN to 3Dhuman pose regression. Our formulation is intuitive and sufficient since both2D and 3D human poses can be represented as a structured graph encoding therelationships between joints in the skeleton of a human body. We carry outcomprehensive studies to validate our method. The results prove that SemGCNoutperforms state of the art while using 90% fewer parameters.

Dual Iterative Hard Thresholding: From Non-convex Sparse Minimization to  Non-smooth Concave Maximization

  Iterative Hard Thresholding (IHT) is a class of projected gradient descentmethods for optimizing sparsity-constrained minimization models, with the bestknown efficiency and scalability in practice. As far as we know, the existingIHT-style methods are designed for sparse minimization in primal form. Itremains open to explore duality theory and algorithms in such a non-convex andNP-hard problem setting. In this paper, we bridge this gap by establishing aduality theory for sparsity-constrained minimization with $\ell_2$-regularizedloss function and proposing an IHT-style algorithm for dual maximization. Oursparse duality theory provides a set of sufficient and necessary conditionsunder which the original NP-hard/non-convex problem can be equivalently solvedin a dual formulation. The proposed dual IHT algorithm is a super-gradientmethod for maximizing the non-smooth dual objective. An interesting finding isthat the sparse recovery performance of dual IHT is invariant to the RestrictedIsometry Property (RIP), which is required by virtually all the existing primalIHT algorithms without sparsity relaxation. Moreover, a stochastic variant ofdual IHT is proposed for large-scale stochastic optimization. Numerical resultsdemonstrate the superiority of dual IHT algorithms to the state-of-the-artprimal IHT-style algorithms in model estimation accuracy and computationalefficiency.

Scenarios: A New Representation for Complex Scene Understanding

  The ability for computational agents to reason about the high-level contentof real world scene images is important for many applications. Existingattempts at addressing the problem of complex scene understanding lackrepresentational power, efficiency, and the ability to create robustmeta-knowledge about scenes. In this paper, we introduce scenarios as a new wayof representing scenes. The scenario is a simple, low-dimensional, data-drivenrepresentation consisting of sets of frequently co-occurring objects and isuseful for a wide range of scene understanding tasks. We learn scenarios fromdata using a novel matrix factorization method which we integrate into a newneural network architecture, the ScenarioNet. Using ScenarioNet, we can recoversemantic information about real world scene images at three levels ofgranularity: 1) scene categories, 2) scenarios, and 3) objects. Training asingle ScenarioNet model enables us to perform scene classification, scenariorecognition, multi-object recognition, content-based scene image retrieval, andcontent-based image comparison. In addition to solving many tasks in a single,unified framework, ScenarioNet is more computationally efficient than otherCNNs because it requires significantly fewer parameters while achieving similarperformance on benchmark tasks and is more interpretable because it producesexplanations when making decisions. We validate the utility of scenarios andScenarioNet on a diverse set of scene understanding tasks on several benchmarkdatasets.

