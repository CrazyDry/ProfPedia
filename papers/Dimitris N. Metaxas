Derivative expansion and gauge independence of the false vacuum decay
  rate in various gauges

  In theories with radiative symmetry breaking, the calculation of the false
vacuum decay rate requires the inclusion of higher-order terms in the
derivative expansion of the effective action. I show here that, in the case of
covariant gauges, the presence of infrared singularities forbids the consistent
calculation by keeping the lowest-order terms. The situation is remedied,
however, in the case of $R_{\xi}$ gauges. Using the Nielsen identities I show
that the final result is gauge independent for generic values of the gauge
parameter $v$ that are not anomalously small.


CU-Net: Coupled U-Nets

  We design a new connectivity pattern for the U-Net architecture. Given
several stacked U-Nets, we couple each U-Net pair through the connections of
their semantic blocks, resulting in the coupled U-Nets (CU-Net). The coupling
connections could make the information flow more efficiently across U-Nets. The
feature reuse across U-Nets makes each U-Net very parameter efficient. We
evaluate the coupled U-Nets on two benchmark datasets of human pose estimation.
Both the accuracy and model parameter number are compared. The CU-Net obtains
comparable accuracy as state-of-the-art methods. However, it only has at least
60% fewer parameters than other approaches.


Visual Tracking via Reliable Memories

  In this paper, we propose a novel visual tracking framework that
intelligently discovers reliable patterns from a wide range of video to resist
drift error for long-term tracking tasks. First, we design a Discrete Fourier
Transform (DFT) based tracker which is able to exploit a large number of
tracked samples while still ensures real-time performance. Second, we propose a
clustering method with temporal constraints to explore and memorize consistent
patterns from previous frames, named as reliable memories. By virtue of this
method, our tracker can utilize uncontaminated information to alleviate
drifting issues. Experimental results show that our tracker performs favorably
against other state of-the-art methods on benchmark datasets. Furthermore, it
is significantly competent in handling drifts and able to robustly track
challenging long videos over 4000 frames, while most of others lose track at
early frames.


A Recurrent Encoder-Decoder Network for Sequential Face Alignment

  We propose a novel recurrent encoder-decoder network model for real-time
video-based face alignment. Our proposed model predicts 2D facial point maps
regularized by a regression loss, while uniquely exploiting recurrent learning
at both spatial and temporal dimensions. At the spatial level, we add a
feedback loop connection between the combined output response map and the
input, in order to enable iterative coarse-to-fine face alignment using a
single network model. At the temporal level, we first decouple the features in
the bottleneck of the network into temporal-variant factors, such as pose and
expression, and temporal-invariant factors, such as identity information.
Temporal recurrent learning is then applied to the decoupled temporal-variant
features, yielding better generalization and significantly more accurate
results at test time. We perform a comprehensive experimental analysis, showing
the importance of each component of our proposed model, as well as superior
results over the state-of-the-art in standard datasets.


Track Facial Points in Unconstrained Videos

  Tracking Facial Points in unconstrained videos is challenging due to the
non-rigid deformation that changes over time. In this paper, we propose to
exploit incremental learning for person-specific alignment in wild conditions.
Our approach takes advantage of part-based representation and cascade
regression for robust and efficient alignment on each frame. Unlike existing
methods that usually rely on models trained offline, we incrementally update
the representation subspace and the cascade of regressors in a unified
framework to achieve personalized modeling on the fly. To alleviate the
drifting issue, the fitting results are evaluated using a deep neural network,
where well-aligned faces are picked out to incrementally update the
representation and fitting models. Both image and video datasets are employed
to valid the proposed method. The results demonstrate the superior performance
of our approach compared with existing approaches in terms of fitting accuracy
and efficiency.


Multispectral Deep Neural Networks for Pedestrian Detection

  Multispectral pedestrian detection is essential for around-the-clock
applications, e.g., surveillance and autonomous driving. We deeply analyze
Faster R-CNN for multispectral pedestrian detection task and then model it into
a convolutional network (ConvNet) fusion problem. Further, we discover that
ConvNet-based pedestrian detectors trained by color or thermal images
separately provide complementary information in discriminating human instances.
Thus there is a large potential to improve pedestrian detection by using color
and thermal images in DNNs simultaneously. We carefully design four ConvNet
fusion architectures that integrate two-branch ConvNets on different DNNs
stages, all of which yield better performance compared with the baseline
detector. Our experimental results on KAIST pedestrian benchmark show that the
Halfway Fusion model that performs fusion on the middle-level convolutional
features outperforms the baseline method by 11% and yields a missing rate 3.5%
lower than the other proposed architectures.


Cartoonish sketch-based face editing in videos using identity
  deformation transfer

  We address the problem of using hand-drawn sketches to create exaggerated
deformations to faces in videos, such as enlarging the shape or modifying the
position of eyes or mouth. This task is formulated as a 3D face model
reconstruction and deformation problem. We first recover the facial identity
and expressions from the video by fitting a face morphable model for each
frame. At the same time, user's editing intention is recognized from input
sketches as a set of facial modifications. Then a novel identity deformation
algorithm is proposed to transfer these facial deformations from 2D space to
the 3D facial identity directly while preserving the facial expressions. After
an optional stage for further refining the 3D face model, these changes are
propagated to the whole video with the modified identity. Both the user study
and experimental results demonstrate that our sketching framework can help
users effectively edit facial identities in videos, while high consistency and
fidelity are ensured at the same time.


RED-Net: A Recurrent Encoder-Decoder Network for Video-based Face
  Alignment

  We propose a novel method for real-time face alignment in videos based on a
recurrent encoder-decoder network model. Our proposed model predicts 2D facial
point heat maps regularized by both detection and regression loss, while
uniquely exploiting recurrent learning at both spatial and temporal dimensions.
At the spatial level, we add a feedback loop connection between the combined
output response map and the input, in order to enable iterative coarse-to-fine
face alignment using a single network model, instead of relying on traditional
cascaded model ensembles. At the temporal level, we first decouple the features
in the bottleneck of the network into temporal-variant factors, such as pose
and expression, and temporal-invariant factors, such as identity information.
Temporal recurrent learning is then applied to the decoupled temporal-variant
features. We show that such feature disentangling yields better generalization
and significantly more accurate results at test time. We perform a
comprehensive experimental analysis, showing the importance of each component
of our proposed model, as well as superior results over the state of the art
and several variations of our method in standard datasets.


CR-GAN: Learning Complete Representations for Multi-view Generation

  Generating multi-view images from a single-view input is an essential yet
challenging problem. It has broad applications in vision, graphics, and
robotics. Our study indicates that the widely-used generative adversarial
network (GAN) may learn "incomplete" representations due to the single-pathway
framework: an encoder-decoder network followed by a discriminator network. We
propose CR-GAN to address this problem. In addition to the single
reconstruction path, we introduce a generation sideway to maintain the
completeness of the learned embedding space. The two learning pathways
collaborate and compete in a parameter-sharing manner, yielding considerably
improved generalization ability to "unseen" dataset. More importantly, the
two-pathway framework makes it possible to combine both labeled and unlabeled
data for self-supervised learning, which further enriches the embedding space
for realistic generations. The experimental results prove that CR-GAN
significantly outperforms state-of-the-art methods, especially when generating
from "unseen" inputs in wild conditions.


Semantic Graph Convolutional Networks for 3D Human Pose Regression

  In this paper, we study the problem of learning Graph Convolutional Networks
(GCNs) for regression. Current architectures of GCNs are limited to the small
receptive field of convolution filters and shared transformation matrix for
each node. To address these limitations, we propose Semantic Graph
Convolutional Networks (SemGCN), a novel neural network architecture that
operates on regression tasks with graph-structured data. SemGCN learns to
capture semantic information such as local and global node relationships, which
is not explicitly represented in the graph. These semantic relationships can be
learned through end-to-end training from the ground truth without additional
supervision or hand-crafted rules. We further investigate applying SemGCN to 3D
human pose regression. Our formulation is intuitive and sufficient since both
2D and 3D human poses can be represented as a structured graph encoding the
relationships between joints in the skeleton of a human body. We carry out
comprehensive studies to validate our method. The results prove that SemGCN
outperforms state of the art while using 90% fewer parameters.


Dual Iterative Hard Thresholding: From Non-convex Sparse Minimization to
  Non-smooth Concave Maximization

  Iterative Hard Thresholding (IHT) is a class of projected gradient descent
methods for optimizing sparsity-constrained minimization models, with the best
known efficiency and scalability in practice. As far as we know, the existing
IHT-style methods are designed for sparse minimization in primal form. It
remains open to explore duality theory and algorithms in such a non-convex and
NP-hard problem setting. In this paper, we bridge this gap by establishing a
duality theory for sparsity-constrained minimization with $\ell_2$-regularized
loss function and proposing an IHT-style algorithm for dual maximization. Our
sparse duality theory provides a set of sufficient and necessary conditions
under which the original NP-hard/non-convex problem can be equivalently solved
in a dual formulation. The proposed dual IHT algorithm is a super-gradient
method for maximizing the non-smooth dual objective. An interesting finding is
that the sparse recovery performance of dual IHT is invariant to the Restricted
Isometry Property (RIP), which is required by virtually all the existing primal
IHT algorithms without sparsity relaxation. Moreover, a stochastic variant of
dual IHT is proposed for large-scale stochastic optimization. Numerical results
demonstrate the superiority of dual IHT algorithms to the state-of-the-art
primal IHT-style algorithms in model estimation accuracy and computational
efficiency.


Scenarios: A New Representation for Complex Scene Understanding

  The ability for computational agents to reason about the high-level content
of real world scene images is important for many applications. Existing
attempts at addressing the problem of complex scene understanding lack
representational power, efficiency, and the ability to create robust
meta-knowledge about scenes. In this paper, we introduce scenarios as a new way
of representing scenes. The scenario is a simple, low-dimensional, data-driven
representation consisting of sets of frequently co-occurring objects and is
useful for a wide range of scene understanding tasks. We learn scenarios from
data using a novel matrix factorization method which we integrate into a new
neural network architecture, the ScenarioNet. Using ScenarioNet, we can recover
semantic information about real world scene images at three levels of
granularity: 1) scene categories, 2) scenarios, and 3) objects. Training a
single ScenarioNet model enables us to perform scene classification, scenario
recognition, multi-object recognition, content-based scene image retrieval, and
content-based image comparison. In addition to solving many tasks in a single,
unified framework, ScenarioNet is more computationally efficient than other
CNNs because it requires significantly fewer parameters while achieving similar
performance on benchmark tasks and is more interpretable because it produces
explanations when making decisions. We validate the utility of scenarios and
ScenarioNet on a diverse set of scene understanding tasks on several benchmark
datasets.


