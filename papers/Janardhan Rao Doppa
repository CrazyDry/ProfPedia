Output Space Search for Structured Prediction

  We consider a framework for structured prediction based on search in thespace of complete structured outputs. Given a structured input, an output isproduced by running a time-bounded search procedure guided by a learned costfunction, and then returning the least cost output uncovered during the search.This framework can be instantiated for a wide range of search spaces and searchprocedures, and easily incorporates arbitrary structured-prediction lossfunctions. In this paper, we make two main technical contributions. First, wedefine the limited-discrepancy search space over structured outputs, which isable to leverage powerful classification learning algorithms to improve thesearch space quality. Second, we give a generic cost function learningapproach, where the key idea is to learn a cost function that attempts to mimicthe behavior of conducting searches guided by the true loss function. Ourexperiments on six benchmark domains demonstrate that using our framework withonly a small amount of search is sufficient for significantly improving onstate-of-the-art structured-prediction performance.

Machine Learning and Manycore Systems Design: A Serendipitous Symbiosis

  Tight collaboration between experts of machine learning and manycore systemdesign is necessary to create a data-driven manycore design framework thatintegrates both learning and expert knowledge. Such a framework will benecessary to address the rising complexity of designing large-scale manycoresystems and machine learning techniques.

Preference-Guided Planning: An Active Elicitation Approach

  Planning with preferences has been employed extensively to quickly generatehigh-quality plans. However, it may be difficult for the human expert to supplythis information without knowledge of the reasoning employed by the planner andthe distribution of planning problems. We consider the problem of activelyeliciting preferences from a human expert during the planning process.Specifically, we study this problem in the context of the Hierarchical TaskNetwork (HTN) planning framework as it allows easy interaction with the human.Our experimental results on several diverse planning domains show that thepreferences gathered using the proposed approach improve the quality and speedof the planner, while reducing the burden on the human expert.

Learning Scripts as Hidden Markov Models

  Scripts have been proposed to model the stereotypical event sequences foundin narratives. They can be applied to make a variety of inferences includingfilling gaps in the narratives and resolving ambiguous references. This paperproposes the first formal framework for scripts based on Hidden Markov Models(HMMs). Our framework supports robust inference and learning algorithms, whichare lacking in previous clustering models. We develop an algorithm forstructure and parameter learning based on Expectation Maximization and evaluateit on a number of natural datasets. The results show that our algorithm issuperior to several informed baselines for predicting missing events in partialobservation sequences.

GLAD: GLocalized Anomaly Detection via Active Feature Space Suppression

  We propose an algorithm called GLAD (GLocalized Anomaly Detection) thatallows end-users to retain the use of simple and understandable global anomalydetectors by automatically learning their local relevance to specific datainstances using label feedback. The key idea is to place a uniform prior on therelevance of each member of the anomaly detection ensemble over the inputfeature space via a neural network trained on unlabeled instances, and tune theweights of the neural network to adjust the local relevance of each ensemblemember using all labeled instances. Our experiments on synthetic and real-worlddata show the effectiveness of GLAD in learning the local relevance of ensemblemembers and discovering anomalies via label feedback.

Design-Space Exploration and Optimization of an Energy-Efficient and  Reliable 3D Small-world Network-on-Chip

  A three-dimensional (3D) Network-on-Chip (NoC) enables the design of highperformance and low power many-core chips. Existing 3D NoCs are inadequate formeeting the ever-increasing performance requirements of many-core processorssince they are simple extensions of regular 2D architectures and they do notfully exploit the advantages provided by 3D integration. Moreover, theanticipated performance gain of a 3D NoC-enabled many-core chip may becompromised due to the potential failures of through-silicon-vias (TSVs) thatare predominantly used as vertical interconnects in a 3D IC. To address theseproblems, we propose a machine-learning-inspired predictive design methodologyfor energy-efficient and reliable many-core architectures enabled by 3Dintegration. We demonstrate that a small-world network-based 3D NoC (3D SWNoC)performs significantly better than its 3D MESH-based counterparts. On average,the 3D SWNoC shows 35% energy-delay-product (EDP) improvement over 3D MESH forthe PARSEC and SPLASH2 benchmarks considered in this work. To improve thereliability of 3D NoC, we propose a computationally efficient spare-verticallink (sVL) allocation algorithm based on a state-space search formulation. Ourresults show that the proposed sVL allocation algorithm can significantlyimprove the reliability as well as the lifetime of 3D SWNoC.

Learning to Speed Up Query Planning in Graph Databases

  Querying graph structured data is a fundamental operation that enablesimportant applications including knowledge graph search, social networkanalysis, and cyber-network security. However, the growing size of real-worlddata graphs poses severe challenges for graph databases to meet theresponse-time requirements of the applications. Planning the computationalsteps of query processing - Query Planning - is central to address thesechallenges. In this paper, we study the problem of learning to speedup queryplanning in graph databases towards the goal of improving thecomputational-efficiency of query processing via training queries.We present aLearning to Plan (L2P) framework that is applicable to a large class of queryreasoners that follow the Threshold Algorithm (TA) approach. First, we define ageneric search space over candidate query plans, and identify target searchtrajectories (query plans) corresponding to the training queries by performingan expensive search. Subsequently, we learn greedy search control knowledge toimitate the search behavior of the target query plans. We provide a concreteinstantiation of our L2P framework for STAR, a state-of-the-art graph queryreasoner. Our experiments on benchmark knowledge graphs including DBpedia,YAGO, and Freebase show that using the query plans generated by the learnedsearch control knowledge, we can significantly improve the speed of STAR withnegligible loss in accuracy.

Active Anomaly Detection via Ensembles

  In critical applications of anomaly detection including computer security andfraud prevention, the anomaly detector must be configurable by the analyst tominimize the effort on false positives. One important way to configure theanomaly detector is by providing true labels for a few instances. We study theproblem of label-efficient active learning to automatically tune anomalydetection ensembles and make four main contributions. First, we present animportant insight into how anomaly detector ensembles are naturally suited foractive learning. This insight allows us to relate the greedy querying strategyto uncertainty sampling, with implications for label-efficiency. Second, wepresent a novel formalism called compact description to describe the discoveredanomalies and show that it can also be employed to improve the diversity of theinstances presented to the analyst without loss in the anomaly discovery rate.Third, we present a novel data drift detection algorithm that not only detectsthe drift robustly, but also allows us to take corrective actions to adapt thedetector in a principled manner. Fourth, we present extensive experiments toevaluate our insights and algorithms in both batch and streaming settings. Ourresults show that in addition to discovering significantly more anomalies thanstate-of-the-art unsupervised baselines, our active learning algorithms underthe streaming-data setup are competitive with the batch setup.

Trading-off Accuracy and Energy of Deep Inference on Embedded Systems: A  Co-Design Approach

  Deep neural networks have seen tremendous success for different modalities ofdata including images, videos, and speech. This success has led to theirdeployment in mobile and embedded systems for real-time applications. However,making repeated inferences using deep networks on embedded systems posessignificant challenges due to constrained resources (e.g., energy and computingpower). To address these challenges, we develop a principled co-designapproach. Building on prior work, we develop a formalism referred to asCoarse-to-Fine Networks (C2F Nets) that allow us to employ classifiers ofvarying complexity to make predictions. We propose a principled optimizationalgorithm to automatically configure C2F Nets for a specified trade-off betweenaccuracy and energy consumption for inference. The key idea is to select aclassifier on-the-fly whose complexity is proportional to the hardness of theinput example: simple classifiers for easy inputs and complex classifiers forhard inputs. We perform comprehensive experimental evaluation using fourdifferent C2F Net architectures on multiple real-world image classificationtasks. Our results show that optimized C2F Net can reduce the Energy DelayProduct (EDP) by 27 to 60 percent with no loss in accuracy when compared to thebaseline solution, where all predictions are made using the most complexclassifier in C2F Net.

On-Chip Communication Network for Efficient Training of Deep  Convolutional Networks on Heterogeneous Manycore Systems

  Convolutional Neural Networks (CNNs) have shown a great deal of success indiverse application domains including computer vision, speech recognition, andnatural language processing. However, as the size of datasets and the depth ofneural network architectures continue to grow, it is imperative to designhigh-performance and energy-efficient computing hardware for training CNNs. Inthis paper, we consider the problem of designing specialized CPU-GPU basedheterogeneous manycore systems for energy-efficient training of CNNs. It hasalready been shown that the typical on-chip communication infrastructuresemployed in conventional CPU-GPU based heterogeneous manycore platforms areunable to handle both CPU and GPU communication requirements efficiently. Toaddress this issue, we first analyze the on-chip traffic patterns that arisefrom the computational processes associated with training two deep CNNarchitectures, namely, LeNet and CDBNet, to perform image classification. Byleveraging this knowledge, we design a hybrid Network-on-Chip (NoC)architecture, which consists of both wireline and wireless links, to improvethe performance of CPU-GPU based heterogeneous manycore platforms running theabove-mentioned CNN training workloads. The proposed NoC achieves 1.8xreduction in network latency and improves the network throughput by a factor of2.2 for training CNNs, when compared to a highly-optimized wireline mesh NoC.For the considered CNN workloads, these network-level improvements translateinto 25% savings in full-system energy-delay-product (EDP). This demonstratesthat the proposed hybrid NoC for heterogeneous manycore architectures iscapable of significantly accelerating training of CNNs while remainingenergy-efficient.

Learning-based Application-Agnostic 3D NoC Design for Heterogeneous  Manycore Systems

  The rising use of deep learning and other big-data algorithms has led to anincreasing demand for hardware platforms that are computationally powerful, yetenergy-efficient. Due to the amount of data parallelism in these algorithms,high-performance 3D manycore platforms that incorporate both CPUs and GPUspresent a promising direction. However, as systems use heterogeneity (e.g., acombination of CPUs, GPUs, and accelerators) to improve performance andefficiency, it becomes more pertinent to address the distinct and likelyconflicting communication requirements (e.g., CPU memory access latency or GPUnetwork throughput) that arise from such heterogeneity. Unfortunately, it isdifficult to quickly explore the hardware design space and choose appropriatetradeoffs between these heterogeneous requirements. To address thesechallenges, we propose the design of a 3D Network-on-Chip (NoC) forheterogeneous manycore platforms that considers the appropriate designobjectives for a 3D heterogeneous system and explores various tradeoffs usingan efficient ML-based multi-objective optimization technique. The proposeddesign space exploration considers the various requirements of itsheterogeneous components and generates a set of 3D NoC architectures thatefficiently trades off these design objectives. Our findings show that byjointly considering these requirements (latency, throughput, temperature, andenergy), we can achieve 9.6% better Energy-Delay Product on average at nearlyiso-temperature conditions when compared to a thermally-optimized design for 3Dheterogeneous NoCs. More importantly, our results suggest that our 3D NoCsoptimized for a few applications can be generalized for unknown applications aswell. Our results show that these generalized 3D NoCs only incur a 1.8%(36-tile system) and 1.1% (64-tile system) average performance loss compared toapplication-specific NoCs.

Active Anomaly Detection via Ensembles: Insights, Algorithms, and  Interpretability

  Anomaly detection (AD) task corresponds to identifying the true anomaliesfrom a given set of data instances. AD algorithms score the data instances andproduce a ranked list of candidate anomalies, which are then analyzed by ahuman to discover the true anomalies. However, this process can be laboriousfor the human analyst when the number of false-positives is very high.Therefore, in many real-world AD applications including computer security andfraud prevention, the anomaly detector must be configurable by the humananalyst to minimize the effort on false positives.  In this paper, we study the problem of active learning to automatically tuneensemble of anomaly detectors to maximize the number of true anomaliesdiscovered. We make four main contributions towards this goal. First, wepresent an important insight that explains the practical successes of ADensembles and how ensembles are naturally suited for active learning. Second,we present several algorithms for active learning with tree-based AD ensembles.These algorithms help us to improve the diversity of discovered anomalies,generate rule sets for improved interpretability of anomalous instances, andadapt to streaming data settings in a principled manner. Third, we present anovel algorithm called GLocalized Anomaly Detection (GLAD) for active learningwith generic AD ensembles. GLAD allows end-users to retain the use of simpleand understandable global anomaly detectors by automatically learning theirlocal relevance to specific data instances using label feedback. Fourth, wepresent extensive experiments to evaluate our insights and algorithms. Ourresults show that in addition to discovering significantly more anomalies thanstate-of-the-art unsupervised baselines, our active learning algorithms underthe streaming-data setup are competitive with the batch setup.

