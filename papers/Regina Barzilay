Learning to Paraphrase: An Unsupervised Approach Using Multiple-Sequence
  Alignment

  We address the text-to-text generation problem of sentence-level paraphrasing
-- a phenomenon distinct from and more difficult than word- or phrase-level
paraphrasing. Our approach applies multiple-sequence alignment to sentences
gathered from unannotated comparable corpora: it learns a set of paraphrasing
patterns represented by word lattice pairs and automatically determines how to
apply these patterns to rewrite new sentences. The results of our evaluation
experiments show that the system derives accurate paraphrases, outperforming
baseline systems.


Catching the Drift: Probabilistic Content Models, with Applications to
  Generation and Summarization

  We consider the problem of modeling the content structure of texts within a
specific domain, in terms of the topics the texts address and the order in
which these topics appear. We first present an effective knowledge-lean method
for learning content models from un-annotated documents, utilizing a novel
adaptation of algorithms for Hidden Markov Models. We then apply our method to
two complementary tasks: information ordering and extractive summarization. Our
experiments show that incorporating content models in these applications yields
substantial improvement over previously-proposed methods.


An Unsupervised Method for Uncovering Morphological Chains

  Most state-of-the-art systems today produce morphological analysis based only
on orthographic patterns. In contrast, we propose a model for unsupervised
morphological analysis that integrates orthographic and semantic views of
words. We model word formation in terms of morphological chains, from base
words to the observed words, breaking the chains into parent-child relations.
We use log-linear models with morpheme and word-level features to predict
possible parents, including their modifications, for each word. The limited set
of candidate parents for each word render contrastive estimation feasible. Our
model consistently matches or outperforms five state-of-the-art systems on
Arabic, English and Turkish.


sk_p: a neural program corrector for MOOCs

  We present a novel technique for automatic program correction in MOOCs,
capable of fixing both syntactic and semantic errors without manual, problem
specific correction strategies. Given an incorrect student program, it
generates candidate programs from a distribution of likely corrections, and
checks each candidate for correctness against a test suite.
  The key observation is that in MOOCs many programs share similar code
fragments, and the seq2seq neural network model, used in the natural-language
processing task of machine translation, can be modified and trained to recover
these fragments.
  Experiment shows our scheme can correct 29% of all incorrect submissions and
out-performs state of the art approach which requires manual, problem specific
correction strategies.


Neural Generation of Regular Expressions from Natural Language with
  Minimal Domain Knowledge

  This paper explores the task of translating natural language queries into
regular expressions which embody their meaning. In contrast to prior work, the
proposed neural model does not utilize domain-specific crafting, learning to
translate directly from a parallel corpus. To fully explore the potential of
neural models, we propose a methodology for collecting a large corpus of
regular expression, natural language pairs. Our resulting model achieves a
performance gain of 19.6% over previous state-of-the-art models.


The Three Pillars of Machine Programming

  In this position paper, we describe our vision of the future of machine
programming through a categorical examination of three pillars of research.
Those pillars are: (i) intention, (ii) invention, and(iii) adaptation.
Intention emphasizes advancements in the human-to-computer and
computer-to-machine-learning interfaces. Invention emphasizes the creation or
refinement of algorithms or core hardware and software building blocks through
machine learning (ML). Adaptation emphasizes advances in the use of ML-based
constructs to autonomously evolve software.


Deriving Machine Attention from Human Rationales

  Attention-based models are successful when trained on large amounts of data.
In this paper, we demonstrate that even in the low-resource scenario, attention
can be learned effectively. To this end, we start with discrete human-annotated
rationales and map them into continuous attention. Our central hypothesis is
that this mapping is general across domains, and thus can be transferred from
resource-rich domains to low-resource ones. Our model jointly learns a
domain-invariant representation and induces the desired mapping between
rationales and attention. Our empirical results validate this hypothesis and
show that our approach delivers significant gains over state-of-the-art
baselines, yielding over 15% average error reduction on benchmark datasets.


Multi-Source Domain Adaptation with Mixture of Experts

  We propose a mixture-of-experts approach for unsupervised domain adaptation
from multiple sources. The key idea is to explicitly capture the relationship
between a target example and different source domains. This relationship,
expressed by a point-to-set metric, determines how to combine predictors
trained on various domains. The metric is learned in an unsupervised fashion
using meta-training. Experimental results on sentiment analysis and
part-of-speech tagging demonstrate that our approach consistently outperforms
multiple baselines and can robustly handle negative transfer.


Bootstrapping Lexical Choice via Multiple-Sequence Alignment

  An important component of any generation system is the mapping dictionary, a
lexicon of elementary semantic expressions and corresponding natural language
realizations. Typically, labor-intensive knowledge-based methods are used to
construct the dictionary. We instead propose to acquire it automatically via a
novel multiple-pass algorithm employing multiple-sequence alignment, a
technique commonly used in bioinformatics. Crucially, our method leverages
latent information contained in multi-parallel corpora -- datasets that supply
several verbalizations of the corresponding semantics rather than just one.
  We used our techniques to generate natural language versions of
computer-generated mathematical proofs, with good results on both a
per-component and overall-output basis. For example, in evaluations involving a
dozen human judges, our system produced output whose readability and
faithfulness to the semantic input rivaled that of a traditional generation
system.


Content Modeling Using Latent Permutations

  We present a novel Bayesian topic model for learning discourse-level document
structure. Our model leverages insights from discourse theory to constrain
latent topic assignments in a way that reflects the underlying organization of
document topics. We propose a global model in which both topic selection and
ordering are biased to be similar across a collection of related documents. We
show that this space of orderings can be effectively represented using a
distribution over permutations called the Generalized Mallows Model. We apply
our method to three complementary discourse-level tasks: cross-document
alignment, document segmentation, and information ordering. Our experiments
show that incorporating our permutation-based model in these applications
yields substantial improvements in performance over previously proposed
methods.


Multilingual Part-of-Speech Tagging: Two Unsupervised Approaches

  We demonstrate the effectiveness of multilingual learning for unsupervised
part-of-speech tagging. The central assumption of our work is that by combining
cues from multiple languages, the structure of each becomes more apparent. We
consider two ways of applying this intuition to the problem of unsupervised
part-of-speech tagging: a model that directly merges tag structures for a pair
of languages into a single sequence and a second model which instead
incorporates multilingual context using latent variables. Both approaches are
formulated as hierarchical Bayesian models, using Markov Chain Monte Carlo
sampling techniques for inference. Our results demonstrate that by
incorporating multilingual evidence we can achieve impressive performance gains
across a range of scenarios. We also found that performance improves steadily
as the number of available languages increases.


Automatic Aggregation by Joint Modeling of Aspects and Values

  We present a model for aggregation of product review snippets by joint aspect
identification and sentiment analysis. Our model simultaneously identifies an
underlying set of ratable aspects presented in the reviews of a product (e.g.,
sushi and miso for a Japanese restaurant) and determines the corresponding
sentiment of each aspect. This approach directly enables discovery of
highly-rated or inconsistent aspects of a product. Our generative model admits
an efficient variational mean-field inference algorithm. It is also easily
extensible, and we describe several modifications and their effects on model
structure and inference. We test our model on two tasks, joint aspect
identification and sentiment analysis on a set of Yelp reviews and aspect
identification alone on a set of medical summaries. We evaluate the performance
of the model on aspect identification, sentiment analysis, and per-word
labeling accuracy. We demonstrate that our model outperforms applicable
baselines by a considerable margin, yielding up to 32% relative error reduction
on aspect identification and up to 20% relative error reduction on sentiment
analysis.


Semi-supervised Question Retrieval with Gated Convolutions

  Question answering forums are rapidly growing in size with no effective
automated ability to refer to and reuse answers already available for previous
posted questions. In this paper, we develop a methodology for finding
semantically related questions. The task is difficult since 1) key pieces of
information are often buried in extraneous details in the question body and 2)
available annotations on similar questions are scarce and fragmented. We design
a recurrent and convolutional model (gated convolution) to effectively map
questions to their semantic representations. The models are pre-trained within
an encoder-decoder framework (from body to title) on the basis of the entire
raw corpus, and fine-tuned discriminatively from limited annotations. Our
evaluation demonstrates that our model yields substantial gains over a standard
IR baseline and various neural network architectures (including CNNs, LSTMs and
GRUs).


Rationalizing Neural Predictions

  Prediction without justification has limited applicability. As a remedy, we
learn to extract pieces of input text as justifications -- rationales -- that
are tailored to be short and coherent, yet sufficient for making the same
prediction. Our approach combines two modular components, generator and
encoder, which are trained to operate well together. The generator specifies a
distribution over text fragments as candidate rationales and these are passed
through the encoder for prediction. Rationales are never given during training.
Instead, the model is regularized by desiderata for rationales. We evaluate the
approach on multi-aspect sentiment analysis against manually annotated test
cases. Our approach outperforms attention-based baseline by a significant
margin. We also successfully illustrate the method on the question retrieval
task.


Improving Information Extraction by Acquiring External Evidence with
  Reinforcement Learning

  Most successful information extraction systems operate with access to a large
collection of documents. In this work, we explore the task of acquiring and
incorporating external evidence to improve extraction accuracy in domains where
the amount of training data is scarce. This process entails issuing search
queries, extraction from new sources and reconciliation of extracted values,
which are repeated until sufficient evidence is collected. We approach the
problem using a reinforcement learning framework where our model learns to
select optimal actions based on contextual information. We employ a deep
Q-network, trained to optimize a reward function that reflects extraction
accuracy while penalizing extra effort. Our experiments on two databases -- of
shooting incidents, and food adulteration cases -- demonstrate that our system
significantly outperforms traditional extractors and a competitive
meta-classifier baseline.


Language Understanding for Text-based Games Using Deep Reinforcement
  Learning

  In this paper, we consider the task of learning control policies for
text-based games. In these games, all interactions in the virtual world are
through text and the underlying state is not observed. The resulting language
barrier makes such environments challenging for automatic game players. We
employ a deep reinforcement learning framework to jointly learn state
representations and action policies using game rewards as feedback. This
framework enables us to map text descriptions into vector representations that
capture the semantics of the game states. We evaluate our approach on two game
worlds, comparing against baselines using bag-of-words and bag-of-bigrams for
state representations. Our algorithm outperforms the baselines on both worlds
demonstrating the importance of learning expressive representations.


Molding CNNs for text: non-linear, non-consecutive convolutions

  The success of deep learning often derives from well-chosen operational
building blocks. In this work, we revise the temporal convolution operation in
CNNs to better adapt it to text processing. Instead of concatenating word
representations, we appeal to tensor algebra and use low-rank n-gram tensors to
directly exploit interactions between words already at the convolution stage.
Moreover, we extend the n-gram convolution to non-consecutive words to
recognize patterns with intervening words. Through a combination of low-rank
tensors, and pattern weighting, we can efficiently evaluate the resulting
convolution operation via dynamic programming. We test the resulting
architecture on standard sentiment classification and news categorization
tasks. Our model achieves state-of-the-art performance both in terms of
accuracy and training speed. For instance, we obtain 51.2% accuracy on the
fine-grained sentiment classification task.


Aspect-augmented Adversarial Networks for Domain Adaptation

  We introduce a neural method for transfer learning between two (source and
target) classification tasks or aspects over the same domain. Rather than
training on target labels, we use a few keywords pertaining to source and
target aspects indicating sentence relevance instead of document class labels.
Documents are encoded by learning to embed and softly select relevant sentences
in an aspect-dependent manner. A shared classifier is trained on the source
encoded documents and labels, and applied to target encoded documents. We
ensure transfer through aspect-adversarial training so that encoded documents
are, as sets, aspect-invariant. Experimental results demonstrate that our
approach outperforms different baselines and model variants on two datasets,
yielding an improvement of 27% on a pathology dataset and 5% on a review
dataset.


Unsupervised Learning of Morphological Forests

  This paper focuses on unsupervised modeling of morphological families,
collectively comprising a forest over the language vocabulary. This formulation
enables us to capture edgewise properties reflecting single-step morphological
derivations, along with global distributional properties of the entire forest.
These global properties constrain the size of the affix set and encourage
formation of tight morphological families. The resulting objective is solved
using Integer Linear Programming (ILP) paired with contrastive estimation. We
train the model by alternating between optimizing the local log-linear model
and the global ILP objective. We evaluate our system on three tasks: root
detection, clustering of morphological families and segmentation. Our
experiments demonstrate that our model yields consistent gains in all three
tasks compared with the best published results.


Deriving Neural Architectures from Sequence and Graph Kernels

  The design of neural architectures for structured objects is typically guided
by experimental insights rather than a formal process. In this work, we appeal
to kernels over combinatorial structures, such as sequences and graphs, to
derive appropriate neural operations. We introduce a class of deep recurrent
neural operations and formally characterize their associated kernel spaces. Our
recurrent modules compare the input to virtual reference objects (cf. filters
in CNN) via the kernels. Similar to traditional neural operations, these
reference objects are parameterized and directly optimized in end-to-end
training. We empirically evaluate the proposed class of neural architectures on
standard applications such as language modeling and molecular graph regression,
achieving state-of-the-art results across these applications.


Style Transfer from Non-Parallel Text by Cross-Alignment

  This paper focuses on style transfer on the basis of non-parallel text. This
is an instance of a broad family of problems including machine translation,
decipherment, and sentiment modification. The key challenge is to separate the
content from other aspects such as style. We assume a shared latent content
distribution across different text corpora, and propose a method that leverages
refined alignment of latent representations to perform style transfer. The
transferred sentences from one style should match example sentences from the
other style as a population. We demonstrate the effectiveness of this
cross-alignment method on three tasks: sentiment modification, decipherment of
word substitution ciphers, and recovery of word order.


Representation Learning for Grounded Spatial Reasoning

  The interpretation of spatial references is highly contextual, requiring
joint inference over both language and the environment. We consider the task of
spatial reasoning in a simulated environment, where an agent can act and
receive rewards. The proposed model learns a representation of the world
steered by instruction text. This design allows for precise alignment of local
neighborhoods with corresponding verbalizations, while also handling global
references in the instructions. We train our model with reinforcement learning
using a variant of generalized value iteration. The model outperforms
state-of-the-art approaches on several metrics, yielding a 45% reduction in
goal localization error.


Grounding Language for Transfer in Deep Reinforcement Learning

  In this paper, we explore the utilization of natural language to drive
transfer for reinforcement learning (RL). Despite the wide-spread application
of deep RL techniques, learning generalized policy representations that work
across domains remains a challenging problem. We demonstrate that textual
descriptions of environments provide a compact intermediate channel to
facilitate effective policy transfer. Specifically, by learning to ground the
meaning of text to the dynamics of the environment such as transitions and
rewards, an autonomous agent can effectively bootstrap policy learning on a new
domain given its description. We employ a model-based RL approach consisting of
a differentiable planning module, a model-free component and a factorized state
representation to effectively use entity descriptions. Our model outperforms
prior work on both transfer and multi-task scenarios in a variety of different
environments. For instance, we achieve up to 14% and 11.5% absolute improvement
over previously existing models in terms of average and initial rewards,
respectively.


Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network

  The prediction of organic reaction outcomes is a fundamental problem in
computational chemistry. Since a reaction may involve hundreds of atoms, fully
exploring the space of possible transformations is intractable. The current
solution utilizes reaction templates to limit the space, but it suffers from
coverage and efficiency issues. In this paper, we propose a template-free
approach to efficiently explore the space of product molecules by first
pinpointing the reaction center -- the set of nodes and edges where graph edits
occur. Since only a small number of atoms contribute to reaction center, we can
directly enumerate candidate products. The generated candidates are scored by a
Weisfeiler-Lehman Difference Network that models high-order interactions
between changes occurring at nodes across the molecule. Our framework
outperforms the top-performing template-based approach with a 10\% margin,
while running orders of magnitude faster. Finally, we demonstrate that the
model accuracy rivals the performance of domain experts.


Junction Tree Variational Autoencoder for Molecular Graph Generation

  We seek to automate the design of molecules based on specific chemical
properties. In computational terms, this task involves continuous embedding and
generation of molecular graphs. Our primary contribution is the direct
realization of molecular graphs, a task previously approached by generating
linear SMILES strings instead of graphs. Our junction tree variational
autoencoder generates molecular graphs in two phases, by first generating a
tree-structured scaffold over chemical substructures, and then combining them
into a molecule with a graph message passing network. This approach allows us
to incrementally expand molecules while maintaining chemical validity at every
step. We evaluate our model on multiple tasks ranging from molecular generation
to optimization. Across these tasks, our model outperforms previous
state-of-the-art baselines by a significant margin.


GraphIE: A Graph-Based Framework for Information Extraction

  Most modern Information Extraction (IE) systems are implemented as sequential
taggers and only model local dependencies. Non-local and non-sequential context
is, however, a valuable source of information to improve predictions. In this
paper, we introduce GraphIE, a framework that operates over a graph
representing a broad set of dependencies between textual units (i.e. words or
sentences). The algorithm propagates information between connected nodes
through graph convolutions, generating a richer representation that can be
exploited to improve word-level predictions. Evaluation on three different
tasks --- namely textual, social media and visual information extraction ---
shows that GraphIE consistently outperforms the state-of-the-art sequence
tagging model by a significant margin.


Learning Multimodal Graph-to-Graph Translation for Molecular
  Optimization

  We view molecular optimization as a graph-to-graph translation problem. The
goal is to learn to map from one molecular graph to another with better
properties based on an available corpus of paired molecules. Since molecules
can be optimized in different ways, there are multiple viable translations for
each input graph. A key challenge is therefore to model diverse translation
outputs. Our primary contributions include a junction tree encoder-decoder for
learning diverse graph translations along with a novel adversarial training
method for aligning distributions of molecules. Diverse output distributions in
our model are explicitly realized by low-dimensional latent vectors that
modulate the translation process. We evaluate our model on multiple molecular
optimization tasks and show that our model outperforms previous
state-of-the-art baselines.


Cross-Lingual Alignment of Contextual Word Embeddings, with Applications
  to Zero-shot Dependency Parsing

  We introduce a novel method for multilingual transfer that utilizes deep
contextual embeddings, pretrained in an unsupervised fashion. While contextual
embeddings have been shown to yield richer representations of meaning compared
to their static counterparts, aligning them poses a challenge due to their
dynamic nature. To this end, we construct context-independent variants of the
original monolingual spaces and utilize their mapping to derive an alignment
for the context-dependent spaces. This mapping readily supports processing of a
target language, improving transfer by context-aware embeddings. Our
experimental results demonstrate the effectiveness of this approach for
zero-shot and few-shot learning of dependency parsing. Specifically, our method
consistently outperforms the previous state-of-the-art on 6 tested languages,
yielding an improvement of 6.8 LAS points on average.


Inferring Which Medical Treatments Work from Reports of Clinical Trials

  How do we know if a particular medical treatment actually works? Ideally one
would consult all available evidence from relevant clinical trials.
Unfortunately, such results are primarily disseminated in natural language
scientific articles, imposing substantial burden on those trying to make sense
of them. In this paper, we present a new task and corpus for making this
unstructured evidence actionable. The task entails inferring reported findings
from a full-text article describing a randomized controlled trial (RCT) with
respect to a given intervention, comparator, and outcome of interest, e.g.,
inferring if an article provides evidence supporting the use of aspirin to
reduce risk of stroke, as compared to placebo.
  We present a new corpus for this task comprising 10,000+ prompts coupled with
full-text articles describing RCTs. Results using a suite of models --- ranging
from heuristic (rule-based) approaches to attentive neural architectures ---
demonstrate the difficulty of the task, which we believe largely owes to the
lengthy, technical input texts. To facilitate further work on this important,
challenging problem we make the corpus, documentation, a website and
leaderboard, and code for baselines and evaluation available at
http://evidence-inference.ebm-nlp.com/.


Learning Document-Level Semantic Properties from Free-Text Annotations

  This paper presents a new method for inferring the semantic properties of
documents by leveraging free-text keyphrase annotations. Such annotations are
becoming increasingly abundant due to the recent dramatic growth in
semi-structured, user-generated online content. One especially relevant domain
is product reviews, which are often annotated by their authors with pros/cons
keyphrases such as a real bargain or good value. These annotations are
representative of the underlying semantic properties; however, unlike expert
annotations, they are noisy: lay authors may use different labels to denote the
same property, and some labels may be missing. To learn using such noisy
annotations, we find a hidden paraphrase structure which clusters the
keyphrases. The paraphrase structure is linked with a latent topic model of the
review texts, enabling the system to predict the properties of unannotated
documents and to effectively aggregate the semantic properties of multiple
reviews. Our approach is implemented as a hierarchical Bayesian model with
joint inference. We find that joint inference increases the robustness of the
keyphrase clustering and encourages the latent topics to correlate with
semantically meaningful properties. Multiple evaluations demonstrate that our
model substantially outperforms alternative approaches for summarizing single
and multiple documents into a set of semantically salient keyphrases.


Learning to Win by Reading Manuals in a Monte-Carlo Framework

  Domain knowledge is crucial for effective performance in autonomous control
systems. Typically, human effort is required to encode this knowledge into a
control algorithm. In this paper, we present an approach to language grounding
which automatically interprets text in the context of a complex control
application, such as a game, and uses domain knowledge extracted from the text
to improve control performance. Both text analysis and control strategies are
learned jointly using only a feedback signal inherent to the application. To
effectively leverage textual information, our method automatically extracts the
text segment most relevant to the current game state, and labels it with a
task-centric predicate structure. This labeled text is then used to bias an
action selection policy for the game, guiding it towards promising regions of
the action space. We encode our model for text analysis and game playing in a
multi-layer neural network, representing linguistic decisions via latent
variables in the hidden layers, and game action quality via the output layer.
Operating within the Monte-Carlo Search framework, we estimate model parameters
using feedback from simulated games. We apply our approach to the complex
strategy game Civilization II using the official game manual as the text guide.
Our results show that a linguistically-informed game-playing agent
significantly outperforms its language-unaware counterpart, yielding a 34%
absolute improvement and winning over 65% of games when playing against the
built-in AI of Civilization.


Are Learned Molecular Representations Ready For Prime Time?

  Advancements in neural machinery have led to a wide range of algorithmic
solutions for molecular property prediction. Two classes of models in
particular have yielded promising results: neural networks applied to computed
molecular fingerprints or expert-crafted descriptors, and graph convolutional
neural networks that construct a learned molecular representation by operating
on the graph structure of the molecule. However, recent literature has yet to
clearly determine which of these two methods is superior when generalizing to
new chemical space. Furthermore, prior research has rarely examined these new
models in industry research settings in comparison to existing employed models.
In this paper, we benchmark models extensively on 19 public and 15 proprietary
industrial datasets spanning a wide variety of chemical endpoints. In addition,
we introduce a graph convolutional model that consistently outperforms models
using fixed molecular descriptors as well as previous graph neural
architectures on both public and proprietary datasets. Our empirical findings
indicate that while approaches based on these representations have yet to reach
the level of experimental reproducibility, our proposed model nevertheless
offers significant improvements over models currently used in industrial
workflows.


