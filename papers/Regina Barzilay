Learning to Paraphrase: An Unsupervised Approach Using Multiple-Sequence  Alignment

  We address the text-to-text generation problem of sentence-level paraphrasing-- a phenomenon distinct from and more difficult than word- or phrase-levelparaphrasing. Our approach applies multiple-sequence alignment to sentencesgathered from unannotated comparable corpora: it learns a set of paraphrasingpatterns represented by word lattice pairs and automatically determines how toapply these patterns to rewrite new sentences. The results of our evaluationexperiments show that the system derives accurate paraphrases, outperformingbaseline systems.

Catching the Drift: Probabilistic Content Models, with Applications to  Generation and Summarization

  We consider the problem of modeling the content structure of texts within aspecific domain, in terms of the topics the texts address and the order inwhich these topics appear. We first present an effective knowledge-lean methodfor learning content models from un-annotated documents, utilizing a noveladaptation of algorithms for Hidden Markov Models. We then apply our method totwo complementary tasks: information ordering and extractive summarization. Ourexperiments show that incorporating content models in these applications yieldssubstantial improvement over previously-proposed methods.

An Unsupervised Method for Uncovering Morphological Chains

  Most state-of-the-art systems today produce morphological analysis based onlyon orthographic patterns. In contrast, we propose a model for unsupervisedmorphological analysis that integrates orthographic and semantic views ofwords. We model word formation in terms of morphological chains, from basewords to the observed words, breaking the chains into parent-child relations.We use log-linear models with morpheme and word-level features to predictpossible parents, including their modifications, for each word. The limited setof candidate parents for each word render contrastive estimation feasible. Ourmodel consistently matches or outperforms five state-of-the-art systems onArabic, English and Turkish.

sk_p: a neural program corrector for MOOCs

  We present a novel technique for automatic program correction in MOOCs,capable of fixing both syntactic and semantic errors without manual, problemspecific correction strategies. Given an incorrect student program, itgenerates candidate programs from a distribution of likely corrections, andchecks each candidate for correctness against a test suite.  The key observation is that in MOOCs many programs share similar codefragments, and the seq2seq neural network model, used in the natural-languageprocessing task of machine translation, can be modified and trained to recoverthese fragments.  Experiment shows our scheme can correct 29% of all incorrect submissions andout-performs state of the art approach which requires manual, problem specificcorrection strategies.

Neural Generation of Regular Expressions from Natural Language with  Minimal Domain Knowledge

  This paper explores the task of translating natural language queries intoregular expressions which embody their meaning. In contrast to prior work, theproposed neural model does not utilize domain-specific crafting, learning totranslate directly from a parallel corpus. To fully explore the potential ofneural models, we propose a methodology for collecting a large corpus ofregular expression, natural language pairs. Our resulting model achieves aperformance gain of 19.6% over previous state-of-the-art models.

The Three Pillars of Machine Programming

  In this position paper, we describe our vision of the future of machineprogramming through a categorical examination of three pillars of research.Those pillars are: (i) intention, (ii) invention, and(iii) adaptation.Intention emphasizes advancements in the human-to-computer andcomputer-to-machine-learning interfaces. Invention emphasizes the creation orrefinement of algorithms or core hardware and software building blocks throughmachine learning (ML). Adaptation emphasizes advances in the use of ML-basedconstructs to autonomously evolve software.

Deriving Machine Attention from Human Rationales

  Attention-based models are successful when trained on large amounts of data.In this paper, we demonstrate that even in the low-resource scenario, attentioncan be learned effectively. To this end, we start with discrete human-annotatedrationales and map them into continuous attention. Our central hypothesis isthat this mapping is general across domains, and thus can be transferred fromresource-rich domains to low-resource ones. Our model jointly learns adomain-invariant representation and induces the desired mapping betweenrationales and attention. Our empirical results validate this hypothesis andshow that our approach delivers significant gains over state-of-the-artbaselines, yielding over 15% average error reduction on benchmark datasets.

Multi-Source Domain Adaptation with Mixture of Experts

  We propose a mixture-of-experts approach for unsupervised domain adaptationfrom multiple sources. The key idea is to explicitly capture the relationshipbetween a target example and different source domains. This relationship,expressed by a point-to-set metric, determines how to combine predictorstrained on various domains. The metric is learned in an unsupervised fashionusing meta-training. Experimental results on sentiment analysis andpart-of-speech tagging demonstrate that our approach consistently outperformsmultiple baselines and can robustly handle negative transfer.

Bootstrapping Lexical Choice via Multiple-Sequence Alignment

  An important component of any generation system is the mapping dictionary, alexicon of elementary semantic expressions and corresponding natural languagerealizations. Typically, labor-intensive knowledge-based methods are used toconstruct the dictionary. We instead propose to acquire it automatically via anovel multiple-pass algorithm employing multiple-sequence alignment, atechnique commonly used in bioinformatics. Crucially, our method leverageslatent information contained in multi-parallel corpora -- datasets that supplyseveral verbalizations of the corresponding semantics rather than just one.  We used our techniques to generate natural language versions ofcomputer-generated mathematical proofs, with good results on both aper-component and overall-output basis. For example, in evaluations involving adozen human judges, our system produced output whose readability andfaithfulness to the semantic input rivaled that of a traditional generationsystem.

Content Modeling Using Latent Permutations

  We present a novel Bayesian topic model for learning discourse-level documentstructure. Our model leverages insights from discourse theory to constrainlatent topic assignments in a way that reflects the underlying organization ofdocument topics. We propose a global model in which both topic selection andordering are biased to be similar across a collection of related documents. Weshow that this space of orderings can be effectively represented using adistribution over permutations called the Generalized Mallows Model. We applyour method to three complementary discourse-level tasks: cross-documentalignment, document segmentation, and information ordering. Our experimentsshow that incorporating our permutation-based model in these applicationsyields substantial improvements in performance over previously proposedmethods.

Multilingual Part-of-Speech Tagging: Two Unsupervised Approaches

  We demonstrate the effectiveness of multilingual learning for unsupervisedpart-of-speech tagging. The central assumption of our work is that by combiningcues from multiple languages, the structure of each becomes more apparent. Weconsider two ways of applying this intuition to the problem of unsupervisedpart-of-speech tagging: a model that directly merges tag structures for a pairof languages into a single sequence and a second model which insteadincorporates multilingual context using latent variables. Both approaches areformulated as hierarchical Bayesian models, using Markov Chain Monte Carlosampling techniques for inference. Our results demonstrate that byincorporating multilingual evidence we can achieve impressive performance gainsacross a range of scenarios. We also found that performance improves steadilyas the number of available languages increases.

Automatic Aggregation by Joint Modeling of Aspects and Values

  We present a model for aggregation of product review snippets by joint aspectidentification and sentiment analysis. Our model simultaneously identifies anunderlying set of ratable aspects presented in the reviews of a product (e.g.,sushi and miso for a Japanese restaurant) and determines the correspondingsentiment of each aspect. This approach directly enables discovery ofhighly-rated or inconsistent aspects of a product. Our generative model admitsan efficient variational mean-field inference algorithm. It is also easilyextensible, and we describe several modifications and their effects on modelstructure and inference. We test our model on two tasks, joint aspectidentification and sentiment analysis on a set of Yelp reviews and aspectidentification alone on a set of medical summaries. We evaluate the performanceof the model on aspect identification, sentiment analysis, and per-wordlabeling accuracy. We demonstrate that our model outperforms applicablebaselines by a considerable margin, yielding up to 32% relative error reductionon aspect identification and up to 20% relative error reduction on sentimentanalysis.

Language Understanding for Text-based Games Using Deep Reinforcement  Learning

  In this paper, we consider the task of learning control policies fortext-based games. In these games, all interactions in the virtual world arethrough text and the underlying state is not observed. The resulting languagebarrier makes such environments challenging for automatic game players. Weemploy a deep reinforcement learning framework to jointly learn staterepresentations and action policies using game rewards as feedback. Thisframework enables us to map text descriptions into vector representations thatcapture the semantics of the game states. We evaluate our approach on two gameworlds, comparing against baselines using bag-of-words and bag-of-bigrams forstate representations. Our algorithm outperforms the baselines on both worldsdemonstrating the importance of learning expressive representations.

Molding CNNs for text: non-linear, non-consecutive convolutions

  The success of deep learning often derives from well-chosen operationalbuilding blocks. In this work, we revise the temporal convolution operation inCNNs to better adapt it to text processing. Instead of concatenating wordrepresentations, we appeal to tensor algebra and use low-rank n-gram tensors todirectly exploit interactions between words already at the convolution stage.Moreover, we extend the n-gram convolution to non-consecutive words torecognize patterns with intervening words. Through a combination of low-ranktensors, and pattern weighting, we can efficiently evaluate the resultingconvolution operation via dynamic programming. We test the resultingarchitecture on standard sentiment classification and news categorizationtasks. Our model achieves state-of-the-art performance both in terms ofaccuracy and training speed. For instance, we obtain 51.2% accuracy on thefine-grained sentiment classification task.

Semi-supervised Question Retrieval with Gated Convolutions

  Question answering forums are rapidly growing in size with no effectiveautomated ability to refer to and reuse answers already available for previousposted questions. In this paper, we develop a methodology for findingsemantically related questions. The task is difficult since 1) key pieces ofinformation are often buried in extraneous details in the question body and 2)available annotations on similar questions are scarce and fragmented. We designa recurrent and convolutional model (gated convolution) to effectively mapquestions to their semantic representations. The models are pre-trained withinan encoder-decoder framework (from body to title) on the basis of the entireraw corpus, and fine-tuned discriminatively from limited annotations. Ourevaluation demonstrates that our model yields substantial gains over a standardIR baseline and various neural network architectures (including CNNs, LSTMs andGRUs).

Improving Information Extraction by Acquiring External Evidence with  Reinforcement Learning

  Most successful information extraction systems operate with access to a largecollection of documents. In this work, we explore the task of acquiring andincorporating external evidence to improve extraction accuracy in domains wherethe amount of training data is scarce. This process entails issuing searchqueries, extraction from new sources and reconciliation of extracted values,which are repeated until sufficient evidence is collected. We approach theproblem using a reinforcement learning framework where our model learns toselect optimal actions based on contextual information. We employ a deepQ-network, trained to optimize a reward function that reflects extractionaccuracy while penalizing extra effort. Our experiments on two databases -- ofshooting incidents, and food adulteration cases -- demonstrate that our systemsignificantly outperforms traditional extractors and a competitivemeta-classifier baseline.

Rationalizing Neural Predictions

  Prediction without justification has limited applicability. As a remedy, welearn to extract pieces of input text as justifications -- rationales -- thatare tailored to be short and coherent, yet sufficient for making the sameprediction. Our approach combines two modular components, generator andencoder, which are trained to operate well together. The generator specifies adistribution over text fragments as candidate rationales and these are passedthrough the encoder for prediction. Rationales are never given during training.Instead, the model is regularized by desiderata for rationales. We evaluate theapproach on multi-aspect sentiment analysis against manually annotated testcases. Our approach outperforms attention-based baseline by a significantmargin. We also successfully illustrate the method on the question retrievaltask.

Aspect-augmented Adversarial Networks for Domain Adaptation

  We introduce a neural method for transfer learning between two (source andtarget) classification tasks or aspects over the same domain. Rather thantraining on target labels, we use a few keywords pertaining to source andtarget aspects indicating sentence relevance instead of document class labels.Documents are encoded by learning to embed and softly select relevant sentencesin an aspect-dependent manner. A shared classifier is trained on the sourceencoded documents and labels, and applied to target encoded documents. Weensure transfer through aspect-adversarial training so that encoded documentsare, as sets, aspect-invariant. Experimental results demonstrate that ourapproach outperforms different baselines and model variants on two datasets,yielding an improvement of 27% on a pathology dataset and 5% on a reviewdataset.

Unsupervised Learning of Morphological Forests

  This paper focuses on unsupervised modeling of morphological families,collectively comprising a forest over the language vocabulary. This formulationenables us to capture edgewise properties reflecting single-step morphologicalderivations, along with global distributional properties of the entire forest.These global properties constrain the size of the affix set and encourageformation of tight morphological families. The resulting objective is solvedusing Integer Linear Programming (ILP) paired with contrastive estimation. Wetrain the model by alternating between optimizing the local log-linear modeland the global ILP objective. We evaluate our system on three tasks: rootdetection, clustering of morphological families and segmentation. Ourexperiments demonstrate that our model yields consistent gains in all threetasks compared with the best published results.

Deriving Neural Architectures from Sequence and Graph Kernels

  The design of neural architectures for structured objects is typically guidedby experimental insights rather than a formal process. In this work, we appealto kernels over combinatorial structures, such as sequences and graphs, toderive appropriate neural operations. We introduce a class of deep recurrentneural operations and formally characterize their associated kernel spaces. Ourrecurrent modules compare the input to virtual reference objects (cf. filtersin CNN) via the kernels. Similar to traditional neural operations, thesereference objects are parameterized and directly optimized in end-to-endtraining. We empirically evaluate the proposed class of neural architectures onstandard applications such as language modeling and molecular graph regression,achieving state-of-the-art results across these applications.

Style Transfer from Non-Parallel Text by Cross-Alignment

  This paper focuses on style transfer on the basis of non-parallel text. Thisis an instance of a broad family of problems including machine translation,decipherment, and sentiment modification. The key challenge is to separate thecontent from other aspects such as style. We assume a shared latent contentdistribution across different text corpora, and propose a method that leveragesrefined alignment of latent representations to perform style transfer. Thetransferred sentences from one style should match example sentences from theother style as a population. We demonstrate the effectiveness of thiscross-alignment method on three tasks: sentiment modification, decipherment ofword substitution ciphers, and recovery of word order.

Representation Learning for Grounded Spatial Reasoning

  The interpretation of spatial references is highly contextual, requiringjoint inference over both language and the environment. We consider the task ofspatial reasoning in a simulated environment, where an agent can act andreceive rewards. The proposed model learns a representation of the worldsteered by instruction text. This design allows for precise alignment of localneighborhoods with corresponding verbalizations, while also handling globalreferences in the instructions. We train our model with reinforcement learningusing a variant of generalized value iteration. The model outperformsstate-of-the-art approaches on several metrics, yielding a 45% reduction ingoal localization error.

Grounding Language for Transfer in Deep Reinforcement Learning

  In this paper, we explore the utilization of natural language to drivetransfer for reinforcement learning (RL). Despite the wide-spread applicationof deep RL techniques, learning generalized policy representations that workacross domains remains a challenging problem. We demonstrate that textualdescriptions of environments provide a compact intermediate channel tofacilitate effective policy transfer. Specifically, by learning to ground themeaning of text to the dynamics of the environment such as transitions andrewards, an autonomous agent can effectively bootstrap policy learning on a newdomain given its description. We employ a model-based RL approach consisting ofa differentiable planning module, a model-free component and a factorized staterepresentation to effectively use entity descriptions. Our model outperformsprior work on both transfer and multi-task scenarios in a variety of differentenvironments. For instance, we achieve up to 14% and 11.5% absolute improvementover previously existing models in terms of average and initial rewards,respectively.

Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network

  The prediction of organic reaction outcomes is a fundamental problem incomputational chemistry. Since a reaction may involve hundreds of atoms, fullyexploring the space of possible transformations is intractable. The currentsolution utilizes reaction templates to limit the space, but it suffers fromcoverage and efficiency issues. In this paper, we propose a template-freeapproach to efficiently explore the space of product molecules by firstpinpointing the reaction center -- the set of nodes and edges where graph editsoccur. Since only a small number of atoms contribute to reaction center, we candirectly enumerate candidate products. The generated candidates are scored by aWeisfeiler-Lehman Difference Network that models high-order interactionsbetween changes occurring at nodes across the molecule. Our frameworkoutperforms the top-performing template-based approach with a 10\% margin,while running orders of magnitude faster. Finally, we demonstrate that themodel accuracy rivals the performance of domain experts.

Junction Tree Variational Autoencoder for Molecular Graph Generation

  We seek to automate the design of molecules based on specific chemicalproperties. In computational terms, this task involves continuous embedding andgeneration of molecular graphs. Our primary contribution is the directrealization of molecular graphs, a task previously approached by generatinglinear SMILES strings instead of graphs. Our junction tree variationalautoencoder generates molecular graphs in two phases, by first generating atree-structured scaffold over chemical substructures, and then combining theminto a molecule with a graph message passing network. This approach allows usto incrementally expand molecules while maintaining chemical validity at everystep. We evaluate our model on multiple tasks ranging from molecular generationto optimization. Across these tasks, our model outperforms previousstate-of-the-art baselines by a significant margin.

GraphIE: A Graph-Based Framework for Information Extraction

  Most modern Information Extraction (IE) systems are implemented as sequentialtaggers and only model local dependencies. Non-local and non-sequential contextis, however, a valuable source of information to improve predictions. In thispaper, we introduce GraphIE, a framework that operates over a graphrepresenting a broad set of dependencies between textual units (i.e. words orsentences). The algorithm propagates information between connected nodesthrough graph convolutions, generating a richer representation that can beexploited to improve word-level predictions. Evaluation on three differenttasks --- namely textual, social media and visual information extraction ---shows that GraphIE consistently outperforms the state-of-the-art sequencetagging model by a significant margin.

Learning Multimodal Graph-to-Graph Translation for Molecular  Optimization

  We view molecular optimization as a graph-to-graph translation problem. Thegoal is to learn to map from one molecular graph to another with betterproperties based on an available corpus of paired molecules. Since moleculescan be optimized in different ways, there are multiple viable translations foreach input graph. A key challenge is therefore to model diverse translationoutputs. Our primary contributions include a junction tree encoder-decoder forlearning diverse graph translations along with a novel adversarial trainingmethod for aligning distributions of molecules. Diverse output distributions inour model are explicitly realized by low-dimensional latent vectors thatmodulate the translation process. We evaluate our model on multiple molecularoptimization tasks and show that our model outperforms previousstate-of-the-art baselines.

Cross-Lingual Alignment of Contextual Word Embeddings, with Applications  to Zero-shot Dependency Parsing

  We introduce a novel method for multilingual transfer that utilizes deepcontextual embeddings, pretrained in an unsupervised fashion. While contextualembeddings have been shown to yield richer representations of meaning comparedto their static counterparts, aligning them poses a challenge due to theirdynamic nature. To this end, we construct context-independent variants of theoriginal monolingual spaces and utilize their mapping to derive an alignmentfor the context-dependent spaces. This mapping readily supports processing of atarget language, improving transfer by context-aware embeddings. Ourexperimental results demonstrate the effectiveness of this approach forzero-shot and few-shot learning of dependency parsing. Specifically, our methodconsistently outperforms the previous state-of-the-art on 6 tested languages,yielding an improvement of 6.8 LAS points on average.

Inferring Which Medical Treatments Work from Reports of Clinical Trials

  How do we know if a particular medical treatment actually works? Ideally onewould consult all available evidence from relevant clinical trials.Unfortunately, such results are primarily disseminated in natural languagescientific articles, imposing substantial burden on those trying to make senseof them. In this paper, we present a new task and corpus for making thisunstructured evidence actionable. The task entails inferring reported findingsfrom a full-text article describing a randomized controlled trial (RCT) withrespect to a given intervention, comparator, and outcome of interest, e.g.,inferring if an article provides evidence supporting the use of aspirin toreduce risk of stroke, as compared to placebo.  We present a new corpus for this task comprising 10,000+ prompts coupled withfull-text articles describing RCTs. Results using a suite of models --- rangingfrom heuristic (rule-based) approaches to attentive neural architectures ---demonstrate the difficulty of the task, which we believe largely owes to thelengthy, technical input texts. To facilitate further work on this important,challenging problem we make the corpus, documentation, a website andleaderboard, and code for baselines and evaluation available athttp://evidence-inference.ebm-nlp.com/.

Learning Document-Level Semantic Properties from Free-Text Annotations

  This paper presents a new method for inferring the semantic properties ofdocuments by leveraging free-text keyphrase annotations. Such annotations arebecoming increasingly abundant due to the recent dramatic growth insemi-structured, user-generated online content. One especially relevant domainis product reviews, which are often annotated by their authors with pros/conskeyphrases such as a real bargain or good value. These annotations arerepresentative of the underlying semantic properties; however, unlike expertannotations, they are noisy: lay authors may use different labels to denote thesame property, and some labels may be missing. To learn using such noisyannotations, we find a hidden paraphrase structure which clusters thekeyphrases. The paraphrase structure is linked with a latent topic model of thereview texts, enabling the system to predict the properties of unannotateddocuments and to effectively aggregate the semantic properties of multiplereviews. Our approach is implemented as a hierarchical Bayesian model withjoint inference. We find that joint inference increases the robustness of thekeyphrase clustering and encourages the latent topics to correlate withsemantically meaningful properties. Multiple evaluations demonstrate that ourmodel substantially outperforms alternative approaches for summarizing singleand multiple documents into a set of semantically salient keyphrases.

Learning to Win by Reading Manuals in a Monte-Carlo Framework

  Domain knowledge is crucial for effective performance in autonomous controlsystems. Typically, human effort is required to encode this knowledge into acontrol algorithm. In this paper, we present an approach to language groundingwhich automatically interprets text in the context of a complex controlapplication, such as a game, and uses domain knowledge extracted from the textto improve control performance. Both text analysis and control strategies arelearned jointly using only a feedback signal inherent to the application. Toeffectively leverage textual information, our method automatically extracts thetext segment most relevant to the current game state, and labels it with atask-centric predicate structure. This labeled text is then used to bias anaction selection policy for the game, guiding it towards promising regions ofthe action space. We encode our model for text analysis and game playing in amulti-layer neural network, representing linguistic decisions via latentvariables in the hidden layers, and game action quality via the output layer.Operating within the Monte-Carlo Search framework, we estimate model parametersusing feedback from simulated games. We apply our approach to the complexstrategy game Civilization II using the official game manual as the text guide.Our results show that a linguistically-informed game-playing agentsignificantly outperforms its language-unaware counterpart, yielding a 34%absolute improvement and winning over 65% of games when playing against thebuilt-in AI of Civilization.

Are Learned Molecular Representations Ready For Prime Time?

  Advancements in neural machinery have led to a wide range of algorithmicsolutions for molecular property prediction. Two classes of models inparticular have yielded promising results: neural networks applied to computedmolecular fingerprints or expert-crafted descriptors, and graph convolutionalneural networks that construct a learned molecular representation by operatingon the graph structure of the molecule. However, recent literature has yet toclearly determine which of these two methods is superior when generalizing tonew chemical space. Furthermore, prior research has rarely examined these newmodels in industry research settings in comparison to existing employed models.In this paper, we benchmark models extensively on 19 public and 15 proprietaryindustrial datasets spanning a wide variety of chemical endpoints. In addition,we introduce a graph convolutional model that consistently outperforms modelsusing fixed molecular descriptors as well as previous graph neuralarchitectures on both public and proprietary datasets. Our empirical findingsindicate that while approaches based on these representations have yet to reachthe level of experimental reproducibility, our proposed model neverthelessoffers significant improvements over models currently used in industrialworkflows.

