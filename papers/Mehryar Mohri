Syntactic Analysis by Local Grammars Automata: an Efficient Algorithm

  Local grammars can be represented in a very convenient way by automata. This
paper describes and illustrates an efficient algorithm for the application of
local grammars put in this form to lemmatized texts.


Compact Representations by Finite-State Transducers

  Finite-state transducers give efficient representations of many Natural
Language phenomena. They allow to account for complex lexicon restrictions
encountered, without involving the use of a large set of complex rules
difficult to analyze. We here show that these representations can be made very
compact, indicate how to perform the corresponding minimization, and point out
interesting linguistic side-effects of this operation.


Perceptron Mistake Bounds

  We present a brief survey of existing mistake bounds and introduce novel
bounds for the Perceptron or the kernel Perceptron algorithm. Our novel bounds
generalize beyond standard margin-loss type bounds, allow for any convex and
Lipschitz loss function, and admit a very simple proof.


Weighted Automata in Text and Speech Processing

  Finite-state automata are a very effective tool in natural language
processing. However, in a variety of applications and especially in speech
precessing, it is necessary to consider more general machines in which arcs are
assigned weights or costs. We briefly describe some of the main theoretical and
algorithmic aspects of these machines. In particular, we describe an efficient
composition algorithm for weighted transducers, and give examples illustrating
the value of determinization and minimization algorithms for weighted automata.


Theory and Algorithms for Forecasting Time Series

  We present data-dependent learning bounds for the general scenario of
non-stationary non-mixing stochastic processes. Our learning guarantees are
expressed in terms of a data-dependent measure of sequential complexity and a
discrepancy measure that can be estimated from data under some mild
assumptions. We also also provide novel analysis of stable time series
forecasting algorithm using this new notion of discrepancy that we introduce.
We use our learning bounds to devise new algorithms for non-stationary time
series forecasting for which we report some preliminary experimental results.


Tight Lower Bound on the Probability of a Binomial Exceeding its
  Expectation

  We give the proof of a tight lower bound on the probability that a binomial
random variable exceeds its expected value. The inequality plays an important
role in a variety of contexts, including the analysis of relative deviation
bounds in learning theory and generalization bounds for unbounded loss
functions.


Algorithms for Speech Recognition and Language Processing

  Speech processing requires very efficient methods and algorithms.
Finite-state transducers have been shown recently both to constitute a very
useful abstract model and to lead to highly efficient time and space algorithms
in this field. We present these methods and algorithms and illustrate them in
the case of speech recognition. In addition to classical techniques, we
describe many new algorithms such as minimization, global and local on-the-fly
determinization of weighted automata, and efficient composition of transducers.
These methods are currently used in large vocabulary speech recognition
systems. We then show how the same formalism and algorithms can be used in
text-to-speech applications and related areas of language processing such as
morphology, syntax, and local grammars, in a very efficient way. The tutorial
is self-contained and requires no specific computational or linguistic
knowledge other than classical results.


An Efficient Compiler for Weighted Rewrite Rules

  Context-dependent rewrite rules are used in many areas of natural language
and speech processing. Work in computational phonology has demonstrated that,
given certain conditions, such rewrite rules can be represented as finite-state
transducers (FSTs). We describe a new algorithm for compiling rewrite rules
into FSTs. We show the algorithm to be simpler and more efficient than existing
algorithms. Further, many of our applications demand the ability to compile
weighted rules into weighted FSTs, transducers generalized by providing
transitions with weights. We have extended the algorithm to allow for this.


Stability Analysis and Learning Bounds for Transductive Regression
  Algorithms

  This paper uses the notion of algorithmic stability to derive novel
generalization bounds for several families of transductive regression
algorithms, both by using convexity and closed-form solutions. Our analysis
helps compare the stability of these algorithms. It also shows that a number of
widely used transductive regression algorithms are in fact unstable. Finally,
it reports the results of experiments with local transductive regression
demonstrating the benefit of our stability bounds for model selection, for one
of the algorithms, in particular for determining the radius of the local
neighborhood used by the algorithm.


Linear-Space Computation of the Edit-Distance between a String and a
  Finite Automaton

  The problem of computing the edit-distance between a string and a finite
automaton arises in a variety of applications in computational biology, text
processing, and speech recognition. This paper presents linear-space algorithms
for computing the edit-distance between a string and an arbitrary weighted
automaton over the tropical semiring, or an unambiguous weighted automaton over
an arbitrary semiring. It also gives an efficient linear-space algorithm for
finding an optimal alignment of a string and such a weighted automaton.


New Generalization Bounds for Learning Kernels

  This paper presents several novel generalization bounds for the problem of
learning kernels based on the analysis of the Rademacher complexity of the
corresponding hypothesis sets. Our bound for learning kernels with a convex
combination of p base kernels has only a log(p) dependency on the number of
kernels, p, which is considerably more favorable than the previous best bound
given for the same problem. We also give a novel bound for learning with a
linear combination of p base kernels with an L_2 regularization whose
dependency on p is only in p^{1/4}.


Relative Deviation Learning Bounds and Generalization with Unbounded
  Loss Functions

  We present an extensive analysis of relative deviation bounds, including
detailed proofs of two-sided inequalities and their implications. We also give
detailed proofs of two-sided generalization bounds that hold in the general
case of unbounded loss functions, under the assumption that a moment of the
loss is bounded. These bounds are useful in the analysis of importance
weighting and other learning tasks such as unbounded regression.


Adaptation Algorithm and Theory Based on Generalized Discrepancy

  We present a new algorithm for domain adaptation improving upon a discrepancy
minimization algorithm previously shown to outperform a number of algorithms
for this task. Unlike many previous algorithms for domain adaptation, our
algorithm does not consist of a fixed reweighting of the losses over the
training sample. We show that our algorithm benefits from a solid theoretical
foundation and more favorable learning bounds than discrepancy minimization. We
present a detailed description of our algorithm and give several efficient
solutions for solving its optimization problem. We also report the results of
several experiments showing that it outperforms discrepancy minimization.


Automata and Graph Compression

  We present a theoretical framework for the compression of automata, which are
widely used in speech processing and other natural language processing tasks.
The framework extends to graph compression. Similar to stationary ergodic
processes, we formulate a probabilistic process of graph and automata
generation that captures real world phenomena and provide a universal
compression scheme LZA for this probabilistic model. Further, we show that LZA
significantly outperforms other compression techniques such as gzip and the
UNIX compress command for several synthetic and real data sets.


Voted Kernel Regularization

  This paper presents an algorithm, Voted Kernel Regularization , that provides
the flexibility of using potentially very complex kernel functions such as
predictors based on much higher-degree polynomial kernels, while benefitting
from strong learning guarantees. The success of our algorithm arises from
derived bounds that suggest a new regularization penalty in terms of the
Rademacher complexities of the corresponding families of kernel maps. In a
series of experiments we demonstrate the improved performance of our algorithm
as compared to baselines. Furthermore, the algorithm enjoys several favorable
properties. The optimization problem is convex, it allows for learning with
non-PDS kernels, and the solutions are highly sparse, resulting in improved
classification speed and memory requirements.


Accelerating Optimization via Adaptive Prediction

  We present a powerful general framework for designing data-dependent
optimization algorithms, building upon and unifying recent techniques in
adaptive regularization, optimistic gradient predictions, and problem-dependent
randomization. We first present a series of new regret guarantees that hold at
any time and under very minimal assumptions, and then show how different
relaxations recover existing algorithms, both basic as well as more recent
sophisticated ones. Finally, we show how combining adaptivity, optimism, and
problem-dependent randomization can guide the design of algorithms that benefit
from more favorable guarantees than recent state-of-the-art methods.


AdaNet: Adaptive Structural Learning of Artificial Neural Networks

  We present new algorithms for adaptively learning artificial neural networks.
Our algorithms (AdaNet) adaptively learn both the structure of the network and
its weights. They are based on a solid theoretical analysis, including
data-dependent generalization guarantees that we prove and discuss in detail.
We report the results of large-scale experiments with one of our algorithms on
several binary classification tasks extracted from the CIFAR-10 dataset. The
results demonstrate that our algorithm can automatically learn network
structures with very competitive performance accuracies when compared with
those achieved for neural networks found by standard approaches.


Generalization Bounds for Weighted Automata

  This paper studies the problem of learning weighted automata from a finite
labeled training sample. We consider several general families of weighted
automata defined in terms of three different measures: the norm of an
automaton's weights, the norm of the function computed by an automaton, or the
norm of the corresponding Hankel matrix. We present new data-dependent
generalization guarantees for learning weighted automata expressed in terms of
the Rademacher complexity of these families. We further present upper bounds on
these Rademacher complexities, which reveal key new data-dependent terms
related to the complexity of learning weighted automata.


Multiple-Source Adaptation for Regression Problems

  We present a detailed theoretical analysis of the problem of multiple-source
adaptation in the general stochastic scenario, extending known results that
assume a single target labeling function. Our results cover a more realistic
scenario and show the existence of a single robust predictor accurate for
\emph{any} target mixture of the source distributions. Moreover, we present an
efficient and practical optimization solution to determine the robust predictor
in the important case of squared loss, by casting the problem as an instance of
DC-programming. We report the results of experiments with both an artificial
task and a sentiment analysis task. We find that our algorithm outperforms
competing approaches by producing a single robust model that performs well on
any target mixture distribution.


Hypothesis Set Stability and Generalization

  We present an extensive study of generalization for data-dependent hypothesis
sets. We give a general learning guarantee for data-dependent hypothesis sets
based on a notion of transductive Rademacher complexity. Our main results are
two generalization bounds for data-dependent hypothesis sets expressed in terms
of a notion of hypothesis set stability and a notion of Rademacher complexity
for data-dependent hypothesis sets that we introduce. These bounds admit as
special cases both standard Rademacher complexity bounds and
algorithm-dependent uniform stability bounds. We also illustrate the use of
these learning bounds in the analysis of several scenarios.


General Algorithms for Testing the Ambiguity of Finite Automata

  This paper presents efficient algorithms for testing the finite, polynomial,
and exponential ambiguity of finite automata with $\epsilon$-transitions. It
gives an algorithm for testing the exponential ambiguity of an automaton $A$ in
time $O(|A|_E^2)$, and finite or polynomial ambiguity in time $O(|A|_E^3)$.
These complexities significantly improve over the previous best complexities
given for the same problem. Furthermore, the algorithms presented are simple
and are based on a general algorithm for the composition or intersection of
automata. We also give an algorithm to determine the degree of polynomial
ambiguity of a finite automaton $A$ that is polynomially ambiguous in time
$O(|A|_E^3)$. Finally, we present an application of our algorithms to an
approximate computation of the entropy of a probabilistic automaton.


Domain Adaptation: Learning Bounds and Algorithms

  This paper addresses the general problem of domain adaptation which arises in
a variety of applications where the distribution of the labeled sample
available somewhat differs from that of the test data. Building on previous
work by Ben-David et al. (2007), we introduce a novel distance between
distributions, discrepancy distance, that is tailored to adaptation problems
with arbitrary loss functions. We give Rademacher complexity bounds for
estimating the discrepancy distance from finite samples for different loss
functions. Using this distance, we derive novel generalization bounds for
domain adaptation for a wide family of loss functions. We also present a series
of novel adaptation bounds for large classes of regularization-based
algorithms, including support vector machines and kernel ridge regression based
on the empirical discrepancy. This motivates our analysis of the problem of
minimizing the empirical discrepancy for various loss functions for which we
also give novel algorithms. We report the results of preliminary experiments
that demonstrate the benefits of our discrepancy minimization algorithms for
domain adaptation.


On the Estimation of Coherence

  Low-rank matrix approximations are often used to help scale standard machine
learning algorithms to large-scale problems. Recently, matrix coherence has
been used to characterize the ability to extract global information from a
subset of matrix entries in the context of these low-rank approximations and
other sampling-based algorithms, e.g., matrix com- pletion, robust PCA. Since
coherence is defined in terms of the singular vectors of a matrix and is
expensive to compute, the practical significance of these results largely
hinges on the following question: Can we efficiently and accurately estimate
the coherence of a matrix? In this paper we address this question. We propose a
novel algorithm for estimating coherence from a small number of columns,
formally analyze its behavior, and derive a new coherence-based matrix
approximation bound based on this analysis. We then present extensive
experimental results on synthetic and real datasets that corroborate our
worst-case theoretical analysis, yet provide strong support for the use of our
proposed algorithm whenever low-rank approximation is being considered. Our
algorithm efficiently and accurately estimates matrix coherence across a wide
range of datasets, and these coherence estimates are excellent predictors of
the effectiveness of sampling-based matrix approximation on a case-by-case
basis.


Ensembles of Kernel Predictors

  This paper examines the problem of learning with a finite and possibly large
set of p base kernels. It presents a theoretical and empirical analysis of an
approach addressing this problem based on ensembles of kernel predictors. This
includes novel theoretical guarantees based on the Rademacher complexity of the
corresponding hypothesis sets, the introduction and analysis of a learning
algorithm based on these hypothesis sets, and a series of experiments using
ensembles of kernel predictors with several data sets. Both convex combinations
of kernel-based hypotheses and more general Lq-regularized nonnegative
combinations are analyzed. These theoretical, algorithmic, and empirical
results are compared with those achieved by using learning kernel techniques,
which can be viewed as another approach for solving the same problem.


Multiple Source Adaptation and the Renyi Divergence

  This paper presents a novel theoretical study of the general problem of
multiple source adaptation using the notion of Renyi divergence. Our results
build on our previous work [12], but significantly broaden the scope of that
work in several directions. We extend previous multiple source loss guarantees
based on distribution weighted combinations to arbitrary target distributions
P, not necessarily mixtures of the source distributions, analyze both known and
unknown target distribution cases, and prove a lower bound. We further extend
our bounds to deal with the case where the learner receives an approximate
distribution for each source instead of the exact one, and show that similar
loss guarantees can be achieved depending on the divergence between the
approximate and true distributions. We also analyze the case where the labeling
functions of the source domains are somewhat different. Finally, we report the
results of experiments with both an artificial data set and a sentiment
analysis task, showing the performance benefits of the distribution weighted
combinations and the quality of our bounds based on the Renyi divergence.


New Analysis and Algorithm for Learning with Drifting Distributions

  We present a new analysis of the problem of learning with drifting
distributions in the batch setting using the notion of discrepancy. We prove
learning bounds based on the Rademacher complexity of the hypothesis set and
the discrepancy of distributions both for a drifting PAC scenario and a
tracking scenario. Our bounds are always tighter and in some cases
substantially improve upon previous ones based on the $L_1$ distance. We also
present a generalization of the standard on-line to batch conversion to the
drifting scenario in terms of the discrepancy and arbitrary convex combinations
of hypotheses. We introduce a new algorithm exploiting these learning
guarantees, which we show can be formulated as a simple QP. Finally, we report
the results of preliminary experiments demonstrating the benefits of this
algorithm.


Learning Theory and Algorithms for Revenue Optimization in Second-Price
  Auctions with Reserve

  Second-price auctions with reserve play a critical role for modern search
engine and popular online sites since the revenue of these companies often
directly de- pends on the outcome of such auctions. The choice of the reserve
price is the main mechanism through which the auction revenue can be influenced
in these electronic markets. We cast the problem of selecting the reserve price
to optimize revenue as a learning problem and present a full theoretical
analysis dealing with the complex properties of the corresponding loss
function. We further give novel algorithms for solving this problem and report
the results of several experiments in both synthetic and real data
demonstrating their effectiveness.


Sample Selection Bias Correction Theory

  This paper presents a theoretical analysis of sample selection bias
correction. The sample bias correction technique commonly used in machine
learning consists of reweighting the cost of an error on each training point of
a biased sample to more closely reflect the unbiased distribution. This relies
on weights derived by various estimation techniques based on finite samples. We
analyze the effect of an error in that estimation on the accuracy of the
hypothesis returned by the learning algorithm for two estimation techniques: a
cluster-based estimation technique and kernel mean matching. We also report the
results of sample bias correction experiments with several data sets using
these techniques. Our analysis is based on the novel concept of distributional
stability which generalizes the existing concept of point-based stability. Much
of our work and proof techniques can be used to analyze other importance
weighting techniques and their effect on accuracy when using a distributionally
stable algorithm.


On the Disambiguation of Weighted Automata

  We present a disambiguation algorithm for weighted automata. The algorithm
admits two main stages: a pre-disambiguation stage followed by a transition
removal stage. We give a detailed description of the algorithm and the proof of
its correctness. The algorithm is not applicable to all weighted automata but
we prove sufficient conditions for its applicability in the case of the
tropical semiring by introducing the *weak twins property*. In particular, the
algorithm can be used with all acyclic weighted automata, relevant to
applications. While disambiguation can sometimes be achieved using
determinization, our disambiguation algorithm in some cases can return a result
that is exponentially smaller than any equivalent deterministic automaton. We
also present some empirical evidence of the space benefits of disambiguation
over determinization in speech recognition and machine translation
applications.


Revenue Optimization in Posted-Price Auctions with Strategic Buyers

  We study revenue optimization learning algorithms for posted-price auctions
with strategic buyers. We analyze a very broad family of monotone regret
minimization algorithms for this problem, which includes the previously best
known algorithm, and show that no algorithm in that family admits a strategic
regret more favorable than $\Omega(\sqrt{T})$. We then introduce a new
algorithm that achieves a strategic regret differing from the lower bound only
by a factor in $O(\log T)$, an exponential improvement upon the previous best
algorithm. Our new algorithm admits a natural analysis and simpler proofs, and
the ideas behind its design are general. We also report the results of
empirical evaluations comparing our algorithm with the previous state of the
art and show a consistent exponential improvement in several different
scenarios.


Non-parametric Revenue Optimization for Generalized Second Price
  Auctions

  We present an extensive analysis of the key problem of learning optimal
reserve prices for generalized second price auctions. We describe two
algorithms for this task: one based on density estimation, and a novel
algorithm benefiting from solid theoretical guarantees and with a very
favorable running-time complexity of $O(n S \log (n S))$, where $n$ is the
sample size and $S$ the number of slots. Our theoretical guarantees are more
favorable than those previously presented in the literature. Additionally, we
show that even if bidders do not play at an equilibrium, our second algorithm
is still well defined and minimizes a quantity of interest. To our knowledge,
this is the first attempt to apply learning algorithms to the problem of
reserve price optimization in GSP auctions. Finally, we present the first
convergence analysis of empirical equilibrium bidding functions to the unique
symmetric Bayesian-Nash equilibrium of a GSP.


Structured Prediction Theory Based on Factor Graph Complexity

  We present a general theoretical analysis of structured prediction with a
series of new results. We give new data-dependent margin guarantees for
structured prediction for a very wide family of loss functions and a general
family of hypotheses, with an arbitrary factor graph decomposition. These are
the tightest margin bounds known for both standard multi-class and general
structured prediction problems. Our guarantees are expressed in terms of a
data-dependent complexity measure, factor graph complexity, which we show can
be estimated from data and bounded in terms of familiar quantities. We further
extend our theory by leveraging the principle of Voted Risk Minimization (VRM)
and show that learning is possible even with complex factor graphs. We present
new learning bounds for this advanced setting, which we use to design two new
algorithms, Voted Conditional Random Field (VCRF) and Voted Structured Boosting
(StructBoost). These algorithms can make use of complex features and factor
graphs and yet benefit from favorable learning guarantees. We also report the
results of experiments with VCRF on several datasets to validate our theory.


Online Learning with Abstention

  We present an extensive study of the key problem of online learning where
algorithms are allowed to abstain from making predictions. In the adversarial
setting, we show how existing online algorithms and guarantees can be adapted
to this problem. In the stochastic setting, we first point out a bias problem
that limits the straightforward extension of algorithms such as UCB-N to
time-varying feedback graphs, as needed in this context. Next, we give a new
algorithm, UCB-GT, that exploits historical data and is adapted to time-varying
feedback graphs. We show that this algorithm benefits from more favorable
regret guarantees than a possible, but limited, extension of UCB-N. We further
report the results of a series of experiments demonstrating that UCB-GT largely
outperforms that extension of UCB-N, as well as more standard baselines.


Online Learning with Automata-based Expert Sequences

  We consider a general framework of online learning with expert advice where
regret is defined with respect to sequences of experts accepted by a weighted
automaton. Our framework covers several problems previously studied, including
competing against k-shifting experts. We give a series of algorithms for this
problem, including an automata-based algorithm extending weighted-majority and
more efficient algorithms based on the notion of failure transitions. We
further present efficient algorithms based on an approximation of the
competitor automaton, in particular n-gram models obtained by minimizing the
\infty-R\'{e}nyi divergence, and present an extensive study of the
approximation properties of such models. Finally, we also extend our algorithms
and results to the framework of sleeping experts.


Discrepancy-Based Algorithms for Non-Stationary Rested Bandits

  We study the multi-armed bandit problem where the rewards are realizations of
general non-stationary stochastic processes, a setting that generalizes many
existing lines of work and analyses. In particular, we present a theoretical
analysis and derive regret guarantees for rested bandits in which the reward
distribution of each arm changes only when we pull that arm. Remarkably, our
regret bounds are logarithmic in the number of rounds under several natural
conditions. We introduce a new algorithm based on classical UCB ideas combined
with the notion of weighted discrepancy, a useful tool for measuring the
non-stationarity of a stochastic process. We show that the notion of
discrepancy can be used to design very general algorithms and a unified
framework for the analysis of multi-armed rested bandit problems with
non-stationary rewards. In particular, we show that we can recover the regret
guarantees of many specific instances of bandit problems with non-stationary
rewards that have been studied in the literature. We also provide experiments
demonstrating that our algorithms can enjoy a significant improvement in
practice compared to standard benchmarks.


Parameter-free online learning via model selection

  We introduce an efficient algorithmic framework for model selection in online
learning, also known as parameter-free online learning. Departing from previous
work, which has focused on highly structured function classes such as nested
balls in Hilbert space, we propose a generic meta-algorithm framework that
achieves online model selection oracle inequalities under minimal structural
assumptions. We give the first computationally efficient parameter-free
algorithms that work in arbitrary Banach spaces under mild smoothness
assumptions; previous results applied only to Hilbert spaces. We further derive
new oracle inequalities for matrix classes, non-nested convex sets, and
$\mathbb{R}^{d}$ with generic regularizers. Finally, we generalize these
results by providing oracle inequalities for arbitrary non-linear classes in
the online supervised learning model. These results are all derived through a
unified meta-algorithm scheme using a novel "multi-scale" algorithm for
prediction with expert advice based on random playout, which may be of
independent interest.


Online Non-Additive Path Learning under Full and Partial Information

  We study the problem of online path learning with non-additive gains, which
is a central problem appearing in several applications, including ensemble
structured prediction. We present new online algorithms for path learning with
non-additive count-based gains for the three settings of full information,
semi-bandit and full bandit with very favorable regret guarantees. A key
component of our algorithms is the definition and computation of an
intermediate context-dependent automaton that enables us to use existing
algorithms designed for additive gains. We further apply our methods to the
important application of ensemble structured prediction. Finally, beyond
count-based gains, we give an efficient implementation of the EXP3 algorithm
for the full bandit setting with an arbitrary (non-additive) gain.


Algorithms and Theory for Multiple-Source Adaptation

  This work includes a number of novel contributions for the multiple-source
adaptation problem. We present new normalized solutions with strong theoretical
guarantees for the cross-entropy loss and other similar losses. We also provide
new guarantees that hold in the case where the conditional probabilities for
the source domains are distinct. Moreover, we give new algorithms for
determining the distribution-weighted combination solution for the
cross-entropy loss and other losses. We report the results of a series of
experiments with real-world datasets. We find that our algorithm outperforms
competing approaches by producing a single robust model that performs well on
any target mixture distribution. Altogether, our theory, algorithms, and
empirical results provide a full solution for the multiple-source adaptation
problem with very practical benefits.


Agnostic Federated Learning

  A key learning scenario in large-scale applications is that of federated
learning, where a centralized model is trained based on data originating from a
large number of clients. We argue that, with the existing training and
inference, federated models can be biased towards different clients. Instead,
we propose a new framework of agnostic federated learning, where the
centralized model is optimized for any target distribution formed by a mixture
of the client distributions. We further show that this framework naturally
yields a notion of fairness. We present data-dependent Rademacher complexity
guarantees for learning with this objective, which guide the definition of an
algorithm for agnostic federated learning. We also give a fast stochastic
optimization algorithm for solving the corresponding optimization problem, for
which we prove convergence bounds, assuming a convex loss function and
hypothesis set. We further empirically demonstrate the benefits of our approach
in several datasets. Beyond federated learning, our framework and algorithm can
be of interest to other learning scenarios such as cloud computing, domain
adaptation, drifting, and other contexts where the training and test
distributions do not coincide.


An efficient reduction of ranking to classification

  This paper describes an efficient reduction of the learning problem of
ranking to binary classification. The reduction guarantees an average pairwise
misranking regret of at most that of the binary classifier regret, improving a
recent result of Balcan et al which only guarantees a factor of 2. Moreover,
our reduction applies to a broader class of ranking loss functions, admits a
simpler proof, and the expected running time complexity of our algorithm in
terms of number of calls to a classifier or preference function is improved
from $\Omega(n^2)$ to $O(n \log n)$. In addition, when the top $k$ ranked
elements only are required ($k \ll n$), as in many applications in information
extraction or search engines, the time complexity of our algorithm can be
further reduced to $O(k \log k + n)$. Our reduction and algorithm are thus
practical for realistic applications where the number of points to rank exceeds
several thousands. Much of our results also extend beyond the bipartite case
previously studied.
  Our rediction is a randomized one. To complement our result, we also derive
lower bounds on any deterministic reduction from binary (preference)
classification to ranking, implying that our use of a randomized reduction is
essentially necessary for the guarantees we provide.


3-Way Composition of Weighted Finite-State Transducers

  Composition of weighted transducers is a fundamental algorithm used in many
applications, including for computing complex edit-distances between automata,
or string kernels in machine learning, or to combine different components of a
speech recognition, speech synthesis, or information extraction system. We
present a generalization of the composition of weighted transducers, 3-way
composition, which is dramatically faster in practice than the standard
composition algorithm when combining more than two transducers. The worst-case
complexity of our algorithm for composing three transducers $T_1$, $T_2$, and
$T_3$ resulting in $T$, \ignore{depending on the strategy used, is $O(|T|_Q
d(T_1) d(T_3) + |T|_E)$ or $(|T|_Q d(T_2) + |T|_E)$,} is $O(|T|_Q \min(d(T_1)
d(T_3), d(T_2)) + |T|_E)$, where $|\cdot|_Q$ denotes the number of states,
$|\cdot|_E$ the number of transitions, and $d(\cdot)$ the maximum out-degree.
As in regular composition, the use of perfect hashing requires a pre-processing
step with linear-time expected complexity in the size of the input transducers.
In many cases, this approach significantly improves on the complexity of
standard composition. Our algorithm also leads to a dramatically faster
composition in practice. Furthermore, standard composition can be obtained as a
special case of our algorithm. We report the results of several experiments
demonstrating this improvement. These theoretical and empirical improvements
significantly enhance performance in the applications already mentioned.


Stability Bound for Stationary Phi-mixing and Beta-mixing Processes

  Most generalization bounds in learning theory are based on some measure of
the complexity of the hypothesis class used, independently of any algorithm. In
contrast, the notion of algorithmic stability can be used to derive tight
generalization bounds that are tailored to specific learning algorithms by
exploiting their particular properties. However, as in much of learning theory,
existing stability analyses and bounds apply only in the scenario where the
samples are independently and identically distributed. In many machine learning
applications, however, this assumption does not hold. The observations received
by the learning algorithm often have some inherent temporal dependence.
  This paper studies the scenario where the observations are drawn from a
stationary phi-mixing or beta-mixing sequence, a widely adopted assumption in
the study of non-i.i.d. processes that implies a dependence between
observations weakening over time. We prove novel and distinct stability-based
generalization bounds for stationary phi-mixing and beta-mixing sequences.
These bounds strictly generalize the bounds given in the i.i.d. case and apply
to all stable learning algorithms, thereby extending the use of
stability-bounds to non-i.i.d. scenarios.
  We also illustrate the application of our phi-mixing generalization bounds to
general classes of learning algorithms, including Support Vector Regression,
Kernel Ridge Regression, and Support Vector Machines, and many other kernel
regularization-based and relative entropy-based regularization algorithms.
These novel bounds can thus be viewed as the first theoretical basis for the
use of these algorithms in non-i.i.d. scenarios.


L2 Regularization for Learning Kernels

  The choice of the kernel is critical to the success of many learning
algorithms but it is typically left to the user. Instead, the training data can
be used to learn the kernel by selecting it out of a given family, such as that
of non-negative linear combinations of p base kernels, constrained by a trace
or L1 regularization. This paper studies the problem of learning kernels with
the same family of kernels but with an L2 regularization instead, and for
regression problems. We analyze the problem of learning kernels with ridge
regression. We derive the form of the solution of the optimization problem and
give an efficient iterative algorithm for computing that solution. We present a
novel theoretical analysis of the problem based on stability and give learning
bounds for orthogonal kernels that contain only an additive term O(pp/m) when
compared to the standard kernel ridge regression stability bound. We also
report the results of experiments indicating that L1 regularization can lead to
modest improvements for a small number of kernels, but to performance
degradations in larger-scale cases. In contrast, L2 regularization never
degrades performance and in fact achieves significant improvements with a large
number of kernels.


Algorithms for Learning Kernels Based on Centered Alignment

  This paper presents new and effective algorithms for learning kernels. In
particular, as shown by our empirical results, these algorithms consistently
outperform the so-called uniform combination solution that has proven to be
difficult to improve upon in the past, as well as other algorithms for learning
kernels based on convex combinations of base kernels in both classification and
regression. Our algorithms are based on the notion of centered alignment which
is used as a similarity measure between kernels or kernel matrices. We present
a number of novel algorithmic, theoretical, and empirical results for learning
kernels based on our notion of centered alignment. In particular, we describe
efficient algorithms for learning a maximum alignment kernel by showing that
the problem can be reduced to a simple QP and discuss a one-stage algorithm for
learning both a kernel and a hypothesis based on that kernel using an
alignment-based regularization. Our theoretical results include a novel
concentration bound for centered alignment between kernel matrices, the proof
of the existence of effective predictors for kernels with high alignment, both
for classification and for regression, and the proof of stability-based
generalization bounds for a broad family of algorithms for learning kernels
based on centered alignment. We also report the results of experiments with our
centered alignment-based algorithms in both classification and regression.


Foundations of Coupled Nonlinear Dimensionality Reduction

  In this paper we introduce and analyze the learning scenario of \emph{coupled
nonlinear dimensionality reduction}, which combines two major steps of machine
learning pipeline: projection onto a manifold and subsequent supervised
learning. First, we present new generalization bounds for this scenario and,
second, we introduce an algorithm that follows from these bounds. The
generalization error bound is based on a careful analysis of the empirical
Rademacher complexity of the relevant hypothesis set. In particular, we show an
upper bound on the Rademacher complexity that is in $\widetilde
O(\sqrt{\Lambda_{(r)}/m})$, where $m$ is the sample size and $\Lambda_{(r)}$
the upper bound on the Ky-Fan $r$-norm of the associated kernel matrix. We give
both upper and lower bound guarantees in terms of that Ky-Fan $r$-norm, which
strongly justifies the definition of our hypothesis set. To the best of our
knowledge, these are the first learning guarantees for the problem of coupled
dimensionality reduction. Our analysis and learning guarantees further apply to
several special cases, such as that of using a fixed kernel with supervised
dimensionality reduction or that of unsupervised learning of a kernel for
dimensionality reduction followed by a supervised learning algorithm. Based on
theoretical analysis, we suggest a structural risk minimization algorithm
consisting of the coupled fitting of a low dimensional manifold and a
separation function on that manifold.


Policy Regret in Repeated Games

  The notion of \emph{policy regret} in online learning is a well defined?
performance measure for the common scenario of adaptive adversaries, which more
traditional quantities such as external regret do not take into account. We
revisit the notion of policy regret and first show that there are online
learning settings in which policy regret and external regret are incompatible:
any sequence of play that achieves a favorable regret with respect to one
definition must do poorly with respect to the other. We then focus on the
game-theoretic setting where the adversary is a self-interested agent. In that
setting, we show that external regret and policy regret are not in conflict
and, in fact, that a wide class of algorithms can ensure a favorable regret
with respect to both definitions, so long as the adversary is also using such
an algorithm. We also show that the sequence of play of no-policy regret
algorithms converges to a \emph{policy equilibrium}, a new notion of
equilibrium that we introduce. Relating this back to external regret, we show
that coarse correlated equilibria, which no-external regret players converge
to, are a strict subset of policy equilibria. Thus, in game-theoretic settings,
every sequence of play with no external regret also admits no policy regret,
but the converse does not hold.


Logistic Regression: The Importance of Being Improper

  Learning linear predictors with the logistic loss---both in stochastic and
online settings---is a fundamental task in machine learning and statistics,
with direct connections to classification and boosting. Existing "fast rates"
for this setting exhibit exponential dependence on the predictor norm, and
Hazan et al. (2014) showed that this is unfortunately unimprovable. Starting
with the simple observation that the logistic loss is $1$-mixable, we design a
new efficient improper learning algorithm for online logistic regression that
circumvents the aforementioned lower bound with a regret bound exhibiting a
doubly-exponential improvement in dependence on the predictor norm. This
provides a positive resolution to a variant of the COLT 2012 open problem of
McMahan and Streeter (2012) when improper learning is allowed. This improvement
is obtained both in the online setting and, with some extra work, in the batch
statistical setting with high probability. We also show that the improved
dependence on predictor norm is near-optimal.
  Leveraging this improved dependency on the predictor norm yields the
following applications: (a) we give algorithms for online bandit multiclass
learning with the logistic loss with an $\tilde{O}(\sqrt{n})$ relative mistake
bound across essentially all parameter ranges, thus providing a solution to the
COLT 2009 open problem of Abernethy and Rakhlin (2009), and (b) we give an
adaptive algorithm for online multiclass boosting with optimal sample
complexity, thus partially resolving an open problem of Beygelzimer et al.
(2015) and Jung et al. (2017). Finally, we give information-theoretic bounds on
the optimal rates for improper logistic regression with general function
classes, thereby characterizing the extent to which our improvement for linear
classes extends to other parametric and even nonparametric settings.


