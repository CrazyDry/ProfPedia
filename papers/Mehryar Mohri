Syntactic Analysis by Local Grammars Automata: an Efficient Algorithm

  Local grammars can be represented in a very convenient way by automata. Thispaper describes and illustrates an efficient algorithm for the application oflocal grammars put in this form to lemmatized texts.

Compact Representations by Finite-State Transducers

  Finite-state transducers give efficient representations of many NaturalLanguage phenomena. They allow to account for complex lexicon restrictionsencountered, without involving the use of a large set of complex rulesdifficult to analyze. We here show that these representations can be made verycompact, indicate how to perform the corresponding minimization, and point outinteresting linguistic side-effects of this operation.

Perceptron Mistake Bounds

  We present a brief survey of existing mistake bounds and introduce novelbounds for the Perceptron or the kernel Perceptron algorithm. Our novel boundsgeneralize beyond standard margin-loss type bounds, allow for any convex andLipschitz loss function, and admit a very simple proof.

Weighted Automata in Text and Speech Processing

  Finite-state automata are a very effective tool in natural languageprocessing. However, in a variety of applications and especially in speechprecessing, it is necessary to consider more general machines in which arcs areassigned weights or costs. We briefly describe some of the main theoretical andalgorithmic aspects of these machines. In particular, we describe an efficientcomposition algorithm for weighted transducers, and give examples illustratingthe value of determinization and minimization algorithms for weighted automata.

Theory and Algorithms for Forecasting Time Series

  We present data-dependent learning bounds for the general scenario ofnon-stationary non-mixing stochastic processes. Our learning guarantees areexpressed in terms of a data-dependent measure of sequential complexity and adiscrepancy measure that can be estimated from data under some mildassumptions. We also also provide novel analysis of stable time seriesforecasting algorithm using this new notion of discrepancy that we introduce.We use our learning bounds to devise new algorithms for non-stationary timeseries forecasting for which we report some preliminary experimental results.

Tight Lower Bound on the Probability of a Binomial Exceeding its  Expectation

  We give the proof of a tight lower bound on the probability that a binomialrandom variable exceeds its expected value. The inequality plays an importantrole in a variety of contexts, including the analysis of relative deviationbounds in learning theory and generalization bounds for unbounded lossfunctions.

Algorithms for Speech Recognition and Language Processing

  Speech processing requires very efficient methods and algorithms.Finite-state transducers have been shown recently both to constitute a veryuseful abstract model and to lead to highly efficient time and space algorithmsin this field. We present these methods and algorithms and illustrate them inthe case of speech recognition. In addition to classical techniques, wedescribe many new algorithms such as minimization, global and local on-the-flydeterminization of weighted automata, and efficient composition of transducers.These methods are currently used in large vocabulary speech recognitionsystems. We then show how the same formalism and algorithms can be used intext-to-speech applications and related areas of language processing such asmorphology, syntax, and local grammars, in a very efficient way. The tutorialis self-contained and requires no specific computational or linguisticknowledge other than classical results.

An Efficient Compiler for Weighted Rewrite Rules

  Context-dependent rewrite rules are used in many areas of natural languageand speech processing. Work in computational phonology has demonstrated that,given certain conditions, such rewrite rules can be represented as finite-statetransducers (FSTs). We describe a new algorithm for compiling rewrite rulesinto FSTs. We show the algorithm to be simpler and more efficient than existingalgorithms. Further, many of our applications demand the ability to compileweighted rules into weighted FSTs, transducers generalized by providingtransitions with weights. We have extended the algorithm to allow for this.

Stability Analysis and Learning Bounds for Transductive Regression  Algorithms

  This paper uses the notion of algorithmic stability to derive novelgeneralization bounds for several families of transductive regressionalgorithms, both by using convexity and closed-form solutions. Our analysishelps compare the stability of these algorithms. It also shows that a number ofwidely used transductive regression algorithms are in fact unstable. Finally,it reports the results of experiments with local transductive regressiondemonstrating the benefit of our stability bounds for model selection, for oneof the algorithms, in particular for determining the radius of the localneighborhood used by the algorithm.

Linear-Space Computation of the Edit-Distance between a String and a  Finite Automaton

  The problem of computing the edit-distance between a string and a finiteautomaton arises in a variety of applications in computational biology, textprocessing, and speech recognition. This paper presents linear-space algorithmsfor computing the edit-distance between a string and an arbitrary weightedautomaton over the tropical semiring, or an unambiguous weighted automaton overan arbitrary semiring. It also gives an efficient linear-space algorithm forfinding an optimal alignment of a string and such a weighted automaton.

New Generalization Bounds for Learning Kernels

  This paper presents several novel generalization bounds for the problem oflearning kernels based on the analysis of the Rademacher complexity of thecorresponding hypothesis sets. Our bound for learning kernels with a convexcombination of p base kernels has only a log(p) dependency on the number ofkernels, p, which is considerably more favorable than the previous best boundgiven for the same problem. We also give a novel bound for learning with alinear combination of p base kernels with an L_2 regularization whosedependency on p is only in p^{1/4}.

Relative Deviation Learning Bounds and Generalization with Unbounded  Loss Functions

  We present an extensive analysis of relative deviation bounds, includingdetailed proofs of two-sided inequalities and their implications. We also givedetailed proofs of two-sided generalization bounds that hold in the generalcase of unbounded loss functions, under the assumption that a moment of theloss is bounded. These bounds are useful in the analysis of importanceweighting and other learning tasks such as unbounded regression.

Adaptation Algorithm and Theory Based on Generalized Discrepancy

  We present a new algorithm for domain adaptation improving upon a discrepancyminimization algorithm previously shown to outperform a number of algorithmsfor this task. Unlike many previous algorithms for domain adaptation, ouralgorithm does not consist of a fixed reweighting of the losses over thetraining sample. We show that our algorithm benefits from a solid theoreticalfoundation and more favorable learning bounds than discrepancy minimization. Wepresent a detailed description of our algorithm and give several efficientsolutions for solving its optimization problem. We also report the results ofseveral experiments showing that it outperforms discrepancy minimization.

Automata and Graph Compression

  We present a theoretical framework for the compression of automata, which arewidely used in speech processing and other natural language processing tasks.The framework extends to graph compression. Similar to stationary ergodicprocesses, we formulate a probabilistic process of graph and automatageneration that captures real world phenomena and provide a universalcompression scheme LZA for this probabilistic model. Further, we show that LZAsignificantly outperforms other compression techniques such as gzip and theUNIX compress command for several synthetic and real data sets.

Voted Kernel Regularization

  This paper presents an algorithm, Voted Kernel Regularization , that providesthe flexibility of using potentially very complex kernel functions such aspredictors based on much higher-degree polynomial kernels, while benefittingfrom strong learning guarantees. The success of our algorithm arises fromderived bounds that suggest a new regularization penalty in terms of theRademacher complexities of the corresponding families of kernel maps. In aseries of experiments we demonstrate the improved performance of our algorithmas compared to baselines. Furthermore, the algorithm enjoys several favorableproperties. The optimization problem is convex, it allows for learning withnon-PDS kernels, and the solutions are highly sparse, resulting in improvedclassification speed and memory requirements.

Accelerating Optimization via Adaptive Prediction

  We present a powerful general framework for designing data-dependentoptimization algorithms, building upon and unifying recent techniques inadaptive regularization, optimistic gradient predictions, and problem-dependentrandomization. We first present a series of new regret guarantees that hold atany time and under very minimal assumptions, and then show how differentrelaxations recover existing algorithms, both basic as well as more recentsophisticated ones. Finally, we show how combining adaptivity, optimism, andproblem-dependent randomization can guide the design of algorithms that benefitfrom more favorable guarantees than recent state-of-the-art methods.

AdaNet: Adaptive Structural Learning of Artificial Neural Networks

  We present new algorithms for adaptively learning artificial neural networks.Our algorithms (AdaNet) adaptively learn both the structure of the network andits weights. They are based on a solid theoretical analysis, includingdata-dependent generalization guarantees that we prove and discuss in detail.We report the results of large-scale experiments with one of our algorithms onseveral binary classification tasks extracted from the CIFAR-10 dataset. Theresults demonstrate that our algorithm can automatically learn networkstructures with very competitive performance accuracies when compared withthose achieved for neural networks found by standard approaches.

Generalization Bounds for Weighted Automata

  This paper studies the problem of learning weighted automata from a finitelabeled training sample. We consider several general families of weightedautomata defined in terms of three different measures: the norm of anautomaton's weights, the norm of the function computed by an automaton, or thenorm of the corresponding Hankel matrix. We present new data-dependentgeneralization guarantees for learning weighted automata expressed in terms ofthe Rademacher complexity of these families. We further present upper bounds onthese Rademacher complexities, which reveal key new data-dependent termsrelated to the complexity of learning weighted automata.

Multiple-Source Adaptation for Regression Problems

  We present a detailed theoretical analysis of the problem of multiple-sourceadaptation in the general stochastic scenario, extending known results thatassume a single target labeling function. Our results cover a more realisticscenario and show the existence of a single robust predictor accurate for\emph{any} target mixture of the source distributions. Moreover, we present anefficient and practical optimization solution to determine the robust predictorin the important case of squared loss, by casting the problem as an instance ofDC-programming. We report the results of experiments with both an artificialtask and a sentiment analysis task. We find that our algorithm outperformscompeting approaches by producing a single robust model that performs well onany target mixture distribution.

Hypothesis Set Stability and Generalization

  We present an extensive study of generalization for data-dependent hypothesissets. We give a general learning guarantee for data-dependent hypothesis setsbased on a notion of transductive Rademacher complexity. Our main results aretwo generalization bounds for data-dependent hypothesis sets expressed in termsof a notion of hypothesis set stability and a notion of Rademacher complexityfor data-dependent hypothesis sets that we introduce. These bounds admit asspecial cases both standard Rademacher complexity bounds andalgorithm-dependent uniform stability bounds. We also illustrate the use ofthese learning bounds in the analysis of several scenarios.

General Algorithms for Testing the Ambiguity of Finite Automata

  This paper presents efficient algorithms for testing the finite, polynomial,and exponential ambiguity of finite automata with $\epsilon$-transitions. Itgives an algorithm for testing the exponential ambiguity of an automaton $A$ intime $O(|A|_E^2)$, and finite or polynomial ambiguity in time $O(|A|_E^3)$.These complexities significantly improve over the previous best complexitiesgiven for the same problem. Furthermore, the algorithms presented are simpleand are based on a general algorithm for the composition or intersection ofautomata. We also give an algorithm to determine the degree of polynomialambiguity of a finite automaton $A$ that is polynomially ambiguous in time$O(|A|_E^3)$. Finally, we present an application of our algorithms to anapproximate computation of the entropy of a probabilistic automaton.

Sample Selection Bias Correction Theory

  This paper presents a theoretical analysis of sample selection biascorrection. The sample bias correction technique commonly used in machinelearning consists of reweighting the cost of an error on each training point ofa biased sample to more closely reflect the unbiased distribution. This relieson weights derived by various estimation techniques based on finite samples. Weanalyze the effect of an error in that estimation on the accuracy of thehypothesis returned by the learning algorithm for two estimation techniques: acluster-based estimation technique and kernel mean matching. We also report theresults of sample bias correction experiments with several data sets usingthese techniques. Our analysis is based on the novel concept of distributionalstability which generalizes the existing concept of point-based stability. Muchof our work and proof techniques can be used to analyze other importanceweighting techniques and their effect on accuracy when using a distributionallystable algorithm.

Domain Adaptation: Learning Bounds and Algorithms

  This paper addresses the general problem of domain adaptation which arises ina variety of applications where the distribution of the labeled sampleavailable somewhat differs from that of the test data. Building on previouswork by Ben-David et al. (2007), we introduce a novel distance betweendistributions, discrepancy distance, that is tailored to adaptation problemswith arbitrary loss functions. We give Rademacher complexity bounds forestimating the discrepancy distance from finite samples for different lossfunctions. Using this distance, we derive novel generalization bounds fordomain adaptation for a wide family of loss functions. We also present a seriesof novel adaptation bounds for large classes of regularization-basedalgorithms, including support vector machines and kernel ridge regression basedon the empirical discrepancy. This motivates our analysis of the problem ofminimizing the empirical discrepancy for various loss functions for which wealso give novel algorithms. We report the results of preliminary experimentsthat demonstrate the benefits of our discrepancy minimization algorithms fordomain adaptation.

On the Estimation of Coherence

  Low-rank matrix approximations are often used to help scale standard machinelearning algorithms to large-scale problems. Recently, matrix coherence hasbeen used to characterize the ability to extract global information from asubset of matrix entries in the context of these low-rank approximations andother sampling-based algorithms, e.g., matrix com- pletion, robust PCA. Sincecoherence is defined in terms of the singular vectors of a matrix and isexpensive to compute, the practical significance of these results largelyhinges on the following question: Can we efficiently and accurately estimatethe coherence of a matrix? In this paper we address this question. We propose anovel algorithm for estimating coherence from a small number of columns,formally analyze its behavior, and derive a new coherence-based matrixapproximation bound based on this analysis. We then present extensiveexperimental results on synthetic and real datasets that corroborate ourworst-case theoretical analysis, yet provide strong support for the use of ourproposed algorithm whenever low-rank approximation is being considered. Ouralgorithm efficiently and accurately estimates matrix coherence across a widerange of datasets, and these coherence estimates are excellent predictors ofthe effectiveness of sampling-based matrix approximation on a case-by-casebasis.

Ensembles of Kernel Predictors

  This paper examines the problem of learning with a finite and possibly largeset of p base kernels. It presents a theoretical and empirical analysis of anapproach addressing this problem based on ensembles of kernel predictors. Thisincludes novel theoretical guarantees based on the Rademacher complexity of thecorresponding hypothesis sets, the introduction and analysis of a learningalgorithm based on these hypothesis sets, and a series of experiments usingensembles of kernel predictors with several data sets. Both convex combinationsof kernel-based hypotheses and more general Lq-regularized nonnegativecombinations are analyzed. These theoretical, algorithmic, and empiricalresults are compared with those achieved by using learning kernel techniques,which can be viewed as another approach for solving the same problem.

Multiple Source Adaptation and the Renyi Divergence

  This paper presents a novel theoretical study of the general problem ofmultiple source adaptation using the notion of Renyi divergence. Our resultsbuild on our previous work [12], but significantly broaden the scope of thatwork in several directions. We extend previous multiple source loss guaranteesbased on distribution weighted combinations to arbitrary target distributionsP, not necessarily mixtures of the source distributions, analyze both known andunknown target distribution cases, and prove a lower bound. We further extendour bounds to deal with the case where the learner receives an approximatedistribution for each source instead of the exact one, and show that similarloss guarantees can be achieved depending on the divergence between theapproximate and true distributions. We also analyze the case where the labelingfunctions of the source domains are somewhat different. Finally, we report theresults of experiments with both an artificial data set and a sentimentanalysis task, showing the performance benefits of the distribution weightedcombinations and the quality of our bounds based on the Renyi divergence.

New Analysis and Algorithm for Learning with Drifting Distributions

  We present a new analysis of the problem of learning with driftingdistributions in the batch setting using the notion of discrepancy. We provelearning bounds based on the Rademacher complexity of the hypothesis set andthe discrepancy of distributions both for a drifting PAC scenario and atracking scenario. Our bounds are always tighter and in some casessubstantially improve upon previous ones based on the $L_1$ distance. We alsopresent a generalization of the standard on-line to batch conversion to thedrifting scenario in terms of the discrepancy and arbitrary convex combinationsof hypotheses. We introduce a new algorithm exploiting these learningguarantees, which we show can be formulated as a simple QP. Finally, we reportthe results of preliminary experiments demonstrating the benefits of thisalgorithm.

Learning Theory and Algorithms for Revenue Optimization in Second-Price  Auctions with Reserve

  Second-price auctions with reserve play a critical role for modern searchengine and popular online sites since the revenue of these companies oftendirectly de- pends on the outcome of such auctions. The choice of the reserveprice is the main mechanism through which the auction revenue can be influencedin these electronic markets. We cast the problem of selecting the reserve priceto optimize revenue as a learning problem and present a full theoreticalanalysis dealing with the complex properties of the corresponding lossfunction. We further give novel algorithms for solving this problem and reportthe results of several experiments in both synthetic and real datademonstrating their effectiveness.

On the Disambiguation of Weighted Automata

  We present a disambiguation algorithm for weighted automata. The algorithmadmits two main stages: a pre-disambiguation stage followed by a transitionremoval stage. We give a detailed description of the algorithm and the proof ofits correctness. The algorithm is not applicable to all weighted automata butwe prove sufficient conditions for its applicability in the case of thetropical semiring by introducing the *weak twins property*. In particular, thealgorithm can be used with all acyclic weighted automata, relevant toapplications. While disambiguation can sometimes be achieved usingdeterminization, our disambiguation algorithm in some cases can return a resultthat is exponentially smaller than any equivalent deterministic automaton. Wealso present some empirical evidence of the space benefits of disambiguationover determinization in speech recognition and machine translationapplications.

Revenue Optimization in Posted-Price Auctions with Strategic Buyers

  We study revenue optimization learning algorithms for posted-price auctionswith strategic buyers. We analyze a very broad family of monotone regretminimization algorithms for this problem, which includes the previously bestknown algorithm, and show that no algorithm in that family admits a strategicregret more favorable than $\Omega(\sqrt{T})$. We then introduce a newalgorithm that achieves a strategic regret differing from the lower bound onlyby a factor in $O(\log T)$, an exponential improvement upon the previous bestalgorithm. Our new algorithm admits a natural analysis and simpler proofs, andthe ideas behind its design are general. We also report the results ofempirical evaluations comparing our algorithm with the previous state of theart and show a consistent exponential improvement in several differentscenarios.

Non-parametric Revenue Optimization for Generalized Second Price  Auctions

  We present an extensive analysis of the key problem of learning optimalreserve prices for generalized second price auctions. We describe twoalgorithms for this task: one based on density estimation, and a novelalgorithm benefiting from solid theoretical guarantees and with a veryfavorable running-time complexity of $O(n S \log (n S))$, where $n$ is thesample size and $S$ the number of slots. Our theoretical guarantees are morefavorable than those previously presented in the literature. Additionally, weshow that even if bidders do not play at an equilibrium, our second algorithmis still well defined and minimizes a quantity of interest. To our knowledge,this is the first attempt to apply learning algorithms to the problem ofreserve price optimization in GSP auctions. Finally, we present the firstconvergence analysis of empirical equilibrium bidding functions to the uniquesymmetric Bayesian-Nash equilibrium of a GSP.

Structured Prediction Theory Based on Factor Graph Complexity

  We present a general theoretical analysis of structured prediction with aseries of new results. We give new data-dependent margin guarantees forstructured prediction for a very wide family of loss functions and a generalfamily of hypotheses, with an arbitrary factor graph decomposition. These arethe tightest margin bounds known for both standard multi-class and generalstructured prediction problems. Our guarantees are expressed in terms of adata-dependent complexity measure, factor graph complexity, which we show canbe estimated from data and bounded in terms of familiar quantities. We furtherextend our theory by leveraging the principle of Voted Risk Minimization (VRM)and show that learning is possible even with complex factor graphs. We presentnew learning bounds for this advanced setting, which we use to design two newalgorithms, Voted Conditional Random Field (VCRF) and Voted Structured Boosting(StructBoost). These algorithms can make use of complex features and factorgraphs and yet benefit from favorable learning guarantees. We also report theresults of experiments with VCRF on several datasets to validate our theory.

Online Learning with Abstention

  We present an extensive study of the key problem of online learning wherealgorithms are allowed to abstain from making predictions. In the adversarialsetting, we show how existing online algorithms and guarantees can be adaptedto this problem. In the stochastic setting, we first point out a bias problemthat limits the straightforward extension of algorithms such as UCB-N totime-varying feedback graphs, as needed in this context. Next, we give a newalgorithm, UCB-GT, that exploits historical data and is adapted to time-varyingfeedback graphs. We show that this algorithm benefits from more favorableregret guarantees than a possible, but limited, extension of UCB-N. We furtherreport the results of a series of experiments demonstrating that UCB-GT largelyoutperforms that extension of UCB-N, as well as more standard baselines.

Online Learning with Automata-based Expert Sequences

  We consider a general framework of online learning with expert advice whereregret is defined with respect to sequences of experts accepted by a weightedautomaton. Our framework covers several problems previously studied, includingcompeting against k-shifting experts. We give a series of algorithms for thisproblem, including an automata-based algorithm extending weighted-majority andmore efficient algorithms based on the notion of failure transitions. Wefurther present efficient algorithms based on an approximation of thecompetitor automaton, in particular n-gram models obtained by minimizing the\infty-R\'{e}nyi divergence, and present an extensive study of theapproximation properties of such models. Finally, we also extend our algorithmsand results to the framework of sleeping experts.

Discrepancy-Based Algorithms for Non-Stationary Rested Bandits

  We study the multi-armed bandit problem where the rewards are realizations ofgeneral non-stationary stochastic processes, a setting that generalizes manyexisting lines of work and analyses. In particular, we present a theoreticalanalysis and derive regret guarantees for rested bandits in which the rewarddistribution of each arm changes only when we pull that arm. Remarkably, ourregret bounds are logarithmic in the number of rounds under several naturalconditions. We introduce a new algorithm based on classical UCB ideas combinedwith the notion of weighted discrepancy, a useful tool for measuring thenon-stationarity of a stochastic process. We show that the notion ofdiscrepancy can be used to design very general algorithms and a unifiedframework for the analysis of multi-armed rested bandit problems withnon-stationary rewards. In particular, we show that we can recover the regretguarantees of many specific instances of bandit problems with non-stationaryrewards that have been studied in the literature. We also provide experimentsdemonstrating that our algorithms can enjoy a significant improvement inpractice compared to standard benchmarks.

Parameter-free online learning via model selection

  We introduce an efficient algorithmic framework for model selection in onlinelearning, also known as parameter-free online learning. Departing from previouswork, which has focused on highly structured function classes such as nestedballs in Hilbert space, we propose a generic meta-algorithm framework thatachieves online model selection oracle inequalities under minimal structuralassumptions. We give the first computationally efficient parameter-freealgorithms that work in arbitrary Banach spaces under mild smoothnessassumptions; previous results applied only to Hilbert spaces. We further derivenew oracle inequalities for matrix classes, non-nested convex sets, and$\mathbb{R}^{d}$ with generic regularizers. Finally, we generalize theseresults by providing oracle inequalities for arbitrary non-linear classes inthe online supervised learning model. These results are all derived through aunified meta-algorithm scheme using a novel "multi-scale" algorithm forprediction with expert advice based on random playout, which may be ofindependent interest.

Online Non-Additive Path Learning under Full and Partial Information

  We study the problem of online path learning with non-additive gains, whichis a central problem appearing in several applications, including ensemblestructured prediction. We present new online algorithms for path learning withnon-additive count-based gains for the three settings of full information,semi-bandit and full bandit with very favorable regret guarantees. A keycomponent of our algorithms is the definition and computation of anintermediate context-dependent automaton that enables us to use existingalgorithms designed for additive gains. We further apply our methods to theimportant application of ensemble structured prediction. Finally, beyondcount-based gains, we give an efficient implementation of the EXP3 algorithmfor the full bandit setting with an arbitrary (non-additive) gain.

Algorithms and Theory for Multiple-Source Adaptation

  This work includes a number of novel contributions for the multiple-sourceadaptation problem. We present new normalized solutions with strong theoreticalguarantees for the cross-entropy loss and other similar losses. We also providenew guarantees that hold in the case where the conditional probabilities forthe source domains are distinct. Moreover, we give new algorithms fordetermining the distribution-weighted combination solution for thecross-entropy loss and other losses. We report the results of a series ofexperiments with real-world datasets. We find that our algorithm outperformscompeting approaches by producing a single robust model that performs well onany target mixture distribution. Altogether, our theory, algorithms, andempirical results provide a full solution for the multiple-source adaptationproblem with very practical benefits.

Agnostic Federated Learning

  A key learning scenario in large-scale applications is that of federatedlearning, where a centralized model is trained based on data originating from alarge number of clients. We argue that, with the existing training andinference, federated models can be biased towards different clients. Instead,we propose a new framework of agnostic federated learning, where thecentralized model is optimized for any target distribution formed by a mixtureof the client distributions. We further show that this framework naturallyyields a notion of fairness. We present data-dependent Rademacher complexityguarantees for learning with this objective, which guide the definition of analgorithm for agnostic federated learning. We also give a fast stochasticoptimization algorithm for solving the corresponding optimization problem, forwhich we prove convergence bounds, assuming a convex loss function andhypothesis set. We further empirically demonstrate the benefits of our approachin several datasets. Beyond federated learning, our framework and algorithm canbe of interest to other learning scenarios such as cloud computing, domainadaptation, drifting, and other contexts where the training and testdistributions do not coincide.

An efficient reduction of ranking to classification

  This paper describes an efficient reduction of the learning problem ofranking to binary classification. The reduction guarantees an average pairwisemisranking regret of at most that of the binary classifier regret, improving arecent result of Balcan et al which only guarantees a factor of 2. Moreover,our reduction applies to a broader class of ranking loss functions, admits asimpler proof, and the expected running time complexity of our algorithm interms of number of calls to a classifier or preference function is improvedfrom $\Omega(n^2)$ to $O(n \log n)$. In addition, when the top $k$ rankedelements only are required ($k \ll n$), as in many applications in informationextraction or search engines, the time complexity of our algorithm can befurther reduced to $O(k \log k + n)$. Our reduction and algorithm are thuspractical for realistic applications where the number of points to rank exceedsseveral thousands. Much of our results also extend beyond the bipartite casepreviously studied.  Our rediction is a randomized one. To complement our result, we also derivelower bounds on any deterministic reduction from binary (preference)classification to ranking, implying that our use of a randomized reduction isessentially necessary for the guarantees we provide.

3-Way Composition of Weighted Finite-State Transducers

  Composition of weighted transducers is a fundamental algorithm used in manyapplications, including for computing complex edit-distances between automata,or string kernels in machine learning, or to combine different components of aspeech recognition, speech synthesis, or information extraction system. Wepresent a generalization of the composition of weighted transducers, 3-waycomposition, which is dramatically faster in practice than the standardcomposition algorithm when combining more than two transducers. The worst-casecomplexity of our algorithm for composing three transducers $T_1$, $T_2$, and$T_3$ resulting in $T$, \ignore{depending on the strategy used, is $O(|T|_Qd(T_1) d(T_3) + |T|_E)$ or $(|T|_Q d(T_2) + |T|_E)$,} is $O(|T|_Q \min(d(T_1)d(T_3), d(T_2)) + |T|_E)$, where $|\cdot|_Q$ denotes the number of states,$|\cdot|_E$ the number of transitions, and $d(\cdot)$ the maximum out-degree.As in regular composition, the use of perfect hashing requires a pre-processingstep with linear-time expected complexity in the size of the input transducers.In many cases, this approach significantly improves on the complexity ofstandard composition. Our algorithm also leads to a dramatically fastercomposition in practice. Furthermore, standard composition can be obtained as aspecial case of our algorithm. We report the results of several experimentsdemonstrating this improvement. These theoretical and empirical improvementssignificantly enhance performance in the applications already mentioned.

Stability Bound for Stationary Phi-mixing and Beta-mixing Processes

  Most generalization bounds in learning theory are based on some measure ofthe complexity of the hypothesis class used, independently of any algorithm. Incontrast, the notion of algorithmic stability can be used to derive tightgeneralization bounds that are tailored to specific learning algorithms byexploiting their particular properties. However, as in much of learning theory,existing stability analyses and bounds apply only in the scenario where thesamples are independently and identically distributed. In many machine learningapplications, however, this assumption does not hold. The observations receivedby the learning algorithm often have some inherent temporal dependence.  This paper studies the scenario where the observations are drawn from astationary phi-mixing or beta-mixing sequence, a widely adopted assumption inthe study of non-i.i.d. processes that implies a dependence betweenobservations weakening over time. We prove novel and distinct stability-basedgeneralization bounds for stationary phi-mixing and beta-mixing sequences.These bounds strictly generalize the bounds given in the i.i.d. case and applyto all stable learning algorithms, thereby extending the use ofstability-bounds to non-i.i.d. scenarios.  We also illustrate the application of our phi-mixing generalization bounds togeneral classes of learning algorithms, including Support Vector Regression,Kernel Ridge Regression, and Support Vector Machines, and many other kernelregularization-based and relative entropy-based regularization algorithms.These novel bounds can thus be viewed as the first theoretical basis for theuse of these algorithms in non-i.i.d. scenarios.

Algorithms for Learning Kernels Based on Centered Alignment

  This paper presents new and effective algorithms for learning kernels. Inparticular, as shown by our empirical results, these algorithms consistentlyoutperform the so-called uniform combination solution that has proven to bedifficult to improve upon in the past, as well as other algorithms for learningkernels based on convex combinations of base kernels in both classification andregression. Our algorithms are based on the notion of centered alignment whichis used as a similarity measure between kernels or kernel matrices. We presenta number of novel algorithmic, theoretical, and empirical results for learningkernels based on our notion of centered alignment. In particular, we describeefficient algorithms for learning a maximum alignment kernel by showing thatthe problem can be reduced to a simple QP and discuss a one-stage algorithm forlearning both a kernel and a hypothesis based on that kernel using analignment-based regularization. Our theoretical results include a novelconcentration bound for centered alignment between kernel matrices, the proofof the existence of effective predictors for kernels with high alignment, bothfor classification and for regression, and the proof of stability-basedgeneralization bounds for a broad family of algorithms for learning kernelsbased on centered alignment. We also report the results of experiments with ourcentered alignment-based algorithms in both classification and regression.

L2 Regularization for Learning Kernels

  The choice of the kernel is critical to the success of many learningalgorithms but it is typically left to the user. Instead, the training data canbe used to learn the kernel by selecting it out of a given family, such as thatof non-negative linear combinations of p base kernels, constrained by a traceor L1 regularization. This paper studies the problem of learning kernels withthe same family of kernels but with an L2 regularization instead, and forregression problems. We analyze the problem of learning kernels with ridgeregression. We derive the form of the solution of the optimization problem andgive an efficient iterative algorithm for computing that solution. We present anovel theoretical analysis of the problem based on stability and give learningbounds for orthogonal kernels that contain only an additive term O(pp/m) whencompared to the standard kernel ridge regression stability bound. We alsoreport the results of experiments indicating that L1 regularization can lead tomodest improvements for a small number of kernels, but to performancedegradations in larger-scale cases. In contrast, L2 regularization neverdegrades performance and in fact achieves significant improvements with a largenumber of kernels.

Foundations of Coupled Nonlinear Dimensionality Reduction

  In this paper we introduce and analyze the learning scenario of \emph{couplednonlinear dimensionality reduction}, which combines two major steps of machinelearning pipeline: projection onto a manifold and subsequent supervisedlearning. First, we present new generalization bounds for this scenario and,second, we introduce an algorithm that follows from these bounds. Thegeneralization error bound is based on a careful analysis of the empiricalRademacher complexity of the relevant hypothesis set. In particular, we show anupper bound on the Rademacher complexity that is in $\widetildeO(\sqrt{\Lambda_{(r)}/m})$, where $m$ is the sample size and $\Lambda_{(r)}$the upper bound on the Ky-Fan $r$-norm of the associated kernel matrix. We giveboth upper and lower bound guarantees in terms of that Ky-Fan $r$-norm, whichstrongly justifies the definition of our hypothesis set. To the best of ourknowledge, these are the first learning guarantees for the problem of coupleddimensionality reduction. Our analysis and learning guarantees further apply toseveral special cases, such as that of using a fixed kernel with superviseddimensionality reduction or that of unsupervised learning of a kernel fordimensionality reduction followed by a supervised learning algorithm. Based ontheoretical analysis, we suggest a structural risk minimization algorithmconsisting of the coupled fitting of a low dimensional manifold and aseparation function on that manifold.

Policy Regret in Repeated Games

  The notion of \emph{policy regret} in online learning is a well defined?performance measure for the common scenario of adaptive adversaries, which moretraditional quantities such as external regret do not take into account. Werevisit the notion of policy regret and first show that there are onlinelearning settings in which policy regret and external regret are incompatible:any sequence of play that achieves a favorable regret with respect to onedefinition must do poorly with respect to the other. We then focus on thegame-theoretic setting where the adversary is a self-interested agent. In thatsetting, we show that external regret and policy regret are not in conflictand, in fact, that a wide class of algorithms can ensure a favorable regretwith respect to both definitions, so long as the adversary is also using suchan algorithm. We also show that the sequence of play of no-policy regretalgorithms converges to a \emph{policy equilibrium}, a new notion ofequilibrium that we introduce. Relating this back to external regret, we showthat coarse correlated equilibria, which no-external regret players convergeto, are a strict subset of policy equilibria. Thus, in game-theoretic settings,every sequence of play with no external regret also admits no policy regret,but the converse does not hold.

Logistic Regression: The Importance of Being Improper

  Learning linear predictors with the logistic loss---both in stochastic andonline settings---is a fundamental task in machine learning and statistics,with direct connections to classification and boosting. Existing "fast rates"for this setting exhibit exponential dependence on the predictor norm, andHazan et al. (2014) showed that this is unfortunately unimprovable. Startingwith the simple observation that the logistic loss is $1$-mixable, we design anew efficient improper learning algorithm for online logistic regression thatcircumvents the aforementioned lower bound with a regret bound exhibiting adoubly-exponential improvement in dependence on the predictor norm. Thisprovides a positive resolution to a variant of the COLT 2012 open problem ofMcMahan and Streeter (2012) when improper learning is allowed. This improvementis obtained both in the online setting and, with some extra work, in the batchstatistical setting with high probability. We also show that the improveddependence on predictor norm is near-optimal.  Leveraging this improved dependency on the predictor norm yields thefollowing applications: (a) we give algorithms for online bandit multiclasslearning with the logistic loss with an $\tilde{O}(\sqrt{n})$ relative mistakebound across essentially all parameter ranges, thus providing a solution to theCOLT 2009 open problem of Abernethy and Rakhlin (2009), and (b) we give anadaptive algorithm for online multiclass boosting with optimal samplecomplexity, thus partially resolving an open problem of Beygelzimer et al.(2015) and Jung et al. (2017). Finally, we give information-theoretic bounds onthe optimal rates for improper logistic regression with general functionclasses, thereby characterizing the extent to which our improvement for linearclasses extends to other parametric and even nonparametric settings.

