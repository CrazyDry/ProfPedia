English Out-of-Vocabulary Lexical Evaluation Task

  Unlike previous unknown nouns tagging task (Curran, 2005) (Ciaramita andJohnson, 2003), this is the first attempt to focus on out-of-vocabulary(OOV)lexical evaluation tasks that does not require any prior knowledge. The OOVwords are words that only appear in test samples. The goal of tasks is toprovide solutions for OOV lexical classification and predication. The tasksrequire annotators to conclude the attributes of the OOV words based on theirrelated contexts. Then, we utilize unsupervised word embedding methods such asWord2Vec(Mikolov et al., 2013) and Word2GM (Athiwaratkun and Wilson, 2017) toperform the baseline experiments on the categorical classification task and OOVwords attribute prediction tasks.

Comparing Sample-wise Learnability Across Deep Neural Network Models

  Estimating the relative importance of each sample in a training set hasimportant practical and theoretical value, such as in importance sampling orcurriculum learning. This kind of focus on individual samples invokes theconcept of sample-wise learnability: How easy is it to correctly learn eachsample (cf. PAC learnability)? In this paper, we approach the sample-wiselearnability problem within a deep learning context. We propose a measure ofthe learnability of a sample with a given deep neural network (DNN) model. Thebasic idea is to train the given model on the training set, and for eachsample, aggregate the hits and misses over the entire training epochs. Ourexperiments show that the sample-wise learnability measure collected this wayis highly linearly correlated across different DNN models (ResNet-20, VGG-16,and MobileNet), suggesting that such a measure can provide deep generalinsights on the data's properties. We expect our method to help develop bettercurricula for training, and help us better understand the data itself.

How Compact?: Assessing Compactness of Representations through  Layer-Wise Pruning

  Various forms of representations may arise in the many layers embedded indeep neural networks (DNNs). Of these, where can we find the most compactrepresentation? We propose to use a pruning framework to answer this question:How compact can each layer be compressed, without losing performance? Most ofthe existing DNN compression methods do not consider the relativecompressibility of the individual layers. They uniformly apply a single targetsparsity to all layers or adapt layer sparsity using heuristics and additionaltraining. We propose a principled method that automatically determines thesparsity of individual layers derived from the importance of each layer. To dothis, we consider a metric to measure the importance of each layer based on thelayer-wise capacity. Given the trained model and the total target sparsity, wefirst evaluate the importance of each layer from the model. From the evaluatedimportance, we compute the layer-wise sparsity of each layer. The proposedmethod can be applied to any DNN architecture and can be combined with anypruning method that takes the total target sparsity as a parameter. To validatethe proposed method, we carried out an image classification task with two typesof DNN architectures on two benchmark datasets and used three pruning methodsfor compression. In case of VGG-16 model with weight pruning on the ImageNetdataset, we achieved up to 75% (17.5% on average) better top-5 accuracy thanthe baseline under the same total target sparsity. Furthermore, we analyzedwhere the maximum compression can occur in the network. This kind of analysiscan help us identify the most compact representation within a deep neuralnetwork.

