High Dimensional Multivariate Regression and Precision Matrix Estimation  via Nonconvex Optimization

  We propose a nonconvex estimator for joint multivariate regression andprecision matrix estimation in the high dimensional regime, under sparsityconstraints. A gradient descent algorithm with hard thresholding is developedto solve the nonconvex estimator, and it attains a linear rate of convergenceto the true regression coefficients and precision matrix simultaneously, up tothe statistical error. Compared with existing methods along this line ofresearch, which have little theoretical guarantee, the proposed algorithm notonly is computationally much more efficient with provable convergenceguarantee, but also attains the optimal finite sample statistical rate up to alogarithmic factor. Thorough experiments on both synthetic and real datasetsback up our theory.

Stochastic Variance-Reduced Cubic Regularized Newton Method

  We propose a stochastic variance-reduced cubic regularized Newton method fornon-convex optimization. At the core of our algorithm is a novelsemi-stochastic gradient along with a semi-stochastic Hessian, which arespecifically designed for cubic regularization method. We show that ouralgorithm is guaranteed to converge to an$(\epsilon,\sqrt{\epsilon})$-approximately local minimum within$\tilde{O}(n^{4/5}/\epsilon^{3/2})$ second-order oracle calls, whichoutperforms the state-of-the-art cubic regularization algorithms includingsubsampled cubic regularization. Our work also sheds light on the applicationof variance reduction technique to high-order non-convex optimization methods.Thorough experiments on various non-convex optimization problems support ourtheory.

On the Convergence of Adaptive Gradient Methods for Nonconvex  Optimization

  Adaptive gradient methods are workhorses in deep learning. However, theconvergence guarantees of adaptive gradient methods for nonconvex optimizationhave not been sufficiently studied. In this paper, we provide a sharp analysisof a recently proposed adaptive gradient method namely partially adaptivemomentum estimation method (Padam) (Chen and Gu, 2018), which admits manyexisting adaptive gradient methods such as RMSProp and AMSGrad as specialcases. Our analysis shows that, for smooth nonconvex functions, Padam convergesto a first-order stationary point at the rate of$O\big((\sum_{i=1}^d\|\mathbf{g}_{1:T,i}\|_2)^{1/2}/T^{3/4} + d/T\big)$, where$T$ is the number of iterations, $d$ is the dimension,$\mathbf{g}_1,\ldots,\mathbf{g}_T$ are the stochastic gradients, and$\mathbf{g}_{1:T,i} = [g_{1,i},g_{2,i},\ldots,g_{T,i}]^\top$. Our theoreticalresult also suggests that in order to achieve faster convergence rate, it isnecessary to use Padam instead of AMSGrad. This is well-aligned with theempirical results of deep learning reported in Chen and Gu (2018).

Generalized Fisher Score for Feature Selection

  Fisher score is one of the most widely used supervised feature selectionmethods. However, it selects each feature independently according to theirscores under the Fisher criterion, which leads to a suboptimal subset offeatures. In this paper, we present a generalized Fisher score to jointlyselect features. It aims at finding an subset of features, which maximize thelower bound of traditional Fisher score. The resulting feature selectionproblem is a mixed integer programming, which can be reformulated as aquadratically constrained linear programming (QCLP). It is solved by cuttingplane algorithm, in each iteration of which a multiple kernel learning problemis solved alternatively by multivariate ridge regression and projected gradientdescent. Experiments on benchmark data sets indicate that the proposed methodoutperforms Fisher score as well as many other state-of-the-art featureselection methods.

Local and Global Inference for High Dimensional Nonparanormal Graphical  Models

  This paper proposes a unified framework to quantify local and globalinferential uncertainty for high dimensional nonparanormal graphical models. Inparticular, we consider the problems of testing the presence of a single edgeand constructing a uniform confidence subgraph. Due to the presence of unknownmarginal transformations, we propose a pseudo likelihood based inferentialapproach. In sharp contrast to the existing high dimensional score test method,our method is free of tuning parameters given an initial estimator, and extendsthe scope of the existing likelihood based inferential framework. Furthermore,we propose a U-statistic multiplier bootstrap method to construct theconfidence subgraph. We show that the constructed subgraph is contained in thetrue graph with probability greater than a given nominal level. Compared withexisting methods for constructing confidence subgraphs, our method does notrely on Gaussian or sub-Gaussian assumptions. The theoretical properties of theproposed inferential methods are verified by thorough numerical experiments andreal data analysis.

Towards Faster Rates and Oracle Property for Low-Rank Matrix Estimation

  We present a unified framework for low-rank matrix estimation with nonconvexpenalties. We first prove that the proposed estimator attains a fasterstatistical rate than the traditional low-rank matrix estimator with nuclearnorm penalty. Moreover, we rigorously show that under a certain condition onthe magnitude of the nonzero singular values, the proposed estimator enjoysoracle property (i.e., exactly recovers the true rank of the matrix), besidesattaining a faster rate. As far as we know, this is the first work thatestablishes the theory of low-rank matrix estimation with nonconvex penalties,confirming the advantages of nonconvex penalties for matrix completion.Numerical experiments on both synthetic and real world datasets corroborate ourtheory.

Communication-efficient Distributed Sparse Linear Discriminant Analysis

  We propose a communication-efficient distributed estimation method for sparselinear discriminant analysis (LDA) in the high dimensional regime. Our methoddistributes the data of size $N$ into $m$ machines, and estimates a localsparse LDA estimator on each machine using the data subset of size $N/m$. Afterthe distributed estimation, our method aggregates the debiased local estimatorsfrom $m$ machines, and sparsifies the aggregated estimator. We show that theaggregated estimator attains the same statistical rate as the centralizedestimation method, as long as the number of machines $m$ is chosenappropriately. Moreover, we prove that our method can attain the modelselection consistency under a milder condition than the centralized method.Experiments on both synthetic and real datasets corroborate our theory.

A Unified Computational and Statistical Framework for Nonconvex Low-Rank  Matrix Estimation

  We propose a unified framework for estimating low-rank matrices throughnonconvex optimization based on gradient descent algorithm. Our framework isquite general and can be applied to both noisy and noiseless observations. Inthe general case with noisy observations, we show that our algorithm isguaranteed to linearly converge to the unknown low-rank matrix up to minimaxoptimal statistical error, provided an appropriate initial estimator. While inthe generic noiseless setting, our algorithm converges to the unknown low-rankmatrix at a linear rate and enables exact recovery with optimal samplecomplexity. In addition, we develop a new initialization algorithm to provide adesired initial estimator, which outperforms existing initialization algorithmsfor nonconvex low-rank matrix estimation. We illustrate the superiority of ourframework through three examples: matrix regression, matrix completion, andone-bit matrix completion. We also corroborate our theory through extensiveexperiments on synthetic data.

Stochastic Variance-reduced Gradient Descent for Low-rank Matrix  Recovery from Linear Measurements

  We study the problem of estimating low-rank matrices from linear measurements(a.k.a., matrix sensing) through nonconvex optimization. We propose anefficient stochastic variance reduced gradient descent algorithm to solve anonconvex optimization problem of matrix sensing. Our algorithm is applicableto both noisy and noiseless settings. In the case with noisy observations, weprove that our algorithm converges to the unknown low-rank matrix at a linearrate up to the minimax optimal statistical error. And in the noiseless setting,our algorithm is guaranteed to linearly converge to the unknown low-rank matrixand achieves exact recovery with optimal sample complexity. Most notably, theoverall computational complexity of our proposed algorithm, which is defined asthe iteration complexity times per iteration time complexity, is lower than thestate-of-the-art algorithms based on gradient descent. Experiments on syntheticdata corroborate the superiority of the proposed algorithm over thestate-of-the-art algorithms.

A Universal Variance Reduction-Based Catalyst for Nonconvex Low-Rank  Matrix Recovery

  We propose a generic framework based on a new stochastic variance-reducedgradient descent algorithm for accelerating nonconvex low-rank matrix recovery.Starting from an appropriate initial estimator, our proposed algorithm performsprojected gradient descent based on a novel semi-stochastic gradientspecifically designed for low-rank matrix recovery. Based upon the mildrestricted strong convexity and smoothness conditions, we derive a projectednotion of the restricted Lipschitz continuous gradient property, and prove thatour algorithm enjoys linear convergence rate to the unknown low-rank matrixwith an improved computational complexity. Moreover, our algorithm can beemployed to both noiseless and noisy observations, where the optimal samplecomplexity and the minimax optimal statistical rate can be attainedrespectively. We further illustrate the superiority of our generic frameworkthrough several specific examples, both theoretically and experimentally.

A Unified Framework for Low-Rank plus Sparse Matrix Recovery

  We propose a unified framework to solve general low-rank plus sparse matrixrecovery problems based on matrix factorization, which covers a broad family ofobjective functions satisfying the restricted strong convexity and smoothnessconditions. Based on projected gradient descent and the double thresholdingoperator, our proposed generic algorithm is guaranteed to converge to theunknown low-rank and sparse matrices at a locally linear rate, while matchingthe best-known robustness guarantee (i.e., tolerance for sparsity). At the coreof our theory is a novel structural Lipschitz gradient condition for low-rankplus sparse matrices, which is essential for proving the linear convergencerate of our algorithm, and we believe is of independent interest to prove fastrates for general superposition-structured models. We illustrate theapplication of our framework through two concrete examples: robust matrixsensing and robust PCA. Experiments on both synthetic and real datasetscorroborate our theory.

Speeding Up Latent Variable Gaussian Graphical Model Estimation via  Nonconvex Optimizations

  We study the estimation of the latent variable Gaussian graphical model(LVGGM), where the precision matrix is the superposition of a sparse matrix anda low-rank matrix. In order to speed up the estimation of the sparse pluslow-rank components, we propose a sparsity constrained maximum likelihoodestimator based on matrix factorization, and an efficient alternating gradientdescent algorithm with hard thresholding to solve it. Our algorithm is ordersof magnitude faster than the convex relaxation based methods for LVGGM. Inaddition, we prove that our algorithm is guaranteed to linearly converge to theunknown sparse and low-rank components up to the optimal statistical precision.Experiments on both synthetic and genomic data demonstrate the superiority ofour algorithm over the state-of-the-art algorithms and corroborate our theory.

Robust Wirtinger Flow for Phase Retrieval with Arbitrary Corruption

  We consider the robust phase retrieval problem of recovering the unknownsignal from the magnitude-only measurements, where the measurements can becontaminated by both sparse arbitrary corruption and bounded random noise. Wepropose a new nonconvex algorithm for robust phase retrieval, namely RobustWirtinger Flow to jointly estimate the unknown signal and the sparsecorruption. We show that our proposed algorithm is guaranteed to convergelinearly to the unknown true signal up to a minimax optimal statisticalprecision in such a challenging setting. Compared with existing robust phaseretrieval methods, we achieve an optimal sample complexity of $O(n)$ in bothnoisy and noise-free settings. Thorough experiments on both synthetic and realdatasets corroborate our theory.

Saving Gradient and Negative Curvature Computations: Finding Local  Minima More Efficiently

  We propose a family of nonconvex optimization algorithms that are able tosave gradient and negative curvature computations to a large extent, and areguaranteed to find an approximate local minimum with improved runtimecomplexity. At the core of our algorithms is the division of the entire domainof the objective function into small and large gradient regions: our algorithmsonly perform gradient descent based procedure in the large gradient region, andonly perform negative curvature descent in the small gradient region. Our novelanalysis shows that the proposed algorithms can escape the small gradientregion in only one negative curvature descent step whenever they enter it, andthus they only need to perform at most $N_{\epsilon}$ negative curvaturedirection computations, where $N_{\epsilon}$ is the number of times thealgorithms enter small gradient regions. For both deterministic and stochasticsettings, we show that the proposed algorithms can potentially beat thestate-of-the-art local minima finding algorithms. For the finite-sum setting,our algorithm can also outperform the best algorithm in a certain regime.

Third-order Smoothness Helps: Even Faster Stochastic Optimization  Algorithms for Finding Local Minima

  We propose stochastic optimization algorithms that can find local minimafaster than existing algorithms for nonconvex optimization problems, byexploiting the third-order smoothness to escape non-degenerate saddle pointsmore efficiently. More specifically, the proposed algorithm only needs$\tilde{O}(\epsilon^{-10/3})$ stochastic gradient evaluations to converge to anapproximate local minimum $\mathbf{x}$, which satisfies $\|\nablaf(\mathbf{x})\|_2\leq\epsilon$ and $\lambda_{\min}(\nabla^2 f(\mathbf{x}))\geq-\sqrt{\epsilon}$ in the general stochastic optimization setting, where$\tilde{O}(\cdot)$ hides logarithm polynomial terms and constants. Thisimproves upon the $\tilde{O}(\epsilon^{-7/2})$ gradient complexity achieved bythe state-of-the-art stochastic local minima finding algorithms by a factor of$\tilde{O}(\epsilon^{-1/6})$. For nonconvex finite-sum optimization, ouralgorithm also outperforms the best known algorithms in a certain regime.

Stochastic Variance-Reduced Hamilton Monte Carlo Methods

  We propose a fast stochastic Hamilton Monte Carlo (HMC) method, for samplingfrom a smooth and strongly log-concave distribution. At the core of ourproposed method is a variance reduction technique inspired by the recentadvance in stochastic optimization. We show that, to achieve $\epsilon$accuracy in 2-Wasserstein distance, our algorithm achieves $\tildeO\big(n+\kappa^{2}d^{1/2}/\epsilon+\kappa^{4/3}d^{1/3}n^{2/3}/\epsilon^{2/3}\big)$gradient complexity (i.e., number of component gradient evaluations), whichoutperforms the state-of-the-art HMC and stochastic gradient HMC methods in awide regime. We also extend our algorithm for sampling from smooth and generallog-concave distributions, and prove the corresponding gradient complexity aswell. Experiments on both synthetic and real data demonstrate the superiorperformance of our algorithm.

Fast and Sample Efficient Inductive Matrix Completion via Multi-Phase  Procrustes Flow

  We revisit the inductive matrix completion problem that aims to recover arank-$r$ matrix with ambient dimension $d$ given $n$ features as the side priorinformation. The goal is to make use of the known $n$ features to reduce sampleand computational complexities. We present and analyze a new gradient-basednon-convex optimization algorithm that converges to the true underlying matrixat a linear rate with sample complexity only linearly depending on $n$ andlogarithmically depending on $d$. To the best of our knowledge, all previousalgorithms either have a quadratic dependency on the number of features insample complexity or a sub-linear computational convergence rate. In addition,we provide experiments on both synthetic and real world data to demonstrate theeffectiveness of our proposed algorithm.

Closing the Generalization Gap of Adaptive Gradient Methods in Training  Deep Neural Networks

  Adaptive gradient methods, which adopt historical gradient information toautomatically adjust the learning rate, have been observed to generalize worsethan stochastic gradient descent (SGD) with momentum in training deep neuralnetworks. This leaves how to close the generalization gap of adaptive gradientmethods an open problem. In this work, we show that adaptive gradient methodssuch as Adam, Amsgrad, are sometimes "over adapted". We design a new algorithm,called Partially adaptive momentum estimation method (Padam), which unifies theAdam/Amsgrad with SGD to achieve the best from both worlds. Experiments onstandard benchmarks show that Padam can maintain fast convergence rate asAdam/Amsgrad while generalizing as well as SGD in training deep neuralnetworks. These results would suggest practitioners pick up adaptive gradientmethods once again for faster training of deep neural networks.

Learning One-hidden-layer ReLU Networks via Gradient Descent

  We study the problem of learning one-hidden-layer neural networks withRectified Linear Unit (ReLU) activation function, where the inputs are sampledfrom standard Gaussian distribution and the outputs are generated from a noisyteacher network. We analyze the performance of gradient descent for trainingsuch kind of neural networks based on empirical risk minimization, and providealgorithm-dependent guarantees. In particular, we prove that tensorinitialization followed by gradient descent can converge to the ground-truthparameters at a linear rate up to some statistical error. To the best of ourknowledge, this is the first work characterizing the recovery guarantee forpractical learning of one-hidden-layer ReLU networks with multiple neurons.Numerical experiments verify our theoretical findings.

Stochastic Nested Variance Reduction for Nonconvex Optimization

  We study finite-sum nonconvex optimization problems, where the objectivefunction is an average of $n$ nonconvex functions. We propose a new stochasticgradient descent algorithm based on nested variance reduction. Compared withconventional stochastic variance reduced gradient (SVRG) algorithm that usestwo reference points to construct a semi-stochastic gradient with diminishingvariance in each iteration, our algorithm uses $K+1$ nested reference points tobuild a semi-stochastic gradient to further reduce its variance in eachiteration. For smooth nonconvex functions, the proposed algorithm converges toan $\epsilon$-approximate first-order stationary point (i.e., $\|\nablaF(\mathbf{x})\|_2\leq \epsilon$) within $\tilde{O}(n\land\epsilon^{-2}+\epsilon^{-3}\land n^{1/2}\epsilon^{-2})$ number of stochasticgradient evaluations. This improves the best known gradient complexity of SVRG$O(n+n^{2/3}\epsilon^{-2})$ and that of SCSG $O(n\land\epsilon^{-2}+\epsilon^{-10/3}\land n^{2/3}\epsilon^{-2})$. For gradientdominated functions, our algorithm also achieves a better gradient complexitythan the state-of-the-art algorithms.

Finding Local Minima via Stochastic Nested Variance Reduction

  We propose two algorithms that can find local minima faster than thestate-of-the-art algorithms in both finite-sum and general stochastic nonconvexoptimization. At the core of the proposed algorithms is$\text{One-epoch-SNVRG}^+$ using stochastic nested variance reduction (Zhou etal., 2018a), which outperforms the state-of-the-art variance reductionalgorithms such as SCSG (Lei et al., 2017). In particular, for finite-sumoptimization problems, the proposed$\text{SNVRG}^{+}+\text{Neon2}^{\text{finite}}$ algorithm achieves$\tilde{O}(n^{1/2}\epsilon^{-2}+n\epsilon_H^{-3}+n^{3/4}\epsilon_H^{-7/2})$gradient complexity to converge to an $(\epsilon, \epsilon_H)$-second-orderstationary point, which outperforms $\text{SVRG}+\text{Neon2}^{\text{finite}}$(Allen-Zhu and Li, 2017) , the best existing algorithm, in a wide regime. Forgeneral stochastic optimization problems, the proposed$\text{SNVRG}^{+}+\text{Neon2}^{\text{online}}$ achieves$\tilde{O}(\epsilon^{-3}+\epsilon_H^{-5}+\epsilon^{-2}\epsilon_H^{-3})$gradient complexity, which is better than both$\text{SVRG}+\text{Neon2}^{\text{online}}$ (Allen-Zhu and Li, 2017) andNatasha2 (Allen-Zhu, 2017) in certain regimes. Furthermore, we explore theacceleration brought by third-order smoothness of the objective function.

A Frank-Wolfe Framework for Efficient and Effective Adversarial Attacks

  Depending on how much information an adversary can access to, adversarialattacks can be classified as white-box attack and black-box attack. In bothcases, optimization-based attack algorithms can achieve relatively lowdistortions and high attack success rates. However, they usually suffer frompoor time and query complexities, thereby limiting their practical usefulness.In this work, we focus on the problem of developing efficient and effectiveoptimization-based adversarial attack algorithms. In particular, we propose anovel adversarial attack framework for both white-box and black-box settingsbased on the non-convex Frank-Wolfe algorithm. We show in theory that theproposed attack algorithms are efficient with an $O(1/\sqrt{T})$ convergencerate. The empirical results of attacking Inception V3 model and ResNet V2 modelon the ImageNet dataset also verify the efficiency and effectiveness of theproposed algorithms. More specific, our proposed algorithms attain the highestattack success rate in both white-box and black-box attacks among allbaselines, and are more time and query efficient than the state-of-the-art.

Sample Efficient Stochastic Variance-Reduced Cubic Regularization Method

  We propose a sample efficient stochastic variance-reduced cubicregularization (Lite-SVRC) algorithm for finding the local minimum efficientlyin nonconvex optimization. The proposed algorithm achieves a lower samplecomplexity of Hessian matrix computation than existing cubic regularizationbased methods. At the heart of our analysis is the choice of a constant batchsize of Hessian matrix computation at each iteration and the stochasticvariance reduction techniques. In detail, for a nonconvex function with $n$component functions, Lite-SVRC converges to the local minimum within$\tilde{O}(n+n^{2/3}/\epsilon^{3/2})$ Hessian sample complexity, which isfaster than all existing cubic regularization based methods. Numericalexperiments with different nonconvex optimization problems conducted on realdatasets validate our theoretical results.

Lower Bounds for Smooth Nonconvex Finite-Sum Optimization

  Smooth finite-sum optimization has been widely studied in both convex andnonconvex settings. However, existing lower bounds for finite-sum optimizationare mostly limited to the setting where each component function is (strongly)convex, while the lower bounds for nonconvex finite-sum optimization remainlargely unsolved. In this paper, we study the lower bounds for smooth nonconvexfinite-sum optimization, where the objective function is the average of $n$nonconvex component functions. We prove tight lower bounds for the complexityof finding $\epsilon$-suboptimal point and $\epsilon$-approximate stationarypoint in different settings, for a wide regime of the smallest eigenvalue ofthe Hessian of the objective function (or each component function). Given ourlower bounds, we can show that existing algorithms including KatyushaX(Allen-Zhu, 2018), Natasha (Allen-Zhu, 2017), RapGrad (Lan and Yang, 2018) andStagewiseKatyusha (Chen and Yang, 2018) have achieved optimal IncrementalFirst-order Oracle (IFO) complexity (i.e., number of IFO calls) up to logarithmfactors for nonconvex finite-sum optimization. We also point out potential waysto further improve these complexity results, in terms of making strongerassumptions or by a different convergence analysis.

Generalization Error Bounds of Gradient Descent for Learning  Over-parameterized Deep ReLU Networks

  Empirical studies show that gradient-based methods can learn deep neuralnetworks (DNNs) with very good generalization performance in theover-parameterization regime, where DNNs can easily fit a random labeling ofthe training data. While a line of recent work explains in theory that withover-parameterization and proper random initialization, gradient-based methodscan find the global minima of the training loss for DNNs, it does not explainthe good generalization performance of the gradient-based methods for learningover-parameterized DNNs. In this work, we take a step further, and prove thatunder certain assumption on the data distribution that is milder than linearseparability, gradient descent (GD) with proper random initialization is ableto train a sufficiently over-parameterized DNN to achieve arbitrarily smallexpected error (i.e., population error). This leads to an algorithmic-dependentgeneralization error bound for deep learning. To the best of our knowledge,this is the first result of its kind that can explain the good generalizationperformance of over-parameterized deep neural networks learned by gradientdescent.

Communication-efficient Distributed Estimation and Inference for  Transelliptical Graphical Models

  We propose communication-efficient distributed estimation and inferencemethods for the transelliptical graphical model, a semiparametric extension ofthe elliptical distribution in the high dimensional regime. In detail, theproposed method distributes the $d$-dimensional data of size $N$ generated froma transelliptical graphical model into $m$ worker machines, and estimates thelatent precision matrix on each worker machine based on the data of size$n=N/m$. It then debiases the local estimators on the worker machines and sendthem back to the master machine. Finally, on the master machine, it aggregatesthe debiased local estimators by averaging and hard thresholding. We show thatthe aggregated estimator attains the same statistical rate as the centralizedestimator based on all the data, provided that the number of machines satisfies$m \lesssim \min\{N\log d/d,\sqrt{N/(s^2\log d)}\}$, where $s$ is the maximumnumber of nonzero entries in each column of the latent precision matrix. It isworth noting that our algorithm and theory can be directly applied to Gaussiangraphical models, Gaussian copula graphical models and elliptical graphicalmodels, since they are all special cases of transelliptical graphical models.Thorough experiments on synthetic data back up our theory.

Global Convergence of Langevin Dynamics Based Algorithms for Nonconvex  Optimization

  We present a unified framework to analyze the global convergence of Langevindynamics based algorithms for nonconvex finite-sum optimization with $n$component functions. At the core of our analysis is a direct analysis of theergodicity of the numerical approximations to Langevin dynamics, which leads tofaster convergence rates. Specifically, we show that gradient Langevin dynamics(GLD) and stochastic gradient Langevin dynamics (SGLD) converge to the almostminimizer within $\tilde O\big(nd/(\lambda\epsilon) \big)$ and $\tildeO\big(d^7/(\lambda^5\epsilon^5) \big)$ stochastic gradient evaluationsrespectively, where $d$ is the problem dimension, and $\lambda$ is the spectralgap of the Markov chain generated by GLD. Both of the results improve upon thebest known gradient complexity results. Furthermore, for the first time weprove the global convergence guarantee for variance reduced stochastic gradientLangevin dynamics (VR-SGLD) to the almost minimizer after $\tildeO\big(\sqrt{n}d^5/(\lambda^4\epsilon^{5/2})\big)$ stochastic gradientevaluations, which outperforms the gradient complexities of GLD and SGLD in awide regime. Our theoretical analyses shed some light on using Langevindynamics based algorithms for nonconvex optimization with provable guarantees.

Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU  Networks

  We study the problem of training deep neural networks with Rectified LinearUnit (ReLU) activation function using gradient descent and stochastic gradientdescent. In particular, we study the binary classification problem and showthat for a broad family of loss functions, with proper random weightinitialization, both gradient descent and stochastic gradient descent can findthe global minima of the training loss for an over-parameterized deep ReLUnetwork, under mild assumption on the training data. The key idea of our proofis that Gaussian random initialization followed by (stochastic) gradientdescent produces a sequence of iterates that stay inside a small perturbationregion centering around the initial weights, in which the empirical lossfunction of deep ReLU networks enjoys nice local curvature properties thatensure the global convergence of (stochastic) gradient descent. Our theoreticalresults shed light on understanding the optimization for deep learning, andpave the way for studying the optimization dynamics of training modern deepneural networks.

Stochastic Recursive Variance-Reduced Cubic Regularization Methods

  Stochastic Variance-Reduced Cubic regularization (SVRC) algorithms havereceived increasing attention due to its improved gradient/Hessian complexities(i.e., number of queries to stochastic gradient/Hessian oracles) to find localminima for nonconvex finite-sum optimization. However, it is unclear whetherexisting SVRC algorithms can be further improved. Moreover, the semi-stochasticHessian estimator adopted in existing SVRC algorithms prevents the use ofHessian-vector product-based fast cubic subproblem solvers, which makes SVRCalgorithms computationally intractable for high-dimensional problems. In thispaper, we first present a Stochastic Recursive Variance-Reduced Cubicregularization method (SRVRC) using a recursively updated semi-stochasticgradient and Hessian estimators. It enjoys improved gradient and Hessiancomplexities to find an $(\epsilon, \sqrt{\epsilon})$-approximate localminimum, and outperforms the state-of-the-art SVRC algorithms. Built uponSRVRC, we further propose a Hessian-free SRVRC algorithm, namelySRVRC$_{\text{free}}$, which only requires stochastic gradient andHessian-vector product computations, and achieves $\tilde O(dn\epsilon^{-2}\land d\epsilon^{-3})$ runtime complexity, where $n$ is the number of componentfunctions in the finite-sum structure, $d$ is the problem dimension, and$\epsilon$ is the optimization precision. This outperforms the best-knownruntime complexity $\tilde O(d\epsilon^{-3.5})$ achieved by stochastic cubicregularization algorithm proposed in Tripuraneni et al. 2018.

High Dimensional Expectation-Maximization Algorithm: Statistical  Optimization and Asymptotic Normality

  We provide a general theory of the expectation-maximization (EM) algorithmfor inferring high dimensional latent variable models. In particular, we maketwo contributions: (i) For parameter estimation, we propose a novel highdimensional EM algorithm which naturally incorporates sparsity structure intoparameter estimation. With an appropriate initialization, this algorithmconverges at a geometric rate and attains an estimator with the (near-)optimalstatistical rate of convergence. (ii) Based on the obtained estimator, wepropose new inferential procedures for testing hypotheses and constructingconfidence intervals for low dimensional components of high dimensionalparameters. For a broad family of statistical models, our framework establishesthe first computationally feasible approach for optimal estimation andasymptotic inference in high dimensions. Our theory is supported by thoroughnumerical results.

Statistical Limits of Convex Relaxations

  Many high dimensional sparse learning problems are formulated as nonconvexoptimization. A popular approach to solve these nonconvex optimization problemsis through convex relaxations such as linear and semidefinite programming. Inthis paper, we study the statistical limits of convex relaxations.Particularly, we consider two problems: Mean estimation for sparse principalsubmatrix and edge probability estimation for stochastic block model. Weexploit the sum-of-squares relaxation hierarchy to sharply characterize thelimits of a broad class of convex relaxations. Our result shows statisticaloptimality needs to be compromised for achieving computational tractabilityusing convex relaxations. Compared with existing results on computational lowerbounds for statistical problems, which consider general polynomial-timealgorithms and rely on computational hardness hypotheses on problems likeplanted clique detection, our theory focuses on a broad class of convexrelaxations and does not rely on unproven hypotheses.

Sharp Computational-Statistical Phase Transitions via Oracle  Computational Model

  We study the fundamental tradeoffs between computational tractability andstatistical accuracy for a general family of hypothesis testing problems withcombinatorial structures. Based upon an oracle model of computation, whichcaptures the interactions between algorithms and data, we establish a generallower bound that explicitly connects the minimum testing risk undercomputational budget constraints with the intrinsic probabilistic andcombinatorial structures of statistical problems. This lower bound mirrors theclassical statistical lower bound by Le Cam (1986) and allows us to quantifythe optimal statistical performance achievable given limited computationalbudgets in a systematic fashion. Under this unified framework, we sharplycharacterize the statistical-computational phase transition for two testingproblems, namely, normal mean detection and sparse principal componentdetection. For normal mean detection, we consider two combinatorial structures,namely, sparse set and perfect matching. For these problems we identifysignificant gaps between the optimal statistical accuracy that is achievableunder computational tractability constraints and the classical statisticallower bounds. Compared with existing works on computational lower bounds forstatistical problems, which consider general polynomial-time algorithms onTuring machines, and rely on computational hardness hypotheses on problems likeplanted clique detection, we focus on the oracle computational model, whichcovers a broad range of popular algorithms, and do not rely on unprovenhypotheses. Moreover, our result provides an intuitive and concreteinterpretation for the intrinsic computational intractability ofhigh-dimensional statistical problems. One byproduct of our result is a lowerbound for a strict generalization of the matrix permanent problem, which is ofindependent interest.

