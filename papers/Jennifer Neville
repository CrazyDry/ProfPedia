Stochastic Gradient Descent for Relational Logistic Regression via
  Partial Network Crawls

  Research in statistical relational learning has produced a number of methods
for learning relational models from large-scale network data. While these
methods have been successfully applied in various domains, they have been
developed under the unrealistic assumption of full data access. In practice,
however, the data are often collected by crawling the network, due to
proprietary access, limited resources, and privacy concerns. Recently, we
showed that the parameter estimates for relational Bayes classifiers computed
from network samples collected by existing network crawlers can be quite
inaccurate, and developed a crawl-aware estimation method for such models
(Yang, Ribeiro, and Neville, 2017). In this work, we extend the methodology to
learning relational logistic regression models via stochastic gradient descent
from partial network crawls, and show that the proposed method yields accurate
parameter estimates and confidence intervals.


Dynamic Behavioral Mixed-Membership Model for Large Evolving Networks

  The majority of real-world networks are dynamic and extremely large (e.g.,
Internet Traffic, Twitter, Facebook, ...). To understand the structural
behavior of nodes in these large dynamic networks, it may be necessary to model
the dynamics of behavioral roles representing the main connectivity patterns
over time. In this paper, we propose a dynamic behavioral mixed-membership
model (DBMM) that captures the roles of nodes in the graph and how they evolve
over time. Unlike other node-centric models, our model is scalable for
analyzing large dynamic networks. In addition, DBMM is flexible,
parameter-free, has no functional form or parameterization, and is
interpretable (identifies explainable patterns). The performance results
indicate our approach can be applied to very large networks while the
experimental results show that our model uncovers interesting patterns
underlying the dynamics of these networks.


Representations and Ensemble Methods for Dynamic Relational
  Classification

  Temporal networks are ubiquitous and evolve over time by the addition,
deletion, and changing of links, nodes, and attributes. Although many
relational datasets contain temporal information, the majority of existing
techniques in relational learning focus on static snapshots and ignore the
temporal dynamics. We propose a framework for discovering temporal
representations of relational data to increase the accuracy of statistical
relational learning algorithms. The temporal relational representations serve
as a basis for classification, ensembles, and pattern mining in evolving
domains. The framework includes (1) selecting the time-varying relational
components (links, attributes, nodes), (2) selecting the temporal granularity,
(3) predicting the temporal influence of each time-varying relational
component, and (4) choosing the weighted relational classifier. Additionally,
we propose temporal ensemble methods that exploit the temporal-dimension of
relational data. These ensembles outperform traditional and more sophisticated
relational ensembles while avoiding the issue of learning the most optimal
representation. Finally, the space of temporal-relational models are evaluated
using a sample of classifiers. In all cases, the proposed temporal-relational
classifiers outperform competing models that ignore the temporal information.
The results demonstrate the capability and necessity of the temporal-relational
representations for classification, ensembles, and for mining temporal
datasets.


Space-Efficient Sampling from Social Activity Streams

  In order to efficiently study the characteristics of network domains and
support development of network systems (e.g. algorithms, protocols that operate
on networks), it is often necessary to sample a representative subgraph from a
large complex network. Although recent subgraph sampling methods have been
shown to work well, they focus on sampling from memory-resident graphs and
assume that the sampling algorithm can access the entire graph in order to
decide which nodes/edges to select. Many large-scale network datasets, however,
are too large and/or dynamic to be processed using main memory (e.g., email,
tweets, wall posts). In this work, we formulate the problem of sampling from
large graph streams. We propose a streaming graph sampling algorithm that
dynamically maintains a representative sample in a reservoir based setting. We
evaluate the efficacy of our proposed methods empirically using several
real-world data sets. Across all datasets, we found that our method produce
samples that preserve better the original graph distributions.


Learning the Latent State Space of Time-Varying Graphs

  From social networks to Internet applications, a wide variety of electronic
communication tools are producing streams of graph data; where the nodes
represent users and the edges represent the contacts between them over time.
This has led to an increased interest in mechanisms to model the dynamic
structure of time-varying graphs. In this work, we develop a framework for
learning the latent state space of a time-varying email graph. We show how the
framework can be used to find subsequences that correspond to global real-time
events in the Email graph (e.g. vacations, breaks, ...etc.). These events
impact the underlying graph process to make its characteristics non-stationary.
Within the framework, we compare two different representations of the temporal
relationships; discrete vs. probabilistic. We use the two representations as
inputs to a mixture model to learn the latent state transitions that correspond
to important changes in the Email graph structure over time.


Using Bayesian Network Representations for Effective Sampling from
  Generative Network Models

  Bayesian networks (BNs) are used for inference and sampling by exploiting
conditional independence among random variables. Context specific independence
(CSI) is a property of graphical models where additional independence relations
arise in the context of particular values of random variables (RVs).
Identifying and exploiting CSI properties can simplify inference. Some
generative network models (models that generate social/information network
samples from a network distribution P(G)), with complex interactions among a
set of RVs, can be represented with probabilistic graphical models, in
particular with BNs. In the present work we show one such a case. We discuss
how a mixed Kronecker Product Graph Model can be represented as a BN, and study
its BN properties that can be used for efficient sampling. Specifically, we
show that instead of exhibiting CSI properties, the model has deterministic
context-specific dependence (DCSD). Exploiting this property focuses the
sampling method on a subset of the sampling space that improves efficiency.


Size-Consistent Statistics for Anomaly Detection in Dynamic Networks

  An important task in network analysis is the detection of anomalous events in
a network time series. These events could merely be times of interest in the
network timeline or they could be examples of malicious activity or network
malfunction. Hypothesis testing using network statistics to summarize the
behavior of the network provides a robust framework for the anomaly detection
decision process. Unfortunately, choosing network statistics that are dependent
on confounding factors like the total number of nodes or edges can lead to
incorrect conclusions (e.g., false positives and false negatives). In this
dissertation we describe the challenges that face anomaly detection in dynamic
network streams regarding confounding factors. We also provide two solutions to
avoiding error due to confounding factors: the first is a randomization testing
method that controls for confounding factors, and the second is a set of
size-consistent network statistics which avoid confounding due to the most
common factors, edge count and node count.


Identifying User Survival Types via Clustering of Censored Social
  Network Data

  The goal of cluster analysis in survival data is to identify clusters that
are decidedly associated with the survival outcome. Previous research has
explored this problem primarily in the medical domain with relatively small
datasets, but the need for such a clustering methodology could arise in other
domains with large datasets, such as social networks. Concretely, we wish to
identify different survival classes in a social network by clustering the users
based on their lifespan in the network. In this paper, we propose a decision
tree based algorithm that uses a global normalization of $p$-values to identify
clusters with significantly different survival distributions. We evaluate the
clusters from our model with the help of a simple survival prediction task and
show that our model outperforms other competing methods.


Exploring Student Check-In Behavior for Improved Point-of-Interest
  Prediction

  With the availability of vast amounts of user visitation history on
location-based social networks (LBSN), the problem of Point-of-Interest (POI)
prediction has been extensively studied. However, much of the research has been
conducted solely on voluntary checkin datasets collected from social apps such
as Foursquare or Yelp. While these data contain rich information about
recreational activities (e.g., restaurants, nightlife, and entertainment),
information about more prosaic aspects of people's lives is sparse. This not
only limits our understanding of users' daily routines, but more importantly
the modeling assumptions developed based on characteristics of recreation-based
data may not be suitable for richer check-in data. In this work, we present an
analysis of education "check-in" data using WiFi access logs collected at
Purdue University. We propose a heterogeneous graph-based method to encode the
correlations between users, POIs, and activities, and then jointly learn
embeddings for the vertices. We evaluate our method compared to previous
state-of-the-art POI prediction methods, and show that the assumptions made by
previous methods significantly degrade performance on our data with dense(r)
activity signals. We also show how our learned embeddings could be used to
identify similar students (e.g., for friend suggestions).


Community detection over a heterogeneous population of non-aligned
  networks

  Clustering and community detection with multiple graphs have typically
focused on aligned graphs, where there is a mapping between nodes across the
graphs (e.g., multi-view, multi-layer, temporal graphs). However, there are
numerous application areas with multiple graphs that are only partially
aligned, or even unaligned. These graphs are often drawn from the same
population, with communities of potentially different sizes that exhibit
similar structure. In this paper, we develop a joint stochastic blockmodel
(Joint SBM) to estimate shared communities across sets of heterogeneous
non-aligned graphs. We derive an efficient spectral clustering approach to
learn the parameters of the joint SBM. We evaluate the model on both synthetic
and real-world datasets and show that the joint model is able to exploit
cross-graph information to better estimate the communities compared to learning
separate SBMs on each individual graph.


Guided Data Repair

  In this paper we present GDR, a Guided Data Repair framework that
incorporates user feedback in the cleaning process to enhance and accelerate
existing automatic repair techniques while minimizing user involvement. GDR
consults the user on the updates that are most likely to be beneficial in
improving data quality. GDR also uses machine learning methods to identify and
apply the correct updates directly to the database without the actual
involvement of the user on these specific updates. To rank potential updates
for consultation by the user, we first group these repairs and quantify the
utility of each group using the decision-theory concept of value of information
(VOI). We then apply active learning to order updates within a group based on
their ability to improve the learned model. User feedback is used to repair the
database and to adaptively refine the training set for the model. We
empirically evaluate GDR on a real-world dataset and show significant
improvement in data quality using our user guided repairing process. We also,
assess the trade-off between the user efforts and the resulting data quality.


Methods to Determine Node Centrality and Clustering in Graphs with
  Uncertain Structure

  Much of the past work in network analysis has focused on analyzing discrete
graphs, where binary edges represent the "presence" or "absence" of a
relationship. Since traditional network measures (e.g., betweenness centrality)
utilize a discrete link structure, complex systems must be transformed to this
representation in order to investigate network properties. However, in many
domains there may be uncertainty about the relationship structure and any
uncertainty information would be lost in translation to a discrete
representation. Uncertainty may arise in domains where there is moderating link
information that cannot be easily observed, i.e., links become inactive over
time but may not be dropped or observed links may not always corresponds to a
valid relationship. In order to represent and reason with these types of
uncertainty, we move beyond the discrete graph framework and develop social
network measures based on a probabilistic graph representation. More
specifically, we develop measures of path length, betweenness centrality, and
clustering coefficient---one set based on sampling and one based on
probabilistic paths. We evaluate our methods on three real-world networks from
Enron, Facebook, and DBLP, showing that our proposed methods more accurately
capture salient effects without being susceptible to local noise, and that the
resulting analysis produces a better understanding of the graph structure and
the uncertainty resulting from its change over time.


Network Sampling: From Static to Streaming Graphs

  Network sampling is integral to the analysis of social, information, and
biological networks. Since many real-world networks are massive in size,
continuously evolving, and/or distributed in nature, the network structure is
often sampled in order to facilitate study. For these reasons, a more thorough
and complete understanding of network sampling is critical to support the field
of network science. In this paper, we outline a framework for the general
problem of network sampling, by highlighting the different objectives,
population and units of interest, and classes of network sampling methods. In
addition, we propose a spectrum of computational models for network sampling
methods, ranging from the traditionally studied model based on the assumption
of a static domain to a more challenging model that is appropriate for
streaming domains. We design a family of sampling methods based on the concept
of graph induction that generalize across the full spectrum of computational
models (from static to streaming) while efficiently preserving many of the
topological properties of the input graphs. Furthermore, we demonstrate how
traditional static sampling algorithms can be modified for graph streams for
each of the three main classes of sampling methods: node, edge, and
topology-based sampling. Our experimental results indicate that our proposed
family of sampling methods more accurately preserves the underlying properties
of the graph for both static and streaming graphs. Finally, we study the impact
of network sampling algorithms on the parameter estimation and performance
evaluation of relational classification algorithms.


Role-Dynamics: Fast Mining of Large Dynamic Networks

  To understand the structural dynamics of a large-scale social, biological or
technological network, it may be useful to discover behavioral roles
representing the main connectivity patterns present over time. In this paper,
we propose a scalable non-parametric approach to automatically learn the
structural dynamics of the network and individual nodes. Roles may represent
structural or behavioral patterns such as the center of a star, peripheral
nodes, or bridge nodes that connect different communities. Our novel approach
learns the appropriate structural role dynamics for any arbitrary network and
tracks the changes over time. In particular, we uncover the specific global
network dynamics and the local node dynamics of a technological, communication,
and social network. We identify interesting node and network patterns such as
stationary and non-stationary roles, spikes/steps in role-memberships (perhaps
indicating anomalies), increasing/decreasing role trends, among many others.
Our results indicate that the nodes in each of these networks have distinct
connectivity patterns that are non-stationary and evolve considerably over
time. Overall, the experiments demonstrate the effectiveness of our approach
for fast mining and tracking of the dynamics in large networks. Furthermore,
the dynamic structural representation provides a basis for building more
sophisticated models and tools that are fast for exploring large dynamic
networks.


Transforming Graph Representations for Statistical Relational Learning

  Relational data representations have become an increasingly important topic
due to the recent proliferation of network datasets (e.g., social, biological,
information networks) and a corresponding increase in the application of
statistical relational learning (SRL) algorithms to these domains. In this
article, we examine a range of representation issues for graph-based relational
data. Since the choice of relational data representation for the nodes, links,
and features can dramatically affect the capabilities of SRL algorithms, we
survey approaches and opportunities for relational representation
transformation designed to improve the performance of these algorithms. This
leads us to introduce an intuitive taxonomy for data representation
transformations in relational domains that incorporates link transformation and
node transformation as symmetric representation tasks. In particular, the
transformation tasks for both nodes and links include (i) predicting their
existence, (ii) predicting their label or type, (iii) estimating their weight
or importance, and (iv) systematically constructing their relevant features. We
motivate our taxonomy through detailed examples and use it to survey and
compare competing approaches for each of these tasks. We also discuss general
conditions for transforming links, nodes, and features. Finally, we highlight
challenges that remain to be addressed.


Graph Sample and Hold: A Framework for Big-Graph Analytics

  Sampling is a standard approach in big-graph analytics; the goal is to
efficiently estimate the graph properties by consulting a sample of the whole
population. A perfect sample is assumed to mirror every property of the whole
population. Unfortunately, such a perfect sample is hard to collect in complex
populations such as graphs (e.g. web graphs, social networks etc), where an
underlying network connects the units of the population. Therefore, a good
sample will be representative in the sense that graph properties of interest
can be estimated with a known degree of accuracy. While previous work focused
particularly on sampling schemes used to estimate certain graph properties
(e.g. triangle count), much less is known for the case when we need to estimate
various graph properties with the same sampling scheme. In this paper, we
propose a generic stream sampling framework for big-graph analytics, called
Graph Sample and Hold (gSH). To begin, the proposed framework samples from
massive graphs sequentially in a single pass, one edge at a time, while
maintaining a small state. We then show how to produce unbiased estimators for
various graph properties from the sample. Given that the graph analysis
algorithms will run on a sample instead of the whole population, the runtime
complexity of these algorithm is kept under control. Moreover, given that the
estimators of graph properties are unbiased, the approximation error is kept
under control. Finally, we show the performance of the proposed framework (gSH)
on various types of graphs, such as social graphs, among others.


Anomaly Detection in Dynamic Networks of Varying Size

  Dynamic networks, also called network streams, are an important data
representation that applies to many real-world domains. Many sets of network
data such as e-mail networks, social networks, or internet traffic networks are
best represented by a dynamic network due to the temporal component of the
data. One important application in the domain of dynamic network analysis is
anomaly detection. Here the task is to identify points in time where the
network exhibits behavior radically different from a typical time, either due
to some event (like the failure of machines in a computer network) or a shift
in the network properties. This problem is made more difficult by the fluid
nature of what is considered "normal" network behavior. The volume of traffic
on a network, for example, can change over the course of a month or even vary
based on the time of the day without being considered unusual. Anomaly
detection tests using traditional network statistics have difficulty in these
scenarios due to their Density Dependence: as the volume of edges changes the
value of the statistics changes as well making it difficult to determine if the
change in signal is due to the traffic volume or due to some fundamental shift
in the behavior of the network. To more accurately detect anomalies in dynamic
networks, we introduce the concept of Density-Consistent network statistics. On
synthetically generated graphs anomaly detectors using these statistics show a
a 20-400% improvement in the recall when distinguishing graphs drawn from
different distributions. When applied to several real datasets
Density-Consistent statistics recover multiple network events which standard
statistics failed to find.


Graphlet Decomposition: Framework, Algorithms, and Applications

  From social science to biology, numerous applications often rely on graphlets
for intuitive and meaningful characterization of networks at both the global
macro-level as well as the local micro-level. While graphlets have witnessed a
tremendous success and impact in a variety of domains, there has yet to be a
fast and efficient approach for computing the frequencies of these subgraph
patterns. However, existing methods are not scalable to large networks with
millions of nodes and edges, which impedes the application of graphlets to new
problems that require large-scale network analysis. To address these problems,
we propose a fast, efficient, and parallel algorithm for counting graphlets of
size k={3,4}-nodes that take only a fraction of the time to compute when
compared with the current methods used. The proposed graphlet counting
algorithms leverages a number of proven combinatorial arguments for different
graphlets. For each edge, we count a few graphlets, and with these counts along
with the combinatorial arguments, we obtain the exact counts of others in
constant time. On a large collection of 300+ networks from a variety of
domains, our graphlet counting strategies are on average 460x faster than
current methods. This brings new opportunities to investigate the use of
graphlets on much larger networks and newer applications as we show in the
experiments. To the best of our knowledge, this paper provides the largest
graphlet computations to date as well as the largest systematic investigation
on over 300+ networks from a variety of domains.


Combining Gradient Boosting Machines with Collective Inference to
  Predict Continuous Values

  Gradient boosting of regression trees is a competitive procedure for learning
predictive models of continuous data that fits the data with an additive
non-parametric model. The classic version of gradient boosting assumes that the
data is independent and identically distributed. However, relational data with
interdependent, linked instances is now common and the dependencies in such
data can be exploited to improve predictive performance. Collective inference
is one approach to exploit relational correlation patterns and significantly
reduce classification error. However, much of the work on collective learning
and inference has focused on discrete prediction tasks rather than continuous.
%target values has not got that attention in terms of collective inference. In
this work, we investigate how to combine these two paradigms together to
improve regression in relational domains. Specifically, we propose a boosting
algorithm for learning a collective inference model that predicts a continuous
target variable. In the algorithm, we learn a basic relational model,
collectively infer the target values, and then iteratively learn relational
models to predict the residuals. We evaluate our proposed algorithm on a real
network dataset and show that it outperforms alternative boosting methods.
However, our investigation also revealed that the relational features interact
together to produce better predictions.


Multi-level hypothesis testing for populations of heterogeneous networks

  In this work, we consider hypothesis testing and anomaly detection on
datasets where each observation is a weighted network. Examples of such data
include brain connectivity networks from fMRI flow data, or word co-occurrence
counts for populations of individuals. Current approaches to hypothesis testing
for weighted networks typically requires thresholding the edge-weights, to
transform the data to binary networks. This results in a loss of information,
and outcomes are sensitivity to choice of threshold levels. Our work avoids
this, and we consider weighted-graph observations in two situations, 1) where
each graph belongs to one of two populations, and 2) where entities belong to
one of two populations, with each entity possessing multiple graphs (indexed
e.g. by time). Specifically, we propose a hierarchical Bayesian hypothesis
testing framework that models each population with a mixture of latent space
models for weighted networks, and then tests populations of networks for
differences in distribution over components. Our framework is capable of
population-level, entity-specific, as well as edge-specific hypothesis testing.
We apply it to synthetic data and three real-world datasets: two social media
datasets involving word co-occurrences from discussions on Twitter of the
political unrest in Brazil, and on Instagram concerning Attention Deficit
Hyperactivity Disorder (ADHD) and depression drugs, and one medical dataset
involving fMRI brain-scans of human subjects. The results show that our
proposed method has lower Type I error and higher statistical power compared to
alternatives that need to threshold the edge weights. Moreover, they show our
proposed method is better suited to deal with highly heterogeneous datasets.


ASPCAP: The Apogee Stellar Parameter and Chemical Abundances Pipeline

  The Apache Point Observatory Galactic Evolution Experiment (APOGEE) has built
the largest moderately high-resolution (R=22, 500) spectroscopic map of the
stars across the Milky Way, and including dust-obscured areas. The APOGEE
Stellar Parameter and Chemical Abundances Pipeline (ASPCAP) is the software
developed for the automated analysis of these spectra. ASPCAP determines
atmospheric parameters and chemical abundances from observed spectra by
comparing observed spectra to libraries of theoretical spectra, using chi-2
minimization in a multidimensional parameter space. The package consists of a
fortran90 code that does the actual minimization, and a wrapper IDL code for
book-keeping and data handling. This paper explains in detail the ASPCAP
components and functionality, and presents results from a number of tests
designed to check its performance. ASPCAP provides stellar effective
temperatures, surface gravities, and metallicities precise to 2%, 0.1 dex, and
0.05 dex, respectively, for most APOGEE stars, which are predominantly giants.
It also provides abundances for up to 15 chemical elements with various levels
of precision, typically under 0.1 dex. The final data release (DR12) of the
Sloan Digital Sky Survey III contains an APOGEE database of more than 150,000
stars. ASPCAP development continues in the SDSS-IV APOGEE-2 survey.


Fast Generation of Large Scale Social Networks with Clustering

  A key challenge within the social network literature is the problem of
network generation - that is, how can we create synthetic networks that match
characteristics traditionally found in most real world networks? Important
characteristics that are present in social networks include a power law degree
distribution, small diameter and large amounts of clustering; however, most
current network generators, such as the Chung Lu and Kronecker models, largely
ignore the clustering present in a graph and choose to focus on preserving
other network statistics, such as the power law distribution. Models such as
the exponential random graph model have a transitivity parameter, but are
computationally difficult to learn, making scaling to large real world networks
intractable. In this work, we propose an extension to the Chung Lu ran- dom
graph model, the Transitive Chung Lu (TCL) model, which incorporates the notion
of a random transitive edge. That is, with some probability it will choose to
connect to a node exactly two hops away, having been introduced to a 'friend of
a friend'. In all other cases it will follow the standard Chung Lu model,
selecting a 'random surfer' from anywhere in the graph according to the given
invariant distribution. We prove TCL's expected degree distribution is equal to
the degree distribution of the original graph, while being able to capture the
clustering present in the network. The single parameter required by our model
can be learned in seconds on graphs with millions of edges, while networks can
be generated in time that is linear in the number of edges. We demonstrate the
performance TCL on four real- world social networks, including an email dataset
with hundreds of thousands of nodes and millions of edges, showing TCL
generates graphs that match the degree distribution, clustering coefficients
and hop plots of the original networks.


Abundances, Stellar Parameters, and Spectra From the SDSS-III/APOGEE
  Survey

  The SDSS-III/APOGEE survey operated from 2011-2014 using the APOGEE
spectrograph, which collects high-resolution (R~22,500), near-IR (1.51-1.70
microns) spectra with a multiplexing (300 fiber-fed objects) capability. We
describe the survey data products that are publicly available, which include
catalogs with radial velocity, stellar parameters, and 15 elemental abundances
for over 150,000 stars, as well as the more than 500,000 spectra from which
these quantities are derived. Calibration relations for the stellar parameters
(Teff, log g, [M/H], [alpha/M]) and abundances (C, N, O, Na, Mg, Al, Si, S, K,
Ca, Ti, V, Mn, Fe, Ni) are presented and discussed. The internal scatter of the
abundances within clusters indicates that abundance precision is generally
between 0.05 and 0.09 dex across a broad temperature range; within more limited
ranges and at high S/N, it is smaller for some elemental abundances. We assess
the accuracy of the abundances using comparison of mean cluster metallicities
with literature values, APOGEE observations of the solar spectrum and of
Arcturus, comparison of individual star abundances with other measurements, and
consideration of the locus of derived parameters and abundances of the entire
sample, and find that it is challenging to determine the absolute abundance
scale; external accuracy may be good to 0.1-0.2 dex. Uncertainties may be
larger at cooler temperatures (Teff<4000K). Access to the public data release
and data products is described, and some guidance for using the data products
is provided.


The Apache Point Observatory Galactic Evolution Experiment (APOGEE)

  The Apache Point Observatory Galactic Evolution Experiment (APOGEE), one of
the programs in the Sloan Digital Sky Survey III (SDSS-III), has now completed
its systematic, homogeneous spectroscopic survey sampling all major populations
of the Milky Way. After a three year observing campaign on the Sloan 2.5-m
Telescope, APOGEE has collected a half million high resolution (R~22,500), high
S/N (>100), infrared (1.51-1.70 microns) spectra for 146,000 stars, with time
series information via repeat visits to most of these stars. This paper
describes the motivations for the survey and its overall design---hardware,
field placement, target selection, operations---and gives an overview of these
aspects as well as the data reduction, analysis and products. An index is also
given to the complement of technical papers that describe various critical
survey components in detail. Finally, we discuss the achieved survey
performance and illustrate the variety of potential uses of the data products
by way of a number of science demonstrations, which span from time series
analysis of stellar spectral variations and radial velocity variations from
stellar companions, to spatial maps of kinematics, metallicity and abundance
patterns across the Galaxy and as a function of age, to new views of the
interstellar medium, the chemistry of star clusters, and the discovery of rare
stellar species. As part of SDSS-III Data Release 12, all of the APOGEE data
products are now publicly available.


The Eleventh and Twelfth Data Releases of the Sloan Digital Sky Survey:
  Final Data from SDSS-III

  The third generation of the Sloan Digital Sky Survey (SDSS-III) took data
from 2008 to 2014 using the original SDSS wide-field imager, the original and
an upgraded multi-object fiber-fed optical spectrograph, a new near-infrared
high-resolution spectrograph, and a novel optical interferometer. All the data
from SDSS-III are now made public. In particular, this paper describes Data
Release 11 (DR11) including all data acquired through 2013 July, and Data
Release 12 (DR12) adding data acquired through 2014 July (including all data
included in previous data releases), marking the end of SDSS-III observing.
Relative to our previous public release (DR10), DR12 adds one million new
spectra of galaxies and quasars from the Baryon Oscillation Spectroscopic
Survey (BOSS) over an additional 3000 sq. deg of sky, more than triples the
number of H-band spectra of stars as part of the Apache Point Observatory (APO)
Galactic Evolution Experiment (APOGEE), and includes repeated accurate radial
velocity measurements of 5500 stars from the Multi-Object APO Radial Velocity
Exoplanet Large-area Survey (MARVELS). The APOGEE outputs now include measured
abundances of 15 different elements for each star. In total, SDSS-III added
2350 sq. deg of ugriz imaging; 155,520 spectra of 138,099 stars as part of the
Sloan Exploration of Galactic Understanding and Evolution 2 (SEGUE-2) survey;
2,497,484 BOSS spectra of 1,372,737 galaxies, 294,512 quasars, and 247,216
stars over 9376 sq. deg; 618,080 APOGEE spectra of 156,593 stars; and 197,040
MARVELS spectra of 5,513 stars. Since its first light in 1998, SDSS has imaged
over 1/3 of the Celestial sphere in five bands and obtained over five million
astronomical spectra.


