Stochastic Gradient Descent for Relational Logistic Regression via  Partial Network Crawls

  Research in statistical relational learning has produced a number of methodsfor learning relational models from large-scale network data. While thesemethods have been successfully applied in various domains, they have beendeveloped under the unrealistic assumption of full data access. In practice,however, the data are often collected by crawling the network, due toproprietary access, limited resources, and privacy concerns. Recently, weshowed that the parameter estimates for relational Bayes classifiers computedfrom network samples collected by existing network crawlers can be quiteinaccurate, and developed a crawl-aware estimation method for such models(Yang, Ribeiro, and Neville, 2017). In this work, we extend the methodology tolearning relational logistic regression models via stochastic gradient descentfrom partial network crawls, and show that the proposed method yields accurateparameter estimates and confidence intervals.

Representations and Ensemble Methods for Dynamic Relational  Classification

  Temporal networks are ubiquitous and evolve over time by the addition,deletion, and changing of links, nodes, and attributes. Although manyrelational datasets contain temporal information, the majority of existingtechniques in relational learning focus on static snapshots and ignore thetemporal dynamics. We propose a framework for discovering temporalrepresentations of relational data to increase the accuracy of statisticalrelational learning algorithms. The temporal relational representations serveas a basis for classification, ensembles, and pattern mining in evolvingdomains. The framework includes (1) selecting the time-varying relationalcomponents (links, attributes, nodes), (2) selecting the temporal granularity,(3) predicting the temporal influence of each time-varying relationalcomponent, and (4) choosing the weighted relational classifier. Additionally,we propose temporal ensemble methods that exploit the temporal-dimension ofrelational data. These ensembles outperform traditional and more sophisticatedrelational ensembles while avoiding the issue of learning the most optimalrepresentation. Finally, the space of temporal-relational models are evaluatedusing a sample of classifiers. In all cases, the proposed temporal-relationalclassifiers outperform competing models that ignore the temporal information.The results demonstrate the capability and necessity of the temporal-relationalrepresentations for classification, ensembles, and for mining temporaldatasets.

Dynamic Behavioral Mixed-Membership Model for Large Evolving Networks

  The majority of real-world networks are dynamic and extremely large (e.g.,Internet Traffic, Twitter, Facebook, ...). To understand the structuralbehavior of nodes in these large dynamic networks, it may be necessary to modelthe dynamics of behavioral roles representing the main connectivity patternsover time. In this paper, we propose a dynamic behavioral mixed-membershipmodel (DBMM) that captures the roles of nodes in the graph and how they evolveover time. Unlike other node-centric models, our model is scalable foranalyzing large dynamic networks. In addition, DBMM is flexible,parameter-free, has no functional form or parameterization, and isinterpretable (identifies explainable patterns). The performance resultsindicate our approach can be applied to very large networks while theexperimental results show that our model uncovers interesting patternsunderlying the dynamics of these networks.

Space-Efficient Sampling from Social Activity Streams

  In order to efficiently study the characteristics of network domains andsupport development of network systems (e.g. algorithms, protocols that operateon networks), it is often necessary to sample a representative subgraph from alarge complex network. Although recent subgraph sampling methods have beenshown to work well, they focus on sampling from memory-resident graphs andassume that the sampling algorithm can access the entire graph in order todecide which nodes/edges to select. Many large-scale network datasets, however,are too large and/or dynamic to be processed using main memory (e.g., email,tweets, wall posts). In this work, we formulate the problem of sampling fromlarge graph streams. We propose a streaming graph sampling algorithm thatdynamically maintains a representative sample in a reservoir based setting. Weevaluate the efficacy of our proposed methods empirically using severalreal-world data sets. Across all datasets, we found that our method producesamples that preserve better the original graph distributions.

Learning the Latent State Space of Time-Varying Graphs

  From social networks to Internet applications, a wide variety of electroniccommunication tools are producing streams of graph data; where the nodesrepresent users and the edges represent the contacts between them over time.This has led to an increased interest in mechanisms to model the dynamicstructure of time-varying graphs. In this work, we develop a framework forlearning the latent state space of a time-varying email graph. We show how theframework can be used to find subsequences that correspond to global real-timeevents in the Email graph (e.g. vacations, breaks, ...etc.). These eventsimpact the underlying graph process to make its characteristics non-stationary.Within the framework, we compare two different representations of the temporalrelationships; discrete vs. probabilistic. We use the two representations asinputs to a mixture model to learn the latent state transitions that correspondto important changes in the Email graph structure over time.

Using Bayesian Network Representations for Effective Sampling from  Generative Network Models

  Bayesian networks (BNs) are used for inference and sampling by exploitingconditional independence among random variables. Context specific independence(CSI) is a property of graphical models where additional independence relationsarise in the context of particular values of random variables (RVs).Identifying and exploiting CSI properties can simplify inference. Somegenerative network models (models that generate social/information networksamples from a network distribution P(G)), with complex interactions among aset of RVs, can be represented with probabilistic graphical models, inparticular with BNs. In the present work we show one such a case. We discusshow a mixed Kronecker Product Graph Model can be represented as a BN, and studyits BN properties that can be used for efficient sampling. Specifically, weshow that instead of exhibiting CSI properties, the model has deterministiccontext-specific dependence (DCSD). Exploiting this property focuses thesampling method on a subset of the sampling space that improves efficiency.

Size-Consistent Statistics for Anomaly Detection in Dynamic Networks

  An important task in network analysis is the detection of anomalous events ina network time series. These events could merely be times of interest in thenetwork timeline or they could be examples of malicious activity or networkmalfunction. Hypothesis testing using network statistics to summarize thebehavior of the network provides a robust framework for the anomaly detectiondecision process. Unfortunately, choosing network statistics that are dependenton confounding factors like the total number of nodes or edges can lead toincorrect conclusions (e.g., false positives and false negatives). In thisdissertation we describe the challenges that face anomaly detection in dynamicnetwork streams regarding confounding factors. We also provide two solutions toavoiding error due to confounding factors: the first is a randomization testingmethod that controls for confounding factors, and the second is a set ofsize-consistent network statistics which avoid confounding due to the mostcommon factors, edge count and node count.

Identifying User Survival Types via Clustering of Censored Social  Network Data

  The goal of cluster analysis in survival data is to identify clusters thatare decidedly associated with the survival outcome. Previous research hasexplored this problem primarily in the medical domain with relatively smalldatasets, but the need for such a clustering methodology could arise in otherdomains with large datasets, such as social networks. Concretely, we wish toidentify different survival classes in a social network by clustering the usersbased on their lifespan in the network. In this paper, we propose a decisiontree based algorithm that uses a global normalization of $p$-values to identifyclusters with significantly different survival distributions. We evaluate theclusters from our model with the help of a simple survival prediction task andshow that our model outperforms other competing methods.

Exploring Student Check-In Behavior for Improved Point-of-Interest  Prediction

  With the availability of vast amounts of user visitation history onlocation-based social networks (LBSN), the problem of Point-of-Interest (POI)prediction has been extensively studied. However, much of the research has beenconducted solely on voluntary checkin datasets collected from social apps suchas Foursquare or Yelp. While these data contain rich information aboutrecreational activities (e.g., restaurants, nightlife, and entertainment),information about more prosaic aspects of people's lives is sparse. This notonly limits our understanding of users' daily routines, but more importantlythe modeling assumptions developed based on characteristics of recreation-baseddata may not be suitable for richer check-in data. In this work, we present ananalysis of education "check-in" data using WiFi access logs collected atPurdue University. We propose a heterogeneous graph-based method to encode thecorrelations between users, POIs, and activities, and then jointly learnembeddings for the vertices. We evaluate our method compared to previousstate-of-the-art POI prediction methods, and show that the assumptions made byprevious methods significantly degrade performance on our data with dense(r)activity signals. We also show how our learned embeddings could be used toidentify similar students (e.g., for friend suggestions).

Community detection over a heterogeneous population of non-aligned  networks

  Clustering and community detection with multiple graphs have typicallyfocused on aligned graphs, where there is a mapping between nodes across thegraphs (e.g., multi-view, multi-layer, temporal graphs). However, there arenumerous application areas with multiple graphs that are only partiallyaligned, or even unaligned. These graphs are often drawn from the samepopulation, with communities of potentially different sizes that exhibitsimilar structure. In this paper, we develop a joint stochastic blockmodel(Joint SBM) to estimate shared communities across sets of heterogeneousnon-aligned graphs. We derive an efficient spectral clustering approach tolearn the parameters of the joint SBM. We evaluate the model on both syntheticand real-world datasets and show that the joint model is able to exploitcross-graph information to better estimate the communities compared to learningseparate SBMs on each individual graph.

Guided Data Repair

  In this paper we present GDR, a Guided Data Repair framework thatincorporates user feedback in the cleaning process to enhance and accelerateexisting automatic repair techniques while minimizing user involvement. GDRconsults the user on the updates that are most likely to be beneficial inimproving data quality. GDR also uses machine learning methods to identify andapply the correct updates directly to the database without the actualinvolvement of the user on these specific updates. To rank potential updatesfor consultation by the user, we first group these repairs and quantify theutility of each group using the decision-theory concept of value of information(VOI). We then apply active learning to order updates within a group based ontheir ability to improve the learned model. User feedback is used to repair thedatabase and to adaptively refine the training set for the model. Weempirically evaluate GDR on a real-world dataset and show significantimprovement in data quality using our user guided repairing process. We also,assess the trade-off between the user efforts and the resulting data quality.

Methods to Determine Node Centrality and Clustering in Graphs with  Uncertain Structure

  Much of the past work in network analysis has focused on analyzing discretegraphs, where binary edges represent the "presence" or "absence" of arelationship. Since traditional network measures (e.g., betweenness centrality)utilize a discrete link structure, complex systems must be transformed to thisrepresentation in order to investigate network properties. However, in manydomains there may be uncertainty about the relationship structure and anyuncertainty information would be lost in translation to a discreterepresentation. Uncertainty may arise in domains where there is moderating linkinformation that cannot be easily observed, i.e., links become inactive overtime but may not be dropped or observed links may not always corresponds to avalid relationship. In order to represent and reason with these types ofuncertainty, we move beyond the discrete graph framework and develop socialnetwork measures based on a probabilistic graph representation. Morespecifically, we develop measures of path length, betweenness centrality, andclustering coefficient---one set based on sampling and one based onprobabilistic paths. We evaluate our methods on three real-world networks fromEnron, Facebook, and DBLP, showing that our proposed methods more accuratelycapture salient effects without being susceptible to local noise, and that theresulting analysis produces a better understanding of the graph structure andthe uncertainty resulting from its change over time.

Role-Dynamics: Fast Mining of Large Dynamic Networks

  To understand the structural dynamics of a large-scale social, biological ortechnological network, it may be useful to discover behavioral rolesrepresenting the main connectivity patterns present over time. In this paper,we propose a scalable non-parametric approach to automatically learn thestructural dynamics of the network and individual nodes. Roles may representstructural or behavioral patterns such as the center of a star, peripheralnodes, or bridge nodes that connect different communities. Our novel approachlearns the appropriate structural role dynamics for any arbitrary network andtracks the changes over time. In particular, we uncover the specific globalnetwork dynamics and the local node dynamics of a technological, communication,and social network. We identify interesting node and network patterns such asstationary and non-stationary roles, spikes/steps in role-memberships (perhapsindicating anomalies), increasing/decreasing role trends, among many others.Our results indicate that the nodes in each of these networks have distinctconnectivity patterns that are non-stationary and evolve considerably overtime. Overall, the experiments demonstrate the effectiveness of our approachfor fast mining and tracking of the dynamics in large networks. Furthermore,the dynamic structural representation provides a basis for building moresophisticated models and tools that are fast for exploring large dynamicnetworks.

Transforming Graph Representations for Statistical Relational Learning

  Relational data representations have become an increasingly important topicdue to the recent proliferation of network datasets (e.g., social, biological,information networks) and a corresponding increase in the application ofstatistical relational learning (SRL) algorithms to these domains. In thisarticle, we examine a range of representation issues for graph-based relationaldata. Since the choice of relational data representation for the nodes, links,and features can dramatically affect the capabilities of SRL algorithms, wesurvey approaches and opportunities for relational representationtransformation designed to improve the performance of these algorithms. Thisleads us to introduce an intuitive taxonomy for data representationtransformations in relational domains that incorporates link transformation andnode transformation as symmetric representation tasks. In particular, thetransformation tasks for both nodes and links include (i) predicting theirexistence, (ii) predicting their label or type, (iii) estimating their weightor importance, and (iv) systematically constructing their relevant features. Wemotivate our taxonomy through detailed examples and use it to survey andcompare competing approaches for each of these tasks. We also discuss generalconditions for transforming links, nodes, and features. Finally, we highlightchallenges that remain to be addressed.

Network Sampling: From Static to Streaming Graphs

  Network sampling is integral to the analysis of social, information, andbiological networks. Since many real-world networks are massive in size,continuously evolving, and/or distributed in nature, the network structure isoften sampled in order to facilitate study. For these reasons, a more thoroughand complete understanding of network sampling is critical to support the fieldof network science. In this paper, we outline a framework for the generalproblem of network sampling, by highlighting the different objectives,population and units of interest, and classes of network sampling methods. Inaddition, we propose a spectrum of computational models for network samplingmethods, ranging from the traditionally studied model based on the assumptionof a static domain to a more challenging model that is appropriate forstreaming domains. We design a family of sampling methods based on the conceptof graph induction that generalize across the full spectrum of computationalmodels (from static to streaming) while efficiently preserving many of thetopological properties of the input graphs. Furthermore, we demonstrate howtraditional static sampling algorithms can be modified for graph streams foreach of the three main classes of sampling methods: node, edge, andtopology-based sampling. Our experimental results indicate that our proposedfamily of sampling methods more accurately preserves the underlying propertiesof the graph for both static and streaming graphs. Finally, we study the impactof network sampling algorithms on the parameter estimation and performanceevaluation of relational classification algorithms.

Graph Sample and Hold: A Framework for Big-Graph Analytics

  Sampling is a standard approach in big-graph analytics; the goal is toefficiently estimate the graph properties by consulting a sample of the wholepopulation. A perfect sample is assumed to mirror every property of the wholepopulation. Unfortunately, such a perfect sample is hard to collect in complexpopulations such as graphs (e.g. web graphs, social networks etc), where anunderlying network connects the units of the population. Therefore, a goodsample will be representative in the sense that graph properties of interestcan be estimated with a known degree of accuracy. While previous work focusedparticularly on sampling schemes used to estimate certain graph properties(e.g. triangle count), much less is known for the case when we need to estimatevarious graph properties with the same sampling scheme. In this paper, wepropose a generic stream sampling framework for big-graph analytics, calledGraph Sample and Hold (gSH). To begin, the proposed framework samples frommassive graphs sequentially in a single pass, one edge at a time, whilemaintaining a small state. We then show how to produce unbiased estimators forvarious graph properties from the sample. Given that the graph analysisalgorithms will run on a sample instead of the whole population, the runtimecomplexity of these algorithm is kept under control. Moreover, given that theestimators of graph properties are unbiased, the approximation error is keptunder control. Finally, we show the performance of the proposed framework (gSH)on various types of graphs, such as social graphs, among others.

Anomaly Detection in Dynamic Networks of Varying Size

  Dynamic networks, also called network streams, are an important datarepresentation that applies to many real-world domains. Many sets of networkdata such as e-mail networks, social networks, or internet traffic networks arebest represented by a dynamic network due to the temporal component of thedata. One important application in the domain of dynamic network analysis isanomaly detection. Here the task is to identify points in time where thenetwork exhibits behavior radically different from a typical time, either dueto some event (like the failure of machines in a computer network) or a shiftin the network properties. This problem is made more difficult by the fluidnature of what is considered "normal" network behavior. The volume of trafficon a network, for example, can change over the course of a month or even varybased on the time of the day without being considered unusual. Anomalydetection tests using traditional network statistics have difficulty in thesescenarios due to their Density Dependence: as the volume of edges changes thevalue of the statistics changes as well making it difficult to determine if thechange in signal is due to the traffic volume or due to some fundamental shiftin the behavior of the network. To more accurately detect anomalies in dynamicnetworks, we introduce the concept of Density-Consistent network statistics. Onsynthetically generated graphs anomaly detectors using these statistics show aa 20-400% improvement in the recall when distinguishing graphs drawn fromdifferent distributions. When applied to several real datasetsDensity-Consistent statistics recover multiple network events which standardstatistics failed to find.

Graphlet Decomposition: Framework, Algorithms, and Applications

  From social science to biology, numerous applications often rely on graphletsfor intuitive and meaningful characterization of networks at both the globalmacro-level as well as the local micro-level. While graphlets have witnessed atremendous success and impact in a variety of domains, there has yet to be afast and efficient approach for computing the frequencies of these subgraphpatterns. However, existing methods are not scalable to large networks withmillions of nodes and edges, which impedes the application of graphlets to newproblems that require large-scale network analysis. To address these problems,we propose a fast, efficient, and parallel algorithm for counting graphlets ofsize k={3,4}-nodes that take only a fraction of the time to compute whencompared with the current methods used. The proposed graphlet countingalgorithms leverages a number of proven combinatorial arguments for differentgraphlets. For each edge, we count a few graphlets, and with these counts alongwith the combinatorial arguments, we obtain the exact counts of others inconstant time. On a large collection of 300+ networks from a variety ofdomains, our graphlet counting strategies are on average 460x faster thancurrent methods. This brings new opportunities to investigate the use ofgraphlets on much larger networks and newer applications as we show in theexperiments. To the best of our knowledge, this paper provides the largestgraphlet computations to date as well as the largest systematic investigationon over 300+ networks from a variety of domains.

Combining Gradient Boosting Machines with Collective Inference to  Predict Continuous Values

  Gradient boosting of regression trees is a competitive procedure for learningpredictive models of continuous data that fits the data with an additivenon-parametric model. The classic version of gradient boosting assumes that thedata is independent and identically distributed. However, relational data withinterdependent, linked instances is now common and the dependencies in suchdata can be exploited to improve predictive performance. Collective inferenceis one approach to exploit relational correlation patterns and significantlyreduce classification error. However, much of the work on collective learningand inference has focused on discrete prediction tasks rather than continuous.%target values has not got that attention in terms of collective inference. Inthis work, we investigate how to combine these two paradigms together toimprove regression in relational domains. Specifically, we propose a boostingalgorithm for learning a collective inference model that predicts a continuoustarget variable. In the algorithm, we learn a basic relational model,collectively infer the target values, and then iteratively learn relationalmodels to predict the residuals. We evaluate our proposed algorithm on a realnetwork dataset and show that it outperforms alternative boosting methods.However, our investigation also revealed that the relational features interacttogether to produce better predictions.

Multi-level hypothesis testing for populations of heterogeneous networks

  In this work, we consider hypothesis testing and anomaly detection ondatasets where each observation is a weighted network. Examples of such datainclude brain connectivity networks from fMRI flow data, or word co-occurrencecounts for populations of individuals. Current approaches to hypothesis testingfor weighted networks typically requires thresholding the edge-weights, totransform the data to binary networks. This results in a loss of information,and outcomes are sensitivity to choice of threshold levels. Our work avoidsthis, and we consider weighted-graph observations in two situations, 1) whereeach graph belongs to one of two populations, and 2) where entities belong toone of two populations, with each entity possessing multiple graphs (indexede.g. by time). Specifically, we propose a hierarchical Bayesian hypothesistesting framework that models each population with a mixture of latent spacemodels for weighted networks, and then tests populations of networks fordifferences in distribution over components. Our framework is capable ofpopulation-level, entity-specific, as well as edge-specific hypothesis testing.We apply it to synthetic data and three real-world datasets: two social mediadatasets involving word co-occurrences from discussions on Twitter of thepolitical unrest in Brazil, and on Instagram concerning Attention DeficitHyperactivity Disorder (ADHD) and depression drugs, and one medical datasetinvolving fMRI brain-scans of human subjects. The results show that ourproposed method has lower Type I error and higher statistical power compared toalternatives that need to threshold the edge weights. Moreover, they show ourproposed method is better suited to deal with highly heterogeneous datasets.

ASPCAP: The Apogee Stellar Parameter and Chemical Abundances Pipeline

  The Apache Point Observatory Galactic Evolution Experiment (APOGEE) has builtthe largest moderately high-resolution (R=22, 500) spectroscopic map of thestars across the Milky Way, and including dust-obscured areas. The APOGEEStellar Parameter and Chemical Abundances Pipeline (ASPCAP) is the softwaredeveloped for the automated analysis of these spectra. ASPCAP determinesatmospheric parameters and chemical abundances from observed spectra bycomparing observed spectra to libraries of theoretical spectra, using chi-2minimization in a multidimensional parameter space. The package consists of afortran90 code that does the actual minimization, and a wrapper IDL code forbook-keeping and data handling. This paper explains in detail the ASPCAPcomponents and functionality, and presents results from a number of testsdesigned to check its performance. ASPCAP provides stellar effectivetemperatures, surface gravities, and metallicities precise to 2%, 0.1 dex, and0.05 dex, respectively, for most APOGEE stars, which are predominantly giants.It also provides abundances for up to 15 chemical elements with various levelsof precision, typically under 0.1 dex. The final data release (DR12) of theSloan Digital Sky Survey III contains an APOGEE database of more than 150,000stars. ASPCAP development continues in the SDSS-IV APOGEE-2 survey.

Fast Generation of Large Scale Social Networks with Clustering

  A key challenge within the social network literature is the problem ofnetwork generation - that is, how can we create synthetic networks that matchcharacteristics traditionally found in most real world networks? Importantcharacteristics that are present in social networks include a power law degreedistribution, small diameter and large amounts of clustering; however, mostcurrent network generators, such as the Chung Lu and Kronecker models, largelyignore the clustering present in a graph and choose to focus on preservingother network statistics, such as the power law distribution. Models such asthe exponential random graph model have a transitivity parameter, but arecomputationally difficult to learn, making scaling to large real world networksintractable. In this work, we propose an extension to the Chung Lu ran- domgraph model, the Transitive Chung Lu (TCL) model, which incorporates the notionof a random transitive edge. That is, with some probability it will choose toconnect to a node exactly two hops away, having been introduced to a 'friend ofa friend'. In all other cases it will follow the standard Chung Lu model,selecting a 'random surfer' from anywhere in the graph according to the giveninvariant distribution. We prove TCL's expected degree distribution is equal tothe degree distribution of the original graph, while being able to capture theclustering present in the network. The single parameter required by our modelcan be learned in seconds on graphs with millions of edges, while networks canbe generated in time that is linear in the number of edges. We demonstrate theperformance TCL on four real- world social networks, including an email datasetwith hundreds of thousands of nodes and millions of edges, showing TCLgenerates graphs that match the degree distribution, clustering coefficientsand hop plots of the original networks.

Abundances, Stellar Parameters, and Spectra From the SDSS-III/APOGEE  Survey

  The SDSS-III/APOGEE survey operated from 2011-2014 using the APOGEEspectrograph, which collects high-resolution (R~22,500), near-IR (1.51-1.70microns) spectra with a multiplexing (300 fiber-fed objects) capability. Wedescribe the survey data products that are publicly available, which includecatalogs with radial velocity, stellar parameters, and 15 elemental abundancesfor over 150,000 stars, as well as the more than 500,000 spectra from whichthese quantities are derived. Calibration relations for the stellar parameters(Teff, log g, [M/H], [alpha/M]) and abundances (C, N, O, Na, Mg, Al, Si, S, K,Ca, Ti, V, Mn, Fe, Ni) are presented and discussed. The internal scatter of theabundances within clusters indicates that abundance precision is generallybetween 0.05 and 0.09 dex across a broad temperature range; within more limitedranges and at high S/N, it is smaller for some elemental abundances. We assessthe accuracy of the abundances using comparison of mean cluster metallicitieswith literature values, APOGEE observations of the solar spectrum and ofArcturus, comparison of individual star abundances with other measurements, andconsideration of the locus of derived parameters and abundances of the entiresample, and find that it is challenging to determine the absolute abundancescale; external accuracy may be good to 0.1-0.2 dex. Uncertainties may belarger at cooler temperatures (Teff<4000K). Access to the public data releaseand data products is described, and some guidance for using the data productsis provided.

The Apache Point Observatory Galactic Evolution Experiment (APOGEE)

  The Apache Point Observatory Galactic Evolution Experiment (APOGEE), one ofthe programs in the Sloan Digital Sky Survey III (SDSS-III), has now completedits systematic, homogeneous spectroscopic survey sampling all major populationsof the Milky Way. After a three year observing campaign on the Sloan 2.5-mTelescope, APOGEE has collected a half million high resolution (R~22,500), highS/N (>100), infrared (1.51-1.70 microns) spectra for 146,000 stars, with timeseries information via repeat visits to most of these stars. This paperdescribes the motivations for the survey and its overall design---hardware,field placement, target selection, operations---and gives an overview of theseaspects as well as the data reduction, analysis and products. An index is alsogiven to the complement of technical papers that describe various criticalsurvey components in detail. Finally, we discuss the achieved surveyperformance and illustrate the variety of potential uses of the data productsby way of a number of science demonstrations, which span from time seriesanalysis of stellar spectral variations and radial velocity variations fromstellar companions, to spatial maps of kinematics, metallicity and abundancepatterns across the Galaxy and as a function of age, to new views of theinterstellar medium, the chemistry of star clusters, and the discovery of rarestellar species. As part of SDSS-III Data Release 12, all of the APOGEE dataproducts are now publicly available.

The Eleventh and Twelfth Data Releases of the Sloan Digital Sky Survey:  Final Data from SDSS-III

  The third generation of the Sloan Digital Sky Survey (SDSS-III) took datafrom 2008 to 2014 using the original SDSS wide-field imager, the original andan upgraded multi-object fiber-fed optical spectrograph, a new near-infraredhigh-resolution spectrograph, and a novel optical interferometer. All the datafrom SDSS-III are now made public. In particular, this paper describes DataRelease 11 (DR11) including all data acquired through 2013 July, and DataRelease 12 (DR12) adding data acquired through 2014 July (including all dataincluded in previous data releases), marking the end of SDSS-III observing.Relative to our previous public release (DR10), DR12 adds one million newspectra of galaxies and quasars from the Baryon Oscillation SpectroscopicSurvey (BOSS) over an additional 3000 sq. deg of sky, more than triples thenumber of H-band spectra of stars as part of the Apache Point Observatory (APO)Galactic Evolution Experiment (APOGEE), and includes repeated accurate radialvelocity measurements of 5500 stars from the Multi-Object APO Radial VelocityExoplanet Large-area Survey (MARVELS). The APOGEE outputs now include measuredabundances of 15 different elements for each star. In total, SDSS-III added2350 sq. deg of ugriz imaging; 155,520 spectra of 138,099 stars as part of theSloan Exploration of Galactic Understanding and Evolution 2 (SEGUE-2) survey;2,497,484 BOSS spectra of 1,372,737 galaxies, 294,512 quasars, and 247,216stars over 9376 sq. deg; 618,080 APOGEE spectra of 156,593 stars; and 197,040MARVELS spectra of 5,513 stars. Since its first light in 1998, SDSS has imagedover 1/3 of the Celestial sphere in five bands and obtained over five millionastronomical spectra.

