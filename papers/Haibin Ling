StructVIO : Visual-inertial Odometry with Structural Regularity of
  Man-made Environments

  We propose a novel visual-inertial odometry approach that adopts structural
regularity in man-made environments. Instead of using Manhattan world
assumption, we use Atlanta world model to describe such regularity. An Atlanta
world is a world that contains multiple local Manhattan worlds with different
heading directions. Each local Manhattan world is detected on-the-fly, and
their headings are gradually refined by the state estimator when new
observations are coming. With fully exploration of structural lines that
aligned with each local Manhattan worlds, our visual-inertial odometry method
become more accurate and robust, as well as much more flexible to different
kinds of complex man-made environments. Through extensive benchmark tests and
real-world tests, the results show that the proposed approach outperforms
existing visual-inertial systems in large-scale man-made environments


A Comparative Study of Object Trackers for Infrared Flying Bird Tracking

  Bird strikes present a huge risk for aircraft, especially since traditional
airport bird surveillance is mainly dependent on inefficient human observation.
Computer vision based technology has been proposed to automatically detect
birds, determine bird flying trajectories, and predict aircraft takeoff delays.
However, the characteristics of bird flight using imagery and the performance
of existing methods applied to flying bird task are not well known. Therefore,
we perform infrared flying bird tracking experiments using 12 state-of-the-art
algorithms on a real BIRDSITE-IR dataset to obtain useful clues and recommend
feature analysis. We also develop a Struck-scale method to demonstrate the
effectiveness of multiple scale sampling adaption in handling the object of
flying bird with varying shape and scale. The general analysis can be used to
develop specialized bird tracking methods for airport safety, wildness and
urban bird population studies.


SANet: Structure-Aware Network for Visual Tracking

  Convolutional neural network (CNN) has drawn increasing interest in visual
tracking owing to its powerfulness in feature extraction. Most existing
CNN-based trackers treat tracking as a classification problem. However, these
trackers are sensitive to similar distractors because their CNN models mainly
focus on inter-class classification. To address this problem, we use
self-structure information of object to distinguish it from distractors.
Specifically, we utilize recurrent neural network (RNN) to model object
structure, and incorporate it into CNN to improve its robustness to similar
distractors. Considering that convolutional layers in different levels
characterize the object from different perspectives, we use multiple RNNs to
model object structure in different levels respectively. Extensive experiments
on three benchmarks, OTB100, TC-128 and VOT2015, show that the proposed
algorithm outperforms other methods. Code is released at
http://www.dabi.temple.edu/~hbling/code/SANet/SANet.html.


Planar Object Tracking in the Wild: A Benchmark

  Planar object tracking is an actively studied problem in vision-based robotic
applications. While several benchmarks have been constructed for evaluating
state-of-the-art algorithms, there is a lack of video sequences captured in the
wild rather than in constrained laboratory environment. In this paper, we
present a carefully designed planar object tracking benchmark containing 210
videos of 30 planar objects sampled in the natural environment. In particular,
for each object, we shoot seven videos involving various challenging factors,
namely scale change, rotation, perspective distortion, motion blur, occlusion,
out-of-view, and unconstrained. The ground truth is carefully annotated
semi-manually to ensure the quality. Moreover, eleven state-of-the-art
algorithms are evaluated on the benchmark using two evaluation metrics, with
detailed analysis provided for the evaluation results. We expect the proposed
benchmark to benefit future studies on planar object tracking.


Dense Recurrent Neural Networks for Scene Labeling

  Recently recurrent neural networks (RNNs) have demonstrated the ability to
improve scene labeling through capturing long-range dependencies among image
units. In this paper, we propose dense RNNs for scene labeling by exploring
various long-range semantic dependencies among image units. In comparison with
existing RNN based approaches, our dense RNNs are able to capture richer
contextual dependencies for each image unit via dense connections between each
pair of image units, which significantly enhances their discriminative power.
Besides, to select relevant and meanwhile restrain irrelevant dependencies for
each unit from dense connections, we introduce an attention model into dense
RNNs. The attention model enables automatically assigning more importance to
helpful dependencies while less weight to unconcerned dependencies. Integrating
with convolutional neural networks (CNNs), our method achieves state-of-the-art
performances on the PASCAL Context, MIT ADE20K and SiftFlow benchmarks.


A Single-shot-per-pose Camera-Projector Calibration System For Imperfect
  Planar Targets

  Existing camera-projector calibration methods typically warp feature points
from a camera image to a projector image using estimated homographies, and
often suffer from errors in camera parameters and noise due to imperfect
planarity of the calibration target. In this paper we propose a simple yet
robust solution that explicitly deals with these challenges. Following the
structured light (SL) camera-project calibration framework, a carefully
designed correspondence algorithm is built on top of the De Bruijn patterns.
Such correspondence is then used for initial camera-projector calibration.
Then, to gain more robustness against noises, especially those from an
imperfect planar calibration board, a bundle adjustment algorithm is developed
to jointly optimize the estimated camera and projector models. Aside from the
robustness, our solution requires only one shot of SL pattern for each
calibration board pose, which is much more convenient than multi-shot solutions
in practice. Data validations are conducted on both synthetic and real
datasets, and our method shows clear advantages over existing methods in all
experiments.


Vision Meets Drones: A Challenge

  In this paper we present a large-scale visual object detection and tracking
benchmark, named VisDrone2018, aiming at advancing visual understanding tasks
on the drone platform. The images and video sequences in the benchmark were
captured over various urban/suburban areas of 14 different cities across China
from north to south. Specifically, VisDrone2018 consists of 263 video clips and
10,209 images (no overlap with video clips) with rich annotations, including
object bounding boxes, object categories, occlusion, truncation ratios, etc.
With intensive amount of effort, our benchmark has more than 2.5 million
annotated instances in 179,264 images/video frames. Being the largest such
dataset ever published, the benchmark enables extensive evaluation and
investigation of visual analysis algorithms on the drone platform. In
particular, we design four popular tasks with the benchmark, including object
detection in images, object detection in videos, single object tracking, and
multi-object tracking. All these tasks are extremely challenging in the
proposed dataset due to factors such as occlusion, large scale and pose
variation, and fast motion. We hope the benchmark largely boost the research
and development in visual analysis on drone platforms.


Privacy-Protective-GAN for Face De-identification

  Face de-identification has become increasingly important as the image sources
are explosively growing and easily accessible. The advance of new face
recognition techniques also arises people's concern regarding the privacy
leakage. The mainstream pipelines of face de-identification are mostly based on
the k-same framework, which bears critiques of low effectiveness and poor
visual quality. In this paper, we propose a new framework called
Privacy-Protective-GAN (PP-GAN) that adapts GAN with novel verificator and
regulator modules specially designed for the face de-identification problem to
ensure generating de-identified output with retained structure similarity
according to a single input. We evaluate the proposed approach in terms of
privacy protection, utility preservation, and structure similarity. Our
approach not only outperforms existing face de-identification techniques but
also provides a practical framework of adapting GAN with priors of domain
knowledge.


Scene Parsing via Dense Recurrent Neural Networks with Attentional
  Selection

  Recurrent neural networks (RNNs) have shown the ability to improve scene
parsing through capturing long-range dependencies among image units. In this
paper, we propose dense RNNs for scene labeling by exploring various long-range
semantic dependencies among image units. Different from existing RNN based
approaches, our dense RNNs are able to capture richer contextual dependencies
for each image unit by enabling immediate connections between each pair of
image units, which significantly enhances their discriminative power. Besides,
to select relevant dependencies and meanwhile to restrain irrelevant ones for
each unit from dense connections, we introduce an attention model into dense
RNNs. The attention model allows automatically assigning more importance to
helpful dependencies while less weight to unconcerned dependencies. Integrating
with convolutional neural networks (CNNs), we develop an end-to-end scene
labeling system. Extensive experiments on three large-scale benchmarks
demonstrate that the proposed approach can improve the baselines by large
margins and outperform other state-of-the-art algorithms.


Feature Pyramid and Hierarchical Boosting Network for Pavement Crack
  Detection

  Pavement crack detection is a critical task for insuring road safety. Manual
crack detection is extremely time-consuming. Therefore, an automatic road crack
detection method is required to boost this progress. However, it remains a
challenging task due to the intensity inhomogeneity of cracks and complexity of
the background, e.g., the low contrast with surrounding pavements and possible
shadows with similar intensity. Inspired by recent advances of deep learning in
computer vision, we propose a novel network architecture, named Feature Pyramid
and Hierarchical Boosting Network (FPHBN), for pavement crack detection. The
proposed network integrates semantic information to low-level features for
crack detection in a feature pyramid way. And, it balances the contribution of
both easy and hard samples to loss by nested sample reweighting in a
hierarchical way. To demonstrate the superiority and generality of the proposed
method, we evaluate the proposed method on five crack datasets and compare it
with state-of-the-art crack detection, edge detection, semantic segmentation
methods. Extensive experiments show that the proposed method outperforms these
state-of-the-art methods in terms of accuracy and generality.


Generic Multiview Visual Tracking

  Recent progresses in visual tracking have greatly improved the tracking
performance. However, challenges such as occlusion and view change remain
obstacles in real world deployment. A natural solution to these challenges is
to use multiple cameras with multiview inputs, though existing systems are
mostly limited to specific targets (e.g. human), static cameras, and/or camera
calibration. To break through these limitations, we propose a generic multiview
tracking (GMT) framework that allows camera movement, while requiring neither
specific object model nor camera calibration. A key innovation in our framework
is a cross-camera trajectory prediction network (TPN), which implicitly and
dynamically encodes camera geometric relations, and hence addresses missing
target issues such as occlusion. Moreover, during tracking, we assemble
information across different cameras to dynamically update a novel
collaborative correlation filter (CCF), which is shared among cameras to
achieve robustness against view change. The two components are integrated into
a correlation filter tracking framework, where the features are trained offline
using existing single view tracking datasets. For evaluation, we first
contribute a new generic multiview tracking dataset (GMTD) with careful
annotations, and then run experiments on GMTD and the PETS2009 datasets. On
both datasets, the proposed GMT algorithm shows clear advantages over
state-of-the-art ones.


FAMNet: Joint Learning of Feature, Affinity and Multi-dimensional
  Assignment for Online Multiple Object Tracking

  Data association-based multiple object tracking (MOT) involves multiple
separated modules processed or optimized differently, which results in complex
method design and requires non-trivial tuning of parameters. In this paper, we
present an end-to-end model, named FAMNet, where Feature extraction, Affinity
estimation and Multi-dimensional assignment are refined in a single network.
All layers in FAMNet are designed differentiable thus can be optimized jointly
to learn the discriminative features and higher-order affinity model for robust
MOT, which is supervised by the loss directly from the assignment ground truth.
We also integrate single object tracking technique and a dedicated target
management scheme into the FAMNet-based tracking system to further recover
false negatives and inhibit noisy target candidates generated by the external
detector. The proposed method is evaluated on a diverse set of benchmarks
including MOT2015, MOT2017, KITTI-Car and UA-DETRAC, and achieves promising
performance on all of them in comparison with state-of-the-arts.


A Richly Annotated Dataset for Pedestrian Attribute Recognition

  In this paper, we aim to improve the dataset foundation for pedestrian
attribute recognition in real surveillance scenarios. Recognition of human
attributes, such as gender, and clothes types, has great prospects in real
applications. However, the development of suitable benchmark datasets for
attribute recognition remains lagged behind. Existing human attribute datasets
are collected from various sources or an integration of pedestrian
re-identification datasets. Such heterogeneous collection poses a big challenge
on developing high quality fine-grained attribute recognition algorithms.
Furthermore, human attribute recognition are generally severely affected by
environmental or contextual factors, such as viewpoints, occlusions and body
parts, while existing attribute datasets barely care about them. To tackle
these problems, we build a Richly Annotated Pedestrian (RAP) dataset from real
multi-camera surveillance scenarios with long term collection, where data
samples are annotated with not only fine-grained human attributes but also
environmental and contextual factors. RAP has in total 41,585 pedestrian
samples, each of which is annotated with 72 attributes as well as viewpoints,
occlusions, body parts information. To our knowledge, the RAP dataset is the
largest pedestrian attribute dataset, which is expected to greatly promote the
study of large-scale attribute recognition systems. Furthermore, we empirically
analyze the effects of different environmental and contextual factors on
pedestrian attribute recognition. Experimental results demonstrate that
viewpoints, occlusions and body parts information could assist attribute
recognition a lot in real applications.


Adaptive Objectness for Object Tracking

  Object tracking is a long standing problem in vision. While great efforts
have been spent to improve tracking performance, a simple yet reliable prior
knowledge is left unexploited: the target object in tracking must be an object
other than non-object. The recently proposed and popularized objectness measure
provides a natural way to model such prior in visual tracking. Thus motivated,
in this paper we propose to adapt objectness for visual object tracking.
Instead of directly applying an existing objectness measure that is generic and
handles various objects and environments, we adapt it to be compatible to the
specific tracking sequence and object. More specifically, we use the newly
proposed BING objectness as the base, and then train an object-adaptive
objectness for each tracking task. The training is implemented by using an
adaptive support vector machine that integrates information from the specific
tracking target into the BING measure. We emphasize that the benefit of the
proposed adaptive objectness, named ADOBING, is generic. To show this, we
combine ADOBING with seven top performed trackers in recent evaluations. We run
the ADOBING-enhanced trackers with their base trackers on two popular
benchmarks, the CVPR2013 benchmark (50 sequences) and the Princeton Tracking
Benchmark (100 sequences). On both benchmarks, our methods not only
consistently improve the base trackers, but also achieve the best known
performances. Noting that the way we integrate objectness in visual tracking is
generic and straightforward, we expect even more improvement by using
tracker-specific objectness.


DeepSaliency: Multi-Task Deep Neural Network Model for Salient Object
  Detection

  A key problem in salient object detection is how to effectively model the
semantic properties of salient objects in a data-driven manner. In this paper,
we propose a multi-task deep saliency model based on a fully convolutional
neural network (FCNN) with global input (whole raw images) and global output
(whole saliency maps). In principle, the proposed saliency model takes a
data-driven strategy for encoding the underlying saliency prior information,
and then sets up a multi-task learning scheme for exploring the intrinsic
correlations between saliency detection and semantic image segmentation.
Through collaborative feature learning from such two correlated tasks, the
shared fully convolutional layers produce effective features for object
perception. Moreover, it is capable of capturing the semantic information on
salient objects across different levels using the fully convolutional layers,
which investigate the feature-sharing properties of salient object detection
with great feature redundancy reduction. Finally, we present a graph Laplacian
regularized nonlinear regression model for saliency refinement. Experimental
results demonstrate the effectiveness of our approach in comparison with the
state-of-the-art approaches.


Multi-level Contextual RNNs with Attention Model for Scene Labeling

  Context in image is crucial for scene labeling while existing methods only
exploit local context generated from a small surrounding area of an image patch
or a pixel, by contrast long-range and global contextual information is
ignored. To handle this issue, we in this work propose a novel approach for
scene labeling by exploring multi-level contextual recurrent neural networks
(ML-CRNNs). Specifically, we encode three kinds of contextual cues, i.e., local
context, global context and image topic context in structural recurrent neural
networks (RNNs) to model long-range local and global dependencies in image. In
this way, our method is able to `see' the image in terms of both long-range
local and holistic views, and make a more reliable inference for image
labeling. Besides, we integrate the proposed contextual RNNs into hierarchical
convolutional neural networks (CNNs), and exploit dependence relationships in
multiple levels to provide rich spatial and semantic information. Moreover, we
novelly adopt an attention model to effectively merge multiple levels and show
that it outperforms average- or max-pooling fusion strategies. Extensive
experiments demonstrate that the proposed approach achieves new
state-of-the-art results on the CamVid, SiftFlow and Stanford-background
datasets.


Transductive Zero-Shot Learning with a Self-training dictionary approach

  As an important and challenging problem in computer vision, zero-shot
learning (ZSL) aims at automatically recognizing the instances from unseen
object classes without training data. To address this problem, ZSL is usually
carried out in the following two aspects: 1) capturing the domain distribution
connections between seen classes data and unseen classes data; and 2) modeling
the semantic interactions between the image feature space and the label
embedding space. Motivated by these observations, we propose a bidirectional
mapping based semantic relationship modeling scheme that seeks for crossmodal
knowledge transfer by simultaneously projecting the image features and label
embeddings into a common latent space. Namely, we have a bidirectional
connection relationship that takes place from the image feature space to the
latent space as well as from the label embedding space to the latent space. To
deal with the domain shift problem, we further present a transductive learning
approach that formulates the class prediction problem in an iterative refining
process, where the object classification capacity is progressively reinforced
through bootstrapping-based model updating over highly reliable instances.
Experimental results on three benchmark datasets (AwA, CUB and SUN) demonstrate
the effectiveness of the proposed approach against the state-of-the-art
approaches.


Parallel Tracking and Verifying: A Framework for Real-Time and High
  Accuracy Visual Tracking

  Being intensively studied, visual tracking has seen great recent advances in
either speed (e.g., with correlation filters) or accuracy (e.g., with deep
features). Real-time and high accuracy tracking algorithms, however, remain
scarce. In this paper we study the problem from a new perspective and present a
novel parallel tracking and verifying (PTAV) framework, by taking advantage of
the ubiquity of multi-thread techniques and borrowing from the success of
parallel tracking and mapping in visual SLAM. Our PTAV framework typically
consists of two components, a tracker T and a verifier V, working in parallel
on two separate threads. The tracker T aims to provide a super real-time
tracking inference and is expected to perform well most of the time; by
contrast, the verifier V checks the tracking results and corrects T when
needed. The key innovation is that, V does not work on every frame but only
upon the requests from T; on the other end, T may adjust the tracking according
to the feedback from V. With such collaboration, PTAV enjoys both the high
efficiency provided by T and the strong discriminative power by V. In our
extensive experiments on popular benchmarks including OTB2013, OTB2015, TC128
and UAV20L, PTAV achieves the best tracking accuracy among all real-time
trackers, and in fact performs even better than many deep learning based
solutions. Moreover, as a general framework, PTAV is very flexible and has
great rooms for improvement and generalization.


Parallel Tracking and Verifying

  Being intensively studied, visual object tracking has witnessed great
advances in either speed (e.g., with correlation filters) or accuracy (e.g.,
with deep features). Real-time and high accuracy tracking algorithms, however,
remain scarce. In this paper we study the problem from a new perspective and
present a novel parallel tracking and verifying (PTAV) framework, by taking
advantage of the ubiquity of multi-thread techniques and borrowing ideas from
the success of parallel tracking and mapping in visual SLAM. The proposed PTAV
framework is typically composed of two components, a (base) tracker T and a
verifier V, working in parallel on two separate threads. The tracker T aims to
provide a super real-time tracking inference and is expected to perform well
most of the time; by contrast, the verifier V validates the tracking results
and corrects T when needed. The key innovation is that, V does not work on
every frame but only upon the requests from T; on the other end, T may adjust
the tracking according to the feedback from V. With such collaboration, PTAV
enjoys both the high efficiency provided by T and the strong discriminative
power by V. Meanwhile, to adapt V to object appearance changes over time, we
maintain a dynamic target template pool for adaptive verification, resulting in
further performance improvements. In our extensive experiments on popular
benchmarks including OTB2015, TC128, UAV20L and VOT2016, PTAV achieves the best
tracking accuracy among all real-time trackers, and in fact even outperforms
many deep learning based algorithms. Moreover, as a general framework, PTAV is
very flexible with great potentials for future improvement and generalization.


Weighted Bilinear Coding over Salient Body Parts for Person
  Re-identification

  Deep convolutional neural networks (CNNs) have demonstrated dominant
performance in person re-identification (Re-ID). Existing CNN based methods
utilize global average pooling (GAP) to aggregate intermediate convolutional
features for Re-ID. However, this strategy only considers the first-order
statistics of local features and treats local features at different locations
equally important, leading to sub-optimal feature representation. To deal with
these issues, we propose a novel \emph{weighted bilinear coding} (WBC) model
for local feature aggregation in CNN networks to pursue more representative and
discriminative feature representations. In specific, bilinear coding is used to
encode the channel-wise feature correlations to capture richer feature
interactions. Meanwhile, a weighting scheme is applied on the bilinear coding
to adaptively adjust the weights of local features at different locations based
on their importance in recognition, further improving the discriminability of
feature aggregation. To handle the spatial misalignment issue, we use a salient
part net to derive salient body parts, and apply the WBC model on each part.
The final representation, formed by concatenating the WBC eoncoded features of
each part, is both discriminative and resistant to spatial misalignment.
Experiments on three benchmarks including Market-1501, DukeMTMC-reID and CUHK03
evidence the favorable performance of our method against other state-of-the-art
methods.


Graph Correspondence Transfer for Person Re-identification

  In this paper, we propose a graph correspondence transfer (GCT) approach for
person re-identification. Unlike existing methods, the GCT model formulates
person re-identification as an off-line graph matching and on-line
correspondence transferring problem. In specific, during training, the GCT
model aims to learn off-line a set of correspondence templates from positive
training pairs with various pose-pair configurations via patch-wise graph
matching. During testing, for each pair of test samples, we select a few
training pairs with the most similar pose-pair configurations as references,
and transfer the correspondences of these references to test pair for feature
distance calculation. The matching score is derived by aggregating distances
from different references. For each probe image, the gallery image with the
highest matching score is the re-identifying result. Compared to existing
algorithms, our GCT can handle spatial misalignment caused by large variations
in view angles and human poses owing to the benefits of patch-wise graph
matching. Extensive experiments on five benchmarks including VIPeR, Road,
PRID450S, 3DPES and CUHK01 evidence the superior performance of GCT model over
other state-of-the-art methods.


MTFH: A Matrix Tri-Factorization Hashing Framework for Efficient
  Cross-Modal Retrieval

  Hashing has recently sparked a great revolution in cross-modal retrieval due
to its low storage cost and high query speed. Most existing cross-modal hashing
methods learn unified hash codes in a common Hamming space to represent all
multi-modal data and make them intuitively comparable. However, such unified
hash codes could inherently sacrifice their representation scalability because
the data from different modalities may not have one-to-one correspondence and
could be stored more efficiently by different hash codes of unequal lengths. To
mitigate this problem, this paper proposes a generalized and flexible
cross-modal hashing framework, termed Matrix Tri-Factorization Hashing (MTFH),
which not only preserves the semantic similarity between the multi-modal data
points, but also works seamlessly in various settings including paired or
unpaired multi-modal data, and equal or varying hash length encoding scenarios.
Specifically, MTFH exploits an efficient objective function to jointly learn
the flexible modality-specific hash codes with different length settings, while
simultaneously excavating two semantic correlation matrices to ensure
heterogeneous data comparable. As a result, the derived hash codes are more
semantically meaningful for various challenging cross-modal retrieval tasks.
Extensive experiments evaluated on public benchmark datasets highlight the
superiority of MTFH under various retrieval scenarios and show its very
competitive performance with the state-of-the-arts.


LaSOT: A High-quality Benchmark for Large-scale Single Object Tracking

  In this paper, we present LaSOT, a high-quality benchmark for Large-scale
Single Object Tracking. LaSOT consists of 1,400 sequences with more than 3.5M
frames in total. Each frame in these sequences is carefully and manually
annotated with a bounding box, making LaSOT the largest, to the best of our
knowledge, densely annotated tracking benchmark. The average video length of
LaSOT is more than 2,500 frames, and each sequence comprises various challenges
deriving from the wild where target objects may disappear and re-appear again
in the view. By releasing LaSOT, we expect to provide the community with a
large-scale dedicated benchmark with high quality for both the training of deep
trackers and the veritable evaluation of tracking algorithms. Moreover,
considering the close connections of visual appearance and natural language, we
enrich LaSOT by providing additional language specification, aiming at
encouraging the exploration of natural linguistic feature for tracking. A
thorough experimental evaluation of 35 tracking algorithms on LaSOT is
presented with detailed analysis, and the results demonstrate that there is
still a big room for improvements.


Siamese Cascaded Region Proposal Networks for Real-Time Visual Tracking

  Region proposal networks (RPN) have been recently combined with the Siamese
network for tracking, and shown excellent accuracy with high efficiency.
Nevertheless, previously proposed one-stage Siamese-RPN trackers degenerate in
presence of similar distractors and large scale variation. Addressing these
issues, we propose a multi-stage tracking framework, Siamese Cascaded RPN
(C-RPN), which consists of a sequence of RPNs cascaded from deep high-level to
shallow low-level layers in a Siamese network. Compared to previous solutions,
C-RPN has several advantages: (1) Each RPN is trained using the outputs of RPN
in the previous stage. Such process stimulates hard negative sampling,
resulting in more balanced training samples. Consequently, the RPNs are
sequentially more discriminative in distinguishing difficult background (i.e.,
similar distractors). (2) Multi-level features are fully leveraged through a
novel feature transfer block (FTB) for each RPN, further improving the
discriminability of C-RPN using both high-level semantic and low-level spatial
information. (3) With multiple steps of regressions, C-RPN progressively
refines the location and shape of the target in each RPN with adjusted anchor
boxes in the previous stage, which makes localization more accurate. C-RPN is
trained end-to-end with the multi-task loss function. In inference, C-RPN is
deployed as it is, without any temporal adaption, for real-time tracking. In
extensive experiments on OTB-2013, OTB-2015, VOT-2016, VOT-2017, LaSOT and
TrackingNet, C-RPN consistently achieves state-of-the-art results and runs in
real-time.


Online Multi-Object Tracking with Instance-Aware Tracker and Dynamic
  Model Refreshment

  Recent progresses in model-free single object tracking (SOT) algorithms have
largely inspired applying SOT to \emph{multi-object tracking} (MOT) to improve
the robustness as well as relieving dependency on external detector. However,
SOT algorithms are generally designed for distinguishing a target from its
environment, and hence meet problems when a target is spatially mixed with
similar objects as observed frequently in MOT. To address this issue, in this
paper we propose an instance-aware tracker to integrate SOT techniques for MOT
by encoding awareness both within and between target models. In particular, we
construct each target model by fusing information for distinguishing target
both from background and other instances (tracking targets). To conserve
uniqueness of all target models, our instance-aware tracker considers response
maps from all target models and assigns spatial locations exclusively to
optimize the overall accuracy. Another contribution we make is a dynamic model
refreshing strategy learned by a convolutional neural network. This strategy
helps to eliminate initialization noise as well as to adapt to the variation of
target size and appearance. To show the effectiveness of the proposed approach,
it is evaluated on the popular MOT15 and MOT16 challenge benchmarks. On both
benchmarks, our approach achieves the best overall performances in comparison
with published results.


PFLD: A Practical Facial Landmark Detector

  Being accurate, efficient, and compact is essential to a facial landmark
detector for practical use. To simultaneously consider the three concerns, this
paper investigates a neat model with promising detection accuracy under wild
environments e.g., unconstrained pose, expression, lighting, and occlusion
conditions) and super real-time speed on a mobile device. More concretely, we
customize an end-to-end single stage network associated with acceleration
techniques. During the training phase, for each sample, rotation information is
estimated for geometrically regularizing landmark localization, which is then
NOT involved in the testing phase. A novel loss is designed to, besides
considering the geometrical regularization, mitigate the issue of data
imbalance by adjusting weights of samples to different states, such as large
pose, extreme lighting, and occlusion, in the training set. Extensive
experiments are conducted to demonstrate the efficacy of our design and reveal
its superior performance over state-of-the-art alternatives on widely-adopted
challenging benchmarks, i.e., 300W (including iBUG, LFPW, AFW, HELEN, and
XM2VTS) and AFLW. Our model can be merely 2.1Mb of size and reach over 140 fps
per face on a mobile phone (Qualcomm ARM 845 processor) with high precision,
making it attractive for large-scale or real-time applications. We have made
our practical system based on PFLD 0.25X model publicly available at
\url{http://sites.google.com/view/xjguo/fld} for encouraging comparisons and
improvements from the community.


End-to-end Projector Photometric Compensation

  Projector photometric compensation aims to modify a projector input image
such that it can compensate for disturbance from the appearance of projection
surface. In this paper, for the first time, we formulate the compensation
problem as an end-to-end learning problem and propose a convolutional neural
network, named CompenNet, to implicitly learn the complex compensation
function. CompenNet consists of a UNet-like backbone network and an autoencoder
subnet. Such architecture encourages rich multi-level interactions between the
camera-captured projection surface image and the input image, and thus captures
both photometric and environment information of the projection surface. In
addition, the visual details and interaction information are carried to deeper
layers along the multi-level skip convolution layers. The architecture is of
particular importance for the projector compensation task, for which only a
small training dataset is allowed in practice. Another contribution we make is
a novel evaluation benchmark, which is independent of system setup and thus
quantitatively verifiable. Such benchmark is not previously available, to our
best knowledge, due to the fact that conventional evaluation requests the
hardware system to actually project the final results. Our key idea, motivated
from our end-to-end problem formulation, is to use a reasonable surrogate to
avoid such projection process so as to be setup-independent. Our method is
evaluated carefully on the benchmark, and the results show that our end-to-end
learning solution outperforms state-of-the-arts both qualitatively and
quantitatively by a significant margin.


M2Det: A Single-Shot Object Detector based on Multi-Level Feature
  Pyramid Network

  Feature pyramids are widely exploited by both the state-of-the-art one-stage
object detectors (e.g., DSSD, RetinaNet, RefineDet) and the two-stage object
detectors (e.g., Mask R-CNN, DetNet) to alleviate the problem arising from
scale variation across object instances. Although these object detectors with
feature pyramids achieve encouraging results, they have some limitations due to
that they only simply construct the feature pyramid according to the inherent
multi-scale, pyramidal architecture of the backbones which are actually
designed for object classification task. Newly, in this work, we present a
method called Multi-Level Feature Pyramid Network (MLFPN) to construct more
effective feature pyramids for detecting objects of different scales. First, we
fuse multi-level features (i.e. multiple layers) extracted by backbone as the
base feature. Second, we feed the base feature into a block of alternating
joint Thinned U-shape Modules and Feature Fusion Modules and exploit the
decoder layers of each u-shape module as the features for detecting objects.
Finally, we gather up the decoder layers with equivalent scales (sizes) to
develop a feature pyramid for object detection, in which every feature map
consists of the layers (features) from multiple levels. To evaluate the
effectiveness of the proposed MLFPN, we design and train a powerful end-to-end
one-stage object detector we call M2Det by integrating it into the architecture
of SSD, which gets better detection performance than state-of-the-art one-stage
detectors. Specifically, on MS-COCO benchmark, M2Det achieves AP of 41.0 at
speed of 11.8 FPS with single-scale inference strategy and AP of 44.2 with
multi-scale inference strategy, which is the new state-of-the-art results among
one-stage detectors. The code will be made available on
\url{https://github.com/qijiezhao/M2Det.


Robust and Efficient Graph Correspondence Transfer for Person
  Re-identification

  Spatial misalignment caused by variations in poses and viewpoints is one of
the most critical issues that hinders the performance improvement in existing
person re-identification (Re-ID) algorithms. To address this problem, in this
paper, we present a robust and efficient graph correspondence transfer (REGCT)
approach for explicit spatial alignment in Re-ID. Specifically, we propose to
establish the patch-wise correspondences of positive training pairs via graph
matching. By exploiting both spatial and visual contexts of human appearance in
graph matching, meaningful semantic correspondences can be obtained. To
circumvent the cumbersome \emph{on-line} graph matching in testing phase, we
propose to transfer the \emph{off-line} learned patch-wise correspondences from
the positive training pairs to test pairs. In detail, for each test pair, the
training pairs with similar pose-pair configurations are selected as
references. The matching patterns (i.e., the correspondences) of the selected
references are then utilized to calculate the patch-wise feature distances of
this test pair. To enhance the robustness of correspondence transfer, we design
a novel pose context descriptor to accurately model human body configurations,
and present an approach to measure the similarity between a pair of pose
context descriptors. Meanwhile, to improve testing efficiency, we propose a
correspondence template ensemble method using the voting mechanism, which
significantly reduces the amount of patch-wise matchings involved in distance
calculation. With aforementioned strategies, the REGCT model can effectively
and efficiently handle the spatial misalignment problem in Re-ID. Extensive
experiments on five challenging benchmarks, including VIPeR, Road, PRID450S,
3DPES and CUHK01, evidence the superior performance of REGCT over other
state-of-the-art approaches.


