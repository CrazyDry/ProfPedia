How transferable are the datasets collected by active learners?

  Active learning is a widely-used training strategy for maximizing predictiveperformance subject to a fixed annotation budget. Between rounds of training,an active learner iteratively selects examples for annotation, typically basedon some measure of the model's uncertainty, coupling the acquired dataset withthe underlying model. However, owing to the high cost of annotation and therapid pace of model development, labeled datasets may remain valuable longafter a particular model is surpassed by new technology. In this paper, weinvestigate the transferability of datasets collected with an acquisition modelA to a distinct successor model S. We seek to characterize whether the benefitsof active learning persist when A and S are different models. To this end, weconsider two standard NLP tasks and associated datasets: text classificationand sequence tagging. We find that training S on a dataset actively acquiredwith a (different) model A typically yields worse performance than when S istrained with "native" data (i.e., acquired actively using S), and oftenperforms worse than training on i.i.d. sampled data. These findings haveimplications for the use of active learning in practice,suggesting that it isbetter suited to cases where models are updated no more frequently than labeleddata.

Rationale-Augmented Convolutional Neural Networks for Text  Classification

  We present a new Convolutional Neural Network (CNN) model for textclassification that jointly exploits labels on documents and their componentsentences. Specifically, we consider scenarios in which annotators explicitlymark sentences (or snippets) that support their overall documentcategorization, i.e., they provide rationales. Our model exploits suchsupervision via a hierarchical approach in which each document is representedby a linear combination of the vector representations of its componentsentences. We propose a sentence-level convolutional model that estimates theprobability that a given sentence is a rationale, and we then scale thecontribution of each sentence to the aggregate document representation inproportion to these estimates. Experiments on five classification datasets thathave document labels and associated rationales demonstrate that our approachconsistently outperforms strong baselines. Moreover, our model naturallyprovides explanations for its predictions.

Modelling Context with User Embeddings for Sarcasm Detection in Social  Media

  We introduce a deep neural network for automated sarcasm detection. Recentwork has emphasized the need for models to capitalize on contextual features,beyond lexical and syntactic cues present in utterances. For example, differentspeakers will tend to employ sarcasm regarding different subjects and, thus,sarcasm detection models ought to encode such speaker information. Currentmethods have achieved this by way of laborious feature engineering. Bycontrast, we propose to automatically learn and then exploit user embeddings,to be used in concert with lexical signals to recognize sarcasm. Our approachdoes not require elaborate feature engineering (and concomitant data scraping);fitting user embeddings requires only the text from their previous posts. Theexperimental results show that our model outperforms a state-of-the-artapproach leveraging an extensive set of carefully crafted features.

Crowdsourcing Information Extraction for Biomedical Systematic Reviews

  Information extraction is a critical step in the practice of conductingbiomedical systematic literature reviews. Extracted structured data can beaggregated via methods such as statistical meta-analysis. Typically highlytrained domain experts extract data for systematic reviews. The high expense ofconducting biomedical systematic reviews has motivated researchers to explorelower cost methods that achieve similar rigor without compromising quality.Crowdsourcing represents one such promising approach. In this work-in-progressstudy, we designed a crowdsourcing task for biomedical information extraction.We briefly report the iterative design process and the results of two pilottestings. We found that giving more concrete examples in the task instructioncan help workers better understand the task, especially for concepts that areabstract and confusing. We found a few workers completed most of the work, andour payment level appeared more attractive to workers from low-incomecountries. In the future, we will further evaluate our results with referenceto gold standard extractions, thus assessing the feasibility of tasking crowdworkers with extracting biomedical intervention information for systematicreviews.

Exploiting Domain Knowledge via Grouped Weight Sharing with Application  to Text Categorization

  A fundamental advantage of neural models for NLP is their ability to learnrepresentations from scratch. However, in practice this often means ignoringexisting external linguistic resources, e.g., WordNet or domain specificontologies such as the Unified Medical Language System (UMLS). We propose ageneral, novel method for exploiting such resources via weight sharing. Priorwork on weight sharing in neural networks has considered it largely as a meansof model compression. In contrast, we treat weight sharing as a flexiblemechanism for incorporating prior knowledge into neural models. We show thatthis approach consistently yields improved performance on classification taskscompared to baseline strategies that do not exploit weight sharing.

Retrofitting Concept Vector Representations of Medical Concepts to  Improve Estimates of Semantic Similarity and Relatedness

  Estimation of semantic similarity and relatedness between biomedical conceptshas utility for many informatics applications. Automated methods fall into twocategories: methods based on distributional statistics drawn from text corpora,and methods using the structure of existing knowledge resources. Methods in theformer category disregard taxonomic structure, while those in the latter failto consider semantically relevant empirical information. In this paper, wepresent a method that retrofits distributional context vector representationsof biomedical concepts using structural information from the UMLSMetathesaurus, such that the similarity between vector representations oflinked concepts is augmented. We evaluated it on the UMNSRS benchmark. Ourresults demonstrate that retrofitting of concept vector representations leadsto better correlation with human raters for both similarity and relatedness,surpassing the best results reported to date. They also demonstrate a clearimprovement in performance on this reference standard for retrofitted vectorrepresentations, as compared to those without retrofitting.

Learning Disentangled Representations of Texts with Application to  Biomedical Abstracts

  We propose a method for learning disentangled representations of texts thatcode for distinct and complementary aspects, with the aim of affordingefficient model transfer and interpretability. To induce disentangledembeddings, we propose an adversarial objective based on the (dis)similaritybetween triplets of documents with respect to specific aspects. Our motivatingapplication is embedding biomedical abstracts describing clinical trials in amanner that disentangles the populations, interventions, and outcomes in agiven trial. We show that our method learns representations that encode theseclinically salient aspects, and that these can be effectively used to performaspect-specific retrieval. We demonstrate that the approach generalizes beyondour motivating application in experiments on two multi-aspect review corpora.

A Corpus with Multi-Level Annotations of Patients, Interventions and  Outcomes to Support Language Processing for Medical Literature

  We present a corpus of 5,000 richly annotated abstracts of medical articlesdescribing clinical randomized controlled trials. Annotations includedemarcations of text spans that describe the Patient population enrolled, theInterventions studied and to what they were Compared, and the Outcomes measured(the `PICO' elements). These spans are further annotated at a more granularlevel, e.g., individual interventions within them are marked and mapped onto astructured medical vocabulary. We acquired annotations from a diverse set ofworkers with varying levels of expertise and cost. We describe our datacollection process and the corpus itself in detail. We then outline a set ofchallenging NLP tasks that would aid searching of the medical literature andthe practice of evidence-based medicine.

Structured Multi-Label Biomedical Text Tagging via Attentive Neural Tree  Decoding

  We propose a model for tagging unstructured texts with an arbitrary number ofterms drawn from a tree-structured vocabulary (i.e., an ontology). We treatthis as a special case of sequence-to-sequence learning in which the decoderbegins at the root node of an ontological tree and recursively elects to expandchild nodes as a function of the input text, the current node, and the latentdecoder state. In our experiments the proposed method outperformsstate-of-the-art approaches on the important task of automatically assigningMeSH terms to biomedical abstracts.

Structured Neural Topic Models for Reviews

  We present Variational Aspect-based Latent Topic Allocation (VALTA), a familyof autoencoding topic models that learn aspect-based representations ofreviews. VALTA defines a user-item encoder that maps bag-of-words vectors forcombined reviews associated with each paired user and item onto structuredembeddings, which in turn define per-aspect topic weights. We model individualreviews in a structured manner by inferring an aspect assignment for eachsentence in a given review, where the per-aspect topic weights obtained by theuser-item encoder serve to define a mixture over topics, conditioned on theaspect. The result is an autoencoding neural topic model for reviews, which canbe trained in a fully unsupervised manner to learn topics that are structuredinto aspects. Experimental evaluation on large number of datasets demonstratesthat aspects are interpretable, yield higher coherence scores thannon-structured autoencoding topic model variants, and can be utilized toperform aspect-based comparison and genre discovery.

Attention is not Explanation

  Attention mechanisms have seen wide adoption in neural NLP models. Inaddition to improving predictive performance, these are often touted asaffording transparency: models equipped with attention provide a distributionover attended-to input units, and this is often presented (at least implicitly)as communicating the relative importance of inputs. However, it is unclear whatrelationship exists between attention weights and model outputs. In this work,we perform extensive experiments across a variety of NLP tasks that aim toassess the degree to which attention weights provide meaningful `explanations'for predictions. We find that they largely do not. For example, learnedattention weights are frequently uncorrelated with gradient-based measures offeature importance, and one can identify very different attention distributionsthat nonetheless yield equivalent predictions. Our findings show that standardattention modules do not provide meaningful explanations and should not betreated as though they do. Code for all experiments is available athttps://github.com/successar/AttentionExplanation.

Inferring Which Medical Treatments Work from Reports of Clinical Trials

  How do we know if a particular medical treatment actually works? Ideally onewould consult all available evidence from relevant clinical trials.Unfortunately, such results are primarily disseminated in natural languagescientific articles, imposing substantial burden on those trying to make senseof them. In this paper, we present a new task and corpus for making thisunstructured evidence actionable. The task entails inferring reported findingsfrom a full-text article describing a randomized controlled trial (RCT) withrespect to a given intervention, comparator, and outcome of interest, e.g.,inferring if an article provides evidence supporting the use of aspirin toreduce risk of stroke, as compared to placebo.  We present a new corpus for this task comprising 10,000+ prompts coupled withfull-text articles describing RCTs. Results using a suite of models --- rangingfrom heuristic (rule-based) approaches to attentive neural architectures ---demonstrate the difficulty of the task, which we believe largely owes to thelengthy, technical input texts. To facilitate further work on this important,challenging problem we make the corpus, documentation, a website andleaderboard, and code for baselines and evaluation available athttp://evidence-inference.ebm-nlp.com/.

An Analysis of Attention over Clinical Notes for Predictive Tasks

  The shift to electronic medical records (EMRs) has engendered research intomachine learning and natural language technologies to analyze patient records,and to predict from these clinical outcomes of interest. Two observationsmotivate our aims here. First, unstructured notes contained within EMR oftencontain key information, and hence should be exploited by models. Second, whilestrong predictive performance is important, interpretability of models isperhaps equally so for applications in this domain. Together, these pointssuggest that neural models for EMR may benefit from incorporation of attentionover notes, which one may hope will both yield performance gains and affordtransparency in predictions. In this work we perform experiments to explorethis question using two EMR corpora and four different predictive tasks, that:(i) inclusion of attention mechanisms is critical for neural encoder modulesthat operate over notes fields in order to yield competitive performance, but,(ii) unfortunately, while these boost predictive performance, it is decidedlyless clear whether they provide meaningful support for predictions.

Active Discriminative Text Representation Learning

  We propose a new active learning (AL) method for text classification withconvolutional neural networks (CNNs). In AL, one selects the instances to bemanually labeled with the aim of maximizing model performance with minimaleffort. Neural models capitalize on word embeddings as representations(features), tuning these to the task at hand. We argue that AL strategies formulti-layered neural models should focus on selecting instances that mostaffect the embedding space (i.e., induce discriminative word representations).This is in contrast to traditional AL approaches (e.g., entropy-baseduncertainty sampling), which specify higher level objectives. We propose asimple approach for sentence classification that selects instances containingwords whose embeddings are likely to be updated with the greatest magnitude,thereby rapidly learning discriminative, task-specific embeddings. We extendthis approach to document classification by jointly considering: (1) theexpected changes to the constituent word representations; and (2) the model'scurrent overall uncertainty regarding the instance. The relative emphasisplaced on these criteria is governed by a stochastic process that favorsselecting instances likely to improve representations at the outset oflearning, and then shifts toward general uncertainty sampling as AL progresses.Empirical results show that our method outperforms baseline AL approaches onboth sentence and document classification tasks. We also show that, asexpected, the method quickly learns discriminative word embeddings. To the bestof our knowledge, this is the first work on AL addressing neural models fortext classification.

Neural Information Retrieval: A Literature Review

  A recent "third wave" of Neural Network (NN) approaches now deliversstate-of-the-art performance in many machine learning tasks, spanning speechrecognition, computer vision, and natural language processing. Because thesemodern NNs often comprise multiple interconnected layers, this new NN researchis often referred to as deep learning. Stemming from this tide of NN work, anumber of researchers have recently begun to investigate NN approaches toInformation Retrieval (IR). While deep NNs have yet to achieve the same levelof success in IR as seen in other areas, the recent surge of interest and workin NNs for IR suggest that this state of affairs may be quickly changing. Inthis work, we survey the current landscape of Neural IR research, payingspecial attention to the use of learned representations of queries anddocuments (i.e., neural embeddings). We highlight the successes of neural IRthus far, catalog obstacles to its wider adoption, and suggest potentiallypromising directions for future research.

Quantifying Mental Health from Social Media with Neural User Embeddings

  Mental illnesses adversely affect a significant proportion of the populationworldwide. However, the methods traditionally used for estimating andcharacterizing the prevalence of mental health conditions are time-consumingand expensive. Consequently, best-available estimates concerning the prevalenceof mental health conditions are often years out of date. Automated approachesto supplement these survey methods with broad, aggregated information derivedfrom social media content provides a potential means for near real-timeestimates at scale. These may, in turn, provide grist for supporting,evaluating and iteratively improving upon public health programs andinterventions.  We propose a novel model for automated mental health status quantificationthat incorporates user embeddings. This builds upon recent work exploringrepresentation learning methods that induce embeddings by leveraging socialmedia post histories. Such embeddings capture latent characteristics ofindividuals (e.g., political leanings) and encode a soft notion of homophily.In this paper, we investigate whether user embeddings learned from twitter posthistories encode information that correlates with mental health statuses. Tothis end, we estimated user embeddings for a set of users known to be affectedby depression and post-traumatic stress disorder (PTSD), and for a set ofdemographically matched `control' users. We then evaluated these embeddingswith respect to: (i) their ability to capture homophilic relations with respectto mental health status; and (ii) the performance of downstream mental healthprediction models based on these features. Our experimental results demonstratethat the user embeddings capture similarities between users with respect tomental conditions, and are predictive of mental health.

