The Neural Network Pushdown Automaton: Model, Stack and Learning  Simulations

  In order for neural networks to learn complex languages or grammars, theymust have sufficient computational power or resources to recognize or generatesuch languages. Though many approaches have been discussed, one ob- viousapproach to enhancing the processing power of a recurrent neural network is tocouple it with an external stack memory - in effect creating a neural networkpushdown automata (NNPDA). This paper discusses in detail this NNPDA - itsconstruction, how it can be trained and how useful symbolic information can beextracted from the trained network.  In order to couple the external stack to the neural network, an optimizationmethod is developed which uses an error function that connects the learning ofthe state automaton of the neural network to the learning of the operation ofthe external stack. To minimize the error function using gradient descentlearning, an analog stack is designed such that the action and storage ofinformation in the stack are continuous. One interpretation of a continuousstack is the probabilistic storage of and action on data. After training onsample strings of an unknown source grammar, a quantization procedure extractsfrom the analog stack and neural network a discrete pushdown automata (PDA).Simulations show that in learning deterministic context-free grammars - thebalanced parenthesis language, 1*n0*n, and the deterministic Palindrome - theextracted PDA is correct in the sense that it can correctly recognize unseenstrings of arbitrary length. In addition, the extracted PDAs can be shown to beidentical or equivalent to the PDAs of the source grammars which were used togenerate the training strings.

A Comparison of On-Line Computer Science Citation Databases

  This paper examines the difference and similarities between the two on-linecomputer science citation databases DBLP and CiteSeer. The database entries inDBLP are inserted manually while the CiteSeer entries are obtained autonomouslyvia a crawl of the Web and automatic processing of user submissions. CiteSeer'sautonomous citation database can be considered a form of self-selected on-linesurvey. It is important to understand the limitations of such databases,particularly when citation information is used to assess the performance ofauthors, institutions and funding bodies.  We show that the CiteSeer database contains considerably fewer single authorpapers. This bias can be modeled by an exponential process with intuitiveexplanation. The model permits us to predict that the DBLP database coversapproximately 24% of the entire literature of Computer Science. CiteSeer isalso biased against low-cited papers.  Despite their difference, both databases exhibit similar and significantlydifferent citation distributions compared with previous analysis of the Physicscommunity. In both databases, we also observe that the number of authors perpaper has been increasing over time.

ChemXSeer Digital Library Gaussian Search

  We report on the Gaussian file search system designed as part of theChemXSeer digital library. Gaussian files are produced by the Gaussian software[4], a software package used for calculating molecular electronic structure andproperties. The output files are semi-structured, allowing relatively easyaccess to the Gaussian attributes and metadata. Our system is currently capableof searching Gaussian documents using a boolean combination of atoms (chemicalelements) and attributes. We have also implemented a faceted browsing featureon three important Gaussian attribute types - Basis Set, Job Type and MethodUsed. The faceted browsing feature enables a user to view and process asmaller, filtered subset of documents.

Modelling Information Incorporation in Markets, with Application to  Detecting and Explaining Events

  We develop a model of how information flows into a market, and derivealgorithms for automatically detecting and explaining relevant events. Weanalyze data from twenty-two "political stock markets" (i.e., betting marketson political outcomes) on the Iowa Electronic Market (IEM). We prove that,under certain efficiency assumptions, prices in such betting markets will onaverage approach the correct outcomes over time, and show that IEM dataconforms closely to the theory. We present a simple model of a betting marketwhere information is revealed over time, and show a qualitative correspondencebetween the model and real market data. We also present an algorithm forautomatically detecting significant events and generating semantic explanationsof their origin. The algorithm operates by discovering significant changes invocabulary on online news sources (using expected entropy loss) that align withmajor price spikes in related betting markets.

Graph-based Approach to Automatic Taxonomy Generation (GraBTax)

  We propose a novel graph-based approach for constructing concept hierarchyfrom a large text corpus. Our algorithm, GraBTax, incorporates both statisticalco-occurrences and lexical similarity in optimizing the structure of thetaxonomy. To automatically generate topic-dependent taxonomies from a largetext corpus, GraBTax first extracts topical terms and their relationships fromthe corpus. The algorithm then constructs a weighted graph representing topicsand their associations. A graph partitioning algorithm is then used torecursively partition the topic graph into a taxonomy. For evaluation, we applyGraBTax to articles, primarily computer science, in the CiteSeerX digitallibrary and search engine. The quality of the resulting concept hierarchy isassessed by both human judges and comparison with Wikipedia categories.

An Investigation of Machine Learning Methods Applied to Structure  Prediction in Condensed Matter

  Materials characterization remains a significant, time-consuming undertaking.Generally speaking, spectroscopic techniques are used in conjunction withempirical and ab-initio calculations in order to elucidate structure. Theseexperimental and computational methods typically require significant humaninput and interpretation, particularly with regards to novel materials.Recently, the application of data mining and machine learning to problems inmaterial science have shown great promise in reducing this overhead. In thework presented here, several aspects of machine learning are explored withregards to characterizing a model material, titania, using solid-state NuclearMagnetic Resonance (NMR). Specifically, a large dataset is generated,corresponding to NMR $^{47}$Ti spectra, using ab-initio calculations forgenerated TiO$_2$ structures. Principal Components Analysis (PCA) reveals thatinput spectra may be compressed by more than 90%, before being used forsubsequent machine learning. Two key methods are used to learn the complexmapping between structural details and input NMR spectra, demonstratingexcellent accuracy when presented with test sample spectra. This work comparesSupport Vector Regression (SVR) and Artificial Neural Networks (ANNs), as onestep towards the construction of an expert system for solid state materialscharacterization.

Science and Ethnicity: How Ethnicities Shape the Evolution of Computer  Science Research Community

  Globalization and the world wide web has resulted in academia and sciencebeing an international and multicultural community forged by researchers andscientists with different ethnicities. How ethnicity shapes the evolution ofmembership, status and interactions of the scientific community, however, isnot well understood. This is due to the difficulty of ethnicity identificationat the large scale. We use name ethnicity classification as an indicator ofethnicity. Based on automatic name ethnicity classification of 1.7+ millionauthors gathered from Web, the name ethnicity of computer science scholars isinvestigated by population size, publication contribution and collaborationstrength. By showing the evolution of name ethnicity from 1936 to 2010, wediscover that ethnicity diversity has increased significantly over time andthat different research communities in certain publication venues havedifferent ethnicity compositions. We notice a clear rise in the number of Asianname ethnicities in papers. Their fraction of publication contributionincreases from approximately 10% to near 50% from 1970 to 2010. We also findthat name ethnicity acts as a homophily factor on coauthor networks, shapingthe formation of coauthorship as well as evolution of research communities.

Online Semi-Supervised Learning with Deep Hybrid Boltzmann Machines and  Denoising Autoencoders

  Two novel deep hybrid architectures, the Deep Hybrid Boltzmann Machine andthe Deep Hybrid Denoising Auto-encoder, are proposed for handlingsemi-supervised learning problems. The models combine experts that modelrelevant distributions at different levels of abstraction to improve overallpredictive performance on discriminative tasks. Theoretical motivations andalgorithms for joint learning for each are presented. We apply the new modelsto the domain of data-streams in work towards life-long learning. The proposedarchitectures show improved performance compared to a pseudo-labeled, drop-outrectifier network.

Unifying Adversarial Training Algorithms with Flexible Deep Data  Gradient Regularization

  Many previous proposals for adversarial training of deep neural nets haveincluded di- rectly modifying the gradient, training on a mix of original andadversarial examples, using contractive penalties, and approximately optimizingconstrained adversarial ob- jective functions. In this paper, we show theseproposals are actually all instances of optimizing a general, regularizedobjective we call DataGrad. Our proposed DataGrad framework, which can beviewed as a deep extension of the layerwise contractive au- toencoder penalty,cleanly simplifies prior work and easily allows extensions such as adversarialtraining with multi-task cues. In our experiments, we find that the deep gra-dient regularization of DataGrad (which also has L1 and L2 flavors ofregularization) outperforms alternative forms of regularization, includingclassical L1, L2, and multi- task, both on the original dataset as well as onadversarial sets. Furthermore, we find that combining multi-task optimizationwith DataGrad adversarial training results in the most robust performance.

Random Forest DBSCAN for USPTO Inventor Name Disambiguation

  Name disambiguation and the subsequent name conflation are essential for thecorrect processing of person name queries in a digital library or otherdatabase. It distinguishes each unique person from all other records in thedatabase. We study inventor name disambiguation for a patent database usingmethods and features from earlier work on author name disambiguation andpropose a feature set appropriate for a patent database. A random forest wasselected for the pairwise linking classifier since they outperform Naive Bayes,Logistic Regression, Support Vector Machines (SVM), Conditional Inference Tree,and Decision Trees. Blocking size, very important for scaling, was selectedbased on experiments that determined feature importance and accuracy. TheDBSCAN algorithm is used for clustering records, using a distance functionderived from random forest classifier. For additional scalability clusteringwas parallelized. Tests on the USPTO patent database show that our methodsuccessfully disambiguated 12 million inventor mentions within 6.5 hours.Evaluation on datasets from USPTO PatentsView inventor name disambiguationcompetition shows our algorithm outperforms all algorithms in the competition.

Smart Library: Identifying Books in a Library using Richly Supervised  Deep Scene Text Reading

  Physical library collections are valuable and long standing resources forknowledge and learning. However, managing books in a large bookshelf andfinding books on it often leads to tedious manual work, especially for largebook collections where books might be missing or misplaced. Recently, deepneural models, such as Convolutional Neural Networks (CNN) and Recurrent NeuralNetworks (RNN) have achieved great success for scene text detection andrecognition. Motivated by these recent successes, we aim to investigate theirviability in facilitating book management, a task that introduces furtherchallenges including large amounts of cluttered scene text, distortion, andvaried lighting conditions. In this paper, we present a library inventorybuilding and retrieval system based on scene text reading methods. Wespecifically design our scene text recognition model using rich supervision toaccelerate training and achieve state-of-the-art performance on severalbenchmark datasets. Our proposed system has the potential to greatly reduce theamount of human labor required in managing book inventories as well as thespace needed to store book information.

Learning a Hierarchical Latent-Variable Model of 3D Shapes

  We propose the Variational Shape Learner (VSL), a generative model thatlearns the underlying structure of voxelized 3D shapes in an unsupervisedfashion. Through the use of skip-connections, our model can successfully learnand infer a latent, hierarchical representation of objects. Furthermore,realistic 3D objects can be easily generated by sampling the VSL's latentprobabilistic manifold. We show that our generative model can be trainedend-to-end from 2D images to perform single image 3D model retrieval.Experiments show, both quantitatively and qualitatively, the improvedgeneralization of our proposed model over a range of tasks, performing betteror comparable to various state-of-the-art alternatives.

Learning to Extract Semantic Structure from Documents Using Multimodal  Fully Convolutional Neural Network

  We present an end-to-end, multimodal, fully convolutional network forextracting semantic structures from document images. We consider documentsemantic structure extraction as a pixel-wise segmentation task, and propose aunified model that classifies pixels based not only on their visual appearance,as in the traditional page segmentation task, but also on the content ofunderlying text. Moreover, we propose an efficient synthetic documentgeneration process that we use to generate pretraining data for our network.Once the network is trained on a large set of synthetic documents, we fine-tunethe network on unlabeled real documents using a semi-supervised approach. Wesystematically study the optimum network architecture and show that both ourmultimodal approach and the synthetic data pretraining significantly boost theperformance.

Scaling Author Name Disambiguation with CNF Blocking

  An author name disambiguation (AND) algorithm identifies a unique authorentity record from all similar or same publication records in scholarly orsimilar databases. Typically, a clustering method is used that requirescalculation of similarities between each possible record pair. However, thetotal number of pairs grows quadratically with the size of the author databasemaking such clustering difficult for millions of records. One remedy for thisis a blocking function that reduces the number of pairwise similaritycalculations. Here, we introduce a new way of learning blocking schemes byusing a conjunctive normal form (CNF) in contrast to the disjunctive normalform (DNF). We demonstrate on PubMed author records that CNF blocking reducesmore pairs while preserving high pairs completeness compared to the previousmethods that use a DNF with the computation time significantly reduced. Thus,these concepts in scholarly data can be better represented with CNFs. Moreover,we also show how to ensure that the method produces disjoint blocks so that therest of the AND algorithm can be easily paralleled. Our CNF blocking tested onthe entire PubMed database of 80 million author mentions efficiently removes82.17% of all author record pairs in 10 minutes.

An Empirical Evaluation of Rule Extraction from Recurrent Neural  Networks

  Rule extraction from black-box models is critical in domains that requiremodel validation before implementation, as can be the case in credit scoringand medical diagnosis. Though already a challenging problem in statisticallearning in general, the difficulty is even greater when highly non-linear,recursive models, such as recurrent neural networks (RNNs), are fit to data.Here, we study the extraction of rules from second-order recurrent neuralnetworks trained to recognize the Tomita grammars. We show that productionrules can be stably extracted from trained RNNs and that in certain cases therules outperform the trained RNNs.

Learning to Adapt by Minimizing Discrepancy

  We explore whether useful temporal neural generative models can be learnedfrom sequential data without back-propagation through time. We investigate theviability of a more neurocognitively-grounded approach in the context ofunsupervised generative modeling of sequences. Specifically, we build on theconcept of predictive coding, which has gained influence in cognitive science,in a neural framework. To do so we develop a novel architecture, the TemporalNeural Coding Network, and its learning algorithm, Discrepancy Reduction. Theunderlying directed generative model is fully recurrent, meaning that itemploys structural feedback connections and temporal feedback connections,yielding information propagation cycles that create local learning signals.This facilitates a unified bottom-up and top-down approach for informationtransfer inside the architecture. Our proposed algorithm shows promise on thebouncing balls generative modeling problem. Further experiments could beconducted to explore the strengths and weaknesses of our approach.

Active Learning of Strict Partial Orders: A Case Study on Concept  Prerequisite Relations

  Strict partial order is a mathematical structure commonly seen in relationaldata. One obstacle to extracting such type of relations at scale is the lack oflarge-scale labels for building effective data-driven solutions. We develop anactive learning framework for mining such relations subject to a strict order.Our approach incorporates relational reasoning not only in finding newunlabeled pairs whose labels can be deduced from an existing label set, butalso in devising new query strategies that consider the relational structure oflabels. Our experiments on concept prerequisite relations show our proposedframework can substantially improve the classification performance with thesame query budget compared to other baseline approaches.

Conducting Credit Assignment by Aligning Local Representations

  Using back-propagation and its variants to train deep networks is oftenproblematic for new users. Issues such as exploding gradients, vanishinggradients, and high sensitivity to weight initialization strategies often makenetworks difficult to train, especially when users are experimenting with newarchitectures. Here, we present Local Representation Alignment (LRA), atraining procedure that is much less sensitive to bad initializations, does notrequire modifications to the network architecture, and can be adapted tonetworks with highly nonlinear and discrete-valued activation functions.Furthermore, we show that one variation of LRA can start with a nullinitialization of network weights and still successfully train networks with awide variety of nonlinearities, including tanh, ReLU-6, softplus, signum andothers that may draw their inspiration from biology.  A comprehensive set of experiments on MNIST and the much harder Fashion MNISTdata sets show that LRA can be used to train networks robustly and effectively,succeeding even when back-propagation fails and outperforming other alternativelearning algorithms, such as target propagation and feedback alignment.

Learned Neural Iterative Decoding for Lossy Image Compression Systems

  For lossy image compression systems, we develop an algorithm, iterativerefinement, to improve the decoder's reconstruction compared to standarddecoding techniques. Specifically, we propose a recurrent neural networkapproach for nonlinear, iterative decoding. Our decoder, which works with anyencoder, employs self-connected memory units that make use of causal andnon-causal spatial context information to progressively reduce reconstructionerror over a fixed number of steps. We experiment with variants of ourestimator and find that iterative refinement consistently creates lowerdistortion images of higher perceptual quality compared to other approaches.Specifically, on the Kodak Lossless True Color Image Suite, we observe as muchas a 0.871 decibel (dB) gain over JPEG, a 1.095 dB gain over JPEG 2000, and a0.971 dB gain over a competitive neural model.

Adversarial Training for Community Question Answer Selection Based on  Multi-scale Matching

  Community-based question answering (CQA) websites represent an importantsource of information. As a result, the problem of matching the most valuableanswers to their corresponding questions has become an increasingly popularresearch topic. We frame this task as a binary (relevant/irrelevant)classification problem, and present an adversarial training framework toalleviate label imbalance issue. We employ a generative model to iterativelysample a subset of challenging negative samples to fool our classificationmodel. Both models are alternatively optimized using REINFORCE algorithm. Theproposed method is completely different from previous ones, where negativesamples in training set are directly used or uniformly down-sampled. Further,we propose using Multi-scale Matching which explicitly inspects the correlationbetween words and ngrams of different levels of granularity. We evaluate theproposed method on SemEval 2016 and SemEval 2017 datasets and achievesstate-of-the-art or similar performance.

TextContourNet: a Flexible and Effective Framework for Improving Scene  Text Detection Architecture with a Multi-task Cascade

  We study the problem of extracting text instance contour information fromimages and use it to assist scene text detection. We propose a novel andeffective framework for this and experimentally demonstrate that: (1) A CNNthat can be effectively used to extract instance-level text contour fromnatural images. (2) The extracted contour information can be used for betterscene text detection. We propose two ways for learning the contour tasktogether with the scene text detection: (1) as an auxiliary task and (2) asmulti-task cascade. Extensive experiments with different benchmark datasetsdemonstrate that both designs improve the performance of a state-of-the-artscene text detector and that a multi-task cascade design achieves the bestperformance.

On the utility of graphics cards to perform massively parallel  simulation of advanced Monte Carlo methods

  We present a case-study on the utility of graphics cards to perform massivelyparallel simulation of advanced Monte Carlo methods. Graphics cards, containingmultiple Graphics Processing Units (GPUs), are self-contained parallelcomputational devices that can be housed in conventional desktop and laptopcomputers. For certain classes of Monte Carlo algorithms they offer massivelyparallel simulation, with the added advantage over conventional distributedmulti-core processors that they are cheap, easily accessible, easy to maintain,easy to code, dedicated local devices with low power consumption. On acanonical set of stochastic simulation examples including population-basedMarkov chain Monte Carlo methods and Sequential Monte Carlo methods, we findspeedups from 35 to 500 fold over conventional single-threaded computer code.Our findings suggest that GPUs have the potential to facilitate the growth ofstatistical modelling into complex data rich domains through the availabilityof cheap and accessible many-core computation. We believe the speedup weobserve should motivate wider use of parallelizable simulation methods andgreater methodological attention to their design.

Effectively Searching Maps in Web Documents

  Maps are an important source of information in archaeology and othersciences. Users want to search for historical maps to determine recordedhistory of the political geography of regions at different eras, to find outwhere exactly archaeological artifacts were discovered, etc. Currently, theyhave to use a generic search engine and add the term map along with otherkeywords to search for maps. This crude method will generate a significantnumber of false positives that the user will need to cull through to get thedesired results. To reduce their manual effort, we propose an automatic mapidentification, indexing, and retrieval system that enables users to search andretrieve maps appearing in a large corpus of digital documents using simplekeyword queries. We identify features that can help in distinguishing maps fromother figures in digital documents and show how a Support-Vector-Machine-basedclassifier can be used to identify maps. We propose map-level-metadata e.g.,captions, references to the maps in text, etc. and document-level metadata,e.g., title, abstract, citations, how recent the publication is, etc. and showhow they can be automatically extracted and indexed. Our novel rankingalgorithm weights different metadata fields differently and also uses thedocument-level metadata to help rank retrieved maps. Empirical evaluations showwhich features should be selected and which metadata fields should be weightedmore. We also demonstrate improved retrieval results in comparison toadaptations of existing methods for map retrieval. Our map search engine hasbeen deployed in an online map-search system that is part of the Blind-Reviewdigital library system.

Extreme alpha-clustering in the 18O nucleus

  The structure of the 18O nucleus at excitation energies above the alpha decaythreshold was studied using 14C+alpha resonance elastic scattering. A number ofstates with large alpha reduced widths have been observed, indicating that thealpha-cluster degree of freedom plays an important role in this N not equal Znucleus. However, the alpha-cluster structure of this nucleus is very differentfrom the relatively simple pattern of strong alpha-cluster quasi-rotationalbands in the neighboring 16O and 20Ne nuclei. A 0+ state with an alpha reducedwidth exceeding the single particle limit was identified at an excitationenergy of 9.9+/-0.3 MeV. We discuss evidence that states of this kind arecommon in light nuclei and give possible explanations of this feature.

Bayesian Classification and Feature Selection from Finite Data Sets

  Feature selection aims to select the smallest subset of features for aspecified level of performance. The optimal achievable classificationperformance on a feature subset is summarized by its Receiver Operating Curve(ROC). When infinite data is available, the Neyman- Pearson (NP) designprocedure provides the most efficient way of obtaining this curve. In practicethe design procedure is applied to density estimates from finite data sets. Weperform a detailed statistical analysis of the resulting error propagation onfinite alphabets. We show that the estimated performance curve (EPC) producedby the design procedure is arbitrarily accurate given sufficient data,independent of the size of the feature set. However, the underlying likelihoodranking procedure is highly sensitive to errors that reduces the probabilitythat the EPC is in fact the ROC. In the worst case, guaranteeing that the EPCis equal to the ROC may require data sizes exponential in the size of thefeature set. These results imply that in theory the NP design approach may onlybe valid for characterizing relatively small feature subsets, even when theperformance of any given classifier can be estimated very accurately. Wediscuss the practical limitations for on-line methods that ensures that the NPprocedure operates in a statistically valid region.

Collaborative Filtering by Personality Diagnosis: A Hybrid Memory- and  Model-Based Approach

  The growth of Internet commerce has stimulated the use of collaborativefiltering (CF) algorithms as recommender systems. Such systems leverageknowledge about the known preferences of multiple users to recommend items ofinterest to other users. CF methods have been harnessed to make recommendationsabout such items as web pages, movies, books, and toys. Researchers haveproposed and evaluated many approaches for generating recommendations. Wedescribe and evaluate a new method called emph{personality diagnosis (PD)}.Given a user's preferences for some items, we compute the probability that heor she is of the same "personality type" as other users, and, in turn, theprobability that he or she will like new items. PD retains some of theadvantages of traditional similarity-weighting techniques in that all data isbrought to bear on each prediction and new data can be added easily andincrementally. Additionally, PD has a meaningful probabilistic interpretation,which may be leveraged to justify, explain, and augment results. We reportempirical results on the EachMovie database of movie ratings, and on userprofile data collected from the CiteSeer digital library of Computer Scienceresearch papers. The probabilistic framework naturally supports a variety ofdescriptive measurements - in particular, we consider the applicability of avalue of information (VOI) computation.

Using Non-invertible Data Transformations to Build Adversarial-Robust  Neural Networks

  Deep neural networks have proven to be quite effective in a wide variety ofmachine learning tasks, ranging from improved speech recognition systems toadvancing the development of autonomous vehicles. However, despite theirsuperior performance in many applications, these models have been recentlyshown to be susceptible to a particular type of attack possible through thegeneration of particular synthetic examples referred to as adversarial samples.These samples are constructed by manipulating real examples from the trainingdata distribution in order to "fool" the original neural model, resulting inmisclassification (with high confidence) of previously correctly classifiedsamples. Addressing this weakness is of utmost importance if deep neuralarchitectures are to be applied to critical applications, such as those in thedomain of cybersecurity. In this paper, we present an analysis of thisfundamental flaw lurking in all neural architectures to uncover limitations ofpreviously proposed defense mechanisms. More importantly, we present a unifyingframework for protecting deep neural models using a non-invertible datatransformation--developing two adversary-resilient architectures utilizing bothlinear and nonlinear dimensionality reduction. Empirical results indicate thatour framework provides better robustness compared to state-of-art solutionswhile having negligible degradation in accuracy.

A Comparative Study of Rule Extraction for Recurrent Neural Networks

  Understanding recurrent networks through rule extraction has a long history.This has taken on new interests due to the need for interpreting or verifyingneural networks. One basic form for representing stateful rules isdeterministic finite automata (DFA). Previous research shows that extractingDFAs from trained second-order recurrent networks is not only possible but alsorelatively stable. Recently, several new types of recurrent networks with morecomplicated architectures have been introduced. These handle challenginglearning tasks usually involving sequential data. However, it remains an openproblem whether DFAs can be adequately extracted from these models.Specifically, it is not clear how DFA extraction will be affected when appliedto different recurrent networks trained on data sets with different levels ofcomplexity. Here, we investigate DFA extraction on several widely adoptedrecurrent networks that are trained to learn a set of seven regular Tomitagrammars. We first formally analyze the complexity of Tomita grammars andcategorize these grammars according to that complexity. Then we empiricallyevaluate different recurrent networks for their performance of DFA extractionon all Tomita grammars. Our experiments show that for most recurrent networks,their extraction performance decreases as the complexity of the underlyinggrammar increases. On grammars of lower complexity, most recurrent networksobtain desirable extraction performance. As for grammars with the highest levelof complexity, while several complicated models fail with only certainrecurrent networks having satisfactory extraction performance.

Large Scale Scene Text Verification with Guided Attention

  Many tasks are related to determining if a particular text string exists inan image. In this work, we propose a new framework that learns this task in anend-to-end way. The framework takes an image and a text string as input andthen outputs the probability of the text string being present in the image.This is the first end-to-end framework that learns such relationships betweentext and images in scene text area. The framework does not require explicitscene text detection or recognition and thus no bounding box annotations areneeded for it. It is also the first work in scene text area that tackles suh aweakly labeled problem. Based on this framework, we developed a model calledGuided Attention. Our designed model achieves much better results than severalstate-of-the-art scene text reading based solutions for a challenging StreetView Business Matching task. The task tries to find correct business names forstorefront images and the dataset we collected for it is substantially larger,and more challenging than existing scene text dataset. This new real-world taskprovides a new perspective for studying scene text related problems. We alsodemonstrate the uniqueness of our task via a comparison between our problem anda typical Visual Question Answering problem.

A Neural Temporal Model for Human Motion Prediction

  We propose novel neural temporal models for predicting and synthesizing humanmotion, achieving state-of-the-art in modeling long-term motion trajectorieswhile being competitive with prior work in short-term prediction, withsignificantly less required computation. Key aspects of our proposed systeminclude: 1) a novel, two-level processing architecture that aids in generatingplanned trajectories, 2) a simple set of easily computable features thatintegrate derivative information into the model, and 3) a novel multi-objectiveloss function that helps the model to slowly progress from the simpler task ofnext-step prediction to the harder task of multi-step closed-loop prediction.Our results demonstrate that these innovations facilitate improved modeling oflong-term motion trajectories. Finally, we propose a novel metric, calledNormalized Power Spectrum Similarity (NPSS), to evaluate the long-termpredictive ability of motion synthesis models, complementing the popularmean-squared error (MSE) measure of the Euler joint angles over time. Weconduct a user study to determine if the proposed NPSS correlates with humanevaluation of long-term motion more strongly than MSE and find that it indeeddoes.

Verification of Recurrent Neural Networks Through Rule Extraction

  The verification problem for neural networks is verifying whether a neuralnetwork will suffer from adversarial samples, or approximating the maximalallowed scale of adversarial perturbation that can be endured. While most priorwork contributes to verifying feed-forward networks, little has been exploredfor verifying recurrent networks. This is due to the existence of a morerigorous constraint on the perturbation space for sequential data, and the lackof a proper metric for measuring the perturbation. In this work, we addressthese challenges by proposing a metric which measures the distance betweenstrings, and use deterministic finite automata (DFA) to represent a rigorousoracle which examines if the generated adversarial samples violate certainconstraints on a perturbation. More specifically, we empirically show thatcertain recurrent networks allow relatively stable DFA extraction. As such,DFAs extracted from these recurrent networks can serve as a surrogate oraclefor when the ground truth DFA is unknown. We apply our verification mechanismto several widely used recurrent networks on a set of the Tomita grammars. Theresults demonstrate that only a few models remain robust against adversarialsamples. In addition, we show that for grammars with different levels ofcomplexity, there is also a difference in the difficulty of robust learning ofthese grammars.

ExpertSeer: a Keyphrase Based Expert Recommender for Digital Libraries

  We describe ExpertSeer, a generic framework for expert recommendation basedon the contents of a digital library. Given a query term q, ExpertSeerrecommends experts of q by retrieving authors who published relevant papersdetermined by related keyphrases and the quality of papers. The system is basedon a simple yet effective keyphrase extractor and the Bayes' rule for expertrecommendation. ExpertSeer is domain independent and can be applied todifferent disciplines and applications since the system is automated and nottailored to a specific discipline. Digital library providers can employ thesystem to enrich their services and organizations can discover experts ofinterest within an organization. To demonstrate the power of ExpertSeer, weapply the framework to build two expert recommender systems. The first, CSSeer,utilizes the CiteSeerX digital library to recommend experts primarily incomputer science. The second, ChemSeer, uses publicly available documents fromthe Royal Society of Chemistry (RSC) to recommend experts in chemistry. Usingone thousand computer science terms as benchmark queries, we compared the top-nexperts (n=3, 5, 10) returned by CSSeer to two other expert recommenders --Microsoft Academic Search and ArnetMiner -- and a simulator that imitates theranking function of Google Scholar. Although CSSeer, Microsoft Academic Search,and ArnetMiner mostly return prestigious researchers who published severalpapers related to the query term, it was found that different expertrecommenders return moderately different recommendations. To further studytheir performance, we obtained a widely used benchmark dataset as the groundtruth for comparison. The results show that our system outperforms MicrosoftAcademic Search and ArnetMiner in terms of Precision-at-k (P@k) for k=3, 5, 10.We also conducted several case studies to validate the usefulness of oursystem.

Adversary Resistant Deep Neural Networks with an Application to Malware  Detection

  Beyond its highly publicized victories in Go, there have been numeroussuccessful applications of deep learning in information retrieval, computervision and speech recognition. In cybersecurity, an increasing number ofcompanies have become excited about the potential of deep learning, and havestarted to use it for various security incidents, the most popular beingmalware detection. These companies assert that deep learning (DL) could helpturn the tide in the battle against malware infections. However, deep neuralnetworks (DNNs) are vulnerable to adversarial samples, a flaw that plagues mostif not all statistical learning models. Recent research has demonstrated thatthose with malicious intent can easily circumvent deep learning-powered malwaredetection by exploiting this flaw.  In order to address this problem, previous work has developed various defensemechanisms that either augmenting training data or enhance model's complexity.However, after a thorough analysis of the fundamental flaw in DNNs, we discoverthat the effectiveness of current defenses is limited and, more importantly,cannot provide theoretical guarantees as to their robustness againstadversarial sampled-based attacks. As such, we propose a new adversaryresistant technique that obstructs attackers from constructing impactfuladversarial samples by randomly nullifying features within samples. In thiswork, we evaluate our proposed technique against a real world dataset with14,679 malware variants and 17,399 benign programs. We theoretically validatethe robustness of our technique, and empirically show that our techniquesignificantly boosts DNN robustness to adversarial samples while maintaininghigh accuracy in classification. To demonstrate the general applicability ofour proposed method, we also conduct experiments using the MNIST and CIFAR-10datasets, generally used in image recognition research.

Learning Adversary-Resistant Deep Neural Networks

  Deep neural networks (DNNs) have proven to be quite effective in a vast arrayof machine learning tasks, with recent examples in cyber security andautonomous vehicles. Despite the superior performance of DNNs in theseapplications, it has been recently shown that these models are susceptible to aparticular type of attack that exploits a fundamental flaw in their design.This attack consists of generating particular synthetic examples referred to asadversarial samples. These samples are constructed by slightly manipulatingreal data-points in order to "fool" the original DNN model, forcing it tomis-classify previously correctly classified samples with high confidence.Addressing this flaw in the model is essential if DNNs are to be used incritical applications such as those in cyber security.  Previous work has provided various learning algorithms to enhance therobustness of DNN models, and they all fall into the tactic of "securitythrough obscurity". This means security can be guaranteed only if one canobscure the learning algorithms from adversaries. Once the learning techniqueis disclosed, DNNs protected by these defense mechanisms are still susceptibleto adversarial samples. In this work, we investigate this issue shared acrossprevious research work and propose a generic approach to escalate a DNN'sresistance to adversarial samples. More specifically, our approach integrates adata transformation module with a DNN, making it robust even if we reveal theunderlying learning algorithm. To demonstrate the generality of our proposedapproach and its potential for handling cyber security applications, weevaluate our method and several other existing solutions on datasets publiclyavailable. Our results indicate that our approach typically provides superiorclassification performance and resistance in comparison with state-of-artsolutions.

Hot Streaks in Artistic, Cultural, and Scientific Careers

  The hot streak, loosely defined as winning begets more winnings, highlights aspecific period during which an individual's performance is substantiallyhigher than her typical performance. While widely debated in sports, gambling,and financial markets over the past several decades, little is known if hotstreaks apply to individual careers. Here, building on rich literature onlifecycle of creativity, we collected large-scale career histories ofindividual artists, movie directors and scientists, tracing the artworks,movies, and scientific publications they produced. We find that, across allthree domains, hit works within a career show a high degree of temporalregularity, each career being characterized by bursts of high-impact worksoccurring in sequence. We demonstrate that these observations can be explainedby a simple hot-streak model we developed, allowing us to probe quantitativelythe hot streak phenomenon governing individual careers, which we find to beremarkably universal across diverse domains we analyzed: The hot streaks areubiquitous yet unique across different careers. While the vast majority ofindividuals have at least one hot streak, hot streaks are most likely to occuronly once. The hot streak emerges randomly within an individual's sequence ofworks, is temporally localized, and is unassociated with any detectable changein productivity. We show that, since works produced during hot streaks garnersignificantly more impact, the uncovered hot streaks fundamentally drives thecollective impact of an individual, ignoring which leads us to systematicallyover- or under-estimate the future impact of a career. These results not onlydeepen our quantitative understanding of patterns governing individualingenuity and success, they may also have implications for decisions andpolicies involving predicting and nurturing individuals with lasting impact.

Text Extraction and Retrieval from Smartphone Screenshots: Building a  Repository for Life in Media

  Daily engagement in life experiences is increasingly interwoven with mobiledevice use. Screen capture at the scale of seconds is being used in behavioralstudies and to implement "just-in-time" health interventions. The increasingpsychological breadth of digital information will continue to make the actualscreens that people view a preferred if not required source of data about lifeexperiences. Effective and efficient Information Extraction and Retrieval fromdigital screenshots is a crucial prerequisite to successful use of screen data.In this paper, we present the experimental workflow we exploited to: (i)pre-process a unique collection of screen captures, (ii) extract unstructuredtext embedded in the images, (iii) organize image text and metadata based on astructured schema, (iv) index the resulting document collection, and (v) allowfor Image Retrieval through a dedicated vertical search engine application. Theadopted procedure integrates different open source libraries for traditionalimage processing, Optical Character Recognition (OCR), and Image Retrieval. Ouraim is to assess whether and how state-of-the-art methodologies can be appliedto this novel data set. We show how combining OpenCV-based pre-processingmodules with a Long short-term memory (LSTM) based release of Tesseract OCR,without ad hoc training, led to a 74% character-level accuracy of the extractedtext. Further, we used the processed repository as baseline for a dedicatedImage Retrieval system, for the immediate use and application for behavioraland prevention scientists. We discuss issues of Text Information Extraction andRetrieval that are particular to the screenshot image case and suggestimportant future work.

Continual Learning of Recurrent Neural Networks by Locally Aligning  Distributed Representations

  Temporal models based on recurrent neural networks have proven to be quitepowerful in a wide variety of applications, including language modeling andspeech processing. However, training these models relies on back-propagationthrough time, which entails unfolding the network over many time steps, makingthe process of conducting credit assignment considerably more challenging.Furthermore, the nature of back-propagation itself does not permit the use ofnon-differentiable activation functions and is inherently sequential, makingparallelization of the training process very difficult.  In this work, we propose the Parallel Temporal Neural Coding Network(P-TNCN), a biologically inspired model trained by the learning algorithm knownas Local Representation Alignment, that aims to resolve the difficulties thatplague recurrent networks trained by back-propagation through time. Mostnotably, this architecture requires neither unrolling nor the derivatives ofits internal activation functions. We compare our model and learning procedureto other online back-propagation-through-time alternatives (which tend to becomputationally expensive), including real-time recurrent learning, echo statenetworks, and unbiased online recurrent optimization, and show that itoutperforms them on sequence benchmarks such as Bouncing MNIST, a new benchmarkwe call Bouncing NotMNIST, and Penn Treebank. Notably, our approach can, insome instances, outperform full back-propagation through time and variants suchas sparse attentive back-tracking.  Significantly, the hidden unit correction phase of P-TNCN allows it to adaptto new datasets even if its synaptic weights are held fixed (zero-shotadaptation) and facilitates retention of prior knowledge when faced with a tasksequence. We present results that show the P-TNCN's ability to conductzero-shot adaptation and continual sequence modeling.

A Fermi Gamma-ray Burst Monitor Search for Electromagnetic Signals  Coincident with Gravitational-Wave Candidates in Advanced LIGO's First  Observing Run

  We present a search for prompt gamma-ray counterparts to compact binarycoalescence gravitational wave (GW) candidates from Advanced LIGO's firstobserving run (O1). As demonstrated by the multimessenger observations ofGW170817/GRB 170817A, electromagnetic and GW observations provide complementaryinformation about the astrophysical source and, in the case of weakercandidates, may strengthen the case for an astrophysical origin. Here weinvestigate low-significance GW candidates from the O1 compact-binarycoalescence searches using the Fermi Gamma-ray Burst Monitor (GBM), leveragingits all-sky and broad energy coverage. Candidates are ranked and compared tobackground to measure significance. Those with false alarm rates of less than10^-5 Hz (about one per day) are used as the search sample for gamma-rayfollow-up. No GW candidates were found to be coincident with gamma-raytransients independently identified by blind searches of the GBM data. Inaddition, GW candidate event times were followed up by a separate targetedsearch of GBM data. Among the resulting GBM events, the two with lowest falsealarm rates were the gamma-ray transient GW150914-GBM presented in Connaughtonet al. (2016) and a solar flare in chance coincidence with a GW candidate.

Localization and broadband follow-up of the gravitational-wave transient  GW150914

  A gravitational-wave (GW) transient was identified in data recorded by theAdvanced Laser Interferometer Gravitational-wave Observatory (LIGO) detectorson 2015 September 14. The event, initially designated G184098 and later giventhe name GW150914, is described in detail elsewhere. By prior arrangement,preliminary estimates of the time, significance, and sky location of the eventwere shared with 63 teams of observers covering radio, optical, near-infrared,X-ray, and gamma-ray wavelengths with ground- and space-based facilities. Inthis Letter we describe the low-latency analysis of the GW data and present thesky localization of the first observed compact binary merger. We summarize thefollow-up observations reported by 25 teams via private Gamma-ray CoordinatesNetwork circulars, giving an overview of the participating facilities, the GWsky localization coverage, the timeline and depth of the observations. As thisevent turned out to be a binary black hole merger, there is little expectationof a detectable electromagnetic (EM) signature. Nevertheless, this firstbroadband campaign to search for a counterpart of an Advanced LIGO sourcerepresents a milestone and highlights the broad capabilities of the transientastronomy community and the observing strategies that have been developed topursue neutron star binary merger events. Detailed investigations of the EMdata and results of the EM follow-up campaign are being disseminated in papersby the individual teams.

Supplement: Localization and broadband follow-up of the  gravitational-wave transient GW150914

  This Supplement provides supporting material for arXiv:1602.08492 . Webriefly summarize past electromagnetic (EM) follow-up efforts as well as theorganization and policy of the current EM follow-up program. We compare thefour probability sky maps produced for the gravitational-wave transientGW150914, and provide additional details of the EM follow-up observations thatwere performed in the different bands.

The Long-Baseline Neutrino Experiment: Exploring Fundamental Symmetries  of the Universe

  The preponderance of matter over antimatter in the early Universe, thedynamics of the supernova bursts that produced the heavy elements necessary forlife and whether protons eventually decay --- these mysteries at the forefrontof particle physics and astrophysics are key to understanding the earlyevolution of our Universe, its current state and its eventual fate. TheLong-Baseline Neutrino Experiment (LBNE) represents an extensively developedplan for a world-class experiment dedicated to addressing these questions. LBNEis conceived around three central components: (1) a new, high-intensityneutrino source generated from a megawatt-class proton accelerator at FermiNational Accelerator Laboratory, (2) a near neutrino detector just downstreamof the source, and (3) a massive liquid argon time-projection chamber deployedas a far detector deep underground at the Sanford Underground ResearchFacility. This facility, located at the site of the former Homestake Mine inLead, South Dakota, is approximately 1,300 km from the neutrino source atFermilab -- a distance (baseline) that delivers optimal sensitivity to neutrinocharge-parity symmetry violation and mass ordering effects. This ambitious yetcost-effective design incorporates scalability and flexibility and canaccommodate a variety of upgrades and contributions. With its exceptionalcombination of experimental configuration, technical capabilities, andpotential for transformative discoveries, LBNE promises to be a vital facilityfor the field of particle physics worldwide, providing physicists from aroundthe globe with opportunities to collaborate in a twenty to thirty year programof exciting science. In this document we provide a comprehensive overview ofLBNE's scientific objectives, its place in the landscape of neutrino physicsworldwide, the technologies it will incorporate and the capabilities it willpossess.

