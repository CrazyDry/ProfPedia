On Spectral Analysis of Directed Signed Graphs

  It has been shown that the adjacency eigenspace of a network contains key
information of its underlying structure. However, there has been no study on
spectral analysis of the adjacency matrices of directed signed graphs. In this
paper, we derive theoretical approximations of spectral projections from such
directed signed networks using matrix perturbation theory. We use the derived
theoretical results to study the influences of negative intra cluster and inter
cluster directed edges on node spectral projections. We then develop a spectral
clustering based graph partition algorithm, SC-DSG, and conduct evaluations on
both synthetic and real datasets. Both theoretical analysis and empirical
evaluation demonstrate the effectiveness of the proposed algorithm.


FairGAN: Fairness-aware Generative Adversarial Networks

  Fairness-aware learning is increasingly important in data mining.
Discrimination prevention aims to prevent discrimination in the training data
before it is used to conduct predictive analysis. In this paper, we focus on
fair data generation that ensures the generated data is discrimination free.
Inspired by generative adversarial networks (GAN), we present fairness-aware
generative adversarial networks, called FairGAN, which are able to learn a
generator producing fair data and also preserving good data utility. Compared
with the naive fair data generation models, FairGAN further ensures the
classifiers which are trained on generated data can achieve fair classification
on real data. Experiments on a real dataset show the effectiveness of FairGAN.


Fairness-aware Classification: Criterion, Convexity, and Bounds

  Fairness-aware classification is receiving increasing attention in the
machine learning fields. Recently research proposes to formulate the
fairness-aware classification as constrained optimization problems. However,
several limitations exist in previous works due to the lack of a theoretical
framework for guiding the formulation. In this paper, we propose a general
framework for learning fair classifiers which addresses previous limitations.
The framework formulates various commonly-used fairness metrics as convex
constraints that can be directly incorporated into classic classification
models. Within the framework, we propose a constraint-free criterion on the
training data which ensures that any classifier learned from the data is fair.
We also derive the constraints which ensure that the real fairness metric is
satisfied when surrogate functions are used to achieve convexity. Our framework
can be used to for formulating fairness-aware classification with fairness
guarantee and computational efficiency. The experiments using real-world
datasets demonstrate our theoretical results and show the effectiveness of
proposed framework and methods.


SNE: Signed Network Embedding

  Several network embedding models have been developed for unsigned networks.
However, these models based on skip-gram cannot be applied to signed networks
because they can only deal with one type of link. In this paper, we present our
signed network embedding model called SNE. Our SNE adopts the log-bilinear
model, uses node representations of all nodes along a given path, and further
incorporates two signed-type vectors to capture the positive or negative
relationship of each edge along the path. We conduct two experiments, node
classification and link prediction, on both directed and undirected signed
networks and compare with four baselines including a matrix factorization
method and three state-of-the-art unsigned network embedding models. The
experimental results demonstrate the effectiveness of our signed network
embedding.


Wikipedia Vandal Early Detection: from User Behavior to User Embedding

  Wikipedia is the largest online encyclopedia that allows anyone to edit
articles. In this paper, we propose the use of deep learning to detect vandals
based on their edit history. In particular, we develop a multi-source
long-short term memory network (M-LSTM) to model user behaviors by using a
variety of user edit aspects as inputs, including the history of edit reversion
information, edit page titles and categories. With M-LSTM, we can encode each
user into a low dimensional real vector, called user embedding. Meanwhile, as a
sequential model, M-LSTM updates the user embedding each time after the user
commits a new edit. Thus, we can predict whether a user is benign or vandal
dynamically based on the up-to-date user embedding. Furthermore, those user
embeddings are crucial to discover collaborative vandals.


Spectrum-based deep neural networks for fraud detection

  In this paper, we focus on fraud detection on a signed graph with only a
small set of labeled training data. We propose a novel framework that combines
deep neural networks and spectral graph analysis. In particular, we use the
node projection (called as spectral coordinate) in the low dimensional spectral
space of the graph's adjacency matrix as input of deep neural networks.
Spectral coordinates in the spectral space capture the most useful topology
information of the network. Due to the small dimension of spectral coordinates
(compared with the dimension of the adjacency matrix derived from a graph),
training deep neural networks becomes feasible. We develop and evaluate two
neural networks, deep autoencoder and convolutional neural network, in our
fraud detection framework. Experimental results on a real signed graph show
that our spectrum based deep neural networks are effective in fraud detection.


One-Class Adversarial Nets for Fraud Detection

  Many online applications, such as online social networks or knowledge bases,
are often attacked by malicious users who commit different types of actions
such as vandalism on Wikipedia or fraudulent reviews on eBay. Currently, most
of the fraud detection approaches require a training dataset that contains
records of both benign and malicious users. However, in practice, there are
often no or very few records of malicious users. In this paper, we develop
one-class adversarial nets (OCAN) for fraud detection using training data with
only benign users. OCAN first uses LSTM-Autoencoder to learn the
representations of benign users from their sequences of online activities. It
then detects malicious users by training a discriminator with a complementary
GAN model that is different from the regular GAN model. Experimental results
show that our OCAN outperforms the state-of-the-art one-class classification
models and achieves comparable performance with the latest multi-source LSTM
model that requires both benign and malicious users in the training phase.


On Discrimination Discovery and Removal in Ranked Data using Causal
  Graph

  Predictive models learned from historical data are widely used to help
companies and organizations make decisions. However, they may digitally
unfairly treat unwanted groups, raising concerns about fairness and
discrimination. In this paper, we study the fairness-aware ranking problem
which aims to discover discrimination in ranked datasets and reconstruct the
fair ranking. Existing methods in fairness-aware ranking are mainly based on
statistical parity that cannot measure the true discriminatory effect since
discrimination is causal. On the other hand, existing methods in causal-based
anti-discrimination learning focus on classification problems and cannot be
directly applied to handle the ranked data. To address these limitations, we
propose to map the rank position to a continuous score variable that represents
the qualification of the candidates. Then, we build a causal graph that
consists of both the discrete profile attributes and the continuous score. The
path-specific effect technique is extended to the mixed-variable causal graph
to identify both direct and indirect discrimination. The relationship between
the path-specific effects for the ranked data and those for the binary decision
is theoretically analyzed. Finally, algorithms for discovering and removing
discrimination from a ranked dataset are developed. Experiments using the real
dataset show the effectiveness of our approaches.


Task-specific Word Identification from Short Texts Using a Convolutional
  Neural Network

  Task-specific word identification aims to choose the task-related words that
best describe a short text. Existing approaches require well-defined seed words
or lexical dictionaries (e.g., WordNet), which are often unavailable for many
applications such as social discrimination detection and fake review detection.
However, we often have a set of labeled short texts where each short text has a
task-related class label, e.g., discriminatory or non-discriminatory, specified
by users or learned by classification algorithms. In this paper, we focus on
identifying task-specific words and phrases from short texts by exploiting
their class labels rather than using seed words or lexical dictionaries. We
consider the task-specific word and phrase identification as feature learning.
We train a convolutional neural network over a set of labeled texts and use
score vectors to localize the task-specific words and phrases. Experimental
results on sentiment word identification show that our approach significantly
outperforms existing methods. We further conduct two case studies to show the
effectiveness of our approach. One case study on a crawled tweets dataset
demonstrates that our approach can successfully capture the
discrimination-related words/phrases. The other case study on fake review
detection shows that our approach can identify the fake-review words/phrases.


ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks

  The Super-Resolution Generative Adversarial Network (SRGAN) is a seminal work
that is capable of generating realistic textures during single image
super-resolution. However, the hallucinated details are often accompanied with
unpleasant artifacts. To further enhance the visual quality, we thoroughly
study three key components of SRGAN - network architecture, adversarial loss
and perceptual loss, and improve each of them to derive an Enhanced SRGAN
(ESRGAN). In particular, we introduce the Residual-in-Residual Dense Block
(RRDB) without batch normalization as the basic network building unit.
Moreover, we borrow the idea from relativistic GAN to let the discriminator
predict relative realness instead of the absolute value. Finally, we improve
the perceptual loss by using the features before activation, which could
provide stronger supervision for brightness consistency and texture recovery.
Benefiting from these improvements, the proposed ESRGAN achieves consistently
better visual quality with more realistic and natural textures than SRGAN and
won the first place in the PIRM2018-SR Challenge. The code is available at
https://github.com/xinntao/ESRGAN .


Achieving non-discrimination in data release

  Discrimination discovery and prevention/removal are increasingly important
tasks in data mining. Discrimination discovery aims to unveil discriminatory
practices on the protected attribute (e.g., gender) by analyzing the dataset of
historical decision records, and discrimination prevention aims to remove
discrimination by modifying the biased data before conducting predictive
analysis. In this paper, we show that the key to discrimination discovery and
prevention is to find the meaningful partitions that can be used to provide
quantitative evidences for the judgment of discrimination. With the support of
the causal graph, we present a graphical condition for identifying a meaningful
partition. Based on that, we develop a simple criterion for the claim of
non-discrimination, and propose discrimination removal algorithms which
accurately remove discrimination while retaining good data utility. Experiments
using real datasets show the effectiveness of our approaches.


A causal framework for discovering and removing direct and indirect
  discrimination

  Anti-discrimination is an increasingly important task in data science. In
this paper, we investigate the problem of discovering both direct and indirect
discrimination from the historical data, and removing the discriminatory
effects before the data is used for predictive analysis (e.g., building
classifiers). We make use of the causal network to capture the causal structure
of the data. Then we model direct and indirect discrimination as the
path-specific effects, which explicitly distinguish the two types of
discrimination as the causal effects transmitted along different paths in the
network. Based on that, we propose an effective algorithm for discovering
direct and indirect discrimination, as well as an algorithm for precisely
removing both types of discrimination while retaining good data utility.
Different from previous works, our approaches can ensure that the predictive
models built from the modified data will not incur discrimination in decision
making. Experiments using real datasets show the effectiveness of our
approaches.


Achieving non-discrimination in prediction

  Discrimination-aware classification is receiving an increasing attention in
data science fields. The pre-process methods for constructing a
discrimination-free classifier first remove discrimination from the training
data, and then learn the classifier from the cleaned data. However, they lack a
theoretical guarantee for the potential discrimination when the classifier is
deployed for prediction. In this paper, we fill this gap by mathematically
bounding the probability of the discrimination in prediction being within a
given interval in terms of the training data and classifier. We adopt the
causal model for modeling the data generation mechanism, and formally defining
discrimination in population, in a dataset, and in prediction. We obtain two
important theoretical results: (1) the discrimination in prediction can still
exist even if the discrimination in the training data is completely removed;
and (2) not all pre-process methods can ensure non-discrimination in prediction
even though they can achieve non-discrimination in the modified training data.
Based on the results, we develop a two-phase framework for constructing a
discrimination-free classifier with a theoretical guarantee. The experiments
demonstrate the theoretical results and show the effectiveness of our two-phase
framework.


Approximate Inverse Frequent Itemset Mining: Privacy, Complexity, and
  Approximation

  In order to generate synthetic basket data sets for better benchmark testing,
it is important to integrate characteristics from real-life databases into the
synthetic basket data sets. The characteristics that could be used for this
purpose include the frequent itemsets and association rules. The problem of
generating synthetic basket data sets from frequent itemsets is generally
referred to as inverse frequent itemset mining. In this paper, we show that the
problem of approximate inverse frequent itemset mining is {\bf NP}-complete.
Then we propose and analyze an approximate algorithm for approximate inverse
frequent itemset mining, and discuss privacy issues related to the synthetic
basket data set. In particular, we propose an approximate algorithm to
determine the privacy leakage in a synthetic basket data set.


Adaptive Laplace Mechanism: Differential Privacy Preservation in Deep
  Learning

  In this paper, we focus on developing a novel mechanism to preserve
differential privacy in deep neural networks, such that: (1) The privacy budget
consumption is totally independent of the number of training steps; (2) It has
the ability to adaptively inject noise into features based on the contribution
of each to the output; and (3) It could be applied in a variety of different
deep neural networks. To achieve this, we figure out a way to perturb affine
transformations of neurons, and loss functions used in deep neural networks. In
addition, our mechanism intentionally adds "more noise" into features which are
"less relevant" to the model output, and vice-versa. Our theoretical analysis
further derives the sensitivities and error bounds of our mechanism. Rigorous
experiments conducted on MNIST and CIFAR-10 datasets show that our mechanism is
highly effective and outperforms existing solutions.


Preserving Differential Privacy in Convolutional Deep Belief Networks

  The remarkable development of deep learning in medicine and healthcare domain
presents obvious privacy issues, when deep neural networks are built on users'
personal and highly sensitive data, e.g., clinical records, user profiles,
biomedical images, etc. However, only a few scientific studies on preserving
privacy in deep learning have been conducted. In this paper, we focus on
developing a private convolutional deep belief network (pCDBN), which
essentially is a convolutional deep belief network (CDBN) under differential
privacy. Our main idea of enforcing epsilon-differential privacy is to leverage
the functional mechanism to perturb the energy-based objective functions of
traditional CDBNs, rather than their results. One key contribution of this work
is that we propose the use of Chebyshev expansion to derive the approximate
polynomial representation of objective functions. Our theoretical analysis
shows that we can further derive the sensitivity and error bounds of the
approximate polynomial representation. As a result, preserving differential
privacy in CDBNs is feasible. We applied our model in a health social network,
i.e., YesiWell data, and in a handwriting digit dataset, i.e., MNIST data, for
human behavior prediction, human behavior classification, and handwriting digit
recognition tasks. Theoretical analysis and rigorous experimental evaluations
show that the pCDBN is highly effective. It significantly outperforms existing
solutions.


SAFE: A Neural Survival Analysis Model for Fraud Early Detection

  Many online platforms have deployed anti-fraud systems to detect and prevent
fraudulent activities. However, there is usually a gap between the time that a
user commits a fraudulent action and the time that the user is suspended by the
platform. How to detect fraudsters in time is a challenging problem. Most of
the existing approaches adopt classifiers to predict fraudsters given their
activity sequences along time. The main drawback of classification models is
that the prediction results between consecutive timestamps are often
inconsistent. In this paper, we propose a survival analysis based fraud early
detection model, SAFE, which maps dynamic user activities to survival
probabilities that are guaranteed to be monotonically decreasing along time.
SAFE adopts recurrent neural network (RNN) to handle user activity sequences
and directly outputs hazard values at each timestamp, and then, survival
probability derived from hazard values is deployed to achieve consistent
predictions. Because we only observe the user suspended time instead of the
fraudulent activity time in the training data, we revise the loss function of
the regular survival model to achieve fraud early detection. Experimental
results on two real world datasets demonstrate that SAFE outperforms both the
survival analysis model and recurrent neural network model alone as well as
state-of-the-art fraud early detection approaches.


