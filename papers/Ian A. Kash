General Truthfulness Characterizations Via Convex Analysis

  We present a model of truthful elicitation which generalizes and extends
mechanisms, scoring rules, and a number of related settings that do not quite
qualify as one or the other. Our main result is a characterization theorem,
yielding characterizations for all of these settings, including a new
characterization of scoring rules for non-convex sets of distributions. We
generalize this model to eliciting some property of the agent's private
information, and provide the first general characterization for this setting.
We also show how this yields a new proof of a result in mechanism design due to
Saks and Yu.


Efficiency and Nash Equilibria in a Scrip System for P2P Networks

  A model of providing service in a P2P network is analyzed. It is shown that
by adding a scrip system, a mechanism that admits a reasonable Nash equilibrium
that reduces free riding can be obtained. The effect of varying the total
amount of money (scrip) in the system on efficiency (i.e., social welfare) is
analyzed, and it is shown that by maintaining the appropriate ratio between the
total amount of money and the number of agents, efficiency is maximized. The
work has implications for many online systems, not only P2P networks but also a
wide variety of online forums for which scrip systems are popular, but formal
analyses have been lacking.


The Lotus-Eater Attack

  Many protocols for distributed and peer-to-peer systems have the feature that
nodes will stop providing service for others once they have received a certain
amount of service. Examples include BitTorent's unchoking policy, BAR Gossip's
balanced exchanges, and threshold strategies in scrip systems. An attacker can
exploit this by providing service in a targeted way to prevent chosen nodes
from providing service. While such attacks cannot be prevented, we discuss
techniques that can be used to limit the damage they do. These techniques
presume that a certain number of processes will follow the recommended
protocol, even if they could do better by ``gaming'' the system.


Fixed and Market Pricing for Cloud Services

  We study a model of congestible resources, where pricing and scheduling are
intertwined. Motivated by the problem of pricing cloud instances, we model a
cloud computing service as linked $GI/GI/\cdot$ queuing systems where the
provider chooses to offer a fixed pricing service, a dynamic market based
service, or a hybrid of both, where jobs can be preempted in the market-based
service. Users (jobs), who are heterogeneous in both the value they place on
service and their cost for waiting, then choose between the services offered.
Combining insights from auction theory with queuing theory we are able to
characterize user equilibrium behavior, and show its insensitivity to the
precise market design mechanism used. We then provide theoretical and
simulation based evidence suggesting that a fixed price typically, though not
always, generates a higher expected revenue than the hybrid system for the
provider.


Optimizing Scrip Systems: Efficiency, Crashes, Hoarders, and Altruists

  We discuss the design of efficient scrip systems and develop tools for
empirically analyzing them. For those interested in the empirical study of
scrip systems, we demonstrate how characteristics of agents in a system can be
inferred from the equilibrium distribution of money. From the perspective of a
system designer, we examine the effect of the money supply on social welfare
and show that social welfare is maximized by increasing the money supply up to
the point that the system experiences a ``monetary crash,'' where money is
sufficiently devalued that no agent is willing to perform a service. We also
examine the implications of the presence of altruists and hoarders on the
performance of the system. While a small number of altruists may improve social
welfare, too many can also cause the system to experience a monetary crash,
which may be bad for social welfare. Hoarders generally decrease social welfare
but, surprisingly, they also promote system stability by helping prevent
monetary crashes. In addition, we provide new technical tools for analyzing and
computing equilibria by showing that our model exhibits strategic
complementarities, which implies that there exist equilibria in pure strategies
that can be computed efficiently.


Truthful Mechanisms for Agents that Value Privacy

  Recent work has constructed economic mechanisms that are both truthful and
differentially private. In these mechanisms, privacy is treated separately from
the truthfulness; it is not incorporated in players' utility functions (and
doing so has been shown to lead to non-truthfulness in some cases). In this
work, we propose a new, general way of modelling privacy in players' utility
functions. Specifically, we only assume that if an outcome $o$ has the property
that any report of player $i$ would have led to $o$ with approximately the same
probability, then $o$ has small privacy cost to player $i$. We give three
mechanisms that are truthful with respect to our modelling of privacy: for an
election between two candidates, for a discrete version of the facility
location problem, and for a general social choice problem with discrete
utilities (via a VCG-like mechanism). As the number $n$ of players increases,
the social welfare achieved by our mechanisms approaches optimal (as a fraction
of $n$).


Ranking and Tradeoffs in Sponsored Search Auctions

  In a sponsored search auction, decisions about how to rank ads impose
tradeoffs between objectives such as revenue and welfare. In this paper, we
examine how these tradeoffs should be made. We begin by arguing that the most
natural solution concept to evaluate these tradeoffs is the lowest symmetric
Nash equilibrium (SNE). As part of this argument, we generalise the well known
connection between the lowest SNE and the VCG outcome. We then propose a new
ranking algorithm, loosely based on the revenue-optimal auction, that uses a
reserve price to order the ads (not just to filter them) and give conditions
under which it raises more revenue than simply applying that reserve price.
Finally, we conduct extensive simulations examining the tradeoffs enabled by
different ranking algorithms and show that our proposed algorithm enables
superior operating points by a variety of metrics.


An Equilibrium Analysis of Scrip Systems

  A game-theoretic model of scrip (artificial currency) systems is analyzed. It
is shown that relative entropy can be used to characterize the distribution of
agent wealth when all agents use threshold strategies---that is, they volunteer
to do work iff they have below a threshold amount of money. Monotonicity of
agents' best-reply functions is used to show that scrip systems have pure
strategy equilibria where all agents use threshold strategies. An algorithm is
given that can compute such an equilibrium and the resulting distribution of
wealth.


Multiagent Learning in Large Anonymous Games

  In large systems, it is important for agents to learn to act effectively, but
sophisticated multi-agent learning algorithms generally do not scale. An
alternative approach is to find restricted classes of games where simple,
efficient algorithms converge. It is shown that stage learning efficiently
converges to Nash equilibria in large anonymous games if best-reply dynamics
converge. Two features are identified that improve convergence. First, rather
than making learning more difficult, more agents are actually beneficial in
many settings. Second, providing agents with statistical information about the
behavior of others can significantly reduce the number of observations needed.


Optimal Auctions with Restricted Allocations

  We study the problem of designing optimal auctions under restrictions on the
set of permissible allocations. In addition to allowing us to restrict to
deterministic mechanisms, we can also indirectly model non-additive valuations.
We prove a strong duality result, extending a result due to Daskalakis et al.
[2015], that guarantees the existence of a certificate of optimality for
optimal restricted mechanisms. As a corollary of our result, we provide a new
characterization of the set of allocations that the optimal mechanism may
actually use. To illustrate our result we find and certify optimal mechanisms
for four settings where previous frameworks do not apply, and provide new
economic intuition about some of the tools that have previously been used to
find optimal mechanisms.


Partial Verification as a Substitute for Money

  Recent work shows that we can use partial verification instead of money to
implement truthful mechanisms. In this paper we develop tools to answer the
following question. Given an allocation rule that can be made truthful with
payments, what is the minimal verification needed to make it truthful without
them? Our techniques leverage the geometric relationship between the type space
and the set of possible allocations.


Mix and Match

  Consider a matching problem on a graph where disjoint sets of vertices are
privately owned by self-interested agents. An edge between a pair of vertices
indicates compatibility and allows the vertices to match. We seek a mechanism
to maximize the number of matches despite self-interest, with agents that each
want to maximize the number of their own vertices that match. Each agent can
choose to hide some of its vertices, and then privately match the hidden
vertices with any of its own vertices that go unmatched by the mechanism. A
prominent application of this model is to kidney exchange, where agents
correspond to hospitals and vertices to donor-patient pairs. Here hospitals may
game an exchange by holding back pairs and harm social welfare. In this paper
we seek to design mechanisms that are strategyproof, in the sense that agents
cannot benefit from hiding vertices, and approximately maximize efficiency,
i.e., produce a matching that is close in cardinality to the maximum
cardinality matching. Our main result is the design and analysis of the
eponymous Mix-and-Match mechanism; we show that this randomized mechanism is
strategyproof and provides a 2-approximation. Lower bounds establish that the
mechanism is near optimal.


Optimizing Scrip Systems: Crashes, Altruists, Hoarders, Sybils and
  Collusion

  Scrip, or artificial currency, is a useful tool for designing systems that
are robust to selfish behavior by users. However, it also introduces problems
for a system designer, such as how the amount of money in the system should be
set. In this paper, the effect of varying the total amount of money in a scrip
system on efficiency (i.e., social welfare---the total utility of all the
agents in the system) is analyzed, and it is shown that by maintaining the
appropriate ratio between the total amount of money and the number of agents,
efficiency is maximized. This ratio can be found by increasing the money supply
to just below the point that the system would experience a "monetary crash,"
where money is sufficiently devalued that no agent is willing to perform a
service. The implications of the presence of altruists, hoarders, sybils, and
collusion on the performance of the system are examined. Approaches are
discussed to identify the strategies and types of agents.


Manipulating Scrip Systems: Sybils and Collusion

  Game-theoretic analyses of distributed and peer-to-peer systems typically use
the Nash equilibrium solution concept, but this explicitly excludes the
possibility of strategic behavior involving more than one agent. We examine the
effects of two types of strategic behavior involving more than one agent,
sybils and collusion, in the context of scrip systems where agents provide each
other with service in exchange for scrip. Sybils make an agent more likely to
be chosen to provide service, which generally makes it harder for agents
without sybils to earn money and decreases social welfare. Surprisingly, in
certain circumstances it is possible for sybils to make all agents better off.
While collusion is generally bad, in the context of scrip systems it actually
tends to make all agents better off, not merely those who collude. These
results also provide insight into the effects of allowing agents to advertise
and loan money. While many extensions of Nash equilibrium have been proposed
that address collusion and other issues relevant to distributed and
peer-to-peer systems, our results show that none of them adequately address the
issues raised by sybils and collusion in scrip systems.


Elicitation Complexity of Statistical Properties

  A property, or statistical functional, is said to be elicitable if it
minimizes expected loss for some loss function. The study of which properties
are elicitable sheds light on the capabilities and limits of empirical risk
minimization. While several recent papers have asked which properties are
elicitable, we instead advocate for a more nuanced question: how many
dimensions are required to indirectly elicit a given property? This number is
called the elicitation complexity of the property. We lay the foundation for a
general theory of elicitation complexity, including several basic results about
how elicitation complexity behaves, and the complexity of standard properties
of interest. Building on this foundation, we establish several upper and lower
bounds for the broad class of Bayes risks. We apply these results by proving
tight complexity bounds, with respect to identifiable properties, for variance,
financial risk measures, entropy, norms, and new properties of interest. We
then show how some of these bounds can extend to other practical classes of
properties, and conclude with a discussion of open directions.


On the Zero-Error Capacity Threshold for Deletion Channels

  We consider the zero-error capacity of deletion channels. Specifically, we
consider the setting where we choose a codebook ${\cal C}$ consisting of
strings of $n$ bits, and our model of the channel corresponds to an adversary
who may delete up to $pn$ of these bits for a constant $p$. Our goal is to
decode correctly without error regardless of the actions of the adversary. We
consider what values of $p$ allow non-zero capacity in this setting. We suggest
multiple approaches, one of which makes use of the natural connection between
this problem and the problem of finding the expected length of the longest
common subsequence of two random sequences.


Elicitation for Aggregation

  We study the problem of eliciting and aggregating probabilistic information
from multiple agents. In order to successfully aggregate the predictions of
agents, the principal needs to elicit some notion of confidence from agents,
capturing how much experience or knowledge led to their predictions. To
formalize this, we consider a principal who wishes to elicit predictions about
a random variable from a group of Bayesian agents, each of whom have privately
observed some independent samples of the random variable, and hopes to
aggregate the predictions as if she had directly observed the samples of all
agents. Leveraging techniques from Bayesian statistics, we represent confidence
as the number of samples an agent has observed, which is quantified by a
hyperparameter from a conjugate family of prior distributions. This then allows
us to show that if the principal has access to a few samples, she can achieve
her aggregation goal by eliciting predictions from agents using proper scoring
rules. In particular, if she has access to one sample, she can successfully
aggregate the agents' predictions if and only if every posterior predictive
distribution corresponds to a unique value of the hyperparameter. Furthermore,
this uniqueness holds for many common distributions of interest. When this
uniqueness property does not hold, we construct a novel and intuitive mechanism
where a principal with two samples can elicit and optimally aggregate the
agents' predictions.


Simple Pricing Schemes for the Cloud

  The problem of pricing the cloud has attracted much recent attention due to
the widespread use of cloud computing and cloud services. From a theoretical
perspective, several mechanisms that provide strong efficiency or fairness
guarantees and desirable incentive properties have been designed. However,
these mechanisms often rely on a rigid model, with several parameters needing
to be precisely known in order for the guarantees to hold. In this paper, we
consider a stochastic model and show that it is possible to obtain good welfare
and revenue guarantees with simple mechanisms that do not make use of the
information on some of these parameters. In particular, we prove that a
mechanism that sets the same price per time step for jobs of any length
achieves at least 50% of the welfare and revenue obtained by a mechanism that
can set different prices for jobs of different lengths, and the ratio can be
improved if we have more specific knowledge of some parameters. Similarly, a
mechanism that sets the same price for all servers even though the servers may
receive different kinds of jobs can provide a reasonable welfare and revenue
approximation compared to a mechanism that is allowed to set different prices
for different servers.


Bayesian Admission Policies for Cloud Computing Clusters

  Cloud computing providers must handle heterogeneous customer workloads for
resources such as (virtual) CPU or GPU cores. This is particularly challenging
if customers, who are already running a job on a cluster, scale their resource
usage up and down over time. The provider therefore has to continuously decide
whether she can add additional workloads to a given cluster or if doing so
would impact existing workloads' ability to scale. Currently, this is often
done using simple threshold policies to reserve large parts of each cluster,
which leads to low average utilization of the cluster. In this paper, we
propose more sophisticated Bayesian policies for controlling admission to a
cluster and demonstrate that they significantly increase cluster utilization.
We first introduce the cluster admission problem and formalize it as a
constrained Partially Observable Markov Decision Problem (POMDP). We then fit
the parameters of the POMDP on a data trace from Microsoft Azure. As it is
infeasible to solve the POMDP optimally, we then systematically design
heuristic Bayesian admission policies that estimate moments of each workload's
distribution of future resource usage. Via simulations we show that our
Bayesian admission policies lead to a substantial improvement over the simple
threshold policy. We then evaluate how much further this can be improved with
learned or elicited prior information and how to incentivize users to provide
this information.


Optimising Trade-offs Among Stakeholders in Ad Auctions

  We examine trade-offs among stakeholders in ad auctions. Our metrics are the
revenue for the utility of the auctioneer, the number of clicks for the utility
of the users and the welfare for the utility of the advertisers. We show how to
optimize linear combinations of the stakeholder utilities, showing that these
can be tackled through a GSP auction with a per-click reserve price. We then
examine constrained optimization of stakeholder utilities.
  We use simulations and analysis of real-world sponsored search auction data
to demonstrate the feasible trade-offs, examining the effect of changing the
allowed number of ads on the utilities of the stakeholders. We investigate both
short term effects, when the players do not have the time to modify their
behavior, and long term equilibrium conditions.
  Finally, we examine a combinatorially richer constrained optimization
problem, where there are several possible allowed configurations (templates) of
ad formats. This model captures richer ad formats, which allow using the
available screen real estate in various ways. We show that two natural
generalizations of the GSP auction rules to this domain are poorly behaved,
resulting in not having a symmetric Nash equilibrium or having one with poor
welfare. We also provide positive results for restricted cases.


Decentralised Norm Monitoring in Open Multi-Agent Systems

  We consider the problem of detecting norm violations in open multi-agent
systems (MAS). We show how, using ideas from scrip systems, we can design
mechanisms where the agents comprising the MAS are incentivised to monitor the
actions of other agents for norm violations. The cost of providing the
incentives is not borne by the MAS and does not come from fines charged for
norm violations (fines may be impossible to levy in a system where agents are
free to leave and rejoin again under a different identity). Instead, monitoring
incentives come from (scrip) fees for accessing the services provided by the
MAS. In some cases, perfect monitoring (and hence enforcement) can be achieved:
no norms will be violated in equilibrium. In other cases, we show that, while
it is impossible to achieve perfect enforcement, we can get arbitrarily close;
we can make the probability of a norm violation in equilibrium arbitrarily
small. We show using simulations that our theoretical results hold for
multi-agent systems with as few as 1000 agents---the system rapidly converges
to the steady-state distribution of scrip tokens necessary to ensure monitoring
and then remains close to the steady state.


