Embedding a Forest in a Graph

  For \math{p\ge 1}, we prove that every forest with \math{p} trees whose sizesare $a_1,..., a_p$ can be embedded in any graph containing at least$\sum_{i=1}^p (a_i + 1)$ vertices and having a minimum degree at least$\sum_{i=1}^p a_i$.

A Note On Estimating the Spectral Norm of A Matrix Efficiently

  We give an efficient algorithm which can obtain a relative errorapproximation to the spectral norm of a matrix, combining the power iterationmethod with some techniques from matrix reconstruction which use randomsampling.

NP-Hardness and Inapproximability of Sparse PCA

  We give a reduction from {\sc clique} to establish that sparse PCA isNP-hard. The reduction has a gap which we use to exclude an FPTAS for sparsePCA (unless P=NP). Under weaker complexity assumptions, we also excludepolynomial constant-factor approximation algorithms.

Node-By-Node Greedy Deep Learning for Interpretable Features

  Multilayer networks have seen a resurgence under the umbrella of deeplearning. Current deep learning algorithms train the layers of the networksequentially, improving algorithmic performance as well as providing someregularization. We present a new training algorithm for deep networks whichtrains \emph{each node in the network} sequentially. Our algorithm is orders ofmagnitude faster, creates more interpretable internal representations at thenode level, while not sacrificing on the ultimate out-of-sample performance.

Efficient Computation of Optimal Trading Strategies

  Given the return series for a set of instruments, a \emph{trading strategy}is a switching function that transfers wealth from one instrument to another atspecified times. We present efficient algorithms for constructing (ex-post)trading strategies that are optimal with respect to the total return, theSterling ratio and the Sharpe ratio. Such ex-post optimal strategies are usefulanalysis tools. They can be used to analyze the "profitability of a market" interms of optimal trading; to develop benchmarks against which real trading canbe compared; and, within an inductive framework, the optimal trades can be usedto to teach learning systems (predictors) which are then used to identifyfuture trading opportunities.

Using a Non-Commutative Bernstein Bound to Approximate Some Matrix  Algorithms in the Spectral Norm

  We focus on \emph{row sampling} based approximations for matrix algorithms,in particular matrix multipication, sparse matrix reconstruction, and\math{\ell_2} regression. For \math{\matA\in\R^{m\times d}} (\math{m} points in\math{d\ll m} dimensions), and appropriate row-sampling probabilities, whichtypically depend on the norms of the rows of the \math{m\times d} left singularmatrix of \math{\matA} (the \emph{leverage scores}), we give row-samplingalgorithms with linear (up to polylog factors) dependence on the stable rank of\math{\matA}. This result is achieved through the application ofnon-commutative Bernstein bounds. Keywords: row-sampling; matrixmultiplication; matrix reconstruction; estimating spectral norm; linearregression; randomized

Spreading Processes and Large Components in Ordered, Directed Random  Graphs

  Order the vertices of a directed random graph \math{v_1,...,v_n}; edge\math{(v_i,v_j)} for \math{i<j} exists independently with probability \math{p}.This random graph model is related to certain spreading processes on networks.We consider the component reachable from \math{v_1} and prove existence of asharp threshold \math{p^*=\log n/n} at which this reachable componenttransitions from \math{o(n)} to \math{\Omega(n)}.

Seeding Influential Nodes in Non-Submodular Models of Information  Diffusion

  We consider the model of information diffusion in social networks from\cite{Hui2010a} which incorporates trust (weighted links) between actors, andallows actors to actively participate in the spreading process, specificallythrough the ability to query friends for additional information. This modelcaptures how social agents transmit and act upon information more realisticallyas compared to the simpler threshold and cascade models. However, it is moredifficult to analyze, in particular with respect to seeding strategies. Wepresent efficient, scalable algorithms for determining good seed sets --initial nodes to inject with the information. Our general approach is to reduceour model to a class of simpler models for which provably good sets can beconstructed. By tuning this class of simpler models, we obtain a good seed setfor the original more complex model. We call this the \emph{projected greedyapproach} because you `project' your model onto a class of simpler models wherea greedy seed set selection is near-optimal. We demonstrate the effectivenessof our seeding strategy on synthetic graphs as well as a realistic San Diegoevacuation network constructed during the 2007 fires.

Optimal Sparse Linear Auto-Encoders and Sparse PCA

  Principal components analysis (PCA) is the optimal linear auto-encoder ofdata, and it is often used to construct features. Enforcing sparsity on theprincipal components can promote better generalization, while improving theinterpretability of the features. We study the problem of constructing optimalsparse linear auto-encoders. Two natural questions in such a setting are: i)Given a level of sparsity, what is the best approximation to PCA that can beachieved? ii) Are there low-order polynomial-time algorithms which canasymptotically achieve this optimal tradeoff between the sparsity and theapproximation quality?  In this work, we answer both questions by giving efficient low-orderpolynomial-time algorithms for constructing asymptotically \emph{optimal}linear auto-encoders (in particular, sparse features with near-PCAreconstruction error) and demonstrate the performance of our algorithms on realdata.

PD-ML-Lite: Private Distributed Machine Learning from Lighweight  Cryptography

  Privacy is a major issue in learning from distributed data. Recently thecryptographic literature has provided several tools for this task. However,these tools either reduce the quality/accuracy of the learningalgorithm---e.g., by adding noise---or they incur a high performance penaltyand/or involve trusting external authorities.  We propose a methodology for {\sl private distributed machine learning fromlight-weight cryptography} (in short, PD-ML-Lite). We apply our methodology totwo major ML algorithms, namely non-negative matrix factorization (NMF) andsingular value decomposition (SVD). Our resulting protocols are communicationoptimal, achieve the same accuracy as their non-private counterparts, andsatisfy a notion of privacy---which we define---that is both intuitive andmeasurable. Our approach is to use lightweight cryptographic protocols (securesum and normalized secure sum) to build learning algorithms rather than wrapcomplex learning algorithms in a heavy-cost MPC framework.  We showcase our algorithms' utility and privacy on several applications: forNMF we consider topic modeling and recommender systems, and for SVD, principalcomponent regression, and low rank approximation.

Row Sampling for Matrix Algorithms via a Non-Commutative Bernstein Bound

  We focus the use of \emph{row sampling} for approximating matrix algorithms.We give applications to matrix multipication; sparse matrix reconstruction;and, \math{\ell_2} regression. For a matrix \math{\matA\in\R^{m\times d}} whichrepresents \math{m} points in \math{d\ll m} dimensions, all of these tasks canbe achieved in \math{O(md^2)} via the singular value decomposition (SVD). Forappropriate row-sampling probabilities (which typically depend on the norms ofthe rows of the \math{m\times d} left singular matrix of \math{\matA} (the\emph{leverage scores}), we give row-sampling algorithms with linear (up topolylog factors) dependence on the stable rank of \math{\matA}. This result isachieved through the application of non-commutative Bernstein bounds.  We then give, to our knowledge, the first algorithms for computingapproximations to the appropriate row-sampling probabilities without goingthrough the SVD of \math{\matA}. Thus, these are the first \math{o(md^2)}algorithms for row-sampling based approximations to the matrix algorithms whichuse leverage scores as the sampling probabilities. The techniques we use toapproximate sampling according to the leverage scores uses some powerful recentresults in the theory of random projections for embedding, and may be of someindependent interest. We confess that one may perform all these matrix tasksmore efficiently using these same random projection methods, however theresulting algorithms are in terms of a small number of linear combinations ofall the rows. In many applications, the actual rows of \math{\matA} have somephysical meaning and so methods based on a small number of the actual rows areof interest.

An Analysis of Optimal Link Bombs

  We analyze the phenomenon of collusion for the purpose of boosting thepagerank of a node in an interlinked environment. We investigate the optimalattack pattern for a group of nodes (attackers) attempting to improve theranking of a specific node (the victim). We consider attacks where theattackers can only manipulate their own outgoing links. We show that theoptimal attacks in this scenario are uncoordinated, i.e. the attackers linkdirectly to the victim and no one else. nodes do not link to each other. Wealso discuss optimal attack patterns for a group that wants to hide itself bynot pointing directly to the victim. In these disguised attacks, the attackerslink to nodes $l$ hops away from the victim. We show that an optimal disguisedattack exists and how it can be computed. The optimal disguised attack alsoallows us to find optimal link farm configurations. A link farm can beconsidered a special case of our approach: the target page of the link farm isthe victim and the other nodes in the link farm are the attackers for thepurpose of improving the rank of the victim. The target page can howevercontrol its own outgoing links for the purpose of improving its own rank, whichcan be modeled as an optimal disguised attack of 1-hop on itself. Our resultsare unique in the literature as we show optimality not only in the pagerankscore, but also in the rank based on the pagerank score. We further validateour results with experiments on a variety of random graph models.

Extracting Hidden Groups and their Structure from Streaming Interaction  Data

  When actors in a social network interact, it usually means they have somegeneral goal towards which they are collaborating. This could be a researchcollaboration in a company or a foursome planning a golf game. We call suchgroups \emph{planning groups}. In many social contexts, it might be possible toobserve the \emph{dyadic interactions} between actors, even if the actors donot explicitly declare what groups they belong too. When groups are notexplicitly declared, we call them \emph{hidden groups}. Our particular focus ishidden planning groups. By virtue of their need to further their goal, theactors within such groups must interact in a manner which differentiates theircommunications from random background communications. In such a case, one caninfer (from these interactions) the composition and structure of the hiddenplanning groups. We formulate the problem of hidden group discovery fromstreaming interaction data, and we propose efficient algorithms for identifyingthe hidden group structures by isolating the hidden group's non-random,planning-related, communications from the random background communications. Wevalidate our algorithms on real data (the Enron email corpus and Blogcommunication data). Analysis of the results reveals that our algorithmsextract meaningful hidden group structures.

