Embedding a Forest in a Graph

  For \math{p\ge 1}, we prove that every forest with \math{p} trees whose sizes
are $a_1,..., a_p$ can be embedded in any graph containing at least
$\sum_{i=1}^p (a_i + 1)$ vertices and having a minimum degree at least
$\sum_{i=1}^p a_i$.


A Note On Estimating the Spectral Norm of A Matrix Efficiently

  We give an efficient algorithm which can obtain a relative error
approximation to the spectral norm of a matrix, combining the power iteration
method with some techniques from matrix reconstruction which use random
sampling.


Node-By-Node Greedy Deep Learning for Interpretable Features

  Multilayer networks have seen a resurgence under the umbrella of deep
learning. Current deep learning algorithms train the layers of the network
sequentially, improving algorithmic performance as well as providing some
regularization. We present a new training algorithm for deep networks which
trains \emph{each node in the network} sequentially. Our algorithm is orders of
magnitude faster, creates more interpretable internal representations at the
node level, while not sacrificing on the ultimate out-of-sample performance.


NP-Hardness and Inapproximability of Sparse PCA

  We give a reduction from {\sc clique} to establish that sparse PCA is
NP-hard. The reduction has a gap which we use to exclude an FPTAS for sparse
PCA (unless P=NP). Under weaker complexity assumptions, we also exclude
polynomial constant-factor approximation algorithms.


Efficient Computation of Optimal Trading Strategies

  Given the return series for a set of instruments, a \emph{trading strategy}
is a switching function that transfers wealth from one instrument to another at
specified times. We present efficient algorithms for constructing (ex-post)
trading strategies that are optimal with respect to the total return, the
Sterling ratio and the Sharpe ratio. Such ex-post optimal strategies are useful
analysis tools. They can be used to analyze the "profitability of a market" in
terms of optimal trading; to develop benchmarks against which real trading can
be compared; and, within an inductive framework, the optimal trades can be used
to to teach learning systems (predictors) which are then used to identify
future trading opportunities.


Using a Non-Commutative Bernstein Bound to Approximate Some Matrix
  Algorithms in the Spectral Norm

  We focus on \emph{row sampling} based approximations for matrix algorithms,
in particular matrix multipication, sparse matrix reconstruction, and
\math{\ell_2} regression. For \math{\matA\in\R^{m\times d}} (\math{m} points in
\math{d\ll m} dimensions), and appropriate row-sampling probabilities, which
typically depend on the norms of the rows of the \math{m\times d} left singular
matrix of \math{\matA} (the \emph{leverage scores}), we give row-sampling
algorithms with linear (up to polylog factors) dependence on the stable rank of
\math{\matA}. This result is achieved through the application of
non-commutative Bernstein bounds. Keywords: row-sampling; matrix
multiplication; matrix reconstruction; estimating spectral norm; linear
regression; randomized


Spreading Processes and Large Components in Ordered, Directed Random
  Graphs

  Order the vertices of a directed random graph \math{v_1,...,v_n}; edge
\math{(v_i,v_j)} for \math{i<j} exists independently with probability \math{p}.
This random graph model is related to certain spreading processes on networks.
We consider the component reachable from \math{v_1} and prove existence of a
sharp threshold \math{p^*=\log n/n} at which this reachable component
transitions from \math{o(n)} to \math{\Omega(n)}.


Seeding Influential Nodes in Non-Submodular Models of Information
  Diffusion

  We consider the model of information diffusion in social networks from
\cite{Hui2010a} which incorporates trust (weighted links) between actors, and
allows actors to actively participate in the spreading process, specifically
through the ability to query friends for additional information. This model
captures how social agents transmit and act upon information more realistically
as compared to the simpler threshold and cascade models. However, it is more
difficult to analyze, in particular with respect to seeding strategies. We
present efficient, scalable algorithms for determining good seed sets --
initial nodes to inject with the information. Our general approach is to reduce
our model to a class of simpler models for which provably good sets can be
constructed. By tuning this class of simpler models, we obtain a good seed set
for the original more complex model. We call this the \emph{projected greedy
approach} because you `project' your model onto a class of simpler models where
a greedy seed set selection is near-optimal. We demonstrate the effectiveness
of our seeding strategy on synthetic graphs as well as a realistic San Diego
evacuation network constructed during the 2007 fires.


Optimal Sparse Linear Auto-Encoders and Sparse PCA

  Principal components analysis (PCA) is the optimal linear auto-encoder of
data, and it is often used to construct features. Enforcing sparsity on the
principal components can promote better generalization, while improving the
interpretability of the features. We study the problem of constructing optimal
sparse linear auto-encoders. Two natural questions in such a setting are: i)
Given a level of sparsity, what is the best approximation to PCA that can be
achieved? ii) Are there low-order polynomial-time algorithms which can
asymptotically achieve this optimal tradeoff between the sparsity and the
approximation quality?
  In this work, we answer both questions by giving efficient low-order
polynomial-time algorithms for constructing asymptotically \emph{optimal}
linear auto-encoders (in particular, sparse features with near-PCA
reconstruction error) and demonstrate the performance of our algorithms on real
data.


PD-ML-Lite: Private Distributed Machine Learning from Lighweight
  Cryptography

  Privacy is a major issue in learning from distributed data. Recently the
cryptographic literature has provided several tools for this task. However,
these tools either reduce the quality/accuracy of the learning
algorithm---e.g., by adding noise---or they incur a high performance penalty
and/or involve trusting external authorities.
  We propose a methodology for {\sl private distributed machine learning from
light-weight cryptography} (in short, PD-ML-Lite). We apply our methodology to
two major ML algorithms, namely non-negative matrix factorization (NMF) and
singular value decomposition (SVD). Our resulting protocols are communication
optimal, achieve the same accuracy as their non-private counterparts, and
satisfy a notion of privacy---which we define---that is both intuitive and
measurable. Our approach is to use lightweight cryptographic protocols (secure
sum and normalized secure sum) to build learning algorithms rather than wrap
complex learning algorithms in a heavy-cost MPC framework.
  We showcase our algorithms' utility and privacy on several applications: for
NMF we consider topic modeling and recommender systems, and for SVD, principal
component regression, and low rank approximation.


An Analysis of Optimal Link Bombs

  We analyze the phenomenon of collusion for the purpose of boosting the
pagerank of a node in an interlinked environment. We investigate the optimal
attack pattern for a group of nodes (attackers) attempting to improve the
ranking of a specific node (the victim). We consider attacks where the
attackers can only manipulate their own outgoing links. We show that the
optimal attacks in this scenario are uncoordinated, i.e. the attackers link
directly to the victim and no one else. nodes do not link to each other. We
also discuss optimal attack patterns for a group that wants to hide itself by
not pointing directly to the victim. In these disguised attacks, the attackers
link to nodes $l$ hops away from the victim. We show that an optimal disguised
attack exists and how it can be computed. The optimal disguised attack also
allows us to find optimal link farm configurations. A link farm can be
considered a special case of our approach: the target page of the link farm is
the victim and the other nodes in the link farm are the attackers for the
purpose of improving the rank of the victim. The target page can however
control its own outgoing links for the purpose of improving its own rank, which
can be modeled as an optimal disguised attack of 1-hop on itself. Our results
are unique in the literature as we show optimality not only in the pagerank
score, but also in the rank based on the pagerank score. We further validate
our results with experiments on a variety of random graph models.


Row Sampling for Matrix Algorithms via a Non-Commutative Bernstein Bound

  We focus the use of \emph{row sampling} for approximating matrix algorithms.
We give applications to matrix multipication; sparse matrix reconstruction;
and, \math{\ell_2} regression. For a matrix \math{\matA\in\R^{m\times d}} which
represents \math{m} points in \math{d\ll m} dimensions, all of these tasks can
be achieved in \math{O(md^2)} via the singular value decomposition (SVD). For
appropriate row-sampling probabilities (which typically depend on the norms of
the rows of the \math{m\times d} left singular matrix of \math{\matA} (the
\emph{leverage scores}), we give row-sampling algorithms with linear (up to
polylog factors) dependence on the stable rank of \math{\matA}. This result is
achieved through the application of non-commutative Bernstein bounds.
  We then give, to our knowledge, the first algorithms for computing
approximations to the appropriate row-sampling probabilities without going
through the SVD of \math{\matA}. Thus, these are the first \math{o(md^2)}
algorithms for row-sampling based approximations to the matrix algorithms which
use leverage scores as the sampling probabilities. The techniques we use to
approximate sampling according to the leverage scores uses some powerful recent
results in the theory of random projections for embedding, and may be of some
independent interest. We confess that one may perform all these matrix tasks
more efficiently using these same random projection methods, however the
resulting algorithms are in terms of a small number of linear combinations of
all the rows. In many applications, the actual rows of \math{\matA} have some
physical meaning and so methods based on a small number of the actual rows are
of interest.


Extracting Hidden Groups and their Structure from Streaming Interaction
  Data

  When actors in a social network interact, it usually means they have some
general goal towards which they are collaborating. This could be a research
collaboration in a company or a foursome planning a golf game. We call such
groups \emph{planning groups}. In many social contexts, it might be possible to
observe the \emph{dyadic interactions} between actors, even if the actors do
not explicitly declare what groups they belong too. When groups are not
explicitly declared, we call them \emph{hidden groups}. Our particular focus is
hidden planning groups. By virtue of their need to further their goal, the
actors within such groups must interact in a manner which differentiates their
communications from random background communications. In such a case, one can
infer (from these interactions) the composition and structure of the hidden
planning groups. We formulate the problem of hidden group discovery from
streaming interaction data, and we propose efficient algorithms for identifying
the hidden group structures by isolating the hidden group's non-random,
planning-related, communications from the random background communications. We
validate our algorithms on real data (the Enron email corpus and Blog
communication data). Analysis of the results reveals that our algorithms
extract meaningful hidden group structures.


