Using Context Events in Neural Network Models for Event Temporal Status  Identification

  Focusing on the task of identifying event temporal status, we find thatevents directly or indirectly governing the target event in a dependency treeare most important contexts. Therefore, we extract dependency chains containingcontext events and use them as input in neural network models, whichconsistently outperform previous models using local context words as input.Visualization verifies that the dependency chain representation can effectivelycapture the context events which are closely related to the target event andplay key roles in predicting event temporal status.

Recognizing Explicit and Implicit Hate Speech Using a Weakly Supervised  Two-path Bootstrapping Approach

  In the wake of a polarizing election, social media is laden with hatefulcontent. To address various limitations of supervised hate speechclassification methods including corpus bias and huge cost of annotation, wepropose a weakly supervised two-path bootstrapping approach for an online hatespeech detection model leveraging large-scale unlabeled data. This systemsignificantly outperforms hate speech detection systems that are trained in asupervised manner using manually annotated data. Applying this model on a largequantity of tweets collected before, after, and on election day revealsmotivations and patterns of inflammatory language.

Improving Implicit Discourse Relation Classification by Modeling  Inter-dependencies of Discourse Units in a Paragraph

  We argue that semantic meanings of a sentence or clause can not beinterpreted independently from the rest of a paragraph, or independently fromall discourse relations and the overall paragraph-level discourse structure.With the goal of improving implicit discourse relation classification, weintroduce a paragraph-level neural networks that model inter-dependenciesbetween discourse units as well as discourse relation continuity and patterns,and predict a sequence of discourse relations in a paragraph. Experimentalresults show that our model outperforms the previous state-of-the-art systemson the benchmark corpus of PDTB.

Temporal Event Knowledge Acquisition via Identifying Narratives

  Inspired by the double temporality characteristic of narrative texts, wepropose a novel approach for acquiring rich temporal "before/after" eventknowledge across sentences in narrative stories. The double temporality statesthat a narrative story often describes a sequence of events following thechronological order and therefore, the temporal order of events matches withtheir textual order. We explored narratology principles and built a weaklysupervised approach that identifies 287k narrative paragraphs from three largetext corpora. We then extracted rich temporal event knowledge from thesenarrative paragraphs. Such event knowledge is shown useful to improve temporalrelation classification and outperform several recent neural network models onthe narrative cloze task.

A Sequential Model for Classifying Temporal Relations between  Intra-Sentence Events

  We present a sequential model for temporal relation classification betweenintra-sentence events. The key observation is that the overall syntacticstructure and compositional meanings of the multi-word context between eventsare important for distinguishing among fine-grained temporal relations.Specifically, our approach first extracts a sequence of context words thatindicates the temporal relation between two events, which well align with thedependency path between two event mentions. The context word sequence, togetherwith a parts-of-speech tag sequence and a dependency relation sequence that aregenerated corresponding to the word sequence, are then provided as input tobidirectional recurrent neural network (LSTM) models. The neural nets learncompositional syntactic and semantic representations of contexts surroundingthe two events and predict the temporal relation between them. Evaluation ofthe proposed approach on TimeBank corpus shows that sequential modeling iscapable of accurately recognizing temporal relations between events, whichoutperforms a neural net model using various discrete features as input thatimitates previous feature based models.

Event Coreference Resolution by Iteratively Unfolding Inter-dependencies  among Events

  We introduce a novel iterative approach for event coreference resolution thatgradually builds event clusters by exploiting inter-dependencies among eventmentions within the same chain as well as across event chains. Among eventmentions in the same chain, we distinguish within- and cross-document eventcoreference links by using two distinct pairwise classifiers, trainedseparately to capture differences in feature distributions of within- andcross-document event clusters. Our event coreference approach alternatesbetween WD and CD clustering and combines arguments from both event clustersafter every merge, continuing till no more merge can be made. And then itperforms further merging between event chains that are both closely related toa set of other chains of events. Experiments on the ECB+ corpus show that ourmodel outperforms state-of-the-art methods in joint task of WD and CD eventcoreference resolution.

Online Deception Detection Refueled by Real World Data Collection

  The lack of large realistic datasets presents a bottleneck in onlinedeception detection studies. In this paper, we apply a data collection methodbased on social network analysis to quickly identify high-quality deceptive andtruthful online reviews from Amazon. The dataset contains more than 10,000deceptive reviews and is diverse in product domains and reviewers. Using thisdataset, we explore effective general features for online deception detectionthat perform well across domains. We demonstrate that with generalized features- advertising speak and writing complexity scores - deception detectionperformance can be further improved by adding additional deceptive reviews fromassorted domains in training. Finally, reviewer level evaluation gives aninteresting insight into different deceptive reviewers' writing styles.

A Weakly Supervised Approach to Train Temporal Relation Classifiers and  Acquire Regular Event Pairs Simultaneously

  Capabilities of detecting temporal relations between two events can benefitmany applications. Most of existing temporal relation classifiers were trainedin a supervised manner. Instead, we explore the observation that regular eventpairs show a consistent temporal relation despite of their various contexts,and these rich contexts can be used to train a contextual temporal relationclassifier, which can further recognize new temporal relation contexts andidentify new regular event pairs. We focus on detecting after and beforetemporal relations and design a weakly supervised learning approach thatextracts thousands of regular event pairs and learns a contextual temporalrelation classifier simultaneously. Evaluation shows that the acquired regularevent pairs are of high quality and contain rich commonsense knowledge anddomain specific knowledge. In addition, the weakly supervised trained temporalrelation classifier achieves comparable performance with the state-of-the-artsupervised systems.

Detecting Online Hate Speech Using Context Aware Models

  In the wake of a polarizing election, the cyber world is laden with hatespeech. Context accompanying a hate speech text is useful for identifying hatespeech, which however has been largely overlooked in existing datasets and hatespeech detection models. In this paper, we provide an annotated corpus of hatespeech with context information well kept. Then we propose two types of hatespeech detection models that incorporate context information, a logisticregression model with context features and a neural network model with learningcomponents for context. Our evaluation shows that both models outperform astrong baseline by around 3% to 4% in F1 score and combining these two modelsfurther improve the performance by another 7% in F1 score.

TAMU at KBP 2017: Event Nugget Detection and Coreference Resolution

  In this paper, we describe TAMU's system submitted to the TAC KBP 2017 eventnugget detection and coreference resolution task. Our system builds on thestatistical and empirical observations made on training and development data.We found that modifiers of event nuggets tend to have unique syntacticdistribution. Their parts-of-speech tags and dependency relations provides themessential characteristics that are useful in identifying their span and alsodefining their types and realis status. We further found that the jointmodeling of event span detection and realis status identification performsbetter than the individual models for both tasks. Our simple system designedusing minimal features achieved the micro-average F1 scores of 57.72, 44.27 and42.47 for event span detection, type identification and realis statusclassification tasks respectively. Also, our system achieved the CoNLL F1 scoreof 27.20 in event coreference resolution task.

Building Context-aware Clause Representations for Situation Entity Type  Classification

  Capabilities to categorize a clause based on the type of situation entity(e.g., events, states and generic statements) the clause introduces to thediscourse can benefit many NLP applications. Observing that the situationentity type of a clause depends on discourse functions the clause plays in aparagraph and the interpretation of discourse functions depends heavily onparagraph-wide contexts, we propose to build context-aware clauserepresentations for predicting situation entity types of clauses. Specifically,we propose a hierarchical recurrent neural network model to read a wholeparagraph at a time and jointly learn representations for all the clauses inthe paragraph by extensively modeling context influences and inter-dependenciesof clauses. Experimental results show that our model achieves thestate-of-the-art performance for clause-level situation entity classificationon the genre-rich MASC+Wiki corpus, which approaches human-level performance.

Improving Dialogue State Tracking by Discerning the Relevant Context

  A typical conversation comprises of multiple turns between participants wherethey go back-and-forth between different topics. At each user turn, dialoguestate tracking (DST) aims to estimate user's goal by processing the currentutterance. However, in many turns, users implicitly refer to the previous goal,necessitating the use of relevant dialogue history. Nonetheless, distinguishingrelevant history is challenging and a popular method of using dialogue recencyfor that is inefficient. We, therefore, propose a novel framework for DST thatidentifies relevant historical context by referring to the past utteranceswhere a particular slot-value changes and uses that together with weightedsystem utterance to identify the relevant context. Specifically, we use thecurrent user utterance and the most recent system utterance to determine therelevance of a system utterance. Empirical analyses show that our methodimproves joint goal accuracy by 2.75% and 2.36% on WoZ 2.0 and MultiWoZ 2.0restaurant domain datasets respectively over the previous state-of-the-art GLADmodel.

