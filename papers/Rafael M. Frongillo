Topological Entropy Bounds for Hyperbolic Plateaus of the HÃ©non Map

  Combining two existing rigorous computational methods, for verifying
hyperbolicity (due to Arai) and for computing topological entropy bounds (due
to Day et al.), we prove lower bounds on topological entropy for 43 hyperbolic
plateaus of the H\'enon map. We also examine the 16 area-preserving plateaus
studied by Arai and compare our results with related work. Along the way, we
augment the algorithms of Day et al. with routines to optimize the algorithmic
parameters and simplify the resulting semi-conjugate subshift.


Efficient automation of index pairs in computational Conley index theory

  We present new methods of automating the construction of index pairs,
essential ingredients of discrete Conley index theory. These new algorithms are
further steps in the direction of automating computer-assisted proofs of
semi-conjugacies from a map on a manifold to a subshift of finite type. We
apply these new algorithms to the standard map at different values of the
perturbative parameter {\epsilon} and obtain rigorous lower bounds for its
topological entropy for {\epsilon} in [.7, 2].


Risk Dynamics in Trade Networks

  We introduce a new framework to model interactions among agents which seek to
trade to minimize their risk with respect to some future outcome. We quantify
this risk using the concept of risk measures from finance, and introduce a
class of trade dynamics which allow agents to trade contracts contingent upon
the future outcome. We then show that these trade dynamics exactly correspond
to a variant of randomized coordinate descent. By extending the analysis of
these coordinate descent methods to account for our more organic setting, we
are able to show convergence rates for very general trade dynamics, showing
that the market or network converges to a unique steady state. Applying these
results to prediction markets, we expand on recent results by adding
convergence rates and general aggregation properties. Finally, we illustrate
the generality of our framework by applying it to agent interactions on a
scale-free network.


A Collaborative Mechanism for Crowdsourcing Prediction Problems

  Machine Learning competitions such as the Netflix Prize have proven
reasonably successful as a method of "crowdsourcing" prediction tasks. But
these competitions have a number of weaknesses, particularly in the incentive
structure they create for the participants. We propose a new approach, called a
Crowdsourced Learning Mechanism, in which participants collaboratively "learn"
a hypothesis for a given prediction task. The approach draws heavily from the
concept of a prediction market, where traders bet on the likelihood of a future
event. In our framework, the mechanism continues to publish the current
hypothesis, and participants can modify this hypothesis by wagering on an
update. The critical incentive property is that a participant will profit an
amount that scales according to how much her update improves performance on a
released test set.


Market Making with Decreasing Utility for Information

  We study information elicitation in cost-function-based combinatorial
prediction markets when the market maker's utility for information decreases
over time. In the sudden revelation setting, it is known that some piece of
information will be revealed to traders, and the market maker wishes to prevent
guaranteed profits for trading on the sure information. In the gradual decrease
setting, the market maker's utility for (partial) information decreases
continuously over time. We design adaptive cost functions for both settings
which: (1) preserve the information previously gathered in the market; (2)
eliminate (or diminish) rewards to traders for the publicly revealed
information; (3) leave the reward structure unaffected for other information;
and (4) maintain the market maker's worst-case loss. Our constructions utilize
mixed Bregman divergence, which matches our notion of utility for information.


General Truthfulness Characterizations Via Convex Analysis

  We present a model of truthful elicitation which generalizes and extends
mechanisms, scoring rules, and a number of related settings that do not quite
qualify as one or the other. Our main result is a characterization theorem,
yielding characterizations for all of these settings, including a new
characterization of scoring rules for non-convex sets of distributions. We
generalize this model to eliciting some property of the agent's private
information, and provide the first general characterization for this setting.
We also show how this yields a new proof of a result in mechanism design due to
Saks and Yu.


Generalised Mixability, Constant Regret, and Bayesian Updating

  Mixability of a loss is known to characterise when constant regret bounds are
achievable in games of prediction with expert advice through the use of Vovk's
aggregating algorithm. We provide a new interpretation of mixability via convex
analysis that highlights the role of the Kullback-Leibler divergence in its
definition. This naturally generalises to what we call $\Phi$-mixability where
the Bregman divergence $D_\Phi$ replaces the KL divergence. We prove that
losses that are $\Phi$-mixable also enjoy constant regret bounds via a
generalised aggregating algorithm that is similar to mirror descent.


Elicitation for Aggregation

  We study the problem of eliciting and aggregating probabilistic information
from multiple agents. In order to successfully aggregate the predictions of
agents, the principal needs to elicit some notion of confidence from agents,
capturing how much experience or knowledge led to their predictions. To
formalize this, we consider a principal who wishes to elicit predictions about
a random variable from a group of Bayesian agents, each of whom have privately
observed some independent samples of the random variable, and hopes to
aggregate the predictions as if she had directly observed the samples of all
agents. Leveraging techniques from Bayesian statistics, we represent confidence
as the number of samples an agent has observed, which is quantified by a
hyperparameter from a conjugate family of prior distributions. This then allows
us to show that if the principal has access to a few samples, she can achieve
her aggregation goal by eliciting predictions from agents using proper scoring
rules. In particular, if she has access to one sample, she can successfully
aggregate the agents' predictions if and only if every posterior predictive
distribution corresponds to a unique value of the hyperparameter. Furthermore,
this uniqueness holds for many common distributions of interest. When this
uniqueness property does not hold, we construct a novel and intuitive mechanism
where a principal with two samples can elicit and optimally aggregate the
agents' predictions.


Minimax Option Pricing Meets Black-Scholes in the Limit

  Option contracts are a type of financial derivative that allow investors to
hedge risk and speculate on the variation of an asset's future market price. In
short, an option has a particular payout that is based on the market price for
an asset on a given date in the future. In 1973, Black and Scholes proposed a
valuation model for options that essentially estimates the tail risk of the
asset price under the assumption that the price will fluctuate according to
geometric Brownian motion. More recently, DeMarzo et al., among others, have
proposed more robust valuation schemes, where we can even assume an adversary
chooses the price fluctuations. This framework can be considered as a
sequential two-player zero-sum game between the investor and Nature. We analyze
the value of this game in the limit, where the investor can trade at smaller
and smaller time intervals. Under weak assumptions on the actions of Nature (an
adversary), we show that the minimax option price asymptotically approaches
exactly the Black-Scholes valuation. The key piece of our analysis is showing
that Nature's minimax optimal dual strategy converges to geometric Brownian
motion in the limit.


Generalized Mixability via Entropic Duality

  Mixability is a property of a loss which characterizes when fast convergence
is possible in the game of prediction with expert advice. We show that a key
property of mixability generalizes, and the exp and log operations present in
the usual theory are not as special as one might have thought. In doing this we
introduce a more general notion of $\Phi$-mixability where $\Phi$ is a general
entropy (\ie, any convex function on probabilities). We show how a property
shared by the convex dual of any such entropy yields a natural algorithm (the
minimizer of a regret bound) which, analogous to the classical aggregating
algorithm, is guaranteed a constant regret when used with $\Phi$-mixable
losses. We characterize precisely which $\Phi$ have $\Phi$-mixable losses and
put forward a number of conjectures about the optimality and relationships
between different choices of entropy.


Power Diagram Detection with Applications to Information Elicitation

  Power diagrams, a type of weighted Voronoi diagrams, have many applications
throughout operations research. We study the problem of power diagram
detection: determining whether a given finite partition of $\mathbb{R}^d$ takes
the form of a power diagram. This detection problem is particularly prevalent
in the field of information elicitation, where one wishes to design contracts
to incentivize self-minded agents to provide honest information.
  We devise a simple linear program to decide whether a polyhedral cell
decomposition can be described as a power diagram. Further, we discuss
applications to property elicitation, peer prediction, and mechanism design,
where this question arises. Our model is able to efficiently decide the
question for decompositions of $\mathbb{R}^d$ or of a restricted domain in
$\mathbb{R}^d$. The approach is based on the use of an alternative
representation of power diagrams, and invariance of a power diagram under
uniform scaling of the parameters in this representation.


Social Learning in a Changing World

  We study a model of learning on social networks in dynamic environments,
describing a group of agents who are each trying to estimate an underlying
state that varies over time, given access to weak signals and the estimates of
their social network neighbors.
  We study three models of agent behavior. In the "fixed response" model,
agents use a fixed linear combination to incorporate information from their
peers into their own estimate. This can be thought of as an extension of the
DeGroot model to a dynamic setting. In the "best response" model, players
calculate minimum variance linear estimators of the underlying state.
  We show that regardless of the initial configuration, fixed response dynamics
converge to a steady state, and that the same holds for best response on the
complete graph. We show that best response dynamics can, in the long term, lead
to estimators with higher variance than is achievable using well chosen fixed
responses.
  The "penultimate prediction" model is an elaboration of the best response
model. While this model only slightly complicates the computations required of
the agents, we show that in some cases it greatly increases the efficiency of
learning, and on complete graphs is in fact optimal, in a strong sense.


