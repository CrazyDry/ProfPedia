Topological Entropy Bounds for Hyperbolic Plateaus of the HÃ©non Map

  Combining two existing rigorous computational methods, for verifyinghyperbolicity (due to Arai) and for computing topological entropy bounds (dueto Day et al.), we prove lower bounds on topological entropy for 43 hyperbolicplateaus of the H\'enon map. We also examine the 16 area-preserving plateausstudied by Arai and compare our results with related work. Along the way, weaugment the algorithms of Day et al. with routines to optimize the algorithmicparameters and simplify the resulting semi-conjugate subshift.

Efficient automation of index pairs in computational Conley index theory

  We present new methods of automating the construction of index pairs,essential ingredients of discrete Conley index theory. These new algorithms arefurther steps in the direction of automating computer-assisted proofs ofsemi-conjugacies from a map on a manifold to a subshift of finite type. Weapply these new algorithms to the standard map at different values of theperturbative parameter {\epsilon} and obtain rigorous lower bounds for itstopological entropy for {\epsilon} in [.7, 2].

A Collaborative Mechanism for Crowdsourcing Prediction Problems

  Machine Learning competitions such as the Netflix Prize have provenreasonably successful as a method of "crowdsourcing" prediction tasks. Butthese competitions have a number of weaknesses, particularly in the incentivestructure they create for the participants. We propose a new approach, called aCrowdsourced Learning Mechanism, in which participants collaboratively "learn"a hypothesis for a given prediction task. The approach draws heavily from theconcept of a prediction market, where traders bet on the likelihood of a futureevent. In our framework, the mechanism continues to publish the currenthypothesis, and participants can modify this hypothesis by wagering on anupdate. The critical incentive property is that a participant will profit anamount that scales according to how much her update improves performance on areleased test set.

Risk Dynamics in Trade Networks

  We introduce a new framework to model interactions among agents which seek totrade to minimize their risk with respect to some future outcome. We quantifythis risk using the concept of risk measures from finance, and introduce aclass of trade dynamics which allow agents to trade contracts contingent uponthe future outcome. We then show that these trade dynamics exactly correspondto a variant of randomized coordinate descent. By extending the analysis ofthese coordinate descent methods to account for our more organic setting, weare able to show convergence rates for very general trade dynamics, showingthat the market or network converges to a unique steady state. Applying theseresults to prediction markets, we expand on recent results by addingconvergence rates and general aggregation properties. Finally, we illustratethe generality of our framework by applying it to agent interactions on ascale-free network.

Market Making with Decreasing Utility for Information

  We study information elicitation in cost-function-based combinatorialprediction markets when the market maker's utility for information decreasesover time. In the sudden revelation setting, it is known that some piece ofinformation will be revealed to traders, and the market maker wishes to preventguaranteed profits for trading on the sure information. In the gradual decreasesetting, the market maker's utility for (partial) information decreasescontinuously over time. We design adaptive cost functions for both settingswhich: (1) preserve the information previously gathered in the market; (2)eliminate (or diminish) rewards to traders for the publicly revealedinformation; (3) leave the reward structure unaffected for other information;and (4) maintain the market maker's worst-case loss. Our constructions utilizemixed Bregman divergence, which matches our notion of utility for information.

General Truthfulness Characterizations Via Convex Analysis

  We present a model of truthful elicitation which generalizes and extendsmechanisms, scoring rules, and a number of related settings that do not quitequalify as one or the other. Our main result is a characterization theorem,yielding characterizations for all of these settings, including a newcharacterization of scoring rules for non-convex sets of distributions. Wegeneralize this model to eliciting some property of the agent's privateinformation, and provide the first general characterization for this setting.We also show how this yields a new proof of a result in mechanism design due toSaks and Yu.

Generalised Mixability, Constant Regret, and Bayesian Updating

  Mixability of a loss is known to characterise when constant regret bounds areachievable in games of prediction with expert advice through the use of Vovk'saggregating algorithm. We provide a new interpretation of mixability via convexanalysis that highlights the role of the Kullback-Leibler divergence in itsdefinition. This naturally generalises to what we call $\Phi$-mixability wherethe Bregman divergence $D_\Phi$ replaces the KL divergence. We prove thatlosses that are $\Phi$-mixable also enjoy constant regret bounds via ageneralised aggregating algorithm that is similar to mirror descent.

Elicitation for Aggregation

  We study the problem of eliciting and aggregating probabilistic informationfrom multiple agents. In order to successfully aggregate the predictions ofagents, the principal needs to elicit some notion of confidence from agents,capturing how much experience or knowledge led to their predictions. Toformalize this, we consider a principal who wishes to elicit predictions abouta random variable from a group of Bayesian agents, each of whom have privatelyobserved some independent samples of the random variable, and hopes toaggregate the predictions as if she had directly observed the samples of allagents. Leveraging techniques from Bayesian statistics, we represent confidenceas the number of samples an agent has observed, which is quantified by ahyperparameter from a conjugate family of prior distributions. This then allowsus to show that if the principal has access to a few samples, she can achieveher aggregation goal by eliciting predictions from agents using proper scoringrules. In particular, if she has access to one sample, she can successfullyaggregate the agents' predictions if and only if every posterior predictivedistribution corresponds to a unique value of the hyperparameter. Furthermore,this uniqueness holds for many common distributions of interest. When thisuniqueness property does not hold, we construct a novel and intuitive mechanismwhere a principal with two samples can elicit and optimally aggregate theagents' predictions.

Minimax Option Pricing Meets Black-Scholes in the Limit

  Option contracts are a type of financial derivative that allow investors tohedge risk and speculate on the variation of an asset's future market price. Inshort, an option has a particular payout that is based on the market price foran asset on a given date in the future. In 1973, Black and Scholes proposed avaluation model for options that essentially estimates the tail risk of theasset price under the assumption that the price will fluctuate according togeometric Brownian motion. More recently, DeMarzo et al., among others, haveproposed more robust valuation schemes, where we can even assume an adversarychooses the price fluctuations. This framework can be considered as asequential two-player zero-sum game between the investor and Nature. We analyzethe value of this game in the limit, where the investor can trade at smallerand smaller time intervals. Under weak assumptions on the actions of Nature (anadversary), we show that the minimax option price asymptotically approachesexactly the Black-Scholes valuation. The key piece of our analysis is showingthat Nature's minimax optimal dual strategy converges to geometric Brownianmotion in the limit.

Generalized Mixability via Entropic Duality

  Mixability is a property of a loss which characterizes when fast convergenceis possible in the game of prediction with expert advice. We show that a keyproperty of mixability generalizes, and the exp and log operations present inthe usual theory are not as special as one might have thought. In doing this weintroduce a more general notion of $\Phi$-mixability where $\Phi$ is a generalentropy (\ie, any convex function on probabilities). We show how a propertyshared by the convex dual of any such entropy yields a natural algorithm (theminimizer of a regret bound) which, analogous to the classical aggregatingalgorithm, is guaranteed a constant regret when used with $\Phi$-mixablelosses. We characterize precisely which $\Phi$ have $\Phi$-mixable losses andput forward a number of conjectures about the optimality and relationshipsbetween different choices of entropy.

Power Diagram Detection with Applications to Information Elicitation

  Power diagrams, a type of weighted Voronoi diagrams, have many applicationsthroughout operations research. We study the problem of power diagramdetection: determining whether a given finite partition of $\mathbb{R}^d$ takesthe form of a power diagram. This detection problem is particularly prevalentin the field of information elicitation, where one wishes to design contractsto incentivize self-minded agents to provide honest information.  We devise a simple linear program to decide whether a polyhedral celldecomposition can be described as a power diagram. Further, we discussapplications to property elicitation, peer prediction, and mechanism design,where this question arises. Our model is able to efficiently decide thequestion for decompositions of $\mathbb{R}^d$ or of a restricted domain in$\mathbb{R}^d$. The approach is based on the use of an alternativerepresentation of power diagrams, and invariance of a power diagram underuniform scaling of the parameters in this representation.

Social Learning in a Changing World

  We study a model of learning on social networks in dynamic environments,describing a group of agents who are each trying to estimate an underlyingstate that varies over time, given access to weak signals and the estimates oftheir social network neighbors.  We study three models of agent behavior. In the "fixed response" model,agents use a fixed linear combination to incorporate information from theirpeers into their own estimate. This can be thought of as an extension of theDeGroot model to a dynamic setting. In the "best response" model, playerscalculate minimum variance linear estimators of the underlying state.  We show that regardless of the initial configuration, fixed response dynamicsconverge to a steady state, and that the same holds for best response on thecomplete graph. We show that best response dynamics can, in the long term, leadto estimators with higher variance than is achievable using well chosen fixedresponses.  The "penultimate prediction" model is an elaboration of the best responsemodel. While this model only slightly complicates the computations required ofthe agents, we show that in some cases it greatly increases the efficiency oflearning, and on complete graphs is in fact optimal, in a strong sense.

