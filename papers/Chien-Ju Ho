Working in Pairs: Understanding the Effects of Worker Interactions in
  Crowdwork

  Crowdsourcing has gained popularity as a tool to harness human brain power to
help solve problems that are difficult for computers. Previous work in
crowdsourcing often assumes that workers complete crowdwork independently. In
this paper, we relax the independent property of crowdwork and explore how
introducing direct, synchronous, and free-style interactions between workers
would affect crowdwork. In particular, motivated by the concept of peer
instruction in educational settings, we study the effects of peer communication
in crowdsourcing environments. In the crowdsourcing setting with peer
communication, pairs of workers are asked to complete the same task together by
first generating their initial answers to the task independently and then
freely discussing the tasks with each other and updating their answers after
the discussion. We experimentally examine the effects of peer communication in
crowdwork on various common types of tasks on crowdsourcing platforms,
including image labeling, optical character recognition (OCR), audio
transcription, and nutrition analysis. Our experiment results show that the
work quality is significantly improved in tasks with peer communication
compared to tasks where workers complete the work independently. However,
participating in tasks with peer communication has limited effects on
influencing worker's independent performance in tasks of the same type in the
future.


Adaptive Contract Design for Crowdsourcing Markets: Bandit Algorithms
  for Repeated Principal-Agent Problems

  Crowdsourcing markets have emerged as a popular platform for matching
available workers with tasks to complete. The payment for a particular task is
typically set by the task's requester, and may be adjusted based on the quality
of the completed work, for example, through the use of "bonus" payments. In
this paper, we study the requester's problem of dynamically adjusting
quality-contingent payments for tasks. We consider a multi-round version of the
well-known principal-agent model, whereby in each round a worker makes a
strategic choice of the effort level which is not directly observable by the
requester. In particular, our formulation significantly generalizes the
budget-free online task pricing problems studied in prior work.
  We treat this problem as a multi-armed bandit problem, with each "arm"
representing a potential contract. To cope with the large (and in fact,
infinite) number of arms, we propose a new algorithm, AgnosticZooming, which
discretizes the contract space into a finite number of regions, effectively
treating each region as a single arm. This discretization is adaptively
refined, so that more promising regions of the contract space are eventually
discretized more finely. We analyze this algorithm, showing that it achieves
regret sublinear in the time horizon and substantially improves over
non-adaptive discretization (which is the only competing approach in the
literature).
  Our results advance the state of art on several different topics: the theory
of crowdsourcing markets, principal-agent problems, multi-armed bandits, and
dynamic pricing.


Incentivizing High Quality Crowdwork

  We study the causal effects of financial incentives on the quality of
crowdwork. We focus on performance-based payments (PBPs), bonus payments
awarded to workers for producing high quality work. We design and run
randomized behavioral experiments on the popular crowdsourcing platform Amazon
Mechanical Turk with the goal of understanding when, where, and why PBPs help,
identifying properties of the payment, payment structure, and the task itself
that make them most effective. We provide examples of tasks for which PBPs do
improve quality. For such tasks, the effectiveness of PBPs is not too sensitive
to the threshold for quality required to receive the bonus, while the magnitude
of the bonus must be large enough to make the reward salient. We also present
examples of tasks for which PBPs do not improve quality. Our results suggest
that for PBPs to improve quality, the task must be effort-responsive: the task
must allow workers to produce higher quality work by exerting more effort. We
also give a simple method to determine if a task is effort-responsive a priori.
Furthermore, our experiments suggest that all payments on Mechanical Turk are,
to some degree, implicitly performance-based in that workers believe their work
may be rejected if their performance is sufficiently poor. Finally, we propose
a new model of worker behavior that extends the standard principal-agent model
from economics to include a worker's subjective beliefs about his likelihood of
being paid, and show that the predictions of this model are in line with our
experimental findings. This model may be useful as a foundation for theoretical
studies of incentives in crowdsourcing markets.


