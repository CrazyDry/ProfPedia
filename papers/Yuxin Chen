Channel Capacity under General Nonuniform Sampling

  This paper develops the fundamental capacity limits of a sampled analog
channel under a sub-Nyquist sampling rate constraint. In particular, we derive
the capacity of sampled analog channels over a general class of time-preserving
sampling methods including irregular nonuniform sampling. Our results indicate
that the optimal sampling structures extract out the set of frequencies that
exhibits the highest SNR among all spectral sets of support size equal to the
sampling rate. The capacity under sub-Nyquist sampling can be attained through
filter-bank sampling, or through a single branch of modulation and filtering
followed by uniform sampling. The capacity under sub-Nyquist sampling is a
monotone function of the sampling rate. These results indicate that the optimal
sampling schemes suppress aliasing, and that employing irregular nonuniform
sampling does not provide capacity gain over uniform sampling sets with
appropriate preprocessing for a large class of channels.


An Algorithm for Exact Super-resolution and Phase Retrieval

  We explore a fundamental problem of super-resolving a signal of interest from
a few measurements of its low-pass magnitudes. We propose a 2-stage tractable
algorithm that, in the absence of noise, admits perfect super-resolution of an
$r$-sparse signal from $2r^2-2r+2$ low-pass magnitude measurements. The spike
locations of the signal can assume any value over a continuous disk, without
increasing the required sample size. The proposed algorithm first employs a
conventional super-resolution algorithm (e.g. the matrix pencil approach) to
recover unlabeled sets of signal correlation coefficients, and then applies a
simple sorting algorithm to disentangle and retrieve the true parameters in a
deterministic manner. Our approach can be adapted to multi-dimensional spike
models and random Fourier sampling by replacing its first step with other
harmonic retrieval algorithms.


Community Recovery in Graphs with Locality

  Motivated by applications in domains such as social networks and
computational biology, we study the problem of community recovery in graphs
with locality. In this problem, pairwise noisy measurements of whether two
nodes are in the same community or different communities come mainly or
exclusively from nearby nodes rather than uniformly sampled between all nodes
pairs, as in most existing models. We present an algorithm that runs nearly
linearly in the number of measurements and which achieves the information
theoretic limit for exact recovery.


Near-optimal Bayesian Active Learning with Correlated and Noisy Tests

  We consider the Bayesian active learning and experimental design problem,
where the goal is to learn the value of some unknown target variable through a
sequence of informative, noisy tests. In contrast to prior work, we focus on
the challenging, yet practically relevant setting where test outcomes can be
conditionally dependent given the hidden target variable. Under such
assumptions, common heuristics, such as greedily performing tests that maximize
the reduction in uncertainty of the target, often perform poorly. In this
paper, we propose ECED, a novel, computationally efficient active learning
algorithm, and prove strong theoretical guarantees that hold with correlated,
noisy tests. Rather than directly optimizing the prediction error, at each
step, ECED picks the test that maximizes the gain in a surrogate objective,
which takes into account the dependencies between tests. Our analysis relies on
an information-theoretic auxiliary function to track the progress of ECED, and
utilizes adaptive submodularity to attain the near-optimal bound. We
demonstrate strong empirical performance of ECED on two problem instances,
including a Bayesian experimental design task intended to distinguish among
economic theories of how people make risky decisions, and an active preference
learning task via pairwise comparisons.


Noise-induced tipping under periodic forcing: preferred tipping phase in
  a non-adiabatic forcing regime

  We consider a periodically-forced 1-D Langevin equation that possesses two
stable periodic solutions in the absence of noise. We ask the question: is
there a most likely noise-induced transition path between these periodic
solutions that allows us to identify a preferred phase of the forcing when
tipping occurs? The quasistatic regime, where the forcing period is long
compared to the adiabatic relaxation time, has been well studied; our work
instead explores the case when these timescales are comparable. We compute
optimal paths using the path integral method incorporating the Onsager-Machlup
functional and validate results with Monte Carlo simulations. Results for the
preferred tipping phase are compared with the deterministic aspects of the
problem. We identify parameter regimes where nullclines, associated with the
deterministic problem in a 2-D extended phase space, form passageways through
which the optimal paths transit. As the nullclines are independent of the
relaxation time and the noise strength, this leads to a robust deterministic
predictor of preferred tipping phase in a regime where forcing is neither too
fast, nor too slow.


Nonconvex Optimization Meets Low-Rank Matrix Factorization: An Overview

  Substantial progress has been made recently on developing provably accurate
and efficient algorithms for low-rank matrix factorization via nonconvex
optimization. While conventional wisdom often takes a dim view of nonconvex
optimization algorithms due to their susceptibility to spurious local minima,
simple iterative methods such as gradient descent have been remarkably
successful in practice. The theoretical footings, however, had been largely
lacking until recently.
  In this tutorial-style overview, we highlight the important role of
statistical models in enabling efficient nonconvex optimization with
performance guarantees. We review two contrasting approaches: (1) two-stage
algorithms, which consist of a tailored initialization step followed by
successive refinement; and (2) global landscape analysis and
initialization-free algorithms. Several canonical matrix factorization problems
are discussed, including but not limited to matrix sensing, phase retrieval,
matrix completion, blind deconvolution, robust principal component analysis,
phase synchronization, and joint alignment. Special care is taken to illustrate
the key technical insights underlying their analyses. This article serves as a
testament that the integrated thinking of optimization and statistics leads to
fruitful research findings.


Addressing Training Bias via Automated Image Annotation

  Build accurate DNN models requires training on large labeled, context
specific datasets, especially those matching the target scenario. We believe
advances in wireless localization, working in unison with cameras, can produce
automated annotation of targets on images and videos captured in the wild.
Using pedestrian and vehicle detection as examples, we demonstrate the
feasibility, benefits, and challenges of an automatic image annotation system.
Our work calls for new technical development on passive localization, mobile
data analytics, and error-resilient ML models, as well as design issues in user
privacy policies.


An Upper Bound on Multi-hop Transmission Capacity with Dynamic Routing
  Selection

  This paper develops upper bounds on the end-to-end transmission capacity of
multi-hop wireless networks. Potential source-destination paths are dynamically
selected from a pool of randomly located relays, from which a closed-form lower
bound on the outage probability is derived in terms of the expected number of
potential paths. This is in turn used to provide an upper bound on the number
of successful transmissions that can occur per unit area, which is known as the
transmission capacity. The upper bound results from assuming independence among
the potential paths, and can be viewed as the maximum diversity case. A useful
aspect of the upper bound is its simple form for an arbitrary-sized network,
which allows insights into how the number of hops and other network parameters
affect spatial throughput in the non-asymptotic regime. The outage probability
analysis is then extended to account for retransmissions with a maximum number
of allowed attempts. In contrast to prevailing wisdom, we show that
predetermined routing (such as nearest-neighbor) is suboptimal, since more hops
are not useful once the network is interference-limited. Our results also make
clear that randomness in the location of relay sets and dynamically varying
channel states is helpful in obtaining higher aggregate throughput, and that
dynamic route selection should be used to exploit path diversity.


Spectral Compressed Sensing via Structured Matrix Completion

  The paper studies the problem of recovering a spectrally sparse object from a
small number of time domain samples. Specifically, the object of interest with
ambient dimension $n$ is assumed to be a mixture of $r$ complex
multi-dimensional sinusoids, while the underlying frequencies can assume any
value in the unit disk. Conventional compressed sensing paradigms suffer from
the {\em basis mismatch} issue when imposing a discrete dictionary on the
Fourier representation. To address this problem, we develop a novel
nonparametric algorithm, called enhanced matrix completion (EMaC), based on
structured matrix completion. The algorithm starts by arranging the data into a
low-rank enhanced form with multi-fold Hankel structure, then attempts recovery
via nuclear norm minimization. Under mild incoherence conditions, EMaC allows
perfect recovery as soon as the number of samples exceeds the order of
$\mathcal{O}(r\log^{2} n)$. We also show that, in many instances, accurate
completion of a low-rank multi-fold Hankel matrix is possible when the number
of observed entries is proportional to the information theoretical limits
(except for a logarithmic gap). The robustness of EMaC against bounded noise
and its applicability to super resolution are further demonstrated by numerical
experiments.


On the Minimax Capacity Loss under Sub-Nyquist Universal Sampling

  This paper investigates the information rate loss in analog channels when the
sampler is designed to operate independent of the instantaneous channel
occupancy. Specifically, a multiband linear time-invariant Gaussian channel
under universal sub-Nyquist sampling is considered. The entire channel
bandwidth is divided into $n$ subbands of equal bandwidth. At each time only
$k$ constant-gain subbands are active, where the instantaneous subband
occupancy is not known at the receiver and the sampler. We study the
information loss through a capacity loss metric, that is, the capacity gap
caused by the lack of instantaneous subband occupancy information. We
characterize the minimax capacity loss for the entire sub-Nyquist rate regime,
provided that the number $n$ of subbands and the SNR are both large. The
minimax limits depend almost solely on the band sparsity factor and the
undersampling factor, modulo some residual terms that vanish as $n$ and SNR
grow. Our results highlight the power of randomized sampling methods (i.e. the
samplers that consist of random periodic modulation and low-pass filters),
which are able to approach the minimax capacity loss with exponentially high
probability.


Information Recovery from Pairwise Measurements

  A variety of information processing tasks in practice involve recovering $n$
objects from single-shot graph-based measurements, particularly those taken
over the edges of some measurement graph $\mathcal{G}$. This paper concerns the
situation where each object takes value over a group of $M$ different values,
and where one is interested to recover all these values based on observations
of certain pairwise relations over $\mathcal{G}$. The imperfection of
measurements presents two major challenges for information recovery: 1)
$\textit{inaccuracy}$: a (dominant) portion $1-p$ of measurements are
corrupted; 2) $\textit{incompleteness}$: a significant fraction of pairs are
unobservable, i.e. $\mathcal{G}$ can be highly sparse.
  Under a natural random outlier model, we characterize the $\textit{minimax
recovery rate}$, that is, the critical threshold of non-corruption rate $p$
below which exact information recovery is infeasible. This accommodates a very
general class of pairwise relations. For various homogeneous random graph
models (e.g. Erdos Renyi random graphs, random geometric graphs, small world
graphs), the minimax recovery rate depends almost exclusively on the edge
sparsity of the measurement graph $\mathcal{G}$ irrespective of other graphical
metrics. This fundamental limit decays with the group size $M$ at a square root
rate before entering a connectivity-limited regime. Under the Erdos Renyi
random graph, a tractable combinatorial algorithm is proposed to approach the
limit for large $M$ ($M=n^{\Omega(1)}$), while order-optimal recovery is
enabled by semidefinite programs in the small $M$ regime.
  The extended (and most updated) version of this work can be found at
(http://arxiv.org/abs/1504.01369).


Scalable Semidefinite Relaxation for Maximum A Posterior Estimation

  Maximum a posteriori (MAP) inference over discrete Markov random fields is a
fundamental task spanning a wide spectrum of real-world applications, which is
known to be NP-hard for general graphs. In this paper, we propose a novel
semidefinite relaxation formulation (referred to as SDR) to estimate the MAP
assignment. Algorithmically, we develop an accelerated variant of the
alternating direction method of multipliers (referred to as SDPAD-LR) that can
effectively exploit the special structure of the new relaxation. Encouragingly,
the proposed procedure allows solving SDR for large-scale problems, e.g.,
problems on a grid graph comprising hundreds of thousands of variables with
multiple states per node. Compared with prior SDP solvers, SDPAD-LR is capable
of attaining comparable accuracy while exhibiting remarkably improved
scalability, in contrast to the commonly held belief that semidefinite
relaxation can only been applied on small-scale MRF problems. We have evaluated
the performance of SDR on various benchmark datasets including OPENGM2 and PIC
in terms of both the quality of the solutions and computation time.
Experimental results demonstrate that for a broad class of problems, SDPAD-LR
outperforms state-of-the-art algorithms in producing better MAP assignment in
an efficient manner.


Spectral MLE: Top-$K$ Rank Aggregation from Pairwise Comparisons

  This paper explores the preference-based top-$K$ rank aggregation problem.
Suppose that a collection of items is repeatedly compared in pairs, and one
wishes to recover a consistent ordering that emphasizes the top-$K$ ranked
items, based on partially revealed preferences. We focus on the
Bradley-Terry-Luce (BTL) model that postulates a set of latent preference
scores underlying all items, where the odds of paired comparisons depend only
on the relative scores of the items involved.
  We characterize the minimax limits on identifiability of top-$K$ ranked
items, in the presence of random and non-adaptive sampling. Our results
highlight a separation measure that quantifies the gap of preference scores
between the $K^{\text{th}}$ and $(K+1)^{\text{th}}$ ranked items. The minimum
sample complexity required for reliable top-$K$ ranking scales inversely with
the separation measure irrespective of other preference distribution metrics.
To approach this minimax limit, we propose a nearly linear-time ranking scheme,
called \emph{Spectral MLE}, that returns the indices of the top-$K$ items in
accordance to a careful score estimate. In a nutshell, Spectral MLE starts with
an initial score estimate with minimal squared loss (obtained via a spectral
method), and then successively refines each component with the assistance of
coordinate-wise MLEs. Encouragingly, Spectral MLE allows perfect top-$K$ item
identification under minimal sample complexity. The practical applicability of
Spectral MLE is further corroborated by numerical experiments.


Shannon Meets Nyquist: Capacity of Sampled Gaussian Channels

  We explore two fundamental questions at the intersection of sampling theory
and information theory: how channel capacity is affected by sampling below the
channel's Nyquist rate, and what sub-Nyquist sampling strategy should be
employed to maximize capacity. In particular, we derive the capacity of sampled
analog channels for three prevalent sampling strategies: sampling with
filtering, sampling with filter banks, and sampling with modulation and filter
banks. These sampling mechanisms subsume most nonuniform sampling techniques
applied in practice. Our analyses illuminate interesting connections between
under-sampled channels and multiple-input multiple-output channels. The optimal
sampling structures are shown to extract out the frequencies with the highest
SNR from each aliased frequency set, while suppressing aliasing and out-of-band
noise. We also highlight connections between undersampled channel capacity and
minimum mean-squared error (MSE) estimation from sampled data. In particular,
we show that the filters maximizing capacity and the ones minimizing MSE are
equivalent under both filtering and filter-bank sampling strategies. These
results demonstrate the effect upon channel capacity of sub-Nyquist sampling
techniques, and characterize the tradeoff between information rate and sampling
rate.


Channel Capacity under Sub-Nyquist Nonuniform Sampling

  This paper investigates the effect of sub-Nyquist sampling upon the capacity
of an analog channel. The channel is assumed to be a linear time-invariant
Gaussian channel, where perfect channel knowledge is available at both the
transmitter and the receiver. We consider a general class of right-invertible
time-preserving sampling methods which include irregular nonuniform sampling,
and characterize in closed form the channel capacity achievable by this class
of sampling methods, under a sampling rate and power constraint. Our results
indicate that the optimal sampling structures extract out the set of
frequencies that exhibits the highest signal-to-noise ratio among all spectral
sets of measure equal to the sampling rate. This can be attained through
filterbank sampling with uniform sampling at each branch with possibly
different rates, or through a single branch of modulation and filtering
followed by uniform sampling. These results reveal that for a large class of
channels, employing irregular nonuniform sampling sets, while typically
complicated to realize, does not provide capacity gain over uniform sampling
sets with appropriate preprocessing. Our findings demonstrate that aliasing or
scrambling of spectral components does not provide capacity gain, which is in
contrast to the benefits obtained from random mixing in spectrum-blind
compressive sampling schemes.


Backing off from Infinity: Performance Bounds via Concentration of
  Spectral Measure for Random MIMO Channels

  The performance analysis of random vector channels, particularly
multiple-input-multiple-output (MIMO) channels, has largely been established in
the asymptotic regime of large channel dimensions, due to the analytical
intractability of characterizing the exact distribution of the objective
performance metrics. This paper exposes a new non-asymptotic framework that
allows the characterization of many canonical MIMO system performance metrics
to within a narrow interval under moderate-to-large channel dimensionality,
provided that these metrics can be expressed as a separable function of the
singular values of the matrix. The effectiveness of our framework is
illustrated through two canonical examples. Specifically, we characterize the
mutual information and power offset of random MIMO channels, as well as the
minimum mean squared estimation error of MIMO channel inputs from the channel
outputs. Our results lead to simple, informative, and reasonably accurate
control of various performance metrics in the finite-dimensional regime, as
corroborated by the numerical simulations. Our analysis framework is
established via the concentration of spectral measure phenomenon for random
matrices uncovered by Guionnet and Zeitouni, which arises in a variety of
random matrix ensembles irrespective of the precise distributions of the matrix
entries.


The Projected Power Method: An Efficient Algorithm for Joint Alignment
  from Pairwise Differences

  Various applications involve assigning discrete label values to a collection
of objects based on some pairwise noisy data. Due to the discrete---and hence
nonconvex---structure of the problem, computing the optimal assignment
(e.g.~maximum likelihood assignment) becomes intractable at first sight. This
paper makes progress towards efficient computation by focusing on a concrete
joint alignment problem---that is, the problem of recovering $n$ discrete
variables $x_i \in \{1,\cdots, m\}$, $1\leq i\leq n$ given noisy observations
of their modulo differences $\{x_i - x_j~\mathsf{mod}~m\}$. We propose a
low-complexity and model-free procedure, which operates in a lifted space by
representing distinct label values in orthogonal directions, and which attempts
to optimize quadratic functions over hypercubes. Starting with a first guess
computed via a spectral method, the algorithm successively refines the iterates
via projected power iterations. We prove that for a broad class of statistical
models, the proposed projected power method makes no error---and hence
converges to the maximum likelihood estimate---in a suitable regime. Numerical
experiments have been carried out on both synthetic and real data to
demonstrate the practicality of our algorithm. We expect this algorithmic
framework to be effective for a broad range of discrete assignment problems.


Efficient Online Learning for Optimizing Value of Information: Theory
  and Application to Interactive Troubleshooting

  We consider the optimal value of information (VoI) problem, where the goal is
to sequentially select a set of tests with a minimal cost, so that one can
efficiently make the best decision based on the observed outcomes. Existing
algorithms are either heuristics with no guarantees, or scale poorly (with
exponential run time in terms of the number of available tests). Moreover,
these methods assume a known distribution over the test outcomes, which is
often not the case in practice. We propose an efficient sampling-based online
learning framework to address the above issues. First, assuming the
distribution over hypotheses is known, we propose a dynamic hypothesis
enumeration strategy, which allows efficient information gathering with strong
theoretical guarantees. We show that with sufficient amount of samples, one can
identify a near-optimal decision with high probability. Second, when the
parameters of the hypotheses distribution are unknown, we propose an algorithm
which learns the parameters progressively via posterior sampling in an online
fashion. We further establish a rigorous bound on the expected regret. We
demonstrate the effectiveness of our approach on a real-world interactive
troubleshooting application and show that one can efficiently make high-quality
decisions with low cost.


Understanding the Role of Adaptivity in Machine Teaching: The Case of
  Version Space Learners

  In real-world applications of education, an effective teacher adaptively
chooses the next example to teach based on the learner's current state.
However, most existing work in algorithmic machine teaching focuses on the
batch setting, where adaptivity plays no role. In this paper, we study the case
of teaching consistent, version space learners in an interactive setting. At
any time step, the teacher provides an example, the learner performs an update,
and the teacher observes the learner's new state. We highlight that adaptivity
does not speed up the teaching process when considering existing models of
version space learners, such as "worst-case" (the learner picks the next
hypothesis randomly from the version space) and "preference-based" (the learner
picks hypothesis according to some global preference). Inspired by human
teaching, we propose a new model where the learner picks hypotheses according
to some local preference defined by the current hypothesis. We show that our
model exhibits several desirable properties, e.g., adaptivity plays a key role,
and the learner's transitions over hypotheses are smooth/interpretable. We
develop efficient teaching algorithms and demonstrate our results via
simulation and user studies.


Teaching Multiple Concepts to a Forgetful Learner

  How can we help a forgetful learner learn multiple concepts within a limited
time frame? For long-term learning, it is crucial to devise teaching strategies
that leverage the underlying forgetting mechanisms of the learner. In this
paper, we cast the problem of adaptively teaching a forgetful learner as a
novel discrete optimization problem, where we seek to optimize a natural
objective function that characterizes the learner's expected performance
throughout the teaching session. We then propose a simple greedy teaching
strategy and derive strong performance guarantees based on two intuitive
data-dependent properties, which capture the degree of diminishing returns of
teaching each concept. We show that, given some assumptions about the learner's
memory model, one can efficiently compute the performance bounds. Furthermore,
we identify parameter settings of the memory model where the greedy strategy is
guaranteed to achieve high performance. We demonstrate the effectiveness of our
algorithm using extensive simulations along with user studies in two concrete
applications, namely (i) an educational app for online vocabulary teaching and
(ii) an app for teaching novices how to recognize animal species from images.


Robust Spectral Compressed Sensing via Structured Matrix Completion

  The paper explores the problem of \emph{spectral compressed sensing}, which
aims to recover a spectrally sparse signal from a small random subset of its
$n$ time domain samples. The signal of interest is assumed to be a
superposition of $r$ multi-dimensional complex sinusoids, while the underlying
frequencies can assume any \emph{continuous} values in the normalized frequency
domain. Conventional compressed sensing paradigms suffer from the basis
mismatch issue when imposing a discrete dictionary on the Fourier
representation. To address this issue, we develop a novel algorithm, called
\emph{Enhanced Matrix Completion (EMaC)}, based on structured matrix completion
that does not require prior knowledge of the model order. The algorithm starts
by arranging the data into a low-rank enhanced form exhibiting multi-fold
Hankel structure, and then attempts recovery via nuclear norm minimization.
Under mild incoherence conditions, EMaC allows perfect recovery as soon as the
number of samples exceeds the order of $r\log^{4}n$, and is stable against
bounded noise. Even if a constant portion of samples are corrupted with
arbitrary magnitude, EMaC still allows exact recovery, provided that the sample
complexity exceeds the order of $r^{2}\log^{3}n$. Along the way, our results
demonstrate the power of convex relaxation in completing a low-rank multi-fold
Hankel or Toeplitz matrix from minimal observed entries. The performance of our
algorithm and its applicability to super resolution are further validated by
numerical experiments.


Exact and Stable Covariance Estimation from Quadratic Sampling via
  Convex Programming

  Statistical inference and information processing of high-dimensional data
often require efficient and accurate estimation of their second-order
statistics. With rapidly changing data, limited processing power and storage at
the acquisition devices, it is desirable to extract the covariance structure
from a single pass over the data and a small number of stored measurements. In
this paper, we explore a quadratic (or rank-one) measurement model which
imposes minimal memory requirements and low computational complexity during the
sampling process, and is shown to be optimal in preserving various
low-dimensional covariance structures. Specifically, four popular structural
assumptions of covariance matrices, namely low rank, Toeplitz low rank,
sparsity, jointly rank-one and sparse structure, are investigated, while
recovery is achieved via convex relaxation paradigms for the respective
structure.
  The proposed quadratic sampling framework has a variety of potential
applications including streaming data processing, high-frequency wireless
communication, phase space tomography and phase retrieval in optics, and
non-coherent subspace detection. Our method admits universally accurate
covariance estimation in the absence of noise, as soon as the number of
measurements exceeds the information theoretic limits. We also demonstrate the
robustness of this approach against noise and imperfect structural assumptions.
Our analysis is established upon a novel notion called the mixed-norm
restricted isometry property (RIP-$\ell_{2}/\ell_{1}$), as well as the
conventional RIP-$\ell_{2}/\ell_{2}$ for near-isotropic and bounded
measurements. In addition, our results improve upon the best-known phase
retrieval (including both dense and sparse signals) guarantees using PhaseLift
with a significantly simpler approach.


Information Recovery from Pairwise Measurements

  This paper is concerned with jointly recovering $n$ node-variables $\left\{
x_{i}\right\}_{1\leq i\leq n}$ from a collection of pairwise difference
measurements. Imagine we acquire a few observations taking the form of
$x_{i}-x_{j}$; the observation pattern is represented by a measurement graph
$\mathcal{G}$ with an edge set $\mathcal{E}$ such that $x_{i}-x_{j}$ is
observed if and only if $(i,j)\in\mathcal{E}$. To account for noisy
measurements in a general manner, we model the data acquisition process by a
set of channels with given input/output transition measures. Employing
information-theoretic tools applied to channel decoding problems, we develop a
\emph{unified} framework to characterize the fundamental recovery criterion,
which accommodates general graph structures, alphabet sizes, and channel
transition measures. In particular, our results isolate a family of
\emph{minimum} \emph{channel divergence measures} to characterize the degree of
measurement corruption, which together with the size of the minimum cut of
$\mathcal{G}$ dictates the feasibility of exact information recovery. For
various homogeneous graphs, the recovery condition depends almost only on the
edge sparsity of the measurement graph irrespective of other graphical metrics;
alternatively, the minimum sample complexity required for these graphs scales
like \[ \text{minimum sample complexity }\asymp\frac{n\log
n}{\mathsf{Hel}_{1/2}^{\min}} \] for certain information metric
$\mathsf{Hel}_{1/2}^{\min}$ defined in the main text, as long as the alphabet
size is not super-polynomial in $n$. We apply our general theory to three
concrete applications, including the stochastic block model, the outlier model,
and the haplotype assembly problem. Our theory leads to order-wise tight
recovery conditions for all these scenarios.


Solving Random Quadratic Systems of Equations Is Nearly as Easy as
  Solving Linear Systems

  We consider the fundamental problem of solving quadratic systems of equations
in $n$ variables, where $y_i = |\langle \boldsymbol{a}_i, \boldsymbol{x}
\rangle|^2$, $i = 1, \ldots, m$ and $\boldsymbol{x} \in \mathbb{R}^n$ is
unknown. We propose a novel method, which starting with an initial guess
computed by means of a spectral method, proceeds by minimizing a nonconvex
functional as in the Wirtinger flow approach. There are several key
distinguishing features, most notably, a distinct objective functional and
novel update rules, which operate in an adaptive fashion and drop terms bearing
too much influence on the search direction. These careful selection rules
provide a tighter initial guess, better descent directions, and thus enhanced
practical performance. On the theoretical side, we prove that for certain
unstructured models of quadratic systems, our algorithms return the correct
solution in linear time, i.e. in time proportional to reading the data
$\{\boldsymbol{a}_i\}$ and $\{y_i\}$ as soon as the ratio $m/n$ between the
number of equations and unknowns exceeds a fixed numerical constant. We extend
the theory to deal with noisy systems in which we only have $y_i \approx
|\langle \boldsymbol{a}_i, \boldsymbol{x} \rangle|^2$ and prove that our
algorithms achieve a statistical accuracy, which is nearly un-improvable. We
complement our theoretical study with numerical examples showing that solving
random quadratic systems is both computationally and statistically not much
harder than solving linear systems of the same size---hence the title of this
paper. For instance, we demonstrate empirically that the computational cost of
our algorithm is about four times that of solving a least-squares problem of
the same size.


On the Role of Mobility for Multi-message Gossip

  We consider information dissemination in a large $n$-user wireless network in
which $k$ users wish to share a unique message with all other users. Each of
the $n$ users only has knowledge of its own contents and state information;
this corresponds to a one-sided push-only scenario. The goal is to disseminate
all messages efficiently, hopefully achieving an order-optimal spreading rate
over unicast wireless random networks. First, we show that a random-push
strategy -- where a user sends its own or a received packet at random -- is
order-wise suboptimal in a random geometric graph: specifically,
$\Omega(\sqrt{n})$ times slower than optimal spreading. It is known that this
gap can be closed if each user has "full" mobility, since this effectively
creates a complete graph. We instead consider velocity-constrained mobility
where at each time slot the user moves locally using a discrete random walk
with velocity $v(n)$ that is much lower than full mobility. We propose a simple
two-stage dissemination strategy that alternates between individual message
flooding ("self promotion") and random gossiping. We prove that this scheme
achieves a close to optimal spreading rate (within only a logarithmic gap) as
long as the velocity is at least $v(n)=\omega(\sqrt{\log n/k})$. The key
insight is that the mixing property introduced by the partial mobility helps
users to spread in space within a relatively short period compared to the
optimal spreading time, which macroscopically mimics message dissemination over
a complete graph.


Near-Optimal Joint Object Matching via Convex Relaxation

  Joint matching over a collection of objects aims at aggregating information
from a large collection of similar instances (e.g. images, graphs, shapes) to
improve maps between pairs of them. Given multiple matches computed between a
few object pairs in isolation, the goal is to recover an entire collection of
maps that are (1) globally consistent, and (2) close to the provided maps ---
and under certain conditions provably the ground-truth maps. Despite recent
advances on this problem, the best-known recovery guarantees are limited to a
small constant barrier --- none of the existing methods find theoretical
support when more than $50\%$ of input correspondences are corrupted. Moreover,
prior approaches focus mostly on fully similar objects, while it is practically
more demanding to match instances that are only partially similar to each
other.
  In this paper, we develop an algorithm to jointly match multiple objects that
exhibit only partial similarities, given a few pairwise matches that are
densely corrupted. Specifically, we propose to recover the ground-truth maps
via a parameter-free convex program called MatchLift, following a spectral
method that pre-estimates the total number of distinct elements to be matched.
Encouragingly, MatchLift exhibits near-optimal error-correction ability, i.e.
in the asymptotic regime it is guaranteed to work even when a dominant fraction
$1-\Theta\left(\frac{\log^{2}n}{\sqrt{n}}\right)$ of the input maps behave like
random outliers. Furthermore, MatchLift succeeds with minimal input complexity,
namely, perfect matching can be achieved as soon as the provided maps form a
connected map graph. We evaluate the proposed algorithm on various benchmark
data sets including synthetic examples and real-world examples, all of which
confirm the practical applicability of MatchLift.


Asymmetry Helps: Eigenvalue and Eigenvector Analyses of Asymmetrically
  Perturbed Low-Rank Matrices

  This paper is concerned with a curious phenomenon in spectral estimation.
Suppose we are interested in a rank-1 and symmetric matrix
$\boldsymbol{M}^{\star}\in \mathbb{R}^{n\times n}$, yet only a randomly
perturbed version $\boldsymbol{M}$ is observed. The perturbation/noise matrix
$\boldsymbol{M}-\boldsymbol{M}^{\star}$ is composed of independent and
zero-mean entries and is not symmetric. This might arise, for example, when we
have two independent samples for each entry of $\boldsymbol{M}^{\star}$ and
arrange them into an $\mathit{asymmetric}$ data matrix $\boldsymbol{M}$. The
aim is to estimate the leading eigenvalue and eigenvector of
$\boldsymbol{M}^{\star}$. Somewhat unexpectedly, our findings reveal that the
leading eigenvalue of the data matrix $\boldsymbol{M}$ can be $\sqrt{n}$ times
more accurate than its leading singular value in eigenvalue estimation.
Further, the perturbation of any linear form of the leading eigenvector of
$\boldsymbol{M}$ (e.g. entrywise eigenvector perturbation) is provably
well-controlled. We further provide partial theory for the more general
rank-$r$ case; this allows us to accommodate the case when
$\boldsymbol{M}^{\star}$ is rank-1 but asymmetric, by considering
eigen-decomposition of the associated rank-2 dilation matrix. The takeaway
message is this: arranging the data samples in an asymmetric manner and
performing eigen-decomposition (as opposed to SVD) could sometimes be quite
beneficial.


Near Optimal Bayesian Active Learning for Decision Making

  How should we gather information to make effective decisions? We address
Bayesian active learning and experimental design problems, where we
sequentially select tests to reduce uncertainty about a set of hypotheses.
Instead of minimizing uncertainty per se, we consider a set of overlapping
decision regions of these hypotheses. Our goal is to drive uncertainty into a
single decision region as quickly as possible.
  We identify necessary and sufficient conditions for correctly identifying a
decision region that contains all hypotheses consistent with observations. We
develop a novel Hyperedge Cutting (HEC) algorithm for this problem, and prove
that is competitive with the intractable optimal policy. Our efficient
implementation of the algorithm relies on computing subsets of the complete
homogeneous symmetric polynomials. Finally, we demonstrate its effectiveness on
two practical applications: approximate comparison-based learning and active
localization using a robot manipulator.


Accelerated dimension-independent adaptive Metropolis

  This work considers black-box Bayesian inference over high-dimensional
parameter spaces. The well-known adaptive Metropolis (AM) algorithm of (Haario
etal. 2001) is extended herein to scale asymptotically uniformly with respect
to the underlying parameter dimension for Gaussian targets, by respecting the
variance of the target. The resulting algorithm, referred to as the
dimension-independent adaptive Metropolis (DIAM) algorithm, also shows improved
performance with respect to adaptive Metropolis on non-Gaussian targets. This
algorithm is further improved, and the possibility of probing high-dimensional
targets is enabled, via GPU-accelerated numerical libraries and periodically
synchronized concurrent chains (justified a posteriori). Asymptotically in
dimension, this GPU implementation exhibits a factor of four improvement versus
a competitive CPU-based Intel MKL parallel version alone. Strong scaling to
concurrent chains is exhibited, through a combination of longer time per sample
batch (weak scaling) and yet fewer necessary samples to convergence. The
algorithm performance is illustrated on several Gaussian and non-Gaussian
target examples, in which the dimension may be in excess of one thousand.


Nonconvex Matrix Factorization from Rank-One Measurements

  We consider the problem of recovering low-rank matrices from random rank-one
measurements, which spans numerous applications including covariance sketching,
phase retrieval, quantum state tomography, and learning shallow polynomial
neural networks, among others. Our approach is to directly estimate the
low-rank factor by minimizing a nonconvex quadratic loss function via vanilla
gradient descent, following a tailored spectral initialization. When the true
rank is small, this algorithm is guaranteed to converge to the ground truth (up
to global ambiguity) with near-optimal sample complexity and computational
complexity. To the best of our knowledge, this is the first guarantee that
achieves near-optimality in both metrics. In particular, the key enabler of
near-optimal computational guarantees is an implicit regularization phenomenon:
without explicit regularization, both spectral initialization and the gradient
descent iterates automatically stay within a region incoherent with the
measurement vectors. This feature allows one to employ much more aggressive
step sizes compared with the ones suggested in prior literature, without the
need of sample splitting.


Teaching Categories to Human Learners with Visual Explanations

  We study the problem of computer-assisted teaching with explanations.
Conventional approaches for machine teaching typically only provide feedback at
the instance level e.g., the category or label of the instance. However, it is
intuitive that clear explanations from a knowledgeable teacher can
significantly improve a student's ability to learn a new concept. To address
these existing limitations, we propose a teaching framework that provides
interpretable explanations as feedback and models how the learner incorporates
this additional information. In the case of images, we show that we can
automatically generate explanations that highlight the parts of the image that
are responsible for the class label. Experiments on human learners illustrate
that, on average, participants achieve better test set performance on
challenging categorization tasks when taught with our interpretable approach
compared to existing methods.


Gradient Descent with Random Initialization: Fast Global Convergence for
  Nonconvex Phase Retrieval

  This paper considers the problem of solving systems of quadratic equations,
namely, recovering an object of interest
$\mathbf{x}^{\natural}\in\mathbb{R}^{n}$ from $m$ quadratic equations/samples
$y_{i}=(\mathbf{a}_{i}^{\top}\mathbf{x}^{\natural})^{2}$, $1\leq i\leq m$. This
problem, also dubbed as phase retrieval, spans multiple domains including
physical sciences and machine learning.
  We investigate the efficiency of gradient descent (or Wirtinger flow)
designed for the nonconvex least squares problem. We prove that under Gaussian
designs, gradient descent --- when randomly initialized --- yields an
$\epsilon$-accurate solution in $O\big(\log n+\log(1/\epsilon)\big)$ iterations
given nearly minimal samples, thus achieving near-optimal computational and
sample complexities at once. This provides the first global convergence
guarantee concerning vanilla gradient descent for phase retrieval, without the
need of (i) carefully-designed initialization, (ii) sample splitting, or (iii)
sophisticated saddle-point escaping schemes. All of these are achieved by
exploiting the statistical models in analyzing optimization algorithms, via a
leave-one-out approach that enables the decoupling of certain statistical
dependency between the gradient descent iterates and the data.


Barrier Certificates for Assured Machine Teaching

  Machine teaching has received significant attention in the past few years as
a paradigm shift from machine learning. While machine learning is often
concerned with improving the performance of learners, machine teaching pertains
to the efficiency of teachers. For example, machine teaching seeks to find the
optimal (minimum) number of data samples needed for teaching a target
hypothesis to a learner. Hence, it is natural to raise the question of how can
we provide assurances for teaching given a machine teaching algorithm. In this
paper, we address this question by borrowing notions from control theory. We
begin by proposing a model based on partially observable Markov decision
processes (POMDPs) for a class of machine teaching problems. We then show that
the POMDP formulation can be cast as a special hybrid system, i.e., a
discrete-time switched system. Subsequently, we use barrier certificates to
verify properties of this special hybrid system. We show how the computation of
the barrier certificate can be decomposed and numerically implemented as the
solution to a sum-of-squares (SOS) program. For illustration, we show how the
proposed framework based on control theory can be used to verify the teaching
performance of two well-known machine teaching methods.


Adversarial WiFi Sensing using a Single Smartphone

  Wireless devices are everywhere, at home, at the office, and on the street.
Devices are bombarding us with transmissions across a wide range of RF
frequencies. Many of these invisible transmissions reflect off our bodies,
carrying off information about ou location, movement, and other physiological
properties. While a boon to professionals with carefully calibrated
instruments, they may also be revealing private data about us to potential
attackers nearby.
  In this paper, we examine the problem of adversarial WiFi sensing, and
consider whether ambient WiFi signals around us pose real risks to our personal
privacy. We identify a passive adversarial sensing attack, where bad actors
using a single smartphone can silently localize and track individuals in their
home or office from outside walls, by just listening to ambient WiFi signals.
We experimentally validate this attack in 11 real-world locations, and show
user tracking with high accuracy. Finally, we propose and evaluate defenses
including geo-fencing, rate limiting, and signal obfuscation by WiFi access
points.


Pixel Level Data Augmentation for Semantic Image Segmentation using
  Generative Adversarial Networks

  Semantic segmentation is one of the basic topics in computer vision, it aims
to assign semantic labels to every pixel of an image. Unbalanced semantic label
distribution could have a negative influence on segmentation accuracy. In this
paper, we investigate using data augmentation approach to balance the semantic
label distribution in order to improve segmentation performance. We propose
using generative adversarial networks (GANs) to generate realistic images for
improving the performance of semantic segmentation networks. Experimental
results show that the proposed method can not only improve segmentation
performance on those classes with low accuracy, but also obtain 1.3% to 2.1%
increase in average segmentation accuracy. It shows that this augmentation
method can boost accuracy and be easily applicable to any other segmentation
models.


A General Framework for Multi-fidelity Bayesian Optimization with
  Gaussian Processes

  How can we efficiently gather information to optimize an unknown function,
when presented with multiple, mutually dependent information sources with
different costs? For example, when optimizing a robotic system, intelligently
trading off computer simulations and real robot testings can lead to
significant savings. Existing methods, such as multi-fidelity GP-UCB or Entropy
Search-based approaches, either make simplistic assumptions on the interaction
among different fidelities or use simple heuristics that lack theoretical
guarantees. In this paper, we study multi-fidelity Bayesian optimization with
complex structural dependencies among multiple outputs, and propose
MF-MI-Greedy, a principled algorithmic framework for addressing this problem.
In particular, we model different fidelities using additive Gaussian processes
based on shared latent structures with the target function. Then we use
cost-sensitive mutual information gain for efficient Bayesian global
optimization. We propose a simple notion of regret which incorporates the cost
of different fidelities, and prove that MF-MI-Greedy achieves low regret. We
demonstrate the strong empirical performance of our algorithm on both synthetic
and real-world datasets.


Optimizing Photonic Nanostructures via Multi-fidelity Gaussian Processes

  We apply numerical methods in combination with finite-difference-time-domain
(FDTD) simulations to optimize transmission properties of plasmonic mirror
color filters using a multi-objective figure of merit over a five-dimensional
parameter space by utilizing novel multi-fidelity Gaussian processes approach.
We compare these results with conventional derivative-free global search
algorithms, such as (single-fidelity) Gaussian Processes optimization scheme,
and Particle Swarm Optimization---a commonly used method in nanophotonics
community, which is implemented in Lumerical commercial photonics software. We
demonstrate the performance of various numerical optimization approaches on
several pre-collected real-world datasets and show that by properly trading off
expensive information sources with cheap simulations, one can more effectively
optimize the transmission properties with a fixed budget.


Trip Prediction by Leveraging Trip Histories from Neighboring Users

  We propose a novel approach for trip prediction by analyzing user's trip
histories. We augment users' (self-) trip histories by adding 'similar' trips
from other users, which could be informative and useful for predicting future
trips for a given user. This also helps to cope with noisy or sparse trip
histories, where the self-history by itself does not provide a reliable
prediction of future trips. We show empirical evidence that by enriching the
users' trip histories with additional trips, one can improve the prediction
error by 15%-40%, evaluated on multiple subsets of the Nancy2012 dataset. This
real-world dataset is collected from public transportation ticket validations
in the city of Nancy, France. Our prediction tool is a central component of a
trip simulator system designed to analyze the functionality of public
transportation in the city of Nancy.


A One-Class Support Vector Machine Calibration Method for Time Series
  Change Point Detection

  It is important to identify the change point of a system's health status,
which usually signifies an incipient fault under development. The One-Class
Support Vector Machine (OC-SVM) is a popular machine learning model for anomaly
detection and hence could be used for identifying change points; however, it is
sometimes difficult to obtain a good OC-SVM model that can be used on sensor
measurement time series to identify the change points in system health status.
In this paper, we propose a novel approach for calibrating OC-SVM models. The
approach uses a heuristic search method to find a good set of input data and
hyperparameters that yield a well-performing model. Our results on the C-MAPSS
dataset demonstrate that OC-SVM can also achieve satisfactory accuracy in
detecting change point in time series with fewer training data, compared to
state-of-the-art deep learning approaches. In our case study, the OC-SVM
calibrated by the proposed model is shown to be useful especially in scenarios
with limited amount of training data.


AED-Net: An Abnormal Event Detection Network

  It is challenging to detect the anomaly in crowded scenes for quite a long
time. In this paper, a self-supervised framework, abnormal event detection
network (AED-Net), which is composed of PCAnet and kernel principal component
analysis (kPCA), is proposed to address this problem. Using surveillance video
sequences of different scenes as raw data, PCAnet is trained to extract
high-level semantics of crowd's situation. Next, kPCA,a one-class classifier,
is trained to determine anomaly of the scene. In contrast to some prevailing
deep learning methods,the framework is completely self-supervised because it
utilizes only video sequences in a normal situation. Experiments of global and
local abnormal event detection are carried out on UMN and UCSD datasets, and
competitive results with higher EER and AUC compared to other state-of-the-art
methods are observed. Furthermore, by adding local response normalization (LRN)
layer, we propose an improvement to original AED-Net. And it is proved to
perform better by promoting the framework's generalization capacity according
to the experiments.


Cloud structure of three Galactic infrared dark star-forming regions
  from combining ground and space based bolometric observations

  We have modified the iterative procedure introduced by Lin et al. (2016), to
systematically combine the submm images taken from ground based (e.g., CSO,
JCMT, APEX) and space (e.g., Herschel, Planck) telescopes. We applied the
updated procedure to observations of three well studied Infrared Dark Clouds
(IRDCs): G11.11-0.12, G14.225-0.506 and G28.34+0.06, and then performed
single-component, modified black-body fits to derive $\sim$10$"$ resolution
dust temperature and column density maps. The derived column density maps show
that these three IRDCs exhibit complex filamentary structures embedding with
rich clumps/cores. We compared the column density probability distribution
functions (N-PDFs) and two-point correlation (2PT) functions of the column
density field between these IRDCs with several OB cluster-forming regions.
Based on the observed correlation and measurements, and complementary
hydrodynamical simulations for a 10$^{4}$ $\rm M_{\odot}$ molecular cloud, we
hypothesize that cloud evolution can be better characterized by the evolution
of the (column) density distribution function and the relative power of dense
structures as a function of spatial scales, rather than merely based on the
presence of star-forming activity. Based on the small analyzed sample, we
propose four evolutionary stages, namely: {\it cloud integration, stellar
assembly, cloud pre-dispersal and dispersed-cloud.} The initial {\it cloud
integration} stage and the final {\it dispersed cloud} stage may be
distinguished from the two intermediate stages by a steeper than $-$4 power-law
index of the N-PDF. The {\it cloud integration} stage and the subsequent {\it
stellar assembly} stage are further distinguished from each other by the larger
luminosity-to-mass ratio ($>$40 $\rm L_{\odot}/M_{\odot}$) of the latter.


Evolution of Iron K$_α$ Line Emission in the Black Hole Candidate
  GX 339-4

  GX 339-4 was regularly monitored with RXTE during a period (in 1999) when its
X-ray flux decreased significantly (from 4.2$\times 10^{-10}$ erg cm$^{-2}
s^{-1}$ to 7.6$\times 10^{-12}$ erg cm$^{-2}$s$^{-1}$ in the 3--20 keV band),
as the source settled into the ``off state''. Our spectral analysis revealed
the presence of a prominent iron K$_{\alpha}$ line in the observed spectrum of
the source for all observations. The line shows an interesting evolution: it is
centered at $\sim$6.4 keV when the measured flux is above 5$\times 10^{-11}$
erg cm$^{-2} s^{-1}$, but is shifted to $\sim$6.7 keV at lower fluxes. The
equivalent width of the line appears to increase significantly toward lower
fluxes, although it is likely to be sensitive to calibration uncertainties.
While the fluorescent emission of neutral or mildly ionized iron atoms in the
accretion disk can perhaps account for the 6.4 keV line, as is often invoked
for black hole candidates, it seems difficult to understand the 6.7 keV line
with this mechanism, because the disk should be less ionized at lower fluxes
(unless its density changes drastically). On the other hand, the 6.7 keV line
might be due to recombination cascade of hydrogen or helium like iron ions in
an optically thin, highly ionized plasma. We discuss the results in the context
of proposed accretion models.


A minimal model of predator-swarm interactions

  We propose a minimal model of predator-swarm interactions which captures many
of the essential dynamics observed in nature. Different outcomes are observed
depending on the predator strength. For a "weak" predator, the swarm is able to
escape the predator completely. As the strength is increased, the predator is
able to catch up with the swarm as a whole, but the individual prey are able to
escape by "confusing" the predator: the prey forms a ring with the predator at
the center. For higher predator strength, complex chasing dynamics are observed
which can become chaotic. For even higher strength, the predator is able to
successfully capture the prey. Our model is simple enough to be amenable to a
full mathematical analysis which is used to predict the shape of the swarm as
well as the resulting predator-prey dynamics as a function of model parameters.
We show that as the predator strength is increased, there is a transition (due
to a Hopf bifurcation) from confusion state to chasing dynamics, and we compute
the threshold analytically. Our analysis indicates that the swarming behaviour
is not helpful in avoiding the predator, suggesting that there are other
reasons why the species may swarm. The complex shape of the swarm in our model
during the chasing dynamics is similar to the shape of a flock of sheep
avoiding a shepherd.


Collective behaviour of large number of vortices in the plane

  We investigate the dynamics of $N$ point vortices in the plane, in the limit
of large $N$. We consider {\em relative equilibria}, which are rigidly rotating
lattice-like configurations of vortices. These configurations were observed in
several recent experiments [Durkin and Fajans, Phys. Fluids (2000) 12, 289-293;
Grzybowski {\em et.al} PRE (2001)64, 011603]. We show that these solutions and
their stability are fully characterized via a related {\em aggregation model}
which was recently investigated in the context of biological swarms [Fetecau
{\em et.al.}, Nonlinearity (2011) 2681; Bertozzi {\em et.al.}, M3AS (2011)]. By
utilizing this connection, we give explicit analytic formulae for many of the
configurations that have been observed experimentally. These include
configurations of vortices of equal strength; the $N+1$ configurations of $N$
vortices of equal strength and one vortex of much higher strength; and more
generally, $N+K$ configurations. We also give examples of configurations that
have not been studied experimentally, including $N+2$ configurations where $N$
vortices aggregate inside an ellipse. Finally, we introduce an artificial
``damping'' to the vortex dynamics, in an attempt to explain the phenomenon of
crystalization that is often observed in real experiments. The diffusion breaks
the conservative structure of vortex dynamics so that any initial conditions
converge to the lattice-like relative equilibrium.


Assessing the robustness of spatial pattern sequences in a dryland
  vegetation model

  A particular sequence of patterns, "$\text{gaps} \to \text{labyrinth} \to
\text{spots}$," occurs with decreasing precipitation in previously reported
numerical simulations of PDE dryland vegetation models. These observations have
led to the suggestion that this sequence of patterns can serve as an early
indicator of desertification in some ecosystems. Since parameter values can
take on a range of plausible values in the vegetation models, it is important
to investigate whether the pattern sequence prediction is robust to variation.
For a particular model, we find that a quantity calculated via
bifurcation-theoretic analysis appears to serve as a proxy for the pattern
sequences that occur in numerical simulations across a range of parameter
values. We find in further analysis that the quantity takes on values
consistent with the standard sequence in an ecologically relevant limit of the
model parameter values. This suggests that the standard sequence is a robust
prediction of the model, and we conclude by proposing a methodology for
assessing the robustness of the standard sequence in other models and
formulations.


Implicit Regularization in Nonconvex Statistical Estimation: Gradient
  Descent Converges Linearly for Phase Retrieval, Matrix Completion and Blind
  Deconvolution

  Recent years have seen a flurry of activities in designing provably efficient
nonconvex procedures for solving statistical estimation problems. Due to the
highly nonconvex nature of the empirical loss, state-of-the-art procedures
often require proper regularization (e.g. trimming, regularized cost,
projection) in order to guarantee fast convergence. For vanilla procedures such
as gradient descent, however, prior theory either recommends highly
conservative learning rates to avoid overshooting, or completely lacks
performance guarantees.
  This paper uncovers a striking phenomenon in nonconvex optimization: even in
the absence of explicit regularization, gradient descent enforces proper
regularization implicitly under various statistical models. In fact, gradient
descent follows a trajectory staying within a basin that enjoys nice geometry,
consisting of points incoherent with the sampling mechanism. This "implicit
regularization" feature allows gradient descent to proceed in a far more
aggressive fashion without overshooting, which in turn results in substantial
computational savings. Focusing on three fundamental statistical estimation
problems, i.e. phase retrieval, low-rank matrix completion, and blind
deconvolution, we establish that gradient descent achieves near-optimal
statistical and computational guarantees without explicit regularization. In
particular, by marrying statistical modeling with generic optimization theory,
we develop a general recipe for analyzing the trajectories of iterative
algorithms via a leave-one-out perturbation argument. As a byproduct, for noisy
matrix completion, we demonstrate that gradient descent achieves near-optimal
error control --- measured entrywise and by the spectral norm --- which might
be of independent interest.


Noisy Matrix Completion: Understanding Statistical Guarantees for Convex
  Relaxation via Nonconvex Optimization

  This paper studies noisy low-rank matrix completion: given partial and
corrupted entries of a large low-rank matrix, the goal is to estimate the
underlying matrix faithfully and efficiently. Arguably one of the most popular
paradigms to tackle this problem is convex relaxation, which achieves
remarkable efficacy in practice. However, the theoretical support of this
approach is still far from optimal in the noisy setting, falling short of
explaining the empirical success.
  We make progress towards demystifying the practical efficacy of convex
relaxation vis-\`a-vis random noise. When the rank of the unknown matrix is a
constant, we demonstrate that the convex programming approach achieves
near-optimal estimation errors --- in terms of the Euclidean loss, the
entrywise loss, and the spectral norm loss --- for a wide range of noise
levels. All of this is enabled by bridging convex relaxation with the nonconvex
Burer-Monteiro approach, a seemingly distinct algorithmic paradigm that is
provably robust against noise. More specifically, we show that an approximate
critical point of the nonconvex formulation serves as an extremely tight
approximation of the convex solution, allowing us to transfer the desired
statistical guarantees of the nonconvex approach to its convex counterpart.


Filamentary Accretion Flows in the Infrared Dark Cloud G14.225-0.506
  Revealed by ALMA

  Filaments are ubiquitous structures in molecular clouds and play an important
role in the mass assembly of stars. We present results of dynamical stability
analyses for filaments in the infrared dark cloud G14.225$-$0.506, where a
delayed onset of massive star formation was reported in the two hubs at the
convergence of multiple filaments of parsec length. Full-synthesis imaging is
performed with the Atacama Large Millimeter/submillimeter Array (ALMA) to map
the $\mathrm{N_2H^+} \; (1-0)$ emission in two hub-filament systems with a
spatial resolution of $\sim 0.034 \; \mathrm{pc}$. Kinematics are derived from
sophisticated spectral fitting algorithm that accounts for line blending, large
optical depth, and multiple velocity components. We identify five velocity
coherent filaments and derive their velocity gradients with principal component
analysis. The mass accretion rates along the filaments are up to $10^{-4} \;
\mathrm{M_\odot \, \mathrm{yr^{-1}}}$ and are significant enough to affect the
hub dynamics within one free-fall time ($\sim 10^5 \; \mathrm{yr}$). The
$\mathrm{N_2H^+}$ filaments are in equilibrium with virial parameter
$\alpha_\mathrm{vir} \sim 1.2$. We compare $\alpha_\mathrm{vir}$ measured in
the $\mathrm{N_2H^+}$ filaments, $\mathrm{NH_3}$ filaments, $870 \;
\mu\mathrm{m}$ dense clumps, and $3 \; \mathrm{mm}$ dense cores. The decreasing
trend in $\alpha_\mathrm{vir}$ with decreasing spatial scales persists,
suggesting an increasingly important role of gravity at small scales.
Meanwhile, $\alpha_\mathrm{vir}$ also decreases with decreasing non-thermal
motions. In combination with the absence of high-mass protostars and massive
cores, our results are consistent with the global hierarchical collapse
scenario.


A Luminous Peculiar Type Ia Supernova SN 2011hr: More Like SN 1991T or
  SN 2007if?

  Photometric and spectroscopic observations of a slowly declining, luminous
Type Ia supernova (SN Ia) SN 2011hr in the starburst galaxy NGC 2691 are
presented. SN 2011hr is found to peak at $M_{B}=-19.84 \pm 0.40\,\rm{mag}$,
with a post-maximum decline rate $\Delta$m$_{15}$(B) = 0.92 $\pm$
0.03\,$\rm{mag}$. From the maximum-light bolometric luminosity, $L=(2.30 \pm
0.90) \times 10^{43}\,\rm{erg\,s^{-1}}$, we estimate the mass of synthesized
\Nifs\ in SN 2011hr to be $M(\rm{^{56}Ni})=1.11 \pm 0.43\,M_{\sun}$. SN 2011hr
appears more luminous than SN 1991T at around maximum light, and the absorption
features from its intermediate-mass elements (IMEs) are noticeably weaker than
the latter at similar phases. Spectral modeling suggests that SN 2011hr has the
IMEs of $\sim$\,0.07 M$_{\sun}$ in the outer ejecta, which is much lower than
the typical value of normal SNe Ia (i.e., 0.3 -- 0.4 M$_{\sun}$) and is also
lower than the value of SN 1991T (i.e., $\sim$\,0.18 M$_{\sun}$). These results
indicate that SN 2011hr may arise from a Chandrasekhar-mass white dwarf
progenitor that experienced a more efficient burning process in the explosion.
Nevertheless, it is still possible that SN 2011hr may serve as a transitional
object connecting the SN 1991T-like SNe Ia with the superluminous subclass like
SN 2007if given that the latter also shows very weak IMEs at all phases.


The Likelihood Ratio Test in High-Dimensional Logistic Regression Is
  Asymptotically a Rescaled Chi-Square

  Logistic regression is used thousands of times a day to fit data, predict
future outcomes, and assess the statistical significance of explanatory
variables. When used for the purpose of statistical inference, logistic models
produce p-values for the regression coefficients by using an approximation to
the distribution of the likelihood-ratio test. Indeed, Wilks' theorem asserts
that whenever we have a fixed number $p$ of variables, twice the log-likelihood
ratio (LLR) $2\Lambda$ is distributed as a $\chi^2_k$ variable in the limit of
large sample sizes $n$; here, $k$ is the number of variables being tested. In
this paper, we prove that when $p$ is not negligible compared to $n$, Wilks'
theorem does not hold and that the chi-square approximation is grossly
incorrect; in fact, this approximation produces p-values that are far too small
(under the null hypothesis). Assume that $n$ and $p$ grow large in such a way
that $p/n\rightarrow\kappa$ for some constant $\kappa < 1/2$. We prove that for
a class of logistic models, the LLR converges to a rescaled chi-square, namely,
$2\Lambda~\stackrel{\mathrm{d}}{\rightarrow}~\alpha(\kappa)\chi_k^2$, where the
scaling factor $\alpha(\kappa)$ is greater than one as soon as the
dimensionality ratio $\kappa$ is positive. Hence, the LLR is larger than
classically assumed. For instance, when $\kappa=0.3$,
$\alpha(\kappa)\approx1.5$. In general, we show how to compute the scaling
factor by solving a nonlinear system of two equations with two unknowns. Our
mathematical arguments are involved and use techniques from approximate message
passing theory, non-asymptotic random matrix theory and convex geometry. We
also complement our mathematical study by showing that the new limiting
distribution is accurate for finite sample sizes. Finally, all the results from
this paper extend to some other regression models such as the probit regression
model.


