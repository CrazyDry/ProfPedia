Channel Capacity under General Nonuniform Sampling

  This paper develops the fundamental capacity limits of a sampled analogchannel under a sub-Nyquist sampling rate constraint. In particular, we derivethe capacity of sampled analog channels over a general class of time-preservingsampling methods including irregular nonuniform sampling. Our results indicatethat the optimal sampling structures extract out the set of frequencies thatexhibits the highest SNR among all spectral sets of support size equal to thesampling rate. The capacity under sub-Nyquist sampling can be attained throughfilter-bank sampling, or through a single branch of modulation and filteringfollowed by uniform sampling. The capacity under sub-Nyquist sampling is amonotone function of the sampling rate. These results indicate that the optimalsampling schemes suppress aliasing, and that employing irregular nonuniformsampling does not provide capacity gain over uniform sampling sets withappropriate preprocessing for a large class of channels.

An Algorithm for Exact Super-resolution and Phase Retrieval

  We explore a fundamental problem of super-resolving a signal of interest froma few measurements of its low-pass magnitudes. We propose a 2-stage tractablealgorithm that, in the absence of noise, admits perfect super-resolution of an$r$-sparse signal from $2r^2-2r+2$ low-pass magnitude measurements. The spikelocations of the signal can assume any value over a continuous disk, withoutincreasing the required sample size. The proposed algorithm first employs aconventional super-resolution algorithm (e.g. the matrix pencil approach) torecover unlabeled sets of signal correlation coefficients, and then applies asimple sorting algorithm to disentangle and retrieve the true parameters in adeterministic manner. Our approach can be adapted to multi-dimensional spikemodels and random Fourier sampling by replacing its first step with otherharmonic retrieval algorithms.

Community Recovery in Graphs with Locality

  Motivated by applications in domains such as social networks andcomputational biology, we study the problem of community recovery in graphswith locality. In this problem, pairwise noisy measurements of whether twonodes are in the same community or different communities come mainly orexclusively from nearby nodes rather than uniformly sampled between all nodespairs, as in most existing models. We present an algorithm that runs nearlylinearly in the number of measurements and which achieves the informationtheoretic limit for exact recovery.

Near-optimal Bayesian Active Learning with Correlated and Noisy Tests

  We consider the Bayesian active learning and experimental design problem,where the goal is to learn the value of some unknown target variable through asequence of informative, noisy tests. In contrast to prior work, we focus onthe challenging, yet practically relevant setting where test outcomes can beconditionally dependent given the hidden target variable. Under suchassumptions, common heuristics, such as greedily performing tests that maximizethe reduction in uncertainty of the target, often perform poorly. In thispaper, we propose ECED, a novel, computationally efficient active learningalgorithm, and prove strong theoretical guarantees that hold with correlated,noisy tests. Rather than directly optimizing the prediction error, at eachstep, ECED picks the test that maximizes the gain in a surrogate objective,which takes into account the dependencies between tests. Our analysis relies onan information-theoretic auxiliary function to track the progress of ECED, andutilizes adaptive submodularity to attain the near-optimal bound. Wedemonstrate strong empirical performance of ECED on two problem instances,including a Bayesian experimental design task intended to distinguish amongeconomic theories of how people make risky decisions, and an active preferencelearning task via pairwise comparisons.

Noise-induced tipping under periodic forcing: preferred tipping phase in  a non-adiabatic forcing regime

  We consider a periodically-forced 1-D Langevin equation that possesses twostable periodic solutions in the absence of noise. We ask the question: isthere a most likely noise-induced transition path between these periodicsolutions that allows us to identify a preferred phase of the forcing whentipping occurs? The quasistatic regime, where the forcing period is longcompared to the adiabatic relaxation time, has been well studied; our workinstead explores the case when these timescales are comparable. We computeoptimal paths using the path integral method incorporating the Onsager-Machlupfunctional and validate results with Monte Carlo simulations. Results for thepreferred tipping phase are compared with the deterministic aspects of theproblem. We identify parameter regimes where nullclines, associated with thedeterministic problem in a 2-D extended phase space, form passageways throughwhich the optimal paths transit. As the nullclines are independent of therelaxation time and the noise strength, this leads to a robust deterministicpredictor of preferred tipping phase in a regime where forcing is neither toofast, nor too slow.

Nonconvex Optimization Meets Low-Rank Matrix Factorization: An Overview

  Substantial progress has been made recently on developing provably accurateand efficient algorithms for low-rank matrix factorization via nonconvexoptimization. While conventional wisdom often takes a dim view of nonconvexoptimization algorithms due to their susceptibility to spurious local minima,simple iterative methods such as gradient descent have been remarkablysuccessful in practice. The theoretical footings, however, had been largelylacking until recently.  In this tutorial-style overview, we highlight the important role ofstatistical models in enabling efficient nonconvex optimization withperformance guarantees. We review two contrasting approaches: (1) two-stagealgorithms, which consist of a tailored initialization step followed bysuccessive refinement; and (2) global landscape analysis andinitialization-free algorithms. Several canonical matrix factorization problemsare discussed, including but not limited to matrix sensing, phase retrieval,matrix completion, blind deconvolution, robust principal component analysis,phase synchronization, and joint alignment. Special care is taken to illustratethe key technical insights underlying their analyses. This article serves as atestament that the integrated thinking of optimization and statistics leads tofruitful research findings.

Addressing Training Bias via Automated Image Annotation

  Build accurate DNN models requires training on large labeled, contextspecific datasets, especially those matching the target scenario. We believeadvances in wireless localization, working in unison with cameras, can produceautomated annotation of targets on images and videos captured in the wild.Using pedestrian and vehicle detection as examples, we demonstrate thefeasibility, benefits, and challenges of an automatic image annotation system.Our work calls for new technical development on passive localization, mobiledata analytics, and error-resilient ML models, as well as design issues in userprivacy policies.

An Upper Bound on Multi-hop Transmission Capacity with Dynamic Routing  Selection

  This paper develops upper bounds on the end-to-end transmission capacity ofmulti-hop wireless networks. Potential source-destination paths are dynamicallyselected from a pool of randomly located relays, from which a closed-form lowerbound on the outage probability is derived in terms of the expected number ofpotential paths. This is in turn used to provide an upper bound on the numberof successful transmissions that can occur per unit area, which is known as thetransmission capacity. The upper bound results from assuming independence amongthe potential paths, and can be viewed as the maximum diversity case. A usefulaspect of the upper bound is its simple form for an arbitrary-sized network,which allows insights into how the number of hops and other network parametersaffect spatial throughput in the non-asymptotic regime. The outage probabilityanalysis is then extended to account for retransmissions with a maximum numberof allowed attempts. In contrast to prevailing wisdom, we show thatpredetermined routing (such as nearest-neighbor) is suboptimal, since more hopsare not useful once the network is interference-limited. Our results also makeclear that randomness in the location of relay sets and dynamically varyingchannel states is helpful in obtaining higher aggregate throughput, and thatdynamic route selection should be used to exploit path diversity.

Shannon Meets Nyquist: Capacity of Sampled Gaussian Channels

  We explore two fundamental questions at the intersection of sampling theoryand information theory: how channel capacity is affected by sampling below thechannel's Nyquist rate, and what sub-Nyquist sampling strategy should beemployed to maximize capacity. In particular, we derive the capacity of sampledanalog channels for three prevalent sampling strategies: sampling withfiltering, sampling with filter banks, and sampling with modulation and filterbanks. These sampling mechanisms subsume most nonuniform sampling techniquesapplied in practice. Our analyses illuminate interesting connections betweenunder-sampled channels and multiple-input multiple-output channels. The optimalsampling structures are shown to extract out the frequencies with the highestSNR from each aliased frequency set, while suppressing aliasing and out-of-bandnoise. We also highlight connections between undersampled channel capacity andminimum mean-squared error (MSE) estimation from sampled data. In particular,we show that the filters maximizing capacity and the ones minimizing MSE areequivalent under both filtering and filter-bank sampling strategies. Theseresults demonstrate the effect upon channel capacity of sub-Nyquist samplingtechniques, and characterize the tradeoff between information rate and samplingrate.

Channel Capacity under Sub-Nyquist Nonuniform Sampling

  This paper investigates the effect of sub-Nyquist sampling upon the capacityof an analog channel. The channel is assumed to be a linear time-invariantGaussian channel, where perfect channel knowledge is available at both thetransmitter and the receiver. We consider a general class of right-invertibletime-preserving sampling methods which include irregular nonuniform sampling,and characterize in closed form the channel capacity achievable by this classof sampling methods, under a sampling rate and power constraint. Our resultsindicate that the optimal sampling structures extract out the set offrequencies that exhibits the highest signal-to-noise ratio among all spectralsets of measure equal to the sampling rate. This can be attained throughfilterbank sampling with uniform sampling at each branch with possiblydifferent rates, or through a single branch of modulation and filteringfollowed by uniform sampling. These results reveal that for a large class ofchannels, employing irregular nonuniform sampling sets, while typicallycomplicated to realize, does not provide capacity gain over uniform samplingsets with appropriate preprocessing. Our findings demonstrate that aliasing orscrambling of spectral components does not provide capacity gain, which is incontrast to the benefits obtained from random mixing in spectrum-blindcompressive sampling schemes.

Spectral Compressed Sensing via Structured Matrix Completion

  The paper studies the problem of recovering a spectrally sparse object from asmall number of time domain samples. Specifically, the object of interest withambient dimension $n$ is assumed to be a mixture of $r$ complexmulti-dimensional sinusoids, while the underlying frequencies can assume anyvalue in the unit disk. Conventional compressed sensing paradigms suffer fromthe {\em basis mismatch} issue when imposing a discrete dictionary on theFourier representation. To address this problem, we develop a novelnonparametric algorithm, called enhanced matrix completion (EMaC), based onstructured matrix completion. The algorithm starts by arranging the data into alow-rank enhanced form with multi-fold Hankel structure, then attempts recoveryvia nuclear norm minimization. Under mild incoherence conditions, EMaC allowsperfect recovery as soon as the number of samples exceeds the order of$\mathcal{O}(r\log^{2} n)$. We also show that, in many instances, accuratecompletion of a low-rank multi-fold Hankel matrix is possible when the numberof observed entries is proportional to the information theoretical limits(except for a logarithmic gap). The robustness of EMaC against bounded noiseand its applicability to super resolution are further demonstrated by numericalexperiments.

On the Minimax Capacity Loss under Sub-Nyquist Universal Sampling

  This paper investigates the information rate loss in analog channels when thesampler is designed to operate independent of the instantaneous channeloccupancy. Specifically, a multiband linear time-invariant Gaussian channelunder universal sub-Nyquist sampling is considered. The entire channelbandwidth is divided into $n$ subbands of equal bandwidth. At each time only$k$ constant-gain subbands are active, where the instantaneous subbandoccupancy is not known at the receiver and the sampler. We study theinformation loss through a capacity loss metric, that is, the capacity gapcaused by the lack of instantaneous subband occupancy information. Wecharacterize the minimax capacity loss for the entire sub-Nyquist rate regime,provided that the number $n$ of subbands and the SNR are both large. Theminimax limits depend almost solely on the band sparsity factor and theundersampling factor, modulo some residual terms that vanish as $n$ and SNRgrow. Our results highlight the power of randomized sampling methods (i.e. thesamplers that consist of random periodic modulation and low-pass filters),which are able to approach the minimax capacity loss with exponentially highprobability.

Backing off from Infinity: Performance Bounds via Concentration of  Spectral Measure for Random MIMO Channels

  The performance analysis of random vector channels, particularlymultiple-input-multiple-output (MIMO) channels, has largely been established inthe asymptotic regime of large channel dimensions, due to the analyticalintractability of characterizing the exact distribution of the objectiveperformance metrics. This paper exposes a new non-asymptotic framework thatallows the characterization of many canonical MIMO system performance metricsto within a narrow interval under moderate-to-large channel dimensionality,provided that these metrics can be expressed as a separable function of thesingular values of the matrix. The effectiveness of our framework isillustrated through two canonical examples. Specifically, we characterize themutual information and power offset of random MIMO channels, as well as theminimum mean squared estimation error of MIMO channel inputs from the channeloutputs. Our results lead to simple, informative, and reasonably accuratecontrol of various performance metrics in the finite-dimensional regime, ascorroborated by the numerical simulations. Our analysis framework isestablished via the concentration of spectral measure phenomenon for randommatrices uncovered by Guionnet and Zeitouni, which arises in a variety ofrandom matrix ensembles irrespective of the precise distributions of the matrixentries.

Information Recovery from Pairwise Measurements

  A variety of information processing tasks in practice involve recovering $n$objects from single-shot graph-based measurements, particularly those takenover the edges of some measurement graph $\mathcal{G}$. This paper concerns thesituation where each object takes value over a group of $M$ different values,and where one is interested to recover all these values based on observationsof certain pairwise relations over $\mathcal{G}$. The imperfection ofmeasurements presents two major challenges for information recovery: 1)$\textit{inaccuracy}$: a (dominant) portion $1-p$ of measurements arecorrupted; 2) $\textit{incompleteness}$: a significant fraction of pairs areunobservable, i.e. $\mathcal{G}$ can be highly sparse.  Under a natural random outlier model, we characterize the $\textit{minimaxrecovery rate}$, that is, the critical threshold of non-corruption rate $p$below which exact information recovery is infeasible. This accommodates a verygeneral class of pairwise relations. For various homogeneous random graphmodels (e.g. Erdos Renyi random graphs, random geometric graphs, small worldgraphs), the minimax recovery rate depends almost exclusively on the edgesparsity of the measurement graph $\mathcal{G}$ irrespective of other graphicalmetrics. This fundamental limit decays with the group size $M$ at a square rootrate before entering a connectivity-limited regime. Under the Erdos Renyirandom graph, a tractable combinatorial algorithm is proposed to approach thelimit for large $M$ ($M=n^{\Omega(1)}$), while order-optimal recovery isenabled by semidefinite programs in the small $M$ regime.  The extended (and most updated) version of this work can be found at(http://arxiv.org/abs/1504.01369).

Scalable Semidefinite Relaxation for Maximum A Posterior Estimation

  Maximum a posteriori (MAP) inference over discrete Markov random fields is afundamental task spanning a wide spectrum of real-world applications, which isknown to be NP-hard for general graphs. In this paper, we propose a novelsemidefinite relaxation formulation (referred to as SDR) to estimate the MAPassignment. Algorithmically, we develop an accelerated variant of thealternating direction method of multipliers (referred to as SDPAD-LR) that caneffectively exploit the special structure of the new relaxation. Encouragingly,the proposed procedure allows solving SDR for large-scale problems, e.g.,problems on a grid graph comprising hundreds of thousands of variables withmultiple states per node. Compared with prior SDP solvers, SDPAD-LR is capableof attaining comparable accuracy while exhibiting remarkably improvedscalability, in contrast to the commonly held belief that semidefiniterelaxation can only been applied on small-scale MRF problems. We have evaluatedthe performance of SDR on various benchmark datasets including OPENGM2 and PICin terms of both the quality of the solutions and computation time.Experimental results demonstrate that for a broad class of problems, SDPAD-LRoutperforms state-of-the-art algorithms in producing better MAP assignment inan efficient manner.

Spectral MLE: Top-$K$ Rank Aggregation from Pairwise Comparisons

  This paper explores the preference-based top-$K$ rank aggregation problem.Suppose that a collection of items is repeatedly compared in pairs, and onewishes to recover a consistent ordering that emphasizes the top-$K$ rankeditems, based on partially revealed preferences. We focus on theBradley-Terry-Luce (BTL) model that postulates a set of latent preferencescores underlying all items, where the odds of paired comparisons depend onlyon the relative scores of the items involved.  We characterize the minimax limits on identifiability of top-$K$ rankeditems, in the presence of random and non-adaptive sampling. Our resultshighlight a separation measure that quantifies the gap of preference scoresbetween the $K^{\text{th}}$ and $(K+1)^{\text{th}}$ ranked items. The minimumsample complexity required for reliable top-$K$ ranking scales inversely withthe separation measure irrespective of other preference distribution metrics.To approach this minimax limit, we propose a nearly linear-time ranking scheme,called \emph{Spectral MLE}, that returns the indices of the top-$K$ items inaccordance to a careful score estimate. In a nutshell, Spectral MLE starts withan initial score estimate with minimal squared loss (obtained via a spectralmethod), and then successively refines each component with the assistance ofcoordinate-wise MLEs. Encouragingly, Spectral MLE allows perfect top-$K$ itemidentification under minimal sample complexity. The practical applicability ofSpectral MLE is further corroborated by numerical experiments.

The Projected Power Method: An Efficient Algorithm for Joint Alignment  from Pairwise Differences

  Various applications involve assigning discrete label values to a collectionof objects based on some pairwise noisy data. Due to the discrete---and hencenonconvex---structure of the problem, computing the optimal assignment(e.g.~maximum likelihood assignment) becomes intractable at first sight. Thispaper makes progress towards efficient computation by focusing on a concretejoint alignment problem---that is, the problem of recovering $n$ discretevariables $x_i \in \{1,\cdots, m\}$, $1\leq i\leq n$ given noisy observationsof their modulo differences $\{x_i - x_j~\mathsf{mod}~m\}$. We propose alow-complexity and model-free procedure, which operates in a lifted space byrepresenting distinct label values in orthogonal directions, and which attemptsto optimize quadratic functions over hypercubes. Starting with a first guesscomputed via a spectral method, the algorithm successively refines the iteratesvia projected power iterations. We prove that for a broad class of statisticalmodels, the proposed projected power method makes no error---and henceconverges to the maximum likelihood estimate---in a suitable regime. Numericalexperiments have been carried out on both synthetic and real data todemonstrate the practicality of our algorithm. We expect this algorithmicframework to be effective for a broad range of discrete assignment problems.

Efficient Online Learning for Optimizing Value of Information: Theory  and Application to Interactive Troubleshooting

  We consider the optimal value of information (VoI) problem, where the goal isto sequentially select a set of tests with a minimal cost, so that one canefficiently make the best decision based on the observed outcomes. Existingalgorithms are either heuristics with no guarantees, or scale poorly (withexponential run time in terms of the number of available tests). Moreover,these methods assume a known distribution over the test outcomes, which isoften not the case in practice. We propose an efficient sampling-based onlinelearning framework to address the above issues. First, assuming thedistribution over hypotheses is known, we propose a dynamic hypothesisenumeration strategy, which allows efficient information gathering with strongtheoretical guarantees. We show that with sufficient amount of samples, one canidentify a near-optimal decision with high probability. Second, when theparameters of the hypotheses distribution are unknown, we propose an algorithmwhich learns the parameters progressively via posterior sampling in an onlinefashion. We further establish a rigorous bound on the expected regret. Wedemonstrate the effectiveness of our approach on a real-world interactivetroubleshooting application and show that one can efficiently make high-qualitydecisions with low cost.

Understanding the Role of Adaptivity in Machine Teaching: The Case of  Version Space Learners

  In real-world applications of education, an effective teacher adaptivelychooses the next example to teach based on the learner's current state.However, most existing work in algorithmic machine teaching focuses on thebatch setting, where adaptivity plays no role. In this paper, we study the caseof teaching consistent, version space learners in an interactive setting. Atany time step, the teacher provides an example, the learner performs an update,and the teacher observes the learner's new state. We highlight that adaptivitydoes not speed up the teaching process when considering existing models ofversion space learners, such as "worst-case" (the learner picks the nexthypothesis randomly from the version space) and "preference-based" (the learnerpicks hypothesis according to some global preference). Inspired by humanteaching, we propose a new model where the learner picks hypotheses accordingto some local preference defined by the current hypothesis. We show that ourmodel exhibits several desirable properties, e.g., adaptivity plays a key role,and the learner's transitions over hypotheses are smooth/interpretable. Wedevelop efficient teaching algorithms and demonstrate our results viasimulation and user studies.

Teaching Multiple Concepts to a Forgetful Learner

  How can we help a forgetful learner learn multiple concepts within a limitedtime frame? For long-term learning, it is crucial to devise teaching strategiesthat leverage the underlying forgetting mechanisms of the learner. In thispaper, we cast the problem of adaptively teaching a forgetful learner as anovel discrete optimization problem, where we seek to optimize a naturalobjective function that characterizes the learner's expected performancethroughout the teaching session. We then propose a simple greedy teachingstrategy and derive strong performance guarantees based on two intuitivedata-dependent properties, which capture the degree of diminishing returns ofteaching each concept. We show that, given some assumptions about the learner'smemory model, one can efficiently compute the performance bounds. Furthermore,we identify parameter settings of the memory model where the greedy strategy isguaranteed to achieve high performance. We demonstrate the effectiveness of ouralgorithm using extensive simulations along with user studies in two concreteapplications, namely (i) an educational app for online vocabulary teaching and(ii) an app for teaching novices how to recognize animal species from images.

On the Role of Mobility for Multi-message Gossip

  We consider information dissemination in a large $n$-user wireless network inwhich $k$ users wish to share a unique message with all other users. Each ofthe $n$ users only has knowledge of its own contents and state information;this corresponds to a one-sided push-only scenario. The goal is to disseminateall messages efficiently, hopefully achieving an order-optimal spreading rateover unicast wireless random networks. First, we show that a random-pushstrategy -- where a user sends its own or a received packet at random -- isorder-wise suboptimal in a random geometric graph: specifically,$\Omega(\sqrt{n})$ times slower than optimal spreading. It is known that thisgap can be closed if each user has "full" mobility, since this effectivelycreates a complete graph. We instead consider velocity-constrained mobilitywhere at each time slot the user moves locally using a discrete random walkwith velocity $v(n)$ that is much lower than full mobility. We propose a simpletwo-stage dissemination strategy that alternates between individual messageflooding ("self promotion") and random gossiping. We prove that this schemeachieves a close to optimal spreading rate (within only a logarithmic gap) aslong as the velocity is at least $v(n)=\omega(\sqrt{\log n/k})$. The keyinsight is that the mixing property introduced by the partial mobility helpsusers to spread in space within a relatively short period compared to theoptimal spreading time, which macroscopically mimics message dissemination overa complete graph.

Robust Spectral Compressed Sensing via Structured Matrix Completion

  The paper explores the problem of \emph{spectral compressed sensing}, whichaims to recover a spectrally sparse signal from a small random subset of its$n$ time domain samples. The signal of interest is assumed to be asuperposition of $r$ multi-dimensional complex sinusoids, while the underlyingfrequencies can assume any \emph{continuous} values in the normalized frequencydomain. Conventional compressed sensing paradigms suffer from the basismismatch issue when imposing a discrete dictionary on the Fourierrepresentation. To address this issue, we develop a novel algorithm, called\emph{Enhanced Matrix Completion (EMaC)}, based on structured matrix completionthat does not require prior knowledge of the model order. The algorithm startsby arranging the data into a low-rank enhanced form exhibiting multi-foldHankel structure, and then attempts recovery via nuclear norm minimization.Under mild incoherence conditions, EMaC allows perfect recovery as soon as thenumber of samples exceeds the order of $r\log^{4}n$, and is stable againstbounded noise. Even if a constant portion of samples are corrupted witharbitrary magnitude, EMaC still allows exact recovery, provided that the samplecomplexity exceeds the order of $r^{2}\log^{3}n$. Along the way, our resultsdemonstrate the power of convex relaxation in completing a low-rank multi-foldHankel or Toeplitz matrix from minimal observed entries. The performance of ouralgorithm and its applicability to super resolution are further validated bynumerical experiments.

Exact and Stable Covariance Estimation from Quadratic Sampling via  Convex Programming

  Statistical inference and information processing of high-dimensional dataoften require efficient and accurate estimation of their second-orderstatistics. With rapidly changing data, limited processing power and storage atthe acquisition devices, it is desirable to extract the covariance structurefrom a single pass over the data and a small number of stored measurements. Inthis paper, we explore a quadratic (or rank-one) measurement model whichimposes minimal memory requirements and low computational complexity during thesampling process, and is shown to be optimal in preserving variouslow-dimensional covariance structures. Specifically, four popular structuralassumptions of covariance matrices, namely low rank, Toeplitz low rank,sparsity, jointly rank-one and sparse structure, are investigated, whilerecovery is achieved via convex relaxation paradigms for the respectivestructure.  The proposed quadratic sampling framework has a variety of potentialapplications including streaming data processing, high-frequency wirelesscommunication, phase space tomography and phase retrieval in optics, andnon-coherent subspace detection. Our method admits universally accuratecovariance estimation in the absence of noise, as soon as the number ofmeasurements exceeds the information theoretic limits. We also demonstrate therobustness of this approach against noise and imperfect structural assumptions.Our analysis is established upon a novel notion called the mixed-normrestricted isometry property (RIP-$\ell_{2}/\ell_{1}$), as well as theconventional RIP-$\ell_{2}/\ell_{2}$ for near-isotropic and boundedmeasurements. In addition, our results improve upon the best-known phaseretrieval (including both dense and sparse signals) guarantees using PhaseLiftwith a significantly simpler approach.

Near-Optimal Joint Object Matching via Convex Relaxation

  Joint matching over a collection of objects aims at aggregating informationfrom a large collection of similar instances (e.g. images, graphs, shapes) toimprove maps between pairs of them. Given multiple matches computed between afew object pairs in isolation, the goal is to recover an entire collection ofmaps that are (1) globally consistent, and (2) close to the provided maps ---and under certain conditions provably the ground-truth maps. Despite recentadvances on this problem, the best-known recovery guarantees are limited to asmall constant barrier --- none of the existing methods find theoreticalsupport when more than $50\%$ of input correspondences are corrupted. Moreover,prior approaches focus mostly on fully similar objects, while it is practicallymore demanding to match instances that are only partially similar to eachother.  In this paper, we develop an algorithm to jointly match multiple objects thatexhibit only partial similarities, given a few pairwise matches that aredensely corrupted. Specifically, we propose to recover the ground-truth mapsvia a parameter-free convex program called MatchLift, following a spectralmethod that pre-estimates the total number of distinct elements to be matched.Encouragingly, MatchLift exhibits near-optimal error-correction ability, i.e.in the asymptotic regime it is guaranteed to work even when a dominant fraction$1-\Theta\left(\frac{\log^{2}n}{\sqrt{n}}\right)$ of the input maps behave likerandom outliers. Furthermore, MatchLift succeeds with minimal input complexity,namely, perfect matching can be achieved as soon as the provided maps form aconnected map graph. We evaluate the proposed algorithm on various benchmarkdata sets including synthetic examples and real-world examples, all of whichconfirm the practical applicability of MatchLift.

Information Recovery from Pairwise Measurements

  This paper is concerned with jointly recovering $n$ node-variables $\left\{x_{i}\right\}_{1\leq i\leq n}$ from a collection of pairwise differencemeasurements. Imagine we acquire a few observations taking the form of$x_{i}-x_{j}$; the observation pattern is represented by a measurement graph$\mathcal{G}$ with an edge set $\mathcal{E}$ such that $x_{i}-x_{j}$ isobserved if and only if $(i,j)\in\mathcal{E}$. To account for noisymeasurements in a general manner, we model the data acquisition process by aset of channels with given input/output transition measures. Employinginformation-theoretic tools applied to channel decoding problems, we develop a\emph{unified} framework to characterize the fundamental recovery criterion,which accommodates general graph structures, alphabet sizes, and channeltransition measures. In particular, our results isolate a family of\emph{minimum} \emph{channel divergence measures} to characterize the degree ofmeasurement corruption, which together with the size of the minimum cut of$\mathcal{G}$ dictates the feasibility of exact information recovery. Forvarious homogeneous graphs, the recovery condition depends almost only on theedge sparsity of the measurement graph irrespective of other graphical metrics;alternatively, the minimum sample complexity required for these graphs scaleslike \[ \text{minimum sample complexity }\asymp\frac{n\logn}{\mathsf{Hel}_{1/2}^{\min}} \] for certain information metric$\mathsf{Hel}_{1/2}^{\min}$ defined in the main text, as long as the alphabetsize is not super-polynomial in $n$. We apply our general theory to threeconcrete applications, including the stochastic block model, the outlier model,and the haplotype assembly problem. Our theory leads to order-wise tightrecovery conditions for all these scenarios.

Solving Random Quadratic Systems of Equations Is Nearly as Easy as  Solving Linear Systems

  We consider the fundamental problem of solving quadratic systems of equationsin $n$ variables, where $y_i = |\langle \boldsymbol{a}_i, \boldsymbol{x}\rangle|^2$, $i = 1, \ldots, m$ and $\boldsymbol{x} \in \mathbb{R}^n$ isunknown. We propose a novel method, which starting with an initial guesscomputed by means of a spectral method, proceeds by minimizing a nonconvexfunctional as in the Wirtinger flow approach. There are several keydistinguishing features, most notably, a distinct objective functional andnovel update rules, which operate in an adaptive fashion and drop terms bearingtoo much influence on the search direction. These careful selection rulesprovide a tighter initial guess, better descent directions, and thus enhancedpractical performance. On the theoretical side, we prove that for certainunstructured models of quadratic systems, our algorithms return the correctsolution in linear time, i.e. in time proportional to reading the data$\{\boldsymbol{a}_i\}$ and $\{y_i\}$ as soon as the ratio $m/n$ between thenumber of equations and unknowns exceeds a fixed numerical constant. We extendthe theory to deal with noisy systems in which we only have $y_i \approx|\langle \boldsymbol{a}_i, \boldsymbol{x} \rangle|^2$ and prove that ouralgorithms achieve a statistical accuracy, which is nearly un-improvable. Wecomplement our theoretical study with numerical examples showing that solvingrandom quadratic systems is both computationally and statistically not muchharder than solving linear systems of the same size---hence the title of thispaper. For instance, we demonstrate empirically that the computational cost ofour algorithm is about four times that of solving a least-squares problem ofthe same size.

Asymmetry Helps: Eigenvalue and Eigenvector Analyses of Asymmetrically  Perturbed Low-Rank Matrices

  This paper is concerned with a curious phenomenon in spectral estimation.Suppose we are interested in a rank-1 and symmetric matrix$\boldsymbol{M}^{\star}\in \mathbb{R}^{n\times n}$, yet only a randomlyperturbed version $\boldsymbol{M}$ is observed. The perturbation/noise matrix$\boldsymbol{M}-\boldsymbol{M}^{\star}$ is composed of independent andzero-mean entries and is not symmetric. This might arise, for example, when wehave two independent samples for each entry of $\boldsymbol{M}^{\star}$ andarrange them into an $\mathit{asymmetric}$ data matrix $\boldsymbol{M}$. Theaim is to estimate the leading eigenvalue and eigenvector of$\boldsymbol{M}^{\star}$. Somewhat unexpectedly, our findings reveal that theleading eigenvalue of the data matrix $\boldsymbol{M}$ can be $\sqrt{n}$ timesmore accurate than its leading singular value in eigenvalue estimation.Further, the perturbation of any linear form of the leading eigenvector of$\boldsymbol{M}$ (e.g. entrywise eigenvector perturbation) is provablywell-controlled. We further provide partial theory for the more generalrank-$r$ case; this allows us to accommodate the case when$\boldsymbol{M}^{\star}$ is rank-1 but asymmetric, by consideringeigen-decomposition of the associated rank-2 dilation matrix. The takeawaymessage is this: arranging the data samples in an asymmetric manner andperforming eigen-decomposition (as opposed to SVD) could sometimes be quitebeneficial.

Near Optimal Bayesian Active Learning for Decision Making

  How should we gather information to make effective decisions? We addressBayesian active learning and experimental design problems, where wesequentially select tests to reduce uncertainty about a set of hypotheses.Instead of minimizing uncertainty per se, we consider a set of overlappingdecision regions of these hypotheses. Our goal is to drive uncertainty into asingle decision region as quickly as possible.  We identify necessary and sufficient conditions for correctly identifying adecision region that contains all hypotheses consistent with observations. Wedevelop a novel Hyperedge Cutting (HEC) algorithm for this problem, and provethat is competitive with the intractable optimal policy. Our efficientimplementation of the algorithm relies on computing subsets of the completehomogeneous symmetric polynomials. Finally, we demonstrate its effectiveness ontwo practical applications: approximate comparison-based learning and activelocalization using a robot manipulator.

Accelerated dimension-independent adaptive Metropolis

  This work considers black-box Bayesian inference over high-dimensionalparameter spaces. The well-known adaptive Metropolis (AM) algorithm of (Haarioetal. 2001) is extended herein to scale asymptotically uniformly with respectto the underlying parameter dimension for Gaussian targets, by respecting thevariance of the target. The resulting algorithm, referred to as thedimension-independent adaptive Metropolis (DIAM) algorithm, also shows improvedperformance with respect to adaptive Metropolis on non-Gaussian targets. Thisalgorithm is further improved, and the possibility of probing high-dimensionaltargets is enabled, via GPU-accelerated numerical libraries and periodicallysynchronized concurrent chains (justified a posteriori). Asymptotically indimension, this GPU implementation exhibits a factor of four improvement versusa competitive CPU-based Intel MKL parallel version alone. Strong scaling toconcurrent chains is exhibited, through a combination of longer time per samplebatch (weak scaling) and yet fewer necessary samples to convergence. Thealgorithm performance is illustrated on several Gaussian and non-Gaussiantarget examples, in which the dimension may be in excess of one thousand.

Nonconvex Matrix Factorization from Rank-One Measurements

  We consider the problem of recovering low-rank matrices from random rank-onemeasurements, which spans numerous applications including covariance sketching,phase retrieval, quantum state tomography, and learning shallow polynomialneural networks, among others. Our approach is to directly estimate thelow-rank factor by minimizing a nonconvex quadratic loss function via vanillagradient descent, following a tailored spectral initialization. When the truerank is small, this algorithm is guaranteed to converge to the ground truth (upto global ambiguity) with near-optimal sample complexity and computationalcomplexity. To the best of our knowledge, this is the first guarantee thatachieves near-optimality in both metrics. In particular, the key enabler ofnear-optimal computational guarantees is an implicit regularization phenomenon:without explicit regularization, both spectral initialization and the gradientdescent iterates automatically stay within a region incoherent with themeasurement vectors. This feature allows one to employ much more aggressivestep sizes compared with the ones suggested in prior literature, without theneed of sample splitting.

Teaching Categories to Human Learners with Visual Explanations

  We study the problem of computer-assisted teaching with explanations.Conventional approaches for machine teaching typically only provide feedback atthe instance level e.g., the category or label of the instance. However, it isintuitive that clear explanations from a knowledgeable teacher cansignificantly improve a student's ability to learn a new concept. To addressthese existing limitations, we propose a teaching framework that providesinterpretable explanations as feedback and models how the learner incorporatesthis additional information. In the case of images, we show that we canautomatically generate explanations that highlight the parts of the image thatare responsible for the class label. Experiments on human learners illustratethat, on average, participants achieve better test set performance onchallenging categorization tasks when taught with our interpretable approachcompared to existing methods.

Gradient Descent with Random Initialization: Fast Global Convergence for  Nonconvex Phase Retrieval

  This paper considers the problem of solving systems of quadratic equations,namely, recovering an object of interest$\mathbf{x}^{\natural}\in\mathbb{R}^{n}$ from $m$ quadratic equations/samples$y_{i}=(\mathbf{a}_{i}^{\top}\mathbf{x}^{\natural})^{2}$, $1\leq i\leq m$. Thisproblem, also dubbed as phase retrieval, spans multiple domains includingphysical sciences and machine learning.  We investigate the efficiency of gradient descent (or Wirtinger flow)designed for the nonconvex least squares problem. We prove that under Gaussiandesigns, gradient descent --- when randomly initialized --- yields an$\epsilon$-accurate solution in $O\big(\log n+\log(1/\epsilon)\big)$ iterationsgiven nearly minimal samples, thus achieving near-optimal computational andsample complexities at once. This provides the first global convergenceguarantee concerning vanilla gradient descent for phase retrieval, without theneed of (i) carefully-designed initialization, (ii) sample splitting, or (iii)sophisticated saddle-point escaping schemes. All of these are achieved byexploiting the statistical models in analyzing optimization algorithms, via aleave-one-out approach that enables the decoupling of certain statisticaldependency between the gradient descent iterates and the data.

Barrier Certificates for Assured Machine Teaching

  Machine teaching has received significant attention in the past few years asa paradigm shift from machine learning. While machine learning is oftenconcerned with improving the performance of learners, machine teaching pertainsto the efficiency of teachers. For example, machine teaching seeks to find theoptimal (minimum) number of data samples needed for teaching a targethypothesis to a learner. Hence, it is natural to raise the question of how canwe provide assurances for teaching given a machine teaching algorithm. In thispaper, we address this question by borrowing notions from control theory. Webegin by proposing a model based on partially observable Markov decisionprocesses (POMDPs) for a class of machine teaching problems. We then show thatthe POMDP formulation can be cast as a special hybrid system, i.e., adiscrete-time switched system. Subsequently, we use barrier certificates toverify properties of this special hybrid system. We show how the computation ofthe barrier certificate can be decomposed and numerically implemented as thesolution to a sum-of-squares (SOS) program. For illustration, we show how theproposed framework based on control theory can be used to verify the teachingperformance of two well-known machine teaching methods.

Adversarial WiFi Sensing using a Single Smartphone

  Wireless devices are everywhere, at home, at the office, and on the street.Devices are bombarding us with transmissions across a wide range of RFfrequencies. Many of these invisible transmissions reflect off our bodies,carrying off information about ou location, movement, and other physiologicalproperties. While a boon to professionals with carefully calibratedinstruments, they may also be revealing private data about us to potentialattackers nearby.  In this paper, we examine the problem of adversarial WiFi sensing, andconsider whether ambient WiFi signals around us pose real risks to our personalprivacy. We identify a passive adversarial sensing attack, where bad actorsusing a single smartphone can silently localize and track individuals in theirhome or office from outside walls, by just listening to ambient WiFi signals.We experimentally validate this attack in 11 real-world locations, and showuser tracking with high accuracy. Finally, we propose and evaluate defensesincluding geo-fencing, rate limiting, and signal obfuscation by WiFi accesspoints.

Pixel Level Data Augmentation for Semantic Image Segmentation using  Generative Adversarial Networks

  Semantic segmentation is one of the basic topics in computer vision, it aimsto assign semantic labels to every pixel of an image. Unbalanced semantic labeldistribution could have a negative influence on segmentation accuracy. In thispaper, we investigate using data augmentation approach to balance the semanticlabel distribution in order to improve segmentation performance. We proposeusing generative adversarial networks (GANs) to generate realistic images forimproving the performance of semantic segmentation networks. Experimentalresults show that the proposed method can not only improve segmentationperformance on those classes with low accuracy, but also obtain 1.3% to 2.1%increase in average segmentation accuracy. It shows that this augmentationmethod can boost accuracy and be easily applicable to any other segmentationmodels.

A General Framework for Multi-fidelity Bayesian Optimization with  Gaussian Processes

  How can we efficiently gather information to optimize an unknown function,when presented with multiple, mutually dependent information sources withdifferent costs? For example, when optimizing a robotic system, intelligentlytrading off computer simulations and real robot testings can lead tosignificant savings. Existing methods, such as multi-fidelity GP-UCB or EntropySearch-based approaches, either make simplistic assumptions on the interactionamong different fidelities or use simple heuristics that lack theoreticalguarantees. In this paper, we study multi-fidelity Bayesian optimization withcomplex structural dependencies among multiple outputs, and proposeMF-MI-Greedy, a principled algorithmic framework for addressing this problem.In particular, we model different fidelities using additive Gaussian processesbased on shared latent structures with the target function. Then we usecost-sensitive mutual information gain for efficient Bayesian globaloptimization. We propose a simple notion of regret which incorporates the costof different fidelities, and prove that MF-MI-Greedy achieves low regret. Wedemonstrate the strong empirical performance of our algorithm on both syntheticand real-world datasets.

Optimizing Photonic Nanostructures via Multi-fidelity Gaussian Processes

  We apply numerical methods in combination with finite-difference-time-domain(FDTD) simulations to optimize transmission properties of plasmonic mirrorcolor filters using a multi-objective figure of merit over a five-dimensionalparameter space by utilizing novel multi-fidelity Gaussian processes approach.We compare these results with conventional derivative-free global searchalgorithms, such as (single-fidelity) Gaussian Processes optimization scheme,and Particle Swarm Optimization---a commonly used method in nanophotonicscommunity, which is implemented in Lumerical commercial photonics software. Wedemonstrate the performance of various numerical optimization approaches onseveral pre-collected real-world datasets and show that by properly trading offexpensive information sources with cheap simulations, one can more effectivelyoptimize the transmission properties with a fixed budget.

Trip Prediction by Leveraging Trip Histories from Neighboring Users

  We propose a novel approach for trip prediction by analyzing user's triphistories. We augment users' (self-) trip histories by adding 'similar' tripsfrom other users, which could be informative and useful for predicting futuretrips for a given user. This also helps to cope with noisy or sparse triphistories, where the self-history by itself does not provide a reliableprediction of future trips. We show empirical evidence that by enriching theusers' trip histories with additional trips, one can improve the predictionerror by 15%-40%, evaluated on multiple subsets of the Nancy2012 dataset. Thisreal-world dataset is collected from public transportation ticket validationsin the city of Nancy, France. Our prediction tool is a central component of atrip simulator system designed to analyze the functionality of publictransportation in the city of Nancy.

A One-Class Support Vector Machine Calibration Method for Time Series  Change Point Detection

  It is important to identify the change point of a system's health status,which usually signifies an incipient fault under development. The One-ClassSupport Vector Machine (OC-SVM) is a popular machine learning model for anomalydetection and hence could be used for identifying change points; however, it issometimes difficult to obtain a good OC-SVM model that can be used on sensormeasurement time series to identify the change points in system health status.In this paper, we propose a novel approach for calibrating OC-SVM models. Theapproach uses a heuristic search method to find a good set of input data andhyperparameters that yield a well-performing model. Our results on the C-MAPSSdataset demonstrate that OC-SVM can also achieve satisfactory accuracy indetecting change point in time series with fewer training data, compared tostate-of-the-art deep learning approaches. In our case study, the OC-SVMcalibrated by the proposed model is shown to be useful especially in scenarioswith limited amount of training data.

AED-Net: An Abnormal Event Detection Network

  It is challenging to detect the anomaly in crowded scenes for quite a longtime. In this paper, a self-supervised framework, abnormal event detectionnetwork (AED-Net), which is composed of PCAnet and kernel principal componentanalysis (kPCA), is proposed to address this problem. Using surveillance videosequences of different scenes as raw data, PCAnet is trained to extracthigh-level semantics of crowd's situation. Next, kPCA,a one-class classifier,is trained to determine anomaly of the scene. In contrast to some prevailingdeep learning methods,the framework is completely self-supervised because itutilizes only video sequences in a normal situation. Experiments of global andlocal abnormal event detection are carried out on UMN and UCSD datasets, andcompetitive results with higher EER and AUC compared to other state-of-the-artmethods are observed. Furthermore, by adding local response normalization (LRN)layer, we propose an improvement to original AED-Net. And it is proved toperform better by promoting the framework's generalization capacity accordingto the experiments.

Cloud structure of three Galactic infrared dark star-forming regions  from combining ground and space based bolometric observations

  We have modified the iterative procedure introduced by Lin et al. (2016), tosystematically combine the submm images taken from ground based (e.g., CSO,JCMT, APEX) and space (e.g., Herschel, Planck) telescopes. We applied theupdated procedure to observations of three well studied Infrared Dark Clouds(IRDCs): G11.11-0.12, G14.225-0.506 and G28.34+0.06, and then performedsingle-component, modified black-body fits to derive $\sim$10$"$ resolutiondust temperature and column density maps. The derived column density maps showthat these three IRDCs exhibit complex filamentary structures embedding withrich clumps/cores. We compared the column density probability distributionfunctions (N-PDFs) and two-point correlation (2PT) functions of the columndensity field between these IRDCs with several OB cluster-forming regions.Based on the observed correlation and measurements, and complementaryhydrodynamical simulations for a 10$^{4}$ $\rm M_{\odot}$ molecular cloud, wehypothesize that cloud evolution can be better characterized by the evolutionof the (column) density distribution function and the relative power of densestructures as a function of spatial scales, rather than merely based on thepresence of star-forming activity. Based on the small analyzed sample, wepropose four evolutionary stages, namely: {\it cloud integration, stellarassembly, cloud pre-dispersal and dispersed-cloud.} The initial {\it cloudintegration} stage and the final {\it dispersed cloud} stage may bedistinguished from the two intermediate stages by a steeper than $-$4 power-lawindex of the N-PDF. The {\it cloud integration} stage and the subsequent {\itstellar assembly} stage are further distinguished from each other by the largerluminosity-to-mass ratio ($>$40 $\rm L_{\odot}/M_{\odot}$) of the latter.

Evolution of Iron K$_α$ Line Emission in the Black Hole Candidate  GX 339-4

  GX 339-4 was regularly monitored with RXTE during a period (in 1999) when itsX-ray flux decreased significantly (from 4.2$\times 10^{-10}$ erg cm$^{-2}s^{-1}$ to 7.6$\times 10^{-12}$ erg cm$^{-2}$s$^{-1}$ in the 3--20 keV band),as the source settled into the ``off state''. Our spectral analysis revealedthe presence of a prominent iron K$_{\alpha}$ line in the observed spectrum ofthe source for all observations. The line shows an interesting evolution: it iscentered at $\sim$6.4 keV when the measured flux is above 5$\times 10^{-11}$erg cm$^{-2} s^{-1}$, but is shifted to $\sim$6.7 keV at lower fluxes. Theequivalent width of the line appears to increase significantly toward lowerfluxes, although it is likely to be sensitive to calibration uncertainties.While the fluorescent emission of neutral or mildly ionized iron atoms in theaccretion disk can perhaps account for the 6.4 keV line, as is often invokedfor black hole candidates, it seems difficult to understand the 6.7 keV linewith this mechanism, because the disk should be less ionized at lower fluxes(unless its density changes drastically). On the other hand, the 6.7 keV linemight be due to recombination cascade of hydrogen or helium like iron ions inan optically thin, highly ionized plasma. We discuss the results in the contextof proposed accretion models.

Collective behaviour of large number of vortices in the plane

  We investigate the dynamics of $N$ point vortices in the plane, in the limitof large $N$. We consider {\em relative equilibria}, which are rigidly rotatinglattice-like configurations of vortices. These configurations were observed inseveral recent experiments [Durkin and Fajans, Phys. Fluids (2000) 12, 289-293;Grzybowski {\em et.al} PRE (2001)64, 011603]. We show that these solutions andtheir stability are fully characterized via a related {\em aggregation model}which was recently investigated in the context of biological swarms [Fetecau{\em et.al.}, Nonlinearity (2011) 2681; Bertozzi {\em et.al.}, M3AS (2011)]. Byutilizing this connection, we give explicit analytic formulae for many of theconfigurations that have been observed experimentally. These includeconfigurations of vortices of equal strength; the $N+1$ configurations of $N$vortices of equal strength and one vortex of much higher strength; and moregenerally, $N+K$ configurations. We also give examples of configurations thathave not been studied experimentally, including $N+2$ configurations where $N$vortices aggregate inside an ellipse. Finally, we introduce an artificial``damping'' to the vortex dynamics, in an attempt to explain the phenomenon ofcrystalization that is often observed in real experiments. The diffusion breaksthe conservative structure of vortex dynamics so that any initial conditionsconverge to the lattice-like relative equilibrium.

A minimal model of predator-swarm interactions

  We propose a minimal model of predator-swarm interactions which captures manyof the essential dynamics observed in nature. Different outcomes are observeddepending on the predator strength. For a "weak" predator, the swarm is able toescape the predator completely. As the strength is increased, the predator isable to catch up with the swarm as a whole, but the individual prey are able toescape by "confusing" the predator: the prey forms a ring with the predator atthe center. For higher predator strength, complex chasing dynamics are observedwhich can become chaotic. For even higher strength, the predator is able tosuccessfully capture the prey. Our model is simple enough to be amenable to afull mathematical analysis which is used to predict the shape of the swarm aswell as the resulting predator-prey dynamics as a function of model parameters.We show that as the predator strength is increased, there is a transition (dueto a Hopf bifurcation) from confusion state to chasing dynamics, and we computethe threshold analytically. Our analysis indicates that the swarming behaviouris not helpful in avoiding the predator, suggesting that there are otherreasons why the species may swarm. The complex shape of the swarm in our modelduring the chasing dynamics is similar to the shape of a flock of sheepavoiding a shepherd.

Assessing the robustness of spatial pattern sequences in a dryland  vegetation model

  A particular sequence of patterns, "$\text{gaps} \to \text{labyrinth} \to\text{spots}$," occurs with decreasing precipitation in previously reportednumerical simulations of PDE dryland vegetation models. These observations haveled to the suggestion that this sequence of patterns can serve as an earlyindicator of desertification in some ecosystems. Since parameter values cantake on a range of plausible values in the vegetation models, it is importantto investigate whether the pattern sequence prediction is robust to variation.For a particular model, we find that a quantity calculated viabifurcation-theoretic analysis appears to serve as a proxy for the patternsequences that occur in numerical simulations across a range of parametervalues. We find in further analysis that the quantity takes on valuesconsistent with the standard sequence in an ecologically relevant limit of themodel parameter values. This suggests that the standard sequence is a robustprediction of the model, and we conclude by proposing a methodology forassessing the robustness of the standard sequence in other models andformulations.

Implicit Regularization in Nonconvex Statistical Estimation: Gradient  Descent Converges Linearly for Phase Retrieval, Matrix Completion and Blind  Deconvolution

  Recent years have seen a flurry of activities in designing provably efficientnonconvex procedures for solving statistical estimation problems. Due to thehighly nonconvex nature of the empirical loss, state-of-the-art proceduresoften require proper regularization (e.g. trimming, regularized cost,projection) in order to guarantee fast convergence. For vanilla procedures suchas gradient descent, however, prior theory either recommends highlyconservative learning rates to avoid overshooting, or completely lacksperformance guarantees.  This paper uncovers a striking phenomenon in nonconvex optimization: even inthe absence of explicit regularization, gradient descent enforces properregularization implicitly under various statistical models. In fact, gradientdescent follows a trajectory staying within a basin that enjoys nice geometry,consisting of points incoherent with the sampling mechanism. This "implicitregularization" feature allows gradient descent to proceed in a far moreaggressive fashion without overshooting, which in turn results in substantialcomputational savings. Focusing on three fundamental statistical estimationproblems, i.e. phase retrieval, low-rank matrix completion, and blinddeconvolution, we establish that gradient descent achieves near-optimalstatistical and computational guarantees without explicit regularization. Inparticular, by marrying statistical modeling with generic optimization theory,we develop a general recipe for analyzing the trajectories of iterativealgorithms via a leave-one-out perturbation argument. As a byproduct, for noisymatrix completion, we demonstrate that gradient descent achieves near-optimalerror control --- measured entrywise and by the spectral norm --- which mightbe of independent interest.

Noisy Matrix Completion: Understanding Statistical Guarantees for Convex  Relaxation via Nonconvex Optimization

  This paper studies noisy low-rank matrix completion: given partial andcorrupted entries of a large low-rank matrix, the goal is to estimate theunderlying matrix faithfully and efficiently. Arguably one of the most popularparadigms to tackle this problem is convex relaxation, which achievesremarkable efficacy in practice. However, the theoretical support of thisapproach is still far from optimal in the noisy setting, falling short ofexplaining the empirical success.  We make progress towards demystifying the practical efficacy of convexrelaxation vis-\`a-vis random noise. When the rank of the unknown matrix is aconstant, we demonstrate that the convex programming approach achievesnear-optimal estimation errors --- in terms of the Euclidean loss, theentrywise loss, and the spectral norm loss --- for a wide range of noiselevels. All of this is enabled by bridging convex relaxation with the nonconvexBurer-Monteiro approach, a seemingly distinct algorithmic paradigm that isprovably robust against noise. More specifically, we show that an approximatecritical point of the nonconvex formulation serves as an extremely tightapproximation of the convex solution, allowing us to transfer the desiredstatistical guarantees of the nonconvex approach to its convex counterpart.

Filamentary Accretion Flows in the Infrared Dark Cloud G14.225-0.506  Revealed by ALMA

  Filaments are ubiquitous structures in molecular clouds and play an importantrole in the mass assembly of stars. We present results of dynamical stabilityanalyses for filaments in the infrared dark cloud G14.225$-$0.506, where adelayed onset of massive star formation was reported in the two hubs at theconvergence of multiple filaments of parsec length. Full-synthesis imaging isperformed with the Atacama Large Millimeter/submillimeter Array (ALMA) to mapthe $\mathrm{N_2H^+} \; (1-0)$ emission in two hub-filament systems with aspatial resolution of $\sim 0.034 \; \mathrm{pc}$. Kinematics are derived fromsophisticated spectral fitting algorithm that accounts for line blending, largeoptical depth, and multiple velocity components. We identify five velocitycoherent filaments and derive their velocity gradients with principal componentanalysis. The mass accretion rates along the filaments are up to $10^{-4} \;\mathrm{M_\odot \, \mathrm{yr^{-1}}}$ and are significant enough to affect thehub dynamics within one free-fall time ($\sim 10^5 \; \mathrm{yr}$). The$\mathrm{N_2H^+}$ filaments are in equilibrium with virial parameter$\alpha_\mathrm{vir} \sim 1.2$. We compare $\alpha_\mathrm{vir}$ measured inthe $\mathrm{N_2H^+}$ filaments, $\mathrm{NH_3}$ filaments, $870 \;\mu\mathrm{m}$ dense clumps, and $3 \; \mathrm{mm}$ dense cores. The decreasingtrend in $\alpha_\mathrm{vir}$ with decreasing spatial scales persists,suggesting an increasingly important role of gravity at small scales.Meanwhile, $\alpha_\mathrm{vir}$ also decreases with decreasing non-thermalmotions. In combination with the absence of high-mass protostars and massivecores, our results are consistent with the global hierarchical collapsescenario.

A Luminous Peculiar Type Ia Supernova SN 2011hr: More Like SN 1991T or  SN 2007if?

  Photometric and spectroscopic observations of a slowly declining, luminousType Ia supernova (SN Ia) SN 2011hr in the starburst galaxy NGC 2691 arepresented. SN 2011hr is found to peak at $M_{B}=-19.84 \pm 0.40\,\rm{mag}$,with a post-maximum decline rate $\Delta$m$_{15}$(B) = 0.92 $\pm$0.03\,$\rm{mag}$. From the maximum-light bolometric luminosity, $L=(2.30 \pm0.90) \times 10^{43}\,\rm{erg\,s^{-1}}$, we estimate the mass of synthesized\Nifs\ in SN 2011hr to be $M(\rm{^{56}Ni})=1.11 \pm 0.43\,M_{\sun}$. SN 2011hrappears more luminous than SN 1991T at around maximum light, and the absorptionfeatures from its intermediate-mass elements (IMEs) are noticeably weaker thanthe latter at similar phases. Spectral modeling suggests that SN 2011hr has theIMEs of $\sim$\,0.07 M$_{\sun}$ in the outer ejecta, which is much lower thanthe typical value of normal SNe Ia (i.e., 0.3 -- 0.4 M$_{\sun}$) and is alsolower than the value of SN 1991T (i.e., $\sim$\,0.18 M$_{\sun}$). These resultsindicate that SN 2011hr may arise from a Chandrasekhar-mass white dwarfprogenitor that experienced a more efficient burning process in the explosion.Nevertheless, it is still possible that SN 2011hr may serve as a transitionalobject connecting the SN 1991T-like SNe Ia with the superluminous subclass likeSN 2007if given that the latter also shows very weak IMEs at all phases.

The Likelihood Ratio Test in High-Dimensional Logistic Regression Is  Asymptotically a Rescaled Chi-Square

  Logistic regression is used thousands of times a day to fit data, predictfuture outcomes, and assess the statistical significance of explanatoryvariables. When used for the purpose of statistical inference, logistic modelsproduce p-values for the regression coefficients by using an approximation tothe distribution of the likelihood-ratio test. Indeed, Wilks' theorem assertsthat whenever we have a fixed number $p$ of variables, twice the log-likelihoodratio (LLR) $2\Lambda$ is distributed as a $\chi^2_k$ variable in the limit oflarge sample sizes $n$; here, $k$ is the number of variables being tested. Inthis paper, we prove that when $p$ is not negligible compared to $n$, Wilks'theorem does not hold and that the chi-square approximation is grosslyincorrect; in fact, this approximation produces p-values that are far too small(under the null hypothesis). Assume that $n$ and $p$ grow large in such a waythat $p/n\rightarrow\kappa$ for some constant $\kappa < 1/2$. We prove that fora class of logistic models, the LLR converges to a rescaled chi-square, namely,$2\Lambda~\stackrel{\mathrm{d}}{\rightarrow}~\alpha(\kappa)\chi_k^2$, where thescaling factor $\alpha(\kappa)$ is greater than one as soon as thedimensionality ratio $\kappa$ is positive. Hence, the LLR is larger thanclassically assumed. For instance, when $\kappa=0.3$,$\alpha(\kappa)\approx1.5$. In general, we show how to compute the scalingfactor by solving a nonlinear system of two equations with two unknowns. Ourmathematical arguments are involved and use techniques from approximate messagepassing theory, non-asymptotic random matrix theory and convex geometry. Wealso complement our mathematical study by showing that the new limitingdistribution is accurate for finite sample sizes. Finally, all the results fromthis paper extend to some other regression models such as the probit regressionmodel.

