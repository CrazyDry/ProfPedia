Constraint Satisfaction Propagation: Non-stationary Policy Synthesis for  Temporal Logic Planning

  Problems arise when using reward functions to capture dependencies betweensequential time-constrained goal states because the state-space must beprohibitively expanded to accommodate a history of successfully achievedsub-goals. Also, policies and value functions derived with stationarityassumptions are not readily decomposable, leading to a tension between rewardmaximization and task generalization. We demonstrate a logic-compatibleapproach using model-based knowledge of environment dynamics and deadlineinformation to directly infer non-stationary policies composed of reusablestationary policies. The policies are constructed to maximize the probabilityof satisfying time-sensitive goals while respecting time-varying obstacles. Ourapproach explicitly maintains two different spaces, a high-level logical taskspecification where the task-variables are grounded onto the low-levelstate-space of a Markov decision process. Computing satisfiability at thetask-level is made possible by a Bellman-like equation which operates on atensor that links the temporal relationship between the two spaces; theequation solves for a value function that can be explicitly interpreted as theprobability of sub-goal satisfaction under the synthesized non-stationarypolicy, an approach we term Constraint Satisfaction Propagation (CSP).

