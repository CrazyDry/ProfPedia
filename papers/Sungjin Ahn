Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring

  In this paper we address the following question: Can we approximately samplefrom a Bayesian posterior distribution if we are only allowed to touch a smallmini-batch of data-items for every sample we generate?. An algorithm based onthe Langevin equation with stochastic gradients (SGLD) was previously proposedto solve this, but its mixing rate was slow. By leveraging the Bayesian CentralLimit Theorem, we extend the SGLD algorithm so that at high mixing rates itwill sample from a normal approximation of the posterior, while for slow mixingrates it will mimic the behavior of SGLD with a pre-conditioner matrix. As abonus, the proposed algorithm is reminiscent of Fisher scoring (with stochasticgradients) and as such an efficient optimizer during burn-in.

Large-Scale Distributed Bayesian Matrix Factorization using Stochastic  Gradient MCMC

  Despite having various attractive qualities such as high prediction accuracyand the ability to quantify uncertainty and avoid over-fitting, Bayesian MatrixFactorization has not been widely adopted because of the prohibitive cost ofinference. In this paper, we propose a scalable distributed Bayesian matrixfactorization algorithm using stochastic gradient MCMC. Our algorithm, based onDistributed Stochastic Gradient Langevin Dynamics, can not only match theprediction accuracy of standard MCMC methods like Gibbs sampling, but at thesame time is as fast and simple as stochastic gradient descent. In ourexperiments, we show that our algorithm can achieve the same level ofprediction accuracy as Gibbs sampling an order of magnitude faster. We alsoshow that our method reduces the prediction error as fast as distributedstochastic gradient descent, achieving a 4.1% improvement in RMSE for theNetflix dataset and an 1.8% for the Yahoo music dataset.

A Neural Knowledge Language Model

  Current language models have a significant limitation in the ability toencode and decode factual knowledge. This is mainly because they acquire suchknowledge from statistical co-occurrences although most of the knowledge wordsare rarely observed. In this paper, we propose a Neural Knowledge LanguageModel (NKLM) which combines symbolic knowledge provided by the knowledge graphwith the RNN language model. By predicting whether the word to generate has anunderlying fact or not, the model can generate such knowledge-related words bycopying from the description of the predicted fact. In experiments, we showthat the NKLM significantly improves the performance while generating a muchsmaller number of unknown words.

Scalable MCMC for Mixed Membership Stochastic Blockmodels

  We propose a stochastic gradient Markov chain Monte Carlo (SG-MCMC) algorithmfor scalable inference in mixed-membership stochastic blockmodels (MMSB). Ouralgorithm is based on the stochastic gradient Riemannian Langevin sampler andachieves both faster speed and higher accuracy at every iteration than thecurrent state-of-the-art algorithm based on stochastic variational inference.In addition we develop an approximation that can handle models that entertain avery large number of communities. The experimental results show that SG-MCMCstrictly dominates competing algorithms in all cases.

Interlayer correlation between two $^4$He monolayers adsorbed on both  sides of $Î±$-graphyne

  Path-integral Monte Carlo calculations have been performed to study the$^4$He adsorption on both sides of a single $\alpha$-graphyne sheet. Forinvestigation of the interlayer correlation between the upper and the lowermonolayer of $^4$He adatoms, the $^4$He-substrate interaction is described bythe sum of the $^4$He-C interatomic pair potentials, for which we use bothLennard-Jones and Yukawa-6 anisotropic potentials. When the lower $^4$He layeris a C$_{4/3}$ commensurate solid, the upper-layer $^4$He atoms are found toform a Kagom\'e lattice structure at a Mott insulating density of 0.0706\AA$^{-2}$, and a commensurate solid at an areal density of 0.0941 \AA$^{-2}$for both substrate potentials. The correlation between upper- and lower-layerpseudospins, which were introduced in Ref. [1] for two degenerateconfigurations of three $^4$He atoms in a hexagonal cell, depends on thesubstrate potential used; With the substrate potential based on the anisotropicYukawa-6 pair potentials, the Ising pseudo-spins of both $^4$He layers arefound to be anti-parallel to each other while the parallel and anti-parallelpseudo-spin alignments between the two $^4$He layers are nearly degenerate withthe Lennard-Jones potentials. This is attributed to the difference in theinterlayer distance, which is $\sim 4$ \AA~ with the Yukawa-6 substratepotential but as large as $\sim 4.8$ \AA~with the Lennard-Jones potential.  [1] Y. Kwon, H. Shin, and H. Lee, Phys. Rev. B 88, 201403(R) (2013)

Denoising Criterion for Variational Auto-Encoding Framework

  Denoising autoencoders (DAE) are trained to reconstruct their clean inputswith noise injected at the input level, while variational autoencoders (VAE)are trained with noise injected in their stochastic hidden layer, with aregularizer that encourages this noise injection. In this paper, we show thatinjecting noise both in input and in the stochastic hidden layer can beadvantageous and we propose a modified variational lower bound as an improvedobjective function in this setup. When input is corrupted, then the standardVAE lower bound involves marginalizing the encoder conditional distributionover the input noise, which makes the training criterion intractable. Instead,we propose a modified training criterion which corresponds to a tractable boundwhen input is corrupted. Experimentally, we find that the proposed denoisingvariational autoencoder (DVAE) yields better average log-likelihood than theVAE and the importance weighted autoencoder on the MNIST and Frey Facedatasets.

Generating Factoid Questions With Recurrent Neural Networks: The 30M  Factoid Question-Answer Corpus

  Over the past decade, large-scale supervised learning corpora have enabledmachine learning researchers to make substantial advances. However, to thisdate, there are no large-scale question-answer corpora available. In this paperwe present the 30M Factoid Question-Answer Corpus, an enormous question answerpair corpus produced by applying a novel neural network architecture on theknowledge base Freebase to transduce facts into natural language questions. Theproduced question answer pairs are evaluated both by human evaluators and usingautomatic evaluation metrics, including well-established machine translationand sentence similarity metrics. Across all evaluation criteria thequestion-generation model outperforms the competing template-based baseline.Furthermore, when presented to human evaluators, the generated questions appearcomparable in quality to real human-generated questions.

Pointing the Unknown Words

  The problem of rare and unknown words is an important issue that canpotentially influence the performance of many NLP systems, including both thetraditional count-based and the deep learning models. We propose a novel way todeal with the rare and unseen words for the neural network models usingattention. Our model uses two softmax layers in order to predict the next wordin conditional language models: one predicts the location of a word in thesource sentence, and the other predicts a word in the shortlist vocabulary. Ateach time-step, the decision of which softmax layer to use choose adaptivelymade by an MLP which is conditioned on the context.~We motivate our work from apsychological evidence that humans naturally have a tendency to point towardsobjects in the context or the environment when the name of an object is notknown.~We observe improvements on two tasks, neural machine translation on theEuroparl English to French parallel corpora and text summarization on theGigaword dataset using our proposed model.

Hierarchical Memory Networks

  Memory networks are neural networks with an explicit memory component thatcan be both read and written to by the network. The memory is often addressedin a soft way using a softmax function, making end-to-end training withbackpropagation possible. However, this is not computationally scalable forapplications which require the network to read from extremely large memories.On the other hand, it is well known that hard attention mechanisms based onreinforcement learning are challenging to train successfully. In this paper, weexplore a form of hierarchical memory network, which can be considered as ahybrid between hard and soft attention memory networks. The memory is organizedin a hierarchical structure such that reading from it is done with lesscomputation than soft attention over a flat memory, while also being easier totrain than hard attention over a flat memory. Specifically, we propose toincorporate Maximum Inner Product Search (MIPS) in the training and inferenceprocedures for our hierarchical memory network. We explore the use of variousstate-of-the art approximate MIPS techniques and report results onSimpleQuestions, a challenging large scale factoid question answering task.

Hierarchical Multiscale Recurrent Neural Networks

  Learning both hierarchical and temporal representation has been among thelong-standing challenges of recurrent neural networks. Multiscale recurrentneural networks have been considered as a promising approach to resolve thisissue, yet there has been a lack of empirical evidence showing that this typeof models can actually capture the temporal dependencies by discovering thelatent hierarchical structure of the sequence. In this paper, we propose anovel multiscale approach, called the hierarchical multiscale recurrent neuralnetworks, which can capture the latent hierarchical structure in the sequenceby encoding the temporal dependencies with different timescales using a novelupdate mechanism. We show some evidence that our proposed multiscalearchitecture can discover underlying hierarchical structure in the sequenceswithout using explicit boundary information. We evaluate our proposed model oncharacter-level language modelling and handwriting sequence modelling.

Bayesian Model-Agnostic Meta-Learning

  Learning to infer Bayesian posterior from a few-shot dataset is an importantstep towards robust meta-learning due to the model uncertainty inherent in theproblem. In this paper, we propose a novel Bayesian model-agnosticmeta-learning method. The proposed method combines scalable gradient-basedmeta-learning with nonparametric variational inference in a principledprobabilistic framework. During fast adaptation, the method is capable oflearning complex uncertainty structure beyond a point estimate or a simpleGaussian approximation. In addition, a robust Bayesian meta-update mechanismwith a new meta-loss prevents overfitting during meta-update. Remaining anefficient gradient-based meta-learner, the method is also model-agnostic andsimple to implement. Experiment results show the accuracy and robustness of theproposed method in various tasks: sinusoidal regression, image classification,active learning, and reinforcement learning.

Learning Single-View 3D Reconstruction with Adversarial Training

  Single-view 3D shape reconstruction is an important but challenging problem,mainly for two reasons. First, as shape annotation is very expensive toacquire, current methods rely on synthetic data, in which ground-truth 3Dannotation is easy to obtain. However, this results in domain adaptationproblem when applied to natural images. The second challenge is that it existsmultiple shapes that can explain a given 2D image. In this paper, we propose aframework to improve over these challenges using adversarial training. On onehand, we impose domain-confusion between natural and synthetic imagerepresentations to reduce the distribution gap. On the other hand, we imposethe reconstruction to be `realistic' by forcing it to lie on a (learned)manifold of realistic object shapes. Moreover, our experiments show that theseconstraints improve performance by a large margin over a baselinereconstruction model. We achieve results competitive with the state of the artusing only RGB images and with a much simpler architecture.

Reinforced Imitation in Heterogeneous Action Space

  Imitation learning is an effective alternative approach to learn a policywhen the reward function is sparse. In this paper, we consider a challengingsetting where an agent and an expert use different actions from each other. Weassume that the agent has access to a sparse reward function and state-onlyexpert observations. We propose a method which gradually balances between theimitation learning cost and the reinforcement learning objective. In addition,this method adapts the agent's policy based on either mimicking expert behavioror maximizing sparse reward. We show, through navigation scenarios, that (i) anagent is able to efficiently leverage sparse rewards to outperform standardstate-only imitation learning, (ii) it can learn a policy even when its actionsare different from the expert, and (iii) the performance of the agent is notbounded by that of the expert, due to the optimized usage of sparse rewards.

