Detecting influenza outbreaks by analyzing Twitter messages

  We analyze over 500 million Twitter messages from an eight month period and
find that tracking a small number of flu-related keywords allows us to forecast
future influenza rates with high accuracy, obtaining a 95% correlation with
national health statistics. We then analyze the robustness of this approach to
spurious keyword matches, and we propose a document classification component to
filter these misleading messages. We find that this document classifier can
reduce error rates by over half in simulated false alarm experiments, though
more research is needed to develop methods that are robust in cases of
extremely high noise.


Identifying leading indicators of product recalls from online reviews
  using positive unlabeled learning and domain adaptation

  Consumer protection agencies are charged with safeguarding the public from
hazardous products, but the thousands of products under their jurisdiction make
it challenging to identify and respond to consumer complaints quickly. From the
consumer's perspective, online reviews can provide evidence of product defects,
but manually sifting through hundreds of reviews is not always feasible. In
this paper, we propose a system to mine Amazon.com reviews to identify products
that may pose safety or health hazards. Since labeled data for this task are
scarce, our approach combines positive unlabeled learning with domain
adaptation to train a classifier from consumer complaints submitted to the U.S.
Consumer Product Safety Commission. On a validation set of manually annotated
Amazon product reviews, we find that our approach results in an absolute F1
score improvement of 8% over the best competing baseline. Furthermore, we apply
the classifier to Amazon reviews of known recalled products; the classifier
identifies reviews reporting safety hazards prior to the recall date for 45% of
the products. This suggests that the system may be able to provide an early
warning system to alert consumers to hazardous products before an official
recall is announced.


Controlling for Unobserved Confounds in Classification Using
  Correlational Constraints

  As statistical classifiers become integrated into real-world applications, it
is important to consider not only their accuracy but also their robustness to
changes in the data distribution. In this paper, we consider the case where
there is an unobserved confounding variable $z$ that influences both the
features $\mathbf{x}$ and the class variable $y$. When the influence of $z$
changes from training to testing data, we find that the classifier accuracy can
degrade rapidly. In our approach, we assume that we can predict the value of
$z$ at training time with some error. The prediction for $z$ is then fed to
Pearl's back-door adjustment to build our model. Because of the attenuation
bias caused by measurement error in $z$, standard approaches to controlling for
$z$ are ineffective. In response, we propose a method to properly control for
the influence of $z$ by first estimating its relationship with the class
variable $y$, then updating predictions for $z$ to match that estimated
relationship. By adjusting the influence of $z$, we show that we can build a
model that exceeds competing baselines on accuracy as well as on robustness
over a range of confounding relationships.


When do Words Matter? Understanding the Impact of Lexical Choice on
  Audience Perception using Individual Treatment Effect Estimation

  Studies across many disciplines have shown that lexical choice can affect
audience perception. For example, how users describe themselves in a social
media profile can affect their perceived socio-economic status. However, we
lack general methods for estimating the causal effect of lexical choice on the
perception of a specific sentence. While randomized controlled trials may
provide good estimates, they do not scale to the potentially millions of
comparisons necessary to consider all lexical choices. Instead, in this paper,
we first offer two classes of methods to estimate the effect on perception of
changing one word to another in a given sentence. The first class of algorithms
builds upon quasi-experimental designs to estimate individual treatment effects
from observational data. The second class treats treatment effect estimation as
a classification problem. We conduct experiments with three data sources (Yelp,
Twitter, and Airbnb), finding that the algorithmic estimates align well with
those produced by randomized-control trials. Additionally, we find that it is
possible to transfer treatment effect classifiers across domains and still
maintain high accuracy.


Inferring the Origin Locations of Tweets with Quantitative Confidence

  Social Internet content plays an increasingly critical role in many domains,
including public health, disaster management, and politics. However, its
utility is limited by missing geographic information; for example, fewer than
1.6% of Twitter messages (tweets) contain a geotag. We propose a scalable,
content-based approach to estimate the location of tweets using a novel yet
simple variant of gaussian mixture models. Further, because real-world
applications depend on quantified uncertainty for such estimates, we propose
novel metrics of accuracy, precision, and calibration, and we evaluate our
approach accordingly. Experiments on 13 million global, comprehensively
multi-lingual tweets show that our approach yields reliable, well-calibrated
results competitive with previous computationally intensive methods. We also
show that a relatively small number of training data are required for good
estimates (roughly 30,000 tweets) and models are quite time-invariant
(effective on tweets many weeks newer than the training set). Finally, we show
that toponyms and languages with small geographic footprint provide the most
useful location signals.


Mining the Demographics of Political Sentiment from Twitter Using
  Learning from Label Proportions

  Opinion mining and demographic attribute inference have many applications in
social science. In this paper, we propose models to infer daily joint
probabilities of multiple latent attributes from Twitter data, such as
political sentiment and demographic attributes. Since it is costly and
time-consuming to annotate data for traditional supervised classification, we
instead propose scalable Learning from Label Proportions (LLP) models for
demographic and opinion inference using U.S. Census, national and state
political polls, and Cook partisan voting index as population level data. In
LLP classification settings, the training data is divided into a set of
unlabeled bags, where only the label distribution in of each bag is known,
removing the requirement of instance-level annotations. Our proposed LLP model,
Weighted Label Regularization (WLR), provides a scalable generalization of
prior work on label regularization to support weights for samples inside bags,
which is applicable in this setting where bags are arranged hierarchically
(e.g., county-level bags are nested inside of state-level bags). We apply our
model to Twitter data collected in the year leading up to the 2016 U.S.
presidential election, producing estimates of the relationships among political
sentiment and demographics over time and place. We find that our approach
closely tracks traditional polling data stratified by demographic category,
resulting in error reductions of 28-44% over baseline approaches. We also
provide descriptive evaluations showing how the model may be used to estimate
interactions among many variables and to identify linguistic temporal
variation, capabilities which are typically not feasible using traditional
polling methods.


Co-training for Demographic Classification Using Deep Learning from
  Label Proportions

  Deep learning algorithms have recently produced state-of-the-art accuracy in
many classification tasks, but this success is typically dependent on access to
many annotated training examples. For domains without such data, an attractive
alternative is to train models with light, or distant supervision. In this
paper, we introduce a deep neural network for the Learning from Label
Proportion (LLP) setting, in which the training data consist of bags of
unlabeled instances with associated label distributions for each bag. We
introduce a new regularization layer, Batch Averager, that can be appended to
the last layer of any deep neural network to convert it from supervised
learning to LLP. This layer can be implemented readily with existing deep
learning packages. To further support domains in which the data consist of two
conditionally independent feature views (e.g. image and text), we propose a
co-training algorithm that iteratively generates pseudo bags and refits the
deep LLP model to improve classification accuracy. We demonstrate our models on
demographic attribute classification (gender and race/ethnicity), which has
many applications in social media analysis, public health, and marketing. We
conduct experiments to predict demographics of Twitter users based on their
tweets and profile image, without requiring any user-level annotations for
training. We find that the deep LLP approach outperforms baselines for both
text and image features separately. Additionally, we find that co-training
algorithm improves image and text classification by 4% and 8% absolute F1,
respectively. Finally, an ensemble of text and image classifiers further
improves the absolute F1 measure by 4% on average.


Deceptiveness of internet data for disease surveillance

  Quantifying how many people are or will be sick, and where, is a critical
ingredient in reducing the burden of disease because it helps the public health
system plan and implement effective outbreak response. This process of disease
surveillance is currently based on data gathering using clinical and laboratory
methods; this distributed human contact and resulting bureaucratic data
aggregation yield expensive procedures that lag real time by weeks or months.
The promise of new surveillance approaches using internet data, such as web
event logs or social media messages, is to achieve the same goal but faster and
cheaper. However, prior work in this area lacks a rigorous model of information
flow, making it difficult to assess the reliability of both specific approaches
and the body of work as a whole.
  We model disease surveillance as a Shannon communication. This new framework
lets any two disease surveillance approaches be compared using a unified
vocabulary and conceptual model. Using it, we describe and compare the
deficiencies suffered by traditional and internet-based surveillance, introduce
a new risk metric called deceptiveness, and offer mitigations for some of these
deficiencies. This framework also makes the rich tools of information theory
applicable to disease surveillance. This better understanding will improve the
decision-making of public health practitioners by helping to leverage
internet-based surveillance in a way complementary to the strengths of
traditional surveillance.


Forecasting the presence and intensity of hostility on Instagram using
  linguistic and social features

  Online antisocial behavior, such as cyberbullying, harassment, and trolling,
is a widespread problem that threatens free discussion and has negative
physical and mental health consequences for victims and communities. While
prior work has proposed automated methods to identify hostile comments in
online discussions, these methods work retrospectively on comments that have
already been posted, making it difficult to intervene before an interaction
escalates. In this paper we instead consider the problem of forecasting future
hostilities in online discussions, which we decompose into two tasks: (1) given
an initial sequence of non-hostile comments in a discussion, predict whether
some future comment will contain hostility; and (2) given the first hostile
comment in a discussion, predict whether this will lead to an escalation of
hostility in subsequent comments. Thus, we aim to forecast both the presence
and intensity of hostile comments based on linguistic and social features from
earlier comments. To evaluate our approach, we introduce a corpus of over 30K
annotated Instagram comments from over 1,100 posts. Our approach is able to
predict the appearance of a hostile comment on an Instagram post ten or more
hours in the future with an AUC of .82 (task 1), and can furthermore
distinguish between high and low levels of future hostility with an AUC of .91
(task 2).


