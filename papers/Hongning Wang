Explainable Recommendation via Multi-Task Learning in Opinionated Text
  Data

  Explaining automatically generated recommendations allows users to make more
informed and accurate decisions about which results to utilize, and therefore
improves their satisfaction. In this work, we develop a multi-task learning
solution for explainable recommendation. Two companion learning tasks of user
preference modeling for recommendation} and \textit{opinionated content
modeling for explanation are integrated via a joint tensor factorization. As a
result, the algorithm predicts not only a user's preference over a list of
items, i.e., recommendation, but also how the user would appreciate a
particular item at the feature level, i.e., opinionated textual explanation.
Extensive experiments on two large collections of Amazon and Yelp reviews
confirmed the effectiveness of our solution in both recommendation and
explanation tasks, compared with several existing recommendation algorithms.
And our extensive user study clearly demonstrates the practical value of the
explainable recommendations generated by our algorithm.


Learning Contextual Bandits in a Non-stationary Environment

  Multi-armed bandit algorithms have become a reference solution for handling
the explore/exploit dilemma in recommender systems, and many other important
real-world problems, such as display advertisement. However, such algorithms
usually assume a stationary reward distribution, which hardly holds in practice
as users' preferences are dynamic. This inevitably costs a recommender system
consistent suboptimal performance. In this paper, we consider the situation
where the underlying distribution of reward remains unchanged over (possibly
short) epochs and shifts at unknown time instants. In accordance, we propose a
contextual bandit algorithm that detects possible changes of environment based
on its reward estimation confidence and updates its arm selection strategy
respectively. Rigorous upper regret bound analysis of the proposed algorithm
demonstrates its learning effectiveness in such a non-trivial environment.
Extensive empirical evaluations on both synthetic and real-world datasets for
recommendation confirm its practical utility in a changing environment.


Efficient Exploration of Gradient Space for Online Learning to Rank

  Online learning to rank (OL2R) optimizes the utility of returned search
results based on implicit feedback gathered directly from users. To improve the
estimates, OL2R algorithms examine one or more exploratory gradient directions
and update the current ranker if a proposed one is preferred by users via an
interleaved test. In this paper, we accelerate the online learning process by
efficient exploration in the gradient space. Our algorithm, named as Null Space
Gradient Descent, reduces the exploration space to only the \emph{null space}
of recent poorly performing gradients. This prevents the algorithm from
repeatedly exploring directions that have been discouraged by the most recent
interactions with users. To improve sensitivity of the resulting interleaved
test, we selectively construct candidate rankers to maximize the chance that
they can be differentiated by candidate ranking documents in the current query;
and we use historically difficult queries to identify the best ranker when tie
occurs in comparing the rankers. Extensive experimental comparisons with the
state-of-the-art OL2R algorithms on several public benchmarks confirmed the
effectiveness of our proposal algorithm, especially in its fast learning
convergence and promising ranking quality at an early stage.


