GMine: A System for Scalable, Interactive Graph Visualization and Mining

  Several graph visualization tools exist. However, they are not able to handle
large graphs, and/or they do not allow interaction. We are interested on large
graphs, with hundreds of thousands of nodes. Such graphs bring two challenges:
the first one is that any straightforward interactive manipulation will be
prohibitively slow. The second one is sensory overload: even if we could plot
and replot the graph quickly, the user would be overwhelmed with the vast
volume of information because the screen would be too cluttered as nodes and
edges overlap each other. GMine system addresses both these issues, by using
summarization and multi-resolution. GMine offers multi-resolution graph
exploration by partitioning a given graph into a hierarchy of
com-munities-within-communities and storing it into a novel R-tree-like
structure which we name G-Tree. GMine offers summarization by implementing an
innovative subgraph extraction algorithm and then visualizing its output.


MaTrust: An Effective Multi-Aspect Trust Inference Model

  Trust is a fundamental concept in many real-world applications such as
e-commerce and peer-to-peer networks. In these applications, users can generate
local opinions about the counterparts based on direct experiences, and these
opinions can then be aggregated to build trust among unknown users. The
mechanism to build new trust relationships based on existing ones is referred
to as trust inference. State-of-the-art trust inference approaches employ the
transitivity property of trust by propagating trust along connected users. In
this paper, we propose a novel trust inference model (MaTrust) by exploring an
equally important property of trust, i.e., the multi-aspect property. MaTrust
directly characterizes multiple latent factors for each trustor and trustee
from the locally-generated trust relationships. Furthermore, it can naturally
incorporate prior knowledge as specified factors. These factors in turn serve
as the basis to infer the unseen trustworthiness scores. Experimental
evaluations on real data sets show that the proposed MaTrust significantly
outperforms several benchmark trust inference models in both effectiveness and
efficiency.


Replacing the Irreplaceable: Fast Algorithms for Team Member
  Recommendation

  In this paper, we study the problem of Team Member Replacement: given a team
of people embedded in a social network working on the same task, find a good
candidate who can fit in the team after one team member becomes unavailable. We
conjecture that a good team member replacement should have good skill matching
as well as good structure matching. We formulate this problem using the concept
of graph kernel. To tackle the computational challenges, we propose a family of
fast algorithms by (a) designing effective pruning strategies, and (b)
exploring the smoothness between the existing and the new team structures. We
conduct extensive experimental evaluations on real world datasets to
demonstrate the effectiveness and efficiency. Our algorithms (a) perform
significantly better than the alternative choices in terms of both precision
and recall; and (b) scale sub-linearly.


The Child is Father of the Man: Foresee the Success at the Early Stage

  Understanding the dynamic mechanisms that drive the high-impact scientific
work (e.g., research papers, patents) is a long-debated research topic and has
many important implications, ranging from personal career development and
recruitment search, to the jurisdiction of research resources. Recent advances
in characterizing and modeling scientific success have made it possible to
forecast the long-term impact of scientific work, where data mining techniques,
supervised learning in particular, play an essential role. Despite much
progress, several key algorithmic challenges in relation to predicting
long-term scientific impact have largely remained open. In this paper, we
propose a joint predictive model to forecast the long-term scientific impact at
the early stage, which simultaneously addresses a number of these open
challenges, including the scholarly feature design, the non-linearity, the
domain-heterogeneity and dynamics. In particular, we formulate it as a
regularized optimization problem and propose effective and scalable algorithms
to solve it. We perform extensive empirical evaluations on large, real
scholarly data sets to validate the effectiveness and the efficiency of our
method.


Panther: Fast Top-k Similarity Search in Large Networks

  Estimating similarity between vertices is a fundamental issue in network
analysis across various domains, such as social networks and biological
networks. Methods based on common neighbors and structural contexts have
received much attention. However, both categories of methods are difficult to
scale up to handle large networks (with billions of nodes). In this paper, we
propose a sampling method that provably and accurately estimates the similarity
between vertices. The algorithm is based on a novel idea of random path, and an
extended method is also presented, to enhance the structural similarity when
two vertices are completely disconnected. We provide theoretical proofs for the
error-bound and confidence of the proposed algorithm. We perform extensive
empirical study and show that our algorithm can obtain top-k similar vertices
for any vertex in a network approximately 300x faster than state-of-the-art
methods. We also use identity resolution and structural hole spanner finding,
two important applications in social networks, to evaluate the accuracy of the
estimated similarities. Our experimental results demonstrate that the proposed
algorithm achieves clearly better performance than several alternative methods.


Want a Good Answer? Ask a Good Question First!

  Community Question Answering (CQA) websites have become valuable repositories
which host a massive volume of human knowledge. To maximize the utility of such
knowledge, it is essential to evaluate the quality of an existing question or
answer, especially soon after it is posted on the CQA website.
  In this paper, we study the problem of inferring the quality of questions and
answers through a case study of a software CQA (Stack Overflow). Our key
finding is that the quality of an answer is strongly positively correlated with
that of its question. Armed with this observation, we propose a family of
algorithms to jointly predict the quality of questions and answers, for both
quantifying numerical quality scores and differentiating the high-quality
questions/answers from those of low quality. We conduct extensive experimental
evaluations to demonstrate the effectiveness and efficiency of our methods.


The Links Have It: Infobox Generation by Summarization over Linked
  Entities

  Online encyclopedia such as Wikipedia has become one of the best sources of
knowledge. Much effort has been devoted to expanding and enriching the
structured data by automatic information extraction from unstructured text in
Wikipedia. Although remarkable progresses have been made, their effectiveness
and efficiency is still limited as they try to tackle an extremely difficult
natural language understanding problems and heavily relies on supervised
learning approaches which require large amount effort to label the training
data. In this paper, instead of performing information extraction over
unstructured natural language text directly, we focus on a rich set of
semi-structured data in Wikipedia articles: linked entities. The idea of this
paper is the following: If we can summarize the relationship between the entity
and its linked entities, we immediately harvest some of the most important
information about the entity. To this end, we propose a novel rank aggregation
approach to remove noise, an effective clustering and labeling algorithm to
extract knowledge.


Local Partition in Rich Graphs

  Local graph partitioning is a key graph mining tool that allows researchers
to identify small groups of interrelated nodes (e.g. people) and their
connective edges (e.g. interactions). Because local graph partitioning is
primarily focused on the network structure of the graph (vertices and edges),
it often fails to consider the additional information contained in the
attributes. In this paper we propose---(i) a scalable algorithm to improve
local graph partitioning by taking into account both the network structure of
the graph and the attribute data and (ii) an application of the proposed local
graph partitioning algorithm (AttriPart) to predict the evolution of local
communities (LocalForecasting). Experimental results show that our proposed
AttriPart algorithm finds up to 1.6$\times$ denser local partitions, while
running approximately 43$\times$ faster than traditional local partitioning
techniques (PageRank-Nibble). In addition, our LocalForecasting algorithm shows
a significant improvement in the number of nodes and edges correctly predicted
over baseline methods.


EXTRA: Explaining Team Recommendation in Networks

  State-of-the-art in network science of teams offers effective recommendation
methods to answer questions like who is the best replacement, what is the best
team expansion strategy, but lacks intuitive ways to explain why the
optimization algorithm gives the specific recommendation for a given team
optimization scenario. To tackle this problem, we develop an interactive
prototype system, EXTRA, as the first step towards addressing such a
sense-making challenge, through the lens of the underlying network where teams
embed, to explain the team recommendation results. The main advantages are (1)
Algorithm efficacy: we propose an effective and fast algorithm to explain
random walk graph kernel, the central technique for networked team
recommendation; (2) Intuitive visual explanation: we present intuitive visual
analysis of the recommendation results, which can help users better understand
the rationality of the underlying team recommendation algorithm.


Flow-based Influence Graph Visual Summarization

  Visually mining a large influence graph is appealing yet challenging. People
are amazed by pictures of newscasting graph on Twitter, engaged by hidden
citation networks in academics, nevertheless often troubled by the unpleasant
readability of the underlying visualization. Existing summarization methods
enhance the graph visualization with blocked views, but have adverse effect on
the latent influence structure. How can we visually summarize a large graph to
maximize influence flows? In particular, how can we illustrate the impact of an
individual node through the summarization? Can we maintain the appealing graph
metaphor while preserving both the overall influence pattern and fine
readability?
  To answer these questions, we first formally define the influence graph
summarization problem. Second, we propose an end-to-end framework to solve the
new problem. Our method can not only highlight the flow-based influence
patterns in the visual summarization, but also inherently support rich graph
attributes. Last, we present a theoretic analysis and report our experiment
results. Both evidences demonstrate that our framework can effectively
approximate the proposed influence graph summarization objective while
outperforming previous methods in a typical scenario of visually mining
academic citation networks.


Graph-based Anomaly Detection and Description: A Survey

  Detecting anomalies in data is a vital task, with numerous high-impact
applications in areas such as security, finance, health care, and law
enforcement. While numerous techniques have been developed in past years for
spotting outliers and anomalies in unstructured collections of
multi-dimensional points, with graph data becoming ubiquitous, techniques for
structured {\em graph} data have been of focus recently. As objects in graphs
have long-range correlations, a suite of novel technology has been developed
for anomaly detection in graph data.
  This survey aims to provide a general, comprehensive, and structured overview
of the state-of-the-art methods for anomaly detection in data represented as
graphs. As a key contribution, we provide a comprehensive exploration of both
data mining and machine learning algorithms for these {\em detection} tasks. we
give a general framework for the algorithms categorized under various settings:
unsupervised vs. (semi-)supervised approaches, for static vs. dynamic graphs,
for attributed vs. plain graphs. We highlight the effectiveness, scalability,
generality, and robustness aspects of the methods. What is more, we stress the
importance of anomaly {\em attribution} and highlight the major techniques that
facilitate digging out the root cause, or the `why', of the detected anomalies
for further analysis and sense-making. Finally, we present several real-world
applications of graph-based anomaly detection in diverse domains, including
financial, auction, computer traffic, and social networks. We conclude our
survey with a discussion on open theoretical and practical challenges in the
field.


Structured Low-Rank Matrix Factorization with Missing and Grossly
  Corrupted Observations

  Recovering low-rank and sparse matrices from incomplete or corrupted
observations is an important problem in machine learning, statistics,
bioinformatics, computer vision, as well as signal and image processing. In
theory, this problem can be solved by the natural convex joint/mixed
relaxations (i.e., l_{1}-norm and trace norm) under certain conditions.
However, all current provable algorithms suffer from superlinear per-iteration
cost, which severely limits their applicability to large-scale problems. In
this paper, we propose a scalable, provable structured low-rank matrix
factorization method to recover low-rank and sparse matrices from missing and
grossly corrupted data, i.e., robust matrix completion (RMC) problems, or
incomplete and grossly corrupted measurements, i.e., compressive principal
component pursuit (CPCP) problems. Specifically, we first present two
small-scale matrix trace norm regularized bilinear structured factorization
models for RMC and CPCP problems, in which repetitively calculating SVD of a
large-scale matrix is replaced by updating two much smaller factor matrices.
Then, we apply the alternating direction method of multipliers (ADMM) to
efficiently solve the RMC problems. Finally, we provide the convergence
analysis of our algorithm, and extend it to address general CPCP problems.
Experimental results verified both the efficiency and effectiveness of our
method compared with the state-of-the-art methods.


Large Graph Analysis in the GMine System

  Current applications have produced graphs on the order of hundreds of
thousands of nodes and millions of edges. To take advantage of such graphs, one
must be able to find patterns, outliers and communities. These tasks are better
performed in an interactive environment, where human expertise can guide the
process. For large graphs, though, there are some challenges: the excessive
processing requirements are prohibitive, and drawing hundred-thousand nodes
results in cluttered images hard to comprehend. To cope with these problems, we
propose an innovative framework suited for any kind of tree-like graph visual
design. GMine integrates (a) a representation for graphs organized as
hierarchies of partitions - the concepts of SuperGraph and Graph-Tree; and (b)
a graph summarization methodology - CEPS. Our graph representation deals with
the problem of tracing the connection aspects of a graph hierarchy with sub
linear complexity, allowing one to grasp the neighborhood of a single node or
of a group of nodes in a single click. As a proof of concept, the visual
environment of GMine is instantiated as a system in which large graphs can be
investigated globally and locally.


AURORA: Auditing PageRank on Large Graphs

  Ranking on large-scale graphs plays a fundamental role in many high-impact
application domains, ranging from information retrieval, recommender systems,
sports team management, biology to neuroscience and many more. PageRank,
together with many of its random walk based variants, has become one of the
most well-known and widely used algorithms, due to its mathematical elegance
and the superior performance across a variety of application domains. Important
as it might be, state-of-the-art lacks an intuitive way to explain the ranking
results by PageRank (or its variants), e.g., why it thinks the returned top-k
webpages are most important ones in the entire graph; why it gives a higher
rank to actor John than actor Smith in terms of their relevance w.r.t. a
particular movie? In order to answer these questions, this paper proposes a
paradigm shift for PageRank, from identifying which nodes are most important to
understanding why the ranking algorithm gives a particular ranking result. We
formally define the PageRank auditing problem, whose central idea is to
identify a set of key graph elements (e.g., edges, nodes, subgraphs) with the
highest influence on the ranking results. We formulate it as an optimization
problem and propose a family of effective and scalable algorithms (AURORA) to
solve it. Our algorithms measure the influence of graph elements and
incrementally select influential elements w.r.t. their gradients over the
ranking results. We perform extensive empirical evaluations on real-world
datasets, which demonstrate that the proposed methods (AURORA) provide
intuitive explanations with a linear scalability.


