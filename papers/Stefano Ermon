Playing games against nature: optimal policies for renewable resource  allocation

  In this paper we introduce a class of Markov decision processes that arise asa natural model for many renewable resource allocation problems. Upon extendingresults from the inventory control literature, we prove that they admit aclosed form solution and we show how to exploit this structure to speed up itscomputation. We consider the application of the proposed framework to severalproblems arising in very different domains, and as part of the ongoing effortin the emerging field of Computational Sustainability we discuss in detail itsapplication to the Northern Pacific Halibut marine fishery. Our approach isapplied to a model based on real world data, obtaining a policy with aguaranteed lower bound on the utility function that is structurally verydifferent from the one currently employed.

Uniform Solution Sampling Using a Constraint Solver As an Oracle

  We consider the problem of sampling from solutions defined by a set of hardconstraints on a combinatorial space. We propose a new sampling technique that,while enforcing a uniform exploration of the search space, leverages thereasoning power of a systematic constraint solver in a black-box scheme. Wepresent a series of challenging domains, such as energy barriers and highlyasymmetric spaces, that reveal the difficulties introduced by hard constraints.We demonstrate that standard approaches such as Simulated Annealing and GibbsSampling are greatly affected, while our new technique can overcome many ofthese difficulties. Finally, we show that our sampling scheme naturally definesa new approximate model counting technique, which we empirically show to bevery accurate on a range of benchmark problems.

Optimization With Parity Constraints: From Binary Codes to Discrete  Integration

  Many probabilistic inference tasks involve summations over exponentiallylarge sets. Recently, it has been shown that these problems can be reduced tosolving a polynomial number of MAP inference queries for a model augmented withrandomly generated parity constraints. By exploiting a connection withmax-likelihood decoding of binary codes, we show that these optimizations arecomputationally hard. Inspired by iterative message passing decodingalgorithms, we propose an Integer Linear Programming (ILP) formulation for theproblem, enhanced with new sparsification techniques to improve decodingperformance. By solving the ILP through a sequence of LP relaxations, we getboth lower and upper bounds on the partition function, which hold with highprobability and are much tighter than those obtained with variational methods.

Pattern Decomposition with Complex Combinatorial Constraints:  Application to Materials Discovery

  Identifying important components or factors in large amounts of noisy data isa key problem in machine learning and data mining. Motivated by a patterndecomposition problem in materials discovery, aimed at discovering newmaterials for renewable energy, e.g. for fuel and solar cells, we introduceCombiFD, a framework for factor based pattern decomposition that allows theincorporation of a-priori knowledge as constraints, including complexcombinatorial constraints. In addition, we propose a new pattern decompositionalgorithm, called AMIQO, based on solving a sequence of (mixed-integer)quadratic programs. Our approach considerably outperforms the state of the arton the materials discovery problem, scaling to larger datasets and recoveringmore precise and physically meaningful decompositions. We also show theeffectiveness of our approach for enforcing background knowledge on otherapplication domains.

Generative Adversarial Imitation Learning

  Consider learning a policy from example expert behavior, without interactionwith the expert or access to reinforcement signal. One approach is to recoverthe expert's cost function with inverse reinforcement learning, then extract apolicy from that cost function with reinforcement learning. This approach isindirect and can be slow. We propose a new general framework for directlyextracting a policy from data, as if it were obtained by reinforcement learningfollowing inverse reinforcement learning. We show that a certain instantiationof our framework draws an analogy between imitation learning and generativeadversarial networks, from which we derive a model-free imitation learningalgorithm that obtains significant performance gains over existing model-freemethods in imitating complex behaviors in large, high-dimensional environments.

Label-Free Supervision of Neural Networks with Physics and Domain  Knowledge

  In many machine learning applications, labeled data is scarce and obtainingmore labels is expensive. We introduce a new approach to supervising neuralnetworks by specifying constraints that should hold over the output space,rather than direct examples of input-output pairs. These constraints arederived from prior domain knowledge, e.g., from known laws of physics. Wedemonstrate the effectiveness of this approach on real world and simulatedcomputer vision tasks. We are able to train a convolutional neural network todetect and track objects without any labeled examples. Our approach cansignificantly reduce the need for labeled training data, but introduces newchallenges for encoding prior knowledge into appropriate loss functions.

Learning Hierarchical Features from Generative Models

  Deep neural networks have been shown to be very successful at learningfeature hierarchies in supervised learning tasks. Generative models, on theother hand, have benefited less from hierarchical models with multiple layersof latent variables. In this paper, we prove that hierarchical latent variablemodels do not take advantage of the hierarchical structure when trained withexisting variational methods, and provide some limitations on the kind offeatures existing models can learn. Finally we propose an alternativearchitecture that do not suffer from these limitations. Our model is able tolearn highly interpretable and disentangled hierarchical features on severalnatural image datasets with no task specific regularization or prior knowledge.

Boosted Generative Models

  We propose a novel approach for using unsupervised boosting to create anensemble of generative models, where models are trained in sequence to correctearlier mistakes. Our meta-algorithmic framework can leverage any existing baselearner that permits likelihood evaluation, including recent deep expressivemodels. Further, our approach allows the ensemble to include discriminativemodels trained to distinguish real data from model-generated data. We showtheoretical conditions under which incorporating a new model in the ensemblewill improve the fit and empirically demonstrate the effectiveness of ourblack-box boosting algorithms on density estimation, classification, and samplegeneration on benchmark datasets for a wide range of generative models.

Towards Deeper Understanding of Variational Autoencoding Models

  We propose a new family of optimization criteria for variationalauto-encoding models, generalizing the standard evidence lower bound. Weprovide conditions under which they recover the data distribution and learnlatent features, and formally show that common issues such as blurry samplesand uninformative latent features arise when these conditions are not met.Based on these new insights, we propose a new sequential VAE model that cangenerate sharp samples on the LSUN image dataset based on pixel-wisereconstruction loss, and propose an optimization criterion that encouragesunsupervised learning of informative latent features.

Shape optimization in laminar flow with a label-guided variational  autoencoder

  Computational design optimization in fluid dynamics usually requires to solvenon-linear partial differential equations numerically. In this work, we explorea Bayesian optimization approach to minimize an object's drag coefficient inlaminar flow based on predicting drag directly from the object shape. Jointlytraining an architecture combining a variational autoencoder mapping shapes tolatent representations and Gaussian process regression allows us to generateimproved shapes in the two dimensional case we consider.

Improved Training with Curriculum GANs

  In this paper we introduce Curriculum GANs, a curriculum learning strategyfor training Generative Adversarial Networks that increases the strength of thediscriminator over the course of training, thereby making the learning taskprogressively more difficult for the generator. We demonstrate that thisstrategy is key to obtaining state-of-the-art results in image generation. Wealso show evidence that this strategy may be broadly applicable to improvingGAN training in other data modalities.

Multi-Agent Generative Adversarial Imitation Learning

  Imitation learning algorithms can be used to learn a policy from expertdemonstrations without access to a reward signal. However, most existingapproaches are not applicable in multi-agent settings due to the existence ofmultiple (Nash) equilibria and non-stationary environments. We propose a newframework for multi-agent imitation learning for general Markov games, where webuild upon a generalized notion of inverse reinforcement learning. We furtherintroduce a practical multi-agent actor-critic algorithm with good empiricalperformance. Our method can be used to imitate complex behaviors inhigh-dimensional environments with multiple cooperative or competing agents.

Differentiable Antithetic Sampling for Variance Reduction in Stochastic  Variational Inference

  Stochastic optimization techniques are standard in variational inferencealgorithms. These methods estimate gradients by approximating expectations withindependent Monte Carlo samples. In this paper, we explore a technique thatuses correlated, but more representative , samples to reduce estimatorvariance. Specifically, we show how to generate antithetic samples that matchsample moments with the true moments of an underlying importance distribution.Combining a differentiable antithetic sampler with modern stochasticvariational inference, we showcase the effectiveness of this approach forlearning a deep generative model.

Bias and Generalization in Deep Generative Models: An Empirical Study

  In high dimensional settings, density estimation algorithms rely crucially ontheir inductive bias. Despite recent empirical success, the inductive bias ofdeep generative models is not well understood. In this paper we propose aframework to systematically investigate bias and generalization in deepgenerative models of images. Inspired by experimental methods from cognitivepsychology, we probe each learning algorithm with carefully designed trainingdatasets to characterize when and how existing models generate novel attributesand their combinations. We identify similarities to human psychology and verifythat these patterns are consistent across commonly used models andarchitectures.

Taming the Curse of Dimensionality: Discrete Integration by Hashing and  Optimization

  Integration is affected by the curse of dimensionality and quickly becomesintractable as the dimensionality of the problem grows. We propose a randomizedalgorithm that, with high probability, gives a constant-factor approximation ofa general discrete integral defined over an exponentially large set. Thisalgorithm relies on solving only a small number of instances of a discretecombinatorial optimization problem subject to randomly generated parityconstraints used as a hash function. As an application, we demonstrate thatwith a small number of MAP queries we can efficiently approximate the partitionfunction of discrete graphical models, which can in turn be used, for instance,for marginal computation or model selection.

Variable Elimination in the Fourier Domain

  The ability to represent complex high dimensional probability distributionsin a compact form is one of the key insights in the field of graphical models.Factored representations are ubiquitous in machine learning and lead to majorcomputational advantages. We explore a different type of compact representationbased on discrete Fourier representations, complementing the classical approachbased on conditional independencies. We show that a large class ofprobabilistic graphical models have a compact Fourier representation. Thistheoretical result opens up an entirely new way of approximating a probabilitydistribution. We demonstrate the significance of this approach by applying itto the variable elimination algorithm. Compared with the traditional bucketrepresentation and other approximate inference algorithms, we obtainsignificant improvements.

Tight Variational Bounds via Random Projections and I-Projections

  Information projections are the key building block of variational inferencealgorithms and are used to approximate a target probabilistic model byprojecting it onto a family of tractable distributions. In general, there is noguarantee on the quality of the approximation obtained. To overcome this issue,we introduce a new class of random projections to reduce the dimensionality andhence the complexity of the original model. In the spirit of randomprojections, the projection preserves (with high probability) key properties ofthe target distribution. We show that information projections can be combinedwith random projections to obtain provable guarantees on the quality of theapproximation obtained, regardless of the complexity of the original model. Wedemonstrate empirically that augmenting mean field with a random projectionstep dramatically improves partition function and marginal probabilityestimates, both on synthetic and real world data.

Closing the Gap Between Short and Long XORs for Model Counting

  Many recent algorithms for approximate model counting are based on areduction to combinatorial searches over random subsets of the space defined byparity or XOR constraints. Long parity constraints (involving many variables)provide strong theoretical guarantees but are computationally difficult. Shortparity constraints are easier to solve but have weaker statistical properties.It is currently not known how long these parity constraints need to be. Weclose the gap by providing matching necessary and sufficient conditions on therequired asymptotic length of the parity constraints. Further, we provide a newfamily of lower bounds and the first non-trivial upper bounds on the modelcount that are valid for arbitrarily short XORs. We empirically demonstrate theeffectiveness of these bounds on model counting benchmarks and in aSatisfiability Modulo Theory (SMT) application motivated by the analysis ofcontingency tables in statistics.

Model-Free Imitation Learning with Policy Optimization

  In imitation learning, an agent learns how to behave in an environment withan unknown cost function by mimicking expert demonstrations. Existing imitationlearning algorithms typically involve solving a sequence of planning orreinforcement learning problems. Such algorithms are therefore not directlyapplicable to large, high-dimensional environments, and their performance cansignificantly degrade if the planning problems are not solved to optimality.Under the apprenticeship learning formalism, we develop alternative model-freealgorithms for finding a parameterized stochastic policy that performs at leastas well as an expert policy on an unknown cost function, based on sampletrajectories from the expert. Our approach, based on policy gradients, scalesto large continuous environments with guaranteed convergence to local minima.

Estimating Uncertainty Online Against an Adversary

  Assessing uncertainty is an important step towards ensuring the safety andreliability of machine learning systems. Existing uncertainty estimationtechniques may fail when their modeling assumptions are not met, e.g. when thedata distribution differs from the one seen at training time. Here, we proposetechniques that assess a classification algorithm's uncertainty via calibratedprobabilities (i.e. probabilities that match empirical outcome frequencies inthe long run) and which are guaranteed to be reliable (i.e. accurate andcalibrated) on out-of-distribution input, including input generated by anadversary. This represents an extension of classical online learning thathandles uncertainty in addition to guaranteeing accuracy under adversarialassumptions. We establish formal guarantees for our methods, and we validatethem on two real-world problems: question answering and medical diagnosis fromgenomic data.

Solving Marginal MAP Problems with NP Oracles and Parity Constraints

  Arising from many applications at the intersection of decision making andmachine learning, Marginal Maximum A Posteriori (Marginal MAP) Problems unifythe two main classes of inference, namely maximization (optimization) andmarginal inference (counting), and are believed to have higher complexity thanboth of them. We propose XOR_MMAP, a novel approach to solve the Marginal MAPProblem, which represents the intractable counting subproblem with queries toNP oracles, subject to additional parity constraints. XOR_MMAP provides aconstant factor approximation to the Marginal MAP Problem, by encoding it as asingle optimization in polynomial size of the original problem. We evaluate ourapproach in several machine learning and decision making applications, and showthat our approach outperforms several state-of-the-art Marginal MAP solvers.

General Bounds on Satisfiability Thresholds for Random CSPs via Fourier  Analysis

  Random constraint satisfaction problems (CSPs) have been widely studied bothin AI and complexity theory. Empirically and theoretically, many random CSPshave been shown to exhibit a phase transition. As the ratio of constraints tovariables passes certain thresholds, they transition from being almostcertainly satisfiable to unsatisfiable. The exact location of this thresholdhas been thoroughly investigated, but only for certain common classes ofconstraints. In this paper, we present new bounds for the location of thesethresholds in boolean CSPs. Our main contribution is that our bounds are fullygeneral, and apply to any fixed constraint function that could be used togenerate an ensemble of random CSPs. These bounds rely on a novel Fourieranalysis and can be easily computed from the Fourier spectrum of a constraintfunction. Our bounds are within a constant factor of the exact thresholdlocation for many well-studied random CSPs. We demonstrate that our bounds canbe easily instantiated to obtain thresholds for many constraint functions thathad not been previously studied, and evaluate them experimentally.

On the Limits of Learning Representations with Label-Based Supervision

  Advances in neural network based classifiers have transformed automaticfeature learning from a pipe dream of stronger AI to a routine and expectedproperty of practical systems. Since the emergence of AlexNet every winningsubmission of the ImageNet challenge has employed end-to-end representationlearning, and due to the utility of good representations for transfer learning,representation learning has become as an important and distinct task fromsupervised learning. At present, this distinction is inconsequential, assupervised methods are state-of-the-art in learning transferablerepresentations. But recent work has shown that generative models can also bepowerful agents of representation learning. Will the representations learnedfrom these generative methods ever rival the quality of those from theirsupervised competitors? In this work, we argue in the affirmative, that from aninformation theoretic perspective, generative models have greater potential forrepresentation learning. Based on several experimentally validated assumptions,we show that supervised learning is upper bounded in its capacity forrepresentation learning in ways that certain generative models, such asGenerative Adversarial Networks (GANs) are not. We hope that our analysis willprovide a rigorous motivation for further exploration of generativerepresentation learning.

InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations

  The goal of imitation learning is to mimic expert behavior without access toan explicit reward signal. Expert demonstrations provided by humans, however,often show significant variability due to latent factors that are typically notexplicitly modeled. In this paper, we propose a new algorithm that can inferthe latent structure of expert demonstrations in an unsupervised way. Ourmethod, built on top of Generative Adversarial Imitation Learning, can not onlyimitate complex behaviors, but also learn interpretable and meaningfulrepresentations of complex behavioral data, including visual demonstrations. Inthe driving domain, we show that a model learned from human demonstrations isable to both accurately reproduce a variety of behaviors and accuratelyanticipate human actions using raw visual inputs. Compared with variousbaselines, our method can better capture the latent structure underlying expertdemonstrations, often recovering semantically meaningful factors of variationin the data.

Flow-GAN: Combining Maximum Likelihood and Adversarial Learning in  Generative Models

  Adversarial learning of probabilistic models has recently emerged as apromising alternative to maximum likelihood. Implicit models such as generativeadversarial networks (GAN) often generate better samples compared to explicitmodels trained by maximum likelihood. Yet, GANs sidestep the characterizationof an explicit density which makes quantitative evaluations challenging. Tobridge this gap, we propose Flow-GANs, a generative adversarial network forwhich we can perform exact likelihood evaluation, thus supporting bothadversarial and maximum likelihood training. When trained adversarially,Flow-GANs generate high-quality samples but attain extremely poorlog-likelihood scores, inferior even to a mixture model memorizing the trainingdata; the opposite is true when trained by maximum likelihood. Results on MNISTand CIFAR-10 demonstrate that hybrid training can attain high held-outlikelihoods while retaining visual fidelity in the generated samples.

InfoVAE: Information Maximizing Variational Autoencoders

  A key advance in learning generative models is the use of amortized inferencedistributions that are jointly trained with the models. We find that existingtraining objectives for variational autoencoders can lead to inaccurateamortized inference distributions and, in some cases, improving the objectiveprovably degrades the inference quality. In addition, it has been observed thatvariational autoencoders tend to ignore the latent variables when combined witha decoding distribution that is too flexible. We again identify the cause inexisting training criteria and propose a new class of objectives (InfoVAE) thatmitigate these problems. We show that our model can significantly improve thequality of the variational posterior and can make effective use of the latentfeatures regardless of the flexibility of the decoding distribution. Throughextensive qualitative and quantitative analyses, we demonstrate that our modelsoutperform competing approaches on multiple performance metrics.

A-NICE-MC: Adversarial Training for MCMC

  Existing Markov Chain Monte Carlo (MCMC) methods are either based ongeneral-purpose and domain-agnostic schemes which can lead to slow convergence,or hand-crafting of problem-specific proposals by an expert. We proposeA-NICE-MC, a novel method to train flexible parametric Markov chain kernels toproduce samples with desired properties. First, we propose an efficientlikelihood-free adversarial training method to train a Markov chain and mimic agiven data distribution. Then, we leverage flexible volume preserving flows toobtain parametric kernels for MCMC. Using a bootstrap approach, we show how totrain efficient Markov chains to sample from a prescribed posteriordistribution by iteratively improving the quality of both the model and thesamples. A-NICE-MC provides the first framework to automatically designefficient domain-specific MCMC proposals. Empirical results demonstrate thatA-NICE-MC combines the strong guarantees of MCMC with the expressiveness ofdeep neural networks, and is able to significantly outperform competing methodssuch as Hamiltonian Monte Carlo.

Fast Amortized Inference and Learning in Log-linear Models with Randomly  Perturbed Nearest Neighbor Search

  Inference in log-linear models scales linearly in the size of output space inthe worst-case. This is often a bottleneck in natural language processing andcomputer vision tasks when the output space is feasibly enumerable but verylarge. We propose a method to perform inference in log-linear models withsublinear amortized cost. Our idea hinges on using Gumbel random variableperturbations and a pre-computed Maximum Inner Product Search data structure toaccess the most-likely elements in sublinear amortized time. Our method yieldsprovable runtime and accuracy guarantees. Further, we present empiricalexperiments on ImageNet and Word Embeddings showing significant speedups forsampling, inference, and learning in log-linear models.

Audio Super Resolution using Neural Networks

  We introduce a new audio processing technique that increases the samplingrate of signals such as speech or music using deep convolutional neuralnetworks. Our model is trained on pairs of low and high-quality audio examples;at test-time, it predicts missing samples within a low-resolution signal in aninterpolation process similar to image super-resolution. Our method is simpleand does not involve specialized audio processing techniques; in ourexperiments, it outperforms baselines on standard speech and music benchmarksat upscaling ratios of 2x, 4x, and 6x. The method has practical applications intelephony, compression, and text-to-speech generation; it demonstrates theeffectiveness of feed-forward convolutional architectures on an audiogeneration task.

A Survey of Human Activity Recognition Using WiFi CSI

  In this article, we present a survey of recent advances in passive humanbehaviour recognition in indoor areas using the channel state information (CSI)of commercial WiFi systems. Movement of human body causes a change in thewireless signal reflections, which results in variations in the CSI. Byanalyzing the data streams of CSIs for different activities and comparing themagainst stored models, human behaviour can be recognized. This is done byextracting features from CSI data streams and using machine learning techniquesto build models and classifiers. The techniques from the literature that arepresented herein have great performances, however, instead of the machinelearning techniques employed in these works, we propose to use deep learningtechniques such as long-short term memory (LSTM) recurrent neural network(RNN), and show the improved performance. We also discuss about differentchallenges such as environment change, frame rate selection, and multi-userscenario, and suggest possible directions for future work.

Neural Variational Inference and Learning in Undirected Graphical Models

  Many problems in machine learning are naturally expressed in the language ofundirected graphical models. Here, we propose black-box learning and inferencealgorithms for undirected models that optimize a variational approximation tothe log-likelihood of the model. Central to our approach is an upper bound onthe log-partition function parametrized by a function q that we express as aflexible neural network. Our bound makes it possible to track the partitionfunction during learning, to speed-up sampling, and to train a broad class ofhybrid directed/undirected models via a unified variational inferenceframework. We empirically demonstrate the effectiveness of our method onseveral popular generative modeling datasets.

Poverty Prediction with Public Landsat 7 Satellite Imagery and Machine  Learning

  Obtaining detailed and reliable data about local economic livelihoods indeveloping countries is expensive, and data are consequently scarce. Previouswork has shown that it is possible to measure local-level economic livelihoodsusing high-resolution satellite imagery. However, such imagery is relativelyexpensive to acquire, often not updated frequently, and is mainly available forrecent years. We train CNN models on free and publicly available multispectraldaytime satellite images of the African continent from the Landsat 7 satellite,which has collected imagery with global coverage for almost two decades. Weshow that despite these images' lower resolution, we can achieve accuraciesthat exceed previous benchmarks.

Hierarchical Modeling of Seed Variety Yields and Decision Making for  Future Planting Plans

  Eradicating hunger and malnutrition is a key development goal of the 21stcentury. We address the problem of optimally identifying seed varieties toreliably increase crop yield within a risk-sensitive decision-making framework.Specifically, we introduce a novel hierarchical machine learning mechanism forpredicting crop yield (the yield of different seed varieties of the same crop).We integrate this prediction mechanism with a weather forecasting model, andpropose three different approaches for decision making under uncertainty toselect seed varieties for planting so as to balance yield maximization andrisk.We apply our model to the problem of soybean variety selection given inthe 2016 Syngenta Crop Challenge. Our prediction model achieves a medianabsolute error of 3.74 bushels per acre and thus provides good estimates forinput into the decision models.Our decision models identify the selection ofsoybean varieties that appropriately balance yield and risk as a function ofthe farmer's risk aversion level. More generally, our models support farmers indecision making about which seed varieties to plant.

Deterministic Policy Optimization by Combining Pathwise and Score  Function Estimators for Discrete Action Spaces

  Policy optimization methods have shown great promise in solving complexreinforcement and imitation learning tasks. While model-free methods arebroadly applicable, they often require many samples to optimize complexpolicies. Model-based methods greatly improve sample-efficiency but at the costof poor generalization, requiring a carefully handcrafted model of the systemdynamics for each task. Recently, hybrid methods have been successful intrading off applicability for improved sample-complexity. However, these havebeen limited to continuous action spaces. In this work, we present a new hybridmethod based on an approximation of the dynamics as an expectation over thenext state under the current policy. This relaxation allows us to derive anovel hybrid policy gradient estimator, combining score function and pathwisederivative estimators, that is applicable to discrete action spaces. We showsignificant gains in sample complexity, ranging between $1.7$ and $25\times$,when learning parameterized policies on Cart Pole, Acrobot, Mountain Car andHand Mass. Our method is applicable to both discrete and continuous actionspaces, when competing pathwise methods are limited to the latter.

Approximate Inference via Weighted Rademacher Complexity

  Rademacher complexity is often used to characterize the learnability of ahypothesis class and is known to be related to the class size. We leverage thisobservation and introduce a new technique for estimating the size of anarbitrary weighted set, defined as the sum of weights of all elements in theset. Our technique provides upper and lower bounds on a novel generalization ofRademacher complexity to the weighted setting in terms of the weighted setsize. This generalizes Massart's Lemma, a known upper bound on the Rademachercomplexity in terms of the unweighted set size. We show that the weightedRademacher complexity can be estimated by solving a randomly perturbedoptimization problem, allowing us to derive high-probability bounds on the sizeof any weighted set. We apply our method to the problems of calculating thepartition function of an Ising model and computing propositional model counts(#SAT). Our experiments demonstrate that we can produce tighter bounds thancompeting methods in both the weighted and unweighted settings.

Accelerating Natural Gradient with Higher-Order Invariance

  An appealing property of the natural gradient is that it is invariant toarbitrary differentiable reparameterizations of the model. However, thisinvariance property requires infinitesimal steps and is lost in practicalimplementations with small but finite step sizes. In this paper, we studyinvariance properties from a combined perspective of Riemannian geometry andnumerical differential equation solving. We define the order of invariance of anumerical method to be its convergence order to an invariant solution. Wepropose to use higher-order integrators and geodesic corrections to obtain moreinvariant optimization trajectories. We prove the numerical convergenceproperties of geodesic corrected updates and show that they can be ascomputationally efficient as plain natural gradient. Experimentally, wedemonstrate that invariance leads to faster optimization and our techniquesimprove on traditional natural gradient in deep neural network training andnatural policy gradient for reinforcement learning.

Graphite: Iterative Generative Modeling of Graphs

  Graphs are a fundamental abstraction for modeling relational data. However,graphs are discrete and combinatorial in nature, and learning representationssuitable for machine learning tasks poses statistical and computationalchallenges. In this work, we propose Graphite an algorithmic framework forunsupervised learning of representations over nodes in a graph using deeplatent variable generative models. Our model is based on variationalautoencoders (VAE), and uses graph neural networks for parameterizing both thegenerative model (i.e., decoder) and inference model (i.e., encoder). The useof graph neural networks directly incorporates inductive biases due to thespatial, local structure of graphs directly in the generative model. We drawnovel connections of our framework with approximate inference via kernelembeddings. Empirically, Graphite outperforms competing approaches for thetasks of density estimation, link prediction, and node classification onsynthetic and benchmark datasets.

Best arm identification in multi-armed bandits with delayed feedback

  We propose a generalization of the best arm identification problem instochastic multi-armed bandits (MAB) to the setting where every pull of an armis associated with delayed feedback. The delay in feedback increases theeffective sample complexity of standard algorithms, but can be offset if wehave access to partial feedback received before a pull is completed. We proposea general framework to model the relationship between partial and delayedfeedback, and as a special case we introduce efficient algorithms for settingswhere the partial feedback are biased or unbiased estimators of the delayedfeedback. Additionally, we propose a novel extension of the algorithms to theparallel MAB setting where an agent can control a batch of arms. Ourexperiments in real-world settings, involving policy search and hyperparameteroptimization in computational sustainability domains for fast charging ofbatteries and wildlife corridor construction, demonstrate that exploiting thestructure of partial feedback can lead to significant improvements overbaselines in both sequential and parallel MAB.

End-to-End Learning of Motion Representation for Video Understanding

  Despite the recent success of end-to-end learned representations,hand-crafted optical flow features are still widely used in video analysistasks. To fill this gap, we propose TVNet, a novel end-to-end trainable neuralnetwork, to learn optical-flow-like features from data. TVNet subsumes aspecific optical flow solver, the TV-L1 method, and is initialized by unfoldingits optimization iterations as neural layers. TVNet can therefore be useddirectly without any extra learning. Moreover, it can be naturally concatenatedwith other task-specific networks to formulate an end-to-end architecture, thusmaking our method more efficient than current multi-stage approaches byavoiding the need to pre-compute and store features on disk. Finally, theparameters of the TVNet can be further fine-tuned by end-to-end training. Thisenables TVNet to learn richer and task-specific patterns beyond exact opticalflow. Extensive experiments on two action recognition benchmarks verify theeffectiveness of the proposed approach. Our TVNet achieves better accuraciesthan all compared methods, while being competitive with the fastest counterpartin terms of features extraction time.

Variational Rejection Sampling

  Learning latent variable models with stochastic variational inference ischallenging when the approximate posterior is far from the true posterior, dueto high variance in the gradient estimates. We propose a novel rejectionsampling step that discards samples from the variational posterior which areassigned low likelihoods by the model. Our approach provides an arbitrarilyaccurate approximation of the true posterior at the expense of extracomputation. Using a new gradient estimator for the resulting unnormalizedproposal distribution, we achieve average improvements of 3.71 nats and 0.21nats over state-of-the-art single-sample and multi-sample alternativesrespectively for estimating marginal log-likelihoods using sigmoid beliefnetworks on the MNIST dataset.

Tile2Vec: Unsupervised representation learning for spatially distributed  data

  Geospatial analysis lacks methods like the word vector representations andpre-trained networks that significantly boost performance across a wide rangeof natural language and computer vision tasks. To fill this gap, we introduceTile2Vec, an unsupervised representation learning algorithm that extends thedistributional hypothesis from natural language -- words appearing in similarcontexts tend to have similar meanings -- to spatially distributed data. Wedemonstrate empirically that Tile2Vec learns semantically meaningfulrepresentations on three datasets. Our learned representations significantlyimprove performance in downstream classification tasks and, similar to wordvectors, visual analogies can be obtained via simple arithmetic in the latentspace.

Amortized Inference Regularization

  The variational autoencoder (VAE) is a popular model for density estimationand representation learning. Canonically, the variational principle suggests toprefer an expressive inference model so that the variational approximation isaccurate. However, it is often overlooked that an overly-expressive inferencemodel can be detrimental to the test set performance of both the amortizedposterior approximator and, more importantly, the generative density estimator.In this paper, we leverage the fact that VAEs rely on amortized inference andpropose techniques for amortized inference regularization (AIR) that controlthe smoothness of the inference model. We demonstrate that, by applying AIR, itis possible to improve VAE generalization on both inference and generativeperformance. Our paper challenges the belief that amortized inference is simplya mechanism for approximating maximum likelihood training and illustrates thatregularization of the amortization family provides a new direction forunderstanding and improving generalization in VAEs.

Semi-supervised Deep Kernel Learning: Regression with Unlabeled Data by  Minimizing Predictive Variance

  Large amounts of labeled data are typically required to train deep learningmodels. For many real-world problems, however, acquiring additional data can beexpensive or even impossible. We present semi-supervised deep kernel learning(SSDKL), a semi-supervised regression model based on minimizing predictivevariance in the posterior regularization framework. SSDKL combines thehierarchical representation learning of neural networks with the probabilisticmodeling capabilities of Gaussian processes. By leveraging unlabeled data, weshow improvements on a diverse set of real-world regression tasks oversupervised deep kernel learning and semi-supervised methods such as VAT andmean teacher adapted for regression.

Adversarial Constraint Learning for Structured Prediction

  Constraint-based learning reduces the burden of collecting labels by havingusers specify general properties of structured outputs, such as constraintsimposed by physical laws. We propose a novel framework for simultaneouslylearning these constraints and using them for supervision, bypassing thedifficulty of using domain expertise to manually specify constraints. Learningrequires a black-box simulator of structured outputs, which generates validlabels, but need not model their corresponding inputs or the input-labelrelationship. At training time, we constrain the model to produce outputs thatcannot be distinguished from simulated labels by adversarial training.Providing our framework with a small number of labeled inputs gives rise to anew semi-supervised structured prediction model; we evaluate this model onmultiple tasks --- tracking, pose estimation and time series prediction --- andfind that it achieves high accuracy with only a small number of labeled inputs.In some cases, no labels are required at all.

The Information Autoencoding Family: A Lagrangian Perspective on Latent  Variable Generative Models

  A large number of objectives have been proposed to train latent variablegenerative models. We show that many of them are Lagrangian dual functions ofthe same primal optimization problem. The primal problem optimizes the mutualinformation between latent and visible variables, subject to the constraints ofaccurately modeling the data distribution and performing correct amortizedinference. By choosing to maximize or minimize mutual information, and choosingdifferent Lagrange multipliers, we obtain different objectives includingInfoGAN, ALI/BiGAN, ALICE, CycleGAN, beta-VAE, adversarial autoencoders, AVB,AS-VAE and InfoVAE. Based on this observation, we provide an exhaustivecharacterization of the statistical and computational trade-offs made by allthe training objectives in this class of Lagrangian duals. Next, we propose adual optimization method where we optimize model parameters as well as theLagrange multipliers. This method achieves Pareto optimal solutions in terms ofoptimizing information and satisfying the constraints.

Accurate Uncertainties for Deep Learning Using Calibrated Regression

  Methods for reasoning under uncertainty are a key building block of accurateand reliable machine learning systems. Bayesian methods provide a generalframework to quantify uncertainty. However, because of model misspecificationand the use of approximate inference, Bayesian uncertainty estimates are ofteninaccurate -- for example, a 90% credible interval may not contain the trueoutcome 90% of the time. Here, we propose a simple procedure for calibratingany regression algorithm; when applied to Bayesian and probabilistic models, itis guaranteed to produce calibrated uncertainty estimates given enough data.Our procedure is inspired by Platt scaling and extends previous work onclassification. We evaluate this approach on Bayesian linear regression,feedforward, and recurrent neural networks, and find that it consistentlyoutputs well-calibrated credible intervals while improving performance on timeseries forecasting and model-based reinforcement learning tasks.

Learning to Interpret Satellite Images Using Wikipedia

  Despite recent progress in computer vision, fine-grained interpretation ofsatellite images remains challenging because of a lack of labeled trainingdata. To overcome this limitation, we propose using Wikipedia as a previouslyuntapped source of rich, georeferenced textual information with globalcoverage. We construct a novel large-scale, multi-modal dataset by pairinggeo-referenced Wikipedia articles with satellite imagery of their correspondinglocations. To prove the efficacy of this dataset, we focus on the Africancontinent and train a deep network to classify images based on labels extractedfrom articles. We then fine-tune the model on a human annotated dataset anddemonstrate that this weak form of supervision can drastically reduce thequantity of human annotated labels and time required for downstream tasks.

Neural Joint Source-Channel Coding

  For reliable transmission across a noisy communication channel, classicalresults from information theory show that it is asymptotically optimal toseparate out the source and channel coding processes. However, thisdecomposition can fall short in the finite bit-length regime, as it requiresnon-trivial tuning of hand-crafted codes and assumes infinite computationalpower for decoding. In this work, we propose to jointly learn the encoding anddecoding processes using a new discrete variational autoencoder model. Byadding noise into the latent codes to simulate the channel during training, welearn to both compress and error-correct given a fixed bit-length andcomputational budget. We obtain codes that are not only competitive againstseveral separation schemes, but also learn useful robust representations of thedata for downstream tasks such as classification. Finally, inferenceamortization yields an extremely fast neural decoder, almost an order ofmagnitude faster compared to standard decoding methods based on iterativebelief propagation.

Streamlining Variational Inference for Constraint Satisfaction Problems

  Several algorithms for solving constraint satisfaction problems are based onsurvey propagation, a variational inference scheme used to obtain approximatemarginal probability estimates for variable assignments. These marginalscorrespond to how frequently each variable is set to true among satisfyingassignments, and are used to inform branching decisions during search; however,marginal estimates obtained via survey propagation are approximate and can beself-contradictory. We introduce a more general branching strategy based onstreamlining constraints, which sidestep hard assignments to variables. We showthat streamlined solvers consistently outperform decimation-based solvers onrandom k-SAT instances for several problem sizes, shrinking the gap betweenempirical performance and theoretical limits of satisfiability by 16.3% onaverage for k=3,4,5,6.

Learning Controllable Fair Representations

  Learning data representations that are transferable and are fair with respectto certain protected attributes is crucial to reducing unfair decisions whilepreserving the utility of the data. We propose an information-theoreticallymotivated objective for learning maximally expressive representations subjectto fairness constraints. We demonstrate that a range of existing approachesoptimize approximations to the Lagrangian dual of our objective. In contrast tothese existing approaches, our objective allows the user to control thefairness of the representations by specifying limits on unfairness. Exploitingduality, we introduce a method that optimizes the model parameters as well asthe expressiveness-fairness trade-off. Empirical evidence suggests that ourproposed method can balance the trade-off between multiple notions of fairnessand achieves higher expressiveness at a lower computational cost.

