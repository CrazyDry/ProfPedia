Joint Cuts and Matching of Partitions in One Graph

  As two fundamental problems, graph cuts and graph matching have been
investigated over decades, resulting in vast literature in these two topics
respectively. However the way of jointly applying and solving graph cuts and
matching receives few attention. In this paper, we first formalize the problem
of simultaneously cutting a graph into two partitions i.e. graph cuts and
establishing their correspondence i.e. graph matching. Then we develop an
optimization algorithm by updating matching and cutting alternatively, provided
with theoretical analysis. The efficacy of our algorithm is verified on both
synthetic dataset and real-world images containing similar regions or
structures.


Training Neural Networks by Using Power Linear Units (PoLUs)

  In this paper, we introduce "Power Linear Unit" (PoLU) which increases the
nonlinearity capacity of a neural network and thus helps improving its
performance. PoLU adopts several advantages of previously proposed activation
functions. First, the output of PoLU for positive inputs is designed to be
identity to avoid the gradient vanishing problem. Second, PoLU has a non-zero
output for negative inputs such that the output mean of the units is close to
zero, hence reducing the bias shift effect. Thirdly, there is a saturation on
the negative part of PoLU, which makes it more noise-robust for negative
inputs. Furthermore, we prove that PoLU is able to map more portions of every
layer's input to the same space by using the power function and thus increases
the number of response regions of the neural network. We use image
classification for comparing our proposed activation function with others. In
the experiments, MNIST, CIFAR-10, CIFAR-100, Street View House Numbers (SVHN)
and ImageNet are used as benchmark datasets. The neural networks we implemented
include widely-used ELU-Network, ResNet-50, and VGG16, plus a couple of shallow
networks. Experimental results show that our proposed activation function
outperforms other state-of-the-art models with most networks.


Trending Chic: Analyzing the Influence of Social Media on Fashion Brands

  Social media platforms are popular venues for fashion brand marketing and
advertising. With the introduction of native advertising, users don't have to
endure banner ads that hold very little saliency and are unattractive. Using
images and subtle text overlays, even in a world of ever-depreciating attention
span, brands can retain their audience and have a capacious creative potential.
While an assortment of marketing strategies are conjectured, the subtle
distinctions between various types of marketing strategies remain
under-explored. This paper presents a qualitative analysis on the influence of
social media platforms on different behaviors of fashion brand marketing. We
employ both linguistic and computer vision techniques while comparing and
contrasting strategic idiosyncrasies. We also analyze brand audience retention
and social engagement hence providing suggestions in adapting advertising and
marketing strategies over Twitter and Instagram.


Unsupervised Video Analysis Based on a Spatiotemporal Saliency Detector

  Visual saliency, which predicts regions in the field of view that draw the
most visual attention, has attracted a lot of interest from researchers. It has
already been used in several vision tasks, e.g., image classification, object
detection, foreground segmentation. Recently, the spectrum analysis based
visual saliency approach has attracted a lot of interest due to its simplicity
and good performance, where the phase information of the image is used to
construct the saliency map. In this paper, we propose a new approach for
detecting spatiotemporal visual saliency based on the phase spectrum of the
videos, which is easy to implement and computationally efficient. With the
proposed algorithm, we also study how the spatiotemporal saliency can be used
in two important vision task, abnormality detection and spatiotemporal interest
point detection. The proposed algorithm is evaluated on several commonly used
datasets with comparison to the state-of-art methods from the literature. The
experiments demonstrate the effectiveness of the proposed approach to
spatiotemporal visual saliency detection and its application to the above
vision tasks


Joint Regression and Ranking for Image Enhancement

  Research on automated image enhancement has gained momentum in recent years,
partially due to the need for easy-to-use tools for enhancing pictures captured
by ubiquitous cameras on mobile devices. Many of the existing leading methods
employ machine-learning-based techniques, by which some enhancement parameters
for a given image are found by relating the image to the training images with
known enhancement parameters. While knowing the structure of the parameter
space can facilitate search for the optimal solution, none of the existing
methods has explicitly modeled and learned that structure. This paper presents
an end-to-end, novel joint regression and ranking approach to model the
interaction between desired enhancement parameters and images to be processed,
employing a Gaussian process (GP). GP allows searching for ideal parameters
using only the image features. The model naturally leads to a ranking technique
for comparing images in the induced feature space. Comparative evaluation using
the ground-truth based on the MIT-Adobe FiveK dataset plus subjective tests on
an additional data-set were used to demonstrate the effectiveness of the
proposed approach.


A Computational Approach to Relative Aesthetics

  Computational visual aesthetics has recently become an active research area.
Existing state-of-art methods formulate this as a binary classification task
where a given image is predicted to be beautiful or not. In many applications
such as image retrieval and enhancement, it is more important to rank images
based on their aesthetic quality instead of binary-categorizing them.
Furthermore, in such applications, it may be possible that all images belong to
the same category. Hence determining the aesthetic ranking of the images is
more appropriate. To this end, we formulate a novel problem of ranking images
with respect to their aesthetic quality. We construct a new dataset of image
pairs with relative labels by carefully selecting images from the popular AVA
dataset. Unlike in aesthetics classification, there is no single threshold
which would determine the ranking order of the images across our entire
dataset. We propose a deep neural network based approach that is trained on
image pairs by incorporating principles from relative learning. Results show
that such relative training procedure allows our network to rank the images
with a higher accuracy than a state-of-art network trained on the same set of
images using binary labels.


Relative Learning from Web Images for Content-adaptive Enhancement

  Personalized and content-adaptive image enhancement can find many
applications in the age of social media and mobile computing. This paper
presents a relative-learning-based approach, which, unlike previous methods,
does not require matching original and enhanced images for training. This
allows the use of massive online photo collections to train a ranking model for
improved enhancement. We first propose a multi-level ranking model, which is
learned from only relatively-labeled inputs that are automatically crawled.
Then we design a novel parameter sampling scheme under this model to generate
the desired enhancement parameters for a new image. For evaluation, we first
verify the effectiveness and the generalization abilities of our approach,
using images that have been enhanced/labeled by experts. Then we carry out
subjective tests, which show that users prefer images enhanced by our approach
over other existing methods.


Improving Vision-based Self-positioning in Intelligent Transportation
  Systems via Integrated Lane and Vehicle Detection

  Traffic congestion is a widespread problem. Dynamic traffic routing systems
and congestion pricing are getting importance in recent research. Lane
prediction and vehicle density estimation is an important component of such
systems. We introduce a novel problem of vehicle self-positioning which
involves predicting the number of lanes on the road and vehicle's position in
those lanes using videos captured by a dashboard camera. We propose an
integrated closed-loop approach where we use the presence of vehicles to aid
the task of self-positioning and vice-versa. To incorporate multiple factors
and high-level semantic knowledge into the solution, we formulate this problem
as a Bayesian framework. In the framework, the number of lanes, the vehicle's
position in those lanes and the presence of other vehicles are considered as
parameters. We also propose a bounding box selection scheme to reduce the
number of false detections and increase the computational efficiency. We show
that the number of box proposals decreases by a factor of 6 using the selection
approach. It also results in large reduction in the number of false detections.
The entire approach is tested on real-world videos and is found to give
acceptable results.


Investigating Human Factors in Image Forgery Detection

  In today's age of internet and social media, one can find an enormous volume
of forged images on-line. These images have been used in the past to convey
falsified information and achieve harmful intentions. The spread and the effect
of the social media only makes this problem more severe. While creating forged
images has become easier due to software advancements, there is no automated
algorithm which can reliably detect forgery.
  Image forgery detection can be seen as a subset of image understanding
problem. Human performance is still the gold-standard for these type of
problems when compared to existing state-of-art automated algorithms. We
conduct a subjective evaluation test with the aid of eye-tracker to investigate
into human factors associated with this problem. We compare the performance of
an automated algorithm and humans for forgery detection problem. We also
develop an algorithm which uses the data from the evaluation test to predict
the difficulty-level of an image (the difficulty-level of an image here denotes
how difficult it is for humans to detect forgery in an image. Terms such as
"Easy/difficult image" will be used in the same context). The experimental
results presented in this paper should facilitate development of better
algorithms in the future.


Classification of Diabetic Retinopathy Images Using Multi-Class
  Multiple-Instance Learning Based on Color Correlogram Features

  All people with diabetes have the risk of developing diabetic retinopathy
(DR), a vision-threatening complication. Early detection and timely treatment
can reduce the occurrence of blindness due to DR. Computer-aided diagnosis has
the potential benefit of improving the accuracy and speed in DR detection. This
study is concerned with automatic classification of images with microaneurysm
(MA) and neovascularization (NV), two important DR clinical findings. Together
with normal images, this presents a 3-class classification problem. We propose
a modified color auto-correlogram feature (AutoCC) with low dimensionality that
is spectrally tuned towards DR images. Recognizing the fact that the images
with or without MA or NV are generally different only in small, localized
regions, we propose to employ a multi-class, multiple-instance learning
framework for performing the classification task using the proposed feature.
Extensive experiments including comparison with a few state-of-art image
classification approaches have been performed and the results suggest that the
proposed approach is promising as it outperforms other methods by a large
margin.


Supporting Navigation of Outdoor Shopping Complexes for
  Visually-impaired Users through Multi-modal Data Fusion

  Outdoor shopping complexes (OSC) are extremely difficult for people with
visual impairment to navigate. Existing GPS devices are mostly designed for
roadside navigation and seldom transition well into an OSC-like setting. We
report our study on the challenges faced by a blind person in navigating OSC
through developing a new mobile application named iExplore. We first report an
exploratory study aiming at deriving specific design principles for building
this system by learning the unique challenges of the problem. Then we present a
methodology that can be used to derive the necessary information for the
development of iExplore, followed by experimental validation of the technology
by a group of visually impaired users in a local outdoor shopping center. User
feedback and other experiments suggest that iExplore, while at its very initial
phase, has the potential of filling a practical gap in existing assistive
technologies for the visually impaired.


Hierarchical Attention Network for Action Recognition in Videos

  Understanding human actions in wild videos is an important task with a broad
range of applications. In this paper we propose a novel approach named
Hierarchical Attention Network (HAN), which enables to incorporate static
spatial information, short-term motion information and long-term video temporal
structures for complex human action understanding. Compared to recent
convolutional neural network based approaches, HAN has following advantages (1)
HAN can efficiently capture video temporal structures in a longer range; (2)
HAN is able to reveal temporal transitions between frame chunks with different
time steps, i.e. it explicitly models the temporal transitions between frames
as well as video segments and (3) with a multiple step spatial temporal
attention mechanism, HAN automatically learns important regions in video frames
and temporal segments in the video. The proposed model is trained and evaluated
on the standard video action benchmarks, i.e., UCF-101 and HMDB-51, and it
significantly outperforms the state-of-the arts


Recognizing Plans by Learning Embeddings from Observed Action
  Distributions

  Recent advances in visual activity recognition have raised the possibility of
applications such as automated video surveillance. Effective approaches for
such problems however require the ability to recognize the plans of agents from
video information. Although traditional plan recognition algorithms depend on
access to sophisticated planning domain models, one recent promising direction
involves learning approximated (or shallow) domain models directly from the
observed activity sequences DUP. One limitation is that such approaches expect
observed action sequences as inputs. In many cases involving vision/sensing
from raw data, there is considerable uncertainty about the specific action at
any given time point. The most we can expect in such cases is probabilistic
information about the action at that point. The input will then be sequences of
such observed action distributions. In this work, we address the problem of
constructing an effective data-interface that allows a plan recognition module
to directly handle such observation distributions. Such an interface works like
a bridge between the low-level perception module, and the high-level plan
recognition module. We propose two approaches. The first involves resampling
the distribution sequences to single action sequences, from which we could
learn an action affinity model based on learned action (word) embeddings for
plan recognition. The second is to directly learn action distribution
embeddings by our proposed Distr2vec (distribution to vector) model, to
construct an affinity model for plan recognition.


Mean Local Group Average Precision (mLGAP): A New Performance Metric for
  Hashing-based Retrieval

  The research on hashing techniques for visual data is gaining increased
attention in recent years due to the need for compact representations
supporting efficient search/retrieval in large-scale databases such as online
images. Among many possibilities, Mean Average Precision(mAP) has emerged as
the dominant performance metric for hashing-based retrieval. One glaring
shortcoming of mAP is its inability in balancing retrieval accuracy and
utilization of hash codes: pushing a system to attain higher mAP will
inevitably lead to poorer utilization of the hash codes. Poor utilization of
the hash codes hinders good retrieval because of increased collision of samples
in the hash space. This means that a model giving a higher mAP values does not
necessarily do a better job in retrieval. In this paper, we introduce a new
metric named Mean Local Group Average Precision (mLGAP) for better evaluation
of the performance of hashing-based retrieval. The new metric provides a
retrieval performance measure that also reconciles the utilization of hash
codes, leading to a more practically meaningful performance metric than
conventional ones like mAP. To this end, we start by mathematical analysis of
the deficiencies of mAP for hashing-based retrieval. We then propose mLGAP and
show why it is more appropriate for hashing-based retrieval. Experiments on
image retrieval are used to demonstrate the effectiveness of the proposed
metric.


Plan-Recognition-Driven Attention Modeling for Visual Recognition

  Human visual recognition of activities or external agents involves an
interplay between high-level plan recognition and low-level perception. Given
that, a natural question to ask is: can low-level perception be improved by
high-level plan recognition? We formulate the problem of leveraging recognized
plans to generate better top-down attention maps
\cite{gazzaniga2009,baluch2011} to improve the perception performance. We call
these top-down attention maps specifically as plan-recognition-driven attention
maps. To address this problem, we introduce the Pixel Dynamics Network. Pixel
Dynamics Network serves as an observation model, which predicts next states of
object points at each pixel location given observation of pixels and
pixel-level action feature. This is like internally learning a pixel-level
dynamics model. Pixel Dynamics Network is a kind of Convolutional Neural
Network (ConvNet), with specially-designed architecture. Therefore, Pixel
Dynamics Network could take the advantage of parallel computation of ConvNets,
while learning the pixel-level dynamics model. We further prove the equivalence
between Pixel Dynamics Network as an observation model, and the belief update
in partially observable Markov decision process (POMDP) framework. We evaluate
our Pixel Dynamics Network in event recognition tasks. We build an event
recognition system, ER-PRN, which takes Pixel Dynamics Network as a subroutine,
to recognize events based on observations augmented by plan-recognition-driven
attention.


A Structured Approach to Predicting Image Enhancement Parameters

  Social networking on mobile devices has become a commonplace of everyday
life. In addition, photo capturing process has become trivial due to the
advances in mobile imaging. Hence people capture a lot of photos everyday and
they want them to be visually-attractive. This has given rise to automated,
one-touch enhancement tools. However, the inability of those tools to provide
personalized and content-adaptive enhancement has paved way for machine-learned
methods to do the same. The existing typical machine-learned methods
heuristically (e.g. kNN-search) predict the enhancement parameters for a new
image by relating the image to a set of similar training images. These
heuristic methods need constant interaction with the training images which
makes the parameter prediction sub-optimal and computationally expensive at
test time which is undesired. This paper presents a novel approach to
predicting the enhancement parameters given a new image using only its
features, without using any training images. We propose to model the
interaction between the image features and its corresponding enhancement
parameters using the matrix factorization (MF) principles. We also propose a
way to integrate the image features in the MF formulation. We show that our
approach outperforms heuristic approaches as well as recent approaches in MF
and structured prediction on synthetic as well as real-world data of image
enhancement.


Diving deeper into mentee networks

  Modern computer vision is all about the possession of powerful image
representations. Deeper and deeper convolutional neural networks have been
built using larger and larger datasets and are made publicly available. A large
swath of computer vision scientists use these pre-trained networks with varying
degrees of successes in various tasks. Even though there is tremendous success
in copying these networks, the representational space is not learnt from the
target dataset in a traditional manner. One of the reasons for opting to use a
pre-trained network over a network learnt from scratch is that small datasets
provide less supervision and require meticulous regularization, smaller and
careful tweaking of learning rates to even achieve stable learning without
weight explosion. It is often the case that large deep networks are not
portable, which necessitates the ability to learn mid-sized networks from
scratch.
  In this article, we dive deeper into training these mid-sized networks on
small datasets from scratch by drawing additional supervision from a large
pre-trained network. Such learning also provides better generalization
accuracies than networks trained with common regularization techniques such as
l2, l1 and dropouts. We show that features learnt thus, are more general than
those learnt independently. We studied various characteristics of such networks
and found some interesting behaviors.


Neural Dataset Generality

  Often the filters learned by Convolutional Neural Networks (CNNs) from
different datasets appear similar. This is prominent in the first few layers.
This similarity of filters is being exploited for the purposes of transfer
learning and some studies have been made to analyse such transferability of
features. This is also being used as an initialization technique for different
tasks in the same dataset or for the same task in similar datasets.
Off-the-shelf CNN features have capitalized on this idea to promote their
networks as best transferable and most general and are used in a cavalier
manner in day-to-day computer vision tasks.
  It is curious that while the filters learned by these CNNs are related to the
atomic structures of the images from which they are learnt, all datasets learn
similar looking low-level filters. With the understanding that a dataset that
contains many such atomic structures learn general filters and are therefore
useful to initialize other networks with, we propose a way to analyse and
quantify generality among datasets from their accuracies on transferred
filters. We applied this metric on several popular character recognition,
natural image and a medical image dataset, and arrived at some interesting
conclusions. On further experimentation we also discovered that particular
classes in a dataset themselves are more general than others.


A Strategy for an Uncompromising Incremental Learner

  Multi-class supervised learning systems require the knowledge of the entire
range of labels they predict. Often when learnt incrementally, they suffer from
catastrophic forgetting. To avoid this, generous leeways have to be made to the
philosophy of incremental learning that either forces a part of the machine to
not learn, or to retrain the machine again with a selection of the historic
data. While these hacks work to various degrees, they do not adhere to the
spirit of incremental learning. In this article, we redefine incremental
learning with stringent conditions that do not allow for any undesirable
relaxations and assumptions. We design a strategy involving generative models
and the distillation of dark knowledge as a means of hallucinating data along
with appropriate targets from past distributions. We call this technique,
phantom sampling.We show that phantom sampling helps avoid catastrophic
forgetting during incremental learning. Using an implementation based on deep
neural networks, we demonstrate that phantom sampling dramatically avoids
catastrophic forgetting. We apply these strategies to competitive multi-class
incremental learning of deep neural networks. Using various benchmark datasets
and through our strategy, we demonstrate that strict incremental learning could
be achieved. We further put our strategy to test on challenging cases,
including cross-domain increments and incrementing on a novel label space. We
also propose a trivial extension to unbounded-continual learning and identify
potential for future development.


Capturing Localized Image Artifacts through a CNN-based Hyper-image
  Representation

  Training deep CNNs to capture localized image artifacts on a relatively small
dataset is a challenging task. With enough images at hand, one can hope that a
deep CNN characterizes localized artifacts over the entire data and their
effect on the output. However, on smaller datasets, such deep CNNs may overfit
and shallow ones find it hard to capture local artifacts. Thus some image-based
small-data applications first train their framework on a collection of patches
(instead of the entire image) to better learn the representation of localized
artifacts. Then the output is obtained by averaging the patch-level results.
Such an approach ignores the spatial correlation among patches and how various
patch locations affect the output. It also fails in cases where few patches
mainly contribute to the image label. To combat these scenarios, we develop the
notion of hyper-image representations. Our CNN has two stages. The first stage
is trained on patches. The second stage utilizes the last layer representation
developed in the first stage to form a hyper-image, which is used to train the
second stage. We show that this approach is able to develop a better mapping
between the image and its output. We analyze additional properties of our
approach and show its effectiveness on one synthetic and two real-world vision
tasks - no-reference image quality estimation and image tampering detection -
by its performance improvement over existing strong baselines.


Weakly Supervised Deep Image Hashing through Tag Embeddings

  Many approaches to semantic image hashing have been formulated as supervised
learning problems that utilize images and label information to learn the binary
hash codes. However, large-scale labeled image data is expensive to obtain,
thus imposing a restriction on the usage of such algorithms. On the other hand,
unlabelled image data is abundant due to the existence of many Web image
repositories. Such Web images may often come with images tags that contain
useful information, although raw tags, in general, do not readily lead to
semantic labels. Motivated by this scenario, we formulate the problem of
semantic image hashing as a weakly-supervised learning problem. We utilize the
information contained in the user-generated tags associated with the images to
learn the hash codes. More specifically, we extract the word2vec semantic
embeddings of the tags and use the information contained in them for
constraining the learning. Accordingly, we name our model Weakly Supervised
Deep Hashing using Tag Embeddings (WDHT). WDHT is tested for the task of
semantic image retrieval and is compared against several state-of-art models.
Results show that our approach sets a new state-of-art in the area of weekly
supervised image hashing.


