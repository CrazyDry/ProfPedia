Joint Cuts and Matching of Partitions in One Graph

  As two fundamental problems, graph cuts and graph matching have beeninvestigated over decades, resulting in vast literature in these two topicsrespectively. However the way of jointly applying and solving graph cuts andmatching receives few attention. In this paper, we first formalize the problemof simultaneously cutting a graph into two partitions i.e. graph cuts andestablishing their correspondence i.e. graph matching. Then we develop anoptimization algorithm by updating matching and cutting alternatively, providedwith theoretical analysis. The efficacy of our algorithm is verified on bothsynthetic dataset and real-world images containing similar regions orstructures.

Training Neural Networks by Using Power Linear Units (PoLUs)

  In this paper, we introduce "Power Linear Unit" (PoLU) which increases thenonlinearity capacity of a neural network and thus helps improving itsperformance. PoLU adopts several advantages of previously proposed activationfunctions. First, the output of PoLU for positive inputs is designed to beidentity to avoid the gradient vanishing problem. Second, PoLU has a non-zerooutput for negative inputs such that the output mean of the units is close tozero, hence reducing the bias shift effect. Thirdly, there is a saturation onthe negative part of PoLU, which makes it more noise-robust for negativeinputs. Furthermore, we prove that PoLU is able to map more portions of everylayer's input to the same space by using the power function and thus increasesthe number of response regions of the neural network. We use imageclassification for comparing our proposed activation function with others. Inthe experiments, MNIST, CIFAR-10, CIFAR-100, Street View House Numbers (SVHN)and ImageNet are used as benchmark datasets. The neural networks we implementedinclude widely-used ELU-Network, ResNet-50, and VGG16, plus a couple of shallownetworks. Experimental results show that our proposed activation functionoutperforms other state-of-the-art models with most networks.

Unsupervised Video Analysis Based on a Spatiotemporal Saliency Detector

  Visual saliency, which predicts regions in the field of view that draw themost visual attention, has attracted a lot of interest from researchers. It hasalready been used in several vision tasks, e.g., image classification, objectdetection, foreground segmentation. Recently, the spectrum analysis basedvisual saliency approach has attracted a lot of interest due to its simplicityand good performance, where the phase information of the image is used toconstruct the saliency map. In this paper, we propose a new approach fordetecting spatiotemporal visual saliency based on the phase spectrum of thevideos, which is easy to implement and computationally efficient. With theproposed algorithm, we also study how the spatiotemporal saliency can be usedin two important vision task, abnormality detection and spatiotemporal interestpoint detection. The proposed algorithm is evaluated on several commonly useddatasets with comparison to the state-of-art methods from the literature. Theexperiments demonstrate the effectiveness of the proposed approach tospatiotemporal visual saliency detection and its application to the abovevision tasks

Trending Chic: Analyzing the Influence of Social Media on Fashion Brands

  Social media platforms are popular venues for fashion brand marketing andadvertising. With the introduction of native advertising, users don't have toendure banner ads that hold very little saliency and are unattractive. Usingimages and subtle text overlays, even in a world of ever-depreciating attentionspan, brands can retain their audience and have a capacious creative potential.While an assortment of marketing strategies are conjectured, the subtledistinctions between various types of marketing strategies remainunder-explored. This paper presents a qualitative analysis on the influence ofsocial media platforms on different behaviors of fashion brand marketing. Weemploy both linguistic and computer vision techniques while comparing andcontrasting strategic idiosyncrasies. We also analyze brand audience retentionand social engagement hence providing suggestions in adapting advertising andmarketing strategies over Twitter and Instagram.

Hierarchical Attention Network for Action Recognition in Videos

  Understanding human actions in wild videos is an important task with a broadrange of applications. In this paper we propose a novel approach namedHierarchical Attention Network (HAN), which enables to incorporate staticspatial information, short-term motion information and long-term video temporalstructures for complex human action understanding. Compared to recentconvolutional neural network based approaches, HAN has following advantages (1)HAN can efficiently capture video temporal structures in a longer range; (2)HAN is able to reveal temporal transitions between frame chunks with differenttime steps, i.e. it explicitly models the temporal transitions between framesas well as video segments and (3) with a multiple step spatial temporalattention mechanism, HAN automatically learns important regions in video framesand temporal segments in the video. The proposed model is trained and evaluatedon the standard video action benchmarks, i.e., UCF-101 and HMDB-51, and itsignificantly outperforms the state-of-the arts

Joint Regression and Ranking for Image Enhancement

  Research on automated image enhancement has gained momentum in recent years,partially due to the need for easy-to-use tools for enhancing pictures capturedby ubiquitous cameras on mobile devices. Many of the existing leading methodsemploy machine-learning-based techniques, by which some enhancement parametersfor a given image are found by relating the image to the training images withknown enhancement parameters. While knowing the structure of the parameterspace can facilitate search for the optimal solution, none of the existingmethods has explicitly modeled and learned that structure. This paper presentsan end-to-end, novel joint regression and ranking approach to model theinteraction between desired enhancement parameters and images to be processed,employing a Gaussian process (GP). GP allows searching for ideal parametersusing only the image features. The model naturally leads to a ranking techniquefor comparing images in the induced feature space. Comparative evaluation usingthe ground-truth based on the MIT-Adobe FiveK dataset plus subjective tests onan additional data-set were used to demonstrate the effectiveness of theproposed approach.

A Computational Approach to Relative Aesthetics

  Computational visual aesthetics has recently become an active research area.Existing state-of-art methods formulate this as a binary classification taskwhere a given image is predicted to be beautiful or not. In many applicationssuch as image retrieval and enhancement, it is more important to rank imagesbased on their aesthetic quality instead of binary-categorizing them.Furthermore, in such applications, it may be possible that all images belong tothe same category. Hence determining the aesthetic ranking of the images ismore appropriate. To this end, we formulate a novel problem of ranking imageswith respect to their aesthetic quality. We construct a new dataset of imagepairs with relative labels by carefully selecting images from the popular AVAdataset. Unlike in aesthetics classification, there is no single thresholdwhich would determine the ranking order of the images across our entiredataset. We propose a deep neural network based approach that is trained onimage pairs by incorporating principles from relative learning. Results showthat such relative training procedure allows our network to rank the imageswith a higher accuracy than a state-of-art network trained on the same set ofimages using binary labels.

Relative Learning from Web Images for Content-adaptive Enhancement

  Personalized and content-adaptive image enhancement can find manyapplications in the age of social media and mobile computing. This paperpresents a relative-learning-based approach, which, unlike previous methods,does not require matching original and enhanced images for training. Thisallows the use of massive online photo collections to train a ranking model forimproved enhancement. We first propose a multi-level ranking model, which islearned from only relatively-labeled inputs that are automatically crawled.Then we design a novel parameter sampling scheme under this model to generatethe desired enhancement parameters for a new image. For evaluation, we firstverify the effectiveness and the generalization abilities of our approach,using images that have been enhanced/labeled by experts. Then we carry outsubjective tests, which show that users prefer images enhanced by our approachover other existing methods.

Improving Vision-based Self-positioning in Intelligent Transportation  Systems via Integrated Lane and Vehicle Detection

  Traffic congestion is a widespread problem. Dynamic traffic routing systemsand congestion pricing are getting importance in recent research. Laneprediction and vehicle density estimation is an important component of suchsystems. We introduce a novel problem of vehicle self-positioning whichinvolves predicting the number of lanes on the road and vehicle's position inthose lanes using videos captured by a dashboard camera. We propose anintegrated closed-loop approach where we use the presence of vehicles to aidthe task of self-positioning and vice-versa. To incorporate multiple factorsand high-level semantic knowledge into the solution, we formulate this problemas a Bayesian framework. In the framework, the number of lanes, the vehicle'sposition in those lanes and the presence of other vehicles are considered asparameters. We also propose a bounding box selection scheme to reduce thenumber of false detections and increase the computational efficiency. We showthat the number of box proposals decreases by a factor of 6 using the selectionapproach. It also results in large reduction in the number of false detections.The entire approach is tested on real-world videos and is found to giveacceptable results.

Investigating Human Factors in Image Forgery Detection

  In today's age of internet and social media, one can find an enormous volumeof forged images on-line. These images have been used in the past to conveyfalsified information and achieve harmful intentions. The spread and the effectof the social media only makes this problem more severe. While creating forgedimages has become easier due to software advancements, there is no automatedalgorithm which can reliably detect forgery.  Image forgery detection can be seen as a subset of image understandingproblem. Human performance is still the gold-standard for these type ofproblems when compared to existing state-of-art automated algorithms. Weconduct a subjective evaluation test with the aid of eye-tracker to investigateinto human factors associated with this problem. We compare the performance ofan automated algorithm and humans for forgery detection problem. We alsodevelop an algorithm which uses the data from the evaluation test to predictthe difficulty-level of an image (the difficulty-level of an image here denoteshow difficult it is for humans to detect forgery in an image. Terms such as"Easy/difficult image" will be used in the same context). The experimentalresults presented in this paper should facilitate development of betteralgorithms in the future.

Classification of Diabetic Retinopathy Images Using Multi-Class  Multiple-Instance Learning Based on Color Correlogram Features

  All people with diabetes have the risk of developing diabetic retinopathy(DR), a vision-threatening complication. Early detection and timely treatmentcan reduce the occurrence of blindness due to DR. Computer-aided diagnosis hasthe potential benefit of improving the accuracy and speed in DR detection. Thisstudy is concerned with automatic classification of images with microaneurysm(MA) and neovascularization (NV), two important DR clinical findings. Togetherwith normal images, this presents a 3-class classification problem. We proposea modified color auto-correlogram feature (AutoCC) with low dimensionality thatis spectrally tuned towards DR images. Recognizing the fact that the imageswith or without MA or NV are generally different only in small, localizedregions, we propose to employ a multi-class, multiple-instance learningframework for performing the classification task using the proposed feature.Extensive experiments including comparison with a few state-of-art imageclassification approaches have been performed and the results suggest that theproposed approach is promising as it outperforms other methods by a largemargin.

Supporting Navigation of Outdoor Shopping Complexes for  Visually-impaired Users through Multi-modal Data Fusion

  Outdoor shopping complexes (OSC) are extremely difficult for people withvisual impairment to navigate. Existing GPS devices are mostly designed forroadside navigation and seldom transition well into an OSC-like setting. Wereport our study on the challenges faced by a blind person in navigating OSCthrough developing a new mobile application named iExplore. We first report anexploratory study aiming at deriving specific design principles for buildingthis system by learning the unique challenges of the problem. Then we present amethodology that can be used to derive the necessary information for thedevelopment of iExplore, followed by experimental validation of the technologyby a group of visually impaired users in a local outdoor shopping center. Userfeedback and other experiments suggest that iExplore, while at its very initialphase, has the potential of filling a practical gap in existing assistivetechnologies for the visually impaired.

Recognizing Plans by Learning Embeddings from Observed Action  Distributions

  Recent advances in visual activity recognition have raised the possibility ofapplications such as automated video surveillance. Effective approaches forsuch problems however require the ability to recognize the plans of agents fromvideo information. Although traditional plan recognition algorithms depend onaccess to sophisticated planning domain models, one recent promising directioninvolves learning approximated (or shallow) domain models directly from theobserved activity sequences DUP. One limitation is that such approaches expectobserved action sequences as inputs. In many cases involving vision/sensingfrom raw data, there is considerable uncertainty about the specific action atany given time point. The most we can expect in such cases is probabilisticinformation about the action at that point. The input will then be sequences ofsuch observed action distributions. In this work, we address the problem ofconstructing an effective data-interface that allows a plan recognition moduleto directly handle such observation distributions. Such an interface works likea bridge between the low-level perception module, and the high-level planrecognition module. We propose two approaches. The first involves resamplingthe distribution sequences to single action sequences, from which we couldlearn an action affinity model based on learned action (word) embeddings forplan recognition. The second is to directly learn action distributionembeddings by our proposed Distr2vec (distribution to vector) model, toconstruct an affinity model for plan recognition.

Mean Local Group Average Precision (mLGAP): A New Performance Metric for  Hashing-based Retrieval

  The research on hashing techniques for visual data is gaining increasedattention in recent years due to the need for compact representationssupporting efficient search/retrieval in large-scale databases such as onlineimages. Among many possibilities, Mean Average Precision(mAP) has emerged asthe dominant performance metric for hashing-based retrieval. One glaringshortcoming of mAP is its inability in balancing retrieval accuracy andutilization of hash codes: pushing a system to attain higher mAP willinevitably lead to poorer utilization of the hash codes. Poor utilization ofthe hash codes hinders good retrieval because of increased collision of samplesin the hash space. This means that a model giving a higher mAP values does notnecessarily do a better job in retrieval. In this paper, we introduce a newmetric named Mean Local Group Average Precision (mLGAP) for better evaluationof the performance of hashing-based retrieval. The new metric provides aretrieval performance measure that also reconciles the utilization of hashcodes, leading to a more practically meaningful performance metric thanconventional ones like mAP. To this end, we start by mathematical analysis ofthe deficiencies of mAP for hashing-based retrieval. We then propose mLGAP andshow why it is more appropriate for hashing-based retrieval. Experiments onimage retrieval are used to demonstrate the effectiveness of the proposedmetric.

Plan-Recognition-Driven Attention Modeling for Visual Recognition

  Human visual recognition of activities or external agents involves aninterplay between high-level plan recognition and low-level perception. Giventhat, a natural question to ask is: can low-level perception be improved byhigh-level plan recognition? We formulate the problem of leveraging recognizedplans to generate better top-down attention maps\cite{gazzaniga2009,baluch2011} to improve the perception performance. We callthese top-down attention maps specifically as plan-recognition-driven attentionmaps. To address this problem, we introduce the Pixel Dynamics Network. PixelDynamics Network serves as an observation model, which predicts next states ofobject points at each pixel location given observation of pixels andpixel-level action feature. This is like internally learning a pixel-leveldynamics model. Pixel Dynamics Network is a kind of Convolutional NeuralNetwork (ConvNet), with specially-designed architecture. Therefore, PixelDynamics Network could take the advantage of parallel computation of ConvNets,while learning the pixel-level dynamics model. We further prove the equivalencebetween Pixel Dynamics Network as an observation model, and the belief updatein partially observable Markov decision process (POMDP) framework. We evaluateour Pixel Dynamics Network in event recognition tasks. We build an eventrecognition system, ER-PRN, which takes Pixel Dynamics Network as a subroutine,to recognize events based on observations augmented by plan-recognition-drivenattention.

Diving deeper into mentee networks

  Modern computer vision is all about the possession of powerful imagerepresentations. Deeper and deeper convolutional neural networks have beenbuilt using larger and larger datasets and are made publicly available. A largeswath of computer vision scientists use these pre-trained networks with varyingdegrees of successes in various tasks. Even though there is tremendous successin copying these networks, the representational space is not learnt from thetarget dataset in a traditional manner. One of the reasons for opting to use apre-trained network over a network learnt from scratch is that small datasetsprovide less supervision and require meticulous regularization, smaller andcareful tweaking of learning rates to even achieve stable learning withoutweight explosion. It is often the case that large deep networks are notportable, which necessitates the ability to learn mid-sized networks fromscratch.  In this article, we dive deeper into training these mid-sized networks onsmall datasets from scratch by drawing additional supervision from a largepre-trained network. Such learning also provides better generalizationaccuracies than networks trained with common regularization techniques such asl2, l1 and dropouts. We show that features learnt thus, are more general thanthose learnt independently. We studied various characteristics of such networksand found some interesting behaviors.

Neural Dataset Generality

  Often the filters learned by Convolutional Neural Networks (CNNs) fromdifferent datasets appear similar. This is prominent in the first few layers.This similarity of filters is being exploited for the purposes of transferlearning and some studies have been made to analyse such transferability offeatures. This is also being used as an initialization technique for differenttasks in the same dataset or for the same task in similar datasets.Off-the-shelf CNN features have capitalized on this idea to promote theirnetworks as best transferable and most general and are used in a cavaliermanner in day-to-day computer vision tasks.  It is curious that while the filters learned by these CNNs are related to theatomic structures of the images from which they are learnt, all datasets learnsimilar looking low-level filters. With the understanding that a dataset thatcontains many such atomic structures learn general filters and are thereforeuseful to initialize other networks with, we propose a way to analyse andquantify generality among datasets from their accuracies on transferredfilters. We applied this metric on several popular character recognition,natural image and a medical image dataset, and arrived at some interestingconclusions. On further experimentation we also discovered that particularclasses in a dataset themselves are more general than others.

A Structured Approach to Predicting Image Enhancement Parameters

  Social networking on mobile devices has become a commonplace of everydaylife. In addition, photo capturing process has become trivial due to theadvances in mobile imaging. Hence people capture a lot of photos everyday andthey want them to be visually-attractive. This has given rise to automated,one-touch enhancement tools. However, the inability of those tools to providepersonalized and content-adaptive enhancement has paved way for machine-learnedmethods to do the same. The existing typical machine-learned methodsheuristically (e.g. kNN-search) predict the enhancement parameters for a newimage by relating the image to a set of similar training images. Theseheuristic methods need constant interaction with the training images whichmakes the parameter prediction sub-optimal and computationally expensive attest time which is undesired. This paper presents a novel approach topredicting the enhancement parameters given a new image using only itsfeatures, without using any training images. We propose to model theinteraction between the image features and its corresponding enhancementparameters using the matrix factorization (MF) principles. We also propose away to integrate the image features in the MF formulation. We show that ourapproach outperforms heuristic approaches as well as recent approaches in MFand structured prediction on synthetic as well as real-world data of imageenhancement.

A Strategy for an Uncompromising Incremental Learner

  Multi-class supervised learning systems require the knowledge of the entirerange of labels they predict. Often when learnt incrementally, they suffer fromcatastrophic forgetting. To avoid this, generous leeways have to be made to thephilosophy of incremental learning that either forces a part of the machine tonot learn, or to retrain the machine again with a selection of the historicdata. While these hacks work to various degrees, they do not adhere to thespirit of incremental learning. In this article, we redefine incrementallearning with stringent conditions that do not allow for any undesirablerelaxations and assumptions. We design a strategy involving generative modelsand the distillation of dark knowledge as a means of hallucinating data alongwith appropriate targets from past distributions. We call this technique,phantom sampling.We show that phantom sampling helps avoid catastrophicforgetting during incremental learning. Using an implementation based on deepneural networks, we demonstrate that phantom sampling dramatically avoidscatastrophic forgetting. We apply these strategies to competitive multi-classincremental learning of deep neural networks. Using various benchmark datasetsand through our strategy, we demonstrate that strict incremental learning couldbe achieved. We further put our strategy to test on challenging cases,including cross-domain increments and incrementing on a novel label space. Wealso propose a trivial extension to unbounded-continual learning and identifypotential for future development.

Capturing Localized Image Artifacts through a CNN-based Hyper-image  Representation

  Training deep CNNs to capture localized image artifacts on a relatively smalldataset is a challenging task. With enough images at hand, one can hope that adeep CNN characterizes localized artifacts over the entire data and theireffect on the output. However, on smaller datasets, such deep CNNs may overfitand shallow ones find it hard to capture local artifacts. Thus some image-basedsmall-data applications first train their framework on a collection of patches(instead of the entire image) to better learn the representation of localizedartifacts. Then the output is obtained by averaging the patch-level results.Such an approach ignores the spatial correlation among patches and how variouspatch locations affect the output. It also fails in cases where few patchesmainly contribute to the image label. To combat these scenarios, we develop thenotion of hyper-image representations. Our CNN has two stages. The first stageis trained on patches. The second stage utilizes the last layer representationdeveloped in the first stage to form a hyper-image, which is used to train thesecond stage. We show that this approach is able to develop a better mappingbetween the image and its output. We analyze additional properties of ourapproach and show its effectiveness on one synthetic and two real-world visiontasks - no-reference image quality estimation and image tampering detection -by its performance improvement over existing strong baselines.

Weakly Supervised Deep Image Hashing through Tag Embeddings

  Many approaches to semantic image hashing have been formulated as supervisedlearning problems that utilize images and label information to learn the binaryhash codes. However, large-scale labeled image data is expensive to obtain,thus imposing a restriction on the usage of such algorithms. On the other hand,unlabelled image data is abundant due to the existence of many Web imagerepositories. Such Web images may often come with images tags that containuseful information, although raw tags, in general, do not readily lead tosemantic labels. Motivated by this scenario, we formulate the problem ofsemantic image hashing as a weakly-supervised learning problem. We utilize theinformation contained in the user-generated tags associated with the images tolearn the hash codes. More specifically, we extract the word2vec semanticembeddings of the tags and use the information contained in them forconstraining the learning. Accordingly, we name our model Weakly SupervisedDeep Hashing using Tag Embeddings (WDHT). WDHT is tested for the task ofsemantic image retrieval and is compared against several state-of-art models.Results show that our approach sets a new state-of-art in the area of weeklysupervised image hashing.

