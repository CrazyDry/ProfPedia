Boltzmann Machine Learning with the Latent Maximum Entropy Principle

  We present a new statistical learning paradigm for Boltzmann machines basedon a new inference principle we have proposed: the latent maximum entropyprinciple (LME). LME is different both from Jaynes maximum entropy principleand from standard maximum likelihood estimation.We demonstrate the LMEprinciple BY deriving new algorithms for Boltzmann machine parameterestimation, and show how robust and fast new variant of the EM algorithm can bedeveloped.Our experiments show that estimation based on LME generally yieldsbetter results than maximum likelihood estimation, particularly when inferringhidden units from small amounts of data.

Slim Embedding Layers for Recurrent Neural Language Models

  Recurrent neural language models are the state-of-the-art models for languagemodeling. When the vocabulary size is large, the space taken to store the modelparameters becomes the bottleneck for the use of recurrent neural languagemodels. In this paper, we introduce a simple space compression method thatrandomly shares the structured parameters at both the input and outputembedding layers of the recurrent neural language models to significantlyreduce the size of model parameters, but still compactly represent the originalinput and output embedding layers. The method is easy to implement and tune.Experiments on several data sets show that the new method can get similarperplexity and BLEU score results while only using a very tiny fraction ofparameters.

