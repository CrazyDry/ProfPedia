Using Sets of Probability Measures to Represent Uncertainty

  I explore the use of sets of probability measures as a representation of
uncertainty.


CoRR: A Computing Research Repository

  Discusses how CoRR was set up and some policy issues involved with setting up
such a repository.


A Modification of the Halpern-Pearl Definition of Causality

  The original Halpern-Pearl definition of causality [Halpern and Pearl, 2001]
was updated in the journal version of the paper [Halpern and Pearl, 2005] to
deal with some problems pointed out by Hopkins and Pearl [2003]. Here the
definition is modified yet again, in a way that (a) leads to a simpler
definition, (b) handles the problems pointed out by Hopkins and Pearl, and many
others, (c) gives reasonable answers (that agree with those of the original and
updated definition) in the standard problematic examples of causality, and (d)
has lower complexity than either the original or updated definitions.


Why Bother With Syntax?

  This short note discusses the role of syntax vs. semantics and the interplay
between logic, philosophy, and language in computer science and game theory.


Characterizing Solution Concepts in Terms of Common Knowledge of
  Rationality

  Characterizations of Nash equilibrium, correlated equilibrium, and
rationalizability in terms of common knowledge of rationality are well known.
Analogous characterizations of sequential equilibrium, (trembling hand) perfect
equilibrium, and quasi-perfect equilibrium using results of Halpern [2009].


An Introduction to Logics of Knowledge and Belief

  This chapter provides an introduction to some basic concepts of epistemic
logic, basic formal languages, their semantics, and proof systems. It also
contains an overview of the handbook, and a brief history of epistemic logic
and pointers to the literature.


Cox's Theorem Revisited

  The assumptions needed to prove Cox's Theorem are discussed and examined.
Various sets of assumptions under which a Cox-style theorem can be proved are
provided, although all are rather strong and, arguably, not natural.


A response to the commentaries on CoRR

  This is a response to the commentaries on "CoRR: A Computing Research
Repository".


Conditional Plausibility Measures and Bayesian Networks

  A general notion of algebraic conditional plausibility measures is defined.
Probability measures, ranking functions, possibility measures, and (under the
appropriate definitions) sets of probability measures can all be viewed as
defining algebraic conditional plausibility measures. It is shown that
algebraic conditional plausibility measures can be represented using Bayesian
networks.


A computer scientist looks at game theory

  I consider issues in distributed computation that should be of relevance to
game theory. In particular, I focus on (a) representing knowledge and
uncertainty, (b) dealing with failures, and (c) specification of mechanisms.


Great Expectations. Part II: Generalized Expected Utility as a Universal
  Decision Rule

  Many different rules for decision making have been introduced in the
literature. We show that a notion of generalized expected utility proposed in
Part I of this paper is a universal decision rule, in the sense that it can
represent essentially all other decision rules.


Conditional Plausibility Measures and Bayesian Networks

  A general notion of algebraic conditional plausibility measures is defined.
Probability measures, ranking functions, possibility measures, and (under the
appropriate definitions) sets of probability measures can all be viewed as
defining algebraic conditional plausibility measures. It is shown that the
technology of Bayesian networks can be applied to algebraic conditional
plausibility measures.


Towards Formal Definitions of Blameworthiness, Intention, and Moral
  Responsibility

  We provide formal definitions of degree of blameworthiness and intention
relative to an epistemic state (a probability over causal models and a utility
function on outcomes). These, together with a definition of actual causality,
provide the key ingredients for moral responsibility judgments. We show that
these definitions give insight into commonsense intuitions in a variety of
puzzling cases from the literature.


Actual causation and the art of modeling

  We look more carefully at the modeling of causality using structural
equations. It is clear that the structural equations can have a major impact on
the conclusions we draw about causality. In particular, the choice of variables
and their values can also have a significant impact on causality. These choices
are, to some extent, subjective. We consider what counts as an appropriate
choice. More generally, we consider what makes a model an appropriate model,
especially if we want to take defaults into account, as was argued is necessary
in recent work.


Using Counterfactuals in Knowledge-Based Programming

  This paper adds counterfactuals to the framework of knowledge-based programs
of Fagin, Halpern, Moses, and Vardi. The use of counterfactuals is illustrated
by designing a protocol in which an agent stops sending messages once it knows
that it is safe to do so. Such behavior is difficult to capture in the original
framework because it involves reasoning about counterfactual executions,
including ones that are not consistent with the protocol. Attempts to formalize
these notions without counterfactuals are shown to lead to rather
counterintuitive behavior.


Responsibility and blame: a structural-model approach

  Causality is typically treated an all-or-nothing concept; either A is a cause
of B or it is not. We extend the definition of causality introduced by Halpern
and Pearl [2001] to take into account the degree of responsibility of A for B.
For example, if someone wins an election 11--0, then each person who votes for
him is less responsible for the victory than if he had won 6--5. We then define
a notion of degree of blame, which takes into account an agent's epistemic
state. Roughly speaking, the degree of blame of A for B is the expected degree
of responsibility of A for B, taken over the epistemic state of an agent.


Sleeping Beauty Reconsidered: Conditioning and Reflection in
  Asynchronous Systems

  A careful analysis of conditioning in the Sleeping Beauty problem is done,
using the formal model for reasoning about knowledge and probability developed
by Halpern and Tuttle. While the Sleeping Beauty problem has been viewed as
revealing problems with conditioning in the presence of imperfect recall, the
analysis done here reveals that the problems are not so much due to imperfect
recall as to asynchrony. The implications of this analysis for van Fraassen's
Reflection Principle and Savage's Sure-Thing Principle are considered.


Defaults and Normality in Causal Structures

  A serious defect with the Halpern-Pearl (HP) definition of causality is
repaired by combining a theory of causality with a theory of defaults. In
addition, it is shown that (despite a claim to the contrary) a cause according
to the HP condition need not be a single conjunct. A definition of causality
motivated by Wright's NESS test is shown to always hold for a single conjunct.
Moreover, conditions that hold for all the examples considered by HP are given
that guarantee that causality according to (this version) of the NESS test is
equivalent to the HP definition.


Common knowledge revisited

  We consider the common-knowledge paradox raised by Halpern and Moses: common
knowledge is necessary for agreement and coordination, but common knowledge is
unattainable in the real world because of temporal imprecision. We discuss two
solutions to this paradox: (1) modeling the world with a coarser granularity,
and (2) relaxing the requirements for coordination.


The Computational Complexity of Structure-Based Causality

  Halpern and Pearl introduced a definition of actual causality; Eiter and
Lukasiewicz showed that computing whether X=x is a cause of Y=y is NP-complete
in binary models (where all variables can take on only two values) and\
Sigma_2^P-complete in general models. In the final version of their paper,
Halpern and Pearl slightly modified the definition of actual cause, in order to
deal with problems pointed by Hopkins and Pearl. As we show, this modification
has a nontrivial impact on the complexity of computing actual cause. To
characterize the complexity, a new family D_k^P, k= 1, 2, 3, ..., of complexity
classes is introduced, which generalizes the class DP introduced by
Papadimitriou and Yannakakis (DP is just D_1^P). %joe2 %We show that the
complexity of computing causality is $\D_2$-complete %under the new definition.
Chockler and Halpern \citeyear{CH04} extended the We show that the complexity
of computing causality under the updated definition is $D_2^P$-complete.
  Chockler and Halpern extended the definition of causality by introducing
notions of responsibility and blame. The complexity of determining the degree
of responsibility and blame using the original definition of causality was
completely characterized. Again, we show that changing the definition of
causality affects the complexity, and completely characterize it using the
updated definition.


A decision-theoretic approach to reliable message delivery

  We argue that the tools of decision theory need to be taken more seriously in
the specification and analysis of systems. We illustrate this by considering a
simple problem involving reliable communication, showing how considerations of
utility and probability can be used to decide when it is worth sending
heartbeat messages and, if they are sent, how often they should be sent.


Causes and Explanations: A Structural-Model Approach, Part I: Causes

  We propose a new definition of actual cause, using structural equations to
model counterfactuals. We show that the definition yields a plausible and
elegant account of causation that handles well examples which have caused
problems for other definitions and resolves major difficulties in the
traditional account.


On the NP-completeness of Finding an Optimal Strategy in Games with
  Common Payoffs

  Consider a very simple class of (finite) games: after an initial move by
nature, each player makes one move. Moreover, the players have common
interests: at each node, all the players get the same payoff. We show that the
problem of determining whether there exists a joint strategy where each player
has an expected payoff of at least r is NP-complete as a function of the number
of nodes in the extensive-form representation of the game.


Lexicographic probability, conditional probability, and nonstandard
  probability

  The relationship between Popper spaces (conditional probability spaces that
satisfy some regularity conditions), lexicographic probability systems (LPS's),
and nonstandard probability spaces (NPS's) is considered. If countable
additivity is assumed, Popper spaces and a subclass of LPS's are equivalent;
without the assumption of countable additivity, the equivalence no longer
holds. If the state space is finite, LPS's are equivalent to NPS's. However, if
the state space is infinite, NPS's are shown to be more general than LPS's.


A logic for reasoning about upper probabilities

  We present a propositional logic %which can be used to reason about the
uncertainty of events, where the uncertainty is modeled by a set of probability
measures assigning an interval of probability to each event. We give a sound
and complete axiomatization for the logic, and show that the satisfiability
problem is NP-complete, no harder than satisfiability for propositional logic.


Great Expectations. Part I: On the Customizability of Generalized
  Expected Utility

  We propose a generalization of expected utility that we call generalized EU
(GEU), where a decision maker's beliefs are represented by plausibility
measures, and the decision maker's tastes are represented by general (i.e.,not
necessarily real-valued) utility functions. We show that every agent,
``rational'' or not, can be modeled as a GEU maximizer. We then show that we
can customize GEU by selectively imposing just the constraints we want. In
particular, we show how each of Savage's postulates corresponds to constraints
on GEU.


Intransitivity and Vagueness

  There are many examples in the literature that suggest that
indistinguishability is intransitive, despite the fact that the
indistinguishability relation is typically taken to be an equivalence relation
(and thus transitive). It is shown that if the uncertainty perception and the
question of when an agent reports that two things are indistinguishable are
both carefully modeled, the problems disappear, and indistinguishability can
indeed be taken to be an equivalence relation. Moreover, this model also
suggests a logic of vagueness that seems to solve many of the problems related
to vagueness discussed in the philosophical literature. In particular, it is
shown here how the logic can handle the sorites paradox.


Expressing Security Properties Using Selective Interleaving Functions

  McLean's notion of Selective Interleaving Functions (SIFs) is perhaps the
best-known attempt to construct a framework for expressing various security
properties. We examine the expressive power of SIFs carefully. We show that
SIFs cannot capture nondeducibility on strategies (NOS). We also prove that the
set of security properties expressed with SIFs is not closed under conjunction,
from which it follows that separability is strictly stronger than double
generalized noninterference. However, we show that if we generalize the notion
of SIF in a natural way, then NOS is expressible, and the set of security
properties expressible by generalized SIFs is closed under conjunction.


Rational Secret Sharing and Multiparty Computation: Extended Abstract

  We consider the problems of secret sharing and multiparty computation,
assuming that agents prefer to get the secret (resp., function value) to not
getting it, and secondarily, prefer that as few as possible of the other agents
get it. We show that, under these assumptions, neither secret sharing nor
multiparty function computation is possible using a mechanism that has a fixed
running time. However, we show that both are possible using randomized
mechanisms with constant expected running time.


Characterizing Solution Concepts in Games Using Knowledge-Based Programs

  We show how solution concepts in games such as Nash equilibrium, correlated
equilibrium, rationalizability, and sequential equilibrium can be given a
uniform definition in terms of \emph{knowledge-based programs}. Intuitively,
all solution concepts are implementations of two knowledge-based programs, one
appropriate for games represented in normal form, the other for games
represented in extensive form. These knowledge-based programs can be viewed as
embodying rationality. The representation works even if (a) information sets do
not capture an agent's knowledge, (b) uncertainty is not represented by
probability, or (c) the underlying game is not common knowledge.


Computer Science and Game Theory: A Brief Survey

  There has been a remarkable increase in work at the interface of computer
science and game theory in the past decade. In this article I survey some of
the main themes of work in the area, with a focus on the work in computer
science. Given the length constraints, I make no attempt at being
comprehensive, especially since other surveys are also available, and a
comprehensive survey book will appear shortly.


From Qualitative to Quantitative Proofs of Security Properties Using
  First-Order Conditional Logic

  A first-order conditional logic is considered, with semantics given by a
variant of epsilon-semantics, where p -> q means that Pr(q | p) approaches 1
super-polynomially --faster than any inverse polynomial. This type of
convergence is needed for reasoning about security protocols. A complete
axiomatization is provided for this semantics, and it is shown how a
qualitative proof of the correctness of a security protocol can be
automatically converted to a quantitative proof appropriate for reasoning about
concrete security.


A Logic for Reasoning about Upper Probabilities

  We present a propositional logic to reason about the uncertainty of events,
where the uncertainty is modeled by a set of probability measures assigning an
interval of probability to each event. We give a sound and complete
axiomatization for the logic, and show that the satisfiability problem is
NP-complete, no harder than satisfiability for propositional logic.


Causes and Explanations: A Structural-Model Approach --- Part 1: Causes

  We propose a new definition of actual causes, using structural equations to
model counterfactuals.We show that the definitions yield a plausible and
elegant account ofcausation that handles well examples which have caused
problems forother definitions and resolves major difficulties in the
traditionalaccount. In a companion paper, we show how the definition of
causality can beused to give an elegant definition of (causal) explanation.


A logic for reasoning about ambiguity

  Standard models of multi-agent modal logic do not capture the fact that
information is often \emph{ambiguous}, and may be interpreted in different ways
by different agents. We propose a framework that can model this, and consider
different semantics that capture different assumptions about the agents'
beliefs regarding whether or not there is ambiguity. We examine the expressive
power of logics of ambiguity compared to logics that cannot model ambiguity,
with respect to the different semantics that we propose.


Algorithmic Rationality: Game Theory with Costly Computation

  We develop a general game-theoretic framework for reasoning about strategic
agents performing possibly costly computation. In this framework, many
traditional game-theoretic results (such as the existence of a Nash
equilibrium) no longer hold. Nevertheless, we can use the framework to provide
psychologically appealing explanations of observed behavior in well-studied
games (such as finitely repeated prisoner's dilemma and rock-paper-scissors).
Furthermore, we provide natural conditions on games sufficient to guarantee
that equilibria exist.


Viewpoint: Journals for Certification, Conferences for Rapid
  Dissemination

  The publication culture in Computer Science is different from that of all
other disciplines. Whereas other disciplines focus on journal publication, the
standard practice in CS has been to publish in a conference and then
(sometimes) publish a journal version of the conference paper. We discuss the
role of journal publication in CS.
  Indeed, it is through publication in selective, leading conferences that the
quality of CS research is typically assessed.


Graded Causation and Defaults

  Recent work in psychology and experimental philosophy has shown that
judgments of actual causation are often influenced by consideration of
defaults, typicality, and normality. A number of philosophers and computer
scientists have also suggested that an appeal to such factors can help deal
with problems facing existing accounts of actual causation. This paper develops
a flexible formal framework for incorporating defaults, typicality, and
normality into an account of actual causation. The resulting account takes
actual causation to be both graded and comparative. We then show how our
account would handle a number of standard cases.


Compact Representations of Extended Causal Models

  Judea Pearl was the first to propose a definition of actual causation using
causal models. A number of authors have suggested that an adequate account of
actual causation must appeal not only to causal structure, but also to
considerations of normality. In earlier work, we provided a definition of
actual causation using extended causal models, which include information about
both causal structure and normality. Extended causal models are potentially
very complex. In this paper, we show how it is possible to achieve a compact
representation of extended causal models.


A Logic for Reasoning about Evidence

  We introduce a logic for reasoning about evidence, that essentially views
evidence as a function from prior beliefs (before making an observation) to
posterior beliefs (after making the observation). We provide a sound and
complete axiomatization for the logic, and consider the complexity of the
decision problem. Although the reasoning in the logic is mainly propositional,
we allow variables representing numbers and quantification over them. This
expressive power seems necessary to capture important properties of evidence


Cause, Responsibility, and Blame: oA Structural-Model Approach

  A definition of causality introduced by Halpern and Pearl, which uses
structural equations, is reviewed. A more refined definition is then
considered, which takes into account issues of normality and typicality, which
are well known to affect causal ascriptions. Causality is typically an
all-or-nothing notion: either A is a cause of B or it is not. An extension of
the definition of causality to capture notions of degree of responsibility and
degree of blame, due to Chockler and Halpern, is reviewed. For example, if
someone wins an election 11-0, then each person who votes for him is less
responsible for the victory than if he had won 6-5. Degree of blame takes into
account an agent's epistemic state. Roughly speaking, the degree of blame of A
for B is the expected degree of responsibility of A for B, taken over the
epistemic state of an agent. Finally, the structural-equations definition of
causality is compared to Wright's NESS test.


Multi-Agent Only Knowing

  Levesque introduced a notion of ``only knowing'', with the goal of capturing
certain types of nonmonotonic reasoning. Levesque's logic dealt with only the
case of a single agent. Recently, both Halpern and Lakemeyer independently
attempted to extend Levesque's logic to the multi-agent case. Although there
are a number of similarities in their approaches, there are some significant
differences. In this paper, we reexamine the notion of only knowing, going back
to first principles. In the process, we simplify Levesque's completeness proof,
and point out some problems with the earlier definitions. This leads us to
reconsider what the properties of only knowing ought to be. We provide an axiom
system that captures our desiderata, and show that it has a semantics that
corresponds to it. The axiom system has an added feature of interest: it
includes a modal operator for satisfiability, and thus provides a complete
axiomatization for satisfiability in the logic K45.


What Causes a System to Satisfy a Specification?

  Even when a system is proven to be correct with respect to a specification,
there is still a question of how complete the specification is, and whether it
really covers all the behaviors of the system. Coverage metrics attempt to
check which parts of a system are actually relevant for the verification
process to succeed. Recent work on coverage in model checking suggests several
coverage metrics and algorithms for finding parts of the system that are not
covered by the specification. The work has already proven to be effective in
practice, detecting design errors that escape early verification efforts in
industrial settings. In this paper, we relate a formal definition of causality
given by Halpern and Pearl [2001] to coverage. We show that it gives
significant insight into unresolved issues regarding the definition of coverage
and leads to potentially useful extensions of coverage. In particular, we
introduce the notion of responsibility, which assigns to components of a system
a quantitative measure of their relevance to the satisfaction of the
specification.


A Knowledge-Based Analysis of Global Function Computation

  Consider a distributed system N in which each agent has an input value and
each communication link has a weight. Given a global function, that is, a
function f whose value depends on the whole network, the goal is for every
agent to eventually compute the value f(N). We call this problem global
function computation. Various solutions for instances of this problem, such as
Boolean function computation, leader election, (minimum) spanning tree
construction, and network determination, have been proposed, each under
particular assumptions about what processors know about the system and how this
knowledge can be acquired. We give a necessary and sufficient condition for the
problem to be solvable that generalizes a number of well-known results. We then
provide a knowledge-based (kb) program (like those of Fagin, Halpern, Moses,
and Vardi) that solves global function computation whenever possible. Finally,
we improve the message overhead inherent in our initial kb program by giving a
counterfactual belief-based program that also solves the global function
computation whenever possible, but where agents send messages only when they
believe it is necessary to do so. The latter program is shown to be implemented
by a number of well-known algorithms for solving leader election.


Reasoning About Knowledge of Unawareness Revisited

  In earlier work, we proposed a logic that extends the Logic of General
Awareness of Fagin and Halpern [1988] by allowing quantification over primitive
propositions. This makes it possible to express the fact that an agent knows
that there are some facts of which he is unaware. In that logic, it is not
possible to model an agent who is uncertain about whether he is aware of all
formulas. To overcome this problem, we keep the syntax of the earlier paper,
but allow models where, with each world, a possibly different language is
associated. We provide a sound and complete axiomatization for this logic and
show that, under natural assumptions, the quantifier-free fragment of the logic
is characterized by exactly the same axioms as the logic of Heifetz, Meier, and
Schipper [2008].


Weighted regret-based likelihood: a new approach to describing
  uncertainty

  Recently, Halpern and Leung suggested representing uncertainty by a weighted
set of probability measures, and suggested a way of making decisions based on
this representation of uncertainty: maximizing weighted regret. Their paper
does not answer an apparently simpler question: what it means, according to
this representation of uncertainty, for an event E to be more likely than an
event E'. In this paper, a notion of comparative likelihood when uncertainty is
represented by a weighted set of probability measures is defined. It
generalizes the ordering defined by probability (and by lower probability) in a
natural way; a generalization of upper probability can also be defined. A
complete axiomatic characterization of this notion of regret-based likelihood
is given.


Interactive Unawareness Revisited

  We analyze a model of interactive unawareness introduced by Heifetz, Meier
and Schipper (HMS). We consider two axiomatizations for their model, which
capture different notions of validity. These axiomatizations allow us to
compare the HMS approach to both the standard (S5) epistemic logic and two
other approaches to unawareness: that of Fagin and Halpern and that of Modica
and Rustichini. We show that the differences between the HMS approach and the
others are mainly due to the notion of validity used and the fact that the HMS
is based on a 3-valued propositional logic.


Appropriate Causal Models and the Stability of Causation

  Causal models defined in terms of structural equations have proved to be
quite a powerful way of representing knowledge regarding causality. However, a
number of authors have given examples that seem to show that the Halpern-Pearl
(HP) definition of causality gives intuitively unreasonable answers. Here it is
shown that, for each of these examples, we can give two stories consistent with
the description in the example, such that intuitions regarding causality are
quite different for each story. By adding additional variables, we can
disambiguate the stories. Moreover, in the resulting causal models, the HP
definition of causality gives the intuitively correct answer. It is also shown
that, by adding extra variables, a modification to the original HP definition
made to deal with an example of Hopkins and Pearl may not be necessary. Given
how much can be done by adding extra variables, there might be a concern that
the notion of causality is somewhat unstable. Can adding extra variables in a
"conservative" way (i.e., maintaining all the relations between the variables
in the original model) cause the answer to the question "Is X=x a cause of Y=y"
to alternate between "yes" and "no"? It is shown that we can have such
alternation infinitely often, but if we take normality into consideration, we
cannot. Indeed, under appropriate normality assumptions. adding an extra
variable can change the answer from "yes" to "no", but after that, it cannot
cannot change back to "yes".


Complete Axiomatizations for Reasoning About Knowledge and Time

  Sound and complete axiomatizations are provided for a number of different
logics involving modalities for knowledge and time. These logics arise from
different choices for various parameters. All the logics considered involve the
discrete time linear temporal logic operators `next' and `until' and an
operator for the knowledge of each of a number of agents. Both the single agent
and multiple agent cases are studied: in some instances of the latter there is
also an operator for the common knowledge of the group of all agents. Four
different semantic properties of agents are considered: whether they have a
unique initial state, whether they operate synchronously, whether they have
perfect recall, and whether they learn. The property of no learning is
essentially dual to perfect recall. Not all settings of these parameters lead
to recursively axiomatizable logics, but sound and complete axiomatizations are
presented for all the ones that do.


Set-Theoretic Completeness for Epistemic and Conditional Logic

  The standard approach to logic in the literature in philosophy and
mathematics, which has also been adopted in computer science, is to define a
language (the syntax), an appropriate class of models together with an
interpretation of formulas in the language (the semantics), a collection of
axioms and rules of inference characterizing reasoning (the proof theory), and
then relate the proof theory to the semantics via soundness and completeness
results. Here we consider an approach that is more common in the economics
literature, which works purely at the semantic, set-theoretic level. We provide
set-theoretic completeness results for a number of epistemic and conditional
logics, and contrast the expressive power of the syntactic and set-theoretic
approaches


