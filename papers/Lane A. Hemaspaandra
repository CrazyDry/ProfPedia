Take-home Complexity

  We discuss the use of projects in first-year graduate complexity theory
courses.


P-Immune Sets with Holes Lack Self-Reducibility Properties

  No P-immune set having exponential gaps is positive-Turing self-reducible.


All Superlinear Inverse Schemes are coNP-Hard

  How hard is it to invert NP-problems? We show that all superlinearly
certified inverses of NP problems are coNP-hard. To do so, we develop a novel
proof technique that builds diagonalizations against certificates directly into
a circuit.


A Richer Understanding of the Complexity of Election Systems

  We provide an overview of some recent progress on the complexity of election
systems. The issues studied include the complexity of the winner, manipulation,
bribery, and control problems.


Downward Collapse from a Weaker Hypothesis

  Hemaspaandra et al. proved that, for $m > 0$ and $0 < i < k - 1$: if
$\Sigma_i^p \BoldfaceDelta DIFF_m(\Sigma_k^p)$ is closed under complementation,
then $DIFF_m(\Sigma_k^p) = coDIFF_m(\Sigma_k^p)$. This sharply asymmetric
result fails to apply to the case in which the hypothesis is weakened by
allowing the $\Sigma_i^p$ to be replaced by any class in its difference
hierarchy. We so extend the result by proving that, for $s,m > 0$ and $0 < i <
k - 1$: if $DIFF_s(\Sigma_i^p) \BoldfaceDelta DIFF_m(\Sigma_k^p)$ is closed
under complementation, then $DIFF_m(\Sigma_k^p) = coDIFF_m(\Sigma_k^p)$.


On Bounded-Weight Error-Correcting Codes

  This paper computationally obtains optimal bounded-weight, binary,
error-correcting codes for a variety of distance bounds and dimensions. We
compare the sizes of our codes to the sizes of optimal constant-weight, binary,
error-correcting codes, and evaluate the differences.


A Moment of Perfect Clarity II: Consequences of Sparse Sets Hard for NP
  with Respect to Weak Reductions

  This paper discusses advances, due to the work of Cai, Naik, and Sivakumar
and Glasser, in the complexity class collapses that follow if NP has sparse
hard sets under reductions weaker than (full) truth-table reductions.


A Note on Nonuniform versus Uniform ACC^k Circuits for NE

  We note that for each k \in {0,1,2, ...} the following holds: NE has
(nonuniform) ACC^k circuits if and only if NE has P^{NE}-uniform ACC^k
circuits. And we mention how to get analogous results for other circuit and
complexity classes.


Weighted Electoral Control

  Although manipulation and bribery have been extensively studied under
weighted voting, there has been almost no work done on election control under
weighted voting. This is unfortunate, since weighted voting appears in many
important natural settings. In this paper, we study the complexity of
controlling the outcome of weighted elections through adding and deleting
voters. We obtain polynomial-time algorithms, NP-completeness results, and for
many NP-complete cases, approximation algorithms. In particular, for scoring
rules we completely characterize the complexity of weighted voter control. Our
work shows that for quite a few important cases, either polynomial-time exact
algorithms or polynomial-time approximation algorithms exist.


Almost-Everywhere Superiority for Quantum Computing

  Simon as extended by Brassard and H{\o}yer shows that there are tasks on
which polynomial-time quantum machines are exponentially faster than each
classical machine infinitely often. The present paper shows that there are
tasks on which polynomial-time quantum machines are exponentially faster than
each classical machine almost everywhere.


Writing and Editing Complexity Theory: Tales and Tools

  Each researcher should have a full shelf---physical or virtual---of books on
writing and editing prose. Though we make no claim to any special degree of
expertise, we recently edited a book of complexity theory surveys (Complexity
Theory Retrospective II, Springer-Verlag, 1997), and in doing so we were
brought into particularly close contact with the subject of this article, and
with a number of the excellent resources available to writers and editors. In
this article, we list some of these resources, and we also relate some of the
adventures we had as our book moved from concept to reality.


A Moment of Perfect Clarity I: The Parallel Census Technique

  We discuss the history and uses of the parallel census technique---an elegant
tool in the study of certain computational objects having polynomially bounded
census functions. A sequel will discuss advances (including Cai, Naik, and
Sivakumar [CNS95] and Glasser [Gla00]), some related to the parallel census
technique and some due to other approaches, in the complexity-class collapses
that follow if NP has sparse hard sets under reductions weaker than (full)
truth-table reductions.


Computational Social Choice and Computational Complexity: BFFs?

  We discuss the connection between computational social choice (comsoc) and
computational complexity. We stress the work so far on, and urge continued
focus on, two less-recognized aspects of this connection. Firstly, this is very
much a two-way street: Everyone knows complexity classification is used in
comsoc, but we also highlight benefits to complexity that have arisen from its
use in comsoc. Secondly, more subtle, less-known complexity tools often can be
very productively used in comsoc.


P-Selectivity, Immunity, and the Power of One Bit

  We prove that P-sel, the class of all P-selective sets, is EXP-immune, but is
not EXP/1-immune. That is, we prove that some infinite P-selective set has no
infinite EXP-time subset, but we also prove that every infinite P-selective set
has some infinite subset in EXP/1. Informally put, the immunity of P-sel is so
fragile that it is pierced by a single bit of information.
  The above claims follow from broader results that we obtain about the
immunity of the P-selective sets. In particular, we prove that for every
recursive function f, P-sel is DTIME(f)-immune. Yet we also prove that P-sel is
not \Pi_2^p/1-immune.


Frequency of Correctness versus Average-Case Polynomial Time and
  Generalized Juntas

  We prove that every distributional problem solvable in polynomial time on the
average with respect to the uniform distribution has a frequently
self-knowingly correct polynomial-time algorithm. We also study some features
of probability weight of correctness with respect to generalizations of
Procaccia and Rosenschein's junta distributions [PR07b].


Beautiful Structures: An Appreciation of the Contributions of Alan
  Selman

  Professor Alan Selman has been a giant in the field of computational
complexity for the past forty years. This article is an appreciation, on the
occasion of his retirement, of some of the most lovely concepts and results
that Alan has contributed to the field.


Overhead-Free Computation, DCFLs, and CFLs

  We study Turing machines that are allowed absolutely no space overhead. The
only work space the machines have, beyond the fixed amount of memory implicit
in their finite-state control, is that which they can create by cannibalizing
the input bits' own space. This model more closely reflects the fixed-sized
memory of real computers than does the standard complexity-theoretic model of
linear space.
  Though some context-sensitive languages cannot be accepted by such machines,
we show that all context-free languages can be accepted nondeterministically in
polynomial time with absolutely no space overhead, and that all deterministic
context-free languages can be accepted deterministically in polynomial time
with absolutely no space overhead.


The Complexity of Kings

  A king in a directed graph is a node from which each node in the graph can be
reached via paths of length at most two. There is a broad literature on
tournaments (completely oriented digraphs), and it has been known for more than
half a century that all tournaments have at least one king [Lan53]. Recently,
kings have proven useful in theoretical computer science, in particular in the
study of the complexity of the semifeasible sets [HNP98,HT05] and in the study
of the complexity of reachability problems [Tan01,NT02].
  In this paper, we study the complexity of recognizing kings. For each
succinctly specified family of tournaments, the king problem is known to belong
to $\Pi_2^p$ [HOZZ]. We prove that this bound is optimal: We construct a
succinctly specified tournament family whose king problem is
$\Pi_2^p$-complete. It follows easily from our proof approach that the problem
of testing kingship in succinctly specified graphs (which need not be
tournaments) is $\Pi_2^p$-complete. We also obtain $\Pi_2^p$-completeness
results for k-kings in succinctly specified j-partite tournaments, $k,j \geq
2$, and we generalize our main construction to show that $\Pi_2^p$-completeness
holds for testing k-kingship in succinctly specified families of tournaments
for all $k \geq 2$.


On Approximating Optimal Weighted Lobbying, and Frequency of Correctness
  versus Average-Case Polynomial Time

  We investigate issues related to two hard problems related to voting, the
optimal weighted lobbying problem and the winner problem for Dodgson elections.
Regarding the former, Christian et al. [CFRS06] showed that optimal lobbying is
intractable in the sense of parameterized complexity. We provide an efficient
greedy algorithm that achieves a logarithmic approximation ratio for this
problem and even for a more general variant--optimal weighted lobbying. We
prove that essentially no better approximation ratio than ours can be proven
for this greedy algorithm.
  The problem of determining Dodgson winners is known to be complete for
parallel access to NP [HHR97]. Homan and Hemaspaandra [HH06] proposed an
efficient greedy heuristic for finding Dodgson winners with a guaranteed
frequency of success, and their heuristic is a ``frequently self-knowingly
correct algorithm.'' We prove that every distributional problem solvable in
polynomial time on the average with respect to the uniform distribution has a
frequently self-knowingly correct polynomial-time algorithm. Furthermore, we
study some features of probability weight of correctness with respect to
Procaccia and Rosenschein's junta distributions [PR07].


Dichotomy for Voting Systems

  Scoring protocols are a broad class of voting systems. Each is defined by a
vector $(\alpha_1,\alpha_2,...,\alpha_m)$, $\alpha_1 \geq \alpha_2 \geq >...
\geq \alpha_m$, of integers such that each voter contributes $\alpha_1$ points
to his/her first choice, $\alpha_2$ points to his/her second choice, and so on,
and any candidate receiving the most points is a winner.
  What is it about scoring-protocol election systems that makes some have the
desirable property of being NP-complete to manipulate, while others can be
manipulated in polynomial time? We find the complete, dichotomizing answer:
Diversity of dislike. Every scoring-protocol election system having two or more
point values assigned to candidates other than the favorite--i.e., having
$||\{\alpha_i \condition 2 \leq i \leq m\}||\geq 2$--is NP-complete to
manipulate. Every other scoring-protocol election system can be manipulated in
polynomial time. In effect, we show that--other than trivial systems (where all
candidates alway tie), plurality voting, and plurality voting's transparently
disguised translations--\emph{every} scoring-protocol election system is
NP-complete to manipulate.


Hybrid Elections Broaden Complexity-Theoretic Resistance to Control

  Electoral control refers to attempts by an election's organizer ("the chair")
to influence the outcome by adding/deleting/partitioning voters or candidates.
The groundbreaking work of Bartholdi, Tovey, and Trick [BTT92] on
(constructive) control proposes computational complexity as a means of
resisting control attempts: Look for election systems where the chair's task in
seeking control is itself computationally infeasible.
  We introduce and study a method of combining two or more candidate-anonymous
election schemes in such a way that the combined scheme possesses all the
resistances to control (i.e., all the NP-hardnesses of control) possessed by
any of its constituents: It combines their strengths. From this and new
resistance constructions, we prove for the first time that there exists an
election scheme that is resistant to all twenty standard types of electoral
control.


Control in the Presence of Manipulators: Cooperative and Competitive
  Cases

  Control and manipulation are two of the most studied types of attacks on
elections. In this paper, we study the complexity of control attacks on
elections in which there are manipulators. We study both the case where the
"chair" who is seeking to control the election is allied with the manipulators,
and the case where the manipulators seek to thwart the chair. In the latter
case, we see that the order of play substantially influences the complexity. We
prove upper bounds, holding over every election system with a polynomial-time
winner problem, for all standard control cases, and some of these bounds are at
the second or third level of the polynomial hierarchy, and we provide matching
lower bounds to prove these tight. Nonetheless, for important natural systems
the complexity can be much lower. We prove that for approval and plurality
elections, the complexity of even competitive clashes between a controller and
manipulators falls far below those high bounds, even as low as polynomial time.
Yet for a Borda-voting case we show that such clashes raise the complexity
unless NP = coNP.


Algebraic Properties for Selector Functions

  The nondeterministic advice complexity of the P-selective sets is known to be
exactly linear. Regarding the deterministic advice complexity of the
P-selective sets--i.e., the amount of Karp--Lipton advice needed for
polynomial-time machines to recognize them in general--the best current upper
bound is quadratic [Ko, 1983] and the best current lower bound is linear
[Hemaspaandra and Torenvliet, 1996].
  We prove that every associatively P-selective set is commutatively,
associatively P-selective. Using this, we establish an algebraic sufficient
condition for the P-selective sets to have a linear upper bound (which thus
would match the existing lower bound) on their deterministic advice complexity:
If all P-selective sets are associatively P-selective then the deterministic
advice complexity of the P-selective sets is linear. The weakest previously
known sufficient condition was P=NP.
  We also establish related results for algebraic properties of, and advice
complexity of, the nondeterministically selective sets.


Search versus Decision for Election Manipulation Problems

  Most theoretical definitions about the complexity of manipulating elections
focus on the decision problem of recognizing which instances can be
successfully manipulated, rather than the search problem of finding the
successful manipulative actions. Since the latter is a far more natural goal
for manipulators, that definitional focus may be misguided if these two
complexities can differ. Our main result is that they probably do differ: If
integer factoring is hard, then for election manipulation, election bribery,
and some types of election control, there are election systems for which
recognizing which instances can be successfully manipulated is in polynomial
time but producing the successful manipulations cannot be done in polynomial
time.


X THEN X: Manipulation of Same-System Runoff Elections

  Do runoff elections, using the same voting rule as the initial election but
just on the winning candidates, increase or decrease the complexity of
manipulation? Does allowing revoting in the runoff increase or decrease the
complexity relative to just having a runoff without revoting? For both weighted
and unweighted voting, we show that even for election systems with simple
winner problems the complexity of manipulation, manipulation with runoffs, and
manipulation with revoting runoffs are independent, in the abstract. On the
other hand, for some important, well-known election systems we determine what
holds for each of these cases. For no such systems do we find runoffs lowering
complexity, and for some we find that runoffs raise complexity. Ours is the
first paper to show that for natural, unweighted election systems, runoffs can
increase the manipulation complexity.


Credimus

  We believe that economic design and computational complexity---while already
important to each other---should become even more important to each other with
each passing year. But for that to happen, experts in on the one hand such
areas as social choice, economics, and political science and on the other hand
computational complexity will have to better understand each other's
worldviews.
  This article, written by two complexity theorists who also work in
computational social choice theory, focuses on one direction of that process by
presenting a brief overview of how most computational complexity theorists view
the world. Although our immediate motivation is to make the lens through which
complexity theorists see the world be better understood by those in the social
sciences, we also feel that even within computer science it is very important
for nontheoreticians to understand how theoreticians think, just as it is
equally important within computer science for theoreticians to understand how
nontheoreticians think.


A Downward Collapse within the Polynomial Hierarchy

  Downward collapse (a.k.a. upward separation) refers to cases where the
equality of two larger classes implies the equality of two smaller classes. We
provide an unqualified downward collapse result completely within the
polynomial hierarchy. In particular, we prove that, for k > 2, if $\psigkone =
\psigktwo$ then $\sigmak = \pik = \ph$. We extend this to obtain a more general
downward collapse result.


What's Up with Downward Collapse: Using the Easy-Hard Technique to Link
  Boolean and Polynomial Hierarchy Collapses

  During the past decade, nine papers have obtained increasingly strong
consequences from the assumption that boolean or bounded-query hierarchies
collapse. The final four papers of this nine-paper progression actually achieve
downward collapse---that is, they show that high-level collapses induce
collapses at (what beforehand were thought to be) lower complexity levels. For
example, for each $k\geq 2$ it is now known that if $\psigkone=\psigktwo$ then
$\ph=\sigmak$. This article surveys the history, the results, and the
technique---the so-called easy-hard method---of these nine papers.


An Introduction to Query Order

  Hemaspaandra, Hempel, and Wechsung [cs.CC/9909020] raised the following
questions: If one is allowed one question to each of two different information
sources, does the order in which one asks the questions affect the class of
problems that one can solve with the given access? If so, which order yields
the greater computational power?
  The answers to these questions have been learned-inasfar as they can be
learned without resolving whether or not the polynomial hierarchy collapses-for
both the polynomial hierarchy and the boolean hierarchy. In the polynomial
hierarchy, query order never matters. In the boolean hierarchy, query order
sometimes does not matter and, unless the polynomial hierarchy collapses,
sometimes does matter. Furthermore, the study of query order has yielded
dividends in seemingly unrelated areas, such as bottleneck computations and
downward translation of equality.
  In this article, we present some of the central results on query order. The
article is written in such a way as to encourage the reader to try his or her
own hand at proving some of these results. We also give literature pointers to
the quickly growing set of related results and applications.


Query Order and the Polynomial Hierarchy

  Hemaspaandra, Hempel, and Wechsung [cs.CC/9909020] initiated the field of
query order, which studies the ways in which computational power is affected by
the order in which information sources are accessed. The present paper studies,
for the first time, query order as it applies to the levels of the polynomial
hierarchy. We prove that the levels of the polynomial hierarchy are
order-oblivious. Yet, we also show that these ordered query classes form new
levels in the polynomial hierarchy unless the polynomial hierarchy collapses.
We prove that all leaf language classes - and thus essentially all standard
complexity classes - inherit all order-obliviousness results that hold for P.


Using the No-Search Easy-Hard Technique for Downward Collapse

  The top part of the preceding figure [figure appears in actual paper] shows
some classes from the (truth-table) bounded-query and boolean hierarchies. It
is well-known that if either of these hierarchies collapses at a given level,
then all higher levels of that hierarchy collapse to that same level. This is a
standard ``upward translation of equality'' that has been known for over a
decade. The issue of whether these hierarchies can translate equality {\em
downwards\/} has proven vastly more challenging. In particular, with regard to
the figure above, consider the following claim:
  $$P_{m-tt}^{\Sigma_k^p} = P_{m+1-tt}^{\Sigma_k^p} \implies
  DIFF_m(\Sigma_k^p) coDIFF_m(\Sigma_k^p) = BH(\Sigma_k^p). (*) $$
  This claim, if true, says that equality translates downwards between levels
of the bounded-query hierarchy and the boolean hierarchy levels that (before
the fact) are immediately below them.
  Until recently, it was not known whether (*) {\em ever\/} held, except for
the degenerate cases $m=0$ and $k=0$. Then Hemaspaandra, Hemaspaandra, and
Hempel \cite{hem-hem-hem:j:downward-translation} proved that (*) holds for all
$m$, for $k > 2$. Buhrman and Fortnow~\cite{buh-for:j:two-queries} then showed
that, when $k=2$, (*) holds for the case $m = 1$. In this paper, we prove that
for the case $k=2$, (*) holds for all values of $m$. Since there is an oracle
relative to which ``for $k=1$, (*) holds for all $m$'' fails
\cite{buh-for:j:two-queries}, our achievement of the $k=2$ case cannot to be
strengthened to $k=1$ by any relativizable proof technique. The new downward
translation we obtain also tightens the collapse in the polynomial hierarchy
implied by a collapse in the bounded-query hierarchy of the second level of the
polynomial hierarchy.


A Control Dichotomy for Pure Scoring Rules

  Scoring systems are an extremely important class of election systems. A
length-$m$ (so-called) scoring vector applies only to $m$-candidate elections.
To handle general elections, one must use a family of vectors, one per length.
The most elegant approach to making sure such families are "family-like" is the
recently introduced notion of (polynomial-time uniform) pure scoring rules
[Betzler and Dorn 2010], where each scoring vector is obtained from its
precursor by adding one new coefficient. We obtain the first dichotomy theorem
for pure scoring rules for a control problem. In particular, for constructive
control by adding voters (CCAV), we show that CCAV is solvable in polynomial
time for $k$-approval with $k \leq 3$, $k$-veto with $k \leq 2$, every pure
scoring rule in which only the two top-rated candidates gain nonzero scores,
and a particular rule that is a "hybrid" of 1-approval and 1-veto. For all
other pure scoring rules, CCAV is NP-complete. We also investigate the
descriptive richness of different models for defining pure scoring rules,
proving how more rule-generation time gives more rules, proving that rationals
give more rules than do the natural numbers, and proving that some restrictions
previously thought to be "w.l.o.g." in fact do lose generality.


Anyone but Him: The Complexity of Precluding an Alternative

  Preference aggregation in a multiagent setting is a central issue in both
human and computer contexts. In this paper, we study in terms of complexity the
vulnerability of preference aggregation to destructive control. That is, we
study the ability of an election's chair to, through such mechanisms as
voter/candidate addition/suppression/partition, ensure that a particular
candidate (equivalently, alternative) does not win. And we study the extent to
which election systems can make it impossible, or computationally costly
(NP-complete), for the chair to execute such control. Among the systems we
study--plurality, Condorcet, and approval voting--we find cases where systems
immune or computationally resistant to a chair choosing the winner nonetheless
are vulnerable to the chair blocking a victory. Beyond that, we see that among
our studied systems no one system offers the best protection against
destructive control. Rather, the choice of a preference aggregation system will
depend closely on which types of control one wishes to be protected against. We
also find concrete cases where the complexity of or susceptibility to control
varies dramatically based on the choice among natural tie-handling rules.


How Hard Is Bribery in Elections?

  We study the complexity of influencing elections through bribery: How
computationally complex is it for an external actor to determine whether by a
certain amount of bribing voters a specified candidate can be made the
election's winner? We study this problem for election systems as varied as
scoring protocols and Dodgson voting, and in a variety of settings regarding
homogeneous-vs.-nonhomogeneous electorate bribability,
bounded-size-vs.-arbitrary-sized candidate sets, weighted-vs.-unweighted
voters, and succinct-vs.-nonsuccinct input specification. We obtain both
polynomial-time bribery algorithms and proofs of the intractability of bribery,
and indeed our results show that the complexity of bribery is extremely
sensitive to the setting. For example, we find settings in which bribery is
NP-complete but manipulation (by voters) is in P, and we find settings in which
bribing weighted voters is NP-complete but bribing voters with individual bribe
thresholds is in P. For the broad class of elections (including plurality,
Borda, k-approval, and veto) known as scoring protocols, we prove a dichotomy
result for bribery of weighted voters: We find a simple-to-evaluate condition
that classifies every case as either NP-complete or in P.


The Robustness of LWPP and WPP, with an Application to Graph
  Reconstruction

  We show that the counting class LWPP [FFK94] remains unchanged even if one
allows a polynomial number of gap values rather than one. On the other hand, we
show that it is impossible to improve this from polynomially many gap values to
a superpolynomial number of gap values by relativizable proof techniques.
  The first of these results implies that the Legitimate Deck Problem (from the
study of graph reconstruction) is in LWPP (and thus low for PP, i.e., $\rm
PP^{\mbox{Legitimate Deck}} = PP$) if the weakened version of the
Reconstruction Conjecture holds in which the number of nonisomorphic preimages
is assumed merely to be polynomially bounded. This strengthens the 1992 result
of K\"{o}bler, Sch\"{o}ning, and Tor\'{a}n [KST92] that the Legitimate Deck
Problem is in LWPP if the Reconstruction Conjecture holds, and provides
strengthened evidence that the Legitimate Deck Problem is not NP-hard.
  We additionally show on the one hand that our main LWPP robustness result
also holds for WPP, and also holds even when one allows both the rejection- and
acceptance- gap-value targets to simultaneously be polynomial-sized lists; yet
on the other hand, we show that for the #P-based analog of LWPP the behavior
much differs in that, in some relativized worlds, even two target values
already yield a richer class than one value does. Despite that nonrobustness
result for a #P-based class, we show that the #P-based "exact counting" class
$\rm C_{=}P$ remains unchanged even if one allows a polynomial number of target
values for the number of accepting paths of the machine.


Recursion-Theoretic Ranking and Compression

  For which sets A does there exist a mapping, computed by a total or partial
recursive function, such that the mapping, when its domain is restricted to A,
is a 1-to-1, onto mapping to $\Sigma^*$? And for which sets A does there exist
such a mapping that respects the lexicographical ordering within A? Both cases
are types of perfect, minimal hash functions. The complexity-theoretic versions
of these notions are known as compression functions and ranking functions. The
present paper defines and studies the recursion-theoretic versions of
compression and ranking functions, and in particular studies the question of
which sets have, or lack, such functions. Thus, this is a case where, in
contrast to the usual direction of notion transferal, notions from complexity
theory are inspiring notions, and an investigation, in computability theory.
  We show that the rankable and compressible sets broadly populate the
1-truth-table degrees, and we prove that every nonempty coRE cylinder is
recursively compressible.


Complexity Results in Graph Reconstruction

  We investigate the relative complexity of the graph isomorphism problem (GI)
and problems related to the reconstruction of a graph from its vertex-deleted
or edge-deleted subgraphs (in particular, deck checking (DC) and legitimate
deck (LD) problems). We show that these problems are closely related for all
amounts $c \geq 1$ of deletion:
  1) $GI \equiv^{l}_{iso} VDC_{c}$, $GI \equiv^{l}_{iso} EDC_{c}$, $GI
\leq^{l}_{m} LVD_c$, and $GI \equiv^{p}_{iso} LED_c$.
  2) For all $k \geq 2$, $GI \equiv^{p}_{iso} k-VDC_c$ and $GI \equiv^{p}_{iso}
k-EDC_c$.
  3) For all $k \geq 2$, $GI \leq^{l}_{m} k-LVD_c$.
  4)$GI \equiv^{p}_{iso} 2-LVC_c$.
  5) For all $k \geq 2$, $GI \equiv^{p}_{iso} k-LED_c$.
  For many of these results, even the $c = 1$ case was not previously known.
  Similar to the definition of reconstruction numbers $vrn_{\exists}(G)$ [HP85]
and $ern_{\exists}(G)$ (see page 120 of [LS03]), we introduce two new graph
parameters, $vrn_{\forall}(G)$ and $ern_{\forall}(G)$, and give an example of a
family $\{G_n\}_{n \geq 4}$ of graphs on $n$ vertices for which
$vrn_{\exists}(G_n) < vrn_{\forall}(G_n)$. For every $k \geq 2$ and $n \geq 1$,
we show that there exists a collection of $k$ graphs on $(2^{k-1}+1)n+k$
vertices with $2^{n}$ 1-vertex-preimages, i.e., one has families of graph
collections whose number of 1-vertex-preimages is huge relative to the size of
the graphs involved.


Llull and Copeland Voting Computationally Resist Bribery and Control

  The only systems previously known to be resistant to all the standard control
types were highly artificial election systems created by hybridization. We
study a parameterized version of Copeland voting, denoted by Copeland^\alpha,
where the parameter \alpha is a rational number between 0 and 1 that specifies
how ties are valued in the pairwise comparisons of candidates. We prove that
Copeland^{0.5}, the system commonly referred to as "Copeland voting," provides
full resistance to constructive control, and we prove the same for
Copeland^\alpha, for all rational \alpha, 0 < \alpha < 1. Copeland voting is
the first natural election system proven to have full resistance to
constructive control. We also prove that both Copeland^1 (Llull elections) and
Copeland^0 are resistant to all standard types of constructive control other
than one variant of addition of candidates. Moreover, we show that for each
rational \alpha, 0 \leq \alpha \leq 1, Copeland^\alpha voting is fully
resistant to bribery attacks, and we establish fixed-parameter tractability of
bounded-case control for Copeland^\alpha. We also study Copeland^\alpha
elections under more flexible models such as microbribery and extended control
and we integrate the potential irrationality of voter preferences into many of
our results.


Guarantees for the Success Frequency of an Algorithm for Finding
  Dodgson-Election Winners

  In the year 1876 the mathematician Charles Dodgson, who wrote fiction under
the now more famous name of Lewis Carroll, devised a beautiful voting system
that has long fascinated political scientists. However, determining the winner
of a Dodgson election is known to be complete for the \Theta_2^p level of the
polynomial hierarchy. This implies that unless P=NP no polynomial-time solution
to this problem exists, and unless the polynomial hierarchy collapses to NP the
problem is not even in NP. Nonetheless, we prove that when the number of voters
is much greater than the number of candidates--although the number of voters
may still be polynomial in the number of candidates--a simple greedy algorithm
very frequently finds the Dodgson winners in such a way that it ``knows'' that
it has found them, and furthermore the algorithm never incorrectly declares a
nonwinner to be a winner.


More Natural Models of Electoral Control by Partition

  "Control" studies attempts to set the outcome of elections through the
addition, deletion, or partition of voters or candidates. The set of benchmark
control types was largely set in the seminal 1992 paper by Bartholdi, Tovey,
and Trick that introduced control, and there now is a large literature studying
how many of the benchmark types various election systems are vulnerable to,
i.e., have polynomial-time attack algorithms for.
  However, although the longstanding benchmark models of addition and deletion
model relatively well the real-world settings that inspire them, the
longstanding benchmark models of partition model settings that are arguably
quite distant from those they seek to capture.
  In this paper, we introduce--and for some important cases analyze the
complexity of--new partition models that seek to better capture many real-world
partition settings. In particular, in many partition settings one wants the two
parts of the partition to be of (almost) equal size, or is partitioning into
more than two parts, or has groups of actors who must be placed in the same
part of the partition. Our hope is that having these new partition types will
allow studies of control attacks to include such models that more realistically
capture many settings.


Open Questions in the Theory of Semifeasible Computation

  The study of semifeasible algorithms was initiated by Selman's work a quarter
of century ago [Sel79,Sel81,Sel82]. Informally put, this research stream
studies the power of those sets L for which there is a deterministic (or in
some cases, the function may belong to one of various nondeterministic function
classes) polynomial-time function f such that when at least one of x and y
belongs to L, then f(x,y) \in L \cap \{x,y\}. The intuition here is that it is
saying: ``Regarding membership in L, if you put a gun to my head and forced me
to bet on one of x or y as belonging to L, my money would be on f(x,y).''
  In this article, we present a number of open problems from the theory of
semifeasible algorithms. For each we present its background and review what
partial results, if any, are known.


Barbosa, Uniform Polynomial Time Bounds, and Promises

  This note is a commentary on, and critique of, Andre Luiz Barbosa's paper
entitled "P != NP Proof." Despite its provocative title, what the paper is
seeking to do is not to prove P \neq NP in the standard sense in which that
notation is used in the literature. Rather, Barbosa is (and is aware that he
is) arguing that a different meaning should be associated with the notation P
\neq NP, and he claims to prove the truth of the statement P \neq NP in his
quite different sense of that statement. However, we note that (1) the paper
fails even on its own terms, as due to a uniformity problem, the paper's proof
does not establish, even in its unusual sense of the notation, that P \neq NP;
and (2) what the paper means by the claim P \neq NP in fact implies that P \neq
NP holds even under the standard meaning that that notation has in the
literature (and so it is exceedingly unlikely that Barbosa's proof can be fixed
any time soon).


Robust Reductions

  We continue the study of robust reductions initiated by Gavalda and Balcazar.
In particular, a 1991 paper of Gavalda and Balcazar claimed an optimal
separation between the power of robust and nondeterministic strong reductions.
Unfortunately, their proof is invalid. We re-establish their theorem.
  Generalizing robust reductions, we note that robustly strong reductions are
built from two restrictions, robust underproductivity and robust
overproductivity, both of which have been separately studied before in other
contexts. By systematically analyzing the power of these reductions, we explore
the extent to which each restriction weakens the power of reductions. We show
that one of these reductions yields a new, strong form of the Karp-Lipton
Theorem.


The Consequences of Eliminating NP Solutions

  Given a function based on the computation of an NP machine, can one in
general eliminate some solutions? That is, can one in general decrease the
ambiguity? This simple question remains, even after extensive study by many
researchers over many years, mostly unanswered. However, complexity-theoretic
consequences and enabling conditions are known. In this tutorial-style article
we look at some of those, focusing on the most natural framings: reducing the
number of solutions of NP functions, refining the solutions of NP functions,
and subtracting from or otherwise shrinking #P functions. We will see how small
advice strings are important here, but we also will see how increasing advice
size to achieve robustness is central to the proof of a key ambiguity-reduction
result for NP functions.


An Atypical Survey of Typical-Case Heuristic Algorithms

  Heuristic approaches often do so well that they seem to pretty much always
give the right answer. How close can heuristic algorithms get to always giving
the right answer, without inducing seismic complexity-theoretic consequences?
This article first discusses how a series of results by Berman, Buhrman,
Hartmanis, Homer, Longpr\'{e}, Ogiwara, Sch\"{o}ening, and Watanabe, from the
early 1970s through the early 1990s, explicitly or implicitly limited how well
heuristic algorithms can do on NP-hard problems. In particular, many desirable
levels of heuristic success cannot be obtained unless severe, highly unlikely
complexity class collapses occur. Second, we survey work initiated by Goldreich
and Wigderson, who showed how under plausible assumptions deterministic
heuristics for randomized computation can achieve a very high frequency of
correctness. Finally, we consider formal ways in which theory can help explain
the effectiveness of heuristics that solve NP-hard problems in practice.


Schulze and Ranked-Pairs Voting are Fixed-Parameter Tractable to Bribe,
  Manipulate, and Control

  Schulze and ranked-pairs elections have received much attention recently, and
the former has quickly become a quite widely used election system. For many
cases these systems have been proven resistant to bribery, control, or
manipulation, with ranked pairs being particularly praised for being NP-hard
for all three of those. Nonetheless, the present paper shows that with respect
to the number of candidates, Schulze and ranked-pairs elections are
fixed-parameter tractable to bribe, control, and manipulate: we obtain uniform,
polynomial-time algorithms whose degree does not depend on the number of
candidates. We also provide such algorithms for some weighted variants of these
problems.


Creating Strong Total Commutative Associative Complexity-Theoretic
  One-Way Functions from Any Complexity-Theoretic One-Way Function

  Rabi and Sherman [RS97] presented novel digital signature and unauthenticated
secret-key agreement protocols, developed by themselves and by Rivest and
Sherman. These protocols use ``strong,'' total, commutative (in the case of
multi-party secret-key agreement), associative one-way functions as their key
building blocks. Though Rabi and Sherman did prove that associative one-way
functions exist if $\p \neq \np$, they left as an open question whether any
natural complexity-theoretic assumption is sufficient to ensure the existence
of ``strong,'' total, commutative, associative one-way functions. In this
paper, we prove that if $\p \neq \np$ then ``strong,'' total, commutative,
associative one-way functions exist.


Exact Analysis of Dodgson Elections: Lewis Carroll's 1876 Voting System
  is Complete for Parallel Access to NP

  In 1876, Lewis Carroll proposed a voting system in which the winner is the
candidate who with the fewest changes in voters' preferences becomes a
Condorcet winner---a candidate who beats all other candidates in pairwise
majority-rule elections. Bartholdi, Tovey, and Trick provided a lower
bound---NP-hardness---on the computational complexity of determining the
election winner in Carroll's system. We provide a stronger lower bound and an
upper bound that matches our lower bound. In particular, determining the winner
in Carroll's system is complete for parallel access to NP, i.e., it is complete
for $\thetatwo$, for which it becomes the most natural complete problem known.
It follows that determining the winner in Carroll's elections is not
NP-complete unless the polynomial hierarchy collapses.


Translating Equality Downwards

  Downward translation of equality refers to cases where a collapse of some
pair of complexity classes would induce a collapse of some other pair of
complexity classes that (a priori) one expects are smaller. Recently, the first
downward translation of equality was obtained that applied to the polynomial
hierarchy-in particular, to bounded access to its levels [cs.CC/9910007]. In
this paper, we provide a much broader downward translation that extends not
only that downward translation but also that translation's elegant enhancement
by Buhrman and Fortnow. Our work also sheds light on previous research on the
structure of refined polynomial hierarchies, and strengthens the connection
between the collapse of bounded query hierarchies and the collapse of the
polynomial hierarchy.


Multimode Control Attacks on Elections

  In 1992, Bartholdi, Tovey, and Trick opened the study of control attacks on
elections---attempts to improve the election outcome by such actions as
adding/deleting candidates or voters. That work has led to many results on how
algorithms can be used to find attacks on elections and how
complexity-theoretic hardness results can be used as shields against attacks.
However, all the work in this line has assumed that the attacker employs just a
single type of attack. In this paper, we model and study the case in which the
attacker launches a multipronged (i.e., multimode) attack. We do so to more
realistically capture the richness of real-life settings. For example, an
attacker might simultaneously try to suppress some voters, attract new voters
into the election, and introduce a spoiler candidate. Our model provides a
unified framework for such varied attacks, and by constructing polynomial-time
multiprong attack algorithms we prove that for various election systems even
such concerted, flexible attacks can be perfectly planned in deterministic
polynomial time.


