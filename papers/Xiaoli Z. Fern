Multi-Label Classifier Chains for Bird Sound

  Bird sound data collected with unattended microphones for automatic surveys,or mobile devices for citizen science, typically contain multiplesimultaneously vocalizing birds of different species. However, few works haveconsidered the multi-label structure in birdsong. We propose to use an ensembleof classifier chains combined with a histogram-of-segments representation formulti-label classification of birdsong. The proposed method is compared withbinary relevance and three multi-instance multi-label learning (MIML)algorithms from prior work (which focus more on structure in the sound, andless on structure in the label sets). Experiments are conducted on tworeal-world birdsong datasets, and show that the proposed method usuallyoutperforms binary relevance (using the same features and base-classifier), andis better in some cases and worse in others compared to the MIML algorithms.

Active Metric Learning from Relative Comparisons

  This work focuses on active learning of distance metrics from relativecomparison information. A relative comparison specifies, for a data pointtriplet $(x_i,x_j,x_k)$, that instance $x_i$ is more similar to $x_j$ than to$x_k$. Such constraints, when available, have been shown to be useful towarddefining appropriate distance metrics. In real-world applications, acquiringconstraints often require considerable human effort. This motivates us to studyhow to select and query the most useful relative comparisons to achieveeffective metric learning with minimum user effort. Given an underlying classconcept that is employed by the user to provide such constraints, we present aninformation-theoretic criterion that selects the triplet whose answer leads tothe highest expected gain in information about the classes of a set ofexamples. Directly applying the proposed criterion requires examining $O(n^3)$triplets with $n$ instances, which is prohibitive even for datasets of moderatesize. We show that a randomized selection strategy can be used to reduce theselection pool from $O(n^3)$ to $O(n)$, allowing us to scale up to larger-sizeproblems. Experiments show that the proposed method consistently outperformstwo baseline policies.

Dynamic Programming for Instance Annotation in Multi-instance  Multi-label Learning

  Labeling data for classification requires significant human effort. To reducelabeling cost, instead of labeling every instance, a group of instances (bag)is labeled by a single bag label. Computer algorithms are then used to inferthe label for each instance in a bag, a process referred to as instanceannotation. This task is challenging due to the ambiguity regarding theinstance labels. We propose a discriminative probabilistic model for theinstance annotation problem and introduce an expectation maximization frameworkfor inference, based on the maximum likelihood approach. For many probabilisticapproaches, brute-force computation of the instance label posterior probabilitygiven its bag label is exponential in the number of instances in the bag. Ourkey contribution is a dynamic programming method for computing the posteriorthat is linear in the number of instances. We evaluate our methods using bothbenchmark and real world data sets, in the domain of bird song, imageannotation, and activity recognition. In many cases, the proposed frameworkoutperforms, sometimes significantly, the current state-of-the-art MIMLlearning methods, both in instance label prediction and bag label prediction.

Confidence-Constrained Maximum Entropy Framework for Learning from  Multi-Instance Data

  Multi-instance data, in which each object (bag) contains a collection ofinstances, are widespread in machine learning, computer vision, bioinformatics,signal processing, and social sciences. We present a maximum entropy (ME)framework for learning from multi-instance data. In this approach each bag isrepresented as a distribution using the principle of ME. We introduce theconcept of confidence-constrained ME (CME) to simultaneously learn thestructure of distribution space and infer each distribution. The sharedstructure underlying each density is used to learn from instances inside eachbag. The proposed CME is free of tuning parameters. We devise a fastoptimization algorithm capable of handling large scale multi-instance data. Inthe experimental section, we evaluate the performance of the proposed approachin terms of exact rank recovery in the space of distributions and compare itwith the regularized ME approach. Moreover, we compare the performance of CMEwith Multi-Instance Learning (MIL) state-of-the-art algorithms and show acomparable performance in terms of accuracy with reduced computationalcomplexity.

DR-BiLSTM: Dependent Reading Bidirectional LSTM for Natural Language  Inference

  We present a novel deep learning architecture to address the natural languageinference (NLI) task. Existing approaches mostly rely on simple readingmechanisms for independent encoding of the premise and hypothesis. Instead, wepropose a novel dependent reading bidirectional LSTM network (DR-BiLSTM) toefficiently model the relationship between a premise and a hypothesis duringencoding and inference. We also introduce a sophisticated ensemble strategy tocombine our proposed models, which noticeably improves final predictions.Finally, we demonstrate how the results can be improved further with anadditional preprocessing step. Our evaluation shows that DR-BiLSTM obtains thebest single model and ensemble model results achieving the new state-of-the-artscores on the Stanford NLI dataset.

Event Nugget Detection with Forward-Backward Recurrent Neural Networks

  Traditional event detection methods heavily rely on manually engineered richfeatures. Recent deep learning approaches alleviate this problem by automaticfeature engineering. But such efforts, like tradition methods, have so far onlyfocused on single-token event mentions, whereas in practice events can also bea phrase. We instead use forward-backward recurrent neural networks (FBRNNs) todetect events that can be either words or phrases. To the best our knowledge,this is one of the first efforts to handle multi-word events and also the firstattempt to use RNNs for event detection. Experimental results demonstrate thatFBRNN is competitive with the state-of-the-art methods on the ACE 2005 and theRich ERE 2015 event detection tasks.

Dependent Gated Reading for Cloze-Style Question Answering

  We present a novel deep learning architecture to address the cloze-stylequestion answering task. Existing approaches employ reading mechanisms that donot fully exploit the interdependency between the document and the query. Inthis paper, we propose a novel \emph{dependent gated reading} bidirectional GRUnetwork (DGR) to efficiently model the relationship between the document andthe query during encoding and decision making. Our evaluation shows that DGRobtains highly competitive performance on well-known machine comprehensionbenchmarks such as the Children's Book Test (CBT-NE and CBT-CN) and Who DiDWhat (WDW, Strict and Relaxed). Finally, we extensively analyze and validateour model by ablation and attention studies.

Joint Neural Entity Disambiguation with Output Space Search

  In this paper, we present a novel model for entity disambiguation thatcombines both local contextual information and global evidences through LimitedDiscrepancy Search (LDS). Given an input document, we start from a completesolution constructed by a local model and conduct a search in the space ofpossible corrections to improve the local solution from a global view point.Our search utilizes a heuristic function to focus more on the least confidentlocal decisions and a pruning function to score the global solutions based ontheir local fitness and the global coherences among the predicted entities.Experimental results on CoNLL 2003 and TAC 2010 benchmarks verify theeffectiveness of our model.

Interpreting Recurrent and Attention-Based Neural Models: a Case Study  on Natural Language Inference

  Deep learning models have achieved remarkable success in natural languageinference (NLI) tasks. While these models are widely explored, they are hard tointerpret and it is often unclear how and why they actually work. In thispaper, we take a step toward explaining such deep learning based models througha case study on a popular neural model for NLI. In particular, we propose tointerpret the intermediate layers of NLI models by visualizing the saliency ofattention and LSTM gating signals. We present several examples for which ourmethods are able to reveal interesting insights and identify the criticalinformation contributing to the model decisions.

Attentional Multi-Reading Sarcasm Detection

  Recognizing sarcasm often requires a deep understanding of multiple sourcesof information, including the utterance, the conversational context, and realworld facts. Most of the current sarcasm detection systems consider only theutterance in isolation. There are some limited attempts toward taking intoaccount the conversational context. In this paper, we propose an interpretableend-to-end model that combines information from both the utterance and theconversational context to detect sarcasm, and demonstrate its effectivenessthrough empirical evaluations. We also study the behavior of the proposed modelto provide explanations for the model's decisions. Importantly, our model iscapable of determining the impact of utterance and conversational context onthe model's decisions. Finally, we provide an ablation study to illustrate theimpact of different components of the proposed model.

Saliency Learning: Teaching the Model Where to Pay Attention

  Deep learning has emerged as a compelling solution to many NLP tasks withremarkable performances. However, due to their opacity, such models are hard tointerpret and trust. Recent work on explaining deep models has introducedapproaches to provide insights toward the model's behaviour and predictions,which are helpful for assessing the reliability of the model's predictions.However, such methods do not improve the model's reliability. In this paper, weaim to teach the model to make the right prediction for the right reason byproviding explanation training and ensuring the alignment of the model'sexplanation with the ground truth explanation. Our experimental results onmultiple tasks and datasets demonstrate the effectiveness of the proposedmethod, which produces more reliable predictions while delivering betterresults compared to traditionally trained models.

Novelty Detection Under Multi-Instance Multi-Label Framework

  Novelty detection plays an important role in machine learning and signalprocessing. This paper studies novelty detection in a new setting where thedata object is represented as a bag of instances and associated with multipleclass labels, referred to as multi-instance multi-label (MIML) learning.Contrary to the common assumption in MIML that each instance in a bag belongsto one of the known classes, in novelty detection, we focus on the scenariowhere bags may contain novel-class instances. The goal is to determine, for anygiven instance in a new bag, whether it belongs to a known class or a novelclass. Detecting novelty in the MIML setting captures many real-world phenomenaand has many potential applications. For example, in a collection of taggedimages, the tag may only cover a subset of objects existing in the images.Discovering an object whose class has not been previously tagged can be usefulfor the purpose of soliciting a label for the new object class. To address thisnovel problem, we present a discriminative framework for detecting new classinstances. Experiments demonstrate the effectiveness of our proposed method,and reveal that the presence of unlabeled novel instances in training bags ishelpful to the detection of such instances in testing stage.

Discriminative Clustering with Relative Constraints

  We study the problem of clustering with relative constraints, where eachconstraint specifies relative similarities among instances. In particular, eachconstraint $(x_i, x_j, x_k)$ is acquired by posing a query: is instance $x_i$more similar to $x_j$ than to $x_k$? We consider the scenario where answers tosuch queries are based on an underlying (but unknown) class concept, which weaim to discover via clustering. Different from most existing methods that onlyconsider constraints derived from yes and no answers, we also incorporate don'tknow responses. We introduce a Discriminative Clustering method with RelativeConstraints (DCRC) which assumes a natural probabilistic relationship betweeninstances, their underlying cluster memberships, and the observed constraints.The objective is to maximize the model likelihood given the constraints, and inthe meantime enforce cluster separation and cluster balance by also making useof the unlabeled instances. We evaluated the proposed method using constraintsgenerated from ground-truth class labels, and from (noisy) human judgments froma user study. Experimental results demonstrate: 1) the usefulness of relativeconstraints, in particular when don't know answers are considered; 2) theimproved performance of the proposed method over state-of-the-art methods thatutilize either relative or pairwise constraints; and 3) the robustness of ourmethod in the presence of noisy constraints, such as those provided by humanjudgement.

Weakly-supervised Dictionary Learning

  We present a probabilistic modeling and inference framework fordiscriminative analysis dictionary learning under a weak supervision setting.Dictionary learning approaches have been widely used for tasks such aslow-level signal denoising and restoration as well as high-level classificationtasks, which can be applied to audio and image analysis. Synthesis dictionarylearning aims at jointly learning a dictionary and corresponding sparsecoefficients to provide accurate data representation. This approach is usefulfor denoising and signal restoration, but may lead to sub-optimalclassification performance. By contrast, analysis dictionary learning providesa transform that maps data to a sparse discriminative representation suitablefor classification. We consider the problem of analysis dictionary learning fortime-series data under a weak supervision setting in which signals are assignedwith a global label instead of an instantaneous label signal. We propose adiscriminative probabilistic model that incorporates both label information andsparsity constraints on the underlying latent instantaneous label signal usingcardinality control. We present the expectation maximization (EM) procedure formaximum likelihood estimation (MLE) of the proposed model. To facilitate acomputationally efficient E-step, we propose both a chain and a novel treegraph reformulation of the graphical model. The performance of the proposedmodel is demonstrated on both synthetic and real-world data.

