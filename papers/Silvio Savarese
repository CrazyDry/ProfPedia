Translating Navigation Instructions in Natural Language to a High-Level  Plan for Behavioral Robot Navigation

  We propose an end-to-end deep learning model for translating free-formnatural language instructions to a high-level plan for behavioral robotnavigation. We use attention models to connect information from both the userinstructions and a topological representation of the environment. We evaluateour model's performance on a new dataset containing 10,050 pairs of navigationinstructions. Our model significantly outperforms baseline approaches.Furthermore, our results suggest that it is possible to leverage theenvironment map as a relevant knowledge base to facilitate the translation offree-form navigational instruction.

Shrinkage Optimized Directed Information using Pictorial Structures for  Action Recognition

  In this paper, we propose a novel action recognition framework. The methoduses pictorial structures and shrinkage optimized directed informationassessment (SODA) coupled with Markov Random Fields called SODA+MRF to modelthe directional temporal dependency and bidirectional spatial dependency. As avariant of mutual information, directional information captures the directionalinformation flow and temporal structure of video sequences across frames.Meanwhile, within each frame, Markov random fields are utilized to model thespatial relations among different parts of a human body and the body parts ofdifferent people. The proposed SODA+MRF model is robust to view pointtransformations and detect complex interactions accurately. We compare theproposed method against several baseline methods to highlight the effectivenessof the SODA+MRF model. We demonstrate that our algorithm has superior actionrecognition performance on the UCF action recognition dataset, the Olympicsports dataset and the collective activity dataset over severalstate-of-the-art methods.

A Coarse-to-Fine Model for 3D Pose Estimation and Sub-category  Recognition

  Despite the fact that object detection, 3D pose estimation, and sub-categoryrecognition are highly correlated tasks, they are usually addressedindependently from each other because of the huge space of parameters. Tojointly model all of these tasks, we propose a coarse-to-fine hierarchicalrepresentation, where each level of the hierarchy represents objects at adifferent level of granularity. The hierarchical representation preventsperformance loss, which is often caused by the increase in the number ofparameters (as we consider more tasks to model), and the joint modellingenables resolving ambiguities that exist in independent modelling of thesetasks. We augment PASCAL3D+ dataset with annotations for these tasks and showthat our hierarchical model is effective in joint modelling of objectdetection, 3D pose estimation, and sub-category recognition.

Unsupervised Semantic Parsing of Video Collections

  Human communication typically has an underlying structure. This is reflectedin the fact that in many user generated videos, a starting point, ending, andcertain objective steps between these two can be identified. In this paper, wepropose a method for parsing a video into such semantic steps in anunsupervised way. The proposed method is capable of providing a semantic"storyline" of the video composed of its objective steps. We accomplish thisusing both visual and language cues in a joint generative model. The proposedmethod can also provide a textual description for each of the identifiedsemantic steps and video segments. We evaluate this method on a large number ofcomplex YouTube videos and show results of unprecedented quality for thisintricate and impactful problem.

Deep Learning for Single-View Instance Recognition

  Deep learning methods have typically been trained on large datasets in whichmany training examples are available. However, many real-world product datasetshave only a small number of images available for each product. We explore theuse of deep learning methods for recognizing object instances when we have onlya single training example per class. We show that feedforward neural networksoutperform state-of-the-art methods for recognizing objects from novelviewpoints even when trained from just a single image per object. To furtherimprove our performance on this task, we propose to take advantage of asupplementary dataset in which we observe a separate set of objects frommultiple viewpoints. We introduce a new approach for training deep learningmethods for instance recognition with limited training data, in which we use anauxiliary multi-view dataset to train our network to be robust to viewpointchanges. We find that this approach leads to a more robust classifier forrecognizing objects from novel viewpoints, outperforming previousstate-of-the-art approaches including keypoint-matching, template-basedtechniques, and sparse coding.

Action Recognition by Hierarchical Mid-level Action Elements

  Realistic videos of human actions exhibit rich spatiotemporal structures atmultiple levels of granularity: an action can always be decomposed intomultiple finer-grained elements in both space and time. To capture thisintuition, we propose to represent videos by a hierarchy of mid-level actionelements (MAEs), where each MAE corresponds to an action-related spatiotemporalsegment in the video. We introduce an unsupervised method to generate thisrepresentation from videos. Our method is capable of distinguishingaction-related segments from background segments and representing actions atmultiple spatiotemporal resolutions. Given a set of spatiotemporal segmentsgenerated from the training data, we introduce a discriminative clusteringalgorithm that automatically discovers MAEs at multiple levels of granularity.We develop structured models that capture a rich set of spatial, temporal andhierarchical relations among the segments, where the action label and multiplelevels of MAE labels are jointly inferred. The proposed model achievesstate-of-the-art performance in multiple action recognition benchmarks.Moreover, we demonstrate the effectiveness of our model in real-worldapplications such as action recognition in large-scale untrimmed videos andaction parsing.

Semantic Cross-View Matching

  Matching cross-view images is challenging because the appearance andviewpoints are significantly different. While low-level features based ongradient orientations or filter responses can drastically vary with suchchanges in viewpoint, semantic information of images however shows an invariantcharacteristic in this respect. Consequently, semantically labeled regions canbe used for performing cross-view matching. In this paper, we therefore explorethis idea and propose an automatic method for detecting and representing thesemantic information of an RGB image with the goal of performing cross-viewmatching with a (non-RGB) geographic information system (GIS). A segmentedimage forms the input to our system with segments assigned to semantic conceptssuch as traffic signs, lakes, roads, foliage, etc. We design a descriptor torobustly capture both, the presence of semantic concepts and the spatial layoutof those segments. Pairwise distances between the descriptors extracted fromthe GIS map and the query image are then used to generate a shortlist of themost promising locations with similar semantic concepts in a consistent spatiallayout. An experimental evaluation with challenging query images and a largeurban area shows promising results.

Deep Metric Learning via Lifted Structured Feature Embedding

  Learning the distance metric between pairs of examples is of great importancefor learning and visual recognition. With the remarkable success from the stateof the art convolutional neural networks, recent works have shown promisingresults on discriminatively training the networks to learn semantic featureembeddings where similar examples are mapped close to each other and dissimilarexamples are mapped farther apart. In this paper, we describe an algorithm fortaking full advantage of the training batches in the neural network training bylifting the vector of pairwise distances within the batch to the matrix ofpairwise distances. This step enables the algorithm to learn the state of theart feature embedding by optimizing a novel structured prediction objective onthe lifted problem. Additionally, we collected Online Products dataset: 120kimages of 23k classes of online products for metric learning. Our experimentson the CUB-200-2011, CARS196, and Online Products datasets demonstratesignificant improvement over existing deep feature embedding methods on allexperimented embedding sizes with the GoogLeNet network.

ShapeNet: An Information-Rich 3D Model Repository

  We present ShapeNet: a richly-annotated, large-scale repository of shapesrepresented by 3D CAD models of objects. ShapeNet contains 3D models from amultitude of semantic categories and organizes them under the WordNet taxonomy.It is a collection of datasets providing many semantic annotations for each 3Dmodel such as consistent rigid alignments, parts and bilateral symmetry planes,physical sizes, keywords, as well as other planned annotations. Annotations aremade available through a public web-based interface to enable datavisualization of object attributes, promote data-driven geometric analysis, andprovide a large-scale quantitative benchmark for research in computer graphicsand vision. At the time of this technical report, ShapeNet has indexed morethan 3,000,000 models, 220,000 models out of which are classified into 3,135categories (WordNet synsets). In this report we describe the ShapeNet effort asa whole, provide details for all currently available datasets, and summarizefuture plans.

Watch-Bot: Unsupervised Learning for Reminding Humans of Forgotten  Actions

  We present a robotic system that watches a human using a Kinect v2 RGB-Dsensor, detects what he forgot to do while performing an activity, and ifnecessary reminds the person using a laser pointer to point out the relatedobject. Our simple setup can be easily deployed on any assistive robot.  Our approach is based on a learning algorithm trained in a purelyunsupervised setting, which does not require any human annotations. This makesour approach scalable and applicable to variant scenarios. Our model learns theaction/object co-occurrence and action temporal relations in the activity, anduses the learned rich relationships to infer the forgotten action and therelated object. We show that our approach not only improves the unsupervisedaction segmentation and action cluster assignment performance, but alsoeffectively detects the forgotten actions on a challenging human activity RGB-Dvideo dataset. In robotic experiments, we show that our robot is able to remindpeople of forgotten actions successfully.

Unsupervised Transductive Domain Adaptation

  Supervised learning with large scale labeled datasets and deep layered modelshas made a paradigm shift in diverse areas in learning and recognition.However, this approach still suffers generalization issues under the presenceof a domain shift between the training and the test data distribution. In thisregard, unsupervised domain adaptation algorithms have been proposed todirectly address the domain shift problem. In this paper, we approach theproblem from a transductive perspective. We incorporate the domain shift andthe transductive target inference into our framework by jointly solving for anasymmetric similarity metric and the optimal transductive target labelassignment. We also show that our model can easily be extended for deep featurelearning in order to learn features which are discriminative in the targetdomain. Our experiments show that the proposed method significantly outperformsstate-of-the-art algorithms in both object recognition and digit classificationexperiments by a large margin.

Knowledge Transfer for Scene-specific Motion Prediction

  When given a single frame of the video, humans can not only interpret thecontent of the scene, but also they are able to forecast the near future. Thisability is mostly driven by their rich prior knowledge about the visual world,both in terms of (i) the dynamics of moving agents, as well as (ii) thesemantic of the scene. In this work we exploit the interplay between these twokey elements to predict scene-specific motion patterns. First, we extract patchdescriptors encoding the probability of moving to the adjacent patches, and theprobability of being in that particular patch or changing behavior. Then, weintroduce a Dynamic Bayesian Network which exploits this scene specificknowledge for trajectory prediction. Experimental results demonstrate that ourmethod is able to accurately predict trajectories and transfer predictions to anovel scene characterized by similar elements.

3D-R2N2: A Unified Approach for Single and Multi-view 3D Object  Reconstruction

  Inspired by the recent success of methods that employ shape priors to achieverobust 3D reconstructions, we propose a novel recurrent neural networkarchitecture that we call the 3D Recurrent Reconstruction Neural Network(3D-R2N2). The network learns a mapping from images of objects to theirunderlying 3D shapes from a large collection of synthetic data. Our networktakes in one or more images of an object instance from arbitrary viewpoints andoutputs a reconstruction of the object in the form of a 3D occupancy grid.Unlike most of the previous works, our network does not require any imageannotations or object class labels for training or testing. Our extensiveexperimental analysis shows that our reconstruction framework i) outperformsthe state-of-the-art methods for single view reconstruction, and ii) enablesthe 3D reconstruction of objects in situations when traditional SFM/SLAMmethods fail (because of lack of texture and/or wide baseline).

Subcategory-aware Convolutional Neural Networks for Object Proposals and  Detection

  In CNN-based object detection methods, region proposal becomes a bottleneckwhen objects exhibit significant scale variation, occlusion or truncation. Inaddition, these methods mainly focus on 2D object detection and cannot estimatedetailed properties of objects. In this paper, we propose subcategory-awareCNNs for object detection. We introduce a novel region proposal network thatuses subcategory information to guide the proposal generating process, and anew detection network for joint detection and subcategory classification. Byusing subcategories related to object pose, we achieve state-of-the-artperformance on both detection and pose estimation on commonly used benchmarks.

Unsupervised Semantic Action Discovery from Video Collections

  Human communication takes many forms, including speech, text andinstructional videos. It typically has an underlying structure, with a startingpoint, ending, and certain objective steps between them. In this paper, weconsider instructional videos where there are tens of millions of them on theInternet.  We propose a method for parsing a video into such semantic steps in anunsupervised way. Our method is capable of providing a semantic "storyline" ofthe video composed of its objective steps. We accomplish this using both visualand language cues in a joint generative model. Our method can also provide atextual description for each of the identified semantic steps and videosegments. We evaluate our method on a large number of complex YouTube videosand show that our method discovers semantically correct instructions for avariety of tasks.

Universal Correspondence Network

  We present a deep learning framework for accurate visual correspondences anddemonstrate its effectiveness for both geometric and semantic matching,spanning across rigid motions to intra-class shape or appearance variations. Incontrast to previous CNN-based approaches that optimize a surrogate patchsimilarity objective, we use deep metric learning to directly learn a featurespace that preserves either geometric or semantic similarity. Our fullyconvolutional architecture, along with a novel correspondence contrastive lossallows faster training by effective reuse of computations, accurate gradientcomputation through the use of thousands of examples per image pair and fastertesting with $O(n)$ feed forward passes for $n$ keypoints, instead of $O(n^2)$for typical patch similarity methods. We propose a convolutional spatialtransformer to mimic patch normalization in traditional features like SIFT,which is shown to dramatically boost accuracy for semantic correspondencesacross intra-class shape variations. Extensive experiments on KITTI, PASCAL,and CUB-2011 datasets demonstrate the significant advantages of our featuresover prior works that use either hand-constructed or learned features.

Human Centred Object Co-Segmentation

  Co-segmentation is the automatic extraction of the common semantic regionsgiven a set of images. Different from previous approaches mainly based onobject visuals, in this paper, we propose a human centred objectco-segmentation approach, which uses the human as another strong evidence. Inorder to discover the rich internal structure of the objects reflecting theirhuman-object interactions and visual similarities, we propose an unsupervisedfully connected CRF auto-encoder incorporating the rich object features and anovel human-object interaction representation. We propose an efficient learningand inference algorithm to allow the full connectivity of the CRF with theauto-encoder, that establishes pairwise relations on all pairs of the objectproposals in the dataset. Moreover, the auto-encoder learns the parameters fromthe data itself rather than supervised learning or manually assigned parametersin the conventional CRF. In the extensive experiments on four datasets, we showthat our approach is able to extract the common objects more accurately thanthe state-of-the-art co-segmentation algorithms.

Social Scene Understanding: End-to-End Multi-Person Action Localization  and Collective Activity Recognition

  We present a unified framework for understanding human social behaviors inraw image sequences. Our model jointly detects multiple individuals, inferstheir social actions, and estimates the collective actions with a singlefeed-forward pass through a neural network. We propose a single architecturethat does not rely on external detection algorithms but rather is trainedend-to-end to generate dense proposal maps that are refined via a novelinference scheme. The temporal consistency is handled via a person-levelmatching Recurrent Neural Network. The complete model takes as input a sequenceof frames and outputs detections along with the estimates of individual actionsand collective activities. We demonstrate state-of-the-art performance of ouralgorithm on multiple publicly available benchmarks.

Tracking The Untrackable: Learning To Track Multiple Cues with Long-Term  Dependencies

  The majority of existing solutions to the Multi-Target Tracking (MTT) problemdo not combine cues in a coherent end-to-end fashion over a long period oftime. However, we present an online method that encodes long-term temporaldependencies across multiple cues. One key challenge of tracking methods is toaccurately track occluded targets or those which share similar appearanceproperties with surrounding objects. To address this challenge, we present astructure of Recurrent Neural Networks (RNN) that jointly reasons on multiplecues over a temporal window. We are able to correct many data associationerrors and recover observations from an occluded state. We demonstrate therobustness of our data-driven approach by tracking multiple targets using theirappearance, motion, and even interactions. Our method outperforms previousworks on multiple publicly available datasets including the challenging MOTbenchmark.

Joint 2D-3D-Semantic Data for Indoor Scene Understanding

  We present a dataset of large-scale indoor spaces that provides a variety ofmutually registered modalities from 2D, 2.5D and 3D domains, withinstance-level semantic and geometric annotations. The dataset covers over6,000m2 and contains over 70,000 RGB images, along with the correspondingdepths, surface normals, semantic annotations, global XYZ images (all in formsof both regular and 360{\deg} equirectangular images) as well as camerainformation. It also includes registered raw and semantically annotated 3Dmeshes and point clouds. The dataset enables development of joint andcross-modal learning models and potentially unsupervised approaches utilizingthe regularities present in large-scale indoor spaces. The dataset is availablehere: http://3Dsemantics.stanford.edu/

Deep View Morphing

  Recently, convolutional neural networks (CNN) have been successfully appliedto view synthesis problems. However, such CNN-based methods can suffer fromlack of texture details, shape distortions, or high computational complexity.In this paper, we propose a novel CNN architecture for view synthesis called"Deep View Morphing" that does not suffer from these issues. To synthesize amiddle view of two input images, a rectification network first rectifies thetwo input images. An encoder-decoder network then generates densecorrespondences between the rectified images and blending masks to predict thevisibility of pixels of the rectified images in the middle view. A viewmorphing network finally synthesizes the middle view using the densecorrespondences and blending masks. We experimentally show the proposed methodsignificantly outperforms the state-of-the-art CNN-based view synthesis method.

Weakly supervised 3D Reconstruction with Adversarial Constraint

  Supervised 3D reconstruction has witnessed a significant progress through theuse of deep neural networks. However, this increase in performance requireslarge scale annotations of 2D/3D data. In this paper, we explore inexpensive 2Dsupervision as an alternative for expensive 3D CAD annotation. Specifically, weuse foreground masks as weak supervision through a raytrace pooling layer thatenables perspective projection and backpropagation. Additionally, since the 3Dreconstruction from masks is an ill posed problem, we propose to constrain the3D reconstruction to the manifold of unlabeled realistic 3D shapes that matchmask observations. We demonstrate that learning a log-barrier solution to thisconstrained optimization problem resembles the GAN objective, enabling the useof existing tools for training GANs. We evaluate and analyze the manifoldconstrained reconstruction on various datasets for single and multi-viewreconstruction of both synthetic and real images.

DeformNet: Free-Form Deformation Network for 3D Shape Reconstruction  from a Single Image

  3D reconstruction from a single image is a key problem in multipleapplications ranging from robotic manipulation to augmented reality. Priormethods have tackled this problem through generative models which predict 3Dreconstructions as voxels or point clouds. However, these methods can becomputationally expensive and miss fine details. We introduce a newdifferentiable layer for 3D data deformation and use it in DeformNet to learn amodel for 3D reconstruction-through-deformation. DeformNet takes an imageinput, searches the nearest shape template from a database, and deforms thetemplate to match the query image. We evaluate our approach on the ShapeNetdataset and show that - (a) the Free-Form Deformation layer is a powerful newbuilding block for Deep Learning models that manipulate 3D data (b) DeformNetuses this FFD layer combined with shape retrieval for smooth anddetail-preserving 3D reconstruction of qualitatively plausible point cloudswith respect to a single query image (c) compared to other state-of-the-art 3Dreconstruction methods, DeformNet quantitatively matches or outperforms theirbenchmarks by significant margins. For more information, visit:https://deformnet-site.github.io/DeformNet-website/ .

Neural Task Programming: Learning to Generalize Across Hierarchical  Tasks

  In this work, we propose a novel robot learning framework called Neural TaskProgramming (NTP), which bridges the idea of few-shot learning fromdemonstration and neural program induction. NTP takes as input a taskspecification (e.g., video demonstration of a task) and recursively decomposesit into finer sub-task specifications. These specifications are fed to ahierarchical neural program, where bottom-level programs are callablesubroutines that interact with the environment. We validate our method in threerobot manipulation tasks. NTP achieves strong generalization across sequentialtasks that exhibit hierarchal and compositional structures. The experimentalresults show that NTP learns to generalize well to- wards unseen tasks withincreasing lengths, variable topologies, and changing objectives.

Multi-Task Domain Adaptation for Deep Learning of Instance Grasping from  Simulation

  Learning-based approaches to robotic manipulation are limited by thescalability of data collection and accessibility of labels. In this paper, wepresent a multi-task domain adaptation framework for instance grasping incluttered scenes by utilizing simulated robot experiments. Our neural networktakes monocular RGB images and the instance segmentation mask of a specifiedtarget object as inputs, and predicts the probability of successfully graspingthe specified object for each candidate motor command. The proposed transferlearning framework trains a model for instance grasping in simulation and usesa domain-adversarial loss to transfer the trained model to real robots usingindiscriminate grasping data, which is available both in simulation and thereal world. We evaluate our model in real-world robot experiments, comparing itwith alternative model architectures as well as an indiscriminate graspingbaseline.

SEGCloud: Semantic Segmentation of 3D Point Clouds

  3D semantic scene labeling is fundamental to agents operating in the realworld. In particular, labeling raw 3D point sets from sensors providesfine-grained semantics. Recent works leverage the capabilities of NeuralNetworks (NNs), but are limited to coarse voxel predictions and do notexplicitly enforce global consistency. We present SEGCloud, an end-to-endframework to obtain 3D point-level segmentation that combines the advantages ofNNs, trilinear interpolation(TI) and fully connected Conditional Random Fields(FC-CRF). Coarse voxel predictions from a 3D Fully Convolutional NN aretransferred back to the raw 3D points via trilinear interpolation. Then theFC-CRF enforces global consistency and provides fine-grained semantics on thepoints. We implement the latter as a differentiable Recurrent NN to allow jointoptimization. We evaluate the framework on two indoor and two outdoor 3Ddatasets (NYU V2, S3DIS, KITTI, Semantic3D.net), and show performancecomparable or superior to the state-of-the-art on all datasets.

Recurrent Autoregressive Networks for Online Multi-Object Tracking

  The main challenge of online multi-object tracking is to reliably associateobject trajectories with detections in each video frame based on their trackinghistory. In this work, we propose the Recurrent Autoregressive Network (RAN), atemporal generative modeling framework to characterize the appearance andmotion dynamics of multiple objects over time. The RAN couples an externalmemory and an internal memory. The external memory explicitly stores previousinputs of each trajectory in a time window, while the internal memory learns tosummarize long-term tracking history and associate detections by processing theexternal memory. We conduct experiments on the MOT 2015 and 2016 datasets todemonstrate the robustness of our tracking method in highly crowded andoccluded scenes. Our method achieves top-ranked results on the two benchmarks.

Adversarial Feature Augmentation for Unsupervised Domain Adaptation

  Recent works showed that Generative Adversarial Networks (GANs) can besuccessfully applied in unsupervised domain adaptation, where, given a labeledsource dataset and an unlabeled target dataset, the goal is to train powerfulclassifiers for the target samples. In particular, it was shown that a GANobjective function can be used to learn target features indistinguishable fromthe source ones. In this work, we extend this framework by (i) forcing thelearned feature extractor to be domain-invariant, and (ii) training it throughdata augmentation in the feature space, namely performing feature augmentation.While data augmentation in the image space is a well established technique indeep learning, feature augmentation has not yet received the same level ofattention. We accomplish it by means of a feature generator trained by playingthe GAN minimax game against source features. Results show that both enforcingdomain-invariance and performing feature augmentation lead to superior orcomparable performance to state-of-the-art results in several unsuperviseddomain adaptation benchmarks.

Im2Pano3D: Extrapolating 360 Structure and Semantics Beyond the Field of  View

  We present Im2Pano3D, a convolutional neural network that generates a denseprediction of 3D structure and a probability distribution of semantic labelsfor a full 360 panoramic view of an indoor scene when given only a partialobservation (<= 50%) in the form of an RGB-D image. To make this possible,Im2Pano3D leverages strong contextual priors learned from large-scale syntheticand real-world indoor scenes. To ease the prediction of 3D structure, wepropose to parameterize 3D surfaces with their plane equations and train themodel to predict these parameters directly. To provide meaningful trainingsupervision, we use multiple loss functions that consider both pixel levelaccuracy and global context consistency. Experiments demon- strate thatIm2Pano3D is able to predict the semantics and 3D structure of the unobservedscene with more than 56% pixel accuracy and less than 0.52m average distanceerror, which is significantly better than alternative approaches.

GONet: A Semi-Supervised Deep Learning Approach For Traversability  Estimation

  We present semi-supervised deep learning approaches for traversabilityestimation from fisheye images. Our method, GONet, and the proposed extensionsleverage Generative Adversarial Networks (GANs) to effectively predict whetherthe area seen in the input image(s) is safe for a robot to traverse. Thesemethods are trained with many positive images of traversable places, but just asmall set of negative images depicting blocked and unsafe areas. This makes theproposed methods practical. Positive examples can be collected easily by simplyoperating a robot through traversable spaces, while obtaining negative examplesis time consuming, costly, and potentially dangerous. Through extensiveexperiments and several demonstrations, we show that the proposedtraversability estimation approaches are robust and can generalize to unseenscenarios. Further, we demonstrate that our methods are memory efficient andfast, allowing for real-time operation on a mobile robot with single or stereofisheye cameras. As part of our contributions, we open-source two new datasetsfor traversability estimation. These datasets are composed of approximately 24hof videos from more than 25 indoor environments. Our methods outperformbaseline approaches for traversability estimation on these new datasets.

Text2Shape: Generating Shapes from Natural Language by Learning Joint  Embeddings

  We present a method for generating colored 3D shapes from natural language.To this end, we first learn joint embeddings of freeform text descriptions andcolored 3D shapes. Our model combines and extends learning by association andmetric learning approaches to learn implicit cross-modal connections, andproduces a joint representation that captures the many-to-many relationsbetween language and physical properties of 3D shapes such as color and shape.To evaluate our approach, we collect a large dataset of natural languagedescriptions for physical 3D objects in the ShapeNet dataset. With this learnedjoint embedding we demonstrate text-to-shape retrieval that outperformsbaseline approaches. Using our embeddings with a novel conditional WassersteinGAN framework, we generate colored 3D shapes from text. Our method is the firstto connect natural language text with realistic 3D objects exhibiting richvariations in color, texture, and shape detail. See video athttps://youtu.be/zraPvRdl13Q

Social GAN: Socially Acceptable Trajectories with Generative Adversarial  Networks

  Understanding human motion behavior is critical for autonomous movingplatforms (like self-driving cars and social robots) if they are to navigatehuman-centric environments. This is challenging because human motion isinherently multimodal: given a history of human motion paths, there are manysocially plausible ways that people could move in the future. We tackle thisproblem by combining tools from sequence prediction and generative adversarialnetworks: a recurrent sequence-to-sequence model observes motion histories andpredicts future behavior, using a novel pooling mechanism to aggregateinformation across people. We predict socially plausible futures by trainingadversarially against a recurrent discriminator, and encourage diversepredictions with a novel variety loss. Through experiments on several datasetswe demonstrate that our approach outperforms prior work in terms of accuracy,variety, collision avoidance, and computational complexity.

Deep Learning under Privileged Information Using Heteroscedastic Dropout

  Unlike machines, humans learn through rapid, abstract model-building. Therole of a teacher is not simply to hammer home right or wrong answers, butrather to provide intuitive comments, comparisons, and explanations to a pupil.This is what the Learning Under Privileged Information (LUPI) paradigmendeavors to model by utilizing extra knowledge only available during training.We propose a new LUPI algorithm specifically designed for Convolutional NeuralNetworks (CNNs) and Recurrent Neural Networks (RNNs). We propose to use aheteroscedastic dropout (i.e. dropout with a varying variance) and make thevariance of the dropout a function of privileged information. Intuitively, thiscorresponds to using the privileged information to control the uncertainty ofthe model output. We perform experiments using CNNs and RNNs for the tasks ofimage classification and machine translation. Our method significantlyincreases the sample efficiency during learning, resulting in higher accuracywith a large margin when the number of training examples is limited. We alsotheoretically justify the gains in sample efficiency by providing ageneralization error bound decreasing with $O(\frac{1}{n})$, where $n$ is thenumber of training examples, in an oracle case.

Generalizing to Unseen Domains via Adversarial Data Augmentation

  We are concerned with learning models that generalize well to different\emph{unseen} domains. We consider a worst-case formulation over datadistributions that are near the source domain in the feature space. Only usingtraining data from a single source distribution, we propose an iterativeprocedure that augments the dataset with examples from a fictitious targetdomain that is "hard" under the current model. We show that our iterativescheme is an adaptive data augmentation method where we append adversarialexamples at each iteration. For softmax losses, we show that our method is adata-dependent regularization scheme that behaves differently from classicalregularizers that regularize towards zero (e.g., ridge or lasso). On digitrecognition and semantic segmentation tasks, our method learns models improveperformance across a range of a priori unknown target domains.

VUNet: Dynamic Scene View Synthesis for Traversability Estimation using  an RGB Camera

  We present VUNet, a novel view(VU) synthesis method for mobile robots indynamic environments, and its application to the estimation of futuretraversability. Our method predicts future images for given virtual robotvelocity commands using only RGB images at previous and current time steps. Thefuture images result from applying two types of image changes to the previousand current images: 1) changes caused by different camera pose, and 2) changesdue to the motion of the dynamic obstacles. We learn to predict these two typesof changes disjointly using two novel network architectures, SNet and DNet. Wecombine SNet and DNet to synthesize future images that we pass to ourpreviously presented method GONet to estimate the traversable areas around therobot. Our quantitative and qualitative evaluation indicate that our approachfor view synthesis predicts accurate future images in both static and dynamicenvironments. We also show that these virtual images can be used to estimatefuture traversability correctly. We apply our view synthesis-basedtraversability estimation method to two applications for assistedteleoperation.

Learning Task-Oriented Grasping for Tool Manipulation from Simulated  Self-Supervision

  Tool manipulation is vital for facilitating robots to complete challengingtask goals. It requires reasoning about the desired effect of the task and thusproperly grasping and manipulating the tool to achieve the task. Task-agnosticgrasping optimizes for grasp robustness while ignoring crucial task-specificconstraints. In this paper, we propose the Task-Oriented Grasping Network(TOG-Net) to jointly optimize both task-oriented grasping of a tool and themanipulation policy for that tool. The training process of the model is basedon large-scale simulated self-supervision with procedurally generated toolobjects. We perform both simulated and real-world experiments on two tool-basedmanipulation tasks: sweeping and hammering. Our model achieves overall 71.1%task success rate for sweeping and 80.0% task success rate for hammering.Supplementary material is available at: bit.ly/task-oriented-grasp

Neural Task Graphs: Generalizing to Unseen Tasks from a Single Video  Demonstration

  Our goal is to generate a policy to complete an unseen task given just asingle video demonstration of the task in a given domain. We hypothesize thatto successfully generalize to unseen complex tasks from a single videodemonstration, it is necessary to explicitly incorporate the compositionalstructure of the tasks into the model. To this end, we propose Neural TaskGraph (NTG) Networks, which use conjugate task graph as the intermediaterepresentation to modularize both the video demonstration and the derivedpolicy. We empirically show NTG achieves inter-task generalization on twocomplex tasks: Block Stacking in BulletPhysics and Object Collection inAI2-THOR. NTG improves data efficiency with visual input as well as achievestrong generalization without the need for dense hierarchical supervision. Wefurther show that similar performance trends hold when applied to real-worlddata. We show that NTG can effectively predict task structure on the JIGSAWSsurgical dataset and generalize to unseen tasks.

Gibson Env: Real-World Perception for Embodied Agents

  Developing visual perception models for active agents and sensorimotorcontrol are cumbersome to be done in the physical world, as existing algorithmsare too slow to efficiently learn in real-time and robots are fragile andcostly. This has given rise to learning-in-simulation which consequently castsa question on whether the results transfer to real-world. In this paper, we areconcerned with the problem of developing real-world perception for activeagents, propose Gibson Virtual Environment for this purpose, and showcasesample perceptual tasks learned therein. Gibson is based on virtualizing realspaces, rather than using artificially designed ones, and currently includesover 1400 floor spaces from 572 full buildings. The main characteristics ofGibson are: I. being from the real-world and reflecting its semanticcomplexity, II. having an internal synthesis mechanism, "Goggles", enablingdeploying the trained models in real-world without needing further domainadaptation, III. embodiment of agents and making them subject to constraints ofphysics and space.

Making Sense of Vision and Touch: Self-Supervised Learning of Multimodal  Representations for Contact-Rich Tasks

  Contact-rich manipulation tasks in unstructured environments often requireboth haptic and visual feedback. However, it is non-trivial to manually designa robot controller that combines modalities with very differentcharacteristics. While deep reinforcement learning has shown success inlearning control policies for high-dimensional inputs, these algorithms aregenerally intractable to deploy on real robots due to sample complexity. We useself-supervision to learn a compact and multimodal representation of oursensory inputs, which can then be used to improve the sample efficiency of ourpolicy learning. We evaluate our method on a peg insertion task, generalizingover different geometry, configurations, and clearances, while being robust toexternal perturbations. Results for simulated and real robot experiments arepresented.

DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion

  A key technical challenge in performing 6D object pose estimation from RGB-Dimage is to fully leverage the two complementary data sources. Prior workseither extract information from the RGB image and depth separately or usecostly post-processing steps, limiting their performances in highly clutteredscenes and real-time applications. In this work, we present DenseFusion, ageneric framework for estimating 6D pose of a set of known objects from RGB-Dimages. DenseFusion is a heterogeneous architecture that processes the two datasources individually and uses a novel dense fusion network to extractpixel-wise dense feature embedding, from which the pose is estimated.Furthermore, we integrate an end-to-end iterative pose refinement procedurethat further improves the pose estimation while achieving near real-timeinference. Our experiments show that our method outperforms state-of-the-artapproaches in two datasets, YCB-Video and LineMOD. We also deploy our proposedmethod to a real robot to grasp and manipulate objects based on the estimatedpose.

Generalized Intersection over Union: A Metric and A Loss for Bounding  Box Regression

  Intersection over Union (IoU) is the most popular evaluation metric used inthe object detection benchmarks. However, there is a gap between optimizing thecommonly used distance losses for regressing the parameters of a bounding boxand maximizing this metric value. The optimal objective for a metric is themetric itself. In the case of axis-aligned 2D bounding boxes, it can be shownthat $IoU$ can be directly used as a regression loss. However, $IoU$ has aplateau making it infeasible to optimize in the case of non-overlappingbounding boxes. In this paper, we address the weaknesses of $IoU$ byintroducing a generalized version as both a new loss and a new metric. Byincorporating this generalized $IoU$ ($GIoU$) as a loss into the state-of-theart object detection frameworks, we show a consistent improvement on theirperformance using both the standard, $IoU$ based, and new, $GIoU$ based,performance measures on popular object detection benchmarks such as PASCAL VOCand MS COCO.

A Behavioral Approach to Visual Navigation with Graph Localization  Networks

  Inspired by research in psychology, we introduce a behavioral approach forvisual navigation using topological maps. Our goal is to enable a robot tonavigate from one location to another, relying only on its visual input and thetopological map of the environment. We propose using graph neural networks forlocalizing the agent in the map, and decompose the action space into primitivebehaviors implemented as convolutional or recurrent neural networks. Using theGibson simulator, we verify that our approach outperforms relevant baselinesand is able to navigate in both seen and unseen environments.

Scene Memory Transformer for Embodied Agents in Long-Horizon Tasks

  Many robotic applications require the agent to perform long-horizon tasks inpartially observable environments. In such applications, decision making at anystep can depend on observations received far in the past. Hence, being able toproperly memorize and utilize the long-term history is crucial. In this work,we propose a novel memory-based policy, named Scene Memory Transformer (SMT).The proposed policy embeds and adds each observation to a memory and uses theattention mechanism to exploit spatio-temporal dependencies. This model isgeneric and can be efficiently trained with reinforcement learning over longepisodes. On a range of visual navigation tasks, SMT demonstrates superiorperformance to existing reactive and memory-based policies by a margin.

Structural-RNN: Deep Learning on Spatio-Temporal Graphs

  Deep Recurrent Neural Network architectures, though remarkably capable atmodeling sequences, lack an intuitive high-level spatio-temporal structure.That is while many problems in computer vision inherently have an underlyinghigh-level structure and can benefit from it. Spatio-temporal graphs are apopular tool for imposing such high-level intuitions in the formulation of realworld problems. In this paper, we propose an approach for combining the powerof high-level spatio-temporal graphs and sequence learning success of RecurrentNeural Networks~(RNNs). We develop a scalable method for casting an arbitraryspatio-temporal graph as a rich RNN mixture that is feedforward, fullydifferentiable, and jointly trainable. The proposed method is generic andprincipled as it can be used for transforming any spatio-temporal graph throughemploying a certain set of well defined steps. The evaluations of the proposedapproach on a diverse set of problems, ranging from modeling human motion toobject interactions, shows improvement over the state-of-the-art with a largemargin. We expect this method to empower new approaches to problem formulationthrough high-level spatio-temporal graphs and Recurrent Neural Networks.

Forecasting Social Navigation in Crowded Complex Scenes

  When humans navigate a crowed space such as a university campus or thesidewalks of a busy street, they follow common sense rules based on socialetiquette. In this paper, we argue that in order to enable the design of newalgorithms that can take fully advantage of these rules to better solve taskssuch as target tracking or trajectory forecasting, we need to have access tobetter data in the first place. To that end, we contribute the very first largescale dataset (to the best of our knowledge) that collects images and videos ofvarious types of targets (not just pedestrians, but also bikers, skateboarders,cars, buses, golf carts) that navigate in a real-world outdoor environment suchas a university campus. We present an extensive evaluation where differentmethods for trajectory forecasting are evaluated and compared. Moreover, wepresent a new algorithm for trajectory prediction that exploits the complexityof our new dataset and allows to: i) incorporate inter-class interactions intotrajectory prediction models (e.g, pedestrian vs bike) as opposed to justintra-class interactions (e.g., pedestrian vs pedestrian); ii) model the degreeto which the social forces are regulating an interaction. We call the latter"social sensitivity"and it captures the sensitivity to which a target isresponding to a certain interaction. An extensive experimental evaluationdemonstrates the effectiveness of our novel approach.

Watch-n-Patch: Unsupervised Learning of Actions and Relations

  There is a large variation in the activities that humans perform in theireveryday lives. We consider modeling these composite human activities whichcomprises multiple basic level actions in a completely unsupervised setting.Our model learns high-level co-occurrence and temporal relations between theactions. We consider the video as a sequence of short-term action clips, whichcontains human-words and object-words. An activity is about a set ofaction-topics and object-topics indicating which actions are present and whichobjects are interacting with. We then propose a new probabilistic modelrelating the words and the topics. It allows us to model long-range actionrelations that commonly exist in the composite activities, which is challengingin previous works. We apply our model to the unsupervised action segmentationand clustering, and to a novel application that detects forgotten actions,which we call action patching. For evaluation, we contribute a new challengingRGB-D activity video dataset recorded by the new Kinect v2, which containsseveral human daily activities as compositions of multiple actions interactingwith different objects. Moreover, we develop a robotic system that watchespeople and reminds people by applying our action patching algorithm. Ourrobotic setup can be easily deployed on any assistive robot.

Learning to Track at 100 FPS with Deep Regression Networks

  Machine learning techniques are often used in computer vision due to theirability to leverage large amounts of training data to improve performance.Unfortunately, most generic object trackers are still trained from scratchonline and do not benefit from the large number of videos that are readilyavailable for offline training. We propose a method for offline training ofneural networks that can track novel objects at test-time at 100 fps. Ourtracker is significantly faster than previous methods that use neural networksfor tracking, which are typically very slow to run and not practical forreal-time applications. Our tracker uses a simple feed-forward network with noonline training required. The tracker learns a generic relationship betweenobject motion and appearance and can be used to track novel objects that do notappear in the training set. We test our network on a standard trackingbenchmark to demonstrate our tracker's state-of-the-art performance. Further,our performance improves as we add more videos to our offline training set. Tothe best of our knowledge, our tracker is the first neural-network tracker thatlearns to track generic objects at 100 fps.

Feedback Networks

  Currently, the most successful learning models in computer vision are basedon learning successive representations followed by a decision layer. This isusually actualized through feedforward multilayer neural networks, e.g.ConvNets, where each layer forms one of such successive representations.However, an alternative that can achieve the same goal is a feedback basedapproach in which the representation is formed in an iterative manner based ona feedback received from previous iteration's output.  We establish that a feedback based approach has several fundamentaladvantages over feedforward: it enables making early predictions at the querytime, its output naturally conforms to a hierarchical structure in the labelspace (e.g. a taxonomy), and it provides a new basis for Curriculum Learning.We observe that feedback networks develop a considerably differentrepresentation compared to feedforward counterparts, in line with theaforementioned advantages. We put forth a general feedback based learningarchitecture with the endpoint results on par or better than existingfeedforward networks with the addition of the above advantages. We alsoinvestigate several mechanisms in feedback architectures (e.g. skip connectionsin time) and design choices (e.g. feedback length). We hope this study offersnew perspectives in quest for more natural and practical learning models.

ADAPT: Zero-Shot Adaptive Policy Transfer for Stochastic Dynamical  Systems

  Model-free policy learning has enabled robust performance of complex taskswith relatively simple algorithms. However, this simplicity comes at the costof requiring an Oracle and arguably very poor sample complexity. This renderssuch methods unsuitable for physical systems. Variants of model-based methodsaddress this problem through the use of simulators, however, this gives rise tothe problem of policy transfer from simulated to the physical system. Modelmismatch due to systematic parameter shift and unmodelled dynamics error maycause sub-optimal or unsafe behavior upon direct transfer. We introduce theAdaptive Policy Transfer for Stochastic Dynamics (ADAPT) algorithm thatachieves provably safe and robust, dynamically-feasible zero-shot transfer ofRL-policies to new domains with dynamics error. ADAPT combines the strengths ofoffline policy learning in a black-box source simulator with online tube-basedMPC to attenuate bounded model mismatch between the source and target dynamics.ADAPT allows online transfer of policy, trained solely in a simulation offline,to a family of unknown targets without fine-tuning. We also formally show that(i) ADAPT guarantees state and control safety through state-action tubes underthe assumption of Lipschitz continuity of the divergence in dynamics and, (ii)ADAPT results in a bounded loss of reward accumulation relative to a policytrained and evaluated in the source environment. We evaluate ADAPT on 2continuous, non-holonomic simulated dynamical systems with 4 differentdisturbance models, and find that ADAPT performs between 50%-300% better onmean reward accrual than direct policy transfer.

Active Learning for Convolutional Neural Networks: A Core-Set Approach

  Convolutional neural networks (CNNs) have been successfully applied to manyrecognition and learning tasks using a universal recipe; training a deep modelon a very large dataset of supervised examples. However, this approach israther restrictive in practice since collecting a large set of labeled imagesis very expensive. One way to ease this problem is coming up with smart waysfor choosing images to be labelled from a very large collection (ie. activelearning).  Our empirical study suggests that many of the active learning heuristics inthe literature are not effective when applied to CNNs in batch setting.Inspired by these limitations, we define the problem of active learning ascore-set selection, ie. choosing set of points such that a model learned overthe selected subset is competitive for the remaining data points. We furtherpresent a theoretical result characterizing the performance of any selectedsubset using the geometry of the datapoints. As an active learning algorithm,we choose the subset which is expected to yield best result according to ourcharacterization. Our experiments show that the proposed method significantlyoutperforms existing approaches in image classification experiments by a largemargin.

