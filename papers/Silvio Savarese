Translating Navigation Instructions in Natural Language to a High-Level
  Plan for Behavioral Robot Navigation

  We propose an end-to-end deep learning model for translating free-form
natural language instructions to a high-level plan for behavioral robot
navigation. We use attention models to connect information from both the user
instructions and a topological representation of the environment. We evaluate
our model's performance on a new dataset containing 10,050 pairs of navigation
instructions. Our model significantly outperforms baseline approaches.
Furthermore, our results suggest that it is possible to leverage the
environment map as a relevant knowledge base to facilitate the translation of
free-form navigational instruction.


Shrinkage Optimized Directed Information using Pictorial Structures for
  Action Recognition

  In this paper, we propose a novel action recognition framework. The method
uses pictorial structures and shrinkage optimized directed information
assessment (SODA) coupled with Markov Random Fields called SODA+MRF to model
the directional temporal dependency and bidirectional spatial dependency. As a
variant of mutual information, directional information captures the directional
information flow and temporal structure of video sequences across frames.
Meanwhile, within each frame, Markov random fields are utilized to model the
spatial relations among different parts of a human body and the body parts of
different people. The proposed SODA+MRF model is robust to view point
transformations and detect complex interactions accurately. We compare the
proposed method against several baseline methods to highlight the effectiveness
of the SODA+MRF model. We demonstrate that our algorithm has superior action
recognition performance on the UCF action recognition dataset, the Olympic
sports dataset and the collective activity dataset over several
state-of-the-art methods.


A Coarse-to-Fine Model for 3D Pose Estimation and Sub-category
  Recognition

  Despite the fact that object detection, 3D pose estimation, and sub-category
recognition are highly correlated tasks, they are usually addressed
independently from each other because of the huge space of parameters. To
jointly model all of these tasks, we propose a coarse-to-fine hierarchical
representation, where each level of the hierarchy represents objects at a
different level of granularity. The hierarchical representation prevents
performance loss, which is often caused by the increase in the number of
parameters (as we consider more tasks to model), and the joint modelling
enables resolving ambiguities that exist in independent modelling of these
tasks. We augment PASCAL3D+ dataset with annotations for these tasks and show
that our hierarchical model is effective in joint modelling of object
detection, 3D pose estimation, and sub-category recognition.


ShapeNet: An Information-Rich 3D Model Repository

  We present ShapeNet: a richly-annotated, large-scale repository of shapes
represented by 3D CAD models of objects. ShapeNet contains 3D models from a
multitude of semantic categories and organizes them under the WordNet taxonomy.
It is a collection of datasets providing many semantic annotations for each 3D
model such as consistent rigid alignments, parts and bilateral symmetry planes,
physical sizes, keywords, as well as other planned annotations. Annotations are
made available through a public web-based interface to enable data
visualization of object attributes, promote data-driven geometric analysis, and
provide a large-scale quantitative benchmark for research in computer graphics
and vision. At the time of this technical report, ShapeNet has indexed more
than 3,000,000 models, 220,000 models out of which are classified into 3,135
categories (WordNet synsets). In this report we describe the ShapeNet effort as
a whole, provide details for all currently available datasets, and summarize
future plans.


Watch-Bot: Unsupervised Learning for Reminding Humans of Forgotten
  Actions

  We present a robotic system that watches a human using a Kinect v2 RGB-D
sensor, detects what he forgot to do while performing an activity, and if
necessary reminds the person using a laser pointer to point out the related
object. Our simple setup can be easily deployed on any assistive robot.
  Our approach is based on a learning algorithm trained in a purely
unsupervised setting, which does not require any human annotations. This makes
our approach scalable and applicable to variant scenarios. Our model learns the
action/object co-occurrence and action temporal relations in the activity, and
uses the learned rich relationships to infer the forgotten action and the
related object. We show that our approach not only improves the unsupervised
action segmentation and action cluster assignment performance, but also
effectively detects the forgotten actions on a challenging human activity RGB-D
video dataset. In robotic experiments, we show that our robot is able to remind
people of forgotten actions successfully.


Unsupervised Transductive Domain Adaptation

  Supervised learning with large scale labeled datasets and deep layered models
has made a paradigm shift in diverse areas in learning and recognition.
However, this approach still suffers generalization issues under the presence
of a domain shift between the training and the test data distribution. In this
regard, unsupervised domain adaptation algorithms have been proposed to
directly address the domain shift problem. In this paper, we approach the
problem from a transductive perspective. We incorporate the domain shift and
the transductive target inference into our framework by jointly solving for an
asymmetric similarity metric and the optimal transductive target label
assignment. We also show that our model can easily be extended for deep feature
learning in order to learn features which are discriminative in the target
domain. Our experiments show that the proposed method significantly outperforms
state-of-the-art algorithms in both object recognition and digit classification
experiments by a large margin.


Universal Correspondence Network

  We present a deep learning framework for accurate visual correspondences and
demonstrate its effectiveness for both geometric and semantic matching,
spanning across rigid motions to intra-class shape or appearance variations. In
contrast to previous CNN-based approaches that optimize a surrogate patch
similarity objective, we use deep metric learning to directly learn a feature
space that preserves either geometric or semantic similarity. Our fully
convolutional architecture, along with a novel correspondence contrastive loss
allows faster training by effective reuse of computations, accurate gradient
computation through the use of thousands of examples per image pair and faster
testing with $O(n)$ feed forward passes for $n$ keypoints, instead of $O(n^2)$
for typical patch similarity methods. We propose a convolutional spatial
transformer to mimic patch normalization in traditional features like SIFT,
which is shown to dramatically boost accuracy for semantic correspondences
across intra-class shape variations. Extensive experiments on KITTI, PASCAL,
and CUB-2011 datasets demonstrate the significant advantages of our features
over prior works that use either hand-constructed or learned features.


Human Centred Object Co-Segmentation

  Co-segmentation is the automatic extraction of the common semantic regions
given a set of images. Different from previous approaches mainly based on
object visuals, in this paper, we propose a human centred object
co-segmentation approach, which uses the human as another strong evidence. In
order to discover the rich internal structure of the objects reflecting their
human-object interactions and visual similarities, we propose an unsupervised
fully connected CRF auto-encoder incorporating the rich object features and a
novel human-object interaction representation. We propose an efficient learning
and inference algorithm to allow the full connectivity of the CRF with the
auto-encoder, that establishes pairwise relations on all pairs of the object
proposals in the dataset. Moreover, the auto-encoder learns the parameters from
the data itself rather than supervised learning or manually assigned parameters
in the conventional CRF. In the extensive experiments on four datasets, we show
that our approach is able to extract the common objects more accurately than
the state-of-the-art co-segmentation algorithms.


Semantic Cross-View Matching

  Matching cross-view images is challenging because the appearance and
viewpoints are significantly different. While low-level features based on
gradient orientations or filter responses can drastically vary with such
changes in viewpoint, semantic information of images however shows an invariant
characteristic in this respect. Consequently, semantically labeled regions can
be used for performing cross-view matching. In this paper, we therefore explore
this idea and propose an automatic method for detecting and representing the
semantic information of an RGB image with the goal of performing cross-view
matching with a (non-RGB) geographic information system (GIS). A segmented
image forms the input to our system with segments assigned to semantic concepts
such as traffic signs, lakes, roads, foliage, etc. We design a descriptor to
robustly capture both, the presence of semantic concepts and the spatial layout
of those segments. Pairwise distances between the descriptors extracted from
the GIS map and the query image are then used to generate a shortlist of the
most promising locations with similar semantic concepts in a consistent spatial
layout. An experimental evaluation with challenging query images and a large
urban area shows promising results.


Deep Metric Learning via Lifted Structured Feature Embedding

  Learning the distance metric between pairs of examples is of great importance
for learning and visual recognition. With the remarkable success from the state
of the art convolutional neural networks, recent works have shown promising
results on discriminatively training the networks to learn semantic feature
embeddings where similar examples are mapped close to each other and dissimilar
examples are mapped farther apart. In this paper, we describe an algorithm for
taking full advantage of the training batches in the neural network training by
lifting the vector of pairwise distances within the batch to the matrix of
pairwise distances. This step enables the algorithm to learn the state of the
art feature embedding by optimizing a novel structured prediction objective on
the lifted problem. Additionally, we collected Online Products dataset: 120k
images of 23k classes of online products for metric learning. Our experiments
on the CUB-200-2011, CARS196, and Online Products datasets demonstrate
significant improvement over existing deep feature embedding methods on all
experimented embedding sizes with the GoogLeNet network.


Knowledge Transfer for Scene-specific Motion Prediction

  When given a single frame of the video, humans can not only interpret the
content of the scene, but also they are able to forecast the near future. This
ability is mostly driven by their rich prior knowledge about the visual world,
both in terms of (i) the dynamics of moving agents, as well as (ii) the
semantic of the scene. In this work we exploit the interplay between these two
key elements to predict scene-specific motion patterns. First, we extract patch
descriptors encoding the probability of moving to the adjacent patches, and the
probability of being in that particular patch or changing behavior. Then, we
introduce a Dynamic Bayesian Network which exploits this scene specific
knowledge for trajectory prediction. Experimental results demonstrate that our
method is able to accurately predict trajectories and transfer predictions to a
novel scene characterized by similar elements.


Unsupervised Semantic Parsing of Video Collections

  Human communication typically has an underlying structure. This is reflected
in the fact that in many user generated videos, a starting point, ending, and
certain objective steps between these two can be identified. In this paper, we
propose a method for parsing a video into such semantic steps in an
unsupervised way. The proposed method is capable of providing a semantic
"storyline" of the video composed of its objective steps. We accomplish this
using both visual and language cues in a joint generative model. The proposed
method can also provide a textual description for each of the identified
semantic steps and video segments. We evaluate this method on a large number of
complex YouTube videos and show results of unprecedented quality for this
intricate and impactful problem.


Deep Learning for Single-View Instance Recognition

  Deep learning methods have typically been trained on large datasets in which
many training examples are available. However, many real-world product datasets
have only a small number of images available for each product. We explore the
use of deep learning methods for recognizing object instances when we have only
a single training example per class. We show that feedforward neural networks
outperform state-of-the-art methods for recognizing objects from novel
viewpoints even when trained from just a single image per object. To further
improve our performance on this task, we propose to take advantage of a
supplementary dataset in which we observe a separate set of objects from
multiple viewpoints. We introduce a new approach for training deep learning
methods for instance recognition with limited training data, in which we use an
auxiliary multi-view dataset to train our network to be robust to viewpoint
changes. We find that this approach leads to a more robust classifier for
recognizing objects from novel viewpoints, outperforming previous
state-of-the-art approaches including keypoint-matching, template-based
techniques, and sparse coding.


Action Recognition by Hierarchical Mid-level Action Elements

  Realistic videos of human actions exhibit rich spatiotemporal structures at
multiple levels of granularity: an action can always be decomposed into
multiple finer-grained elements in both space and time. To capture this
intuition, we propose to represent videos by a hierarchy of mid-level action
elements (MAEs), where each MAE corresponds to an action-related spatiotemporal
segment in the video. We introduce an unsupervised method to generate this
representation from videos. Our method is capable of distinguishing
action-related segments from background segments and representing actions at
multiple spatiotemporal resolutions. Given a set of spatiotemporal segments
generated from the training data, we introduce a discriminative clustering
algorithm that automatically discovers MAEs at multiple levels of granularity.
We develop structured models that capture a rich set of spatial, temporal and
hierarchical relations among the segments, where the action label and multiple
levels of MAE labels are jointly inferred. The proposed model achieves
state-of-the-art performance in multiple action recognition benchmarks.
Moreover, we demonstrate the effectiveness of our model in real-world
applications such as action recognition in large-scale untrimmed videos and
action parsing.


3D-R2N2: A Unified Approach for Single and Multi-view 3D Object
  Reconstruction

  Inspired by the recent success of methods that employ shape priors to achieve
robust 3D reconstructions, we propose a novel recurrent neural network
architecture that we call the 3D Recurrent Reconstruction Neural Network
(3D-R2N2). The network learns a mapping from images of objects to their
underlying 3D shapes from a large collection of synthetic data. Our network
takes in one or more images of an object instance from arbitrary viewpoints and
outputs a reconstruction of the object in the form of a 3D occupancy grid.
Unlike most of the previous works, our network does not require any image
annotations or object class labels for training or testing. Our extensive
experimental analysis shows that our reconstruction framework i) outperforms
the state-of-the-art methods for single view reconstruction, and ii) enables
the 3D reconstruction of objects in situations when traditional SFM/SLAM
methods fail (because of lack of texture and/or wide baseline).


Subcategory-aware Convolutional Neural Networks for Object Proposals and
  Detection

  In CNN-based object detection methods, region proposal becomes a bottleneck
when objects exhibit significant scale variation, occlusion or truncation. In
addition, these methods mainly focus on 2D object detection and cannot estimate
detailed properties of objects. In this paper, we propose subcategory-aware
CNNs for object detection. We introduce a novel region proposal network that
uses subcategory information to guide the proposal generating process, and a
new detection network for joint detection and subcategory classification. By
using subcategories related to object pose, we achieve state-of-the-art
performance on both detection and pose estimation on commonly used benchmarks.


Unsupervised Semantic Action Discovery from Video Collections

  Human communication takes many forms, including speech, text and
instructional videos. It typically has an underlying structure, with a starting
point, ending, and certain objective steps between them. In this paper, we
consider instructional videos where there are tens of millions of them on the
Internet.
  We propose a method for parsing a video into such semantic steps in an
unsupervised way. Our method is capable of providing a semantic "storyline" of
the video composed of its objective steps. We accomplish this using both visual
and language cues in a joint generative model. Our method can also provide a
textual description for each of the identified semantic steps and video
segments. We evaluate our method on a large number of complex YouTube videos
and show that our method discovers semantically correct instructions for a
variety of tasks.


Social Scene Understanding: End-to-End Multi-Person Action Localization
  and Collective Activity Recognition

  We present a unified framework for understanding human social behaviors in
raw image sequences. Our model jointly detects multiple individuals, infers
their social actions, and estimates the collective actions with a single
feed-forward pass through a neural network. We propose a single architecture
that does not rely on external detection algorithms but rather is trained
end-to-end to generate dense proposal maps that are refined via a novel
inference scheme. The temporal consistency is handled via a person-level
matching Recurrent Neural Network. The complete model takes as input a sequence
of frames and outputs detections along with the estimates of individual actions
and collective activities. We demonstrate state-of-the-art performance of our
algorithm on multiple publicly available benchmarks.


Tracking The Untrackable: Learning To Track Multiple Cues with Long-Term
  Dependencies

  The majority of existing solutions to the Multi-Target Tracking (MTT) problem
do not combine cues in a coherent end-to-end fashion over a long period of
time. However, we present an online method that encodes long-term temporal
dependencies across multiple cues. One key challenge of tracking methods is to
accurately track occluded targets or those which share similar appearance
properties with surrounding objects. To address this challenge, we present a
structure of Recurrent Neural Networks (RNN) that jointly reasons on multiple
cues over a temporal window. We are able to correct many data association
errors and recover observations from an occluded state. We demonstrate the
robustness of our data-driven approach by tracking multiple targets using their
appearance, motion, and even interactions. Our method outperforms previous
works on multiple publicly available datasets including the challenging MOT
benchmark.


Joint 2D-3D-Semantic Data for Indoor Scene Understanding

  We present a dataset of large-scale indoor spaces that provides a variety of
mutually registered modalities from 2D, 2.5D and 3D domains, with
instance-level semantic and geometric annotations. The dataset covers over
6,000m2 and contains over 70,000 RGB images, along with the corresponding
depths, surface normals, semantic annotations, global XYZ images (all in forms
of both regular and 360{\deg} equirectangular images) as well as camera
information. It also includes registered raw and semantically annotated 3D
meshes and point clouds. The dataset enables development of joint and
cross-modal learning models and potentially unsupervised approaches utilizing
the regularities present in large-scale indoor spaces. The dataset is available
here: http://3Dsemantics.stanford.edu/


Deep View Morphing

  Recently, convolutional neural networks (CNN) have been successfully applied
to view synthesis problems. However, such CNN-based methods can suffer from
lack of texture details, shape distortions, or high computational complexity.
In this paper, we propose a novel CNN architecture for view synthesis called
"Deep View Morphing" that does not suffer from these issues. To synthesize a
middle view of two input images, a rectification network first rectifies the
two input images. An encoder-decoder network then generates dense
correspondences between the rectified images and blending masks to predict the
visibility of pixels of the rectified images in the middle view. A view
morphing network finally synthesizes the middle view using the dense
correspondences and blending masks. We experimentally show the proposed method
significantly outperforms the state-of-the-art CNN-based view synthesis method.


Weakly supervised 3D Reconstruction with Adversarial Constraint

  Supervised 3D reconstruction has witnessed a significant progress through the
use of deep neural networks. However, this increase in performance requires
large scale annotations of 2D/3D data. In this paper, we explore inexpensive 2D
supervision as an alternative for expensive 3D CAD annotation. Specifically, we
use foreground masks as weak supervision through a raytrace pooling layer that
enables perspective projection and backpropagation. Additionally, since the 3D
reconstruction from masks is an ill posed problem, we propose to constrain the
3D reconstruction to the manifold of unlabeled realistic 3D shapes that match
mask observations. We demonstrate that learning a log-barrier solution to this
constrained optimization problem resembles the GAN objective, enabling the use
of existing tools for training GANs. We evaluate and analyze the manifold
constrained reconstruction on various datasets for single and multi-view
reconstruction of both synthetic and real images.


DeformNet: Free-Form Deformation Network for 3D Shape Reconstruction
  from a Single Image

  3D reconstruction from a single image is a key problem in multiple
applications ranging from robotic manipulation to augmented reality. Prior
methods have tackled this problem through generative models which predict 3D
reconstructions as voxels or point clouds. However, these methods can be
computationally expensive and miss fine details. We introduce a new
differentiable layer for 3D data deformation and use it in DeformNet to learn a
model for 3D reconstruction-through-deformation. DeformNet takes an image
input, searches the nearest shape template from a database, and deforms the
template to match the query image. We evaluate our approach on the ShapeNet
dataset and show that - (a) the Free-Form Deformation layer is a powerful new
building block for Deep Learning models that manipulate 3D data (b) DeformNet
uses this FFD layer combined with shape retrieval for smooth and
detail-preserving 3D reconstruction of qualitatively plausible point clouds
with respect to a single query image (c) compared to other state-of-the-art 3D
reconstruction methods, DeformNet quantitatively matches or outperforms their
benchmarks by significant margins. For more information, visit:
https://deformnet-site.github.io/DeformNet-website/ .


Neural Task Programming: Learning to Generalize Across Hierarchical
  Tasks

  In this work, we propose a novel robot learning framework called Neural Task
Programming (NTP), which bridges the idea of few-shot learning from
demonstration and neural program induction. NTP takes as input a task
specification (e.g., video demonstration of a task) and recursively decomposes
it into finer sub-task specifications. These specifications are fed to a
hierarchical neural program, where bottom-level programs are callable
subroutines that interact with the environment. We validate our method in three
robot manipulation tasks. NTP achieves strong generalization across sequential
tasks that exhibit hierarchal and compositional structures. The experimental
results show that NTP learns to generalize well to- wards unseen tasks with
increasing lengths, variable topologies, and changing objectives.


Multi-Task Domain Adaptation for Deep Learning of Instance Grasping from
  Simulation

  Learning-based approaches to robotic manipulation are limited by the
scalability of data collection and accessibility of labels. In this paper, we
present a multi-task domain adaptation framework for instance grasping in
cluttered scenes by utilizing simulated robot experiments. Our neural network
takes monocular RGB images and the instance segmentation mask of a specified
target object as inputs, and predicts the probability of successfully grasping
the specified object for each candidate motor command. The proposed transfer
learning framework trains a model for instance grasping in simulation and uses
a domain-adversarial loss to transfer the trained model to real robots using
indiscriminate grasping data, which is available both in simulation and the
real world. We evaluate our model in real-world robot experiments, comparing it
with alternative model architectures as well as an indiscriminate grasping
baseline.


SEGCloud: Semantic Segmentation of 3D Point Clouds

  3D semantic scene labeling is fundamental to agents operating in the real
world. In particular, labeling raw 3D point sets from sensors provides
fine-grained semantics. Recent works leverage the capabilities of Neural
Networks (NNs), but are limited to coarse voxel predictions and do not
explicitly enforce global consistency. We present SEGCloud, an end-to-end
framework to obtain 3D point-level segmentation that combines the advantages of
NNs, trilinear interpolation(TI) and fully connected Conditional Random Fields
(FC-CRF). Coarse voxel predictions from a 3D Fully Convolutional NN are
transferred back to the raw 3D points via trilinear interpolation. Then the
FC-CRF enforces global consistency and provides fine-grained semantics on the
points. We implement the latter as a differentiable Recurrent NN to allow joint
optimization. We evaluate the framework on two indoor and two outdoor 3D
datasets (NYU V2, S3DIS, KITTI, Semantic3D.net), and show performance
comparable or superior to the state-of-the-art on all datasets.


Recurrent Autoregressive Networks for Online Multi-Object Tracking

  The main challenge of online multi-object tracking is to reliably associate
object trajectories with detections in each video frame based on their tracking
history. In this work, we propose the Recurrent Autoregressive Network (RAN), a
temporal generative modeling framework to characterize the appearance and
motion dynamics of multiple objects over time. The RAN couples an external
memory and an internal memory. The external memory explicitly stores previous
inputs of each trajectory in a time window, while the internal memory learns to
summarize long-term tracking history and associate detections by processing the
external memory. We conduct experiments on the MOT 2015 and 2016 datasets to
demonstrate the robustness of our tracking method in highly crowded and
occluded scenes. Our method achieves top-ranked results on the two benchmarks.


Adversarial Feature Augmentation for Unsupervised Domain Adaptation

  Recent works showed that Generative Adversarial Networks (GANs) can be
successfully applied in unsupervised domain adaptation, where, given a labeled
source dataset and an unlabeled target dataset, the goal is to train powerful
classifiers for the target samples. In particular, it was shown that a GAN
objective function can be used to learn target features indistinguishable from
the source ones. In this work, we extend this framework by (i) forcing the
learned feature extractor to be domain-invariant, and (ii) training it through
data augmentation in the feature space, namely performing feature augmentation.
While data augmentation in the image space is a well established technique in
deep learning, feature augmentation has not yet received the same level of
attention. We accomplish it by means of a feature generator trained by playing
the GAN minimax game against source features. Results show that both enforcing
domain-invariance and performing feature augmentation lead to superior or
comparable performance to state-of-the-art results in several unsupervised
domain adaptation benchmarks.


Im2Pano3D: Extrapolating 360 Structure and Semantics Beyond the Field of
  View

  We present Im2Pano3D, a convolutional neural network that generates a dense
prediction of 3D structure and a probability distribution of semantic labels
for a full 360 panoramic view of an indoor scene when given only a partial
observation (<= 50%) in the form of an RGB-D image. To make this possible,
Im2Pano3D leverages strong contextual priors learned from large-scale synthetic
and real-world indoor scenes. To ease the prediction of 3D structure, we
propose to parameterize 3D surfaces with their plane equations and train the
model to predict these parameters directly. To provide meaningful training
supervision, we use multiple loss functions that consider both pixel level
accuracy and global context consistency. Experiments demon- strate that
Im2Pano3D is able to predict the semantics and 3D structure of the unobserved
scene with more than 56% pixel accuracy and less than 0.52m average distance
error, which is significantly better than alternative approaches.


GONet: A Semi-Supervised Deep Learning Approach For Traversability
  Estimation

  We present semi-supervised deep learning approaches for traversability
estimation from fisheye images. Our method, GONet, and the proposed extensions
leverage Generative Adversarial Networks (GANs) to effectively predict whether
the area seen in the input image(s) is safe for a robot to traverse. These
methods are trained with many positive images of traversable places, but just a
small set of negative images depicting blocked and unsafe areas. This makes the
proposed methods practical. Positive examples can be collected easily by simply
operating a robot through traversable spaces, while obtaining negative examples
is time consuming, costly, and potentially dangerous. Through extensive
experiments and several demonstrations, we show that the proposed
traversability estimation approaches are robust and can generalize to unseen
scenarios. Further, we demonstrate that our methods are memory efficient and
fast, allowing for real-time operation on a mobile robot with single or stereo
fisheye cameras. As part of our contributions, we open-source two new datasets
for traversability estimation. These datasets are composed of approximately 24h
of videos from more than 25 indoor environments. Our methods outperform
baseline approaches for traversability estimation on these new datasets.


Text2Shape: Generating Shapes from Natural Language by Learning Joint
  Embeddings

  We present a method for generating colored 3D shapes from natural language.
To this end, we first learn joint embeddings of freeform text descriptions and
colored 3D shapes. Our model combines and extends learning by association and
metric learning approaches to learn implicit cross-modal connections, and
produces a joint representation that captures the many-to-many relations
between language and physical properties of 3D shapes such as color and shape.
To evaluate our approach, we collect a large dataset of natural language
descriptions for physical 3D objects in the ShapeNet dataset. With this learned
joint embedding we demonstrate text-to-shape retrieval that outperforms
baseline approaches. Using our embeddings with a novel conditional Wasserstein
GAN framework, we generate colored 3D shapes from text. Our method is the first
to connect natural language text with realistic 3D objects exhibiting rich
variations in color, texture, and shape detail. See video at
https://youtu.be/zraPvRdl13Q


Social GAN: Socially Acceptable Trajectories with Generative Adversarial
  Networks

  Understanding human motion behavior is critical for autonomous moving
platforms (like self-driving cars and social robots) if they are to navigate
human-centric environments. This is challenging because human motion is
inherently multimodal: given a history of human motion paths, there are many
socially plausible ways that people could move in the future. We tackle this
problem by combining tools from sequence prediction and generative adversarial
networks: a recurrent sequence-to-sequence model observes motion histories and
predicts future behavior, using a novel pooling mechanism to aggregate
information across people. We predict socially plausible futures by training
adversarially against a recurrent discriminator, and encourage diverse
predictions with a novel variety loss. Through experiments on several datasets
we demonstrate that our approach outperforms prior work in terms of accuracy,
variety, collision avoidance, and computational complexity.


Deep Learning under Privileged Information Using Heteroscedastic Dropout

  Unlike machines, humans learn through rapid, abstract model-building. The
role of a teacher is not simply to hammer home right or wrong answers, but
rather to provide intuitive comments, comparisons, and explanations to a pupil.
This is what the Learning Under Privileged Information (LUPI) paradigm
endeavors to model by utilizing extra knowledge only available during training.
We propose a new LUPI algorithm specifically designed for Convolutional Neural
Networks (CNNs) and Recurrent Neural Networks (RNNs). We propose to use a
heteroscedastic dropout (i.e. dropout with a varying variance) and make the
variance of the dropout a function of privileged information. Intuitively, this
corresponds to using the privileged information to control the uncertainty of
the model output. We perform experiments using CNNs and RNNs for the tasks of
image classification and machine translation. Our method significantly
increases the sample efficiency during learning, resulting in higher accuracy
with a large margin when the number of training examples is limited. We also
theoretically justify the gains in sample efficiency by providing a
generalization error bound decreasing with $O(\frac{1}{n})$, where $n$ is the
number of training examples, in an oracle case.


Generalizing to Unseen Domains via Adversarial Data Augmentation

  We are concerned with learning models that generalize well to different
\emph{unseen} domains. We consider a worst-case formulation over data
distributions that are near the source domain in the feature space. Only using
training data from a single source distribution, we propose an iterative
procedure that augments the dataset with examples from a fictitious target
domain that is "hard" under the current model. We show that our iterative
scheme is an adaptive data augmentation method where we append adversarial
examples at each iteration. For softmax losses, we show that our method is a
data-dependent regularization scheme that behaves differently from classical
regularizers that regularize towards zero (e.g., ridge or lasso). On digit
recognition and semantic segmentation tasks, our method learns models improve
performance across a range of a priori unknown target domains.


VUNet: Dynamic Scene View Synthesis for Traversability Estimation using
  an RGB Camera

  We present VUNet, a novel view(VU) synthesis method for mobile robots in
dynamic environments, and its application to the estimation of future
traversability. Our method predicts future images for given virtual robot
velocity commands using only RGB images at previous and current time steps. The
future images result from applying two types of image changes to the previous
and current images: 1) changes caused by different camera pose, and 2) changes
due to the motion of the dynamic obstacles. We learn to predict these two types
of changes disjointly using two novel network architectures, SNet and DNet. We
combine SNet and DNet to synthesize future images that we pass to our
previously presented method GONet to estimate the traversable areas around the
robot. Our quantitative and qualitative evaluation indicate that our approach
for view synthesis predicts accurate future images in both static and dynamic
environments. We also show that these virtual images can be used to estimate
future traversability correctly. We apply our view synthesis-based
traversability estimation method to two applications for assisted
teleoperation.


Learning Task-Oriented Grasping for Tool Manipulation from Simulated
  Self-Supervision

  Tool manipulation is vital for facilitating robots to complete challenging
task goals. It requires reasoning about the desired effect of the task and thus
properly grasping and manipulating the tool to achieve the task. Task-agnostic
grasping optimizes for grasp robustness while ignoring crucial task-specific
constraints. In this paper, we propose the Task-Oriented Grasping Network
(TOG-Net) to jointly optimize both task-oriented grasping of a tool and the
manipulation policy for that tool. The training process of the model is based
on large-scale simulated self-supervision with procedurally generated tool
objects. We perform both simulated and real-world experiments on two tool-based
manipulation tasks: sweeping and hammering. Our model achieves overall 71.1%
task success rate for sweeping and 80.0% task success rate for hammering.
Supplementary material is available at: bit.ly/task-oriented-grasp


Neural Task Graphs: Generalizing to Unseen Tasks from a Single Video
  Demonstration

  Our goal is to generate a policy to complete an unseen task given just a
single video demonstration of the task in a given domain. We hypothesize that
to successfully generalize to unseen complex tasks from a single video
demonstration, it is necessary to explicitly incorporate the compositional
structure of the tasks into the model. To this end, we propose Neural Task
Graph (NTG) Networks, which use conjugate task graph as the intermediate
representation to modularize both the video demonstration and the derived
policy. We empirically show NTG achieves inter-task generalization on two
complex tasks: Block Stacking in BulletPhysics and Object Collection in
AI2-THOR. NTG improves data efficiency with visual input as well as achieve
strong generalization without the need for dense hierarchical supervision. We
further show that similar performance trends hold when applied to real-world
data. We show that NTG can effectively predict task structure on the JIGSAWS
surgical dataset and generalize to unseen tasks.


Gibson Env: Real-World Perception for Embodied Agents

  Developing visual perception models for active agents and sensorimotor
control are cumbersome to be done in the physical world, as existing algorithms
are too slow to efficiently learn in real-time and robots are fragile and
costly. This has given rise to learning-in-simulation which consequently casts
a question on whether the results transfer to real-world. In this paper, we are
concerned with the problem of developing real-world perception for active
agents, propose Gibson Virtual Environment for this purpose, and showcase
sample perceptual tasks learned therein. Gibson is based on virtualizing real
spaces, rather than using artificially designed ones, and currently includes
over 1400 floor spaces from 572 full buildings. The main characteristics of
Gibson are: I. being from the real-world and reflecting its semantic
complexity, II. having an internal synthesis mechanism, "Goggles", enabling
deploying the trained models in real-world without needing further domain
adaptation, III. embodiment of agents and making them subject to constraints of
physics and space.


Making Sense of Vision and Touch: Self-Supervised Learning of Multimodal
  Representations for Contact-Rich Tasks

  Contact-rich manipulation tasks in unstructured environments often require
both haptic and visual feedback. However, it is non-trivial to manually design
a robot controller that combines modalities with very different
characteristics. While deep reinforcement learning has shown success in
learning control policies for high-dimensional inputs, these algorithms are
generally intractable to deploy on real robots due to sample complexity. We use
self-supervision to learn a compact and multimodal representation of our
sensory inputs, which can then be used to improve the sample efficiency of our
policy learning. We evaluate our method on a peg insertion task, generalizing
over different geometry, configurations, and clearances, while being robust to
external perturbations. Results for simulated and real robot experiments are
presented.


DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion

  A key technical challenge in performing 6D object pose estimation from RGB-D
image is to fully leverage the two complementary data sources. Prior works
either extract information from the RGB image and depth separately or use
costly post-processing steps, limiting their performances in highly cluttered
scenes and real-time applications. In this work, we present DenseFusion, a
generic framework for estimating 6D pose of a set of known objects from RGB-D
images. DenseFusion is a heterogeneous architecture that processes the two data
sources individually and uses a novel dense fusion network to extract
pixel-wise dense feature embedding, from which the pose is estimated.
Furthermore, we integrate an end-to-end iterative pose refinement procedure
that further improves the pose estimation while achieving near real-time
inference. Our experiments show that our method outperforms state-of-the-art
approaches in two datasets, YCB-Video and LineMOD. We also deploy our proposed
method to a real robot to grasp and manipulate objects based on the estimated
pose.


Generalized Intersection over Union: A Metric and A Loss for Bounding
  Box Regression

  Intersection over Union (IoU) is the most popular evaluation metric used in
the object detection benchmarks. However, there is a gap between optimizing the
commonly used distance losses for regressing the parameters of a bounding box
and maximizing this metric value. The optimal objective for a metric is the
metric itself. In the case of axis-aligned 2D bounding boxes, it can be shown
that $IoU$ can be directly used as a regression loss. However, $IoU$ has a
plateau making it infeasible to optimize in the case of non-overlapping
bounding boxes. In this paper, we address the weaknesses of $IoU$ by
introducing a generalized version as both a new loss and a new metric. By
incorporating this generalized $IoU$ ($GIoU$) as a loss into the state-of-the
art object detection frameworks, we show a consistent improvement on their
performance using both the standard, $IoU$ based, and new, $GIoU$ based,
performance measures on popular object detection benchmarks such as PASCAL VOC
and MS COCO.


A Behavioral Approach to Visual Navigation with Graph Localization
  Networks

  Inspired by research in psychology, we introduce a behavioral approach for
visual navigation using topological maps. Our goal is to enable a robot to
navigate from one location to another, relying only on its visual input and the
topological map of the environment. We propose using graph neural networks for
localizing the agent in the map, and decompose the action space into primitive
behaviors implemented as convolutional or recurrent neural networks. Using the
Gibson simulator, we verify that our approach outperforms relevant baselines
and is able to navigate in both seen and unseen environments.


Scene Memory Transformer for Embodied Agents in Long-Horizon Tasks

  Many robotic applications require the agent to perform long-horizon tasks in
partially observable environments. In such applications, decision making at any
step can depend on observations received far in the past. Hence, being able to
properly memorize and utilize the long-term history is crucial. In this work,
we propose a novel memory-based policy, named Scene Memory Transformer (SMT).
The proposed policy embeds and adds each observation to a memory and uses the
attention mechanism to exploit spatio-temporal dependencies. This model is
generic and can be efficiently trained with reinforcement learning over long
episodes. On a range of visual navigation tasks, SMT demonstrates superior
performance to existing reactive and memory-based policies by a margin.


Structural-RNN: Deep Learning on Spatio-Temporal Graphs

  Deep Recurrent Neural Network architectures, though remarkably capable at
modeling sequences, lack an intuitive high-level spatio-temporal structure.
That is while many problems in computer vision inherently have an underlying
high-level structure and can benefit from it. Spatio-temporal graphs are a
popular tool for imposing such high-level intuitions in the formulation of real
world problems. In this paper, we propose an approach for combining the power
of high-level spatio-temporal graphs and sequence learning success of Recurrent
Neural Networks~(RNNs). We develop a scalable method for casting an arbitrary
spatio-temporal graph as a rich RNN mixture that is feedforward, fully
differentiable, and jointly trainable. The proposed method is generic and
principled as it can be used for transforming any spatio-temporal graph through
employing a certain set of well defined steps. The evaluations of the proposed
approach on a diverse set of problems, ranging from modeling human motion to
object interactions, shows improvement over the state-of-the-art with a large
margin. We expect this method to empower new approaches to problem formulation
through high-level spatio-temporal graphs and Recurrent Neural Networks.


Forecasting Social Navigation in Crowded Complex Scenes

  When humans navigate a crowed space such as a university campus or the
sidewalks of a busy street, they follow common sense rules based on social
etiquette. In this paper, we argue that in order to enable the design of new
algorithms that can take fully advantage of these rules to better solve tasks
such as target tracking or trajectory forecasting, we need to have access to
better data in the first place. To that end, we contribute the very first large
scale dataset (to the best of our knowledge) that collects images and videos of
various types of targets (not just pedestrians, but also bikers, skateboarders,
cars, buses, golf carts) that navigate in a real-world outdoor environment such
as a university campus. We present an extensive evaluation where different
methods for trajectory forecasting are evaluated and compared. Moreover, we
present a new algorithm for trajectory prediction that exploits the complexity
of our new dataset and allows to: i) incorporate inter-class interactions into
trajectory prediction models (e.g, pedestrian vs bike) as opposed to just
intra-class interactions (e.g., pedestrian vs pedestrian); ii) model the degree
to which the social forces are regulating an interaction. We call the latter
"social sensitivity"and it captures the sensitivity to which a target is
responding to a certain interaction. An extensive experimental evaluation
demonstrates the effectiveness of our novel approach.


Watch-n-Patch: Unsupervised Learning of Actions and Relations

  There is a large variation in the activities that humans perform in their
everyday lives. We consider modeling these composite human activities which
comprises multiple basic level actions in a completely unsupervised setting.
Our model learns high-level co-occurrence and temporal relations between the
actions. We consider the video as a sequence of short-term action clips, which
contains human-words and object-words. An activity is about a set of
action-topics and object-topics indicating which actions are present and which
objects are interacting with. We then propose a new probabilistic model
relating the words and the topics. It allows us to model long-range action
relations that commonly exist in the composite activities, which is challenging
in previous works. We apply our model to the unsupervised action segmentation
and clustering, and to a novel application that detects forgotten actions,
which we call action patching. For evaluation, we contribute a new challenging
RGB-D activity video dataset recorded by the new Kinect v2, which contains
several human daily activities as compositions of multiple actions interacting
with different objects. Moreover, we develop a robotic system that watches
people and reminds people by applying our action patching algorithm. Our
robotic setup can be easily deployed on any assistive robot.


Learning to Track at 100 FPS with Deep Regression Networks

  Machine learning techniques are often used in computer vision due to their
ability to leverage large amounts of training data to improve performance.
Unfortunately, most generic object trackers are still trained from scratch
online and do not benefit from the large number of videos that are readily
available for offline training. We propose a method for offline training of
neural networks that can track novel objects at test-time at 100 fps. Our
tracker is significantly faster than previous methods that use neural networks
for tracking, which are typically very slow to run and not practical for
real-time applications. Our tracker uses a simple feed-forward network with no
online training required. The tracker learns a generic relationship between
object motion and appearance and can be used to track novel objects that do not
appear in the training set. We test our network on a standard tracking
benchmark to demonstrate our tracker's state-of-the-art performance. Further,
our performance improves as we add more videos to our offline training set. To
the best of our knowledge, our tracker is the first neural-network tracker that
learns to track generic objects at 100 fps.


Feedback Networks

  Currently, the most successful learning models in computer vision are based
on learning successive representations followed by a decision layer. This is
usually actualized through feedforward multilayer neural networks, e.g.
ConvNets, where each layer forms one of such successive representations.
However, an alternative that can achieve the same goal is a feedback based
approach in which the representation is formed in an iterative manner based on
a feedback received from previous iteration's output.
  We establish that a feedback based approach has several fundamental
advantages over feedforward: it enables making early predictions at the query
time, its output naturally conforms to a hierarchical structure in the label
space (e.g. a taxonomy), and it provides a new basis for Curriculum Learning.
We observe that feedback networks develop a considerably different
representation compared to feedforward counterparts, in line with the
aforementioned advantages. We put forth a general feedback based learning
architecture with the endpoint results on par or better than existing
feedforward networks with the addition of the above advantages. We also
investigate several mechanisms in feedback architectures (e.g. skip connections
in time) and design choices (e.g. feedback length). We hope this study offers
new perspectives in quest for more natural and practical learning models.


ADAPT: Zero-Shot Adaptive Policy Transfer for Stochastic Dynamical
  Systems

  Model-free policy learning has enabled robust performance of complex tasks
with relatively simple algorithms. However, this simplicity comes at the cost
of requiring an Oracle and arguably very poor sample complexity. This renders
such methods unsuitable for physical systems. Variants of model-based methods
address this problem through the use of simulators, however, this gives rise to
the problem of policy transfer from simulated to the physical system. Model
mismatch due to systematic parameter shift and unmodelled dynamics error may
cause sub-optimal or unsafe behavior upon direct transfer. We introduce the
Adaptive Policy Transfer for Stochastic Dynamics (ADAPT) algorithm that
achieves provably safe and robust, dynamically-feasible zero-shot transfer of
RL-policies to new domains with dynamics error. ADAPT combines the strengths of
offline policy learning in a black-box source simulator with online tube-based
MPC to attenuate bounded model mismatch between the source and target dynamics.
ADAPT allows online transfer of policy, trained solely in a simulation offline,
to a family of unknown targets without fine-tuning. We also formally show that
(i) ADAPT guarantees state and control safety through state-action tubes under
the assumption of Lipschitz continuity of the divergence in dynamics and, (ii)
ADAPT results in a bounded loss of reward accumulation relative to a policy
trained and evaluated in the source environment. We evaluate ADAPT on 2
continuous, non-holonomic simulated dynamical systems with 4 different
disturbance models, and find that ADAPT performs between 50%-300% better on
mean reward accrual than direct policy transfer.


Active Learning for Convolutional Neural Networks: A Core-Set Approach

  Convolutional neural networks (CNNs) have been successfully applied to many
recognition and learning tasks using a universal recipe; training a deep model
on a very large dataset of supervised examples. However, this approach is
rather restrictive in practice since collecting a large set of labeled images
is very expensive. One way to ease this problem is coming up with smart ways
for choosing images to be labelled from a very large collection (ie. active
learning).
  Our empirical study suggests that many of the active learning heuristics in
the literature are not effective when applied to CNNs in batch setting.
Inspired by these limitations, we define the problem of active learning as
core-set selection, ie. choosing set of points such that a model learned over
the selected subset is competitive for the remaining data points. We further
present a theoretical result characterizing the performance of any selected
subset using the geometry of the datapoints. As an active learning algorithm,
we choose the subset which is expected to yield best result according to our
characterization. Our experiments show that the proposed method significantly
outperforms existing approaches in image classification experiments by a large
margin.


