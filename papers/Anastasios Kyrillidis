Non-uniform Feature Sampling for Decision Tree Ensembles

  We study the effectiveness of non-uniform randomized feature selection indecision tree classification. We experimentally evaluate two feature selectionmethodologies, based on information extracted from the provided dataset: $(i)$\emph{leverage scores-based} and $(ii)$ \emph{norm-based} feature selection.Experimental evaluation of the proposed feature selection techniques indicatethat such approaches might be more effective compared to naive uniform featureselection and moreover having comparable performance to the random forestalgorithm [3]

Matrix ALPS: Accelerated Low Rank and Sparse Matrix Reconstruction

  We propose Matrix ALPS for recovering a sparse plus low-rank decomposition ofa matrix given its corrupted and incomplete linear measurements. Our approachis a first-order projected gradient method over non-convex sets, and itexploits a well-known memory-based acceleration technique. We theoreticallycharacterize the convergence properties of Matrix ALPS using the stableembedding properties of the linear measurement operator. We then numericallyillustrate that our algorithm outperforms the existing convex as well asnon-convex state-of-the-art algorithms in computational efficiency withoutsacrificing stability.

Matrix Recipes for Hard Thresholding Methods

  In this paper, we present and analyze a new set of low-rank recoveryalgorithms for linear inverse problems within the class of hard thresholdingmethods. We provide strategies on how to set up these algorithms via basicingredients for different configurations to achieve complexity vs. accuracytradeoffs. Moreover, we study acceleration schemes via memory-based techniquesand randomized, $\epsilon$-approximate matrix projections to decrease thecomputational costs in the recovery process. For most of the configurations, wepresent theoretical analysis that guarantees convergence under mild problemconditions. Simulation results demonstrate notable performance improvements ascompared to state-of-the-art algorithms both in terms of reconstructionaccuracy and computational complexity.

Composite Self-Concordant Minimization

  We propose a variable metric framework for minimizing the sum of aself-concordant function and a possibly non-smooth convex function, endowedwith an easily computable proximal operator. We theoretically establish theconvergence of our framework without relying on the usual Lipschitz gradientassumption on the smooth part. An important highlight of our work is a new setof analytic step-size selection and correction procedures based on thestructure of the problem. We describe concrete algorithmic instances of ourframework for several interesting applications and demonstrate them numericallyon both synthetic and real data.

Linear Inverse Problems with Norm and Sparsity Constraints

  We describe two nonconventional algorithms for linear regression, called GAMEand CLASH. The salient characteristics of these approaches is that they exploitthe convex $\ell_1$-ball and non-convex $\ell_0$-sparsity constraints jointlyin sparse recovery. To establish the theoretical approximation guarantees ofGAME and CLASH, we cover an interesting range of topics from game theory,convex and combinatorial optimization. We illustrate that these approaches leadto improved theoretical guarantees and empirical performance beyond convex andnon-convex solvers alone.

Non-square matrix sensing without spurious local minima via the  Burer-Monteiro approach

  We consider the non-square matrix sensing problem, under restricted isometryproperty (RIP) assumptions. We focus on the non-convex formulation, where anyrank-$r$ matrix $X \in \mathbb{R}^{m \times n}$ is represented as $UV^\top$,where $U \in \mathbb{R}^{m \times r}$ and $V \in \mathbb{R}^{n \times r}$. Inthis paper, we complement recent findings on the non-convex geometry of theanalogous PSD setting [5], and show that matrix factorization does notintroduce any spurious local minima, under RIP.

IHT dies hard: Provable accelerated Iterative Hard Thresholding

  We study --both in theory and practice-- the use of momentum motions inclassic iterative hard thresholding (IHT) methods. By simply modifying plainIHT, we investigate its convergence behavior on convex optimization criteriawith non-convex constraints, under standard assumptions. In diverse scenaria,we observe that acceleration in IHT leads to significant improvements, comparedto state of the art projected gradient descent and Frank-Wolfe variants. As abyproduct of our inspection, we study the impact of selecting the momentumparameter: similar to convex settings, two modes of behavior are observed--"rippling" and linear-- depending on the level of momentum.

Approximate Matrix Multiplication with Application to Linear Embeddings

  In this paper, we study the problem of approximately computing the product oftwo real matrices. In particular, we analyze a dimensionality-reduction-basedapproximation algorithm due to Sarlos [1], introducing the notion of nuclearrank as the ratio of the nuclear norm over the spectral norm. The presentedbound has improved dependence with respect to the approximation error (ascompared to previous approaches), whereas the subspace -- on which we projectthe input matrices -- has dimensions proportional to the maximum of theirnuclear rank and it is independent of the input dimensions. In addition, weprovide an application of this result to linear low-dimensional embeddings.Namely, we show that any Euclidean point-set with bounded nuclear rank isamenable to projection onto number of dimensions that is independent of theinput dimensionality, while achieving additive error guarantees.

Combinatorial Selection and Least Absolute Shrinkage via the CLASH  Algorithm

  The least absolute shrinkage and selection operator (LASSO) for linearregression exploits the geometric interplay of the $\ell_2$-data errorobjective and the $\ell_1$-norm constraint to arbitrarily select sparse models.Guiding this uninformed selection process with sparsity models has beenprecisely the center of attention over the last decade in order to improvelearning performance. To this end, we alter the selection process of LASSO toexplicitly leverage combinatorial sparsity models (CSMs) via the combinatorialselection and least absolute shrinkage (CLASH) operator. We provide concreteguidelines how to leverage combinatorial constraints within CLASH, andcharacterize CLASH's guarantees as a function of the set restricted isometryconstants of the sensing matrix. Finally, our experimental results show thatCLASH can outperform both LASSO and model-based compressive sensing in sparseestimation.

Sparse projections onto the simplex

  Most learning methods with rank or sparsity constraints use convexrelaxations, which lead to optimization with the nuclear norm or the$\ell_1$-norm. However, several important learning applications cannot benefitfrom this approach as they feature these convex norms as constraints inaddition to the non-convex rank and sparsity constraints. In this setting, wederive efficient sparse projections onto the simplex and its extension, andillustrate how to use them to solve high-dimensional learning problems inquantum tomography, sparse density estimation and portfolio selection withnon-convex constraints.

Scalable sparse covariance estimation via self-concordance

  We consider the class of convex minimization problems, composed of aself-concordant function, such as the $\log\det$ metric, a convex data fidelityterm $h(\cdot)$ and, a regularizing -- possibly non-smooth -- function$g(\cdot)$. This type of problems have recently attracted a great deal ofinterest, mainly due to their omnipresence in top-notch applications. Underthis \emph{locally} Lipschitz continuous gradient setting, we analyze theconvergence behavior of proximal Newton schemes with the added twist of aprobable presence of inexact evaluations. We prove attractive convergence rateguarantees and enhance state-of-the-art optimization schemes to accommodatesuch developments. Experimental results on sparse covariance estimation showthe merits of our algorithm, both in terms of recovery efficiency andcomplexity.

Stay on path: PCA along graph paths

  We introduce a variant of (sparse) PCA in which the set of feasible supportsets is determined by a graph. In particular, we consider the followingsetting: given a directed acyclic graph $G$ on $p$ vertices corresponding tovariables, the non-zero entries of the extracted principal component mustcoincide with vertices lying along a path in $G$.  From a statistical perspective, information on the underlying network maypotentially reduce the number of observations required to recover thepopulation principal component. We consider the canonical estimator whichoptimally exploits the prior knowledge by solving a non-convex quadraticmaximization on the empirical covariance. We introduce a simple network andanalyze the estimator under the spiked covariance model. We show that sideinformation potentially improves the statistical complexity.  We propose two algorithms to approximate the solution of the constrainedquadratic maximization, and recover a component with the desired properties. Weempirically evaluate our schemes on synthetic and real datasets.

Trading-off variance and complexity in stochastic gradient descent

  Stochastic gradient descent is the method of choice for large-scale machinelearning problems, by virtue of its light complexity per iteration. However, itlags behind its non-stochastic counterparts with respect to the convergencerate, due to high variance introduced by the stochastic updates. The popularStochastic Variance-Reduced Gradient (SVRG) method mitigates this shortcoming,introducing a new update rule which requires infrequent passes over the entireinput dataset to compute the full-gradient.  In this work, we propose CheapSVRG, a stochastic variance-reductionoptimization scheme. Our algorithm is similar to SVRG but instead of the fullgradient, it uses a surrogate which can be efficiently computed on a smallsubset of the input data. It achieves a linear convergence rate ---up to someerror level, depending on the nature of the optimization problem---and featuresa trade-off between the computational complexity and the convergence rate.Empirical evaluation shows that CheapSVRG performs at least competitivelycompared to the state of the art.

A simple and provable algorithm for sparse diagonal CCA

  Given two sets of variables, derived from a common set of samples, sparseCanonical Correlation Analysis (CCA) seeks linear combinations of a smallnumber of variables in each set, such that the induced canonical variables aremaximally correlated. Sparse CCA is NP-hard.  We propose a novel combinatorial algorithm for sparse diagonal CCA, i.e.,sparse CCA under the additional assumption that variables within each set arestandardized and uncorrelated. Our algorithm operates on a low rankapproximation of the input data and its computational complexity scaleslinearly with the number of input variables. It is simple to implement, andparallelizable. In contrast to most existing approaches, our algorithmadministers precise control on the sparsity of the extracted canonical vectors,and comes with theoretical data-dependent global approximation guarantees, thathinge on the spectrum of the input data. Finally, it can be straightforwardlyadapted to other constrained variants of CCA enforcing structure beyondsparsity.  We empirically evaluate the proposed scheme and apply it on a realneuroimaging dataset to investigate associations between brain activity andbehavior measurements.

Provable Burer-Monteiro factorization for a class of norm-constrained  matrix problems

  We study the projected gradient descent method on low-rank matrix problemswith a strongly convex objective. We use the Burer-Monteiro factorizationapproach to implicitly enforce low-rankness; such factorization introducesnon-convexity in the objective. We focus on constraint sets that include bothpositive semi-definite (PSD) constraints and specific matrix norm-constraints.Such criteria appear in quantum state tomography and phase retrievalapplications.  We show that non-convex projected gradient descent favors local linearconvergence in the factored space. We build our theory on a novel descentlemma, that non-trivially extends recent results on the unconstrained problem.The resulting algorithm is Projected Factored Gradient Descent, abbreviated asProjFGD, and shows superior performance compared to state of the art on quantumstate tomography and sparse phase retrieval applications.

Provable quantum state tomography via non-convex methods

  With nowadays steadily growing quantum processors, it is required to developnew quantum tomography tools that are tailored for high-dimensional systems. Inthis work, we describe such a computational tool, based on recent ideas fromnon-convex optimization. The algorithm excels in the compressed-sensing-likesetting, where only a few data points are measured from a low-rank orhighly-pure quantum state of a high-dimensional system. We show that thealgorithm can practically be used in quantum tomography problems that arebeyond the reach of convex solvers, and, moreover, is faster than otherstate-of-the-art non-convex approaches. Crucially, we prove that, despite beinga non-convex program, under mild conditions, the algorithm is guaranteed toconverge to the global minimum of the problem; thus, it constitutes a provablequantum state tomography protocol.

Simple and practical algorithms for $\ell_p$-norm low-rank approximation

  We propose practical algorithms for entrywise $\ell_p$-norm low-rankapproximation, for $p = 1$ or $p = \infty$. The proposed framework, which isnon-convex and gradient-based, is easy to implement and typically attainsbetter approximations, faster, than state of the art.  From a theoretical standpoint, we show that the proposed scheme can attain$(1 + \varepsilon)$-OPT approximations. Our algorithms are nothyperparameter-free: they achieve the desiderata only assuming algorithm'shyperparameters are known a priori---or are at least approximable. I.e., ourtheory indicates what problem quantities need to be known, in order to get agood solution within polynomial time, and does not contradict to recentinapproximabilty results, as in [46].

Run Procrustes, Run! On the convergence of accelerated Procrustes Flow

  In this work, we present theoretical results on the convergence of non-convexaccelerated gradient descent in matrix factorization models. The technique isapplied to matrix sensing problems with squared loss, for the estimation of arank $r$ optimal solution $X^\star \in \mathbb{R}^{n \times n}$. We show thatthe acceleration leads to linear convergence rate, even under non-convexsettings where the variable $X$ is represented as $U U^\top$ for $U \in\mathbb{R}^{n \times r}$. Our result has the same dependence on the conditionnumber of the objective --and the optimal solution-- as that of the recentresults on non-accelerated algorithms. However, acceleration is observed inpractice, both in synthetic examples and in two real applications: neuronalmulti-unit activities recovery from single electrode recordings, and quantumstate tomography on quantum computing simulators.

Implicit regularization and solution uniqueness in over-parameterized  matrix sensing

  We consider whether algorithmic choices in over-parameterized linear matrixfactorization introduce implicit regularization. We focus on noiseless matrixsensing over rank-$r$ positive semi-definite (PSD) matrices in $\mathbb{R}^{n\times n}$, with a sensing mechanism that satisfies the restricted isometryproperty (RIP). The algorithm we study is that of \emph{factored gradientdescent}, where we model the low-rankness and PSD constraints with thefactorization $UU^\top$, where $U \in \mathbb{R}^{n \times r}$. Surprisingly,recent work argues that the choice of $r \leq n$ is not pivotal: even setting$U \in \mathbb{R}^{n \times n}$ is sufficient for factored gradient descent tofind the rank-$r$ solution, which suggests that operating over the factorsleads to an implicit regularization.  In this note, we provide a different perspective. We show that, in thenoiseless case, under certain conditions, the PSD constraint by itself issufficient to lead to a unique rank-$r$ matrix recovery, without implicit orexplicit low-rank regularization. \emph{I.e.}, under assumptions, the set ofPSD matrices, that are consistent with the observed data, is a singleton,irrespective of the algorithm used.

Validating and Certifying Stabilizer States

  We propose a measurement scheme that validates the preparation of a target$n$-qubit stabilizer state. The scheme involves a measurement of $n$ Pauliobservables, a priori determined from the target stabilizer and which can berealized using single-qubit gates. Based on the proposed validation scheme, wederive an explicit expression for the worse-case fidelity, i.e., the minimumfidelity between the target stabilizer state and any other state consistentwith the measured data. We also show that the worse-case fidelity can becertified, with high probability, using $\mathcal{O}(n)$ copies of the state ofthe system per measured observable.

Sublinear Time, Approximate Model-based Sparse Recovery For All

  We describe a probabilistic, {\it sublinear} runtime, measurement-optimalsystem for model-based sparse recovery problems through dimensionalityreducing, {\em dense} random matrices. Specifically, we obtain a linear sketch$u\in \R^M$ of a vector $\bestsignal\in \R^N$ in high-dimensions through amatrix $\Phi \in \R^{M\times N}$ $(M<N)$. We assume this vector can be wellapproximated by $K$ non-zero coefficients (i.e., it is $K$-sparse). Inaddition, the nonzero coefficients of $\bestsignal$ can obey additionalstructure constraints such as matroid, totally unimodular, or knapsackconstraints, which dub as model-based sparsity. We construct the densemeasurement matrix using a probabilistic method so that it satisfies theso-called restricted isometry property in the $\ell_2$-norm. While recoveryusing such matrices is measurement-optimal as they require the smallest sketchsizes $\numsam= O(\sparsity \log(\dimension/\sparsity))$, the existingalgorithms require superlinear runtime $\Omega(N\log(N/K))$ with the exceptionof Porat and Strauss, which requires $O(\beta^5\epsilon^{-3}K(N/K)^{1/\beta}),~\beta \in \mathbb{Z}_{+}, $ but provides an $\ell_1/\ell_1$ approximationguarantee. In contrast, our approach features $ O\big(\max \lbrace \sketch\sparsity \log^{O(1)} \dimension, ~\sketch \sparsity^2 \log^2(\dimension/\sparsity) \rbrace\big) $ complexity where $ L \in \mathbb{Z}_{+}$is a design parameter, independent of $\dimension$, requires a smaller sketchsize, can accommodate model sparsity, and provides a stronger $\ell_2/\ell_1$guarantee. Our system applies to "for all" sparse signals, is robust againstbounded perturbations in $u$ as well as perturbations on $\bestsignal$ itself.

Fixed-rank Rayleigh Quotient Maximization by an $M$PSK Sequence

  Certain optimization problems in communication systems, such aslimited-feedback constant-envelope beamforming or noncoherent $M$-aryphase-shift keying ($M$PSK) sequence detection, result in the maximization of afixed-rank positive semidefinite quadratic form over the $M$PSK alphabet. Thisform is a special case of the Rayleigh quotient of a matrix and, in general,its maximization by an $M$PSK sequence is $\mathcal{NP}$-hard. However, if therank of the matrix is not a function of its size, then the optimal solution canbe computed with polynomial complexity in the matrix size. In this work, wedevelop a new technique to efficiently solve this problem by utilizingauxiliary continuous-valued angles and partitioning the resulting continuousspace of solutions into a polynomial-size set of regions, each of whichcorresponds to a distinct $M$PSK sequence. The sequence that maximizes theRayleigh quotient is shown to belong to this polynomial-size set of sequences,thus efficiently reducing the size of the feasible set from exponential topolynomial. Based on this analysis, we also develop an algorithm thatconstructs this set in polynomial time and show that it is fullyparallelizable, memory efficient, and rank scalable. The proposed algorithmcompares favorably with other solvers for this problem that have appearedrecently in the literature.

Compressive Mining: Fast and Optimal Data Mining in the Compressed  Domain

  Real-world data typically contain repeated and periodic patterns. Thissuggests that they can be effectively represented and compressed using only afew coefficients of an appropriate basis (e.g., Fourier, Wavelets, etc.).However, distance estimation when the data are represented using different setsof coefficients is still a largely unexplored area. This work studies theoptimization problems related to obtaining the \emph{tightest} lower/upperbound on Euclidean distances when each data object is potentially compressedusing a different set of orthonormal coefficients. Our technique leads totighter distance estimates, which translates into more accurate search,learning and mining operations \textit{directly} in the compressed domain.  We formulate the problem of estimating lower/upper distance bounds as anoptimization problem. We establish the properties of optimal solutions, andleverage the theoretical analysis to develop a fast algorithm to obtain an\emph{exact} solution to the problem. The suggested solution provides thetightest estimation of the $L_2$-norm or the correlation. We show that typicaldata-analysis operations, such as k-NN search or k-Means clustering, canoperate more accurately using the proposed compression and distancereconstruction technique. We compare it with many other prevalent compressionand reconstruction techniques, including random projections and PCA-basedtechniques. We highlight a surprising result, namely that when the data arehighly sparse in some basis, our technique may even outperform PCA-basedcompression.  The contributions of this work are generic as our methodology is applicableto any sequential or high-dimensional data as well as to any orthogonal datatransformation used for the underlying data compression scheme.

Structured Sparsity: Discrete and Convex approaches

  Compressive sensing (CS) exploits sparsity to recover sparse or compressiblesignals from dimensionality reducing, non-adaptive sensing mechanisms. Sparsityis also used to enhance interpretability in machine learning and statisticsapplications: While the ambient dimension is vast in modern data analysisproblems, the relevant information therein typically resides in a much lowerdimensional space. However, many solutions proposed nowadays do not leveragethe true underlying structure. Recent results in CS extend the simple sparsityidea to more sophisticated {\em structured} sparsity models, which describe theinterdependency between the nonzero components of a signal, allowing toincrease the interpretability of the results and lead to better recoveryperformance. In order to better understand the impact of structured sparsity,in this chapter we analyze the connections between the discrete models andtheir convex relaxations, highlighting their relative advantages. We start withthe general group sparse model and then elaborate on two important specialcases: the dispersive and the hierarchical models. For each, we present themodels in their discrete nature, discuss how to solve the ensuing discreteproblems and then describe convex relaxations. We also consider more generalstructures as defined by set functions and present their convex proxies.Further, we discuss efficient optimization solutions for structured sparsityproblems and illustrate structured sparsity in action via three applications.

Dropping Convexity for Faster Semi-definite Optimization

  We study the minimization of a convex function $f(X)$ over the set of$n\times n$ positive semi-definite matrices, but when the problem is recast as$\min_U g(U) := f(UU^\top)$, with $U \in \mathbb{R}^{n \times r}$ and $r \leqn$. We study the performance of gradient descent on $g$---which we refer to asFactored Gradient Descent (FGD)---under standard assumptions on the originalfunction $f$.  We provide a rule for selecting the step size and, with this choice, showthat the local convergence rate of FGD mirrors that of standard gradientdescent on the original $f$: i.e., after $k$ steps, the error is $O(1/k)$ forsmooth $f$, and exponentially small in $k$ when $f$ is (restricted) stronglyconvex. In addition, we provide a procedure to initialize FGD for (restricted)strongly convex objectives and when one only has access to $f$ via afirst-order oracle; for several problem instances, such proper initializationleads to global convergence guarantees.  FGD and similar procedures are widely used in practice for problems that canbe posed as matrix factorization. To the best of our knowledge, this is thefirst paper to provide precise convergence rate guarantees for general convexfunctions under standard convex assumptions.

Convex block-sparse linear regression with expanders -- provably

  Sparse matrices are favorable objects in machine learning and optimization.When such matrices are used, in place of dense ones, the overall complexityrequirements in optimization can be significantly reduced in practice, both interms of space and run-time. Prompted by this observation, we study a convexoptimization scheme for block-sparse recovery from linear measurements. Toobtain linear sketches, we use expander matrices, i.e., sparse matricescontaining only few non-zeros per column. Hitherto, to the best of ourknowledge, such algorithmic solutions have been only studied from a non-convexperspective. Our aim here is to theoretically characterize the performance ofconvex approaches under such setting.  Our key novelty is the expression of the recovery error in terms of themodel-based norm, while assuring that solution lives in the model. To achievethis, we show that sparse model-based matrices satisfy a group version of thenull-space property. Our experimental findings on synthetic and realapplications support our claims for faster recovery in the convex setting -- asopposed to using dense sensing matrices, while showing a competitive recoveryperformance.

Finding Low-Rank Solutions via Non-Convex Matrix Factorization,  Efficiently and Provably

  A rank-$r$ matrix $X \in \mathbb{R}^{m \times n}$ can be written as a product$U V^\top$, where $U \in \mathbb{R}^{m \times r}$ and $V \in \mathbb{R}^{n\times r}$. One could exploit this observation in optimization: e.g., considerthe minimization of a convex function $f(X)$ over rank-$r$ matrices, where theset of rank-$r$ matrices is modeled via the factorization $UV^\top$. Thoughsuch parameterization reduces the number of variables, and is morecomputationally efficient (of particular interest is the case $r \ll \min\{m,n\}$), it comes at a cost: $f(UV^\top)$ becomes a non-convex function w.r.t.$U$ and $V$.  We study such parameterization for optimization of generic convex objectives$f$, and focus on first-order, gradient descent algorithmic solutions. Wepropose the Bi-Factored Gradient Descent (BFGD) algorithm, an efficientfirst-order method that operates on the $U, V$ factors. We show that when $f$is (restricted) smooth, BFGD has local sublinear convergence, and linearconvergence when $f$ is both (restricted) smooth and (restricted) stronglyconvex. For several key applications, we provide simple and efficientinitialization schemes that provide approximate solutions good enough for theabove convergence results to hold.

A proximal Newton framework for composite minimization: Graph learning  without Cholesky decompositions and matrix inversions

  We propose an algorithmic framework for convex minimization problems of acomposite function with two terms: a self-concordant function and a possiblynonsmooth regularization term. Our method is a new proximal Newton algorithmthat features a local quadratic convergence rate. As a specific instance of ourframework, we consider the sparse inverse covariance matrix estimation in graphlearning problems. Via a careful dual formulation and a novel analyticstep-size selection procedure, our approach for graph learning avoids Choleskydecompositions and matrix inversions in its iteration making it attractive forparallel and distributed implementations.

Randomized Low-Memory Singular Value Projection

  Affine rank minimization algorithms typically rely on calculating thegradient of a data error followed by a singular value decomposition at everyiteration. Because these two steps are expensive, heuristic approximations areoften used to reduce computational burden. To this end, we propose a recoveryscheme that merges the two steps with randomized approximations, and as aresult, operates on space proportional to the degrees of freedom in theproblem. We theoretically establish the estimation guarantees of the algorithmas a function of approximation tolerance. While the theoretical approximationrequirements are overly pessimistic, we demonstrate that in practice thealgorithm performs well on the quantum tomography recovery problem.

An Inexact Proximal Path-Following Algorithm for Constrained Convex  Minimization

  Many scientific and engineering applications feature nonsmooth convexminimization problems over convex sets. In this paper, we address an importantinstance of this broad class where we assume that the nonsmooth objective isequipped with a tractable proximity operator and that the convex constraint setaffords a self-concordant barrier. We provide a new joint treatment of proximaland self-concordant barrier concepts and illustrate that such problems can beefficiently solved, without the need of lifting the problem dimensions, as indisciplined convex optimization approach. We propose an inexact path-followingalgorithmic framework and theoretically characterize the worst-case analyticalcomplexity of this framework when the proximal subproblems are solvedinexactly. To show the merits of our framework, we apply its instances to bothsynthetic and real-world applications, where it shows advantages over standardinterior point methods. As a by-product, we describe how our framework canobtain points on the Pareto frontier of regularized problems withself-concordant objectives in a tuning free fashion.

Provable Deterministic Leverage Score Sampling

  We explain theoretically a curious empirical phenomenon: "Approximating amatrix by deterministically selecting a subset of its columns with thecorresponding largest leverage scores results in a good low-rank matrixsurrogate". To obtain provable guarantees, previous work requires randomizedsampling of the columns with probabilities proportional to their leveragescores.  In this work, we provide a novel theoretical analysis of deterministicleverage score sampling. We show that such deterministic sampling can beprovably as accurate as its randomized counterparts, if the leverage scoresfollow a moderately steep power-law decay. We support this power-law assumptionby providing empirical evidence that such decay laws are abundant in real-worlddata sets. We then demonstrate empirically the performance of deterministicleverage score sampling, which many times matches or outperforms thestate-of-the-art techniques.

Location Estimation Using Crowdsourced Geospatial Narratives

  The "crowd" has become a very important geospatial data provider. Subsumedunder the term Volunteered Geographic Information (VGI), non-expert users havebeen providing a wealth of quantitative geospatial data online. With spatialreasoning being a basic form of human cognition, narratives expressinggeospatial experiences, e.g., travel blogs, would provide an even bigger sourceof geospatial data. Textual narratives typically contain qualitative data inthe form of objects and spatial relationships. The scope of this work is (i) toextract these relationships from user-generated texts, (ii) to quantify themand (iii) to reason about object locations based only on this qualitative data.We use information extraction methods to identify toponyms and spatialrelationships and to formulate a quantitative approach based on distance andorientation features to represent the latter. Positional probabilitydistributions for spatial relationships are determined by means of a greedyExpectation Maximization-based (EM) algorithm. These estimates are then used to"triangulate" the positions of unknown object locations. Experiments using atext corpus harvested from travel blog sites establish the considerablelocation estimation accuracy of the proposed approach.

Sparse PCA via Bipartite Matchings

  We consider the following multi-component sparse PCA problem: given a set ofdata points, we seek to extract a small number of sparse components withdisjoint supports that jointly capture the maximum possible variance. Thesecomponents can be computed one by one, repeatedly solving the single-componentproblem and deflating the input data matrix, but as we show this greedyprocedure is suboptimal. We present a novel algorithm for sparse PCA thatjointly optimizes multiple disjoint components. The extracted features capturevariance that lies within a multiplicative factor arbitrarily close to 1 fromthe optimal. Our algorithm is combinatorial and computes the desired componentsby solving multiple instances of the bipartite maximum weight matching problem.Its complexity grows as a low order polynomial in the ambient dimension of theinput data matrix, but exponentially in its rank. However, it can beeffectively applied on a low-dimensional sketch of the data; this allows us toobtain polynomial-time approximation guarantees via spectral bounds. Weevaluate our algorithm on real data-sets and empirically demonstrate that inmany cases it outperforms existing, deflation-based approaches.

Bipartite Correlation Clustering -- Maximizing Agreements

  In Bipartite Correlation Clustering (BCC) we are given a complete bipartitegraph $G$ with `+' and `-' edges, and we seek a vertex clustering thatmaximizes the number of agreements: the number of all `+' edges within clustersplus all `-' edges cut across clusters. BCC is known to be NP-hard.  We present a novel approximation algorithm for $k$-BCC, a variant of BCC withan upper bound $k$ on the number of clusters. Our algorithm outputs a$k$-clustering that provably achieves a number of agreements within amultiplicative ${(1-\delta)}$-factor from the optimal, for any desired accuracy$\delta$. It relies on solving a combinatorially constrained bilinearmaximization on the bi-adjacency matrix of $G$. It runs in time exponential in$k$ and $\delta^{-1}$, but linear in the size of the input.  Further, we show that, in the (unconstrained) BCC setting, an${(1-\delta)}$-approximation can be achieved by $O(\delta^{-1})$ clustersregardless of the size of the graph. In turn, our $k$-BCC algorithm implies anEfficient PTAS for the BCC objective of maximizing agreements.

Statistical inference using SGD

  We present a novel method for frequentist statistical inference in$M$-estimation problems, based on stochastic gradient descent (SGD) with afixed step size: we demonstrate that the average of such SGD sequences can beused for statistical inference, after proper scaling. An intuitive analysisusing the Ornstein-Uhlenbeck process suggests that such averages areasymptotically normal. From a practical perspective, our SGD-based inferenceprocedure is a first order method, and is well-suited for large scale problems.To show its merits, we apply it to both synthetic and real datasets, anddemonstrate that its accuracy is comparable to classical statistical methods,while requiring potentially far less computation.

Approximate Newton-based statistical inference using only stochastic  gradients

  We present a novel statistical inference framework for convex empirical riskminimization, using approximate stochastic Newton steps. The proposed algorithmis based on the notion of finite differences and allows the approximation of aHessian-vector product from first-order information. In theory, our methodefficiently computes the statistical error covariance in $M$-estimation, bothfor unregularized convex learning problems and high-dimensional LASSOregression, without using exact second order information, or resampling theentire data set. We also present a stochastic gradient sampling scheme forstatistical inference in non-i.i.d. time series analysis, where we samplecontiguous blocks of indices. In practice, we demonstrate the effectiveness ofour framework on large-scale machine learning problems, that go even beyondconvexity: as a highlight, our work can be used to detect certain adversarialattacks on neural networks.

Minimum weight norm models do not always generalize well for  over-parameterized problems

  Stochastic gradient descent is the de facto algorithm for training deepneural networks (DNNs). Despite its popularity, it still requires fine tuningin order to achieve its best performance. This has led to the development ofadaptive methods, that claim automatic hyper-parameter optimization.  Recently, researchers have studied both algorithmic classes via toy examples:e.g., for over-parameterized linear regression, Wilson et. al. (2017) showsthat, while SGD always converges to the minimum-norm solution, adaptive methodsshow no such inclination, leading to worse generalization capabilities.  Our aim is to study this conjecture further. We empirically show that theminimum weight norm is not necessarily the proper gauge of good generalizationin simplified scenaria, and different models found by adaptive methods couldoutperform plain gradient methods. In practical DNN settings, we observe thatadaptive methods can outperform SGD, with larger weight norm output models, butwithout necessarily reducing the amount of tuning required.

Compressing Gradient Optimizers via Count-Sketches

  Many popular first-order optimization methods (e.g., Momentum, AdaGrad, Adam)accelerate the convergence rate of deep learning models. However, thesealgorithms require auxiliary parameters, which cost additional memoryproportional to the number of parameters in the model. The problem is becomingmore severe as deep learning models continue to grow larger in order to learnfrom complex, large-scale datasets. Our proposed solution is to maintain alinear sketch to compress the auxiliary variables. We demonstrate that ourtechnique has the same performance as the full-sized baseline, while usingsignificantly less space for the auxiliary variables. Theoretically, we provethat count-sketch optimization maintains the SGD convergence rate, whilegracefully reducing memory usage for large-models. On the large-scale 1-BillionWord dataset, we save 25% of the memory used during training (8.6 GB instead of11.7 GB) by compressing the Adam optimizer in the Embedding and Softmax layerswith negligible accuracy and performance loss. For an Amazon extremeclassification task with over 49.5 million classes, we also reduce the trainingtime by 38%, by increasing the mini-batch size 3.5x using our count-sketchoptimizer.

Group-Sparse Model Selection: Hardness and Relaxations

  Group-based sparsity models are proven instrumental in linear regressionproblems for recovering signals from much fewer measurements than standardcompressive sensing. The main promise of these models is the recovery of"interpretable" signals through the identification of their constituent groups.In this paper, we establish a combinatorial framework for group-model selectionproblems and highlight the underlying tractability issues. In particular, weshow that the group-model selection problem is equivalent to the well-knownNP-hard weighted maximum coverage problem (WMC). Leveraging a graph-basedunderstanding of group models, we describe group structures which enablecorrect model selection in polynomial time via dynamic programming.Furthermore, group structures that lead to totally unimodular constraints havetractable discrete as well as convex relaxations. We also present ageneralization of the group-model that allows for within group sparsity, whichcan be used to model hierarchical sparsity. Finally, we study the Paretofrontier of group-sparse approximations for two tractable models, among whichthe tree sparsity model, and illustrate selection and computation trade-offsbetween our framework and the existing convex relaxations.

On Quantifying Qualitative Geospatial Data: A Probabilistic Approach

  Living in the era of data deluge, we have witnessed a web content explosion,largely due to the massive availability of User-Generated Content (UGC). Inthis work, we specifically consider the problem of geospatial informationextraction and representation, where one can exploit diverse sources ofinformation (such as image and audio data, text data, etc), going beyondtraditional volunteered geographic information. Our ambition is to includeavailable narrative information in an effort to better explain geospatialrelationships: with spatial reasoning being a basic form of human cognition,narratives expressing such experiences typically contain qualitative spatialdata, i.e., spatial objects and spatial relationships.  To this end, we formulate a quantitative approach for the representation ofqualitative spatial relations extracted from UGC in the form of texts. Theproposed method quantifies such relations based on multiple text observations.Such observations provide distance and orientation features which are utilizedby a greedy Expectation Maximization-based (EM) algorithm to infer aprobability distribution over predefined spatial relationships; the latterrepresent the quantified relationships under user-defined probabilisticassumptions. We evaluate the applicability and quality of the proposed approachusing real UGC data originating from an actual travel blog text corpus. Toverify the quality of the result, we generate grid-based maps visualizing thespatial extent of the various relations.

Learning Sparse Additive Models with Interactions in High Dimensions

  A function $f: \mathbb{R}^d \rightarrow \mathbb{R}$ is referred to as aSparse Additive Model (SPAM), if it is of the form $f(\mathbf{x}) = \sum_{l \in\mathcal{S}}\phi_{l}(x_l)$, where $\mathcal{S} \subset [d]$, $|\mathcal{S}| \lld$. Assuming $\phi_l$'s and $\mathcal{S}$ to be unknown, the problem ofestimating $f$ from its samples has been studied extensively. In this work, weconsider a generalized SPAM, allowing for second order interaction terms. Forsome $\mathcal{S}_1 \subset [d], \mathcal{S}_2 \subset {[d] \choose 2}$, thefunction $f$ is assumed to be of the form: $$f(\mathbf{x}) = \sum_{p \in\mathcal{S}_1}\phi_{p} (x_p) + \sum_{(l,l^{\prime}) \in\mathcal{S}_2}\phi_{(l,l^{\prime})} (x_{l},x_{l^{\prime}}).$$ Assuming$\phi_{p},\phi_{(l,l^{\prime})}$, $\mathcal{S}_1$ and, $\mathcal{S}_2$ to beunknown, we provide a randomized algorithm that queries $f$ and exactlyrecovers $\mathcal{S}_1,\mathcal{S}_2$. Consequently, this also enables us toestimate the underlying $\phi_p, \phi_{(l,l^{\prime})}$. We derive samplecomplexity bounds for our scheme and also extend our analysis to include thesituation where the queries are corrupted with noise -- either stochastic, orarbitrary but bounded. Lastly, we provide simulation results on synthetic data,that validate our theoretical findings.

A single-phase, proximal path-following framework

  We propose a new proximal, path-following framework for a class ofconstrained convex problems. We consider settings where the nonlinear---andpossibly non-smooth---objective part is endowed with a proximity operator, andthe constraint set is equipped with a self-concordant barrier. Our approachrelies on the following two main ideas. First, we re-parameterize theoptimality condition as an auxiliary problem, such that a good initial point isavailable; by doing so, a family of alternative paths towards the optimum isgenerated. Second, we combine the proximal operator with path-following ideasto design a single-phase, proximal, path-following algorithm. Our method hasseveral advantages. First, it allows handling non-smooth objectives viaproximal operators; this avoids lifting the problem dimension in order toaccommodate non-smooth components in optimization. Second, it consists of onlya \emph{single phase}: While the overall convergence rate of classicalpath-following schemes for self-concordant objectives does not suffer from theinitialization phase, proximal path-following schemes undergo slow convergence,in order to obtain a good starting point \cite{TranDinh2013e}. In this work, weshow how to overcome this limitation in the proximal setting and prove that ourscheme has the same $\mathcal{O}(\sqrt{\nu}\log(1/\varepsilon))$ worst-caseiteration-complexity with standard approaches \cite{Nesterov2004,Nesterov1994}without requiring an initial phase, where $\nu$ is the barrier parameter and$\varepsilon$ is a desired accuracy. Finally, our framework allows errors inthe calculation of proximal-Newton directions, without sacrificing theworst-case iteration complexity. We demonstrate the merits of our algorithm viathree numerical examples, where proximal operators play a key role.

Algorithms for Learning Sparse Additive Models with Interactions in High  Dimensions

  A function $f: \mathbb{R}^d \rightarrow \mathbb{R}$ is a Sparse AdditiveModel (SPAM), if it is of the form $f(\mathbf{x}) = \sum_{l \in\mathcal{S}}\phi_{l}(x_l)$ where $\mathcal{S} \subset [d]$, $|\mathcal{S}| \lld$. Assuming $\phi$'s, $\mathcal{S}$ to be unknown, there exists extensive workfor estimating $f$ from its samples. In this work, we consider a generalizedversion of SPAMs, that also allows for the presence of a sparse number ofsecond order interaction terms. For some $\mathcal{S}_1 \subset [d],\mathcal{S}_2 \subset {[d] \choose 2}$, with $|\mathcal{S}_1| \ll d,|\mathcal{S}_2| \ll d^2$, the function $f$ is now assumed to be of the form:$\sum_{p \in \mathcal{S}_1}\phi_{p} (x_p) + \sum_{(l,l^{\prime}) \in\mathcal{S}_2}\phi_{(l,l^{\prime})} (x_l,x_{l^{\prime}})$. Assuming we have thefreedom to query $f$ anywhere in its domain, we derive efficient algorithmsthat provably recover $\mathcal{S}_1,\mathcal{S}_2$ with finite sample bounds.Our analysis covers the noiseless setting where exact samples of $f$ areobtained, and also extends to the noisy setting where the queries are corruptedwith noise. For the noisy setting in particular, we consider two noise modelsnamely: i.i.d Gaussian noise and arbitrary but bounded noise. Our main methodsfor identification of $\mathcal{S}_2$ essentially rely on estimation of sparseHessian matrices, for which we provide two novel compressed sensing basedschemes. Once $\mathcal{S}_1, \mathcal{S}_2$ are known, we show how theindividual components $\phi_p$, $\phi_{(l,l^{\prime})}$ can be estimated viaadditional queries of $f$, with uniform error bounds. Lastly, we providesimulation results on synthetic data that validate our theoretical findings.

SysML: The New Frontier of Machine Learning Systems

  Machine learning (ML) techniques are enjoying rapidly increasing adoption.However, designing and implementing the systems that support ML models inreal-world deployments remains a significant obstacle, in large part due to theradically different development and deployment profile of modern ML methods,and the range of practical concerns that come with broader adoption. We proposeto foster a new systems machine learning research community at the intersectionof the traditional systems and ML communities, focused on topics such ashardware systems for ML, software systems for ML, and ML optimized for metricsbeyond predictive accuracy. To do this, we describe a new conference, SysML,that explicitly targets research at the intersection of systems and machinelearning with a program committee split evenly between experts in systems andML, and an explicit focus on topics at the intersection of the two.

