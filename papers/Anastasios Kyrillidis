Non-uniform Feature Sampling for Decision Tree Ensembles

  We study the effectiveness of non-uniform randomized feature selection in
decision tree classification. We experimentally evaluate two feature selection
methodologies, based on information extracted from the provided dataset: $(i)$
\emph{leverage scores-based} and $(ii)$ \emph{norm-based} feature selection.
Experimental evaluation of the proposed feature selection techniques indicate
that such approaches might be more effective compared to naive uniform feature
selection and moreover having comparable performance to the random forest
algorithm [3]


Composite Self-Concordant Minimization

  We propose a variable metric framework for minimizing the sum of a
self-concordant function and a possibly non-smooth convex function, endowed
with an easily computable proximal operator. We theoretically establish the
convergence of our framework without relying on the usual Lipschitz gradient
assumption on the smooth part. An important highlight of our work is a new set
of analytic step-size selection and correction procedures based on the
structure of the problem. We describe concrete algorithmic instances of our
framework for several interesting applications and demonstrate them numerically
on both synthetic and real data.


Matrix ALPS: Accelerated Low Rank and Sparse Matrix Reconstruction

  We propose Matrix ALPS for recovering a sparse plus low-rank decomposition of
a matrix given its corrupted and incomplete linear measurements. Our approach
is a first-order projected gradient method over non-convex sets, and it
exploits a well-known memory-based acceleration technique. We theoretically
characterize the convergence properties of Matrix ALPS using the stable
embedding properties of the linear measurement operator. We then numerically
illustrate that our algorithm outperforms the existing convex as well as
non-convex state-of-the-art algorithms in computational efficiency without
sacrificing stability.


Matrix Recipes for Hard Thresholding Methods

  In this paper, we present and analyze a new set of low-rank recovery
algorithms for linear inverse problems within the class of hard thresholding
methods. We provide strategies on how to set up these algorithms via basic
ingredients for different configurations to achieve complexity vs. accuracy
tradeoffs. Moreover, we study acceleration schemes via memory-based techniques
and randomized, $\epsilon$-approximate matrix projections to decrease the
computational costs in the recovery process. For most of the configurations, we
present theoretical analysis that guarantees convergence under mild problem
conditions. Simulation results demonstrate notable performance improvements as
compared to state-of-the-art algorithms both in terms of reconstruction
accuracy and computational complexity.


Linear Inverse Problems with Norm and Sparsity Constraints

  We describe two nonconventional algorithms for linear regression, called GAME
and CLASH. The salient characteristics of these approaches is that they exploit
the convex $\ell_1$-ball and non-convex $\ell_0$-sparsity constraints jointly
in sparse recovery. To establish the theoretical approximation guarantees of
GAME and CLASH, we cover an interesting range of topics from game theory,
convex and combinatorial optimization. We illustrate that these approaches lead
to improved theoretical guarantees and empirical performance beyond convex and
non-convex solvers alone.


Non-square matrix sensing without spurious local minima via the
  Burer-Monteiro approach

  We consider the non-square matrix sensing problem, under restricted isometry
property (RIP) assumptions. We focus on the non-convex formulation, where any
rank-$r$ matrix $X \in \mathbb{R}^{m \times n}$ is represented as $UV^\top$,
where $U \in \mathbb{R}^{m \times r}$ and $V \in \mathbb{R}^{n \times r}$. In
this paper, we complement recent findings on the non-convex geometry of the
analogous PSD setting [5], and show that matrix factorization does not
introduce any spurious local minima, under RIP.


IHT dies hard: Provable accelerated Iterative Hard Thresholding

  We study --both in theory and practice-- the use of momentum motions in
classic iterative hard thresholding (IHT) methods. By simply modifying plain
IHT, we investigate its convergence behavior on convex optimization criteria
with non-convex constraints, under standard assumptions. In diverse scenaria,
we observe that acceleration in IHT leads to significant improvements, compared
to state of the art projected gradient descent and Frank-Wolfe variants. As a
byproduct of our inspection, we study the impact of selecting the momentum
parameter: similar to convex settings, two modes of behavior are observed
--"rippling" and linear-- depending on the level of momentum.


Approximate Matrix Multiplication with Application to Linear Embeddings

  In this paper, we study the problem of approximately computing the product of
two real matrices. In particular, we analyze a dimensionality-reduction-based
approximation algorithm due to Sarlos [1], introducing the notion of nuclear
rank as the ratio of the nuclear norm over the spectral norm. The presented
bound has improved dependence with respect to the approximation error (as
compared to previous approaches), whereas the subspace -- on which we project
the input matrices -- has dimensions proportional to the maximum of their
nuclear rank and it is independent of the input dimensions. In addition, we
provide an application of this result to linear low-dimensional embeddings.
Namely, we show that any Euclidean point-set with bounded nuclear rank is
amenable to projection onto number of dimensions that is independent of the
input dimensionality, while achieving additive error guarantees.


Combinatorial Selection and Least Absolute Shrinkage via the CLASH
  Algorithm

  The least absolute shrinkage and selection operator (LASSO) for linear
regression exploits the geometric interplay of the $\ell_2$-data error
objective and the $\ell_1$-norm constraint to arbitrarily select sparse models.
Guiding this uninformed selection process with sparsity models has been
precisely the center of attention over the last decade in order to improve
learning performance. To this end, we alter the selection process of LASSO to
explicitly leverage combinatorial sparsity models (CSMs) via the combinatorial
selection and least absolute shrinkage (CLASH) operator. We provide concrete
guidelines how to leverage combinatorial constraints within CLASH, and
characterize CLASH's guarantees as a function of the set restricted isometry
constants of the sensing matrix. Finally, our experimental results show that
CLASH can outperform both LASSO and model-based compressive sensing in sparse
estimation.


Scalable sparse covariance estimation via self-concordance

  We consider the class of convex minimization problems, composed of a
self-concordant function, such as the $\log\det$ metric, a convex data fidelity
term $h(\cdot)$ and, a regularizing -- possibly non-smooth -- function
$g(\cdot)$. This type of problems have recently attracted a great deal of
interest, mainly due to their omnipresence in top-notch applications. Under
this \emph{locally} Lipschitz continuous gradient setting, we analyze the
convergence behavior of proximal Newton schemes with the added twist of a
probable presence of inexact evaluations. We prove attractive convergence rate
guarantees and enhance state-of-the-art optimization schemes to accommodate
such developments. Experimental results on sparse covariance estimation show
the merits of our algorithm, both in terms of recovery efficiency and
complexity.


Provable Burer-Monteiro factorization for a class of norm-constrained
  matrix problems

  We study the projected gradient descent method on low-rank matrix problems
with a strongly convex objective. We use the Burer-Monteiro factorization
approach to implicitly enforce low-rankness; such factorization introduces
non-convexity in the objective. We focus on constraint sets that include both
positive semi-definite (PSD) constraints and specific matrix norm-constraints.
Such criteria appear in quantum state tomography and phase retrieval
applications.
  We show that non-convex projected gradient descent favors local linear
convergence in the factored space. We build our theory on a novel descent
lemma, that non-trivially extends recent results on the unconstrained problem.
The resulting algorithm is Projected Factored Gradient Descent, abbreviated as
ProjFGD, and shows superior performance compared to state of the art on quantum
state tomography and sparse phase retrieval applications.


Sparse projections onto the simplex

  Most learning methods with rank or sparsity constraints use convex
relaxations, which lead to optimization with the nuclear norm or the
$\ell_1$-norm. However, several important learning applications cannot benefit
from this approach as they feature these convex norms as constraints in
addition to the non-convex rank and sparsity constraints. In this setting, we
derive efficient sparse projections onto the simplex and its extension, and
illustrate how to use them to solve high-dimensional learning problems in
quantum tomography, sparse density estimation and portfolio selection with
non-convex constraints.


Trading-off variance and complexity in stochastic gradient descent

  Stochastic gradient descent is the method of choice for large-scale machine
learning problems, by virtue of its light complexity per iteration. However, it
lags behind its non-stochastic counterparts with respect to the convergence
rate, due to high variance introduced by the stochastic updates. The popular
Stochastic Variance-Reduced Gradient (SVRG) method mitigates this shortcoming,
introducing a new update rule which requires infrequent passes over the entire
input dataset to compute the full-gradient.
  In this work, we propose CheapSVRG, a stochastic variance-reduction
optimization scheme. Our algorithm is similar to SVRG but instead of the full
gradient, it uses a surrogate which can be efficiently computed on a small
subset of the input data. It achieves a linear convergence rate ---up to some
error level, depending on the nature of the optimization problem---and features
a trade-off between the computational complexity and the convergence rate.
Empirical evaluation shows that CheapSVRG performs at least competitively
compared to the state of the art.


Stay on path: PCA along graph paths

  We introduce a variant of (sparse) PCA in which the set of feasible support
sets is determined by a graph. In particular, we consider the following
setting: given a directed acyclic graph $G$ on $p$ vertices corresponding to
variables, the non-zero entries of the extracted principal component must
coincide with vertices lying along a path in $G$.
  From a statistical perspective, information on the underlying network may
potentially reduce the number of observations required to recover the
population principal component. We consider the canonical estimator which
optimally exploits the prior knowledge by solving a non-convex quadratic
maximization on the empirical covariance. We introduce a simple network and
analyze the estimator under the spiked covariance model. We show that side
information potentially improves the statistical complexity.
  We propose two algorithms to approximate the solution of the constrained
quadratic maximization, and recover a component with the desired properties. We
empirically evaluate our schemes on synthetic and real datasets.


A simple and provable algorithm for sparse diagonal CCA

  Given two sets of variables, derived from a common set of samples, sparse
Canonical Correlation Analysis (CCA) seeks linear combinations of a small
number of variables in each set, such that the induced canonical variables are
maximally correlated. Sparse CCA is NP-hard.
  We propose a novel combinatorial algorithm for sparse diagonal CCA, i.e.,
sparse CCA under the additional assumption that variables within each set are
standardized and uncorrelated. Our algorithm operates on a low rank
approximation of the input data and its computational complexity scales
linearly with the number of input variables. It is simple to implement, and
parallelizable. In contrast to most existing approaches, our algorithm
administers precise control on the sparsity of the extracted canonical vectors,
and comes with theoretical data-dependent global approximation guarantees, that
hinge on the spectrum of the input data. Finally, it can be straightforwardly
adapted to other constrained variants of CCA enforcing structure beyond
sparsity.
  We empirically evaluate the proposed scheme and apply it on a real
neuroimaging dataset to investigate associations between brain activity and
behavior measurements.


Provable quantum state tomography via non-convex methods

  With nowadays steadily growing quantum processors, it is required to develop
new quantum tomography tools that are tailored for high-dimensional systems. In
this work, we describe such a computational tool, based on recent ideas from
non-convex optimization. The algorithm excels in the compressed-sensing-like
setting, where only a few data points are measured from a low-rank or
highly-pure quantum state of a high-dimensional system. We show that the
algorithm can practically be used in quantum tomography problems that are
beyond the reach of convex solvers, and, moreover, is faster than other
state-of-the-art non-convex approaches. Crucially, we prove that, despite being
a non-convex program, under mild conditions, the algorithm is guaranteed to
converge to the global minimum of the problem; thus, it constitutes a provable
quantum state tomography protocol.


Simple and practical algorithms for $\ell_p$-norm low-rank approximation

  We propose practical algorithms for entrywise $\ell_p$-norm low-rank
approximation, for $p = 1$ or $p = \infty$. The proposed framework, which is
non-convex and gradient-based, is easy to implement and typically attains
better approximations, faster, than state of the art.
  From a theoretical standpoint, we show that the proposed scheme can attain
$(1 + \varepsilon)$-OPT approximations. Our algorithms are not
hyperparameter-free: they achieve the desiderata only assuming algorithm's
hyperparameters are known a priori---or are at least approximable. I.e., our
theory indicates what problem quantities need to be known, in order to get a
good solution within polynomial time, and does not contradict to recent
inapproximabilty results, as in [46].


Run Procrustes, Run! On the convergence of accelerated Procrustes Flow

  In this work, we present theoretical results on the convergence of non-convex
accelerated gradient descent in matrix factorization models. The technique is
applied to matrix sensing problems with squared loss, for the estimation of a
rank $r$ optimal solution $X^\star \in \mathbb{R}^{n \times n}$. We show that
the acceleration leads to linear convergence rate, even under non-convex
settings where the variable $X$ is represented as $U U^\top$ for $U \in
\mathbb{R}^{n \times r}$. Our result has the same dependence on the condition
number of the objective --and the optimal solution-- as that of the recent
results on non-accelerated algorithms. However, acceleration is observed in
practice, both in synthetic examples and in two real applications: neuronal
multi-unit activities recovery from single electrode recordings, and quantum
state tomography on quantum computing simulators.


Implicit regularization and solution uniqueness in over-parameterized
  matrix sensing

  We consider whether algorithmic choices in over-parameterized linear matrix
factorization introduce implicit regularization. We focus on noiseless matrix
sensing over rank-$r$ positive semi-definite (PSD) matrices in $\mathbb{R}^{n
\times n}$, with a sensing mechanism that satisfies the restricted isometry
property (RIP). The algorithm we study is that of \emph{factored gradient
descent}, where we model the low-rankness and PSD constraints with the
factorization $UU^\top$, where $U \in \mathbb{R}^{n \times r}$. Surprisingly,
recent work argues that the choice of $r \leq n$ is not pivotal: even setting
$U \in \mathbb{R}^{n \times n}$ is sufficient for factored gradient descent to
find the rank-$r$ solution, which suggests that operating over the factors
leads to an implicit regularization.
  In this note, we provide a different perspective. We show that, in the
noiseless case, under certain conditions, the PSD constraint by itself is
sufficient to lead to a unique rank-$r$ matrix recovery, without implicit or
explicit low-rank regularization. \emph{I.e.}, under assumptions, the set of
PSD matrices, that are consistent with the observed data, is a singleton,
irrespective of the algorithm used.


Validating and Certifying Stabilizer States

  We propose a measurement scheme that validates the preparation of a target
$n$-qubit stabilizer state. The scheme involves a measurement of $n$ Pauli
observables, a priori determined from the target stabilizer and which can be
realized using single-qubit gates. Based on the proposed validation scheme, we
derive an explicit expression for the worse-case fidelity, i.e., the minimum
fidelity between the target stabilizer state and any other state consistent
with the measured data. We also show that the worse-case fidelity can be
certified, with high probability, using $\mathcal{O}(n)$ copies of the state of
the system per measured observable.


Sublinear Time, Approximate Model-based Sparse Recovery For All

  We describe a probabilistic, {\it sublinear} runtime, measurement-optimal
system for model-based sparse recovery problems through dimensionality
reducing, {\em dense} random matrices. Specifically, we obtain a linear sketch
$u\in \R^M$ of a vector $\bestsignal\in \R^N$ in high-dimensions through a
matrix $\Phi \in \R^{M\times N}$ $(M<N)$. We assume this vector can be well
approximated by $K$ non-zero coefficients (i.e., it is $K$-sparse). In
addition, the nonzero coefficients of $\bestsignal$ can obey additional
structure constraints such as matroid, totally unimodular, or knapsack
constraints, which dub as model-based sparsity. We construct the dense
measurement matrix using a probabilistic method so that it satisfies the
so-called restricted isometry property in the $\ell_2$-norm. While recovery
using such matrices is measurement-optimal as they require the smallest sketch
sizes $\numsam= O(\sparsity \log(\dimension/\sparsity))$, the existing
algorithms require superlinear runtime $\Omega(N\log(N/K))$ with the exception
of Porat and Strauss, which requires $O(\beta^5\epsilon^{-3}K(N/K)^{1/\beta}),
~\beta \in \mathbb{Z}_{+}, $ but provides an $\ell_1/\ell_1$ approximation
guarantee. In contrast, our approach features $ O\big(\max \lbrace \sketch
\sparsity \log^{O(1)} \dimension, ~\sketch \sparsity^2 \log^2
(\dimension/\sparsity) \rbrace\big) $ complexity where $ L \in \mathbb{Z}_{+}$
is a design parameter, independent of $\dimension$, requires a smaller sketch
size, can accommodate model sparsity, and provides a stronger $\ell_2/\ell_1$
guarantee. Our system applies to "for all" sparse signals, is robust against
bounded perturbations in $u$ as well as perturbations on $\bestsignal$ itself.


Fixed-rank Rayleigh Quotient Maximization by an $M$PSK Sequence

  Certain optimization problems in communication systems, such as
limited-feedback constant-envelope beamforming or noncoherent $M$-ary
phase-shift keying ($M$PSK) sequence detection, result in the maximization of a
fixed-rank positive semidefinite quadratic form over the $M$PSK alphabet. This
form is a special case of the Rayleigh quotient of a matrix and, in general,
its maximization by an $M$PSK sequence is $\mathcal{NP}$-hard. However, if the
rank of the matrix is not a function of its size, then the optimal solution can
be computed with polynomial complexity in the matrix size. In this work, we
develop a new technique to efficiently solve this problem by utilizing
auxiliary continuous-valued angles and partitioning the resulting continuous
space of solutions into a polynomial-size set of regions, each of which
corresponds to a distinct $M$PSK sequence. The sequence that maximizes the
Rayleigh quotient is shown to belong to this polynomial-size set of sequences,
thus efficiently reducing the size of the feasible set from exponential to
polynomial. Based on this analysis, we also develop an algorithm that
constructs this set in polynomial time and show that it is fully
parallelizable, memory efficient, and rank scalable. The proposed algorithm
compares favorably with other solvers for this problem that have appeared
recently in the literature.


Compressive Mining: Fast and Optimal Data Mining in the Compressed
  Domain

  Real-world data typically contain repeated and periodic patterns. This
suggests that they can be effectively represented and compressed using only a
few coefficients of an appropriate basis (e.g., Fourier, Wavelets, etc.).
However, distance estimation when the data are represented using different sets
of coefficients is still a largely unexplored area. This work studies the
optimization problems related to obtaining the \emph{tightest} lower/upper
bound on Euclidean distances when each data object is potentially compressed
using a different set of orthonormal coefficients. Our technique leads to
tighter distance estimates, which translates into more accurate search,
learning and mining operations \textit{directly} in the compressed domain.
  We formulate the problem of estimating lower/upper distance bounds as an
optimization problem. We establish the properties of optimal solutions, and
leverage the theoretical analysis to develop a fast algorithm to obtain an
\emph{exact} solution to the problem. The suggested solution provides the
tightest estimation of the $L_2$-norm or the correlation. We show that typical
data-analysis operations, such as k-NN search or k-Means clustering, can
operate more accurately using the proposed compression and distance
reconstruction technique. We compare it with many other prevalent compression
and reconstruction techniques, including random projections and PCA-based
techniques. We highlight a surprising result, namely that when the data are
highly sparse in some basis, our technique may even outperform PCA-based
compression.
  The contributions of this work are generic as our methodology is applicable
to any sequential or high-dimensional data as well as to any orthogonal data
transformation used for the underlying data compression scheme.


Finding Low-Rank Solutions via Non-Convex Matrix Factorization,
  Efficiently and Provably

  A rank-$r$ matrix $X \in \mathbb{R}^{m \times n}$ can be written as a product
$U V^\top$, where $U \in \mathbb{R}^{m \times r}$ and $V \in \mathbb{R}^{n
\times r}$. One could exploit this observation in optimization: e.g., consider
the minimization of a convex function $f(X)$ over rank-$r$ matrices, where the
set of rank-$r$ matrices is modeled via the factorization $UV^\top$. Though
such parameterization reduces the number of variables, and is more
computationally efficient (of particular interest is the case $r \ll \min\{m,
n\}$), it comes at a cost: $f(UV^\top)$ becomes a non-convex function w.r.t.
$U$ and $V$.
  We study such parameterization for optimization of generic convex objectives
$f$, and focus on first-order, gradient descent algorithmic solutions. We
propose the Bi-Factored Gradient Descent (BFGD) algorithm, an efficient
first-order method that operates on the $U, V$ factors. We show that when $f$
is (restricted) smooth, BFGD has local sublinear convergence, and linear
convergence when $f$ is both (restricted) smooth and (restricted) strongly
convex. For several key applications, we provide simple and efficient
initialization schemes that provide approximate solutions good enough for the
above convergence results to hold.


Convex block-sparse linear regression with expanders -- provably

  Sparse matrices are favorable objects in machine learning and optimization.
When such matrices are used, in place of dense ones, the overall complexity
requirements in optimization can be significantly reduced in practice, both in
terms of space and run-time. Prompted by this observation, we study a convex
optimization scheme for block-sparse recovery from linear measurements. To
obtain linear sketches, we use expander matrices, i.e., sparse matrices
containing only few non-zeros per column. Hitherto, to the best of our
knowledge, such algorithmic solutions have been only studied from a non-convex
perspective. Our aim here is to theoretically characterize the performance of
convex approaches under such setting.
  Our key novelty is the expression of the recovery error in terms of the
model-based norm, while assuring that solution lives in the model. To achieve
this, we show that sparse model-based matrices satisfy a group version of the
null-space property. Our experimental findings on synthetic and real
applications support our claims for faster recovery in the convex setting -- as
opposed to using dense sensing matrices, while showing a competitive recovery
performance.


Structured Sparsity: Discrete and Convex approaches

  Compressive sensing (CS) exploits sparsity to recover sparse or compressible
signals from dimensionality reducing, non-adaptive sensing mechanisms. Sparsity
is also used to enhance interpretability in machine learning and statistics
applications: While the ambient dimension is vast in modern data analysis
problems, the relevant information therein typically resides in a much lower
dimensional space. However, many solutions proposed nowadays do not leverage
the true underlying structure. Recent results in CS extend the simple sparsity
idea to more sophisticated {\em structured} sparsity models, which describe the
interdependency between the nonzero components of a signal, allowing to
increase the interpretability of the results and lead to better recovery
performance. In order to better understand the impact of structured sparsity,
in this chapter we analyze the connections between the discrete models and
their convex relaxations, highlighting their relative advantages. We start with
the general group sparse model and then elaborate on two important special
cases: the dispersive and the hierarchical models. For each, we present the
models in their discrete nature, discuss how to solve the ensuing discrete
problems and then describe convex relaxations. We also consider more general
structures as defined by set functions and present their convex proxies.
Further, we discuss efficient optimization solutions for structured sparsity
problems and illustrate structured sparsity in action via three applications.


Dropping Convexity for Faster Semi-definite Optimization

  We study the minimization of a convex function $f(X)$ over the set of
$n\times n$ positive semi-definite matrices, but when the problem is recast as
$\min_U g(U) := f(UU^\top)$, with $U \in \mathbb{R}^{n \times r}$ and $r \leq
n$. We study the performance of gradient descent on $g$---which we refer to as
Factored Gradient Descent (FGD)---under standard assumptions on the original
function $f$.
  We provide a rule for selecting the step size and, with this choice, show
that the local convergence rate of FGD mirrors that of standard gradient
descent on the original $f$: i.e., after $k$ steps, the error is $O(1/k)$ for
smooth $f$, and exponentially small in $k$ when $f$ is (restricted) strongly
convex. In addition, we provide a procedure to initialize FGD for (restricted)
strongly convex objectives and when one only has access to $f$ via a
first-order oracle; for several problem instances, such proper initialization
leads to global convergence guarantees.
  FGD and similar procedures are widely used in practice for problems that can
be posed as matrix factorization. To the best of our knowledge, this is the
first paper to provide precise convergence rate guarantees for general convex
functions under standard convex assumptions.


Randomized Low-Memory Singular Value Projection

  Affine rank minimization algorithms typically rely on calculating the
gradient of a data error followed by a singular value decomposition at every
iteration. Because these two steps are expensive, heuristic approximations are
often used to reduce computational burden. To this end, we propose a recovery
scheme that merges the two steps with randomized approximations, and as a
result, operates on space proportional to the degrees of freedom in the
problem. We theoretically establish the estimation guarantees of the algorithm
as a function of approximation tolerance. While the theoretical approximation
requirements are overly pessimistic, we demonstrate that in practice the
algorithm performs well on the quantum tomography recovery problem.


Location Estimation Using Crowdsourced Geospatial Narratives

  The "crowd" has become a very important geospatial data provider. Subsumed
under the term Volunteered Geographic Information (VGI), non-expert users have
been providing a wealth of quantitative geospatial data online. With spatial
reasoning being a basic form of human cognition, narratives expressing
geospatial experiences, e.g., travel blogs, would provide an even bigger source
of geospatial data. Textual narratives typically contain qualitative data in
the form of objects and spatial relationships. The scope of this work is (i) to
extract these relationships from user-generated texts, (ii) to quantify them
and (iii) to reason about object locations based only on this qualitative data.
We use information extraction methods to identify toponyms and spatial
relationships and to formulate a quantitative approach based on distance and
orientation features to represent the latter. Positional probability
distributions for spatial relationships are determined by means of a greedy
Expectation Maximization-based (EM) algorithm. These estimates are then used to
"triangulate" the positions of unknown object locations. Experiments using a
text corpus harvested from travel blog sites establish the considerable
location estimation accuracy of the proposed approach.


A proximal Newton framework for composite minimization: Graph learning
  without Cholesky decompositions and matrix inversions

  We propose an algorithmic framework for convex minimization problems of a
composite function with two terms: a self-concordant function and a possibly
nonsmooth regularization term. Our method is a new proximal Newton algorithm
that features a local quadratic convergence rate. As a specific instance of our
framework, we consider the sparse inverse covariance matrix estimation in graph
learning problems. Via a careful dual formulation and a novel analytic
step-size selection procedure, our approach for graph learning avoids Cholesky
decompositions and matrix inversions in its iteration making it attractive for
parallel and distributed implementations.


Provable Deterministic Leverage Score Sampling

  We explain theoretically a curious empirical phenomenon: "Approximating a
matrix by deterministically selecting a subset of its columns with the
corresponding largest leverage scores results in a good low-rank matrix
surrogate". To obtain provable guarantees, previous work requires randomized
sampling of the columns with probabilities proportional to their leverage
scores.
  In this work, we provide a novel theoretical analysis of deterministic
leverage score sampling. We show that such deterministic sampling can be
provably as accurate as its randomized counterparts, if the leverage scores
follow a moderately steep power-law decay. We support this power-law assumption
by providing empirical evidence that such decay laws are abundant in real-world
data sets. We then demonstrate empirically the performance of deterministic
leverage score sampling, which many times matches or outperforms the
state-of-the-art techniques.


An Inexact Proximal Path-Following Algorithm for Constrained Convex
  Minimization

  Many scientific and engineering applications feature nonsmooth convex
minimization problems over convex sets. In this paper, we address an important
instance of this broad class where we assume that the nonsmooth objective is
equipped with a tractable proximity operator and that the convex constraint set
affords a self-concordant barrier. We provide a new joint treatment of proximal
and self-concordant barrier concepts and illustrate that such problems can be
efficiently solved, without the need of lifting the problem dimensions, as in
disciplined convex optimization approach. We propose an inexact path-following
algorithmic framework and theoretically characterize the worst-case analytical
complexity of this framework when the proximal subproblems are solved
inexactly. To show the merits of our framework, we apply its instances to both
synthetic and real-world applications, where it shows advantages over standard
interior point methods. As a by-product, we describe how our framework can
obtain points on the Pareto frontier of regularized problems with
self-concordant objectives in a tuning free fashion.


Bipartite Correlation Clustering -- Maximizing Agreements

  In Bipartite Correlation Clustering (BCC) we are given a complete bipartite
graph $G$ with `+' and `-' edges, and we seek a vertex clustering that
maximizes the number of agreements: the number of all `+' edges within clusters
plus all `-' edges cut across clusters. BCC is known to be NP-hard.
  We present a novel approximation algorithm for $k$-BCC, a variant of BCC with
an upper bound $k$ on the number of clusters. Our algorithm outputs a
$k$-clustering that provably achieves a number of agreements within a
multiplicative ${(1-\delta)}$-factor from the optimal, for any desired accuracy
$\delta$. It relies on solving a combinatorially constrained bilinear
maximization on the bi-adjacency matrix of $G$. It runs in time exponential in
$k$ and $\delta^{-1}$, but linear in the size of the input.
  Further, we show that, in the (unconstrained) BCC setting, an
${(1-\delta)}$-approximation can be achieved by $O(\delta^{-1})$ clusters
regardless of the size of the graph. In turn, our $k$-BCC algorithm implies an
Efficient PTAS for the BCC objective of maximizing agreements.


Sparse PCA via Bipartite Matchings

  We consider the following multi-component sparse PCA problem: given a set of
data points, we seek to extract a small number of sparse components with
disjoint supports that jointly capture the maximum possible variance. These
components can be computed one by one, repeatedly solving the single-component
problem and deflating the input data matrix, but as we show this greedy
procedure is suboptimal. We present a novel algorithm for sparse PCA that
jointly optimizes multiple disjoint components. The extracted features capture
variance that lies within a multiplicative factor arbitrarily close to 1 from
the optimal. Our algorithm is combinatorial and computes the desired components
by solving multiple instances of the bipartite maximum weight matching problem.
Its complexity grows as a low order polynomial in the ambient dimension of the
input data matrix, but exponentially in its rank. However, it can be
effectively applied on a low-dimensional sketch of the data; this allows us to
obtain polynomial-time approximation guarantees via spectral bounds. We
evaluate our algorithm on real data-sets and empirically demonstrate that in
many cases it outperforms existing, deflation-based approaches.


Statistical inference using SGD

  We present a novel method for frequentist statistical inference in
$M$-estimation problems, based on stochastic gradient descent (SGD) with a
fixed step size: we demonstrate that the average of such SGD sequences can be
used for statistical inference, after proper scaling. An intuitive analysis
using the Ornstein-Uhlenbeck process suggests that such averages are
asymptotically normal. From a practical perspective, our SGD-based inference
procedure is a first order method, and is well-suited for large scale problems.
To show its merits, we apply it to both synthetic and real datasets, and
demonstrate that its accuracy is comparable to classical statistical methods,
while requiring potentially far less computation.


Approximate Newton-based statistical inference using only stochastic
  gradients

  We present a novel statistical inference framework for convex empirical risk
minimization, using approximate stochastic Newton steps. The proposed algorithm
is based on the notion of finite differences and allows the approximation of a
Hessian-vector product from first-order information. In theory, our method
efficiently computes the statistical error covariance in $M$-estimation, both
for unregularized convex learning problems and high-dimensional LASSO
regression, without using exact second order information, or resampling the
entire data set. We also present a stochastic gradient sampling scheme for
statistical inference in non-i.i.d. time series analysis, where we sample
contiguous blocks of indices. In practice, we demonstrate the effectiveness of
our framework on large-scale machine learning problems, that go even beyond
convexity: as a highlight, our work can be used to detect certain adversarial
attacks on neural networks.


Minimum weight norm models do not always generalize well for
  over-parameterized problems

  Stochastic gradient descent is the de facto algorithm for training deep
neural networks (DNNs). Despite its popularity, it still requires fine tuning
in order to achieve its best performance. This has led to the development of
adaptive methods, that claim automatic hyper-parameter optimization.
  Recently, researchers have studied both algorithmic classes via toy examples:
e.g., for over-parameterized linear regression, Wilson et. al. (2017) shows
that, while SGD always converges to the minimum-norm solution, adaptive methods
show no such inclination, leading to worse generalization capabilities.
  Our aim is to study this conjecture further. We empirically show that the
minimum weight norm is not necessarily the proper gauge of good generalization
in simplified scenaria, and different models found by adaptive methods could
outperform plain gradient methods. In practical DNN settings, we observe that
adaptive methods can outperform SGD, with larger weight norm output models, but
without necessarily reducing the amount of tuning required.


Compressing Gradient Optimizers via Count-Sketches

  Many popular first-order optimization methods (e.g., Momentum, AdaGrad, Adam)
accelerate the convergence rate of deep learning models. However, these
algorithms require auxiliary parameters, which cost additional memory
proportional to the number of parameters in the model. The problem is becoming
more severe as deep learning models continue to grow larger in order to learn
from complex, large-scale datasets. Our proposed solution is to maintain a
linear sketch to compress the auxiliary variables. We demonstrate that our
technique has the same performance as the full-sized baseline, while using
significantly less space for the auxiliary variables. Theoretically, we prove
that count-sketch optimization maintains the SGD convergence rate, while
gracefully reducing memory usage for large-models. On the large-scale 1-Billion
Word dataset, we save 25% of the memory used during training (8.6 GB instead of
11.7 GB) by compressing the Adam optimizer in the Embedding and Softmax layers
with negligible accuracy and performance loss. For an Amazon extreme
classification task with over 49.5 million classes, we also reduce the training
time by 38%, by increasing the mini-batch size 3.5x using our count-sketch
optimizer.


Group-Sparse Model Selection: Hardness and Relaxations

  Group-based sparsity models are proven instrumental in linear regression
problems for recovering signals from much fewer measurements than standard
compressive sensing. The main promise of these models is the recovery of
"interpretable" signals through the identification of their constituent groups.
In this paper, we establish a combinatorial framework for group-model selection
problems and highlight the underlying tractability issues. In particular, we
show that the group-model selection problem is equivalent to the well-known
NP-hard weighted maximum coverage problem (WMC). Leveraging a graph-based
understanding of group models, we describe group structures which enable
correct model selection in polynomial time via dynamic programming.
Furthermore, group structures that lead to totally unimodular constraints have
tractable discrete as well as convex relaxations. We also present a
generalization of the group-model that allows for within group sparsity, which
can be used to model hierarchical sparsity. Finally, we study the Pareto
frontier of group-sparse approximations for two tractable models, among which
the tree sparsity model, and illustrate selection and computation trade-offs
between our framework and the existing convex relaxations.


On Quantifying Qualitative Geospatial Data: A Probabilistic Approach

  Living in the era of data deluge, we have witnessed a web content explosion,
largely due to the massive availability of User-Generated Content (UGC). In
this work, we specifically consider the problem of geospatial information
extraction and representation, where one can exploit diverse sources of
information (such as image and audio data, text data, etc), going beyond
traditional volunteered geographic information. Our ambition is to include
available narrative information in an effort to better explain geospatial
relationships: with spatial reasoning being a basic form of human cognition,
narratives expressing such experiences typically contain qualitative spatial
data, i.e., spatial objects and spatial relationships.
  To this end, we formulate a quantitative approach for the representation of
qualitative spatial relations extracted from UGC in the form of texts. The
proposed method quantifies such relations based on multiple text observations.
Such observations provide distance and orientation features which are utilized
by a greedy Expectation Maximization-based (EM) algorithm to infer a
probability distribution over predefined spatial relationships; the latter
represent the quantified relationships under user-defined probabilistic
assumptions. We evaluate the applicability and quality of the proposed approach
using real UGC data originating from an actual travel blog text corpus. To
verify the quality of the result, we generate grid-based maps visualizing the
spatial extent of the various relations.


Learning Sparse Additive Models with Interactions in High Dimensions

  A function $f: \mathbb{R}^d \rightarrow \mathbb{R}$ is referred to as a
Sparse Additive Model (SPAM), if it is of the form $f(\mathbf{x}) = \sum_{l \in
\mathcal{S}}\phi_{l}(x_l)$, where $\mathcal{S} \subset [d]$, $|\mathcal{S}| \ll
d$. Assuming $\phi_l$'s and $\mathcal{S}$ to be unknown, the problem of
estimating $f$ from its samples has been studied extensively. In this work, we
consider a generalized SPAM, allowing for second order interaction terms. For
some $\mathcal{S}_1 \subset [d], \mathcal{S}_2 \subset {[d] \choose 2}$, the
function $f$ is assumed to be of the form: $$f(\mathbf{x}) = \sum_{p \in
\mathcal{S}_1}\phi_{p} (x_p) + \sum_{(l,l^{\prime}) \in
\mathcal{S}_2}\phi_{(l,l^{\prime})} (x_{l},x_{l^{\prime}}).$$ Assuming
$\phi_{p},\phi_{(l,l^{\prime})}$, $\mathcal{S}_1$ and, $\mathcal{S}_2$ to be
unknown, we provide a randomized algorithm that queries $f$ and exactly
recovers $\mathcal{S}_1,\mathcal{S}_2$. Consequently, this also enables us to
estimate the underlying $\phi_p, \phi_{(l,l^{\prime})}$. We derive sample
complexity bounds for our scheme and also extend our analysis to include the
situation where the queries are corrupted with noise -- either stochastic, or
arbitrary but bounded. Lastly, we provide simulation results on synthetic data,
that validate our theoretical findings.


A single-phase, proximal path-following framework

  We propose a new proximal, path-following framework for a class of
constrained convex problems. We consider settings where the nonlinear---and
possibly non-smooth---objective part is endowed with a proximity operator, and
the constraint set is equipped with a self-concordant barrier. Our approach
relies on the following two main ideas. First, we re-parameterize the
optimality condition as an auxiliary problem, such that a good initial point is
available; by doing so, a family of alternative paths towards the optimum is
generated. Second, we combine the proximal operator with path-following ideas
to design a single-phase, proximal, path-following algorithm. Our method has
several advantages. First, it allows handling non-smooth objectives via
proximal operators; this avoids lifting the problem dimension in order to
accommodate non-smooth components in optimization. Second, it consists of only
a \emph{single phase}: While the overall convergence rate of classical
path-following schemes for self-concordant objectives does not suffer from the
initialization phase, proximal path-following schemes undergo slow convergence,
in order to obtain a good starting point \cite{TranDinh2013e}. In this work, we
show how to overcome this limitation in the proximal setting and prove that our
scheme has the same $\mathcal{O}(\sqrt{\nu}\log(1/\varepsilon))$ worst-case
iteration-complexity with standard approaches \cite{Nesterov2004,Nesterov1994}
without requiring an initial phase, where $\nu$ is the barrier parameter and
$\varepsilon$ is a desired accuracy. Finally, our framework allows errors in
the calculation of proximal-Newton directions, without sacrificing the
worst-case iteration complexity. We demonstrate the merits of our algorithm via
three numerical examples, where proximal operators play a key role.


Algorithms for Learning Sparse Additive Models with Interactions in High
  Dimensions

  A function $f: \mathbb{R}^d \rightarrow \mathbb{R}$ is a Sparse Additive
Model (SPAM), if it is of the form $f(\mathbf{x}) = \sum_{l \in
\mathcal{S}}\phi_{l}(x_l)$ where $\mathcal{S} \subset [d]$, $|\mathcal{S}| \ll
d$. Assuming $\phi$'s, $\mathcal{S}$ to be unknown, there exists extensive work
for estimating $f$ from its samples. In this work, we consider a generalized
version of SPAMs, that also allows for the presence of a sparse number of
second order interaction terms. For some $\mathcal{S}_1 \subset [d],
\mathcal{S}_2 \subset {[d] \choose 2}$, with $|\mathcal{S}_1| \ll d,
|\mathcal{S}_2| \ll d^2$, the function $f$ is now assumed to be of the form:
$\sum_{p \in \mathcal{S}_1}\phi_{p} (x_p) + \sum_{(l,l^{\prime}) \in
\mathcal{S}_2}\phi_{(l,l^{\prime})} (x_l,x_{l^{\prime}})$. Assuming we have the
freedom to query $f$ anywhere in its domain, we derive efficient algorithms
that provably recover $\mathcal{S}_1,\mathcal{S}_2$ with finite sample bounds.
Our analysis covers the noiseless setting where exact samples of $f$ are
obtained, and also extends to the noisy setting where the queries are corrupted
with noise. For the noisy setting in particular, we consider two noise models
namely: i.i.d Gaussian noise and arbitrary but bounded noise. Our main methods
for identification of $\mathcal{S}_2$ essentially rely on estimation of sparse
Hessian matrices, for which we provide two novel compressed sensing based
schemes. Once $\mathcal{S}_1, \mathcal{S}_2$ are known, we show how the
individual components $\phi_p$, $\phi_{(l,l^{\prime})}$ can be estimated via
additional queries of $f$, with uniform error bounds. Lastly, we provide
simulation results on synthetic data that validate our theoretical findings.


SysML: The New Frontier of Machine Learning Systems

  Machine learning (ML) techniques are enjoying rapidly increasing adoption.
However, designing and implementing the systems that support ML models in
real-world deployments remains a significant obstacle, in large part due to the
radically different development and deployment profile of modern ML methods,
and the range of practical concerns that come with broader adoption. We propose
to foster a new systems machine learning research community at the intersection
of the traditional systems and ML communities, focused on topics such as
hardware systems for ML, software systems for ML, and ML optimized for metrics
beyond predictive accuracy. To do this, we describe a new conference, SysML,
that explicitly targets research at the intersection of systems and machine
learning with a program committee split evenly between experts in systems and
ML, and an explicit focus on topics at the intersection of the two.


