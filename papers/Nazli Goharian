A Neural Attention Model for Categorizing Patient Safety Events

  Medical errors are leading causes of death in the US and as such, preventionof these errors is paramount to promoting health care. Patient Safety Eventreports are narratives describing potential adverse events to the patients andare important in identifying and preventing medical errors. We present a neuralnetwork architecture for identifying the type of safety events which is thefirst step in understanding these narratives. Our proposed model is based on asoft neural attention model to improve the effectiveness of encoding longsequences. Empirical results on two large-scale real-world datasets of patientsafety reports demonstrate the effectiveness of our method with significantimprovements over existing methods.

Contextualizing Citations for Scientific Summarization using Word  Embeddings and Domain Knowledge

  Citation texts are sometimes not very informative or in some cases inaccurateby themselves; they need the appropriate context from the referenced paper toreflect its exact contributions. To address this problem, we propose anunsupervised model that uses distributed representation of words as well asdomain knowledge to extract the appropriate context from the reference paper.Evaluation results show the effectiveness of our model by significantlyoutperforming the state-of-the-art. We furthermore demonstrate how an effectivecontextualization method results in improving citation-based summarization ofthe scientific articles.

GU IRLAB at SemEval-2018 Task 7: Tree-LSTMs for Scientific Relation  Classification

  SemEval 2018 Task 7 focuses on relation ex- traction and classification inscientific literature. In this work, we present our tree-based LSTM network forthis shared task. Our approach placed 9th (of 28) for subtask 1.1 (relationclassification), and 5th (of 20) for subtask 1.2 (relation classification withnoisy entities). We also provide an ablation study of features included asinput to the network.

A Discourse-Aware Attention Model for Abstractive Summarization of Long  Documents

  Neural abstractive summarization models have led to promising results insummarizing relatively short documents. We propose the first model forabstractive summarization of single, longer-form documents (e.g., researchpapers). Our approach consists of a new hierarchical encoder that models thediscourse structure of a document, and an attentive discourse-aware decoder togenerate the summary. Empirical results on two large-scale datasets ofscientific papers show that our model significantly outperformsstate-of-the-art models.

Revisiting Summarization Evaluation for Scientific Articles

  Evaluation of text summarization approaches have been mostly based on metricsthat measure similarities of system generated summaries with a set of humanwritten gold-standard summaries. The most widely used metric in summarizationevaluation has been the ROUGE family. ROUGE solely relies on lexical overlapsbetween the terms and phrases in the sentences; therefore, in cases ofterminology variations and paraphrasing, ROUGE is not as effective. Scientificarticle summarization is one such case that is different from general domainsummarization (e.g. newswire data). We provide an extensive analysis of ROUGE'seffectiveness as an evaluation metric for scientific summarization; we showthat, contrary to the common belief, ROUGE is not much reliable in evaluatingscientific summaries. We furthermore show how different variants of ROUGEresult in very different correlations with the manual Pyramid scores. Finally,we propose an alternative metric for summarization evaluation which is based onthe content relevance between a system generated summary and the correspondinghuman written summaries. We call our metric SERA (Summarization Evaluation byRelevance Analysis). Unlike ROUGE, SERA consistently achieves high correlationswith manual scores which shows its effectiveness in evaluation of scientificarticle summarization.

Scientific Article Summarization Using Citation-Context and Article's  Discourse Structure

  We propose a summarization approach for scientific articles which takesadvantage of citation-context and the document discourse model. While citationshave been previously used in generating scientific summaries, they lack therelated context from the referenced article and therefore do not accuratelyreflect the article's content. Our method overcomes the problem ofinconsistency between the citation summary and the article's content byproviding context for each citation. We also leverage the inherent scientificarticle's discourse for producing better summaries. We show that our proposedmethod effectively improves over existing summarization approaches (greaterthan 30% improvement over the best performing baseline) in terms of\textsc{Rouge} scores on TAC2014 scientific summarization dataset. While thedataset we use for evaluation is in the biomedical domain, most of ourapproaches are general and therefore adaptable to other domains.

Scientific document summarization via citation contextualization and  scientific discourse

  The rapid growth of scientific literature has made it difficult for theresearchers to quickly learn about the developments in their respective fields.Scientific document summarization addresses this challenge by providingsummaries of the important contributions of scientific papers. We present aframework for scientific summarization which takes advantage of the citationsand the scientific discourse structure. Citation texts often lack the evidenceand context to support the content of the cited paper and are even sometimesinaccurate. We first address the problem of inaccuracy of the citation texts byfinding the relevant context from the cited paper. We propose three approachesfor contextualizing citations which are based on query reformulation, wordembeddings, and supervised learning. We then train a model to identify thediscourse facets for each citation. We finally propose a method for summarizingscientific papers by leveraging the faceted citations and their correspondingcontexts. We evaluate our proposed method on two scientific summarizationdatasets in the biomedical and computational linguistics domains. Extensiveevaluation results show that our methods can improve over the state of the artby large margins.

Identifying Harm Events in Clinical Care through Medical Narratives

  Preventable medical errors are estimated to be among the leading causes ofinjury and death in the United States. To prevent such errors, healthcaresystems have implemented patient safety and incident reporting systems. Thesesystems enable clinicians to report unsafe conditions and cases where patientshave been harmed due to errors in medical care. These reports are narratives innatural language and while they provide detailed information about thesituation, it is non-trivial to perform large scale analysis for identifyingcommon causes of errors and harm to the patients. In this work, we present amethod based on attentive convolutional and recurrent networks for identifyingharm events in patient care and categorize the harm based on its severitylevel. We demonstrate that our methods can significantly improve theperformance over existing methods in identifying harm in clinical care.

Depression and Self-Harm Risk Assessment in Online Forums

  Users suffering from mental health conditions often turn to online resourcesfor support, including specialized online support communities or generalcommunities such as Twitter and Reddit. In this work, we present a neuralframework for supporting and studying users in both types of communities. Wepropose methods for identifying posts in support communities that may indicatea risk of self-harm, and demonstrate that our approach outperforms strongpreviously proposed methods for identifying such posts. Self-harm is closelyrelated to depression, which makes identifying depressed users on generalforums a crucial related task. We introduce a large-scale general forum dataset("RSDD") consisting of users with self-reported depression diagnoses matchedwith control users. We show how our method can be applied to effectivelyidentify depressed users from their use of language alone. We demonstrate thatour method outperforms strong baselines on this general forum dataset.

Helping or Hurting? Predicting Changes in Users' Risk of Self-Harm  Through Online Community Interactions

  In recent years, online communities have formed around suicide and self-harmprevention. While these communities offer support in moment of crisis, they canalso normalize harmful behavior, discourage professional treatment, andinstigate suicidal ideation. In this work, we focus on how interaction withothers in such a community affects the mental state of users who are seekingsupport. We first build a dataset of conversation threads between users in adistressed state and community members offering support. We then show how toconstruct a classifier to predict whether distressed users are helped or harmedby the interactions in the thread, and we achieve a macro-F1 score of up to0.69.

Characterizing Question Facets for Complex Answer Retrieval

  Complex answer retrieval (CAR) is the process of retrieving answers toquestions that have multifaceted or nuanced answers. In this work, we presenttwo novel approaches for CAR based on the observation that question facets canvary in utility: from structural (facets that can apply to many similar topics,such as 'History') to topical (facets that are specific to the question'stopic, such as the 'Westward expansion' of the United States). We first explorea way to incorporate facet utility into ranking models during query term scorecombination. We then explore a general approach to reform the structure ofranking models to aid in learning of facet utility in the query-document termmatching phase. When we use our techniques with a leading neural ranker on theTREC CAR dataset, our methods rank first in the 2017 TREC CAR benchmark, andyield up to 26% higher performance than the next best method.

SMHD: A Large-Scale Resource for Exploring Online Language Usage for  Multiple Mental Health Conditions

  Mental health is a significant and growing public health concern. As languageusage can be leveraged to obtain crucial insights into mental healthconditions, there is a need for large-scale, labeled, mental health-relateddatasets of users who have been diagnosed with one or more of such conditions.In this paper, we investigate the creation of high-precision patterns toidentify self-reported diagnoses of nine different mental health conditions,and obtain high-quality labeled data without the need for manual labelling. Weintroduce the SMHD (Self-reported Mental Health Diagnoses) dataset and make itavailable. SMHD is a novel large dataset of social media posts from users withone or multiple mental health conditions along with matched control users. Weexamine distinctions in users' language, as measured by linguistic andpsychological variables. We further explore text classification methods toidentify individuals with mental conditions through their language.

RSDD-Time: Temporal Annotation of Self-Reported Mental Health Diagnoses

  Self-reported diagnosis statements have been widely employed in studyinglanguage related to mental health in social media. However, existing researchhas largely ignored the temporality of mental health diagnoses. In this work,we introduce RSDD-Time: a new dataset of 598 manually annotated self-reporteddepression diagnosis posts from Reddit that include temporal information aboutthe diagnosis. Annotations include whether a mental health condition is presentand how recently the diagnosis happened. Furthermore, we include exact temporalspans that relate to the date of diagnosis. This information is valuable forvarious computational methods to examine mental health through social mediabecause one's mental health state is not static. We also test several baselineclassification and extraction approaches, which suggest that extractingtemporal information from self-reported diagnosis statements is challenging.

Triaging Content Severity in Online Mental Health Forums

  Mental health forums are online communities where people express their issuesand seek help from moderators and other users. In such forums, there are oftenposts with severe content indicating that the user is in acute distress andthere is a risk of attempted self-harm. Moderators need to respond to thesesevere posts in a timely manner to prevent potential self-harm. However, thelarge volume of daily posted content makes it difficult for the moderators tolocate and respond to these critical posts. We present a framework for triaginguser content into four severity categories which are defined based onindications of self-harm ideation. Our models are based on a feature-richclassification framework which includes lexical, psycholinguistic, contextualand topic modeling features. Our approaches improve the state of the art intriaging the content severity in mental health forums by large margins (up to17% improvement over the F-1 scores). Using the proposed model, we analyze themental state of users and we show that overall, long-term users of the forumdemonstrate a decreased severity of risk over time. Our analysis on theinteraction of the moderators with the users further indicates that without anautomatic way to identify critical content, it is indeed challenging for themoderators to provide timely response to the users in need.

Overcoming low-utility facets for complex answer retrieval

  Many questions cannot be answered simply; their answers must include numerousnuanced details and additional context. Complex Answer Retrieval (CAR) is theretrieval of answers to such questions. In their simplest form, these questionsare constructed from a topic entity (e.g., `cheese') and a facet (e.g., `healtheffects'). While topic matching has been thoroughly explored, we observe thatsome facets use general language that is unlikely to appear verbatim inanswers. We call these low-utility facets. In this work, we present an approachto CAR that identifies and addresses low-utility facets. We propose twoestimators of facet utility. These include exploiting the hierarchicalstructure of CAR queries and using facet frequency information from trainingdata. To improve the retrieval performance on low-utility headings, we alsoinclude entity similarity scores using knowledge graph embeddings. We apply ourapproaches to a leading neural ranking technique, and evaluate using the TRECCAR dataset. We find that our approach perform significantly better than theunmodified neural ranker and other leading CAR techniques. We also provide adetailed analysis of our results, and verify that low-utility facets are indeedmore difficult to match, and that our approach improves the performance forthese difficult queries.

