A Neural Attention Model for Categorizing Patient Safety Events

  Medical errors are leading causes of death in the US and as such, prevention
of these errors is paramount to promoting health care. Patient Safety Event
reports are narratives describing potential adverse events to the patients and
are important in identifying and preventing medical errors. We present a neural
network architecture for identifying the type of safety events which is the
first step in understanding these narratives. Our proposed model is based on a
soft neural attention model to improve the effectiveness of encoding long
sequences. Empirical results on two large-scale real-world datasets of patient
safety reports demonstrate the effectiveness of our method with significant
improvements over existing methods.


Contextualizing Citations for Scientific Summarization using Word
  Embeddings and Domain Knowledge

  Citation texts are sometimes not very informative or in some cases inaccurate
by themselves; they need the appropriate context from the referenced paper to
reflect its exact contributions. To address this problem, we propose an
unsupervised model that uses distributed representation of words as well as
domain knowledge to extract the appropriate context from the reference paper.
Evaluation results show the effectiveness of our model by significantly
outperforming the state-of-the-art. We furthermore demonstrate how an effective
contextualization method results in improving citation-based summarization of
the scientific articles.


GU IRLAB at SemEval-2018 Task 7: Tree-LSTMs for Scientific Relation
  Classification

  SemEval 2018 Task 7 focuses on relation ex- traction and classification in
scientific literature. In this work, we present our tree-based LSTM network for
this shared task. Our approach placed 9th (of 28) for subtask 1.1 (relation
classification), and 5th (of 20) for subtask 1.2 (relation classification with
noisy entities). We also provide an ablation study of features included as
input to the network.


A Discourse-Aware Attention Model for Abstractive Summarization of Long
  Documents

  Neural abstractive summarization models have led to promising results in
summarizing relatively short documents. We propose the first model for
abstractive summarization of single, longer-form documents (e.g., research
papers). Our approach consists of a new hierarchical encoder that models the
discourse structure of a document, and an attentive discourse-aware decoder to
generate the summary. Empirical results on two large-scale datasets of
scientific papers show that our model significantly outperforms
state-of-the-art models.


Scientific Article Summarization Using Citation-Context and Article's
  Discourse Structure

  We propose a summarization approach for scientific articles which takes
advantage of citation-context and the document discourse model. While citations
have been previously used in generating scientific summaries, they lack the
related context from the referenced article and therefore do not accurately
reflect the article's content. Our method overcomes the problem of
inconsistency between the citation summary and the article's content by
providing context for each citation. We also leverage the inherent scientific
article's discourse for producing better summaries. We show that our proposed
method effectively improves over existing summarization approaches (greater
than 30% improvement over the best performing baseline) in terms of
\textsc{Rouge} scores on TAC2014 scientific summarization dataset. While the
dataset we use for evaluation is in the biomedical domain, most of our
approaches are general and therefore adaptable to other domains.


Revisiting Summarization Evaluation for Scientific Articles

  Evaluation of text summarization approaches have been mostly based on metrics
that measure similarities of system generated summaries with a set of human
written gold-standard summaries. The most widely used metric in summarization
evaluation has been the ROUGE family. ROUGE solely relies on lexical overlaps
between the terms and phrases in the sentences; therefore, in cases of
terminology variations and paraphrasing, ROUGE is not as effective. Scientific
article summarization is one such case that is different from general domain
summarization (e.g. newswire data). We provide an extensive analysis of ROUGE's
effectiveness as an evaluation metric for scientific summarization; we show
that, contrary to the common belief, ROUGE is not much reliable in evaluating
scientific summaries. We furthermore show how different variants of ROUGE
result in very different correlations with the manual Pyramid scores. Finally,
we propose an alternative metric for summarization evaluation which is based on
the content relevance between a system generated summary and the corresponding
human written summaries. We call our metric SERA (Summarization Evaluation by
Relevance Analysis). Unlike ROUGE, SERA consistently achieves high correlations
with manual scores which shows its effectiveness in evaluation of scientific
article summarization.


Scientific document summarization via citation contextualization and
  scientific discourse

  The rapid growth of scientific literature has made it difficult for the
researchers to quickly learn about the developments in their respective fields.
Scientific document summarization addresses this challenge by providing
summaries of the important contributions of scientific papers. We present a
framework for scientific summarization which takes advantage of the citations
and the scientific discourse structure. Citation texts often lack the evidence
and context to support the content of the cited paper and are even sometimes
inaccurate. We first address the problem of inaccuracy of the citation texts by
finding the relevant context from the cited paper. We propose three approaches
for contextualizing citations which are based on query reformulation, word
embeddings, and supervised learning. We then train a model to identify the
discourse facets for each citation. We finally propose a method for summarizing
scientific papers by leveraging the faceted citations and their corresponding
contexts. We evaluate our proposed method on two scientific summarization
datasets in the biomedical and computational linguistics domains. Extensive
evaluation results show that our methods can improve over the state of the art
by large margins.


Identifying Harm Events in Clinical Care through Medical Narratives

  Preventable medical errors are estimated to be among the leading causes of
injury and death in the United States. To prevent such errors, healthcare
systems have implemented patient safety and incident reporting systems. These
systems enable clinicians to report unsafe conditions and cases where patients
have been harmed due to errors in medical care. These reports are narratives in
natural language and while they provide detailed information about the
situation, it is non-trivial to perform large scale analysis for identifying
common causes of errors and harm to the patients. In this work, we present a
method based on attentive convolutional and recurrent networks for identifying
harm events in patient care and categorize the harm based on its severity
level. We demonstrate that our methods can significantly improve the
performance over existing methods in identifying harm in clinical care.


Depression and Self-Harm Risk Assessment in Online Forums

  Users suffering from mental health conditions often turn to online resources
for support, including specialized online support communities or general
communities such as Twitter and Reddit. In this work, we present a neural
framework for supporting and studying users in both types of communities. We
propose methods for identifying posts in support communities that may indicate
a risk of self-harm, and demonstrate that our approach outperforms strong
previously proposed methods for identifying such posts. Self-harm is closely
related to depression, which makes identifying depressed users on general
forums a crucial related task. We introduce a large-scale general forum dataset
("RSDD") consisting of users with self-reported depression diagnoses matched
with control users. We show how our method can be applied to effectively
identify depressed users from their use of language alone. We demonstrate that
our method outperforms strong baselines on this general forum dataset.


Helping or Hurting? Predicting Changes in Users' Risk of Self-Harm
  Through Online Community Interactions

  In recent years, online communities have formed around suicide and self-harm
prevention. While these communities offer support in moment of crisis, they can
also normalize harmful behavior, discourage professional treatment, and
instigate suicidal ideation. In this work, we focus on how interaction with
others in such a community affects the mental state of users who are seeking
support. We first build a dataset of conversation threads between users in a
distressed state and community members offering support. We then show how to
construct a classifier to predict whether distressed users are helped or harmed
by the interactions in the thread, and we achieve a macro-F1 score of up to
0.69.


Characterizing Question Facets for Complex Answer Retrieval

  Complex answer retrieval (CAR) is the process of retrieving answers to
questions that have multifaceted or nuanced answers. In this work, we present
two novel approaches for CAR based on the observation that question facets can
vary in utility: from structural (facets that can apply to many similar topics,
such as 'History') to topical (facets that are specific to the question's
topic, such as the 'Westward expansion' of the United States). We first explore
a way to incorporate facet utility into ranking models during query term score
combination. We then explore a general approach to reform the structure of
ranking models to aid in learning of facet utility in the query-document term
matching phase. When we use our techniques with a leading neural ranker on the
TREC CAR dataset, our methods rank first in the 2017 TREC CAR benchmark, and
yield up to 26% higher performance than the next best method.


SMHD: A Large-Scale Resource for Exploring Online Language Usage for
  Multiple Mental Health Conditions

  Mental health is a significant and growing public health concern. As language
usage can be leveraged to obtain crucial insights into mental health
conditions, there is a need for large-scale, labeled, mental health-related
datasets of users who have been diagnosed with one or more of such conditions.
In this paper, we investigate the creation of high-precision patterns to
identify self-reported diagnoses of nine different mental health conditions,
and obtain high-quality labeled data without the need for manual labelling. We
introduce the SMHD (Self-reported Mental Health Diagnoses) dataset and make it
available. SMHD is a novel large dataset of social media posts from users with
one or multiple mental health conditions along with matched control users. We
examine distinctions in users' language, as measured by linguistic and
psychological variables. We further explore text classification methods to
identify individuals with mental conditions through their language.


RSDD-Time: Temporal Annotation of Self-Reported Mental Health Diagnoses

  Self-reported diagnosis statements have been widely employed in studying
language related to mental health in social media. However, existing research
has largely ignored the temporality of mental health diagnoses. In this work,
we introduce RSDD-Time: a new dataset of 598 manually annotated self-reported
depression diagnosis posts from Reddit that include temporal information about
the diagnosis. Annotations include whether a mental health condition is present
and how recently the diagnosis happened. Furthermore, we include exact temporal
spans that relate to the date of diagnosis. This information is valuable for
various computational methods to examine mental health through social media
because one's mental health state is not static. We also test several baseline
classification and extraction approaches, which suggest that extracting
temporal information from self-reported diagnosis statements is challenging.


Triaging Content Severity in Online Mental Health Forums

  Mental health forums are online communities where people express their issues
and seek help from moderators and other users. In such forums, there are often
posts with severe content indicating that the user is in acute distress and
there is a risk of attempted self-harm. Moderators need to respond to these
severe posts in a timely manner to prevent potential self-harm. However, the
large volume of daily posted content makes it difficult for the moderators to
locate and respond to these critical posts. We present a framework for triaging
user content into four severity categories which are defined based on
indications of self-harm ideation. Our models are based on a feature-rich
classification framework which includes lexical, psycholinguistic, contextual
and topic modeling features. Our approaches improve the state of the art in
triaging the content severity in mental health forums by large margins (up to
17% improvement over the F-1 scores). Using the proposed model, we analyze the
mental state of users and we show that overall, long-term users of the forum
demonstrate a decreased severity of risk over time. Our analysis on the
interaction of the moderators with the users further indicates that without an
automatic way to identify critical content, it is indeed challenging for the
moderators to provide timely response to the users in need.


Overcoming low-utility facets for complex answer retrieval

  Many questions cannot be answered simply; their answers must include numerous
nuanced details and additional context. Complex Answer Retrieval (CAR) is the
retrieval of answers to such questions. In their simplest form, these questions
are constructed from a topic entity (e.g., `cheese') and a facet (e.g., `health
effects'). While topic matching has been thoroughly explored, we observe that
some facets use general language that is unlikely to appear verbatim in
answers. We call these low-utility facets. In this work, we present an approach
to CAR that identifies and addresses low-utility facets. We propose two
estimators of facet utility. These include exploiting the hierarchical
structure of CAR queries and using facet frequency information from training
data. To improve the retrieval performance on low-utility headings, we also
include entity similarity scores using knowledge graph embeddings. We apply our
approaches to a leading neural ranking technique, and evaluate using the TREC
CAR dataset. We find that our approach perform significantly better than the
unmodified neural ranker and other leading CAR techniques. We also provide a
detailed analysis of our results, and verify that low-utility facets are indeed
more difficult to match, and that our approach improves the performance for
these difficult queries.


