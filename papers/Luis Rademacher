Learning convex bodies is hard

  We show that learning a convex body in $\RR^d$, given random samples from the
body, requires $2^{\Omega(\sqrt{d/\eps})}$ samples. By learning a convex body
we mean finding a set having at most $\eps$ relative symmetric difference with
the input body. To prove the lower bound we construct a hard to learn family of
convex bodies. Our construction of this family is very simple and based on
error correcting codes.


On the monotonicity of the expected volume of a random simplex

  Let a random simplex in a d-dimensional convex body be the convex hull of d+1
random points from the body. We study the following question: As a function of
the convex body, is the expected volume of a random simplex monotone
non-decreasing under inclusion? We show that this holds if d is 1 or 2, and
does not hold if d >= 4. We also prove similar results for higher moments of
the volume of a random simplex, in particular for the second moment, which
corresponds to the determinant of the covariance matrix of the convex body.
These questions are motivated by the slicing conjecture.


Query complexity of sampling and small geometric partitions

  In this paper we study the following problem:
  Discrete partitioning problem (DPP): Let $\mathbb{F}_q P^n$ denote the
$n$-dimensional finite projective space over $\mathbb{F}_q$. For positive
integer $k \leq n$, let $\{ A^i\}_{i=1}^N$ be a partition of $(\mathbb{F}_q
P^n)^k$ such that
  (1) for all $i \leq N$, $A^i = \prod_{j=1}^k A^i_j$ (partition into product
sets),
  (2) for all $i \leq N$, there is a $(k-1)$-dimensional subspace $L^i
\subseteq \mathbb{F}_q P^n$ such that $A^i \subseteq (L^i)^k$.
  What is the minimum value of $N$ as a function of $q,n,k$? We will be mainly
interested in the case $k=n$.


Dispersion of Mass and the Complexity of Randomized Geometric Algorithms

  How much can randomness help computation? Motivated by this general question
and by volume computation, one of the few instances where randomness provably
helps, we analyze a notion of dispersion and connect it to asymptotic convex
geometry. We obtain a nearly quadratic lower bound on the complexity of
randomized volume algorithms for convex bodies in R^n (the current best
algorithm has complexity roughly n^4, conjectured to be n^3). Our main tools,
dispersion of random determinants and dispersion of the length of a random
point from a convex body, are of independent interest and applicable more
generally; in particular, the latter is closely related to the variance
hypothesis from convex geometry. This geometric dispersion also leads to lower
bounds for matrix problems and property testing.


Thin sets of integers in Harmonic analysis and p-stable random Fourier
  series

  We investigate the behavior of some thin sets of integers defined through
random trigonometric polynomial when one replaces Gaussian or Rademacher
variables by p-stable ones, with 1 < p < 2. We show that in one case this
behavior is essentially the same as in the Gaussian case, whereas in another
case, this behavior is entirely different.


The Minimum Euclidean-Norm Point on a Convex Polytope: Wolfe's
  Combinatorial Algorithm is Exponential

  The complexity of Philip Wolfe's method for the minimum Euclidean-norm point
problem over a convex polytope has remained unknown since he proposed the
method in 1974. The method is important because it is used as a subroutine for
one of the most practical algorithms for submodular function minimization. We
present the first example that Wolfe's method takes exponential time.
Additionally, we improve previous results to show that linear programming
reduces in strongly-polynomial time to the minimum norm point problem over a
simplex.


Inter-Core Crosstalk Impact of Classical Channels on CV-QKD in Multicore
  Fiber Transmission

  Crosstalk-induced excess noise is experimentally characterized for
continuous-variable quantum key distribution, spatially multiplexed with WDM
PM-16QAM channels in a 19-core fiber. The measured noise-sources are used to
estimate the secret key rates for different wavelength channels.


Expanders via Random Spanning Trees

  Motivated by the problem of routing reliably and scalably in a graph, we
introduce the notion of a splicer, the union of spanning trees of a graph. We
prove that for any bounded-degree n-vertex graph, the union of two random
spanning trees approximates the expansion of every cut of the graph to within a
factor of O(log n). For the random graph G_{n,p}, for p> c log{n}/n, two
spanning trees give an expander. This is suggested by the case of the complete
graph, where we prove that two random spanning trees give an expander. The
construction of the splicer is elementary -- each spanning tree can be produced
independently using an algorithm by Aldous and Broder: a random walk in the
graph with edges leading to previously unvisited vertices included in the tree.
  A second important application of splicers is to graph sparsification where
the goal is to approximate every cut (and more generally the quadratic form of
the Laplacian) using only a small subgraph of the original graph.
Benczur-Karger as well as Spielman-Srivastava have shown sparsifiers with O(n
log n/eps^2)$ edges that achieve approximation within factors 1+eps and 1-eps.
Their methods, based on independent sampling of edges, need Omega(n log n)
edges to get any approximation (else the subgraph could be disconnected) and
leave open the question of linear-size sparsifiers. Splicers address this
question for random graphs by providing sparsifiers of size O(n) that
approximate every cut to within a factor of O(log n).


Lower Bounds for the Average and Smoothed Number of Pareto Optima

  Smoothed analysis of multiobjective 0-1 linear optimization has drawn
considerable attention recently. The number of Pareto-optimal solutions (i.e.,
solutions with the property that no other solution is at least as good in all
the coordinates and better in at least one) for multiobjective optimization
problems is the central object of study. In this paper, we prove several lower
bounds for the expected number of Pareto optima. Our basic result is a lower
bound of \Omega_d(n^(d-1)) for optimization problems with d objectives and n
variables under fairly general conditions on the distributions of the linear
objectives. Our proof relates the problem of lower bounding the number of
Pareto optima to results in geometry connected to arrangements of hyperplanes.
We use our basic result to derive (1) To our knowledge, the first lower bound
for natural multiobjective optimization problems. We illustrate this for the
maximum spanning tree problem with randomly chosen edge weights. Our technique
is sufficiently flexible to yield such lower bounds for other standard
objective functions studied in this setting (such as, multiobjective shortest
path, TSP tour, matching). (2) Smoothed lower bound of min {\Omega_d(n^(d-1.5)
\phi^{(d-log d) (1-\Theta(1/\phi))}), 2^{\Theta(n)}}$ for the 0-1 knapsack
problem with d profits for phi-semirandom distributions for a version of the
knapsack problem. This improves the recent lower bound of Brunsch and Roeglin.


A simplicial polytope that maximizes the isotropic constant must be a
  simplex

  The isotropic constant $L_K$ is an affine-invariant measure of the spread of
a convex body $K$. For a $d$-dimensional convex body $K$, $L_K$ can be defined
by $L_K^{2d} = \det(A(K))/(\mathrm{vol}(K))^2$, where $A(K)$ is the covariance
matrix of the uniform distribution on $K$. It is an outstanding open problem to
find a tight asymptotic upper bound of the isotropic constant as a function of
the dimension. It has been conjectured that there is a universal constant upper
bound. The conjecture is known to be true for several families of bodies, in
particular, highly symmetric bodies such as bodies having an unconditional
basis. It is also known that maximizers cannot be smooth.
  In this work we study the gap between smooth bodies and highly symmetric
bodies by showing progress towards reducing to a highly symmetric case among
non-smooth bodies. More precisely, we study the set of maximizers among
simplicial polytopes and we show that if a simplicial polytope $K$ is a
maximizer of the isotropic constant among $d$-dimensional convex bodies, then
when $K$ is put in isotropic position it is symmetric around any hyperplane
spanned by a $(d-2)$-dimensional face and the origin. By a result of Campi,
Colesanti and Gronchi, this implies that a simplicial polytope that maximizes
the isotropic constant must be a simplex.


Efficient volume sampling for row/column subset selection

  We give efficient algorithms for volume sampling, i.e., for picking
$k$-subsets of the rows of any given matrix with probabilities proportional to
the squared volumes of the simplices defined by them and the origin (or the
squared volumes of the parallelepipeds defined by these subsets of rows). This
solves an open problem from the monograph on spectral algorithms by Kannan and
Vempala. Our first algorithm for volume sampling $k$-subsets of rows from an
$m$-by-$n$ matrix runs in $O(kmn^{\omega} \log n)$ arithmetic operations and a
second variant of it for $(1+\epsilon)$-approximate volume sampling runs in
$O(mn \log m \cdot k^{2}/\epsilon^{2} + m \log^{\omega} m \cdot
k^{2\omega+1}/\epsilon^{2\omega} \cdot \log(k \epsilon^{-1} \log m))$
arithmetic operations, which is almost linear in the size of the input (i.e.,
the number of entries) for small $k$. Our efficient volume sampling algorithms
imply several interesting results for low-rank matrix approximation.


The Hidden Convexity of Spectral Clustering

  In recent years, spectral clustering has become a standard method for data
analysis used in a broad range of applications. In this paper we propose a new
class of algorithms for multiway spectral clustering based on optimization of a
certain "contrast function" over the unit sphere. These algorithms, partly
inspired by certain Independent Component Analysis techniques, are simple, easy
to implement and efficient.
  Geometrically, the proposed algorithms can be interpreted as hidden basis
recovery by means of function optimization. We give a complete characterization
of the contrast functions admissible for provable basis recovery. We show how
these conditions can be interpreted as a "hidden convexity" of our optimization
problem on the sphere; interestingly, we use efficient convex maximization
rather than the more common convex minimization. We also show encouraging
experimental results on real and simulated data.


Heavy-tailed Independent Component Analysis

  Independent component analysis (ICA) is the problem of efficiently recovering
a matrix $A \in \mathbb{R}^{n\times n}$ from i.i.d. observations of $X=AS$
where $S \in \mathbb{R}^n$ is a random vector with mutually independent
coordinates. This problem has been intensively studied, but all existing
efficient algorithms with provable guarantees require that the coordinates
$S_i$ have finite fourth moments. We consider the heavy-tailed ICA problem
where we do not make this assumption, about the second moment. This problem
also has received considerable attention in the applied literature. In the
present work, we first give a provably efficient algorithm that works under the
assumption that for constant $\gamma > 0$, each $S_i$ has finite
$(1+\gamma)$-moment, thus substantially weakening the moment requirement
condition for the ICA problem to be solvable. We then give an algorithm that
works under the assumption that matrix $A$ has orthogonal columns but requires
no moment assumptions. Our techniques draw ideas from convex geometry and
exploit standard properties of the multivariate spherical Gaussian distribution
in a novel way.


Geometric Constellation Shaping for Fiber Optic Communication Systems
  via End-to-end Learning

  In this paper, an unsupervised machine learning method for geometric
constellation shaping is investigated. By embedding a differentiable fiber
channel model within two neural networks, the learning algorithm is optimizing
for a geometric constellation shape. The learned constellations yield improved
performance to state-of-the-art geometrically shaped constellations, and
include an implicit trade-off between amplification noise and nonlinear
effects. Further, the method allows joint optimization of system parameters,
such as the optimal launch power, simultaneously with the constellation shape.
An experimental demonstration validates the findings. Improved performances are
reported, up to 0.13 bit/4D in simulation and experimentally up to 0.12 bit/4D.


Blind Signal Separation in the Presence of Gaussian Noise

  A prototypical blind signal separation problem is the so-called cocktail
party problem, with n people talking simultaneously and n different microphones
within a room. The goal is to recover each speech signal from the microphone
inputs. Mathematically this can be modeled by assuming that we are given
samples from an n-dimensional random variable X=AS, where S is a vector whose
coordinates are independent random variables corresponding to each speaker. The
objective is to recover the matrix A^{-1} given random samples from X. A range
of techniques collectively known as Independent Component Analysis (ICA) have
been proposed to address this problem in the signal processing and machine
learning literature. Many of these techniques are based on using the kurtosis
or other cumulants to recover the components.
  In this paper we propose a new algorithm for solving the blind signal
separation problem in the presence of additive Gaussian noise, when we are
given samples from X=AS+\eta, where \eta is drawn from an unknown, not
necessarily spherical n-dimensional Gaussian distribution. Our approach is
based on a method for decorrelating a sample with additive Gaussian noise under
the assumption that the underlying distribution is a linear transformation of a
distribution with independent components. Our decorrelation routine is based on
the properties of cumulant tensors and can be combined with any standard
cumulant-based method for ICA to get an algorithm that is provably robust in
the presence of Gaussian noise. We derive polynomial bounds for the sample
complexity and error propagation of our method.


Efficient learning of simplices

  We show an efficient algorithm for the following problem: Given uniformly
random points from an arbitrary n-dimensional simplex, estimate the simplex.
The size of the sample and the number of arithmetic operations of our algorithm
are polynomial in n. This answers a question of Frieze, Jerrum and Kannan
[FJK]. Our result can also be interpreted as efficiently learning the
intersection of n+1 half-spaces in R^n in the model where the intersection is
bounded and we are given polynomially many uniform samples from it. Our proof
uses the local search technique from Independent Component Analysis (ICA), also
used by [FJK]. Unlike these previous algorithms, which were based on analyzing
the fourth moment, ours is based on the third moment.
  We also show a direct connection between the problem of learning a simplex
and ICA: a simple randomized reduction to ICA from the problem of learning a
simplex. The connection is based on a known representation of the uniform
measure on a simplex. Similar representations lead to a reduction from the
problem of learning an affine transformation of an n-dimensional l_p ball to
ICA.


Eigenvectors of Orthogonally Decomposable Functions

  The Eigendecomposition of quadratic forms (symmetric matrices) guaranteed by
the spectral theorem is a foundational result in applied mathematics. Motivated
by a shared structure found in inferential problems of recent interest---namely
orthogonal tensor decompositions, Independent Component Analysis (ICA), topic
models, spectral clustering, and Gaussian mixture learning---we generalize the
eigendecomposition from quadratic forms to a broad class of "orthogonally
decomposable" functions. We identify a key role of convexity in our extension,
and we generalize two traditional characterizations of eigenvectors: First, the
eigenvectors of a quadratic form arise from the optima structure of the
quadratic form on the sphere. Second, the eigenvectors are the fixed points of
the power iteration.
  In our setting, we consider a simple first order generalization of the power
method which we call gradient iteration. It leads to efficient and easily
implementable methods for basis recovery. It includes influential Machine
Learning methods such as cumulant-based FastICA and the tensor power iteration
for orthogonally decomposable tensors as special cases.
  We provide a complete theoretical analysis of gradient iteration using the
structure theory of discrete dynamical systems to show almost sure convergence
and fast (super-linear) convergence rates. The analysis also extends to the
case when the observed function is only approximately orthogonally
decomposable, with bounds that are polynomial in dimension and other relevant
parameters, such as perturbation size. Our perturbation results can be
considered as a non-linear version of the classical Davis-Kahan theorem for
perturbations of eigenvectors of symmetric matrices.


A Pseudo-Euclidean Iteration for Optimal Recovery in Noisy ICA

  Independent Component Analysis (ICA) is a popular model for blind signal
separation. The ICA model assumes that a number of independent source signals
are linearly mixed to form the observed signals. We propose a new algorithm,
PEGI (for pseudo-Euclidean Gradient Iteration), for provable model recovery for
ICA with Gaussian noise. The main technical innovation of the algorithm is to
use a fixed point iteration in a pseudo-Euclidean (indefinite "inner product")
space. The use of this indefinite "inner product" resolves technical issues
common to several existing algorithms for noisy ICA. This leads to an algorithm
which is conceptually simple, efficient and accurate in testing.
  Our second contribution is combining PEGI with the analysis of objectives for
optimal recovery in the noisy ICA model. It has been observed that the direct
approach of demixing with the inverse of the mixing matrix is suboptimal for
signal recovery in terms of the natural Signal to Interference plus Noise Ratio
(SINR) criterion. There have been several partial solutions proposed in the ICA
literature. It turns out that any solution to the mixing matrix reconstruction
problem can be used to construct an SINR-optimal ICA demixing, despite the fact
that SINR itself cannot be computed from data. That allows us to obtain a
practical and provably SINR-optimal recovery method for ICA with arbitrary
Gaussian noise.


Heavy-Tailed Analogues of the Covariance Matrix for ICA

  Independent Component Analysis (ICA) is the problem of learning a square
matrix $A$, given samples of $X=AS$, where $S$ is a random vector with
independent coordinates. Most existing algorithms are provably efficient only
when each $S_i$ has finite and moderately valued fourth moment. However, there
are practical applications where this assumption need not be true, such as
speech and finance. Algorithms have been proposed for heavy-tailed ICA, but
they are not practical, using random walks and the full power of the ellipsoid
algorithm multiple times. The main contributions of this paper are:
  (1) A practical algorithm for heavy-tailed ICA that we call HTICA. We provide
theoretical guarantees and show that it outperforms other algorithms in some
heavy-tailed regimes, both on real and synthetic data. Like the current
state-of-the-art, the new algorithm is based on the centroid body (a first
moment analogue of the covariance matrix). Unlike the state-of-the-art, our
algorithm is practically efficient. To achieve this, we use explicit analytic
representations of the centroid body, which bypasses the use of the ellipsoid
method and random walks.
  (2) We study how heavy tails affect different ICA algorithms, including
HTICA. Somewhat surprisingly, we show that some algorithms that use the
covariance matrix or higher moments can successfully solve a range of ICA
instances with infinite second moment. We study this theoretically and
experimentally, with both synthetic and real-world heavy-tailed data.


Pilot-Aided Joint-Channel Carrier-Phase Estimation in Space-Division
  Multiplexed Multicore Fiber Transmission

  The performance of pilot-aided joint-channel carrier-phase estimation (CPE)
in space-division multiplexed multicore fiber (MCF) transmission with
correlated phase noise is studied. To that end, a system model describing
uncoded MCF transmission where the phase noise comprises a common laser phase
noise, in addition to core- and polarization-specific phase drifts, is
introduced. It is then shown that the system model can be regarded as a special
case of a multidimensional random-walk phase-noise model. A pilot-aided CPE
algorithm that was previously developed for this model is used to evaluate two
strategies, namely joint-channel and per-channel CPE. To quantify the
performance differences between the two strategies, their phase-noise
tolerances are assessed through Monte Carlo simulations of uncoded transmission
for different modulation formats, pilot overheads, laser linewidths, numbers of
spatial channels, and degrees of phase-noise correlation across the channels.
For 20 GBd transmission with 200 kHz combined laser linewidth and 1% pilot
overhead, joint-channel CPE yields up to 3.1 dB increase in power efficiency or
25.5% increase in spectral efficiency. Moreover, through MCF transmission
experiments, the system model is validated and the strategies are compared in
terms of bit error rate performance versus transmission distance for uncoded
transmission of different modulation formats. Up to 22.8% increase in
transmission reach is observed for 1% pilot overhead through the use of
joint-channel CPE.


The More, the Merrier: the Blessing of Dimensionality for Learning Large
  Gaussian Mixtures

  In this paper we show that very large mixtures of Gaussians are efficiently
learnable in high dimension. More precisely, we prove that a mixture with known
identical covariance matrices whose number of components is a polynomial of any
fixed degree in the dimension n is polynomially learnable as long as a certain
non-degeneracy condition on the means is satisfied. It turns out that this
condition is generic in the sense of smoothed complexity, as soon as the
dimensionality of the space is high enough. Moreover, we prove that no such
condition can possibly exist in low dimension and the problem of learning the
parameters is generically hard. In contrast, much of the existing work on
Gaussian Mixtures relies on low-dimensional projections and thus hits an
artificial barrier. Our main result on mixture recovery relies on a new
"Poissonization"-based technique, which transforms a mixture of Gaussians to a
linear map of a product distribution. The problem of learning this map can be
efficiently solved using some recent results on tensor decompositions and
Independent Component Analysis (ICA), thus giving an algorithm for recovering
the mixture. In addition, we combine our low-dimensional hardness results for
Gaussian mixtures with Poissonization to show how to embed difficult instances
of low-dimensional Gaussian mixtures into the ICA setting, thus establishing
exponential information-theoretic lower bounds for underdetermined ICA in low
dimension. To the best of our knowledge, this is the first such result in the
literature. In addition to contributing to the problem of Gaussian mixture
learning, we believe that this work is among the first steps toward better
understanding the rare phenomenon of the "blessing of dimensionality" in the
computational aspects of statistical inference.


