Learning convex bodies is hard

  We show that learning a convex body in $\RR^d$, given random samples from thebody, requires $2^{\Omega(\sqrt{d/\eps})}$ samples. By learning a convex bodywe mean finding a set having at most $\eps$ relative symmetric difference withthe input body. To prove the lower bound we construct a hard to learn family ofconvex bodies. Our construction of this family is very simple and based onerror correcting codes.

On the monotonicity of the expected volume of a random simplex

  Let a random simplex in a d-dimensional convex body be the convex hull of d+1random points from the body. We study the following question: As a function ofthe convex body, is the expected volume of a random simplex monotonenon-decreasing under inclusion? We show that this holds if d is 1 or 2, anddoes not hold if d >= 4. We also prove similar results for higher moments ofthe volume of a random simplex, in particular for the second moment, whichcorresponds to the determinant of the covariance matrix of the convex body.These questions are motivated by the slicing conjecture.

Query complexity of sampling and small geometric partitions

  In this paper we study the following problem:  Discrete partitioning problem (DPP): Let $\mathbb{F}_q P^n$ denote the$n$-dimensional finite projective space over $\mathbb{F}_q$. For positiveinteger $k \leq n$, let $\{ A^i\}_{i=1}^N$ be a partition of $(\mathbb{F}_qP^n)^k$ such that  (1) for all $i \leq N$, $A^i = \prod_{j=1}^k A^i_j$ (partition into productsets),  (2) for all $i \leq N$, there is a $(k-1)$-dimensional subspace $L^i\subseteq \mathbb{F}_q P^n$ such that $A^i \subseteq (L^i)^k$.  What is the minimum value of $N$ as a function of $q,n,k$? We will be mainlyinterested in the case $k=n$.

Dispersion of Mass and the Complexity of Randomized Geometric Algorithms

  How much can randomness help computation? Motivated by this general questionand by volume computation, one of the few instances where randomness provablyhelps, we analyze a notion of dispersion and connect it to asymptotic convexgeometry. We obtain a nearly quadratic lower bound on the complexity ofrandomized volume algorithms for convex bodies in R^n (the current bestalgorithm has complexity roughly n^4, conjectured to be n^3). Our main tools,dispersion of random determinants and dispersion of the length of a randompoint from a convex body, are of independent interest and applicable moregenerally; in particular, the latter is closely related to the variancehypothesis from convex geometry. This geometric dispersion also leads to lowerbounds for matrix problems and property testing.

Thin sets of integers in Harmonic analysis and p-stable random Fourier  series

  We investigate the behavior of some thin sets of integers defined throughrandom trigonometric polynomial when one replaces Gaussian or Rademachervariables by p-stable ones, with 1 < p < 2. We show that in one case thisbehavior is essentially the same as in the Gaussian case, whereas in anothercase, this behavior is entirely different.

The Minimum Euclidean-Norm Point on a Convex Polytope: Wolfe's  Combinatorial Algorithm is Exponential

  The complexity of Philip Wolfe's method for the minimum Euclidean-norm pointproblem over a convex polytope has remained unknown since he proposed themethod in 1974. The method is important because it is used as a subroutine forone of the most practical algorithms for submodular function minimization. Wepresent the first example that Wolfe's method takes exponential time.Additionally, we improve previous results to show that linear programmingreduces in strongly-polynomial time to the minimum norm point problem over asimplex.

Inter-Core Crosstalk Impact of Classical Channels on CV-QKD in Multicore  Fiber Transmission

  Crosstalk-induced excess noise is experimentally characterized forcontinuous-variable quantum key distribution, spatially multiplexed with WDMPM-16QAM channels in a 19-core fiber. The measured noise-sources are used toestimate the secret key rates for different wavelength channels.

Expanders via Random Spanning Trees

  Motivated by the problem of routing reliably and scalably in a graph, weintroduce the notion of a splicer, the union of spanning trees of a graph. Weprove that for any bounded-degree n-vertex graph, the union of two randomspanning trees approximates the expansion of every cut of the graph to within afactor of O(log n). For the random graph G_{n,p}, for p> c log{n}/n, twospanning trees give an expander. This is suggested by the case of the completegraph, where we prove that two random spanning trees give an expander. Theconstruction of the splicer is elementary -- each spanning tree can be producedindependently using an algorithm by Aldous and Broder: a random walk in thegraph with edges leading to previously unvisited vertices included in the tree.  A second important application of splicers is to graph sparsification wherethe goal is to approximate every cut (and more generally the quadratic form ofthe Laplacian) using only a small subgraph of the original graph.Benczur-Karger as well as Spielman-Srivastava have shown sparsifiers with O(nlog n/eps^2)$ edges that achieve approximation within factors 1+eps and 1-eps.Their methods, based on independent sampling of edges, need Omega(n log n)edges to get any approximation (else the subgraph could be disconnected) andleave open the question of linear-size sparsifiers. Splicers address thisquestion for random graphs by providing sparsifiers of size O(n) thatapproximate every cut to within a factor of O(log n).

Lower Bounds for the Average and Smoothed Number of Pareto Optima

  Smoothed analysis of multiobjective 0-1 linear optimization has drawnconsiderable attention recently. The number of Pareto-optimal solutions (i.e.,solutions with the property that no other solution is at least as good in allthe coordinates and better in at least one) for multiobjective optimizationproblems is the central object of study. In this paper, we prove several lowerbounds for the expected number of Pareto optima. Our basic result is a lowerbound of \Omega_d(n^(d-1)) for optimization problems with d objectives and nvariables under fairly general conditions on the distributions of the linearobjectives. Our proof relates the problem of lower bounding the number ofPareto optima to results in geometry connected to arrangements of hyperplanes.We use our basic result to derive (1) To our knowledge, the first lower boundfor natural multiobjective optimization problems. We illustrate this for themaximum spanning tree problem with randomly chosen edge weights. Our techniqueis sufficiently flexible to yield such lower bounds for other standardobjective functions studied in this setting (such as, multiobjective shortestpath, TSP tour, matching). (2) Smoothed lower bound of min {\Omega_d(n^(d-1.5)\phi^{(d-log d) (1-\Theta(1/\phi))}), 2^{\Theta(n)}}$ for the 0-1 knapsackproblem with d profits for phi-semirandom distributions for a version of theknapsack problem. This improves the recent lower bound of Brunsch and Roeglin.

A simplicial polytope that maximizes the isotropic constant must be a  simplex

  The isotropic constant $L_K$ is an affine-invariant measure of the spread ofa convex body $K$. For a $d$-dimensional convex body $K$, $L_K$ can be definedby $L_K^{2d} = \det(A(K))/(\mathrm{vol}(K))^2$, where $A(K)$ is the covariancematrix of the uniform distribution on $K$. It is an outstanding open problem tofind a tight asymptotic upper bound of the isotropic constant as a function ofthe dimension. It has been conjectured that there is a universal constant upperbound. The conjecture is known to be true for several families of bodies, inparticular, highly symmetric bodies such as bodies having an unconditionalbasis. It is also known that maximizers cannot be smooth.  In this work we study the gap between smooth bodies and highly symmetricbodies by showing progress towards reducing to a highly symmetric case amongnon-smooth bodies. More precisely, we study the set of maximizers amongsimplicial polytopes and we show that if a simplicial polytope $K$ is amaximizer of the isotropic constant among $d$-dimensional convex bodies, thenwhen $K$ is put in isotropic position it is symmetric around any hyperplanespanned by a $(d-2)$-dimensional face and the origin. By a result of Campi,Colesanti and Gronchi, this implies that a simplicial polytope that maximizesthe isotropic constant must be a simplex.

Efficient volume sampling for row/column subset selection

  We give efficient algorithms for volume sampling, i.e., for picking$k$-subsets of the rows of any given matrix with probabilities proportional tothe squared volumes of the simplices defined by them and the origin (or thesquared volumes of the parallelepipeds defined by these subsets of rows). Thissolves an open problem from the monograph on spectral algorithms by Kannan andVempala. Our first algorithm for volume sampling $k$-subsets of rows from an$m$-by-$n$ matrix runs in $O(kmn^{\omega} \log n)$ arithmetic operations and asecond variant of it for $(1+\epsilon)$-approximate volume sampling runs in$O(mn \log m \cdot k^{2}/\epsilon^{2} + m \log^{\omega} m \cdotk^{2\omega+1}/\epsilon^{2\omega} \cdot \log(k \epsilon^{-1} \log m))$arithmetic operations, which is almost linear in the size of the input (i.e.,the number of entries) for small $k$. Our efficient volume sampling algorithmsimply several interesting results for low-rank matrix approximation.

The Hidden Convexity of Spectral Clustering

  In recent years, spectral clustering has become a standard method for dataanalysis used in a broad range of applications. In this paper we propose a newclass of algorithms for multiway spectral clustering based on optimization of acertain "contrast function" over the unit sphere. These algorithms, partlyinspired by certain Independent Component Analysis techniques, are simple, easyto implement and efficient.  Geometrically, the proposed algorithms can be interpreted as hidden basisrecovery by means of function optimization. We give a complete characterizationof the contrast functions admissible for provable basis recovery. We show howthese conditions can be interpreted as a "hidden convexity" of our optimizationproblem on the sphere; interestingly, we use efficient convex maximizationrather than the more common convex minimization. We also show encouragingexperimental results on real and simulated data.

Heavy-tailed Independent Component Analysis

  Independent component analysis (ICA) is the problem of efficiently recoveringa matrix $A \in \mathbb{R}^{n\times n}$ from i.i.d. observations of $X=AS$where $S \in \mathbb{R}^n$ is a random vector with mutually independentcoordinates. This problem has been intensively studied, but all existingefficient algorithms with provable guarantees require that the coordinates$S_i$ have finite fourth moments. We consider the heavy-tailed ICA problemwhere we do not make this assumption, about the second moment. This problemalso has received considerable attention in the applied literature. In thepresent work, we first give a provably efficient algorithm that works under theassumption that for constant $\gamma > 0$, each $S_i$ has finite$(1+\gamma)$-moment, thus substantially weakening the moment requirementcondition for the ICA problem to be solvable. We then give an algorithm thatworks under the assumption that matrix $A$ has orthogonal columns but requiresno moment assumptions. Our techniques draw ideas from convex geometry andexploit standard properties of the multivariate spherical Gaussian distributionin a novel way.

Geometric Constellation Shaping for Fiber Optic Communication Systems  via End-to-end Learning

  In this paper, an unsupervised machine learning method for geometricconstellation shaping is investigated. By embedding a differentiable fiberchannel model within two neural networks, the learning algorithm is optimizingfor a geometric constellation shape. The learned constellations yield improvedperformance to state-of-the-art geometrically shaped constellations, andinclude an implicit trade-off between amplification noise and nonlineareffects. Further, the method allows joint optimization of system parameters,such as the optimal launch power, simultaneously with the constellation shape.An experimental demonstration validates the findings. Improved performances arereported, up to 0.13 bit/4D in simulation and experimentally up to 0.12 bit/4D.

A Pseudo-Euclidean Iteration for Optimal Recovery in Noisy ICA

  Independent Component Analysis (ICA) is a popular model for blind signalseparation. The ICA model assumes that a number of independent source signalsare linearly mixed to form the observed signals. We propose a new algorithm,PEGI (for pseudo-Euclidean Gradient Iteration), for provable model recovery forICA with Gaussian noise. The main technical innovation of the algorithm is touse a fixed point iteration in a pseudo-Euclidean (indefinite "inner product")space. The use of this indefinite "inner product" resolves technical issuescommon to several existing algorithms for noisy ICA. This leads to an algorithmwhich is conceptually simple, efficient and accurate in testing.  Our second contribution is combining PEGI with the analysis of objectives foroptimal recovery in the noisy ICA model. It has been observed that the directapproach of demixing with the inverse of the mixing matrix is suboptimal forsignal recovery in terms of the natural Signal to Interference plus Noise Ratio(SINR) criterion. There have been several partial solutions proposed in the ICAliterature. It turns out that any solution to the mixing matrix reconstructionproblem can be used to construct an SINR-optimal ICA demixing, despite the factthat SINR itself cannot be computed from data. That allows us to obtain apractical and provably SINR-optimal recovery method for ICA with arbitraryGaussian noise.

Blind Signal Separation in the Presence of Gaussian Noise

  A prototypical blind signal separation problem is the so-called cocktailparty problem, with n people talking simultaneously and n different microphoneswithin a room. The goal is to recover each speech signal from the microphoneinputs. Mathematically this can be modeled by assuming that we are givensamples from an n-dimensional random variable X=AS, where S is a vector whosecoordinates are independent random variables corresponding to each speaker. Theobjective is to recover the matrix A^{-1} given random samples from X. A rangeof techniques collectively known as Independent Component Analysis (ICA) havebeen proposed to address this problem in the signal processing and machinelearning literature. Many of these techniques are based on using the kurtosisor other cumulants to recover the components.  In this paper we propose a new algorithm for solving the blind signalseparation problem in the presence of additive Gaussian noise, when we aregiven samples from X=AS+\eta, where \eta is drawn from an unknown, notnecessarily spherical n-dimensional Gaussian distribution. Our approach isbased on a method for decorrelating a sample with additive Gaussian noise underthe assumption that the underlying distribution is a linear transformation of adistribution with independent components. Our decorrelation routine is based onthe properties of cumulant tensors and can be combined with any standardcumulant-based method for ICA to get an algorithm that is provably robust inthe presence of Gaussian noise. We derive polynomial bounds for the samplecomplexity and error propagation of our method.

Efficient learning of simplices

  We show an efficient algorithm for the following problem: Given uniformlyrandom points from an arbitrary n-dimensional simplex, estimate the simplex.The size of the sample and the number of arithmetic operations of our algorithmare polynomial in n. This answers a question of Frieze, Jerrum and Kannan[FJK]. Our result can also be interpreted as efficiently learning theintersection of n+1 half-spaces in R^n in the model where the intersection isbounded and we are given polynomially many uniform samples from it. Our proofuses the local search technique from Independent Component Analysis (ICA), alsoused by [FJK]. Unlike these previous algorithms, which were based on analyzingthe fourth moment, ours is based on the third moment.  We also show a direct connection between the problem of learning a simplexand ICA: a simple randomized reduction to ICA from the problem of learning asimplex. The connection is based on a known representation of the uniformmeasure on a simplex. Similar representations lead to a reduction from theproblem of learning an affine transformation of an n-dimensional l_p ball toICA.

Eigenvectors of Orthogonally Decomposable Functions

  The Eigendecomposition of quadratic forms (symmetric matrices) guaranteed bythe spectral theorem is a foundational result in applied mathematics. Motivatedby a shared structure found in inferential problems of recent interest---namelyorthogonal tensor decompositions, Independent Component Analysis (ICA), topicmodels, spectral clustering, and Gaussian mixture learning---we generalize theeigendecomposition from quadratic forms to a broad class of "orthogonallydecomposable" functions. We identify a key role of convexity in our extension,and we generalize two traditional characterizations of eigenvectors: First, theeigenvectors of a quadratic form arise from the optima structure of thequadratic form on the sphere. Second, the eigenvectors are the fixed points ofthe power iteration.  In our setting, we consider a simple first order generalization of the powermethod which we call gradient iteration. It leads to efficient and easilyimplementable methods for basis recovery. It includes influential MachineLearning methods such as cumulant-based FastICA and the tensor power iterationfor orthogonally decomposable tensors as special cases.  We provide a complete theoretical analysis of gradient iteration using thestructure theory of discrete dynamical systems to show almost sure convergenceand fast (super-linear) convergence rates. The analysis also extends to thecase when the observed function is only approximately orthogonallydecomposable, with bounds that are polynomial in dimension and other relevantparameters, such as perturbation size. Our perturbation results can beconsidered as a non-linear version of the classical Davis-Kahan theorem forperturbations of eigenvectors of symmetric matrices.

Heavy-Tailed Analogues of the Covariance Matrix for ICA

  Independent Component Analysis (ICA) is the problem of learning a squarematrix $A$, given samples of $X=AS$, where $S$ is a random vector withindependent coordinates. Most existing algorithms are provably efficient onlywhen each $S_i$ has finite and moderately valued fourth moment. However, thereare practical applications where this assumption need not be true, such asspeech and finance. Algorithms have been proposed for heavy-tailed ICA, butthey are not practical, using random walks and the full power of the ellipsoidalgorithm multiple times. The main contributions of this paper are:  (1) A practical algorithm for heavy-tailed ICA that we call HTICA. We providetheoretical guarantees and show that it outperforms other algorithms in someheavy-tailed regimes, both on real and synthetic data. Like the currentstate-of-the-art, the new algorithm is based on the centroid body (a firstmoment analogue of the covariance matrix). Unlike the state-of-the-art, ouralgorithm is practically efficient. To achieve this, we use explicit analyticrepresentations of the centroid body, which bypasses the use of the ellipsoidmethod and random walks.  (2) We study how heavy tails affect different ICA algorithms, includingHTICA. Somewhat surprisingly, we show that some algorithms that use thecovariance matrix or higher moments can successfully solve a range of ICAinstances with infinite second moment. We study this theoretically andexperimentally, with both synthetic and real-world heavy-tailed data.

Pilot-Aided Joint-Channel Carrier-Phase Estimation in Space-Division  Multiplexed Multicore Fiber Transmission

  The performance of pilot-aided joint-channel carrier-phase estimation (CPE)in space-division multiplexed multicore fiber (MCF) transmission withcorrelated phase noise is studied. To that end, a system model describinguncoded MCF transmission where the phase noise comprises a common laser phasenoise, in addition to core- and polarization-specific phase drifts, isintroduced. It is then shown that the system model can be regarded as a specialcase of a multidimensional random-walk phase-noise model. A pilot-aided CPEalgorithm that was previously developed for this model is used to evaluate twostrategies, namely joint-channel and per-channel CPE. To quantify theperformance differences between the two strategies, their phase-noisetolerances are assessed through Monte Carlo simulations of uncoded transmissionfor different modulation formats, pilot overheads, laser linewidths, numbers ofspatial channels, and degrees of phase-noise correlation across the channels.For 20 GBd transmission with 200 kHz combined laser linewidth and 1% pilotoverhead, joint-channel CPE yields up to 3.1 dB increase in power efficiency or25.5% increase in spectral efficiency. Moreover, through MCF transmissionexperiments, the system model is validated and the strategies are compared interms of bit error rate performance versus transmission distance for uncodedtransmission of different modulation formats. Up to 22.8% increase intransmission reach is observed for 1% pilot overhead through the use ofjoint-channel CPE.

The More, the Merrier: the Blessing of Dimensionality for Learning Large  Gaussian Mixtures

  In this paper we show that very large mixtures of Gaussians are efficientlylearnable in high dimension. More precisely, we prove that a mixture with knownidentical covariance matrices whose number of components is a polynomial of anyfixed degree in the dimension n is polynomially learnable as long as a certainnon-degeneracy condition on the means is satisfied. It turns out that thiscondition is generic in the sense of smoothed complexity, as soon as thedimensionality of the space is high enough. Moreover, we prove that no suchcondition can possibly exist in low dimension and the problem of learning theparameters is generically hard. In contrast, much of the existing work onGaussian Mixtures relies on low-dimensional projections and thus hits anartificial barrier. Our main result on mixture recovery relies on a new"Poissonization"-based technique, which transforms a mixture of Gaussians to alinear map of a product distribution. The problem of learning this map can beefficiently solved using some recent results on tensor decompositions andIndependent Component Analysis (ICA), thus giving an algorithm for recoveringthe mixture. In addition, we combine our low-dimensional hardness results forGaussian mixtures with Poissonization to show how to embed difficult instancesof low-dimensional Gaussian mixtures into the ICA setting, thus establishingexponential information-theoretic lower bounds for underdetermined ICA in lowdimension. To the best of our knowledge, this is the first such result in theliterature. In addition to contributing to the problem of Gaussian mixturelearning, we believe that this work is among the first steps toward betterunderstanding the rare phenomenon of the "blessing of dimensionality" in thecomputational aspects of statistical inference.

