HotFlip: White-Box Adversarial Examples for Text Classification

  We propose an efficient method to generate white-box adversarial examples to
trick a character-level neural classifier. We find that only a few
manipulations are needed to greatly decrease the accuracy. Our method relies on
an atomic flip operation, which swaps one token for another, based on the
gradients of the one-hot input vectors. Due to efficiency of our method, we can
perform adversarial training which makes the model more robust to attacks at
test time. With the use of a few semantics-preserving constraints, we
demonstrate that HotFlip can be adapted to attack a word-level classifier as
well.


Preserving Differential Privacy in Adversarial Learning with Provable
  Robustness

  In this paper, we aim to develop a novel mechanism to preserve differential
privacy (DP) in adversarial learning for deep neural networks, with provable
robustness to adversarial examples. We leverage the sequential composition
theory in differential privacy, to establish a new connection between
differential privacy preservation and provable robustness. To address the
trade-off among model utility, privacy loss, and robustness, we design an
original, differentially private, adversarial objective function, based on the
post-processing property in differential privacy, to tighten the sensitivity of
our model. Theoretical analysis and thorough evaluations show that our
mechanism notably improves the robustness of DP deep neural networks.


Ontology Matching with Knowledge Rules

  Ontology matching is the process of automatically determining the semantic
equivalences between the concepts of two ontologies. Most ontology matching
algorithms are based on two types of strategies: terminology-based strategies,
which align concepts based on their names or descriptions, and structure-based
strategies, which exploit concept hierarchies to find the alignment. In many
domains, there is additional information about the relationships of concepts
represented in various ways, such as Bayesian networks, decision trees, and
association rules. We propose to use the similarities between these
relationships to find more accurate alignments. We accomplish this by defining
soft constraints that prefer alignments where corresponding concepts have the
same local relationships encoded as knowledge rules. We use a probabilistic
framework to integrate this new knowledge-based strategy with standard
terminology-based and structure-based strategies. Furthermore, our method is
particularly effective in identifying correspondences between complex concepts.
Our method achieves substantially better F-score than the previous
state-of-the-art on three ontology matching domains.


A Probabilistic Approach to Knowledge Translation

  In this paper, we focus on a novel knowledge reuse scenario where the
knowledge in the source schema needs to be translated to a semantically
heterogeneous target schema. We refer to this task as "knowledge translation"
(KT). Unlike data translation and transfer learning, KT does not require any
data from the source or target schema. We adopt a probabilistic approach to KT
by representing the knowledge in the source schema, the mapping between the
source and target schemas, and the resulting knowledge in the target schema all
as probability distributions, specially using Markov random fields and Markov
logic networks. Given the source knowledge and mappings, we use standard
learning and inference algorithms for probabilistic graphical models to find an
explicit probability distribution in the target schema that minimizes the
Kullback-Leibler divergence from the implicit distribution. This gives us a
compact probabilistic model that represents knowledge from the source schema as
well as possible, respecting the uncertainty in both the source knowledge and
the mapping. In experiments on both propositional and relational domains, we
find that the knowledge obtained by KT is comparable to other approaches that
require data, demonstrating that knowledge can be reused without data.


Adaptive Laplace Mechanism: Differential Privacy Preservation in Deep
  Learning

  In this paper, we focus on developing a novel mechanism to preserve
differential privacy in deep neural networks, such that: (1) The privacy budget
consumption is totally independent of the number of training steps; (2) It has
the ability to adaptively inject noise into features based on the contribution
of each to the output; and (3) It could be applied in a variety of different
deep neural networks. To achieve this, we figure out a way to perturb affine
transformations of neurons, and loss functions used in deep neural networks. In
addition, our mechanism intentionally adds "more noise" into features which are
"less relevant" to the model output, and vice-versa. Our theoretical analysis
further derives the sensitivities and error bounds of our mechanism. Rigorous
experiments conducted on MNIST and CIFAR-10 datasets show that our mechanism is
highly effective and outperforms existing solutions.


On Adversarial Examples for Character-Level Neural Machine Translation

  Evaluating on adversarial examples has become a standard procedure to measure
robustness of deep learning models. Due to the difficulty of creating white-box
adversarial examples for discrete text input, most analyses of the robustness
of NLP models have been done through black-box adversarial examples. We
investigate adversarial examples for character-level neural machine translation
(NMT), and contrast black-box adversaries with a novel white-box adversary,
which employs differentiable string-edit operations to rank adversarial
changes. We propose two novel types of attacks which aim to remove or change a
word in a translation, rather than simply break the NMT. We demonstrate that
white-box adversarial examples are significantly stronger than their black-box
counterparts in different attack scenarios, which show more serious
vulnerabilities than previously known. In addition, after performing
adversarial training, which takes only 3 times longer than regular training, we
can improve the model's robustness significantly.


An Ensemble Deep Learning Model for Drug Abuse Detection in Sparse
  Twitter-Sphere

  As the problem of drug abuse intensifies in the U.S., many studies that
primarily utilize social media data, such as postings on Twitter, to study drug
abuse-related activities use machine learning as a powerful tool for text
classification and filtering. However, given the wide range of topics of
Twitter users, tweets related to drug abuse are rare in most of the datasets.
This imbalanced data remains a major issue in building effective tweet
classifiers, and is especially obvious for studies that include abuse-related
slang terms. In this study, we approach this problem by designing an ensemble
deep learning model that leverages both word-level and character-level features
to classify abuse-related tweets. Experiments are reported on a Twitter
dataset, where we can configure the percentages of the two classes (abuse vs.
non abuse) to simulate the data imbalance with different amplitudes. Results
show that our ensemble deep learning models exhibit better performance than
ensembles of traditional machine learning models, especially on heavily
imbalanced datasets.


Preserving Differential Privacy in Convolutional Deep Belief Networks

  The remarkable development of deep learning in medicine and healthcare domain
presents obvious privacy issues, when deep neural networks are built on users'
personal and highly sensitive data, e.g., clinical records, user profiles,
biomedical images, etc. However, only a few scientific studies on preserving
privacy in deep learning have been conducted. In this paper, we focus on
developing a private convolutional deep belief network (pCDBN), which
essentially is a convolutional deep belief network (CDBN) under differential
privacy. Our main idea of enforcing epsilon-differential privacy is to leverage
the functional mechanism to perturb the energy-based objective functions of
traditional CDBNs, rather than their results. One key contribution of this work
is that we propose the use of Chebyshev expansion to derive the approximate
polynomial representation of objective functions. Our theoretical analysis
shows that we can further derive the sensitivity and error bounds of the
approximate polynomial representation. As a result, preserving differential
privacy in CDBNs is feasible. We applied our model in a health social network,
i.e., YesiWell data, and in a handwriting digit dataset, i.e., MNIST data, for
human behavior prediction, human behavior classification, and handwriting digit
recognition tasks. Theoretical analysis and rigorous experimental evaluations
show that the pCDBN is highly effective. It significantly outperforms existing
solutions.


Logic Rules Powered Knowledge Graph Embedding

  Large scale knowledge graph embedding has attracted much attention from both
academia and industry in the field of Artificial Intelligence. However, most
existing methods concentrate solely on fact triples contained in the given
knowledge graph. Inspired by the fact that logic rules can provide a flexible
and declarative language for expressing rich background knowledge, it is
natural to integrate logic rules into knowledge graph embedding, to transfer
human knowledge to entity and relation embedding, and strengthen the learning
process. In this paper, we propose a novel logic rule-enhanced method which can
be easily integrated with any translation based knowledge graph embedding
model, such as TransE . We first introduce a method to automatically mine the
logic rules and corresponding confidences from the triples. And then, to put
both triples and mined logic rules within the same semantic space, all triples
in the knowledge graph are represented as first-order logic. Finally, we define
several operations on the first-order logic and minimize a global loss over
both of the mined logic rules and the transformed first-order logics. We
conduct extensive experiments for link prediction and triple classification on
three datasets: WN18, FB166, and FB15K. Experiments show that the rule-enhanced
method can significantly improve the performance of several baselines. The
highlight of our model is that the filtered Hits@1, which is a pivotal
evaluation in the knowledge inference task, has a significant improvement (up
to 700% improvement).


