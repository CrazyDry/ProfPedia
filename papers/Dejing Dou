HotFlip: White-Box Adversarial Examples for Text Classification

  We propose an efficient method to generate white-box adversarial examples totrick a character-level neural classifier. We find that only a fewmanipulations are needed to greatly decrease the accuracy. Our method relies onan atomic flip operation, which swaps one token for another, based on thegradients of the one-hot input vectors. Due to efficiency of our method, we canperform adversarial training which makes the model more robust to attacks attest time. With the use of a few semantics-preserving constraints, wedemonstrate that HotFlip can be adapted to attack a word-level classifier aswell.

Preserving Differential Privacy in Adversarial Learning with Provable  Robustness

  In this paper, we aim to develop a novel mechanism to preserve differentialprivacy (DP) in adversarial learning for deep neural networks, with provablerobustness to adversarial examples. We leverage the sequential compositiontheory in differential privacy, to establish a new connection betweendifferential privacy preservation and provable robustness. To address thetrade-off among model utility, privacy loss, and robustness, we design anoriginal, differentially private, adversarial objective function, based on thepost-processing property in differential privacy, to tighten the sensitivity ofour model. Theoretical analysis and thorough evaluations show that ourmechanism notably improves the robustness of DP deep neural networks.

Ontology Matching with Knowledge Rules

  Ontology matching is the process of automatically determining the semanticequivalences between the concepts of two ontologies. Most ontology matchingalgorithms are based on two types of strategies: terminology-based strategies,which align concepts based on their names or descriptions, and structure-basedstrategies, which exploit concept hierarchies to find the alignment. In manydomains, there is additional information about the relationships of conceptsrepresented in various ways, such as Bayesian networks, decision trees, andassociation rules. We propose to use the similarities between theserelationships to find more accurate alignments. We accomplish this by definingsoft constraints that prefer alignments where corresponding concepts have thesame local relationships encoded as knowledge rules. We use a probabilisticframework to integrate this new knowledge-based strategy with standardterminology-based and structure-based strategies. Furthermore, our method isparticularly effective in identifying correspondences between complex concepts.Our method achieves substantially better F-score than the previousstate-of-the-art on three ontology matching domains.

A Probabilistic Approach to Knowledge Translation

  In this paper, we focus on a novel knowledge reuse scenario where theknowledge in the source schema needs to be translated to a semanticallyheterogeneous target schema. We refer to this task as "knowledge translation"(KT). Unlike data translation and transfer learning, KT does not require anydata from the source or target schema. We adopt a probabilistic approach to KTby representing the knowledge in the source schema, the mapping between thesource and target schemas, and the resulting knowledge in the target schema allas probability distributions, specially using Markov random fields and Markovlogic networks. Given the source knowledge and mappings, we use standardlearning and inference algorithms for probabilistic graphical models to find anexplicit probability distribution in the target schema that minimizes theKullback-Leibler divergence from the implicit distribution. This gives us acompact probabilistic model that represents knowledge from the source schema aswell as possible, respecting the uncertainty in both the source knowledge andthe mapping. In experiments on both propositional and relational domains, wefind that the knowledge obtained by KT is comparable to other approaches thatrequire data, demonstrating that knowledge can be reused without data.

Adaptive Laplace Mechanism: Differential Privacy Preservation in Deep  Learning

  In this paper, we focus on developing a novel mechanism to preservedifferential privacy in deep neural networks, such that: (1) The privacy budgetconsumption is totally independent of the number of training steps; (2) It hasthe ability to adaptively inject noise into features based on the contributionof each to the output; and (3) It could be applied in a variety of differentdeep neural networks. To achieve this, we figure out a way to perturb affinetransformations of neurons, and loss functions used in deep neural networks. Inaddition, our mechanism intentionally adds "more noise" into features which are"less relevant" to the model output, and vice-versa. Our theoretical analysisfurther derives the sensitivities and error bounds of our mechanism. Rigorousexperiments conducted on MNIST and CIFAR-10 datasets show that our mechanism ishighly effective and outperforms existing solutions.

On Adversarial Examples for Character-Level Neural Machine Translation

  Evaluating on adversarial examples has become a standard procedure to measurerobustness of deep learning models. Due to the difficulty of creating white-boxadversarial examples for discrete text input, most analyses of the robustnessof NLP models have been done through black-box adversarial examples. Weinvestigate adversarial examples for character-level neural machine translation(NMT), and contrast black-box adversaries with a novel white-box adversary,which employs differentiable string-edit operations to rank adversarialchanges. We propose two novel types of attacks which aim to remove or change aword in a translation, rather than simply break the NMT. We demonstrate thatwhite-box adversarial examples are significantly stronger than their black-boxcounterparts in different attack scenarios, which show more seriousvulnerabilities than previously known. In addition, after performingadversarial training, which takes only 3 times longer than regular training, wecan improve the model's robustness significantly.

An Ensemble Deep Learning Model for Drug Abuse Detection in Sparse  Twitter-Sphere

  As the problem of drug abuse intensifies in the U.S., many studies thatprimarily utilize social media data, such as postings on Twitter, to study drugabuse-related activities use machine learning as a powerful tool for textclassification and filtering. However, given the wide range of topics ofTwitter users, tweets related to drug abuse are rare in most of the datasets.This imbalanced data remains a major issue in building effective tweetclassifiers, and is especially obvious for studies that include abuse-relatedslang terms. In this study, we approach this problem by designing an ensembledeep learning model that leverages both word-level and character-level featuresto classify abuse-related tweets. Experiments are reported on a Twitterdataset, where we can configure the percentages of the two classes (abuse vs.non abuse) to simulate the data imbalance with different amplitudes. Resultsshow that our ensemble deep learning models exhibit better performance thanensembles of traditional machine learning models, especially on heavilyimbalanced datasets.

Preserving Differential Privacy in Convolutional Deep Belief Networks

  The remarkable development of deep learning in medicine and healthcare domainpresents obvious privacy issues, when deep neural networks are built on users'personal and highly sensitive data, e.g., clinical records, user profiles,biomedical images, etc. However, only a few scientific studies on preservingprivacy in deep learning have been conducted. In this paper, we focus ondeveloping a private convolutional deep belief network (pCDBN), whichessentially is a convolutional deep belief network (CDBN) under differentialprivacy. Our main idea of enforcing epsilon-differential privacy is to leveragethe functional mechanism to perturb the energy-based objective functions oftraditional CDBNs, rather than their results. One key contribution of this workis that we propose the use of Chebyshev expansion to derive the approximatepolynomial representation of objective functions. Our theoretical analysisshows that we can further derive the sensitivity and error bounds of theapproximate polynomial representation. As a result, preserving differentialprivacy in CDBNs is feasible. We applied our model in a health social network,i.e., YesiWell data, and in a handwriting digit dataset, i.e., MNIST data, forhuman behavior prediction, human behavior classification, and handwriting digitrecognition tasks. Theoretical analysis and rigorous experimental evaluationsshow that the pCDBN is highly effective. It significantly outperforms existingsolutions.

Logic Rules Powered Knowledge Graph Embedding

  Large scale knowledge graph embedding has attracted much attention from bothacademia and industry in the field of Artificial Intelligence. However, mostexisting methods concentrate solely on fact triples contained in the givenknowledge graph. Inspired by the fact that logic rules can provide a flexibleand declarative language for expressing rich background knowledge, it isnatural to integrate logic rules into knowledge graph embedding, to transferhuman knowledge to entity and relation embedding, and strengthen the learningprocess. In this paper, we propose a novel logic rule-enhanced method which canbe easily integrated with any translation based knowledge graph embeddingmodel, such as TransE . We first introduce a method to automatically mine thelogic rules and corresponding confidences from the triples. And then, to putboth triples and mined logic rules within the same semantic space, all triplesin the knowledge graph are represented as first-order logic. Finally, we defineseveral operations on the first-order logic and minimize a global loss overboth of the mined logic rules and the transformed first-order logics. Weconduct extensive experiments for link prediction and triple classification onthree datasets: WN18, FB166, and FB15K. Experiments show that the rule-enhancedmethod can significantly improve the performance of several baselines. Thehighlight of our model is that the filtered Hits@1, which is a pivotalevaluation in the knowledge inference task, has a significant improvement (upto 700% improvement).

