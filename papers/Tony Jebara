Bethe Bounds and Approximating the Global Optimum

  Inference in general Markov random fields (MRFs) is NP-hard, thoughidentifying the maximum a posteriori (MAP) configuration of pairwise MRFs withsubmodular cost functions is efficiently solvable using graph cuts. Marginalinference, however, even for this restricted class, is in #P. We prove newformulations of derivatives of the Bethe free energy, provide bounds on thederivatives and bracket the locations of stationary points, introducing a newtechnique called Bethe bound propagation. Several results apply to pairwisemodels whether associative or not. Applying these to discretizedpseudo-marginals in the associative case we present a polynomial timeapproximation scheme for global optimization provided the maximum degree is$O(\log n)$, and discuss several extensions.

A refinement of Bennett's inequality with applications to portfolio  optimization

  A refinement of Bennett's inequality is introduced which is strictly tighterthan the classical bound. The new bound establishes the convergence of theaverage of independent random variables to its expected value. It alsocarefully exploits information about the potentially heterogeneous mean,variance, and ceiling of each random variable. The bound is strictly sharper inthe homogeneous setting and very often significantly sharper in theheterogeneous setting. The improved convergence rates are obtained byleveraging Lambert's W function. We apply the new bound in a portfoliooptimization setting to allocate a budget across investments with heterogeneousreturns.

Approximating the Permanent with Belief Propagation

  This work describes a method of approximating matrix permanents efficientlyusing belief propagation. We formulate a probability distribution whosepartition function is exactly the permanent, then use Bethe free energy toapproximate this partition function. After deriving some speedups to standardbelief propagation, the resulting algorithm requires $(n^2)$ time periteration. Finally, we demonstrate the advantages of using this approximation.

Proceedings of the 2017 ICML Workshop on Human Interpretability in  Machine Learning (WHI 2017)

  This is the Proceedings of the 2017 ICML Workshop on Human Interpretabilityin Machine Learning (WHI 2017), which was held in Sydney, Australia, August 10,2017. Invited speakers were Tony Jebara, Pang Wei Koh, and David Sontag.

MAP Estimation, Message Passing, and Perfect Graphs

  Efficiently finding the maximum a posteriori (MAP) configuration of agraphical model is an important problem which is often implemented usingmessage passing algorithms. The optimality of such algorithms is only wellestablished for singly-connected graphs and other limited settings. Thisarticle extends the set of graphs where MAP estimation is in P and wheremessage passing recovers the exact solution to so-called perfect graphs. Thisresult leverages recent progress in defining perfect graphs (the strong perfectgraph theorem), linear programming relaxations of MAP estimation and recentconvergent message passing schemes. The article converts graphical models intonand Markov random fields which are straightforward to relax into linearprograms. Therein, integrality can be established in general by testing forgraph perfection. This perfection test is performed efficiently using apolynomial time algorithm. Alternatively, known decomposition tools fromperfect graph theory may be used to prove perfection for certain families ofgraphs. Thus, a general graph framework is provided for determining when MAPestimation in any graphical model is in P, has an integral linear programmingrelaxation and is exactly recoverable by message passing.

Bayesian Out-Trees

  A Bayesian treatment of latent directed graph structure for non-iid data isprovided where each child datum is sampled with a directed conditionaldependence on a single unknown parent datum. The latent graph structure isassumed to lie in the family of directed out-tree graphs which leads toefficient Bayesian inference. The latent likelihood of the data and itsgradients are computable in closed form via Tutte's directed matrix treetheorem using determinants and inverses of the out-Laplacian. This novellikelihood subsumes iid likelihood, is exchangeable and yields efficientunsupervised and semi-supervised learning algorithms. In addition to handlingtaxonomy and phylogenetic datasets the out-tree assumption performssurprisingly well as a semi-parametric density estimator on standard iiddatasets. Experiments with unsupervised and semisupervised learning are shownon various UCI and taxonomy datasets.

Feature Selection and Dualities in Maximum Entropy Discrimination

  Incorporating feature selection into a classification or regression methodoften carries a number of advantages. In this paper we formalize featureselection specifically from a discriminative perspective of improvingclassification/regression accuracy. The feature selection method is developedas an extension to the recently proposed maximum entropy discrimination (MED)framework. We describe MED as a flexible (Bayesian) regularization approachthat subsumes, e.g., support vector classification, regression and exponentialfamily models. For brevity, we restrict ourselves primarily to featureselection in the context of linear classification/regression methods anddemonstrate that the proposed approach indeed carries substantial improvementsin practice. Moreover, we discuss and develop various extensions of featureselection, including the problem of dealing with example specific butunobserved degrees of freedom -- alignments or invariants.

Thompson Sampling for Noncompliant Bandits

  Thompson sampling, a Bayesian method for balancing exploration andexploitation in bandit problems, has theoretical guarantees and exhibits strongempirical performance in many domains. Traditional Thompson sampling, however,assumes perfect compliance, where an agent's chosen action is treated as theimplemented action. This article introduces a stochastic noncompliance modelthat relaxes this assumption. We prove that any noncompliance in a 2-armedBernoulli bandit increases existing regret bounds. With our noncompliancemodel, we derive Thompson sampling variants that explicitly handle bothobserved and latent noncompliance. With extensive empirical analysis, wedemonstrate that our algorithms either match or outperform traditional Thompsonsampling in both compliant and noncompliant environments.

Dynamical Systems Trees

  We propose dynamical systems trees (DSTs) as a flexible class of models fordescribing multiple processes that interact via a hierarchy of aggregatingparent chains. DSTs extend Kalman filters, hidden Markov models and nonlineardynamical systems to an interactive group scenario. Various individualprocesses interact as communities and sub-communities in a tree structure thatis unrolled in time. To accommodate nonlinear temporal activity, eachindividual leaf process is modeled as a dynamical system containing discreteand/or continuous hidden states with discrete and/or Gaussian emissions.Subsequent higher level parent processes act like hidden Markov models andmediate the interaction between leaf processes or between other parentprocesses in the hierarchy. Aggregator chains are parents of child processesthat they combine and mediate, yielding a compact overall parameterization. Weprovide tractable inference and learning algorithms for arbitrary DSTtopologies via an efficient structured mean-field algorithm. The diverseapplicability of DSTs is demonstrated by experiments on gene expression dataand by modeling group behavior in the setting of an American football game.

$\propto$SVM for learning with label proportions

  We study the problem of learning with label proportions in which the trainingdata is provided in groups and only the proportion of each class in each groupis known. We propose a new method called proportion-SVM, or $\propto$SVM, whichexplicitly models the latent unknown instance labels together with the knowngroup label proportions in a large-margin framework. Unlike the existing works,our approach avoids making restrictive assumptions about the data. The$\propto$SVM model leads to a non-convex integer programming problem. In orderto solve it efficiently, we propose two algorithms: one based on simplealternating optimization and the other based on a convex relaxation. Extensiveexperiments on standard datasets show that $\propto$SVM outperforms thestate-of-the-art, especially for larger group sizes.

Semistochastic Quadratic Bound Methods

  Partition functions arise in a variety of settings, including conditionalrandom fields, logistic regression, and latent gaussian models. In this paper,we consider semistochastic quadratic bound (SQB) methods for maximum likelihoodinference based on partition function optimization. Batch methods based on thequadratic bound were recently proposed for this class of problems, andperformed favorably in comparison to state-of-the-art techniques.Semistochastic methods fall in between batch algorithms, which use all thedata, and stochastic gradient type methods, which use small random selectionsat each iteration. We build semistochastic quadratic bound-based methods, andprove both global convergence (to a stationary point) under very weakassumptions, and linear convergence rate under stronger assumptions on theobjective. To make the proposed methods faster and more stable, we considerinexact subproblem minimization and batch-size selection schemes. The efficacyof SQB methods is demonstrated via comparison with several state-of-the-arttechniques on commonly used datasets.

Stochastic Bound Majorization

  Recently a majorization method for optimizing partition functions oflog-linear models was proposed alongside a novel quadratic variationalupper-bound. In the batch setting, it outperformed state-of-the-art first- andsecond-order optimization methods on various learning tasks. We propose astochastic version of this bound majorization method as well as a low-rankmodification for high-dimensional data-sets. The resulting stochasticsecond-order method outperforms stochastic gradient descent (across variationsand various tunings) both in terms of the number of iterations and computationtime till convergence while finding a better quality parameter setting. Theproposed method bridges first- and second-order stochastic optimization methodsby maintaining a computational complexity that is linear in the data dimensionand while exploiting second order information about the pseudo-global curvatureof the objective function (as opposed to the local curvature in the Hessian).

On MAP Inference by MWSS on Perfect Graphs

  Finding the most likely (MAP) configuration of a Markov random field (MRF) isNP-hard in general. A promising, recent technique is to reduce the problem tofinding a maximum weight stable set (MWSS) on a derived weighted graph, whichif perfect, allows inference in polynomial time. We derive new results for thisapproach, including a general decomposition theorem for MRFs of any order andnumber of labels, extensions of results for binary pairwise models withsubmodular cost functions to higher order, and an exact characterization ofwhich binary pairwise MRFs can be efficiently solved with this method. Thisdefines the power of the approach on this class of models, improves our toolboxand expands the range of tractable models.

Approximating the Bethe partition function

  When belief propagation (BP) converges, it does so to a stationary point ofthe Bethe free energy $F$, and is often strikingly accurate. However, it mayconverge only to a local optimum or may not converge at all. An algorithm wasrecently introduced for attractive binary pairwise MRFs which is guaranteed toreturn an $\epsilon$-approximation to the global minimum of $F$ in polynomialtime provided the maximum degree $\Delta=O(\log n)$, where $n$ is the number ofvariables. Here we significantly improve this algorithm and derive severalresults including a new approach based on analyzing first derivatives of $F$,which leads to performance that is typically far superior and yields a fullypolynomial-time approximation scheme (FPTAS) for attractive models without anydegree restriction. Further, the method applies to general (non-attractive)models, though with no polynomial time guarantee in this case, leading to theimportant result that approximating $\log$ of the Bethe partition function,$\log Z_B=-\min F$, for a general model to additive $\epsilon$-accuracy may bereduced to a discrete MAP inference problem. We explore an application topredicting equipment failure on an urban power network and demonstrate that theBethe approximation can perform well even when BP fails to converge.

Bethe Learning of Conditional Random Fields via MAP Decoding

  Many machine learning tasks can be formulated in terms of predictingstructured outputs. In frameworks such as the structured support vector machine(SVM-Struct) and the structured perceptron, discriminative functions arelearned by iteratively applying efficient maximum a posteriori (MAP) decoding.However, maximum likelihood estimation (MLE) of probabilistic models over thesesame structured spaces requires computing partition functions, which isgenerally intractable. This paper presents a method for learning discreteexponential family models using the Bethe approximation to the MLE. Remarkably,this problem also reduces to iterative (MAP) decoding. This connection emergesby combining the Bethe approximation with a Frank-Wolfe (FW) algorithm on aconvex dual objective which circumvents the intractable partition function. Theresult is a new single loop algorithm MLE-Struct, which is substantially moreefficient than previous double-loop methods for approximate maximum likelihoodestimation. Our algorithm outperforms existing methods in experiments involvingimage segmentation, matching problems from vision, and a new dataset ofuniversity roommate assignments.

Frank-Wolfe Algorithms for Saddle Point Problems

  We extend the Frank-Wolfe (FW) optimization algorithm to solve constrainedsmooth convex-concave saddle point (SP) problems. Remarkably, the method onlyrequires access to linear minimization oracles. Leveraging recent advances inFW optimization, we provide the first proof of convergence of a FW-type saddlepoint solver over polytopes, thereby partially answering a 30 year-oldconjecture. We also survey other convergence results and highlight gaps in thetheoretical underpinnings of FW-style algorithms. Motivating applicationswithout known efficient alternatives are explored through structured predictionwith combinatorial penalties as well as games over matching polytopes involvingan exponential number of constraints.

Initialization and Coordinate Optimization for Multi-way Matching

  We consider the problem of consistently matching multiple sets of elements toeach other, which is a common task in fields such as computer vision. To solvethe underlying NP-hard objective, existing methods often relax or approximateit, but end up with unsatisfying empirical performance due to a misalignedobjective. We propose a coordinate update algorithm that directly optimizes thetarget objective. By using pairwise alignment information to build anundirected graph and initializing the permutation matrices along the edges ofits Maximum Spanning Tree, our algorithm successfully avoids bad local optima.Theoretically, with high probability our algorithm guarantees an optimalsolution under reasonable noise assumptions. Empirically, our algorithmconsistently and significantly outperforms existing methods on severalbenchmark tasks on real datasets.

Variational Autoencoders for Collaborative Filtering

  We extend variational autoencoders (VAEs) to collaborative filtering forimplicit feedback. This non-linear probabilistic model enables us to go beyondthe limited modeling capacity of linear factor models which still largelydominate collaborative filtering research.We introduce a generative model withmultinomial likelihood and use Bayesian inference for parameter estimation.Despite widespread use in language modeling and economics, the multinomiallikelihood receives less attention in the recommender systems literature. Weintroduce a different regularization parameter for the learning objective,which proves to be crucial for achieving competitive performance. Remarkably,there is an efficient way to tune the parameter using annealing. The resultingmodel and learning algorithm has information-theoretic connections to maximumentropy discrimination and the information bottleneck principle. Empirically,we show that the proposed approach significantly outperforms severalstate-of-the-art baselines, including two recently-proposed neural networkapproaches, on several real-world datasets. We also provide extendedexperiments comparing the multinomial likelihood with other commonly usedlikelihood functions in the latent factor collaborative filtering literatureand show favorable results. Finally, we identify the pros and cons of employinga principled Bayesian inference approach and characterize settings where itprovides the most significant improvements.

Subgoal Discovery for Hierarchical Dialogue Policy Learning

  Developing agents to engage in complex goal-oriented dialogues is challengingpartly because the main learning signals are very sparse in long conversations.In this paper, we propose a divide-and-conquer approach that discovers andexploits the hidden structure of the task to enable efficient policy learning.First, given successful example dialogues, we propose the Subgoal DiscoveryNetwork (SDN) to divide a complex goal-oriented task into a set of simplersubgoals in an unsupervised fashion. We then use these subgoals to learn amulti-level policy by hierarchical reinforcement learning. We demonstrate ourmethod by building a dialogue agent for the composite task of travel planning.Experiments with simulated and real users show that our approach performscompetitively against a state-of-the-art method that requires human-definedsubgoals. Moreover, we show that the learned subgoals are often humancomprehensible.

Item Recommendation with Variational Autoencoders and Heterogenous  Priors

  In recent years, Variational Autoencoders (VAEs) have been shown to be highlyeffective in both standard collaborative filtering applications and extensionssuch as incorporation of implicit feedback. We extend VAEs to collaborativefiltering with side information, for instance when ratings are combined withexplicit text feedback from the user. Instead of using a user-agnostic standardGaussian prior, we incorporate user-dependent priors in the latent VAE space toencode users' preferences as functions of the review text. Taking into accountboth the rating and the text information to represent users in this multimodallatent space is promising to improve recommendation quality. Our proposed modelis shown to outperform the existing VAE models for collaborative filtering (upto 29.41% relative improvement in ranking metric) along with other baselinesthat incorporate both user ratings and text for item recommendation.

On Learning from Label Proportions

  Learning from Label Proportions (LLP) is a learning setting, where thetraining data is provided in groups, or "bags", and only the proportion of eachclass in each bag is known. The task is to learn a model to predict the classlabels of the individual instances. LLP has broad applications in politicalscience, marketing, healthcare, and computer vision. This work answers thefundamental question, when and why LLP is possible, by introducing a generalframework, Empirical Proportion Risk Minimization (EPRM). EPRM learns aninstance label classifier to match the given label proportions on the trainingdata. Our result is based on a two-step analysis. First, we provide a VC boundon the generalization error of the bag proportions. We show that the bag samplecomplexity is only mildly sensitive to the bag size. Second, we show that undersome mild assumptions, good bag proportion prediction guarantees good instancelabel prediction. The results together provide a formal guarantee that theindividual labels can indeed be learned in the LLP setting. We discussapplications of the analysis, including justification of LLP algorithms,learning with population proportions, and a paradigm for learning algorithmswith privacy guarantees. We also demonstrate the feasibility of LLP based on acase study in real-world setting: predicting income based on census data.

Coloring tournaments with forbidden substructures

  Coloring graphs is an important algorithmic problem in combinatorics withmany applications in computer science. In this paper we study coloringtournaments. A chromatic number of a random tournament is of order$\Omega(\frac{n}{\log(n)})$. The question arises whether the chromatic numbercan be proven to be smaller for more structured nontrivial classes oftournaments. We analyze the class of tournaments defined by a forbiddensubtournament $H$. This paper gives a first quasi-polynomial algorithm runningin time $e^{O(\log(n)^{2})}$ that constructs colorings of $H$-free tournamentsusing only $O(n^{1-\epsilon(H)}\log(n))$ colors, where $\epsilon(H) \geq2^{-2^{50|H|^{2}+1}}$ for many forbidden tournaments $H$. To the best of ourknowledge all previously known related results required at leastsub-exponential time and relied on the regularity lemma. Since we do not usethe regularity lemma, we obtain the first known lower bounds on $\epsilon(H)$that can be given by a closed-form expression. As a corollary, we give aconstructive proof of the celebrated open Erd\H{o}s-Hajnal conjecture withexplicitly given lower bounds on the EH coefficients for all classes of primetournaments for which the conjecture is known. Such a constractive proof wasnot known before. Thus we significantly reduce the gap between best lower andupper bounds on the EH coefficients from the conjecture for all known primetournaments that satisfy it. We also briefly explain how our methods may beused for coloring $H$-free tournaments under the following conditions: $H$ isany tournament with $\leq 5$ vertices or: $H$ is any but one tournament of sixvertices.

Binary embeddings with structured hashed projections

  We consider the hashing mechanism for constructing binary embeddings, thatinvolves pseudo-random projections followed by nonlinear (sign function)mappings. The pseudo-random projection is described by a matrix, where not allentries are independent random variables but instead a fixed "budget ofrandomness" is distributed across the matrix. Such matrices can be efficientlystored in sub-quadratic or even linear space, provide reduction in randomnessusage (i.e. number of required random values), and very often lead tocomputational speed ups. We prove several theoretical results showing thatprojections via various structured matrices followed by nonlinear mappingsaccurately preserve the angular distance between input high-dimensionalvectors. To the best of our knowledge, these results are the first that givetheoretical ground for the use of general structured matrices in the nonlinearsetting. In particular, they generalize previous extensions of theJohnson-Lindenstrauss lemma and prove the plausibility of the approach that wasso far only heuristically confirmed for some special structured matrices.Consequently, we show that many structured matrices can be used as an efficientinformation compression mechanism. Our findings build a better understanding ofcertain deep architectures, which contain randomly weighted and untrainedlayers, and yet achieve high performance on different learning tasks. Weempirically verify our theoretical findings and show the dependence of learningvia structured hashed projections on the performance of neural network as wellas nearest neighbor classifier.

