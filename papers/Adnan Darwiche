Proceedings of the Eighteenth Conference on Uncertainty in Artificial
  Intelligence (2002)

  This is the Proceedings of the Eighteenth Conference on Uncertainty in
Artificial Intelligence, which was held in Alberta, Canada, August 1-4 2002


Human-Level Intelligence or Animal-Like Abilities?

  The vision systems of the eagle and the snake outperform everything that we
can make in the laboratory, but snakes and eagles cannot build an eyeglass or a
telescope or a microscope. (Judea Pearl)


Objection-Based Causal Networks

  This paper introduces the notion of objection-based causal networks which
resemble probabilistic causal networks except that they are quantified using
objections. An objection is a logical sentence and denotes a condition under
which a, causal dependency does not exist. Objection-based causal networks
enjoy almost all the properties that make probabilistic causal networks
popular, with the added advantage that objections are, arguably more intuitive
than probabilities.


A Standard Approach for Optimizing Belief Network Inference using Query
  DAGs

  This paper proposes a novel, algorithm-independent approach to optimizing
belief network inference. rather than designing optimizations on an algorithm
by algorithm basis, we argue that one should use an unoptimized algorithm to
generate a Q-DAG, a compiled graphical representation of the belief network,
and then optimize the Q-DAG and its evaluator instead. We present a set of
Q-DAG optimizations that supplant optimizations designed for traditional
inference algorithms, including zero compression, network pruning and caching.
We show that our Q-DAG optimizations require time linear in the Q-DAG size, and
significantly simplify the process of designing algorithms for optimizing
belief network inference.


Conditioning Methods for Exact and Approximate Inference in Causal
  Networks

  We present two algorithms for exact and approximate inference in causal
networks. The first algorithm, dynamic conditioning, is a refinement of cutset
conditioning that has linear complexity on some networks for which cutset
conditioning is exponential. The second algorithm, B-conditioning, is an
algorithm for approximate inference that allows one to trade-off the quality of
approximations with the computation time. We also present some experimental
results illustrating the properties of the proposed algorithms.


Argument Calculus and Networks

  A major reason behind the success of probability calculus is that it
possesses a number of valuable tools, which are based on the notion of
probabilistic independence. In this paper, I identify a notion of logical
independence that makes some of these tools available to a class of
propositional databases, called argument databases. Specifically, I suggest a
graphical representation of argument databases, called argument networks, which
resemble Bayesian networks. I also suggest an algorithm for reasoning with
argument networks, which resembles a basic algorithm for reasoning with
Bayesian networks. Finally, I show that argument networks have several
applications: Nonmonotonic reasoning, truth maintenance, and diagnosis.


EDML: A Method for Learning Parameters in Bayesian Networks

  We propose a method called EDML for learning MAP parameters in binary
Bayesian networks under incomplete data. The method assumes Beta priors and can
be used to learn maximum likelihood parameters when the priors are
uninformative. EDML exhibits interesting behaviors, especially when compared to
EM. We introduce EDML, explain its origin, and study some of its properties
both analytically and empirically.


Action Networks: A Framework for Reasoning about Actions and Change
  under Uncertainty

  This work proposes action networks as a semantically well-founded framework
for reasoning about actions and change under uncertainty. Action networks add
two primitives to probabilistic causal networks: controllable variables and
persistent variables. Controllable variables allow the representation of
actions as directly setting the value of specific events in the domain, subject
to preconditions. Persistent variables provide a canonical model of persistence
according to which both the state of a variable and the causal mechanism
dictating its value persist over time unless intervened upon by an action (or
its consequences). Action networks also allow different methods for quantifying
the uncertainty in causal relationships, which go beyond traditional
probabilistic quantification. This paper describes both recent results and work
in progress.


On the Relation between Kappa Calculus and Probabilistic Reasoning

  We study the connection between kappa calculus and probabilistic reasoning in
diagnosis applications. Specifically, we abstract a probabilistic belief
network for diagnosing faults into a kappa network and compare the ordering of
faults computed using both methods. We show that, at least for the example
examined, the ordering of faults coincide as long as all the causal relations
in the original probabilistic network are taken into account. We also provide a
formal analysis of some network structures where the two methods will differ.
Both kappa rankings and infinitesimal probabilities have been used extensively
to study default reasoning and belief revision. But little has been done on
utilizing their connection as outlined above. This is partly because the
relation between kappa and probability calculi assumes that probabilities are
arbitrarily close to one (or zero). The experiments in this paper investigate
this relation when this assumption is not satisfied. The reported results have
important implications on the use of kappa rankings to enhance the knowledge
engineering of uncertainty models.


A Differential Approach to Inference in Bayesian Networks

  We present a new approach for inference in Bayesian networks, which is mainly
based on partial differentiation. According to this approach, one compiles a
Bayesian network into a multivariate polynomial and then computes the partial
derivatives of this polynomial with respect to each variable. We show that once
such derivatives are made available, one can compute in constant-time answers
to a large class of probabilistic queries, which are central to classical
inference, parameter estimation, model validation and sensitivity analysis. We
present a number of complexity results relating to the compilation of such
polynomials and to the computation of their partial derivatives. We argue that
the combined simplicity, comprehensiveness and computational complexity of the
presented framework is unique among existing frameworks for inference in
Bayesian networks.


Any-Space Probabilistic Inference

  We have recently introduced an any-space algorithm for exact inference in
Bayesian networks, called Recursive Conditioning, RC, which allows one to trade
space with time at increments of X-bytes, where X is the number of bytes needed
to cache a floating point number. In this paper, we present three key
extensions of RC. First, we modify the algorithm so it applies to more general
factorization of probability distributions, including (but not limited to)
Bayesian network factorizations. Second, we present a forgetting mechanism
which reduces the space requirements of RC considerably and then compare such
requirmenets with those of variable elimination on a number of realistic
networks, showing orders of magnitude improvements in certain cases. Third, we
present a version of RC for computing maximum a posteriori hypotheses (MAP),
which turns out to be the first MAP algorithm allowing a smooth time-space
tradeoff. A key advantage of presented MAP algorithm is that it does not have
to start from scratch each time a new query is presented, but can reuse some of
its computations across multiple queries, leading to significant savings in
ceratain cases.


Dynamic Jointrees

  It is well known that one can ignore parts of a belief network when computing
answers to certain probabilistic queries. It is also well known that the
ignorable parts (if any) depend on the specific query of interest and,
therefore, may change as the query changes. Algorithms based on jointrees,
however, do not seem to take computational advantage of these facts given that
they typically construct jointrees for worst-case queries; that is, queries for
which every part of the belief network is considered relevant. To address this
limitation, we propose in this paper a method for reconfiguring jointrees
dynamically as the query changes. The reconfiguration process aims at
maintaining a jointree which corresponds to the underlying belief network after
it has been pruned given the current query. Our reconfiguration method is
marked by three characteristics: (a) it is based on a non-classical definition
of jointrees; (b) it is relatively efficient; and (c) it can reuse some of the
computations performed before a jointree is reconfigured. We present
preliminary experimental results which demonstrate significant savings over
using static jointrees when query changes are considerable.


On Bayesian Network Approximation by Edge Deletion

  We consider the problem of deleting edges from a Bayesian network for the
purpose of simplifying models in probabilistic inference. In particular, we
propose a new method for deleting network edges, which is based on the evidence
at hand. We provide some interesting bounds on the KL-divergence between
original and approximate networks, which highlight the impact of given evidence
on the quality of approximation and shed some light on good and bad candidates
for edge deletion. We finally demonstrate empirically the promise of the
proposed edge deletion technique as a basis for approximate inference.


Exploiting Evidence in Probabilistic Inference

  We define the notion of compiling a Bayesian network with evidence and
provide a specific approach for evidence-based compilation, which makes use of
logical processing. The approach is practical and advantageous in a number of
application areas-including maximum likelihood estimation, sensitivity
analysis, and MAP computations-and we provide specific empirical results in the
domain of genetic linkage analysis. We also show that the approach is
applicable for networks that do not contain determinism, and show that it
empirically subsumes the performance of the quickscore algorithm when applied
to noisy-or networks.


Dual Decomposition from the Perspective of Relax, Compensate and then
  Recover

  Relax, Compensate and then Recover (RCR) is a paradigm for approximate
inference in probabilistic graphical models that has previously provided
theoretical and practical insights on iterative belief propagation and some of
its generalizations. In this paper, we characterize the technique of dual
decomposition in the terms of RCR, viewing it as a specific way to compensate
for relaxed equivalence constraints. Among other insights gathered from this
perspective, we propose novel heuristics for recovering relaxed equivalence
constraints with the goal of incrementally tightening dual decomposition
approximations, all the way to reaching exact solutions. We also show
empirically that recovering equivalence constraints can sometimes tighten the
corresponding approximation (and obtaining exact results), without increasing
much the complexity of inference.


Approximating the Partition Function by Deleting and then Correcting for
  Model Edges

  We propose an approach for approximating the partition function which is
based on two steps: (1) computing the partition function of a simplified model
which is obtained by deleting model edges, and (2) rectifying the result by
applying an edge-by-edge correction. The approach leads to an intuitive
framework in which one can trade-off the quality of an approximation with the
complexity of computing it. It also includes the Bethe free energy
approximation as a degenerate case. We develop the approach theoretically in
this paper and provide a number of empirical results that reveal its practical
utility.


On the Robustness of Most Probable Explanations

  In Bayesian networks, a Most Probable Explanation (MPE) is a complete
variable instantiation with a highest probability given the current evidence.
In this paper, we discuss the problem of finding robustness conditions of the
MPE under single parameter changes. Specifically, we ask the question: How much
change in a single network parameter can we afford to apply while keeping the
MPE unchanged? We will describe a procedure, which is the first of its kind,
that computes this answer for each parameter in the Bayesian network variable
in time O(n exp(w)), where n is the number of network variables and w is its
treewidth.


On the Relative Expressiveness of Bayesian and Neural Networks

  A neural network computes a function. A central property of neural networks
is that they are "universal approximators:" for a given continuous function,
there exists a neural network that can approximate it arbitrarily well, given
enough neurons (and some additional assumptions). In contrast, a Bayesian
network is a model, but each of its queries can be viewed as computing a
function. In this paper, we identify some key distinctions between the
functions computed by neural networks and those by marginal Bayesian network
queries, showing that the former are more expressive than the latter. Moreover,
we propose a simple augmentation to Bayesian networks (a testing operator),
which enables their marginal queries to become "universal approximators."


Query DAGs: A Practical Paradigm for Implementing Belief Network
  Inference

  We describe a new paradigm for implementing inference in belief networks,
which relies on compiling a belief network into an arithmetic expression called
a Query DAG (Q-DAG). Each non-leaf node of a Q-DAG represents a numeric
operation, a number, or a symbol for evidence. Each leaf node of a Q-DAG
represents the answer to a network query, that is, the probability of some
event of interest. It appears that Q-DAGs can be generated using any of the
algorithms for exact inference in belief networks --- we show how they can be
generated using clustering and conditioning algorithms. The time and space
complexity of a Q-DAG generation algorithm is no worse than the time complexity
of the inference algorithm on which it is based; that of a Q-DAG on-line
evaluation algorithm is linear in the size of the Q-DAG, and such inference
amounts to a standard evaluation of the arithmetic expression it represents.
The main value of Q-DAGs is in reducing the software and hardware resources
required to utilize belief networks in on-line, real-world applications. The
proposed framework also facilitates the development of on-line inference on
different software and hardware platforms, given the simplicity of the Q-DAG
evaluation algorithm. This paper describes this new paradigm for probabilistic
inference, explaining how it works, its uses, and outlines some of the research
directions that it leads to.


On the tractable counting of theory models and its application to belief
  revision and truth maintenance

  We introduced decomposable negation normal form (DNNF) recently as a
tractable form of propositional theories, and provided a number of powerful
logical operations that can be performed on it in polynomial time. We also
presented an algorithm for compiling any conjunctive normal form (CNF) into
DNNF and provided a structure-based guarantee on its space and time complexity.
We present in this paper a linear-time algorithm for converting an ordered
binary decision diagram (OBDD) representation of a propositional theory into an
equivalent DNNF, showing that DNNFs scale as well as OBDDs. We also identify a
subclass of DNNF which we call deterministic DNNF, d-DNNF, and show that the
previous complexity guarantees on compiling DNNF continue to hold for this
stricter subclass, which has stronger properties. In particular, we present a
new operation on d-DNNF which allows us to count its models under the
assertion, retraction and flipping of every literal by traversing the d-DNNF
twice. That is, after such traversal, we can test in constant-time: the
entailment of any literal by the d-DNNF, and the consistency of the d-DNNF
under the retraction or flipping of any literal. We demonstrate the
significance of these new operations by showing how they allow us to implement
linear-time, complete truth maintenance systems and linear-time, complete
belief revision systems for two important classes of propositional theories.


Compilation of Propositional Weighted Bases

  In this paper, we investigate the extent to which knowledge compilation can
be used to improve inference from propositional weighted bases. We present a
general notion of compilation of a weighted base that is parametrized by any
equivalence--preserving compilation function. Both negative and positive
results are presented. On the one hand, complexity results are identified,
showing that the inference problem from a compiled weighted base is as
difficult as in the general case, when the prime implicates, Horn cover or
renamable Horn cover classes are targeted. On the other hand, we show that the
inference problem becomes tractable whenever DNNF-compilations are used and
clausal queries are considered. Moreover, we show that the set of all preferred
models of a DNNF-compilation of a weighted base can be computed in time
polynomial in the output size. Finally, we sketch how our results can be used
in model-based diagnosis in order to compute the most probable diagnoses of a
system.


Lifted Relax, Compensate and then Recover: From Approximate to Exact
  Lifted Probabilistic Inference

  We propose an approach to lifted approximate inference for first-order
probabilistic models, such as Markov logic networks. It is based on performing
exact lifted inference in a simplified first-order model, which is found by
relaxing first-order constraints, and then compensating for the relaxation.
These simplified models can be incrementally improved by carefully recovering
constraints that have been relaxed, also at the first-order level. This leads
to a spectrum of approximations, with lifted belief propagation on one end, and
exact lifted inference on the other. We discuss how relaxation, compensation,
and recovery can be performed, all at the firstorder level, and show
empirically that our approach substantially improves on the approximations of
both propositional solvers and lifted belief propagation.


New Advances and Theoretical Insights into EDML

  EDML is a recently proposed algorithm for learning MAP parameters in Bayesian
networks. In this paper, we present a number of new advances and insights on
the EDML algorithm. First, we provide the multivalued extension of EDML,
originally proposed for Bayesian networks over binary variables. Next, we
identify a simplified characterization of EDML that further implies a simple
fixed-point algorithm for the convex optimization problem that underlies it.
This characterization further reveals a connection between EDML and EM: a fixed
point of EDML is a fixed point of EM, and vice versa. We thus identify also a
new characterization of EM fixed points, but in the semantics of EDML. Finally,
we propose a hybrid EDML/EM algorithm that takes advantage of the improved
empirical convergence behavior of EDML, while maintaining the monotonic
improvement property of EM.


New Advances in Inference by Recursive Conditioning

  Recursive Conditioning (RC) was introduced recently as the first any-space
algorithm for inference in Bayesian networks which can trade time for space by
varying the size of its cache at the increment needed to store a floating point
number. Under full caching, RC has an asymptotic time and space complexity
which is comparable to mainstream algorithms based on variable elimination and
clustering (exponential in the network treewidth and linear in its size). We
show two main results about RC in this paper. First, we show that its actual
space requirements under full caching are much more modest than those needed by
mainstream methods and study the implications of this finding. Second, we show
that RC can effectively deal with determinism in Bayesian networks by employing
standard logical techniques, such as unit resolution, allowing a significant
reduction in its time requirements in certain cases. We illustrate our results
using a number of benchmark networks, including the very challenging ones that
arise in genetic linkage analysis.


Reasoning about Bayesian Network Classifiers

  Bayesian network classifiers are used in many fields, and one common class of
classifiers are naive Bayes classifiers. In this paper, we introduce an
approach for reasoning about Bayesian network classifiers in which we
explicitly convert them into Ordered Decision Diagrams (ODDs), which are then
used to reason about the properties of these classifiers. Specifically, we
present an algorithm for converting any naive Bayes classifier into an ODD, and
we show theoretically and experimentally that this algorithm can give us an ODD
that is tractable in size even given an intractable number of instances. Since
ODDs are tractable representations of classifiers, our algorithm allows us to
efficiently test the equivalence of two naive Bayes classifiers and
characterize discrepancies between them. We also show a number of additional
results including a count of distinct classifiers that can be induced by
changing some CPT in a naive Bayes classifier, and the range of allowable
changes to a CPT which keeps the current classifier unchanged.


Solving MAP Exactly using Systematic Search

  MAP is the problem of finding a most probable instantiation of a set of
variables in a Bayesian network given some evidence. Unlike computing posterior
probabilities, or MPE (a special case of MAP), the time and space complexity of
structural solutions for MAP are not only exponential in the network treewidth,
but in a larger parameter known as the "constrained" treewidth. In practice,
this means that computing MAP can be orders of magnitude more expensive than
computing posterior probabilities or MPE. This paper introduces a new, simple
upper bound on the probability of a MAP solution, which admits a tradeoff
between the bound quality and the time needed to compute it. The bound is shown
to be generally much tighter than those of other methods of comparable
complexity. We use this proposed upper bound to develop a branch-and-bound
search algorithm for solving MAP exactly. Experimental results demonstrate that
the search algorithm is able to solve many problems that are far beyond the
reach of any structure-based method for MAP. For example, we show that the
proposed algorithm can compute MAP exactly and efficiently for some networks
whose constrained treewidth is more than 40.


When do Numbers Really Matter?

  Common wisdom has it that small distinctions in the probabilities quantifying
a Bayesian network do not matter much for the resultsof probabilistic queries.
However, one can easily develop realistic scenarios under which small
variations in network probabilities can lead to significant changes in computed
queries. A pending theoretical question is then to analytically characterize
parameter changes that do or do not matter. In this paper, we study the
sensitivity of probabilistic queries to changes in network parameters and prove
some tight bounds on the impact that such parameters can have on queries. Our
analytical results pinpoint some interesting situations under which parameter
changes do or do not matter. These results are important for knowledge
engineers as they help them identify influential network parameters. They are
also important for approximate inference algorithms that preprocessnetwork CPTs
to eliminate small distinctions in probabilities.


Sensitivity Analysis in Bayesian Networks: From Single to Multiple
  Parameters

  Previous work on sensitivity analysis in Bayesian networks has focused on
single parameters, where the goal is to understand the sensitivity of queries
to single parameter changes, and to identify single parameter changes that
would enforce a certain query constraint. In this paper, we expand the work to
multiple parameters which may be in the CPT of a single variable, or the CPTs
of multiple variables. Not only do we identify the solution space of multiple
parameter changes that would be needed to enforce a query constraint, but we
also show how to find the optimal solution, that is, the one which disturbs the
current probability distribution the least (with respect to a specific measure
of disturbance). We characterize the computational complexity of our new
techniques and discuss their applications to developing and debugging Bayesian
networks, and to the problem of reasoning about the value (reliability) of new
information.


Approximating MAP using Local Search

  MAP is the problem of finding a most probable instantiation of a set of
variables in a Bayesian network, given evidence. Unlike computing marginals,
posteriors, and MPE (a special case of MAP), the time and space complexity of
MAP is not only exponential in the network treewidth, but also in a larger
parameter known as the "constrained" treewidth. In practice, this means that
computing MAP can be orders of magnitude more expensive than
computingposteriors or MPE. Thus, practitioners generally avoid MAP
computations, resorting instead to approximating them by the most likely value
for each MAP variableseparately, or by MPE.We present a method for
approximating MAP using local search. This method has space complexity which is
exponential onlyin the treewidth, as is the complexity of each search step. We
investigate the effectiveness of different local searchmethods and several
initialization strategies and compare them to otherapproximation
schemes.Experimental results show that local search provides a much more
accurate approximation of MAP, while requiring few search steps.Practically,
this means that the complexity of local search is often exponential only in
treewidth as opposed to the constrained treewidth, making approximating MAP as
efficient as other computations.


On the Role of Canonicity in Bottom-up Knowledge Compilation

  We consider the problem of bottom-up compilation of knowledge bases, which is
usually predicated on the existence of a polytime function for combining
compilations using Boolean operators (usually called an Apply function). While
such a polytime Apply function is known to exist for certain languages (e.g.,
OBDDs) and not exist for others (e.g., DNNF), its existence for certain
languages remains unknown. Among the latter is the recently introduced language
of Sentential Decision Diagrams (SDDs), for which a polytime Apply function
exists for unreduced SDDs, but remains unknown for reduced ones (i.e. canonical
SDDs). We resolve this open question in this paper and consider some of its
theoretical and practical implications. Some of the findings we report question
the common wisdom on the relationship between bottom-up compilation, language
canonicity and the complexity of the Apply function.


On the Complexity and Approximation of Binary Evidence in Lifted
  Inference

  Lifted inference algorithms exploit symmetries in probabilistic models to
speed up inference. They show impressive performance when calculating
unconditional probabilities in relational models, but often resort to
non-lifted inference when computing conditional probabilities. The reason is
that conditioning on evidence breaks many of the model's symmetries, which can
preempt standard lifting techniques. Recent theoretical results show, for
example, that conditioning on evidence which corresponds to binary relations is
#P-hard, suggesting that no lifting is to be expected in the worst case. In
this paper, we balance this negative result by identifying the Boolean rank of
the evidence as a key parameter for characterizing the complexity of
conditioning in lifted inference. In particular, we show that conditioning on
binary evidence with bounded Boolean rank is efficient. This opens up the
possibility of approximating evidence by a low-rank Boolean matrix
factorization, which we investigate both theoretically and empirically.


Node Splitting: A Scheme for Generating Upper Bounds in Bayesian
  Networks

  We formulate in this paper the mini-bucket algorithm for approximate
inference in terms of exact inference on an approximate model produced by
splitting nodes in a Bayesian network. The new formulation leads to a number of
theoretical and practical implications. First, we show that branchand- bound
search algorithms that use minibucket bounds may operate in a drastically
reduced search space. Second, we show that the proposed formulation inspires
new minibucket heuristics and allows us to analyze existing heuristics from a
new perspective. Finally, we show that this new formulation allows mini-bucket
approximations to benefit from recent advances in exact inference, allowing one
to significantly increase the reach of these approximations.


A Variational Approach for Approximating Bayesian Networks by Edge
  Deletion

  We consider in this paper the formulation of approximate inference in
Bayesian networks as a problem of exact inference on an approximate network
that results from deleting edges (to reduce treewidth). We have shown in
earlier work that deleting edges calls for introducing auxiliary network
parameters to compensate for lost dependencies, and proposed intuitive
conditions for determining these parameters. We have also shown that our method
corresponds to IBP when enough edges are deleted to yield a polytree, and
corresponds to some generalizations of IBP when fewer edges are deleted. In
this paper, we propose a different criteria for determining auxiliary
parameters based on optimizing the KL-divergence between the original and
approximate networks. We discuss the relationship between the two methods for
selecting parameters, shedding new light on IBP and its generalizations. We
also discuss the application of our new method to approximating inference
problems which are exponential in constrained treewidth, including MAP and
nonmyopic value of information.


Skolemization for Weighted First-Order Model Counting

  First-order model counting emerged recently as a novel reasoning task, at the
core of efficient algorithms for probabilistic logics. We present a
Skolemization algorithm for model counting problems that eliminates existential
quantifiers from a first-order logic theory without changing its weighted model
count. For certain subsets of first-order logic, lifted model counters were
shown to run in time polynomial in the number of objects in the domain of
discourse, where propositional model counters require exponential time.
However, these guarantees apply only to Skolem normal form theories (i.e., no
existential quantifiers) as the presence of existential quantifiers reduces
lifted model counters to propositional ones. Since textbook Skolemization is
not sound for model counting, these restrictions precluded efficient model
counting for directed models, such as probabilistic logic programs, which rely
on existential quantification. Our Skolemization procedure extends the
applicability of first-order model counters to these representations. Moreover,
it simplifies the design of lifted model counting algorithms.


On Relaxing Determinism in Arithmetic Circuits

  The past decade has seen a significant interest in learning tractable
probabilistic representations. Arithmetic circuits (ACs) were among the first
proposed tractable representations, with some subsequent representations being
instances of ACs with weaker or stronger properties. In this paper, we provide
a formal basis under which variants on ACs can be compared, and where the
precise roles and semantics of their various properties can be made more
transparent. This allows us to place some recent developments on ACs in a
clearer perspective and to also derive new results for ACs. This includes an
exponential separation between ACs with and without determinism; completeness
and incompleteness results; and tractability results (or lack thereof) when
computing most probable explanations (MPEs).


On Compiling DNNFs without Determinism

  State-of-the-art knowledge compilers generate deterministic subsets of DNNF,
which have been recently shown to be exponentially less succinct than DNNF. In
this paper, we propose a new method to compile DNNFs without enforcing
determinism necessarily. Our approach is based on compiling deterministic DNNFs
with the addition of auxiliary variables to the input formula. These variables
are then existentially quantified from the deterministic structure in linear
time, which would lead to a DNNF that is equivalent to the input formula and
not necessarily deterministic. On the theoretical side, we show that the new
method could generate exponentially smaller DNNFs than deterministic ones, even
by adding a single auxiliary variable. Further, we show that various existing
techniques that introduce auxiliary variables to the input formulas can be
employed in our framework. On the practical side, we empirically demonstrate
that our new method can significantly advance DNNF compilation on certain
benchmarks.


A Symbolic Approach to Explaining Bayesian Network Classifiers

  We propose an approach for explaining Bayesian network classifiers, which is
based on compiling such classifiers into decision functions that have a
tractable and symbolic form. We introduce two types of explanations for why a
classifier may have classified an instance positively or negatively and suggest
algorithms for computing these explanations. The first type of explanation
identifies a minimal set of the currently active features that is responsible
for the current classification, while the second type of explanation identifies
a minimal set of features whose current state (active or not) is sufficient for
the classification. We consider in particular the compilation of Naive and
Latent-Tree Bayesian network classifiers into Ordered Decision Diagrams (ODDs),
providing a context for evaluating our proposal using case studies and
experiments based on classifiers from the literature.


