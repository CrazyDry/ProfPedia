Learning Factor Graphs in Polynomial Time & Sample Complexity

  We study computational and sample complexity of parameter and structurelearning in graphical models. Our main result shows that the class of factorgraphs with bounded factor size and bounded connectivity can be learned inpolynomial time and polynomial number of samples, assuming that the data isgenerated by a network in this class. This result covers both parameterestimation for a known network structure and structure learning. It implies asa corollary that we can learn factor graphs for both Bayesian networks andMarkov networks of bounded degree, in polynomial time and sample complexity.Unlike maximum likelihood estimation, our method does not require inference inthe underlying network, and so applies to networks where inference isintractable. We also show that the error of our learned model degradesgracefully when the generating distribution is not a member of the target classof networks.

A K-fold Method for Baseline Estimation in Policy Gradient Algorithms

  The high variance issue in unbiased policy-gradient methods such as VPG andREINFORCE is typically mitigated by adding a baseline. However, the baselinefitting itself suffers from the underfitting or the overfitting problem. Inthis paper, we develop a K-fold method for baseline estimation in policygradient algorithms. The parameter K is the baseline estimation hyperparameterthat can adjust the bias-variance trade-off in the baseline estimates. Wedemonstrate the usefulness of our approach via two state-of-the-art policygradient algorithms on three MuJoCo locomotive control tasks.

UCB Exploration via Q-Ensembles

  We show how an ensemble of $Q^*$-functions can be leveraged for moreeffective exploration in deep reinforcement learning. We build on wellestablished algorithms from the bandit setting, and adapt them to the$Q$-learning setting. We propose an exploration strategy based onupper-confidence bounds (UCB). Our experiments show significant gains on theAtari benchmark.

Interpretable and Pedagogical Examples

  Teachers intentionally pick the most informative examples to show theirstudents. However, if the teacher and student are neural networks, the examplesthat the teacher network learns to give, although effective at teaching thestudent, are typically uninterpretable. We show that training the student andteacher iteratively, rather than jointly, can produce interpretable teachingstrategies. We evaluate interpretability by (1) measuring the similarity of theteacher's emergent strategies to intuitive strategies in each domain and (2)conducting human experiments to evaluate how effective the teacher's strategiesare at teaching humans. We show that the teacher network learns to select orgenerate interpretable, pedagogical examples to teach rule-based,probabilistic, boolean, and hierarchical concepts.

Some Considerations on Learning to Explore via Meta-Reinforcement  Learning

  We consider the problem of exploration in meta reinforcement learning. Twonew meta reinforcement learning algorithms are suggested: E-MAML andE-$\text{RL}^2$. Results are presented on a novel environment we call `KrazyWorld' and a set of maze environments. We show E-MAML and E-$\text{RL}^2$deliver better performance on tasks where exploration is important.

Transfer Learning for Estimating Causal Effects using Neural Networks

  We develop new algorithms for estimating heterogeneous treatment effects,combining recent developments in transfer learning for neural networks withinsights from the causal inference literature. By taking advantage of transferlearning, we are able to efficiently use different data sources that arerelated to the same underlying causal mechanisms. We compare our algorithmswith those in the extant literature using extensive simulation studies based onlarge-scale voter persuasion experiments and the MNIST database. Our methodscan perform an order of magnitude better than existing benchmarks while using afraction of the data.

Toward a Science of Autonomy for Physical Systems: Paths

  An Autonomous Physical System (APS) will be expected to reliably andindependently evaluate, execute, and achieve goals while respecting surroundingrules, laws, or conventions. In doing so, an APS must rely on a broad spectrumof dynamic, complex, and often imprecise information about its surroundings,the task it is to perform, and its own sensors and actuators. For example,cleaning in a home or commercial setting requires the ability to perceive,grasp, and manipulate many physical objects, the ability to reliably perform avariety of subtasks such as washing, folding, and stacking, and knowledge aboutlocal conventions such as how objects are classified and where they should bestored. The information required for reliable autonomous operation may comefrom external sources and from the robot's own sensor observations or in theform of direct instruction by a trainer. Similar considerations apply acrossmany domains - construction, manufacturing, in-home assistance, and healthcare.For example, surgeons spend many years learning about physiology and anatomybefore they touch a patient. They then perform roughly 1000 surgeries under thetutelage of an expert surgeon, and they practice basic maneuvers such as suturetying thousands of times outside the operating room. All of these elements cometogether to achieve expertise at this task. Endowing a system with robustautonomy by traditional programming methods has thus far had limited success.Several promising new paths to acquiring and processing such data are emerging.This white paper outlines three promising research directions for enabling anAPS to learn the physical and information skills necessary to perform taskswith independence and flexibility: Deep Reinforcement Learning, Human-RobotInteraction, and Cloud Robotics.

Learning Contact-Rich Manipulation Skills with Guided Policy Search

  Autonomous learning of object manipulation skills can enable robots toacquire rich behavioral repertoires that scale to the variety of objects foundin the real world. However, current motion skill learning methods typicallyrestrict the behavior to a compact, low-dimensional representation, limitingits expressiveness and generality. In this paper, we extend a recentlydeveloped policy search method \cite{la-lnnpg-14} and use it to learn a rangeof dynamic manipulation behaviors with highly general policy representations,without using known models or example demonstrations. Our approach learns a setof trajectories for the desired motion skill by using iteratively refittedtime-varying linear models, and then unifies these trajectories into a singlecontrol policy that can generalize to new situations. To enable this method torun on a real robot, we introduce several improvements that reduce the samplecount and automate parameter selection. We show that our method can acquirefast, fluent behaviors after only minutes of interaction time, and can learnrobust controllers for complex tasks, including putting together a toyairplane, stacking tight-fitting lego blocks, placing wooden rings ontotight-fitting pegs, inserting a shoe tree into a shoe, and screwing bottle capsonto bottles.

The path inference filter: model-based low-latency map matching of probe  vehicle data

  We consider the problem of reconstructing vehicle trajectories from sparsesequences of GPS points, for which the sampling interval is between 10 secondsand 2 minutes. We introduce a new class of algorithms, called altogether pathinference filter (PIF), that maps GPS data in real time, for a variety oftrade-offs and scenarios, and with a high throughput. Numerous prior approachesin map-matching can be shown to be special cases of the path inference filterpresented in this article. We present an efficient procedure for automaticallytraining the filter on new data, with or without ground truth observations. Theframework is evaluated on a large San Francisco taxi dataset and is shown toimprove upon the current state of the art. This filter also provides insightsabout driving patterns of drivers. The path inference filter has been deployedat an industrial scale inside the Mobile Millennium traffic information system,and is used to map fleets of data in San Francisco, Sacramento, Stockholm andPorto.

Large Scale Estimation in Cyberphysical Systems using Streaming Data: a  Case Study with Smartphone Traces

  Controlling and analyzing cyberphysical and robotics systems is increasinglybecoming a Big Data challenge. Pushing this data to, and processing in thecloud is more efficient than on-board processing. However, current cloud-basedsolutions are not suitable for the latency requirements of these applications.We present a new concept, Discretized Streams or D-Streams, that enablesmassively scalable computations on streaming data with latencies as short as asecond.  We experiment with an implementation of D-Streams on top of the Sparkcomputing framework. We demonstrate the usefulness of this concept with a novelalgorithm to estimate vehicular traffic in urban networks. Our online EMalgorithm can estimate traffic on a very large city network (the San FranciscoBay Area) by processing tens of thousands of observations per second, with alatency of a few seconds.

Discriminative Probabilistic Models for Relational Data

  In many supervised learning tasks, the entities to be labeled are related toeach other in complex ways and their labels are not independent. For example,in hypertext classification, the labels of linked pages are highly correlated.A standard approach is to classify each entity independently, ignoring thecorrelations between them. Recently, Probabilistic Relational Models, arelational version of Bayesian networks, were used to define a jointprobabilistic model for a collection of related entities. In this paper, wepresent an alternative framework that builds on (conditional) Markov networksand addresses two limitations of the previous approach. First, undirectedmodels do not impose the acyclicity constraint that hinders representation ofmany important relational dependencies in directed models. Second, undirectedmodels are well suited for discriminative training, where we optimize theconditional likelihood of the labels given the features, which generallyimproves classification accuracy. We show how to train these modelseffectively, and how to use approximate probabilistic inference over thelearned model for collective classification of multiple related entities. Weprovide experimental results on a webpage classification task, showing thataccuracy can be significantly improved by modeling relational dependencies.

Trust Region Policy Optimization

  We describe an iterative procedure for optimizing policies, with guaranteedmonotonic improvement. By making several approximations to thetheoretically-justified procedure, we develop a practical algorithm, calledTrust Region Policy Optimization (TRPO). This algorithm is similar to naturalpolicy gradient methods and is effective for optimizing large nonlinearpolicies such as neural networks. Our experiments demonstrate its robustperformance on a wide variety of tasks: learning simulated robotic swimming,hopping, and walking gaits; and playing Atari games using images of the screenas input. Despite its approximations that deviate from the theory, TRPO tendsto give monotonic improvement, with little tuning of hyperparameters.

Combinatorial Energy Learning for Image Segmentation

  We introduce a new machine learning approach for image segmentation that usesa neural network to model the conditional energy of a segmentation given animage. Our approach, combinatorial energy learning for image segmentation(CELIS) places a particular emphasis on modeling the inherent combinatorialnature of dense image segmentation problems. We propose efficient algorithmsfor learning deep neural networks to model the energy function, and for localoptimization of this energy in the space of supervoxel agglomerations. Weextensively evaluate our method on a publicly available 3-D microscopy datasetwith 25 billion voxels of ground truth data. On an 11 billion voxel test set,we find that our method improves volumetric reconstruction accuracy by morethan 20% as compared to two state-of-the-art baseline methods: graph-basedsegmentation of the output of a 3-D convolutional neural network trained topredict boundaries, as well as a random forest classifier trained toagglomerate supervoxels that were generated by a 3-D convolutional neuralnetwork.

Gradient Estimation Using Stochastic Computation Graphs

  In a variety of problems originating in supervised, unsupervised, andreinforcement learning, the loss function is defined by an expectation over acollection of random variables, which might be part of a probabilistic model orthe external world. Estimating the gradient of this loss function, usingsamples, lies at the core of gradient-based learning algorithms for theseproblems. We introduce the formalism of stochastic computationgraphs---directed acyclic graphs that include both deterministic functions andconditional probability distributions---and describe how to easily andautomatically derive an unbiased estimator of the loss function's gradient. Theresulting algorithm for computing the gradient estimator is a simplemodification of the standard backpropagation algorithm. The generic scheme wepropose unifies estimators derived in variety of prior work, along withvariance-reduction techniques therein. It could assist researchers indeveloping intricate models involving a combination of stochastic anddeterministic operations, enabling, for example, attention, memory, and controlactions.

Model-based Reinforcement Learning with Parametrized Physical Models and  Optimism-Driven Exploration

  In this paper, we present a robotic model-based reinforcement learning methodthat combines ideas from model identification and model predictive control. Weuse a feature-based representation of the dynamics that allows the dynamicsmodel to be fitted with a simple least squares procedure, and the features areidentified from a high-level specification of the robot's morphology,consisting of the number and connectivity structure of its links. Modelpredictive control is then used to choose the actions under an optimistic modelof the dynamics, which produces an efficient and goal-directed explorationstrategy. We present real time experimental results on standard benchmarkproblems involving the pendulum, cartpole, and double pendulum systems.Experiments indicate that our method is able to learn a range of benchmarktasks substantially faster than the previous best methods. To evaluate ourapproach on a realistic robotic control task, we also demonstrate real timecontrol of a simulated 7 degree of freedom arm.

One-Shot Learning of Manipulation Skills with Online Dynamics Adaptation  and Neural Network Priors

  One of the key challenges in applying reinforcement learning to complexrobotic control tasks is the need to gather large amounts of experience inorder to find an effective policy for the task at hand. Model-basedreinforcement learning can achieve good sample efficiency, but requires theability to learn a model of the dynamics that is good enough to learn aneffective policy. In this work, we develop a model-based reinforcement learningalgorithm that combines prior knowledge from previous tasks with onlineadaptation of the dynamics model. These two ingredients enable highlysample-efficient learning even in regimes where estimating the true dynamics isvery difficult, since the online model adaptation allows the method to locallycompensate for unmodeled variation in the dynamics. We encode the priorexperience into a neural network dynamics model, adapt it online byprogressively refitting a local linear model of the dynamics, and use modelpredictive control to plan under these dynamics. Our experimental results showthat this approach can be used to solve a variety of complex roboticmanipulation tasks in just a single attempt, using prior data from othermanipulation behaviors.

Inverse Reinforcement Learning via Deep Gaussian Process

  We propose a new approach to inverse reinforcement learning (IRL) based onthe deep Gaussian process (deep GP) model, which is capable of learningcomplicated reward structures with few demonstrations. Our model stacksmultiple latent GP layers to learn abstract representations of the statefeature space, which is linked to the demonstrations through the MaximumEntropy learning framework. Incorporating the IRL engine into the nonlinearlatent structure renders existing deep GP inference approaches intractable. Totackle this, we develop a non-standard variational approximation frameworkwhich extends previous inference schemes. This allows for approximate Bayesiantreatment of the feature space and guards against overfitting. Carrying outrepresentation and inverse reinforcement learning simultaneously within ourmodel outperforms state-of-the-art approaches, as we demonstrate withexperiments on standard benchmarks ("object world","highway driving") and a newbenchmark ("binary world").

Value Iteration Networks

  We introduce the value iteration network (VIN): a fully differentiable neuralnetwork with a `planning module' embedded within. VINs can learn to plan, andare suitable for predicting outcomes that involve planning-based reasoning,such as policies for reinforcement learning. Key to our approach is a noveldifferentiable approximation of the value-iteration algorithm, which can berepresented as a convolutional neural network, and trained end-to-end usingstandard backpropagation. We evaluate VIN based policies on discrete andcontinuous path-planning domains, and on a natural-language based search task.We show that by learning an explicit planning computation, VIN policiesgeneralize better to new, unseen domains.

Guided Cost Learning: Deep Inverse Optimal Control via Policy  Optimization

  Reinforcement learning can acquire complex behaviors from high-levelspecifications. However, defining a cost function that can be optimizedeffectively and encodes the correct task is challenging in practice. We explorehow inverse optimal control (IOC) can be used to learn behaviors fromdemonstrations, with applications to torque control of high-dimensional roboticsystems. Our method addresses two key challenges in inverse optimal control:first, the need for informative features and effective regularization to imposestructure on the cost, and second, the difficulty of learning the cost functionunder unknown dynamics for high-dimensional continuous systems. To address theformer challenge, we present an algorithm capable of learning arbitrarynonlinear cost functions, such as neural networks, without meticulous featureengineering. To address the latter challenge, we formulate an efficientsample-based approximation for MaxEnt IOC. We evaluate our method on a seriesof simulated tasks and real-world robotic manipulation problems, demonstratingsubstantial improvement over prior methods both in terms of task complexity andsample efficiency.

Benchmarking Deep Reinforcement Learning for Continuous Control

  Recently, researchers have made significant progress combining the advancesin deep learning for learning feature representations with reinforcementlearning. Some notable examples include training agents to play Atari gamesbased on raw pixel data and to acquire advanced manipulation skills using rawsensory inputs. However, it has been difficult to quantify progress in thedomain of continuous control due to the lack of a commonly adopted benchmark.In this work, we present a benchmark suite of continuous control tasks,including classic tasks like cart-pole swing-up, tasks with very high state andaction dimensionality such as 3D humanoid locomotion, tasks with partialobservations, and tasks with hierarchical structure. We report novel findingsbased on the systematic evaluation of a range of implemented reinforcementlearning algorithms. Both the benchmark and reference implementations arereleased at https://github.com/rllab/rllab in order to facilitate experimentalreproducibility and to encourage adoption by other researchers.

VIME: Variational Information Maximizing Exploration

  Scalable and effective exploration remains a key challenge in reinforcementlearning (RL). While there are methods with optimality guarantees in thesetting of discrete state and action spaces, these methods cannot be applied inhigh-dimensional deep RL scenarios. As such, most contemporary RL relies onsimple heuristics such as epsilon-greedy exploration or adding Gaussian noiseto the controls. This paper introduces Variational Information MaximizingExploration (VIME), an exploration strategy based on maximization ofinformation gain about the agent's belief of environment dynamics. We propose apractical implementation, using variational inference in Bayesian neuralnetworks which efficiently handles continuous state and action spaces. VIMEmodifies the MDP reward function, and can be applied with several differentunderlying RL algorithms. We demonstrate that VIME achieves significantlybetter performance compared to heuristic exploration methods across a varietyof continuous control tasks and algorithms, including tasks with very sparserewards.

Cooperative Inverse Reinforcement Learning

  For an autonomous system to be helpful to humans and to pose no unwarrantedrisks, it needs to align its values with those of the humans in its environmentin such a way that its actions contribute to the maximization of value for thehumans. We propose a formal definition of the value alignment problem ascooperative inverse reinforcement learning (CIRL). A CIRL problem is acooperative, partial-information game with two agents, human and robot; bothare rewarded according to the human's reward function, but the robot does notinitially know what this is. In contrast to classical IRL, where the human isassumed to act optimally in isolation, optimal CIRL solutions produce behaviorssuch as active teaching, active learning, and communicative actions that aremore effective in achieving value alignment. We show that computing optimaljoint policies in CIRL games can be reduced to solving a POMDP, prove thatoptimality in isolation is suboptimal in CIRL, and derive an approximate CIRLalgorithm.

InfoGAN: Interpretable Representation Learning by Information Maximizing  Generative Adversarial Nets

  This paper describes InfoGAN, an information-theoretic extension to theGenerative Adversarial Network that is able to learn disentangledrepresentations in a completely unsupervised manner. InfoGAN is a generativeadversarial network that also maximizes the mutual information between a smallsubset of the latent variables and the observation. We derive a lower bound tothe mutual information objective that can be optimized efficiently, and showthat our training procedure can be interpreted as a variation of the Wake-Sleepalgorithm. Specifically, InfoGAN successfully disentangles writing styles fromdigit shapes on the MNIST dataset, pose from lighting of 3D rendered images,and background digits from the central digit on the SVHN dataset. It alsodiscovers visual concepts that include hair styles, presence/absence ofeyeglasses, and emotions on the CelebA face dataset. Experiments show thatInfoGAN learns interpretable representations that are competitive withrepresentations learned by existing fully supervised methods.

Learning to Poke by Poking: Experiential Learning of Intuitive Physics

  We investigate an experiential learning paradigm for acquiring an internalmodel of intuitive physics. Our model is evaluated on a real-world roboticmanipulation task that requires displacing objects to target locations bypoking. The robot gathered over 400 hours of experience by executing more than100K pokes on different objects. We propose a novel approach based on deepneural networks for modeling the dynamics of robot's interactions directly fromimages, by jointly estimating forward and inverse models of dynamics. Theinverse model objective provides supervision to construct informative visualfeatures, which the forward model can then predict and in turn regularize thefeature space for the inverse model. The interplay between these two objectivescreates useful, accurate models that can then be used for multi-step decisionmaking. This formulation has the additional benefit that it is possible tolearn forward models in an abstract feature space and thus alleviate the needof predicting pixels. Our experiments show that this joint modeling approachoutperforms alternative methods.

Variational Lossy Autoencoder

  Representation learning seeks to expose certain aspects of observed data in alearned representation that's amenable to downstream tasks like classification.For instance, a good representation for 2D images might be one that describesonly global structure and discards information about detailed texture. In thispaper, we present a simple but principled method to learn such globalrepresentations by combining Variational Autoencoder (VAE) with neuralautoregressive models such as RNN, MADE and PixelRNN/CNN. Our proposed VAEmodel allows us to have control over what the global latent code can learn and, by designing the architecture accordingly, we can force the global latentcode to discard irrelevant information such as texture in 2D images, and hencethe VAE only "autoencodes" data in a lossy fashion. In addition, by leveragingautoregressive models as both prior distribution $p(z)$ and decodingdistribution $p(x|z)$, we can greatly improve generative modeling performanceof VAEs, achieving new state-of-the-art results on MNIST, OMNIGLOT andCaltech-101 Silhouettes density estimation tasks.

Adversarial Attacks on Neural Network Policies

  Machine learning classifiers are known to be vulnerable to inputs maliciouslyconstructed by adversaries to force misclassification. Such adversarialexamples have been extensively studied in the context of computer visionapplications. In this work, we show adversarial attacks are also effective whentargeting neural network policies in reinforcement learning. Specifically, weshow existing adversarial example crafting techniques can be used tosignificantly degrade test-time performance of trained policies. Our threatmodel considers adversaries capable of introducing small perturbations to theraw input of the policy. We characterize the degree of vulnerability acrosstasks and training algorithms, for a subclass of adversarial-example attacks inwhite-box and black-box settings. Regardless of the learned task or trainingalgorithm, we observe a significant drop in performance, even with smalladversarial perturbations that do not interfere with human perception. Videosare available at http://rll.berkeley.edu/adversarial.

Reinforcement Learning with Deep Energy-Based Policies

  We propose a method for learning expressive energy-based policies forcontinuous states and actions, which has been feasible only in tabular domainsbefore. We apply our method to learning maximum entropy policies, resultinginto a new algorithm, called soft Q-learning, that expresses the optimal policyvia a Boltzmann distribution. We use the recently proposed amortized Steinvariational gradient descent to learn a stochastic sampling network thatapproximates samples from this distribution. The benefits of the proposedalgorithm include improved exploration and compositionality that allowstransferring skills between tasks, which we confirm in simulated experimentswith swimming and walking robots. We also draw a connection to actor-criticmethods, which can be viewed performing approximate inference on thecorresponding energy-based model.

Combining Self-Supervised Learning and Imitation for Vision-Based Rope  Manipulation

  Manipulation of deformable objects, such as ropes and cloth, is an importantbut challenging problem in robotics. We present a learning-based system where arobot takes as input a sequence of images of a human manipulating a rope froman initial to goal configuration, and outputs a sequence of actions that canreproduce the human demonstration, using only monocular images as input. Toperform this task, the robot learns a pixel-level inverse dynamics model ofrope manipulation directly from images in a self-supervised manner, using about60K interactions with the rope collected autonomously by the robot. The humandemonstration provides a high-level plan of what to do and the low-levelinverse model is used to execute the plan. We show that by combining the highand low-level plans, the robot can successfully manipulate a rope into avariety of target shapes using only a sequence of human-provided images fordirection.

Learning Invariant Feature Spaces to Transfer Skills with Reinforcement  Learning

  People can learn a wide range of tasks from their own experience, but canalso learn from observing other creatures. This can accelerate acquisition ofnew skills even when the observed agent differs substantially from the learningagent in terms of morphology. In this paper, we examine how reinforcementlearning algorithms can transfer knowledge between morphologically differentagents (e.g., different robots). We introduce a problem formulation where twoagents are tasked with learning multiple skills by sharing information. Ourmethod uses the skills that were learned by both agents to train invariantfeature spaces that can then be used to transfer other skills from one agent toanother. The process of learning these invariant feature spaces can be viewedas a kind of "analogy making", or implicit learning of partial correspondencesbetween two distinct domains. We evaluate our transfer learning algorithm intwo simulated robotic manipulation skills, and illustrate that we can transferknowledge between simulated robotic arms with different numbers of links, aswell as simulated arms with different actuation mechanisms, where one robot istorque-driven while the other is tendon-driven.

Prediction and Control with Temporal Segment Models

  We introduce a method for learning the dynamics of complex nonlinear systemsbased on deep generative models over temporal segments of states and actions.Unlike dynamics models that operate over individual discrete timesteps, welearn the distribution over future state trajectories conditioned on paststate, past action, and planned future action trajectories, as well as a latentprior over action trajectories. Our approach is based on convolutionalautoregressive models and variational autoencoders. It makes stable andaccurate predictions over long horizons for complex, stochastic systems,effectively expressing uncertainty and modeling the effects of collisions,sensory noise, and action delays. The learned dynamics model and action priorcan be used for end-to-end, fully differentiable trajectory optimization andmodel-based policy optimization, which we use to evaluate the performance andsample-efficiency of our method.

Emergence of Grounded Compositional Language in Multi-Agent Populations

  By capturing statistical patterns in large corpora, machine learning hasenabled significant advances in natural language processing, including inmachine translation, question answering, and sentiment analysis. However, foragents to intelligently interact with humans, simply capturing the statisticalpatterns is insufficient. In this paper we investigate if, and how, groundedcompositional language can emerge as a means to achieve goals in multi-agentpopulations. Towards this end, we propose a multi-agent learning environmentand learning methods that bring about emergence of a basic compositionallanguage. This language is represented as streams of abstract discrete symbolsuttered by agents over time, but nonetheless has a coherent structure thatpossesses a defined vocabulary and syntax. We also observe emergence ofnon-verbal communication such as pointing and guiding when languagecommunication is unavailable.

Domain Randomization for Transferring Deep Neural Networks from  Simulation to the Real World

  Bridging the 'reality gap' that separates simulated robotics from experimentson hardware could accelerate robotic research through improved dataavailability. This paper explores domain randomization, a simple technique fortraining models on simulated images that transfer to real images by randomizingrendering in the simulator. With enough variability in the simulator, the realworld may appear to the model as just another variation. We focus on the taskof object localization, which is a stepping stone to general roboticmanipulation skills. We find that it is possible to train a real-world objectdetector that is accurate to $1.5$cm and robust to distractors and partialocclusions using only data from a simulator with non-realistic random textures.To demonstrate the capabilities of our detectors, we show they can be used toperform grasping in a cluttered environment. To our knowledge, this is thefirst successful transfer of a deep neural network trained only on simulatedRGB images (without pre-training on real images) to the real world for thepurpose of robotic control.

Stochastic Neural Networks for Hierarchical Reinforcement Learning

  Deep reinforcement learning has achieved many impressive results in recentyears. However, tasks with sparse rewards or long horizons continue to posesignificant challenges. To tackle these important problems, we propose ageneral framework that first learns useful skills in a pre-trainingenvironment, and then leverages the acquired skills for learning faster indownstream tasks. Our approach brings together some of the strengths ofintrinsic motivation and hierarchical methods: the learning of useful skill isguided by a single proxy reward, the design of which requires very minimaldomain knowledge about the downstream tasks. Then a high-level policy istrained on top of these skills, providing a significant improvement of theexploration and allowing to tackle sparse rewards in the downstream tasks. Toefficiently pre-train a large span of skills, we use Stochastic Neural Networkscombined with an information-theoretic regularizer. Our experiments show thatthis combination is effective in learning a wide span of interpretable skillsin a sample-efficient way, and can significantly boost the learning performanceuniformly across a wide range of downstream tasks.

Equivalence Between Policy Gradients and Soft Q-Learning

  Two of the leading approaches for model-free reinforcement learning arepolicy gradient methods and $Q$-learning methods. $Q$-learning methods can beeffective and sample-efficient when they work, however, it is notwell-understood why they work, since empirically, the $Q$-values they estimateare very inaccurate. A partial explanation may be that $Q$-learning methods aresecretly implementing policy gradient updates: we show that there is a preciseequivalence between $Q$-learning and policy gradient methods in the setting ofentropy-regularized reinforcement learning, that "soft" (entropy-regularized)$Q$-learning is exactly equivalent to a policy gradient method. We also pointout a connection between $Q$-learning methods and natural policy gradientmethods. Experimentally, we explore the entropy-regularized versions of$Q$-learning and policy gradients, and we find them to perform as well as (orslightly better than) the standard variants on the Atari benchmark. We alsoshow that the equivalence holds in practical settings by constructing a$Q$-learning method that closely matches the learning dynamics of A3C withoutusing a target network or $\epsilon$-greedy exploration schedule.

Probabilistically Safe Policy Transfer

  Although learning-based methods have great potential for robotics, oneconcern is that a robot that updates its parameters might cause large amountsof damage before it learns the optimal policy. We formalize the idea of safelearning in a probabilistic sense by defining an optimization problem: wedesire to maximize the expected return while keeping the expected damage belowa given safety limit. We study this optimization for the case of a robotmanipulator with safety-based torque limits. We would like to ensure that thedamage constraint is maintained at every step of the optimization and not justat convergence. To achieve this aim, we introduce a novel method which predictshow modifying the torque limit, as well as how updating the policy parameters,might affect the robot's safety. We show through a number of experiments thatour approach allows the robot to improve its performance while ensuring thatthe expected damage constraint is not violated during the learning process.

Constrained Policy Optimization

  For many applications of reinforcement learning it can be more convenient tospecify both a reward function and constraints, rather than trying to designbehavior through the reward function. For example, systems that physicallyinteract with or around humans should satisfy safety constraints. Recentadvances in policy search algorithms (Mnih et al., 2016, Schulman et al., 2015,Lillicrap et al., 2016, Levine et al., 2016) have enabled new capabilities inhigh-dimensional control, but do not consider the constrained setting.  We propose Constrained Policy Optimization (CPO), the first general-purposepolicy search algorithm for constrained reinforcement learning with guaranteesfor near-constraint satisfaction at each iteration. Our method allows us totrain neural network policies for high-dimensional control while makingguarantees about policy behavior all throughout training. Our guarantees arebased on a new theoretical result, which is of independent interest: we prove abound relating the expected returns of two policies to an average divergencebetween them. We demonstrate the effectiveness of our approach on simulatedrobot locomotion tasks where the agent must satisfy constraints motivated bysafety.

Parameter Space Noise for Exploration

  Deep reinforcement learning (RL) methods generally engage in exploratorybehavior through noise injection in the action space. An alternative is to addnoise directly to the agent's parameters, which can lead to more consistentexploration and a richer set of behaviors. Methods such as evolutionarystrategies use parameter perturbations, but discard all temporal structure inthe process and require significantly more samples. Combining parameter noisewith traditional RL methods allows to combine the best of both worlds. Wedemonstrate that both off- and on-policy methods benefit from this approachthrough experimental comparison of DQN, DDPG, and TRPO on high-dimensionaldiscrete action environments as well as continuous control tasks. Our resultsshow that RL with parameter noise learns more efficiently than traditional RLwith action space noise and evolutionary strategies individually.

Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments

  We explore deep reinforcement learning methods for multi-agent domains. Webegin by analyzing the difficulty of traditional algorithms in the multi-agentcase: Q-learning is challenged by an inherent non-stationarity of theenvironment, while policy gradient suffers from a variance that increases asthe number of agents grows. We then present an adaptation of actor-criticmethods that considers action policies of other agents and is able tosuccessfully learn policies that require complex multi-agent coordination.Additionally, we introduce a training regimen utilizing an ensemble of policiesfor each agent that leads to more robust multi-agent policies. We show thestrength of our approach compared to existing methods in cooperative as well ascompetitive scenarios, where agent populations are able to discover variousphysical and informational coordination strategies.

Hindsight Experience Replay

  Dealing with sparse rewards is one of the biggest challenges in ReinforcementLearning (RL). We present a novel technique called Hindsight Experience Replaywhich allows sample-efficient learning from rewards which are sparse and binaryand therefore avoid the need for complicated reward engineering. It can becombined with an arbitrary off-policy RL algorithm and may be seen as a form ofimplicit curriculum.  We demonstrate our approach on the task of manipulating objects with arobotic arm. In particular, we run experiments on three different tasks:pushing, sliding, and pick-and-place, in each case using only binary rewardsindicating whether or not the task is completed. Our ablation studies show thatHindsight Experience Replay is a crucial ingredient which makes trainingpossible in these challenging environments. We show that our policies trainedon a physics simulation can be deployed on a physical robot and successfullycomplete the task.

Mutual Alignment Transfer Learning

  Training robots for operation in the real world is a complex, time consumingand potentially expensive task. Despite significant success of reinforcementlearning in games and simulations, research in real robot applications has notbeen able to match similar progress. While sample complexity can be reduced bytraining policies in simulation, such policies can perform sub-optimally on thereal platform given imperfect calibration of model dynamics. We present anapproach -- supplemental to fine tuning on the real robot -- to further benefitfrom parallel access to a simulator during training and reduce samplerequirements on the real robot. The developed approach harnesses auxiliaryrewards to guide the exploration for the real world agent based on theproficiency of the agent in simulation and vice versa. In this context, wedemonstrate empirically that the reciprocal alignment for both agents providesfurther benefit as the agent in simulation can adjust to optimize its behaviourfor states commonly visited by the real-world agent.

Deep Object-Centric Representations for Generalizable Robot Learning

  Robotic manipulation in complex open-world scenarios requires both reliablephysical manipulation skills and effective and generalizable perception. Inthis paper, we propose a method where general purpose pretrained visual modelsserve as an object-centric prior for the perception system of a learned policy.We devise an object-level attentional mechanism that can be used to determinerelevant objects from a few trajectories or demonstrations, and thenimmediately incorporate those objects into a learned policy. A task-independentmeta-attention locates possible objects in the scene, and a task-specificattention identifies which objects are predictive of the trajectories. Thescope of the task-specific attention is easily adjusted by showingdemonstrations with distractor objects or with diverse relevant objects. Ourresults indicate that this approach exhibits good generalization across objectinstances using very few samples, and can be used to learn a variety ofmanipulation tasks using reinforcement learning.

Learning Generalized Reactive Policies using Deep Neural Networks

  We present a new approach to learning for planning, where knowledge acquiredwhile solving a given set of planning problems is used to plan faster inrelated, but new problem instances. We show that a deep neural network can beused to learn and represent a \emph{generalized reactive policy} (GRP) thatmaps a problem instance and a state to an action, and that the learned GRPsefficiently solve large classes of challenging problem instances. In contrastto prior efforts in this direction, our approach significantly reduces thedependence of learning on handcrafted domain knowledge or feature selection.Instead, the GRP is trained from scratch using a set of successful executiontraces. We show that our approach can also be used to automatically learn aheuristic function that can be used in directed search algorithms. We evaluateour approach using an extensive suite of experiments on two challengingplanning problem domains and show that our approach facilitates learningcomplex decision making policies and powerful heuristic functions with minimalhuman input. Videos of our results are available at goo.gl/Hpy4e3.

One-Shot Visual Imitation Learning via Meta-Learning

  In order for a robot to be a generalist that can perform a wide range ofjobs, it must be able to acquire a wide variety of skills quickly andefficiently in complex unstructured environments. High-capacity models such asdeep neural networks can enable a robot to represent complex skills, butlearning each skill from scratch then becomes infeasible. In this work, wepresent a meta-imitation learning method that enables a robot to learn how tolearn more efficiently, allowing it to acquire new skills from just a singledemonstration. Unlike prior methods for one-shot imitation, our method canscale to raw pixel inputs and requires data from significantly fewer priortasks for effective learning of new skills. Our experiments on both simulatedand real robot platforms demonstrate the ability to learn new tasks,end-to-end, from a single visual demonstration.

Overcoming Exploration in Reinforcement Learning with Demonstrations

  Exploration in environments with sparse rewards has been a persistent problemin reinforcement learning (RL). Many tasks are natural to specify with a sparsereward, and manually shaping a reward function can result in suboptimalperformance. However, finding a non-zero reward is exponentially more difficultwith increasing task horizon or action dimensionality. This puts manyreal-world tasks out of practical reach of RL methods. In this work, we usedemonstrations to overcome the exploration problem and successfully learn toperform long-horizon, multi-step robotics tasks with continuous control such asstacking blocks with a robot arm. Our method, which builds on top of DeepDeterministic Policy Gradients and Hindsight Experience Replay, provides anorder of magnitude of speedup over RL on simulated robotics tasks. It is simpleto implement and makes only the additional assumption that we can collect asmall set of demonstrations. Furthermore, our method is able to solve tasks notsolvable by either RL or behavior cloning alone, and often ends upoutperforming the demonstrator policy.

Continuous Adaptation via Meta-Learning in Nonstationary and Competitive  Environments

  Ability to continuously learn and adapt from limited experience innonstationary environments is an important milestone on the path towardsgeneral intelligence. In this paper, we cast the problem of continuousadaptation into the learning-to-learn framework. We develop a simplegradient-based meta-learning algorithm suitable for adaptation in dynamicallychanging and adversarial scenarios. Additionally, we design a new multi-agentcompetitive environment, RoboSumo, and define iterated adaptation games fortesting various aspects of continuous adaptation strategies. We demonstratethat meta-learning enables significantly more efficient adaptation thanreactive baselines in the few-shot regime. Our experiments with a population ofagents that learn and compete suggest that meta-learners are the fittest.

Synkhronos: a Multi-GPU Theano Extension for Data Parallelism

  We present Synkhronos, an extension to Theano for multi-GPU computationsleveraging data parallelism. Our framework provides automated execution andsynchronization across devices, allowing users to continue to write serialprograms without risk of race conditions. The NVIDIA Collective CommunicationLibrary is used for high-bandwidth inter-GPU communication. Furtherenhancements to the Theano function interface include input slicing (withaggregation) and input indexing, which perform common data-parallel computationpatterns efficiently. One example use case is synchronous SGD, which hasrecently been shown to scale well for a growing set of deep learning problems.When training ResNet-50, we achieve a near-linear speedup of 7.5x on an NVIDIADGX-1 using 8 GPUs, relative to Theano-only code running a single GPU inisolation. Yet Synkhronos remains general to any data-parallel computationprogrammable in Theano. By implementing parallelism at the level of individualTheano functions, our framework uniquely addresses a niche between manualmulti-device programming and prescribed multi-GPU training routines.

Deep Imitation Learning for Complex Manipulation Tasks from Virtual  Reality Teleoperation

  Imitation learning is a powerful paradigm for robot skill acquisition.However, obtaining demonstrations suitable for learning a policy that maps fromraw pixels to actions can be challenging. In this paper we describe howconsumer-grade Virtual Reality headsets and hand tracking hardware can be usedto naturally teleoperate robots to perform complex tasks. We also describe howimitation learning can learn deep neural network policies (mapping from pixelsto actions) that can acquire the demonstrated skills. Our experiments showcasethe effectiveness of our approach for learning visuomotor skills.

Asymmetric Actor Critic for Image-Based Robot Learning

  Deep reinforcement learning (RL) has proven a powerful technique in manysequential decision making domains. However, Robotics poses many challenges forRL, most notably training on a physical system can be expensive and dangerous,which has sparked significant interest in learning control policies using aphysics simulator. While several recent works have shown promising results intransferring policies trained in simulation to the real world, they often donot fully utilize the advantage of working with a simulator. In this work, weexploit the full state observability in the simulator to train better policieswhich take as input only partial observations (RGBD images). We do this byemploying an actor-critic training algorithm in which the critic is trained onfull states while the actor (or policy) gets rendered images as input. We showexperimentally on a range of simulated tasks that using these asymmetric inputssignificantly improves performance. Finally, we combine this method with domainrandomization and show real robot experiments for several tasks like picking,pushing, and moving a block. We achieve this simulation to real world transferwithout training on any real world data.

Meta Learning Shared Hierarchies

  We develop a metalearning approach for learning hierarchically structuredpolicies, improving sample efficiency on unseen tasks through the use of sharedprimitives---policies that are executed for large numbers of timesteps.Specifically, a set of primitives are shared within a distribution of tasks,and are switched between by task-specific policies. We provide a concretemetric for measuring the strength of such hierarchies, leading to anoptimization problem for quickly reaching high reward on unseen tasks. We thenpresent an algorithm to solve this problem end-to-end through the use of anyoff-the-shelf reinforcement learning method, by repeatedly sampling new tasksand resetting task-specific policies. We successfully discover meaningful motorprimitives for the directional movement of four-legged robots, solely byinteracting with distributions of mazes. We also demonstrate thetransferability of primitives to solve long-timescale sparse-reward obstaclecourses, and we enable 3D humanoid robots to robustly walk and crawl with thesame policy.

Inverse Reward Design

  Autonomous agents optimize the reward function we give them. What they don'tknow is how hard it is for us to design a reward function that actuallycaptures what we want. When designing the reward, we might think of somespecific training scenarios, and make sure that the reward will lead to theright behavior in those scenarios. Inevitably, agents encounter new scenarios(e.g., new types of terrain) where optimizing that same reward may lead toundesired behavior. Our insight is that reward functions are merelyobservations about what the designer actually wants, and that they should beinterpreted in the context in which they were designed. We introduce inversereward design (IRD) as the problem of inferring the true objective based on thedesigned reward and the training MDP. We introduce approximate methods forsolving IRD problems, and use their solution to plan risk-averse behavior intest MDPs. Empirical results suggest that this approach can help alleviatenegative side effects of misspecified reward functions and mitigate rewardhacking.

