Learning Factor Graphs in Polynomial Time & Sample Complexity

  We study computational and sample complexity of parameter and structure
learning in graphical models. Our main result shows that the class of factor
graphs with bounded factor size and bounded connectivity can be learned in
polynomial time and polynomial number of samples, assuming that the data is
generated by a network in this class. This result covers both parameter
estimation for a known network structure and structure learning. It implies as
a corollary that we can learn factor graphs for both Bayesian networks and
Markov networks of bounded degree, in polynomial time and sample complexity.
Unlike maximum likelihood estimation, our method does not require inference in
the underlying network, and so applies to networks where inference is
intractable. We also show that the error of our learned model degrades
gracefully when the generating distribution is not a member of the target class
of networks.


A K-fold Method for Baseline Estimation in Policy Gradient Algorithms

  The high variance issue in unbiased policy-gradient methods such as VPG and
REINFORCE is typically mitigated by adding a baseline. However, the baseline
fitting itself suffers from the underfitting or the overfitting problem. In
this paper, we develop a K-fold method for baseline estimation in policy
gradient algorithms. The parameter K is the baseline estimation hyperparameter
that can adjust the bias-variance trade-off in the baseline estimates. We
demonstrate the usefulness of our approach via two state-of-the-art policy
gradient algorithms on three MuJoCo locomotive control tasks.


UCB Exploration via Q-Ensembles

  We show how an ensemble of $Q^*$-functions can be leveraged for more
effective exploration in deep reinforcement learning. We build on well
established algorithms from the bandit setting, and adapt them to the
$Q$-learning setting. We propose an exploration strategy based on
upper-confidence bounds (UCB). Our experiments show significant gains on the
Atari benchmark.


Interpretable and Pedagogical Examples

  Teachers intentionally pick the most informative examples to show their
students. However, if the teacher and student are neural networks, the examples
that the teacher network learns to give, although effective at teaching the
student, are typically uninterpretable. We show that training the student and
teacher iteratively, rather than jointly, can produce interpretable teaching
strategies. We evaluate interpretability by (1) measuring the similarity of the
teacher's emergent strategies to intuitive strategies in each domain and (2)
conducting human experiments to evaluate how effective the teacher's strategies
are at teaching humans. We show that the teacher network learns to select or
generate interpretable, pedagogical examples to teach rule-based,
probabilistic, boolean, and hierarchical concepts.


Some Considerations on Learning to Explore via Meta-Reinforcement
  Learning

  We consider the problem of exploration in meta reinforcement learning. Two
new meta reinforcement learning algorithms are suggested: E-MAML and
E-$\text{RL}^2$. Results are presented on a novel environment we call `Krazy
World' and a set of maze environments. We show E-MAML and E-$\text{RL}^2$
deliver better performance on tasks where exploration is important.


Transfer Learning for Estimating Causal Effects using Neural Networks

  We develop new algorithms for estimating heterogeneous treatment effects,
combining recent developments in transfer learning for neural networks with
insights from the causal inference literature. By taking advantage of transfer
learning, we are able to efficiently use different data sources that are
related to the same underlying causal mechanisms. We compare our algorithms
with those in the extant literature using extensive simulation studies based on
large-scale voter persuasion experiments and the MNIST database. Our methods
can perform an order of magnitude better than existing benchmarks while using a
fraction of the data.


Toward a Science of Autonomy for Physical Systems: Paths

  An Autonomous Physical System (APS) will be expected to reliably and
independently evaluate, execute, and achieve goals while respecting surrounding
rules, laws, or conventions. In doing so, an APS must rely on a broad spectrum
of dynamic, complex, and often imprecise information about its surroundings,
the task it is to perform, and its own sensors and actuators. For example,
cleaning in a home or commercial setting requires the ability to perceive,
grasp, and manipulate many physical objects, the ability to reliably perform a
variety of subtasks such as washing, folding, and stacking, and knowledge about
local conventions such as how objects are classified and where they should be
stored. The information required for reliable autonomous operation may come
from external sources and from the robot's own sensor observations or in the
form of direct instruction by a trainer. Similar considerations apply across
many domains - construction, manufacturing, in-home assistance, and healthcare.
For example, surgeons spend many years learning about physiology and anatomy
before they touch a patient. They then perform roughly 1000 surgeries under the
tutelage of an expert surgeon, and they practice basic maneuvers such as suture
tying thousands of times outside the operating room. All of these elements come
together to achieve expertise at this task. Endowing a system with robust
autonomy by traditional programming methods has thus far had limited success.
Several promising new paths to acquiring and processing such data are emerging.
This white paper outlines three promising research directions for enabling an
APS to learn the physical and information skills necessary to perform tasks
with independence and flexibility: Deep Reinforcement Learning, Human-Robot
Interaction, and Cloud Robotics.


Learning Contact-Rich Manipulation Skills with Guided Policy Search

  Autonomous learning of object manipulation skills can enable robots to
acquire rich behavioral repertoires that scale to the variety of objects found
in the real world. However, current motion skill learning methods typically
restrict the behavior to a compact, low-dimensional representation, limiting
its expressiveness and generality. In this paper, we extend a recently
developed policy search method \cite{la-lnnpg-14} and use it to learn a range
of dynamic manipulation behaviors with highly general policy representations,
without using known models or example demonstrations. Our approach learns a set
of trajectories for the desired motion skill by using iteratively refitted
time-varying linear models, and then unifies these trajectories into a single
control policy that can generalize to new situations. To enable this method to
run on a real robot, we introduce several improvements that reduce the sample
count and automate parameter selection. We show that our method can acquire
fast, fluent behaviors after only minutes of interaction time, and can learn
robust controllers for complex tasks, including putting together a toy
airplane, stacking tight-fitting lego blocks, placing wooden rings onto
tight-fitting pegs, inserting a shoe tree into a shoe, and screwing bottle caps
onto bottles.


Large Scale Estimation in Cyberphysical Systems using Streaming Data: a
  Case Study with Smartphone Traces

  Controlling and analyzing cyberphysical and robotics systems is increasingly
becoming a Big Data challenge. Pushing this data to, and processing in the
cloud is more efficient than on-board processing. However, current cloud-based
solutions are not suitable for the latency requirements of these applications.
We present a new concept, Discretized Streams or D-Streams, that enables
massively scalable computations on streaming data with latencies as short as a
second.
  We experiment with an implementation of D-Streams on top of the Spark
computing framework. We demonstrate the usefulness of this concept with a novel
algorithm to estimate vehicular traffic in urban networks. Our online EM
algorithm can estimate traffic on a very large city network (the San Francisco
Bay Area) by processing tens of thousands of observations per second, with a
latency of a few seconds.


Discriminative Probabilistic Models for Relational Data

  In many supervised learning tasks, the entities to be labeled are related to
each other in complex ways and their labels are not independent. For example,
in hypertext classification, the labels of linked pages are highly correlated.
A standard approach is to classify each entity independently, ignoring the
correlations between them. Recently, Probabilistic Relational Models, a
relational version of Bayesian networks, were used to define a joint
probabilistic model for a collection of related entities. In this paper, we
present an alternative framework that builds on (conditional) Markov networks
and addresses two limitations of the previous approach. First, undirected
models do not impose the acyclicity constraint that hinders representation of
many important relational dependencies in directed models. Second, undirected
models are well suited for discriminative training, where we optimize the
conditional likelihood of the labels given the features, which generally
improves classification accuracy. We show how to train these models
effectively, and how to use approximate probabilistic inference over the
learned model for collective classification of multiple related entities. We
provide experimental results on a webpage classification task, showing that
accuracy can be significantly improved by modeling relational dependencies.


Inverse Reinforcement Learning via Deep Gaussian Process

  We propose a new approach to inverse reinforcement learning (IRL) based on
the deep Gaussian process (deep GP) model, which is capable of learning
complicated reward structures with few demonstrations. Our model stacks
multiple latent GP layers to learn abstract representations of the state
feature space, which is linked to the demonstrations through the Maximum
Entropy learning framework. Incorporating the IRL engine into the nonlinear
latent structure renders existing deep GP inference approaches intractable. To
tackle this, we develop a non-standard variational approximation framework
which extends previous inference schemes. This allows for approximate Bayesian
treatment of the feature space and guards against overfitting. Carrying out
representation and inverse reinforcement learning simultaneously within our
model outperforms state-of-the-art approaches, as we demonstrate with
experiments on standard benchmarks ("object world","highway driving") and a new
benchmark ("binary world").


Value Iteration Networks

  We introduce the value iteration network (VIN): a fully differentiable neural
network with a `planning module' embedded within. VINs can learn to plan, and
are suitable for predicting outcomes that involve planning-based reasoning,
such as policies for reinforcement learning. Key to our approach is a novel
differentiable approximation of the value-iteration algorithm, which can be
represented as a convolutional neural network, and trained end-to-end using
standard backpropagation. We evaluate VIN based policies on discrete and
continuous path-planning domains, and on a natural-language based search task.
We show that by learning an explicit planning computation, VIN policies
generalize better to new, unseen domains.


Cooperative Inverse Reinforcement Learning

  For an autonomous system to be helpful to humans and to pose no unwarranted
risks, it needs to align its values with those of the humans in its environment
in such a way that its actions contribute to the maximization of value for the
humans. We propose a formal definition of the value alignment problem as
cooperative inverse reinforcement learning (CIRL). A CIRL problem is a
cooperative, partial-information game with two agents, human and robot; both
are rewarded according to the human's reward function, but the robot does not
initially know what this is. In contrast to classical IRL, where the human is
assumed to act optimally in isolation, optimal CIRL solutions produce behaviors
such as active teaching, active learning, and communicative actions that are
more effective in achieving value alignment. We show that computing optimal
joint policies in CIRL games can be reduced to solving a POMDP, prove that
optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL
algorithm.


InfoGAN: Interpretable Representation Learning by Information Maximizing
  Generative Adversarial Nets

  This paper describes InfoGAN, an information-theoretic extension to the
Generative Adversarial Network that is able to learn disentangled
representations in a completely unsupervised manner. InfoGAN is a generative
adversarial network that also maximizes the mutual information between a small
subset of the latent variables and the observation. We derive a lower bound to
the mutual information objective that can be optimized efficiently, and show
that our training procedure can be interpreted as a variation of the Wake-Sleep
algorithm. Specifically, InfoGAN successfully disentangles writing styles from
digit shapes on the MNIST dataset, pose from lighting of 3D rendered images,
and background digits from the central digit on the SVHN dataset. It also
discovers visual concepts that include hair styles, presence/absence of
eyeglasses, and emotions on the CelebA face dataset. Experiments show that
InfoGAN learns interpretable representations that are competitive with
representations learned by existing fully supervised methods.


Learning to Poke by Poking: Experiential Learning of Intuitive Physics

  We investigate an experiential learning paradigm for acquiring an internal
model of intuitive physics. Our model is evaluated on a real-world robotic
manipulation task that requires displacing objects to target locations by
poking. The robot gathered over 400 hours of experience by executing more than
100K pokes on different objects. We propose a novel approach based on deep
neural networks for modeling the dynamics of robot's interactions directly from
images, by jointly estimating forward and inverse models of dynamics. The
inverse model objective provides supervision to construct informative visual
features, which the forward model can then predict and in turn regularize the
feature space for the inverse model. The interplay between these two objectives
creates useful, accurate models that can then be used for multi-step decision
making. This formulation has the additional benefit that it is possible to
learn forward models in an abstract feature space and thus alleviate the need
of predicting pixels. Our experiments show that this joint modeling approach
outperforms alternative methods.


The path inference filter: model-based low-latency map matching of probe
  vehicle data

  We consider the problem of reconstructing vehicle trajectories from sparse
sequences of GPS points, for which the sampling interval is between 10 seconds
and 2 minutes. We introduce a new class of algorithms, called altogether path
inference filter (PIF), that maps GPS data in real time, for a variety of
trade-offs and scenarios, and with a high throughput. Numerous prior approaches
in map-matching can be shown to be special cases of the path inference filter
presented in this article. We present an efficient procedure for automatically
training the filter on new data, with or without ground truth observations. The
framework is evaluated on a large San Francisco taxi dataset and is shown to
improve upon the current state of the art. This filter also provides insights
about driving patterns of drivers. The path inference filter has been deployed
at an industrial scale inside the Mobile Millennium traffic information system,
and is used to map fleets of data in San Francisco, Sacramento, Stockholm and
Porto.


Guided Cost Learning: Deep Inverse Optimal Control via Policy
  Optimization

  Reinforcement learning can acquire complex behaviors from high-level
specifications. However, defining a cost function that can be optimized
effectively and encodes the correct task is challenging in practice. We explore
how inverse optimal control (IOC) can be used to learn behaviors from
demonstrations, with applications to torque control of high-dimensional robotic
systems. Our method addresses two key challenges in inverse optimal control:
first, the need for informative features and effective regularization to impose
structure on the cost, and second, the difficulty of learning the cost function
under unknown dynamics for high-dimensional continuous systems. To address the
former challenge, we present an algorithm capable of learning arbitrary
nonlinear cost functions, such as neural networks, without meticulous feature
engineering. To address the latter challenge, we formulate an efficient
sample-based approximation for MaxEnt IOC. We evaluate our method on a series
of simulated tasks and real-world robotic manipulation problems, demonstrating
substantial improvement over prior methods both in terms of task complexity and
sample efficiency.


Stochastic Neural Networks for Hierarchical Reinforcement Learning

  Deep reinforcement learning has achieved many impressive results in recent
years. However, tasks with sparse rewards or long horizons continue to pose
significant challenges. To tackle these important problems, we propose a
general framework that first learns useful skills in a pre-training
environment, and then leverages the acquired skills for learning faster in
downstream tasks. Our approach brings together some of the strengths of
intrinsic motivation and hierarchical methods: the learning of useful skill is
guided by a single proxy reward, the design of which requires very minimal
domain knowledge about the downstream tasks. Then a high-level policy is
trained on top of these skills, providing a significant improvement of the
exploration and allowing to tackle sparse rewards in the downstream tasks. To
efficiently pre-train a large span of skills, we use Stochastic Neural Networks
combined with an information-theoretic regularizer. Our experiments show that
this combination is effective in learning a wide span of interpretable skills
in a sample-efficient way, and can significantly boost the learning performance
uniformly across a wide range of downstream tasks.


Equivalence Between Policy Gradients and Soft Q-Learning

  Two of the leading approaches for model-free reinforcement learning are
policy gradient methods and $Q$-learning methods. $Q$-learning methods can be
effective and sample-efficient when they work, however, it is not
well-understood why they work, since empirically, the $Q$-values they estimate
are very inaccurate. A partial explanation may be that $Q$-learning methods are
secretly implementing policy gradient updates: we show that there is a precise
equivalence between $Q$-learning and policy gradient methods in the setting of
entropy-regularized reinforcement learning, that "soft" (entropy-regularized)
$Q$-learning is exactly equivalent to a policy gradient method. We also point
out a connection between $Q$-learning methods and natural policy gradient
methods. Experimentally, we explore the entropy-regularized versions of
$Q$-learning and policy gradients, and we find them to perform as well as (or
slightly better than) the standard variants on the Atari benchmark. We also
show that the equivalence holds in practical settings by constructing a
$Q$-learning method that closely matches the learning dynamics of A3C without
using a target network or $\epsilon$-greedy exploration schedule.


Trust Region Policy Optimization

  We describe an iterative procedure for optimizing policies, with guaranteed
monotonic improvement. By making several approximations to the
theoretically-justified procedure, we develop a practical algorithm, called
Trust Region Policy Optimization (TRPO). This algorithm is similar to natural
policy gradient methods and is effective for optimizing large nonlinear
policies such as neural networks. Our experiments demonstrate its robust
performance on a wide variety of tasks: learning simulated robotic swimming,
hopping, and walking gaits; and playing Atari games using images of the screen
as input. Despite its approximations that deviate from the theory, TRPO tends
to give monotonic improvement, with little tuning of hyperparameters.


Combinatorial Energy Learning for Image Segmentation

  We introduce a new machine learning approach for image segmentation that uses
a neural network to model the conditional energy of a segmentation given an
image. Our approach, combinatorial energy learning for image segmentation
(CELIS) places a particular emphasis on modeling the inherent combinatorial
nature of dense image segmentation problems. We propose efficient algorithms
for learning deep neural networks to model the energy function, and for local
optimization of this energy in the space of supervoxel agglomerations. We
extensively evaluate our method on a publicly available 3-D microscopy dataset
with 25 billion voxels of ground truth data. On an 11 billion voxel test set,
we find that our method improves volumetric reconstruction accuracy by more
than 20% as compared to two state-of-the-art baseline methods: graph-based
segmentation of the output of a 3-D convolutional neural network trained to
predict boundaries, as well as a random forest classifier trained to
agglomerate supervoxels that were generated by a 3-D convolutional neural
network.


Gradient Estimation Using Stochastic Computation Graphs

  In a variety of problems originating in supervised, unsupervised, and
reinforcement learning, the loss function is defined by an expectation over a
collection of random variables, which might be part of a probabilistic model or
the external world. Estimating the gradient of this loss function, using
samples, lies at the core of gradient-based learning algorithms for these
problems. We introduce the formalism of stochastic computation
graphs---directed acyclic graphs that include both deterministic functions and
conditional probability distributions---and describe how to easily and
automatically derive an unbiased estimator of the loss function's gradient. The
resulting algorithm for computing the gradient estimator is a simple
modification of the standard backpropagation algorithm. The generic scheme we
propose unifies estimators derived in variety of prior work, along with
variance-reduction techniques therein. It could assist researchers in
developing intricate models involving a combination of stochastic and
deterministic operations, enabling, for example, attention, memory, and control
actions.


Model-based Reinforcement Learning with Parametrized Physical Models and
  Optimism-Driven Exploration

  In this paper, we present a robotic model-based reinforcement learning method
that combines ideas from model identification and model predictive control. We
use a feature-based representation of the dynamics that allows the dynamics
model to be fitted with a simple least squares procedure, and the features are
identified from a high-level specification of the robot's morphology,
consisting of the number and connectivity structure of its links. Model
predictive control is then used to choose the actions under an optimistic model
of the dynamics, which produces an efficient and goal-directed exploration
strategy. We present real time experimental results on standard benchmark
problems involving the pendulum, cartpole, and double pendulum systems.
Experiments indicate that our method is able to learn a range of benchmark
tasks substantially faster than the previous best methods. To evaluate our
approach on a realistic robotic control task, we also demonstrate real time
control of a simulated 7 degree of freedom arm.


One-Shot Learning of Manipulation Skills with Online Dynamics Adaptation
  and Neural Network Priors

  One of the key challenges in applying reinforcement learning to complex
robotic control tasks is the need to gather large amounts of experience in
order to find an effective policy for the task at hand. Model-based
reinforcement learning can achieve good sample efficiency, but requires the
ability to learn a model of the dynamics that is good enough to learn an
effective policy. In this work, we develop a model-based reinforcement learning
algorithm that combines prior knowledge from previous tasks with online
adaptation of the dynamics model. These two ingredients enable highly
sample-efficient learning even in regimes where estimating the true dynamics is
very difficult, since the online model adaptation allows the method to locally
compensate for unmodeled variation in the dynamics. We encode the prior
experience into a neural network dynamics model, adapt it online by
progressively refitting a local linear model of the dynamics, and use model
predictive control to plan under these dynamics. Our experimental results show
that this approach can be used to solve a variety of complex robotic
manipulation tasks in just a single attempt, using prior data from other
manipulation behaviors.


Benchmarking Deep Reinforcement Learning for Continuous Control

  Recently, researchers have made significant progress combining the advances
in deep learning for learning feature representations with reinforcement
learning. Some notable examples include training agents to play Atari games
based on raw pixel data and to acquire advanced manipulation skills using raw
sensory inputs. However, it has been difficult to quantify progress in the
domain of continuous control due to the lack of a commonly adopted benchmark.
In this work, we present a benchmark suite of continuous control tasks,
including classic tasks like cart-pole swing-up, tasks with very high state and
action dimensionality such as 3D humanoid locomotion, tasks with partial
observations, and tasks with hierarchical structure. We report novel findings
based on the systematic evaluation of a range of implemented reinforcement
learning algorithms. Both the benchmark and reference implementations are
released at https://github.com/rllab/rllab in order to facilitate experimental
reproducibility and to encourage adoption by other researchers.


VIME: Variational Information Maximizing Exploration

  Scalable and effective exploration remains a key challenge in reinforcement
learning (RL). While there are methods with optimality guarantees in the
setting of discrete state and action spaces, these methods cannot be applied in
high-dimensional deep RL scenarios. As such, most contemporary RL relies on
simple heuristics such as epsilon-greedy exploration or adding Gaussian noise
to the controls. This paper introduces Variational Information Maximizing
Exploration (VIME), an exploration strategy based on maximization of
information gain about the agent's belief of environment dynamics. We propose a
practical implementation, using variational inference in Bayesian neural
networks which efficiently handles continuous state and action spaces. VIME
modifies the MDP reward function, and can be applied with several different
underlying RL algorithms. We demonstrate that VIME achieves significantly
better performance compared to heuristic exploration methods across a variety
of continuous control tasks and algorithms, including tasks with very sparse
rewards.


Variational Lossy Autoencoder

  Representation learning seeks to expose certain aspects of observed data in a
learned representation that's amenable to downstream tasks like classification.
For instance, a good representation for 2D images might be one that describes
only global structure and discards information about detailed texture. In this
paper, we present a simple but principled method to learn such global
representations by combining Variational Autoencoder (VAE) with neural
autoregressive models such as RNN, MADE and PixelRNN/CNN. Our proposed VAE
model allows us to have control over what the global latent code can learn and
, by designing the architecture accordingly, we can force the global latent
code to discard irrelevant information such as texture in 2D images, and hence
the VAE only "autoencodes" data in a lossy fashion. In addition, by leveraging
autoregressive models as both prior distribution $p(z)$ and decoding
distribution $p(x|z)$, we can greatly improve generative modeling performance
of VAEs, achieving new state-of-the-art results on MNIST, OMNIGLOT and
Caltech-101 Silhouettes density estimation tasks.


Adversarial Attacks on Neural Network Policies

  Machine learning classifiers are known to be vulnerable to inputs maliciously
constructed by adversaries to force misclassification. Such adversarial
examples have been extensively studied in the context of computer vision
applications. In this work, we show adversarial attacks are also effective when
targeting neural network policies in reinforcement learning. Specifically, we
show existing adversarial example crafting techniques can be used to
significantly degrade test-time performance of trained policies. Our threat
model considers adversaries capable of introducing small perturbations to the
raw input of the policy. We characterize the degree of vulnerability across
tasks and training algorithms, for a subclass of adversarial-example attacks in
white-box and black-box settings. Regardless of the learned task or training
algorithm, we observe a significant drop in performance, even with small
adversarial perturbations that do not interfere with human perception. Videos
are available at http://rll.berkeley.edu/adversarial.


Reinforcement Learning with Deep Energy-Based Policies

  We propose a method for learning expressive energy-based policies for
continuous states and actions, which has been feasible only in tabular domains
before. We apply our method to learning maximum entropy policies, resulting
into a new algorithm, called soft Q-learning, that expresses the optimal policy
via a Boltzmann distribution. We use the recently proposed amortized Stein
variational gradient descent to learn a stochastic sampling network that
approximates samples from this distribution. The benefits of the proposed
algorithm include improved exploration and compositionality that allows
transferring skills between tasks, which we confirm in simulated experiments
with swimming and walking robots. We also draw a connection to actor-critic
methods, which can be viewed performing approximate inference on the
corresponding energy-based model.


Combining Self-Supervised Learning and Imitation for Vision-Based Rope
  Manipulation

  Manipulation of deformable objects, such as ropes and cloth, is an important
but challenging problem in robotics. We present a learning-based system where a
robot takes as input a sequence of images of a human manipulating a rope from
an initial to goal configuration, and outputs a sequence of actions that can
reproduce the human demonstration, using only monocular images as input. To
perform this task, the robot learns a pixel-level inverse dynamics model of
rope manipulation directly from images in a self-supervised manner, using about
60K interactions with the rope collected autonomously by the robot. The human
demonstration provides a high-level plan of what to do and the low-level
inverse model is used to execute the plan. We show that by combining the high
and low-level plans, the robot can successfully manipulate a rope into a
variety of target shapes using only a sequence of human-provided images for
direction.


Learning Invariant Feature Spaces to Transfer Skills with Reinforcement
  Learning

  People can learn a wide range of tasks from their own experience, but can
also learn from observing other creatures. This can accelerate acquisition of
new skills even when the observed agent differs substantially from the learning
agent in terms of morphology. In this paper, we examine how reinforcement
learning algorithms can transfer knowledge between morphologically different
agents (e.g., different robots). We introduce a problem formulation where two
agents are tasked with learning multiple skills by sharing information. Our
method uses the skills that were learned by both agents to train invariant
feature spaces that can then be used to transfer other skills from one agent to
another. The process of learning these invariant feature spaces can be viewed
as a kind of "analogy making", or implicit learning of partial correspondences
between two distinct domains. We evaluate our transfer learning algorithm in
two simulated robotic manipulation skills, and illustrate that we can transfer
knowledge between simulated robotic arms with different numbers of links, as
well as simulated arms with different actuation mechanisms, where one robot is
torque-driven while the other is tendon-driven.


Prediction and Control with Temporal Segment Models

  We introduce a method for learning the dynamics of complex nonlinear systems
based on deep generative models over temporal segments of states and actions.
Unlike dynamics models that operate over individual discrete timesteps, we
learn the distribution over future state trajectories conditioned on past
state, past action, and planned future action trajectories, as well as a latent
prior over action trajectories. Our approach is based on convolutional
autoregressive models and variational autoencoders. It makes stable and
accurate predictions over long horizons for complex, stochastic systems,
effectively expressing uncertainty and modeling the effects of collisions,
sensory noise, and action delays. The learned dynamics model and action prior
can be used for end-to-end, fully differentiable trajectory optimization and
model-based policy optimization, which we use to evaluate the performance and
sample-efficiency of our method.


Emergence of Grounded Compositional Language in Multi-Agent Populations

  By capturing statistical patterns in large corpora, machine learning has
enabled significant advances in natural language processing, including in
machine translation, question answering, and sentiment analysis. However, for
agents to intelligently interact with humans, simply capturing the statistical
patterns is insufficient. In this paper we investigate if, and how, grounded
compositional language can emerge as a means to achieve goals in multi-agent
populations. Towards this end, we propose a multi-agent learning environment
and learning methods that bring about emergence of a basic compositional
language. This language is represented as streams of abstract discrete symbols
uttered by agents over time, but nonetheless has a coherent structure that
possesses a defined vocabulary and syntax. We also observe emergence of
non-verbal communication such as pointing and guiding when language
communication is unavailable.


Domain Randomization for Transferring Deep Neural Networks from
  Simulation to the Real World

  Bridging the 'reality gap' that separates simulated robotics from experiments
on hardware could accelerate robotic research through improved data
availability. This paper explores domain randomization, a simple technique for
training models on simulated images that transfer to real images by randomizing
rendering in the simulator. With enough variability in the simulator, the real
world may appear to the model as just another variation. We focus on the task
of object localization, which is a stepping stone to general robotic
manipulation skills. We find that it is possible to train a real-world object
detector that is accurate to $1.5$cm and robust to distractors and partial
occlusions using only data from a simulator with non-realistic random textures.
To demonstrate the capabilities of our detectors, we show they can be used to
perform grasping in a cluttered environment. To our knowledge, this is the
first successful transfer of a deep neural network trained only on simulated
RGB images (without pre-training on real images) to the real world for the
purpose of robotic control.


Probabilistically Safe Policy Transfer

  Although learning-based methods have great potential for robotics, one
concern is that a robot that updates its parameters might cause large amounts
of damage before it learns the optimal policy. We formalize the idea of safe
learning in a probabilistic sense by defining an optimization problem: we
desire to maximize the expected return while keeping the expected damage below
a given safety limit. We study this optimization for the case of a robot
manipulator with safety-based torque limits. We would like to ensure that the
damage constraint is maintained at every step of the optimization and not just
at convergence. To achieve this aim, we introduce a novel method which predicts
how modifying the torque limit, as well as how updating the policy parameters,
might affect the robot's safety. We show through a number of experiments that
our approach allows the robot to improve its performance while ensuring that
the expected damage constraint is not violated during the learning process.


Constrained Policy Optimization

  For many applications of reinforcement learning it can be more convenient to
specify both a reward function and constraints, rather than trying to design
behavior through the reward function. For example, systems that physically
interact with or around humans should satisfy safety constraints. Recent
advances in policy search algorithms (Mnih et al., 2016, Schulman et al., 2015,
Lillicrap et al., 2016, Levine et al., 2016) have enabled new capabilities in
high-dimensional control, but do not consider the constrained setting.
  We propose Constrained Policy Optimization (CPO), the first general-purpose
policy search algorithm for constrained reinforcement learning with guarantees
for near-constraint satisfaction at each iteration. Our method allows us to
train neural network policies for high-dimensional control while making
guarantees about policy behavior all throughout training. Our guarantees are
based on a new theoretical result, which is of independent interest: we prove a
bound relating the expected returns of two policies to an average divergence
between them. We demonstrate the effectiveness of our approach on simulated
robot locomotion tasks where the agent must satisfy constraints motivated by
safety.


Parameter Space Noise for Exploration

  Deep reinforcement learning (RL) methods generally engage in exploratory
behavior through noise injection in the action space. An alternative is to add
noise directly to the agent's parameters, which can lead to more consistent
exploration and a richer set of behaviors. Methods such as evolutionary
strategies use parameter perturbations, but discard all temporal structure in
the process and require significantly more samples. Combining parameter noise
with traditional RL methods allows to combine the best of both worlds. We
demonstrate that both off- and on-policy methods benefit from this approach
through experimental comparison of DQN, DDPG, and TRPO on high-dimensional
discrete action environments as well as continuous control tasks. Our results
show that RL with parameter noise learns more efficiently than traditional RL
with action space noise and evolutionary strategies individually.


Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments

  We explore deep reinforcement learning methods for multi-agent domains. We
begin by analyzing the difficulty of traditional algorithms in the multi-agent
case: Q-learning is challenged by an inherent non-stationarity of the
environment, while policy gradient suffers from a variance that increases as
the number of agents grows. We then present an adaptation of actor-critic
methods that considers action policies of other agents and is able to
successfully learn policies that require complex multi-agent coordination.
Additionally, we introduce a training regimen utilizing an ensemble of policies
for each agent that leads to more robust multi-agent policies. We show the
strength of our approach compared to existing methods in cooperative as well as
competitive scenarios, where agent populations are able to discover various
physical and informational coordination strategies.


Hindsight Experience Replay

  Dealing with sparse rewards is one of the biggest challenges in Reinforcement
Learning (RL). We present a novel technique called Hindsight Experience Replay
which allows sample-efficient learning from rewards which are sparse and binary
and therefore avoid the need for complicated reward engineering. It can be
combined with an arbitrary off-policy RL algorithm and may be seen as a form of
implicit curriculum.
  We demonstrate our approach on the task of manipulating objects with a
robotic arm. In particular, we run experiments on three different tasks:
pushing, sliding, and pick-and-place, in each case using only binary rewards
indicating whether or not the task is completed. Our ablation studies show that
Hindsight Experience Replay is a crucial ingredient which makes training
possible in these challenging environments. We show that our policies trained
on a physics simulation can be deployed on a physical robot and successfully
complete the task.


Mutual Alignment Transfer Learning

  Training robots for operation in the real world is a complex, time consuming
and potentially expensive task. Despite significant success of reinforcement
learning in games and simulations, research in real robot applications has not
been able to match similar progress. While sample complexity can be reduced by
training policies in simulation, such policies can perform sub-optimally on the
real platform given imperfect calibration of model dynamics. We present an
approach -- supplemental to fine tuning on the real robot -- to further benefit
from parallel access to a simulator during training and reduce sample
requirements on the real robot. The developed approach harnesses auxiliary
rewards to guide the exploration for the real world agent based on the
proficiency of the agent in simulation and vice versa. In this context, we
demonstrate empirically that the reciprocal alignment for both agents provides
further benefit as the agent in simulation can adjust to optimize its behaviour
for states commonly visited by the real-world agent.


Deep Object-Centric Representations for Generalizable Robot Learning

  Robotic manipulation in complex open-world scenarios requires both reliable
physical manipulation skills and effective and generalizable perception. In
this paper, we propose a method where general purpose pretrained visual models
serve as an object-centric prior for the perception system of a learned policy.
We devise an object-level attentional mechanism that can be used to determine
relevant objects from a few trajectories or demonstrations, and then
immediately incorporate those objects into a learned policy. A task-independent
meta-attention locates possible objects in the scene, and a task-specific
attention identifies which objects are predictive of the trajectories. The
scope of the task-specific attention is easily adjusted by showing
demonstrations with distractor objects or with diverse relevant objects. Our
results indicate that this approach exhibits good generalization across object
instances using very few samples, and can be used to learn a variety of
manipulation tasks using reinforcement learning.


Learning Generalized Reactive Policies using Deep Neural Networks

  We present a new approach to learning for planning, where knowledge acquired
while solving a given set of planning problems is used to plan faster in
related, but new problem instances. We show that a deep neural network can be
used to learn and represent a \emph{generalized reactive policy} (GRP) that
maps a problem instance and a state to an action, and that the learned GRPs
efficiently solve large classes of challenging problem instances. In contrast
to prior efforts in this direction, our approach significantly reduces the
dependence of learning on handcrafted domain knowledge or feature selection.
Instead, the GRP is trained from scratch using a set of successful execution
traces. We show that our approach can also be used to automatically learn a
heuristic function that can be used in directed search algorithms. We evaluate
our approach using an extensive suite of experiments on two challenging
planning problem domains and show that our approach facilitates learning
complex decision making policies and powerful heuristic functions with minimal
human input. Videos of our results are available at goo.gl/Hpy4e3.


One-Shot Visual Imitation Learning via Meta-Learning

  In order for a robot to be a generalist that can perform a wide range of
jobs, it must be able to acquire a wide variety of skills quickly and
efficiently in complex unstructured environments. High-capacity models such as
deep neural networks can enable a robot to represent complex skills, but
learning each skill from scratch then becomes infeasible. In this work, we
present a meta-imitation learning method that enables a robot to learn how to
learn more efficiently, allowing it to acquire new skills from just a single
demonstration. Unlike prior methods for one-shot imitation, our method can
scale to raw pixel inputs and requires data from significantly fewer prior
tasks for effective learning of new skills. Our experiments on both simulated
and real robot platforms demonstrate the ability to learn new tasks,
end-to-end, from a single visual demonstration.


Overcoming Exploration in Reinforcement Learning with Demonstrations

  Exploration in environments with sparse rewards has been a persistent problem
in reinforcement learning (RL). Many tasks are natural to specify with a sparse
reward, and manually shaping a reward function can result in suboptimal
performance. However, finding a non-zero reward is exponentially more difficult
with increasing task horizon or action dimensionality. This puts many
real-world tasks out of practical reach of RL methods. In this work, we use
demonstrations to overcome the exploration problem and successfully learn to
perform long-horizon, multi-step robotics tasks with continuous control such as
stacking blocks with a robot arm. Our method, which builds on top of Deep
Deterministic Policy Gradients and Hindsight Experience Replay, provides an
order of magnitude of speedup over RL on simulated robotics tasks. It is simple
to implement and makes only the additional assumption that we can collect a
small set of demonstrations. Furthermore, our method is able to solve tasks not
solvable by either RL or behavior cloning alone, and often ends up
outperforming the demonstrator policy.


Continuous Adaptation via Meta-Learning in Nonstationary and Competitive
  Environments

  Ability to continuously learn and adapt from limited experience in
nonstationary environments is an important milestone on the path towards
general intelligence. In this paper, we cast the problem of continuous
adaptation into the learning-to-learn framework. We develop a simple
gradient-based meta-learning algorithm suitable for adaptation in dynamically
changing and adversarial scenarios. Additionally, we design a new multi-agent
competitive environment, RoboSumo, and define iterated adaptation games for
testing various aspects of continuous adaptation strategies. We demonstrate
that meta-learning enables significantly more efficient adaptation than
reactive baselines in the few-shot regime. Our experiments with a population of
agents that learn and compete suggest that meta-learners are the fittest.


Synkhronos: a Multi-GPU Theano Extension for Data Parallelism

  We present Synkhronos, an extension to Theano for multi-GPU computations
leveraging data parallelism. Our framework provides automated execution and
synchronization across devices, allowing users to continue to write serial
programs without risk of race conditions. The NVIDIA Collective Communication
Library is used for high-bandwidth inter-GPU communication. Further
enhancements to the Theano function interface include input slicing (with
aggregation) and input indexing, which perform common data-parallel computation
patterns efficiently. One example use case is synchronous SGD, which has
recently been shown to scale well for a growing set of deep learning problems.
When training ResNet-50, we achieve a near-linear speedup of 7.5x on an NVIDIA
DGX-1 using 8 GPUs, relative to Theano-only code running a single GPU in
isolation. Yet Synkhronos remains general to any data-parallel computation
programmable in Theano. By implementing parallelism at the level of individual
Theano functions, our framework uniquely addresses a niche between manual
multi-device programming and prescribed multi-GPU training routines.


Deep Imitation Learning for Complex Manipulation Tasks from Virtual
  Reality Teleoperation

  Imitation learning is a powerful paradigm for robot skill acquisition.
However, obtaining demonstrations suitable for learning a policy that maps from
raw pixels to actions can be challenging. In this paper we describe how
consumer-grade Virtual Reality headsets and hand tracking hardware can be used
to naturally teleoperate robots to perform complex tasks. We also describe how
imitation learning can learn deep neural network policies (mapping from pixels
to actions) that can acquire the demonstrated skills. Our experiments showcase
the effectiveness of our approach for learning visuomotor skills.


Asymmetric Actor Critic for Image-Based Robot Learning

  Deep reinforcement learning (RL) has proven a powerful technique in many
sequential decision making domains. However, Robotics poses many challenges for
RL, most notably training on a physical system can be expensive and dangerous,
which has sparked significant interest in learning control policies using a
physics simulator. While several recent works have shown promising results in
transferring policies trained in simulation to the real world, they often do
not fully utilize the advantage of working with a simulator. In this work, we
exploit the full state observability in the simulator to train better policies
which take as input only partial observations (RGBD images). We do this by
employing an actor-critic training algorithm in which the critic is trained on
full states while the actor (or policy) gets rendered images as input. We show
experimentally on a range of simulated tasks that using these asymmetric inputs
significantly improves performance. Finally, we combine this method with domain
randomization and show real robot experiments for several tasks like picking,
pushing, and moving a block. We achieve this simulation to real world transfer
without training on any real world data.


Meta Learning Shared Hierarchies

  We develop a metalearning approach for learning hierarchically structured
policies, improving sample efficiency on unseen tasks through the use of shared
primitives---policies that are executed for large numbers of timesteps.
Specifically, a set of primitives are shared within a distribution of tasks,
and are switched between by task-specific policies. We provide a concrete
metric for measuring the strength of such hierarchies, leading to an
optimization problem for quickly reaching high reward on unseen tasks. We then
present an algorithm to solve this problem end-to-end through the use of any
off-the-shelf reinforcement learning method, by repeatedly sampling new tasks
and resetting task-specific policies. We successfully discover meaningful motor
primitives for the directional movement of four-legged robots, solely by
interacting with distributions of mazes. We also demonstrate the
transferability of primitives to solve long-timescale sparse-reward obstacle
courses, and we enable 3D humanoid robots to robustly walk and crawl with the
same policy.


Inverse Reward Design

  Autonomous agents optimize the reward function we give them. What they don't
know is how hard it is for us to design a reward function that actually
captures what we want. When designing the reward, we might think of some
specific training scenarios, and make sure that the reward will lead to the
right behavior in those scenarios. Inevitably, agents encounter new scenarios
(e.g., new types of terrain) where optimizing that same reward may lead to
undesired behavior. Our insight is that reward functions are merely
observations about what the designer actually wants, and that they should be
interpreted in the context in which they were designed. We introduce inverse
reward design (IRD) as the problem of inferring the true objective based on the
designed reward and the training MDP. We introduce approximate methods for
solving IRD problems, and use their solution to plan risk-averse behavior in
test MDPs. Empirical results suggest that this approach can help alleviate
negative side effects of misspecified reward functions and mitigate reward
hacking.


