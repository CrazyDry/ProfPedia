A New Approximation Technique for Resource-Allocation Problems

  We develop a rounding method based on random walks in polytopes, which leads
to improved approximation algorithms and integrality gaps for several
assignment problems that arise in resource allocation and scheduling. In
particular, it generalizes the work of Shmoys and Tardos on the generalized
assignment problem to the setting where some jobs can be dropped. New
concentration bounds for random bipartite matching are developed as well.


New Constructive Aspects of the Lovasz Local Lemma

  The Lov\'{a}sz Local Lemma (LLL) states that the probability that none of a
set of "bad" events happens is nonzero if the probability of each event is
small compared to the number of bad events it depends on. A series of results
have provided algorithms to efficiently construct structures whose existence is
(non-constructively) guaranteed by the full asymmetric LLL, culminating in the
recent breakthrough of Moser & Tardos. We show that the output distribution of
the Moser-Tardos procedure has sufficient randomness, leading to two classes of
algorithmic applications. We first show that when an LLL application provides a
small amount of slack, the running time of the Moser-Tardos algorithm is
polynomial in the number of underlying independent variables (not events!), and
can thus be used to give efficient constructions in cases where the underlying
proof applies the LLL to super-polynomially many events (or where finding a bad
event that holds is computationally hard). We demonstrate our method on
applications including: the first constant-factor approximation algorithm for
the Santa Claus problem, as well as efficient algorithms for acyclic edge
coloring, non-repetitive graph colorings, and Ramsey-type graphs. Second, we
show applications to cases where a few of the bad events can hold, leading to
the first such algorithmic applications of the LLL: MAX $k$-SAT is an
illustrative example of this.


Efficiently Computing Edit Distance to Dyck Language

  Given a string $\sigma$ over alphabet $\Sigma$ and a grammar $G$ defined over
the same alphabet, how many minimum number of repairs: insertions, deletions
and substitutions are required to map $\sigma$ into a valid member of $G$ ? We
investigate this basic question in this paper for $Dyck(s)$. $Dyck(s)$ is a
fundamental context free grammar representing the language of well-balanced
parentheses with s different types of parentheses and has played a pivotal role
in the development of theory of context free languages. Computing edit distance
to $Dyck(s)$ significantly generalizes string edit distance problem and has
numerous applications ranging from repairing semi-structured documents such as
XML to memory checking, automated compiler optimization, natural language
processing etc.
  In this paper we give the first near-linear time algorithm for edit distance
computation to $Dyck(s)$ that achieves a nontrivial approximation factor of
$O(\frac{1}{\epsilon}\log{OPT}(\log{n})^{\frac{1}{\epsilon}})$ in
$O(n^{1+\epsilon}\log{n})$ time. In fact, given there exists an algorithm for
computing string edit distance on input of size $n$ in $\alpha(n)$ time with
$\beta(n)$-approximation factor, we can devise an algorithm for edit distance
problem to $Dyck(s)$ running in $\tilde{O}(n^{1+\epsilon}+\alpha(n))$ and
achieving an approximation factor of $O(\frac{1}{\epsilon}\beta(n)\log{OPT})$.
  We show that the framework for efficiently approximating edit distance to
$Dyck(s)$ can be applied to many other languages. We illustrate this by
considering various memory checking languages which comprise of valid
transcripts of stacks, queues, priority queues, double-ended queues etc.
Therefore, any language that can be recognized by these data structures, can
also be repaired efficiently by our algorithm.


Truly Sub-cubic Algorithms for Language Edit Distance and RNA Folding
  via Fast Bounded-Difference Min-Plus Product

  It is a major open problem whether the $(\min,+)$-product of two $n\times n$
matrices has a truly sub-cubic (i.e. $O(n^{3-\epsilon})$ for $\epsilon>0$) time
algorithm, in particular since it is equivalent to the famous
All-Pairs-Shortest-Paths problem (APSP) in $n$-vertex graphs. Some restrictions
of the $(\min,+)$-product to special types of matrices are known to admit truly
sub-cubic algorithms, each giving rise to a special case of APSP that can be
solved faster. In this paper we consider a new, different and powerful
restriction in which all matrix entries are integers and one matrix can be
arbitrary, as long as the other matrix has "bounded differences" in either its
columns or rows, i.e. any two consecutive entries differ by only a small
amount. We obtain the first truly sub-cubic algorithm for this
bounded-difference $(\min,+)$-product (answering an open problem of Chan and
Lewenstein).
  Our new algorithm, combined with a strengthening of an approach of L.~Valiant
for solving context-free grammar parsing with matrix multiplication, yields the
first truly sub-cubic algorithms for the following problems: Language Edit
Distance (a major problem in the parsing community), RNA-folding (a major
problem in bioinformatics) and Optimum Stack Generation (answering an open
problem of Tarjan).


Language Edit Distance & Maximum Likelihood Parsing of Stochastic
  Grammars: Faster Algorithms & Connection to Fundamental Graph Problems

  Given a context free language $L(G)$ over alphabet $\Sigma$ and a string $s
\in \Sigma^*$, the language edit distance (Lan-ED) problem seeks the minimum
number of edits (insertions, deletions and substitutions) required to convert
$s$ into a valid member of $L(G)$. The well-known dynamic programming algorithm
solves this problem in $O(n^3)$ time (ignoring grammar size) where $n$ is the
string length [Aho, Peterson 1972, Myers 1985]. Despite its vast number of
applications, there is no algorithm known till date that computes or
approximates Lan-ED in true sub-cubic time.
  In this paper we give the first such algorithm that computes Lan-ED almost
optimally. For any arbitrary $\epsilon > 0$, our algorithm runs in
$\tilde{O}(\frac{n^{\omega}}{poly(\epsilon)})$ time and returns an estimate
within a multiplicative approximation factor of $(1+\epsilon)$, where $\omega$
is the exponent of ordinary matrix multiplication of $n$ dimensional square
matrices. It also computes the edit script. Further, for all substrings of $s$,
we can estimate their Lan-ED within $(1\pm \epsilon)$ factor in
$\tilde{O}(\frac{n^{\omega}}{poly(\epsilon)})$ time with high probability. We
also design the very first sub-cubic ($\tilde{O}(n^\omega)$) algorithm to
handle arbitrary stochastic context free grammar (SCFG) parsing. SCFGs lie at
the foundation of statistical natural language processing, they generalize
hidden Markov models, and have found widespread applications.
  To complement our upper bound result, we show that exact computation of SCFG
parsing, or Lan-ED with insertion as only edit operation in true sub-cubic time
will imply a truly sub-cubic algorithm for all-pairs shortest paths, and hence
to a large range of problems in graphs and matrices. Known lower bound results
on parsing implies no improvement over our time bound of $O(n^\omega)$ is
possible for any nontrivial multiplicative approximation.


Clustering Via Crowdsourcing

  In recent years, crowdsourcing, aka human aided computation has emerged as an
effective platform for solving problems that are considered complex for
machines alone. Using human is time-consuming and costly due to monetary
compensations. Therefore, a crowd based algorithm must judiciously use any
information computed through an automated process, and ask minimum number of
questions to the crowd adaptively.
  One such problem which has received significant attention is {\em entity
resolution}. Formally, we are given a graph $G=(V,E)$ with unknown edge set $E$
where $G$ is a union of $k$ (again unknown, but typically large $O(n^\alpha)$,
for $\alpha>0$) disjoint cliques $G_i(V_i, E_i)$, $i =1, \dots, k$. The goal is
to retrieve the sets $V_i$s by making minimum number of pair-wise queries $V
\times V\to\{\pm1\}$ to an oracle (the crowd). When the answer to each query is
correct, e.g. via resampling, then this reduces to finding connected components
in a graph. On the other hand, when crowd answers may be incorrect, it
corresponds to clustering over minimum number of noisy inputs. Even, with
perfect answers, a simple lower and upper bound of $\Theta(nk)$ on query
complexity can be shown. A major contribution of this paper is to reduce the
query complexity to linear or even sublinear in $n$ when mild side information
is provided by a machine, and even in presence of crowd errors which are not
correctable via resampling. We develop new information theoretic lower bounds
on the query complexity of clustering with side information and errors, and our
upper bounds closely match with them. Our algorithms are naturally
parallelizable, and also give near-optimal bounds on the number of adaptive
rounds required to match the query complexity.


Dynamic Set Cover: Improved Algorithms & Lower Bounds

  Set cover is a classic problem in combinatorial optimization where a set of
$n$ elements have to be covered by a minimum number of subsets from a given
collection of size $m$. The two traditional lines of inquiry for this problem
are via greedy and primal dual algorithms, and respectively yield (tight)
approximation factors of $\ln n$, where $n$ is the total number of elements,
and $f$, where every element belongs to at most $f$ sets. Recent research has
focused on the dynamic setting, where the set of elements changes over time.
Using the same lines of inquiry, this has led to the following results: (a) an
$O(\log n)$-approximation in $O(f \log n)$ (amortized) update time (Gupta {\em
et al.}, STOC 2017), and (b) an $O(f^2)$-approximation in $O(f \log (m+n))$
(amortized) update time (Bhattacharya {\em et al.}, ICALP 2015). While the
former result matches the offline approximation within a constant factor, the
latter does not; indeed, the only $O(f)$-approximation known in the dynamic
setting is by re-solving the problem after every update.
  In this paper, we show that it is possible to maintain efficiently a solution
(almost) as good as the primal-dual offline one: we give a $(1+\epsilon)
f$-approximation for set cover in $O(f^2\log n/\epsilon)$ (amortized) update
time. If we are in a decremental setting, i.e., there are element deletions but
no insertions, the update time can be improved to $O(f^2/\epsilon)$, while
still obtaining an $(1+\epsilon) f$-approximation. Finally, we study the
dependence of the update time on $f$. Using the recent distributed
PCP-framework, we show that any dynamic algorithm for set cover that has an
amortized update time of $O(f^{1-\epsilon})$ must have an approximation factor
that is $\Omega(n^\delta)$ for some $\delta>0$ under the Strong Exponential
Time Hypothesis.


A Constant Factor Approximation Algorithm for Fault-Tolerant k-Median

  In this paper, we consider the fault-tolerant $k$-median problem and give the
\emph{first} constant factor approximation algorithm for it. In the
fault-tolerant generalization of classical $k$-median problem, each client $j$
needs to be assigned to at least $r_j \ge 1$ distinct open facilities. The
service cost of $j$ is the sum of its distances to the $r_j$ facilities, and
the $k$-median constraint restricts the number of open facilities to at most
$k$. Previously, a constant factor was known only for the special case when all
$r_j$s are the same, and a logarithmic approximation ratio for the general
case. In addition, we present the first polynomial time algorithm for the
fault-tolerant $k$-median problem on a path or a HST by showing that the
corresponding LP always has an integral optimal solution.
  We also consider the fault-tolerant facility location problem, where the
service cost of $j$ can be a weighted sum of its distance to the $r_j$
facilities. We give a simple constant factor approximation algorithm,
generalizing several previous results which only work for nonincreasing weight
vectors.


A Theoretical Analysis of First Heuristics of Crowdsourced Entity
  Resolution

  Entity resolution (ER) is the task of identifying all records in a database
that refer to the same underlying entity, and are therefore duplicates of each
other. Due to inherent ambiguity of data representation and poor data quality,
ER is a challenging task for any automated process. As a remedy, human-powered
ER via crowdsourcing has become popular in recent years. Using crowd to answer
queries is costly and time consuming. Furthermore, crowd-answers can often be
faulty. Therefore, crowd-based ER methods aim to minimize human participation
without sacrificing the quality and use a computer generated similarity matrix
actively. While, some of these methods perform well in practice, no theoretical
analysis exists for them, and further their worst case performances do not
reflect the experimental findings. This creates a disparity in the
understanding of the popular heuristics for this problem. In this paper, we
make the first attempt to close this gap. We provide a thorough analysis of the
prominent heuristic algorithms for crowd-based ER. We justify experimental
observations with our analysis and information theoretic lower bounds.


A Unified Approach to Ranking in Probabilistic Databases

  The dramatic growth in the number of application domains that naturally
generate probabilistic, uncertain data has resulted in a need for efficiently
supporting complex querying and decision-making over such data. In this paper,
we present a unified approach to ranking and top-k query processing in
probabilistic databases by viewing it as a multi-criteria optimization problem,
and by deriving a set of features that capture the key properties of a
probabilistic dataset that dictate the ranked result. We contend that a single,
specific ranking function may not suffice for probabilistic databases, and we
instead propose two parameterized ranking functions, called PRF-w and PRF-e,
that generalize or can approximate many of the previously proposed ranking
functions. We present novel generating functions-based algorithms for
efficiently ranking large datasets according to these ranking functions, even
if the datasets exhibit complex correlations modeled using probabilistic
and/xor trees or Markov networks. We further propose that the parameters of the
ranking function be learned from user preferences, and we develop an approach
to learn those parameters. Finally, we present a comprehensive experimental
study that illustrates the effectiveness of our parameterized ranking
functions, especially PRF-e, at approximating other ranking functions and the
scalability of our proposed algorithms for exact or approximate ranking.


Energy Efficient Scheduling via Partial Shutdown

  Motivated by issues of saving energy in data centers we define a collection
of new problems referred to as "machine activation" problems. The central
framework we introduce considers a collection of $m$ machines (unrelated or
related) with each machine $i$ having an {\em activation cost} of $a_i$. There
is also a collection of $n$ jobs that need to be performed, and $p_{i,j}$ is
the processing time of job $j$ on machine $i$. We assume that there is an
activation cost budget of $A$ -- we would like to {\em select} a subset $S$ of
the machines to activate with total cost $a(S) \le A$ and {\em find} a schedule
for the $n$ jobs on the machines in $S$ minimizing the makespan (or any other
metric).
  For the general unrelated machine activation problem, our main results are
that if there is a schedule with makespan $T$ and activation cost $A$ then we
can obtain a schedule with makespan $\makespanconstant T$ and activation cost
$\costconstant A$, for any $\epsilon >0$. We also consider assignment costs for
jobs as in the generalized assignment problem, and using our framework, provide
algorithms that minimize the machine activation and the assignment cost
simultaneously. In addition, we present a greedy algorithm which only works for
the basic version and yields a makespan of $2T$ and an activation cost $A
(1+\ln n)$.
  For the uniformly related parallel machine scheduling problem, we develop a
polynomial time approximation scheme that outputs a schedule with the property
that the activation cost of the subset of machines is at most $A$ and the
makespan is at most $(1+\epsilon) T$ for any $\epsilon >0$.


AdCell: Ad Allocation in Cellular Networks

  With more than four billion usage of cellular phones worldwide, mobile
advertising has become an attractive alternative to online advertisements. In
this paper, we propose a new targeted advertising policy for Wireless Service
Providers (WSPs) via SMS or MMS- namely {\em AdCell}. In our model, a WSP
charges the advertisers for showing their ads. Each advertiser has a valuation
for specific types of customers in various times and locations and has a limit
on the maximum available budget. Each query is in the form of time and location
and is associated with one individual customer. In order to achieve a
non-intrusive delivery, only a limited number of ads can be sent to each
customer. Recently, new services have been introduced that offer location-based
advertising over cellular network that fit in our model (e.g., ShopAlerts by
AT&T) .
  We consider both online and offline version of the AdCell problem and develop
approximation algorithms with constant competitive ratio. For the online
version, we assume that the appearances of the queries follow a stochastic
distribution and thus consider a Bayesian setting. Furthermore, queries may
come from different distributions on different times. This model generalizes
several previous advertising models such as online secretary problem
\cite{HKP04}, online bipartite matching \cite{KVV90,FMMM09} and AdWords
\cite{saberi05}. ...


Distributed Data Placement via Graph Partitioning

  With the widespread use of shared-nothing clusters of servers, there has been
a proliferation of distributed object stores that offer high availability,
reliability and enhanced performance for MapReduce-style workloads. However,
relational workloads cannot always be evaluated efficiently using MapReduce
without extensive data migrations, which cause network congestion and reduced
query throughput. We study the problem of computing data placement strategies
that minimize the data communication costs incurred by typical relational query
workloads in a distributed setting.
  Our main contribution is a reduction of the data placement problem to the
well-studied problem of {\sc Graph Partitioning}, which is NP-Hard but for
which efficient approximation algorithms exist. The novelty and significance of
this result lie in representing the communication cost exactly and using
standard graphs instead of hypergraphs, which were used in prior work on data
placement that optimized for different objectives (not communication cost).
  We study several practical extensions of the problem: with load balancing,
with replication, with materialized views, and with complex query plans
consisting of sequences of intermediate operations that may be computed on
different servers. We provide integer linear programs (IPs) that may be used
with any IP solver to find an optimal data placement. For the no-replication
case, we use publicly available graph partitioning libraries (e.g., METIS) to
efficiently compute nearly-optimal solutions. For the versions with
replication, we introduce two heuristics that utilize the {\sc Graph
Partitioning} solution of the no-replication case. Using the TPC-DS workload,
it may take an IP solver weeks to compute an optimal data placement, whereas
our reduction produces nearly-optimal solutions in seconds.


The Geometric Block Model

  To capture the inherent geometric features of many community detection
problems, we propose to use a new random graph model of communities that we
call a Geometric Block Model. The geometric block model generalizes the random
geometric graphs in the same way that the well-studied stochastic block model
generalizes the Erdos-Renyi random graphs. It is also a natural extension of
random community models inspired by the recent theoretical and practical
advancement in community detection. While being a topic of fundamental
theoretical interest, our main contribution is to show that many practical
community structures are better explained by the geometric block model. We also
show that a simple triangle-counting algorithm to detect communities in the
geometric block model is near-optimal. Indeed, even in the regime where the
average degree of the graph grows only logarithmically with the number of
vertices (sparse-graph), we show that this algorithm performs extremely well,
both theoretically and practically. In contrast, the triangle-counting
algorithm is far from being optimum for the stochastic block model. We simulate
our results on both real and synthetic datasets to show superior performance of
both the new model as well as our algorithm.


Connectivity in Random Annulus Graphs and the Geometric Block Model

  We provide new connectivity results for {\em vertex-random graphs} or {\em
random annulus graphs} which are significant generalizations of random
geometric graphs. Random geometric graphs (RGG) are one of the most basic
models of random graphs for spatial networks proposed by Gilbert in 1961,
shortly after the introduction of the Erd\H{o}s-R\'{en}yi random graphs. They
resemble social networks in many ways (e.g. by spontaneously creating cluster
of nodes with high modularity). The connectivity properties of RGG have been
studied since its introduction, and analyzing them has been significantly
harder than their Erd\H{o}s-R\'{en}yi counterparts due to correlated edge
formation.
  Our next contribution is in using the connectivity of random annulus graphs
to provide necessary and sufficient conditions for efficient recovery of
communities for {\em the geometric block model} (GBM). The GBM is a
probabilistic model for community detection defined over an RGG in a similar
spirit as the popular {\em stochastic block model}, which is defined over an
Erd\H{o}s-R\'{en}yi random graph. The geometric block model inherits the
transitivity properties of RGGs and thus models communities better than a
stochastic block model. However, analyzing them requires fresh perspectives as
all prior tools fail due to correlation in edge formation. We provide a simple
and efficient algorithm that can recover communities in GBM exactly with high
probability in the regime of connectivity.


Clustering with Noisy Queries

  In this paper, we initiate a rigorous theoretical study of clustering with
noisy queries (or a faulty oracle). Given a set of $n$ elements, our goal is to
recover the true clustering by asking minimum number of pairwise queries to an
oracle. Oracle can answer queries of the form : "do elements $u$ and $v$ belong
to the same cluster?" -- the queries can be asked interactively (adaptive
queries), or non-adaptively up-front, but its answer can be erroneous with
probability $p$. In this paper, we provide the first information theoretic
lower bound on the number of queries for clustering with noisy oracle in both
situations. We design novel algorithms that closely match this query complexity
lower bound, even when the number of clusters is unknown. Moreover, we design
computationally efficient algorithms both for the adaptive and non-adaptive
settings. The problem captures/generalizes multiple application scenarios. It
is directly motivated by the growing body of work that use crowdsourcing for
{\em entity resolution}, a fundamental and challenging data mining task aimed
to identify all records in a database referring to the same entity. Here crowd
represents the noisy oracle, and the number of queries directly relates to the
cost of crowdsourcing. Another application comes from the problem of {\em sign
edge prediction} in social network, where social interactions can be both
positive and negative, and one must identify the sign of all pair-wise
interactions by querying a few pairs. Furthermore, clustering with noisy oracle
is intimately connected to correlation clustering, leading to improvement
therein. Finally, it introduces a new direction of study in the popular {\em
stochastic block model} where one has an incomplete stochastic block model
matrix to recover the clusters.


Query Complexity of Clustering with Side Information

  Suppose, we are given a set of $n$ elements to be clustered into $k$
(unknown) clusters, and an oracle/expert labeler that can interactively answer
pair-wise queries of the form, "do two elements $u$ and $v$ belong to the same
cluster?". The goal is to recover the optimum clustering by asking the minimum
number of queries. In this paper, we initiate a rigorous theoretical study of
this basic problem of query complexity of interactive clustering, and provide
strong information theoretic lower bounds, as well as nearly matching upper
bounds. Most clustering problems come with a similarity matrix, which is used
by an automated process to cluster similar points together. Our main
contribution in this paper is to show the dramatic power of side information
aka similarity matrix on reducing the query complexity of clustering. A
similarity matrix represents noisy pair-wise relationships such as one computed
by some function on attributes of the elements. A natural noisy model is where
similarity values are drawn independently from some arbitrary probability
distribution $f_+$ when the underlying pair of elements belong to the same
cluster, and from some $f_-$ otherwise. We show that given such a similarity
matrix, the query complexity reduces drastically from $\Theta(nk)$ (no
similarity matrix) to $O(\frac{k^2\log{n}}{\cH^2(f_+\|f_-)})$ where $\cH^2$
denotes the squared Hellinger divergence. Moreover, this is also
information-theoretic optimal within an $O(\log{n})$ factor. Our algorithms are
all efficient, and parameter free, i.e., they work without any knowledge of $k,
f_+$ and $f_-$, and only depend logarithmically with $n$. Along the way, our
work also reveals intriguing connection to popular community detection models
such as the {\em stochastic block model}, significantly generalizes them, and
opens up many venues for interesting future research.


