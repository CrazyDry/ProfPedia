A New Approximation Technique for Resource-Allocation Problems

  We develop a rounding method based on random walks in polytopes, which leadsto improved approximation algorithms and integrality gaps for severalassignment problems that arise in resource allocation and scheduling. Inparticular, it generalizes the work of Shmoys and Tardos on the generalizedassignment problem to the setting where some jobs can be dropped. Newconcentration bounds for random bipartite matching are developed as well.

New Constructive Aspects of the Lovasz Local Lemma

  The Lov\'{a}sz Local Lemma (LLL) states that the probability that none of aset of "bad" events happens is nonzero if the probability of each event issmall compared to the number of bad events it depends on. A series of resultshave provided algorithms to efficiently construct structures whose existence is(non-constructively) guaranteed by the full asymmetric LLL, culminating in therecent breakthrough of Moser & Tardos. We show that the output distribution ofthe Moser-Tardos procedure has sufficient randomness, leading to two classes ofalgorithmic applications. We first show that when an LLL application provides asmall amount of slack, the running time of the Moser-Tardos algorithm ispolynomial in the number of underlying independent variables (not events!), andcan thus be used to give efficient constructions in cases where the underlyingproof applies the LLL to super-polynomially many events (or where finding a badevent that holds is computationally hard). We demonstrate our method onapplications including: the first constant-factor approximation algorithm forthe Santa Claus problem, as well as efficient algorithms for acyclic edgecoloring, non-repetitive graph colorings, and Ramsey-type graphs. Second, weshow applications to cases where a few of the bad events can hold, leading tothe first such algorithmic applications of the LLL: MAX $k$-SAT is anillustrative example of this.

Efficiently Computing Edit Distance to Dyck Language

  Given a string $\sigma$ over alphabet $\Sigma$ and a grammar $G$ defined overthe same alphabet, how many minimum number of repairs: insertions, deletionsand substitutions are required to map $\sigma$ into a valid member of $G$ ? Weinvestigate this basic question in this paper for $Dyck(s)$. $Dyck(s)$ is afundamental context free grammar representing the language of well-balancedparentheses with s different types of parentheses and has played a pivotal rolein the development of theory of context free languages. Computing edit distanceto $Dyck(s)$ significantly generalizes string edit distance problem and hasnumerous applications ranging from repairing semi-structured documents such asXML to memory checking, automated compiler optimization, natural languageprocessing etc.  In this paper we give the first near-linear time algorithm for edit distancecomputation to $Dyck(s)$ that achieves a nontrivial approximation factor of$O(\frac{1}{\epsilon}\log{OPT}(\log{n})^{\frac{1}{\epsilon}})$ in$O(n^{1+\epsilon}\log{n})$ time. In fact, given there exists an algorithm forcomputing string edit distance on input of size $n$ in $\alpha(n)$ time with$\beta(n)$-approximation factor, we can devise an algorithm for edit distanceproblem to $Dyck(s)$ running in $\tilde{O}(n^{1+\epsilon}+\alpha(n))$ andachieving an approximation factor of $O(\frac{1}{\epsilon}\beta(n)\log{OPT})$.  We show that the framework for efficiently approximating edit distance to$Dyck(s)$ can be applied to many other languages. We illustrate this byconsidering various memory checking languages which comprise of validtranscripts of stacks, queues, priority queues, double-ended queues etc.Therefore, any language that can be recognized by these data structures, canalso be repaired efficiently by our algorithm.

Truly Sub-cubic Algorithms for Language Edit Distance and RNA Folding  via Fast Bounded-Difference Min-Plus Product

  It is a major open problem whether the $(\min,+)$-product of two $n\times n$matrices has a truly sub-cubic (i.e. $O(n^{3-\epsilon})$ for $\epsilon>0$) timealgorithm, in particular since it is equivalent to the famousAll-Pairs-Shortest-Paths problem (APSP) in $n$-vertex graphs. Some restrictionsof the $(\min,+)$-product to special types of matrices are known to admit trulysub-cubic algorithms, each giving rise to a special case of APSP that can besolved faster. In this paper we consider a new, different and powerfulrestriction in which all matrix entries are integers and one matrix can bearbitrary, as long as the other matrix has "bounded differences" in either itscolumns or rows, i.e. any two consecutive entries differ by only a smallamount. We obtain the first truly sub-cubic algorithm for thisbounded-difference $(\min,+)$-product (answering an open problem of Chan andLewenstein).  Our new algorithm, combined with a strengthening of an approach of L.~Valiantfor solving context-free grammar parsing with matrix multiplication, yields thefirst truly sub-cubic algorithms for the following problems: Language EditDistance (a major problem in the parsing community), RNA-folding (a majorproblem in bioinformatics) and Optimum Stack Generation (answering an openproblem of Tarjan).

Language Edit Distance & Maximum Likelihood Parsing of Stochastic  Grammars: Faster Algorithms & Connection to Fundamental Graph Problems

  Given a context free language $L(G)$ over alphabet $\Sigma$ and a string $s\in \Sigma^*$, the language edit distance (Lan-ED) problem seeks the minimumnumber of edits (insertions, deletions and substitutions) required to convert$s$ into a valid member of $L(G)$. The well-known dynamic programming algorithmsolves this problem in $O(n^3)$ time (ignoring grammar size) where $n$ is thestring length [Aho, Peterson 1972, Myers 1985]. Despite its vast number ofapplications, there is no algorithm known till date that computes orapproximates Lan-ED in true sub-cubic time.  In this paper we give the first such algorithm that computes Lan-ED almostoptimally. For any arbitrary $\epsilon > 0$, our algorithm runs in$\tilde{O}(\frac{n^{\omega}}{poly(\epsilon)})$ time and returns an estimatewithin a multiplicative approximation factor of $(1+\epsilon)$, where $\omega$is the exponent of ordinary matrix multiplication of $n$ dimensional squarematrices. It also computes the edit script. Further, for all substrings of $s$,we can estimate their Lan-ED within $(1\pm \epsilon)$ factor in$\tilde{O}(\frac{n^{\omega}}{poly(\epsilon)})$ time with high probability. Wealso design the very first sub-cubic ($\tilde{O}(n^\omega)$) algorithm tohandle arbitrary stochastic context free grammar (SCFG) parsing. SCFGs lie atthe foundation of statistical natural language processing, they generalizehidden Markov models, and have found widespread applications.  To complement our upper bound result, we show that exact computation of SCFGparsing, or Lan-ED with insertion as only edit operation in true sub-cubic timewill imply a truly sub-cubic algorithm for all-pairs shortest paths, and henceto a large range of problems in graphs and matrices. Known lower bound resultson parsing implies no improvement over our time bound of $O(n^\omega)$ ispossible for any nontrivial multiplicative approximation.

Clustering Via Crowdsourcing

  In recent years, crowdsourcing, aka human aided computation has emerged as aneffective platform for solving problems that are considered complex formachines alone. Using human is time-consuming and costly due to monetarycompensations. Therefore, a crowd based algorithm must judiciously use anyinformation computed through an automated process, and ask minimum number ofquestions to the crowd adaptively.  One such problem which has received significant attention is {\em entityresolution}. Formally, we are given a graph $G=(V,E)$ with unknown edge set $E$where $G$ is a union of $k$ (again unknown, but typically large $O(n^\alpha)$,for $\alpha>0$) disjoint cliques $G_i(V_i, E_i)$, $i =1, \dots, k$. The goal isto retrieve the sets $V_i$s by making minimum number of pair-wise queries $V\times V\to\{\pm1\}$ to an oracle (the crowd). When the answer to each query iscorrect, e.g. via resampling, then this reduces to finding connected componentsin a graph. On the other hand, when crowd answers may be incorrect, itcorresponds to clustering over minimum number of noisy inputs. Even, withperfect answers, a simple lower and upper bound of $\Theta(nk)$ on querycomplexity can be shown. A major contribution of this paper is to reduce thequery complexity to linear or even sublinear in $n$ when mild side informationis provided by a machine, and even in presence of crowd errors which are notcorrectable via resampling. We develop new information theoretic lower boundson the query complexity of clustering with side information and errors, and ourupper bounds closely match with them. Our algorithms are naturallyparallelizable, and also give near-optimal bounds on the number of adaptiverounds required to match the query complexity.

Dynamic Set Cover: Improved Algorithms & Lower Bounds

  Set cover is a classic problem in combinatorial optimization where a set of$n$ elements have to be covered by a minimum number of subsets from a givencollection of size $m$. The two traditional lines of inquiry for this problemare via greedy and primal dual algorithms, and respectively yield (tight)approximation factors of $\ln n$, where $n$ is the total number of elements,and $f$, where every element belongs to at most $f$ sets. Recent research hasfocused on the dynamic setting, where the set of elements changes over time.Using the same lines of inquiry, this has led to the following results: (a) an$O(\log n)$-approximation in $O(f \log n)$ (amortized) update time (Gupta {\emet al.}, STOC 2017), and (b) an $O(f^2)$-approximation in $O(f \log (m+n))$(amortized) update time (Bhattacharya {\em et al.}, ICALP 2015). While theformer result matches the offline approximation within a constant factor, thelatter does not; indeed, the only $O(f)$-approximation known in the dynamicsetting is by re-solving the problem after every update.  In this paper, we show that it is possible to maintain efficiently a solution(almost) as good as the primal-dual offline one: we give a $(1+\epsilon)f$-approximation for set cover in $O(f^2\log n/\epsilon)$ (amortized) updatetime. If we are in a decremental setting, i.e., there are element deletions butno insertions, the update time can be improved to $O(f^2/\epsilon)$, whilestill obtaining an $(1+\epsilon) f$-approximation. Finally, we study thedependence of the update time on $f$. Using the recent distributedPCP-framework, we show that any dynamic algorithm for set cover that has anamortized update time of $O(f^{1-\epsilon})$ must have an approximation factorthat is $\Omega(n^\delta)$ for some $\delta>0$ under the Strong ExponentialTime Hypothesis.

A Constant Factor Approximation Algorithm for Fault-Tolerant k-Median

  In this paper, we consider the fault-tolerant $k$-median problem and give the\emph{first} constant factor approximation algorithm for it. In thefault-tolerant generalization of classical $k$-median problem, each client $j$needs to be assigned to at least $r_j \ge 1$ distinct open facilities. Theservice cost of $j$ is the sum of its distances to the $r_j$ facilities, andthe $k$-median constraint restricts the number of open facilities to at most$k$. Previously, a constant factor was known only for the special case when all$r_j$s are the same, and a logarithmic approximation ratio for the generalcase. In addition, we present the first polynomial time algorithm for thefault-tolerant $k$-median problem on a path or a HST by showing that thecorresponding LP always has an integral optimal solution.  We also consider the fault-tolerant facility location problem, where theservice cost of $j$ can be a weighted sum of its distance to the $r_j$facilities. We give a simple constant factor approximation algorithm,generalizing several previous results which only work for nonincreasing weightvectors.

A Theoretical Analysis of First Heuristics of Crowdsourced Entity  Resolution

  Entity resolution (ER) is the task of identifying all records in a databasethat refer to the same underlying entity, and are therefore duplicates of eachother. Due to inherent ambiguity of data representation and poor data quality,ER is a challenging task for any automated process. As a remedy, human-poweredER via crowdsourcing has become popular in recent years. Using crowd to answerqueries is costly and time consuming. Furthermore, crowd-answers can often befaulty. Therefore, crowd-based ER methods aim to minimize human participationwithout sacrificing the quality and use a computer generated similarity matrixactively. While, some of these methods perform well in practice, no theoreticalanalysis exists for them, and further their worst case performances do notreflect the experimental findings. This creates a disparity in theunderstanding of the popular heuristics for this problem. In this paper, wemake the first attempt to close this gap. We provide a thorough analysis of theprominent heuristic algorithms for crowd-based ER. We justify experimentalobservations with our analysis and information theoretic lower bounds.

A Unified Approach to Ranking in Probabilistic Databases

  The dramatic growth in the number of application domains that naturallygenerate probabilistic, uncertain data has resulted in a need for efficientlysupporting complex querying and decision-making over such data. In this paper,we present a unified approach to ranking and top-k query processing inprobabilistic databases by viewing it as a multi-criteria optimization problem,and by deriving a set of features that capture the key properties of aprobabilistic dataset that dictate the ranked result. We contend that a single,specific ranking function may not suffice for probabilistic databases, and weinstead propose two parameterized ranking functions, called PRF-w and PRF-e,that generalize or can approximate many of the previously proposed rankingfunctions. We present novel generating functions-based algorithms forefficiently ranking large datasets according to these ranking functions, evenif the datasets exhibit complex correlations modeled using probabilisticand/xor trees or Markov networks. We further propose that the parameters of theranking function be learned from user preferences, and we develop an approachto learn those parameters. Finally, we present a comprehensive experimentalstudy that illustrates the effectiveness of our parameterized rankingfunctions, especially PRF-e, at approximating other ranking functions and thescalability of our proposed algorithms for exact or approximate ranking.

Energy Efficient Scheduling via Partial Shutdown

  Motivated by issues of saving energy in data centers we define a collectionof new problems referred to as "machine activation" problems. The centralframework we introduce considers a collection of $m$ machines (unrelated orrelated) with each machine $i$ having an {\em activation cost} of $a_i$. Thereis also a collection of $n$ jobs that need to be performed, and $p_{i,j}$ isthe processing time of job $j$ on machine $i$. We assume that there is anactivation cost budget of $A$ -- we would like to {\em select} a subset $S$ ofthe machines to activate with total cost $a(S) \le A$ and {\em find} a schedulefor the $n$ jobs on the machines in $S$ minimizing the makespan (or any othermetric).  For the general unrelated machine activation problem, our main results arethat if there is a schedule with makespan $T$ and activation cost $A$ then wecan obtain a schedule with makespan $\makespanconstant T$ and activation cost$\costconstant A$, for any $\epsilon >0$. We also consider assignment costs forjobs as in the generalized assignment problem, and using our framework, providealgorithms that minimize the machine activation and the assignment costsimultaneously. In addition, we present a greedy algorithm which only works forthe basic version and yields a makespan of $2T$ and an activation cost $A(1+\ln n)$.  For the uniformly related parallel machine scheduling problem, we develop apolynomial time approximation scheme that outputs a schedule with the propertythat the activation cost of the subset of machines is at most $A$ and themakespan is at most $(1+\epsilon) T$ for any $\epsilon >0$.

AdCell: Ad Allocation in Cellular Networks

  With more than four billion usage of cellular phones worldwide, mobileadvertising has become an attractive alternative to online advertisements. Inthis paper, we propose a new targeted advertising policy for Wireless ServiceProviders (WSPs) via SMS or MMS- namely {\em AdCell}. In our model, a WSPcharges the advertisers for showing their ads. Each advertiser has a valuationfor specific types of customers in various times and locations and has a limiton the maximum available budget. Each query is in the form of time and locationand is associated with one individual customer. In order to achieve anon-intrusive delivery, only a limited number of ads can be sent to eachcustomer. Recently, new services have been introduced that offer location-basedadvertising over cellular network that fit in our model (e.g., ShopAlerts byAT&T) .  We consider both online and offline version of the AdCell problem and developapproximation algorithms with constant competitive ratio. For the onlineversion, we assume that the appearances of the queries follow a stochasticdistribution and thus consider a Bayesian setting. Furthermore, queries maycome from different distributions on different times. This model generalizesseveral previous advertising models such as online secretary problem\cite{HKP04}, online bipartite matching \cite{KVV90,FMMM09} and AdWords\cite{saberi05}. ...

Distributed Data Placement via Graph Partitioning

  With the widespread use of shared-nothing clusters of servers, there has beena proliferation of distributed object stores that offer high availability,reliability and enhanced performance for MapReduce-style workloads. However,relational workloads cannot always be evaluated efficiently using MapReducewithout extensive data migrations, which cause network congestion and reducedquery throughput. We study the problem of computing data placement strategiesthat minimize the data communication costs incurred by typical relational queryworkloads in a distributed setting.  Our main contribution is a reduction of the data placement problem to thewell-studied problem of {\sc Graph Partitioning}, which is NP-Hard but forwhich efficient approximation algorithms exist. The novelty and significance ofthis result lie in representing the communication cost exactly and usingstandard graphs instead of hypergraphs, which were used in prior work on dataplacement that optimized for different objectives (not communication cost).  We study several practical extensions of the problem: with load balancing,with replication, with materialized views, and with complex query plansconsisting of sequences of intermediate operations that may be computed ondifferent servers. We provide integer linear programs (IPs) that may be usedwith any IP solver to find an optimal data placement. For the no-replicationcase, we use publicly available graph partitioning libraries (e.g., METIS) toefficiently compute nearly-optimal solutions. For the versions withreplication, we introduce two heuristics that utilize the {\sc GraphPartitioning} solution of the no-replication case. Using the TPC-DS workload,it may take an IP solver weeks to compute an optimal data placement, whereasour reduction produces nearly-optimal solutions in seconds.

The Geometric Block Model

  To capture the inherent geometric features of many community detectionproblems, we propose to use a new random graph model of communities that wecall a Geometric Block Model. The geometric block model generalizes the randomgeometric graphs in the same way that the well-studied stochastic block modelgeneralizes the Erdos-Renyi random graphs. It is also a natural extension ofrandom community models inspired by the recent theoretical and practicaladvancement in community detection. While being a topic of fundamentaltheoretical interest, our main contribution is to show that many practicalcommunity structures are better explained by the geometric block model. We alsoshow that a simple triangle-counting algorithm to detect communities in thegeometric block model is near-optimal. Indeed, even in the regime where theaverage degree of the graph grows only logarithmically with the number ofvertices (sparse-graph), we show that this algorithm performs extremely well,both theoretically and practically. In contrast, the triangle-countingalgorithm is far from being optimum for the stochastic block model. We simulateour results on both real and synthetic datasets to show superior performance ofboth the new model as well as our algorithm.

Connectivity in Random Annulus Graphs and the Geometric Block Model

  We provide new connectivity results for {\em vertex-random graphs} or {\emrandom annulus graphs} which are significant generalizations of randomgeometric graphs. Random geometric graphs (RGG) are one of the most basicmodels of random graphs for spatial networks proposed by Gilbert in 1961,shortly after the introduction of the Erd\H{o}s-R\'{en}yi random graphs. Theyresemble social networks in many ways (e.g. by spontaneously creating clusterof nodes with high modularity). The connectivity properties of RGG have beenstudied since its introduction, and analyzing them has been significantlyharder than their Erd\H{o}s-R\'{en}yi counterparts due to correlated edgeformation.  Our next contribution is in using the connectivity of random annulus graphsto provide necessary and sufficient conditions for efficient recovery ofcommunities for {\em the geometric block model} (GBM). The GBM is aprobabilistic model for community detection defined over an RGG in a similarspirit as the popular {\em stochastic block model}, which is defined over anErd\H{o}s-R\'{en}yi random graph. The geometric block model inherits thetransitivity properties of RGGs and thus models communities better than astochastic block model. However, analyzing them requires fresh perspectives asall prior tools fail due to correlation in edge formation. We provide a simpleand efficient algorithm that can recover communities in GBM exactly with highprobability in the regime of connectivity.

Clustering with Noisy Queries

  In this paper, we initiate a rigorous theoretical study of clustering withnoisy queries (or a faulty oracle). Given a set of $n$ elements, our goal is torecover the true clustering by asking minimum number of pairwise queries to anoracle. Oracle can answer queries of the form : "do elements $u$ and $v$ belongto the same cluster?" -- the queries can be asked interactively (adaptivequeries), or non-adaptively up-front, but its answer can be erroneous withprobability $p$. In this paper, we provide the first information theoreticlower bound on the number of queries for clustering with noisy oracle in bothsituations. We design novel algorithms that closely match this query complexitylower bound, even when the number of clusters is unknown. Moreover, we designcomputationally efficient algorithms both for the adaptive and non-adaptivesettings. The problem captures/generalizes multiple application scenarios. Itis directly motivated by the growing body of work that use crowdsourcing for{\em entity resolution}, a fundamental and challenging data mining task aimedto identify all records in a database referring to the same entity. Here crowdrepresents the noisy oracle, and the number of queries directly relates to thecost of crowdsourcing. Another application comes from the problem of {\em signedge prediction} in social network, where social interactions can be bothpositive and negative, and one must identify the sign of all pair-wiseinteractions by querying a few pairs. Furthermore, clustering with noisy oracleis intimately connected to correlation clustering, leading to improvementtherein. Finally, it introduces a new direction of study in the popular {\emstochastic block model} where one has an incomplete stochastic block modelmatrix to recover the clusters.

Query Complexity of Clustering with Side Information

  Suppose, we are given a set of $n$ elements to be clustered into $k$(unknown) clusters, and an oracle/expert labeler that can interactively answerpair-wise queries of the form, "do two elements $u$ and $v$ belong to the samecluster?". The goal is to recover the optimum clustering by asking the minimumnumber of queries. In this paper, we initiate a rigorous theoretical study ofthis basic problem of query complexity of interactive clustering, and providestrong information theoretic lower bounds, as well as nearly matching upperbounds. Most clustering problems come with a similarity matrix, which is usedby an automated process to cluster similar points together. Our maincontribution in this paper is to show the dramatic power of side informationaka similarity matrix on reducing the query complexity of clustering. Asimilarity matrix represents noisy pair-wise relationships such as one computedby some function on attributes of the elements. A natural noisy model is wheresimilarity values are drawn independently from some arbitrary probabilitydistribution $f_+$ when the underlying pair of elements belong to the samecluster, and from some $f_-$ otherwise. We show that given such a similaritymatrix, the query complexity reduces drastically from $\Theta(nk)$ (nosimilarity matrix) to $O(\frac{k^2\log{n}}{\cH^2(f_+\|f_-)})$ where $\cH^2$denotes the squared Hellinger divergence. Moreover, this is alsoinformation-theoretic optimal within an $O(\log{n})$ factor. Our algorithms areall efficient, and parameter free, i.e., they work without any knowledge of $k,f_+$ and $f_-$, and only depend logarithmically with $n$. Along the way, ourwork also reveals intriguing connection to popular community detection modelssuch as the {\em stochastic block model}, significantly generalizes them, andopens up many venues for interesting future research.

