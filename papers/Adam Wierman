The Empirical Implications of Privacy-Aware Choice

  This paper initiates the study of the testable implications of choice data in
settings where agents have privacy preferences. We adapt the standard
conceptualization of consumer choice theory to a situation where the consumer
is aware of, and has preferences over, the information revealed by her choices.
The main message of the paper is that little can be inferred about consumers'
preferences once we introduce the possibility that the consumer has concerns
about privacy. This holds even when consumers' privacy preferences are assumed
to be monotonic and separable. This motivates the consideration of stronger
assumptions and, to that end, we introduce an additive model for privacy
preferences that does have testable implications.


A Tale of Two Metrics: Simultaneous Bounds on Competitiveness and Regret

  We consider algorithms for "smoothed online convex optimization" problems, a
variant of the class of online convex optimization problems that is strongly
related to metrical task systems. Prior literature on these problems has
focused on two performance metrics: regret and the competitive ratio. There
exist known algorithms with sublinear regret and known algorithms with constant
competitive ratios; however, no known algorithm achieves both simultaneously.
We show that this is due to a fundamental incompatibility between these two
metrics - no algorithm (deterministic or randomized) can achieve sublinear
regret and a constant competitive ratio, even in the case when the objective
functions are linear. However, we also exhibit an algorithm that, for the
important special case of one-dimensional decision spaces, provides sublinear
regret while maintaining a competitive ratio that grows arbitrarily slowly.


Percolation thresholds on 2D Voronoi networks and Delaunay
  triangulations

  The site percolation threshold for the random Voronoi network is determined
numerically for the first time, with the result p_c = 0.71410 +/- 0.00002,
using Monte-Carlo simulation on periodic systems of up to 40000 sites. The
result is very close to the recent theoretical estimate p_c = 0.7151 of Neher,
Mecke, and Wagner. For the bond threshold on the Voronoi network, we find p_c =
0.666931 +/- 0.000005, implying that for its dual, the Delaunay triangulation,
p_c = 0.333069 +/- 0.000005. These results rule out the conjecture by Hsu and
Huang that the bond thresholds are 2/3 and 1/3 respectively, but support the
conjecture of Wierman that for fully triangulated lattices other than the
regular triangular lattice, the bond threshold is less than 2 sin pi/18 =
0.3473.


Incentives for P2P-Assisted Content Distribution: If You Can't Beat 'Em,
  Join 'Em

  The rapid growth of content distribution on the Internet has brought with it
proportional increases in the costs of distributing content. Adding to
distribution costs is the fact that digital content is easily duplicable, and
hence can be shared in an illicit peer-to-peer (P2P) manner that generates no
revenue for the content provider. In this paper, we study whether the content
provider can recover lost revenue through a more innovative approach to
distribution. In particular, we evaluate the benefits of a hybrid
revenue-sharing system that combines a legitimate P2P swarm and a centralized
client-server approach. We show how the revenue recovered by the content
provider using a server-supported legitimate P2P swarm can exceed that of the
monopolistic scheme by an order of magnitude. Our analytical results are
obtained in a fluid model, and supported by stochastic simulations.


Characterizing the Impact of the Workload on the Value of Dynamic
  Resizing in Data Centers

  Energy consumption imposes a significant cost for data centers; yet much of
that energy is used to maintain excess service capacity during periods of
predictably low load. Resultantly, there has recently been interest in
developing designs that allow the service capacity to be dynamically resized to
match the current workload. However, there is still much debate about the value
of such approaches in real settings. In this paper, we show that the value of
dynamic resizing is highly dependent on statistics of the workload process. In
particular, both slow time-scale non-stationarities of the workload (e.g., the
peak-to-mean ratio) and the fast time-scale stochasticity (e.g., the burstiness
of arrivals) play key roles. To illustrate the impact of these factors, we
combine optimization-based modeling of the slow time-scale with stochastic
modeling of the fast time scale. Within this framework, we provide both
analytic and numerical results characterizing when dynamic resizing does (and
does not) provide benefits.


Approximate dynamic programming using fluid and diffusion approximations
  with applications to power management

  Neuro-dynamic programming is a class of powerful techniques for approximating
the solution to dynamic programming equations. In their most computationally
attractive formulations, these techniques provide the approximate solution only
within a prescribed finite-dimensional function class. Thus, the question that
always arises is how should the function class be chosen? The goal of this
paper is to propose an approach using the solutions to associated fluid and
diffusion approximations. In order to illustrate this approach, the paper
focuses on an application to dynamic speed scaling for power management in
computer processors.


Online Convex Optimization Using Predictions

  Making use of predictions is a crucial, but under-explored, area of online
algorithms. This paper studies a class of online optimization problems where we
have external noisy predictions available. We propose a stochastic prediction
error model that generalizes prior models in the learning and stochastic
control communities, incorporates correlation among prediction errors, and
captures the fact that predictions improve as time passes. We prove that
achieving sublinear regret and constant competitive ratio for online algorithms
requires the use of an unbounded prediction window in adversarial settings, but
that under more realistic stochastic prediction error models it is possible to
use Averaging Fixed Horizon Control (AFHC) to simultaneously achieve sublinear
regret and constant competitive ratio in expectation using only a
constant-sized prediction window. Furthermore, we show that the performance of
AFHC is tightly concentrated around its mean.


Greening Multi-Tenant Data Center Demand Response

  Data centers have emerged as promising resources for demand response,
particularly for emergency demand response (EDR), which saves the power grid
from incurring blackouts during emergency situations. However, currently, data
centers typically participate in EDR by turning on backup (diesel) generators,
which is both expensive and environmentally unfriendly. In this paper, we focus
on "greening" demand response in multi-tenant data centers, i.e., colocation
data centers, by designing a pricing mechanism through which the data center
operator can efficiently extract load reductions from tenants during emergency
periods to fulfill energy reduction requirement for EDR. In particular, we
propose a pricing mechanism for both mandatory and voluntary EDR programs,
ColoEDR, that is based on parameterized supply function bidding and provides
provably near-optimal efficiency guarantees, both when tenants are price-taking
and when they are price-anticipating. In addition to analytic results, we
extend the literature on supply function mechanism design, and evaluate ColoEDR
using trace-based simulation studies. These validate the efficiency analysis
and conclude that the pricing mechanism is both beneficial to the environment
and to the data center operator (by decreasing the need for backup diesel
generation), while also aiding tenants (by providing payments for load
reductions).


On the Existence of Low-Rank Explanations for Mixed Strategy Behavior

  Nash equilibrium is used as a model to explain the observed behavior of
players in strategic settings. For example, in many empirical applications we
observe player behavior, and the problem is to determine if there exist payoffs
for the players for which the equilibrium corresponds to observed player
behavior. Computational complexity of Nash equilibria is an important
consideration in this framework. If the instance of the model that explains
observed player behavior requires players to have solved a computationally hard
problem, then the explanation provided is questionable. In this paper we
provide conditions under which Nash equilibrium is a reasonable explanation for
strategic behavior, i.e., conditions under which observed behavior of players
can be explained by games in which Nash equilibria are easy to compute. We
identify three structural conditions and show that if the data set of observed
behavior satisfies any of these conditions, then it is consistent with payoff
matrices for which the observed Nash equilibria could have been computed
efficiently. Our conditions admit large and structurally complex data sets of
observed behavior, showing that even with complexity considerations, Nash
equilibrium is often a reasonable model.


Opportunities for Price Manipulation by Aggregators in Electricity
  Markets

  Aggregators are playing an increasingly crucial role in the integration of
renewable generation in power systems. However, the intermittent nature of
renewable generation makes market interactions of aggregators difficult to
monitor and regulate, raising concerns about potential market manipulation by
aggregators. In this paper, we study this issue by quantifying the profit an
aggregator can obtain through strategic curtailment of generation in an
electricity market. We show that, while the problem of maximizing the benefit
from curtailment is hard in general, efficient algorithms exist when the
topology of the network is radial (acyclic). Further, we highlight that
significant increases in profit are possible via strategic curtailment in
practical settings.


On the Inefficiency of Forward Markets in Leader-Follower Competition

  Motivated by electricity markets, this paper studies the impact of forward
contracting in situations where firms have capacity constraints and
heterogeneous production lead times. We consider a model with two types of
firms - leaders and followers - that choose production at two different times.
Followers choose productions in the second stage but can sell forward contracts
in the first stage. Our main result is an explicit characterization of the
equilibrium outcomes. Classic results on forward contracting suggest that it
can mitigate market power in simple settings; however the results in this paper
show that the impact of forward markets in this setting is delicate - forward
contracting can enhance or mitigate market power. In particular, our results
show that leader-follower interactions created by heterogeneous production lead
times may cause forward markets to be inefficient, even when there are a large
number of followers. In fact, symmetric equilibria do not necessarily exist due
to differences in market power among the leaders and followers.


Distributional Analysis for Model Predictive Deferrable Load Control

  Deferrable load control is essential for handling the uncertainties
associated with the increasing penetration of renewable generation. Model
predictive control has emerged as an effective approach for deferrable load
control, and has received considerable attention. In particular, previous work
has analyzed the average-case performance of model predictive deferrable load
control. However, to this point, distributional analysis of model predictive
deferrable load control has been elusive. In this paper, we prove strong
concentration results on the distribution of the load variance obtained by
model predictive deferrable load control. These concentration results highlight
that the typical performance of model predictive deferrable load control is
tightly concentrated around the average-case performance.


The Role of a Market Maker in Networked Cournot Competition

  We study the role of a market maker (or market operator) in a transmission
constrained electricity market. We model the market as a one-shot networked
Cournot competition where generators supply quantity bids and load serving
entities provide downward sloping inverse demand functions. This mimics the
operation of a spot market in a deregulated market structure. In this paper, we
focus on possible mechanisms employed by the market maker to balance demand and
supply. In particular, we consider three candidate objective functions that the
market maker optimizes - social welfare, residual social welfare, and consumer
surplus. We characterize the existence of Generalized Nash Equilibrium (GNE) in
this setting and demonstrate that market outcomes at equilibrium can be very
different under the candidate objective functions.


Thinking Fast and Slow: Optimization Decomposition Across Timescales

  Many real-world control systems, such as the smart grid and human
sensorimotor control systems, have decentralized components that react quickly
using local information and centralized components that react slowly using a
more global view. This paper seeks to provide a theoretical framework for how
to design controllers that are decomposed across timescales in this way. The
framework is analogous to how the network utility maximization framework uses
optimization decomposition to distribute a global control problem across
independent controllers, each of which solves a local problem; except our goal
is to decompose a global problem temporally, extracting a timescale separation.
Our results highlight that decomposition of a multi-timescale controller into a
fast timescale, reactive controller and a slow timescale, predictive controller
can be near-optimal in a strong sense. In particular, we exhibit such a design,
named Multi-timescale Reflexive Predictive Control (MRPC), which maintains a
per-timestep cost within a constant factor of the offline optimal in an
adversarial setting.


The Empirical Implications of Rank in Bimatrix Games

  We study the structural complexity of bimatrix games, formalized via rank,
from an empirical perspective. We consider a setting where we have data on
player behavior in diverse strategic situations, but where we do not observe
the relevant payoff functions. We prove that high complexity (high rank) has
empirical consequences when arbitrary data is considered. Additionally, we
prove that, in more restrictive classes of data (termed laminar), any
observation is rationalizable using a low-rank game: specifically a zero-sum
game. Hence complexity as a structural property of a game is not always
testable. Finally, we prove a general result connecting the structure of the
feasible data sets with the highest rank that may be needed to rationalize a
set of observations.


The Cost of an Epidemic over a Complex Network: A Random Matrix Approach

  In this paper we quantify the total economic impact of an epidemic over a
complex network using tools from random matrix theory. Incorporating the direct
and indirect costs of infection, we calculate the disease cost in the large
graph limit for an SIS (Susceptible - Infected - Susceptible) infection
process. We also give an upper bound on this cost for arbitrary finite graphs
and illustrate both calculated costs using extensive simulations on random and
real-world networks. We extend these calculations by considering the total
social cost of an epidemic, accounting for both the immunization and disease
costs for various immunization strategies and determining the optimal
immunization. Our work focuses on the transient behavior of the epidemic, in
contrast to previous research, which typically focuses on determining the
steady-state system equilibrium.


Optimizing Energy Storage Participation in Emerging Power Markets

  The growing amount of intermittent renewables in power generation creates
challenges for real-time matching of supply and demand in the power grid.
Emerging ancillary power markets provide new incentives to consumers (e.g.,
electrical vehicles, data centers, and others) to perform demand response to
help stabilize the electricity grid. A promising class of potential demand
response providers includes energy storage systems (ESSs). This paper evaluates
the benefits of using various types of novel ESS technologies for a variety of
emerging smart grid demand response programs, such as regulation services
reserves (RSRs), contingency reserves, and peak shaving. We model, formulate
and solve optimization problems to maximize the net profit of ESSs in providing
each demand response. Our solution selects the optimal power and energy
capacities of the ESS, determines the optimal reserve value to provide as well
as the ESS real-time operational policy for program participation. Our results
highlight that applying ultra-capacitors and flywheels in RSR has the potential
to be up to 30 times more profitable than using common battery technologies
such as LI and LA batteries for peak shaving.


Joint Data Purchasing and Data Placement in a Geo-Distributed Data
  Market

  This paper studies two design tasks faced by a geo-distributed cloud data
market: which data to purchase (data purchasing) and where to place/replicate
the data for delivery (data placement). We show that the joint problem of data
purchasing and data placement within a cloud data market can be viewed as a
facility location problem, and is thus NP-hard. However, we give a provably
optimal algorithm for the case of a data market made up of a single data
center, and then generalize the structure from the single data center setting
in order to develop a near-optimal, polynomial-time algorithm for a
geo-distributed data market. The resulting design, Datum, decomposes the joint
purchasing and placement problem into two subproblems, one for data purchasing
and one for data placement, using a transformation of the underlying bandwidth
costs. We show, via a case study, that Datum is near-optimal (within 1.6%) in
practical settings.


Distributed optimization decomposition for joint economic dispatch and
  frequency regulation

  Economic dispatch and frequency regulation are typically viewed as
fundamentally different problems in power systems and, hence, are typically
studied separately. In this paper, we frame and study a joint problem that co-
optimizes both slow timescale economic dispatch resources and fast timescale
frequency regulation resources. We show how the joint problem can be decomposed
without loss of optimality into slow and fast timescale sub-problems that have
appealing interpretations as the economic dispatch and frequency regulation
problems respectively. We solve the fast timescale sub-problem using a
distributed frequency control algorithm that preserves the stability of the
network during transients. We solve the slow timescale sub-problem using an
efficient market mechanism that coordinates with the fast timescale
sub-problem. We investigate the performance of the decomposition on the IEEE
24-bus reliability test system.


On the Role of a Market Maker in Networked Cournot Competition

  We study Cournot competition among firms in a networked marketplace that is
centrally managed by a market maker. In particular, we study a situation in
which a market maker facilitates trade between geographically separate markets
via a constrained transport network. Our focus is on understanding the
consequences of the design of the market maker and on providing tools for
optimal design. To that end we provide a characterization of the equilibrium
outcomes of the game between the firms and the market maker. Our results
highlight that the equilibrium structure is impacted dramatically by the market
maker's objective - depending on the objective there may be a unique
equilibrium, multiple equilibria, or no equilibria. Further, the game may be a
potential game (as in the case of classical Cournot competition) or not. Beyond
characterizing the equilibria of the game, we provide an approach for designing
the market maker in order to optimize a design objective (e.g., social welfare)
at the equilibrium of the game. Additionally, we use our results to explore the
value of transport (trade) and the efficiency of the market maker (as compared
to a single, aggregate market).


A Parallelizable Acceleration Framework for Packing Linear Programs

  This paper presents an acceleration framework for packing linear programming
problems where the amount of data available is limited, i.e., where the number
of constraints m is small compared to the variable dimension n. The framework
can be used as a black box to speed up linear programming solvers dramatically,
by two orders of magnitude in our experiments. We present worst-case guarantees
on the quality of the solution and the speedup provided by the algorithm,
showing that the framework provides an approximately optimal solution while
running the original solver on a much smaller problem. The framework can be
used to accelerate exact solvers, approximate solvers, and parallel/distributed
solvers. Further, it can be used for both linear programs and integer linear
programs.


Third-Party Data Providers Ruin Simple Mechanisms

  This paper studies the revenue of simple mechanisms in settings where a
third-party data provider is present. When no data provider is present, it is
known that simple mechanisms achieve a constant fraction of the revenue of
optimal mechanisms. The results in this paper demonstrate that this is no
longer true in the presence of a third party data provider who can provide the
bidder with a signal that is correlated with the item type. Specifically, we
show that even with a single seller, a single bidder, and a single item of
uncertain type for sale, pricing each item-type separately (the analog of item
pricing for multi-item auctions) and bundling all item-types under a single
price (the analog of grand bundling) can both simultaneously be a logarithmic
factor worse than the optimal revenue. Further, in the presence of a data
provider, item-type partitioning mechanisms---a more general class of
mechanisms which divide item-types into disjoint groups and offer prices for
each group---still cannot achieve within a $\log \log$ factor of the optimal
revenue.


Smoothed Online Convex Optimization in High Dimensions via Online
  Balanced Descent

  We study Smoothed Online Convex Optimization, a version of online convex
optimization where the learner incurs a penalty for changing her actions
between rounds. Given a $\Omega(\sqrt{d})$ lower bound on the competitive ratio
of any online algorithm, where $d$ is the dimension of the action space, we ask
under what conditions this bound can be beaten. We introduce a novel
algorithmic framework for this problem, Online Balanced Descent (OBD), which
works by iteratively projecting the previous point onto a carefully chosen
level set of the current cost function so as to balance the switching costs and
hitting costs. We demonstrate the generality of the OBD framework by showing
how, with different choices of "balance," OBD can improve upon state-of-the-art
performance guarantees for both competitive ratio and regret, in particular,
OBD is the first algorithm to achieve a dimension-free competitive ratio, $3 +
O(1/\alpha)$, for locally polyhedral costs, where $\alpha$ measures the
"steepness" of the costs. We also prove bounds on the dynamic regret of OBD
when the balance is performed in the dual space that are dimension-free and
imply that OBD has sublinear static regret.


Loyalty Programs in the Sharing Economy: Optimality and Competition

  Loyalty programs are important tools for sharing platforms seeking to grow
supply. Online sharing platforms use loyalty programs to heavily subsidize
resource providers, encouraging participation and boosting supply. As the
sharing economy has evolved and competition has increased, the design of
loyalty programs has begun to play a crucial role in the pursuit of maximal
revenue. In this paper, we first characterize the optimal loyalty program for a
platform with homogeneous users. We then show that optimal revenue in a
heterogeneous market can be achieved by a class of multi-threshold loyalty
program (MTLP) which admits a simple implementation-friendly structure. We also
study the performance of loyalty programs in a setting with two competing
sharing platforms, showing that the degree of heterogeneity is a crucial factor
for both loyalty programs and pricing strategies. Our results show that
sophisticated loyalty programs that reward suppliers via stepwise linear
functions outperform simple sign-up bonuses, which give them a one time reward
for participating.


Smoothed Online Optimization for Regression and Control

  We consider Online Convex Optimization (OCO) in the setting where the costs
are $m$-strongly convex and the online learner pays a switching cost for
changing decisions between rounds. We show that the recently proposed Online
Balanced Descent (OBD) algorithm is constant competitive in this setting, with
competitive ratio $3 + O(1/m)$, irrespective of the ambient dimension.
Additionally, we show that when the sequence of cost functions is
$\epsilon$-smooth, OBD has near-optimal dynamic regret and maintains strong
per-round accuracy. We demonstrate the generality of our approach by showing
that the OBD framework can be used to construct competitive algorithms for a
variety of online problems across learning and control, including online
variants of ridge regression, logistic regression, maximum likelihood
estimation, and LQR control.


Transparency and Control in Platforms for Networked Markets

  In this work, we analyze the worst case efficiency loss of online platform
designs under a networked Cournot competition model. Inspired by some of the
largest platforms today, the platform designs considered tradeoffs between
transparency and control, namely, (i) open access, (ii) controlled allocation
and (iii) discriminatory access. Our results show that open access designs
incentivize increased production towards perfectly competitive levels and limit
efficiency loss, while controlled allocation designs lead to producer-platform
incentive misalignment, resulting in low participation and unbounded efficiency
loss. We also show that discriminatory access designs seek a balance between
transparency and control, and achieve the best of both worlds, maintaining high
participation rates while limiting efficiency loss. We also study a model of
consumer search cost which further distinguishes between the three designs.


Peer Effects and Stability in Matching Markets

  Many-to-one matching markets exist in numerous different forms, such as
college admissions, matching medical interns to hospitals for residencies,
assigning housing to college students, and the classic firms and workers
market. In all these markets, externalities such as complementarities and peer
effects severely complicate the preference ordering of each agent. Further,
research has shown that externalities lead to serious problems for market
stability and for developing efficient algorithms to find stable matchings. In
this paper we make the observation that peer effects are often the result of
underlying social connections, and we explore a formulation of the many-to-one
matching market where peer effects are derived from an underlying social
network. The key feature of our model is that it captures peer effects and
complementarities using utility functions, rather than traditional preference
ordering. With this model and considering a weaker notion of stability, namely
two-sided exchange stability, we prove that stable matchings always exist and
characterize the set of stable matchings in terms of social welfare. We also
give distributed algorithms that are guaranteed to converge to a two-sided
exchange stable matching. To assess the competitive ratio of these algorithms
and to more generally characterize the efficiency of matching markets with
externalities, we provide general bounds on how far the welfare of the
worst-case stable matching can be from the welfare of the optimal matching, and
find that the structure of the social network (e.g. how well clustered the
network is) plays a large role.


Potential Games are Necessary to Ensure Pure Nash Equilibria in Cost
  Sharing Games

  We consider the problem of designing distribution rules to share "welfare"
(cost or revenue) among individually strategic agents. There are many known
distribution rules that guarantee the existence of a (pure) Nash equilibrium in
this setting, e.g., the Shapley value and its weighted variants; however, a
characterization of the space of distribution rules that guarantee the
existence of a Nash equilibrium is unknown. Our work provides an exact
characterization of this space for a specific class of scalable and separable
games, which includes a variety of applications such as facility location,
routing, network formation, and coverage games. Given arbitrary local welfare
functions W, we prove that a distribution rule guarantees equilibrium existence
for all games (i.e., all possible sets of resources, agent action sets, etc.)
if and only if it is equivalent to a generalized weighted Shapley value on some
"ground" welfare functions W', which can be distinct from W. However, if
budget-balance is required in addition to the existence of a Nash equilibrium,
then W' must be the same as W. We also provide an alternate characterization of
this space in terms of "generalized" marginal contributions, which is more
appealing from the point of view of computational tractability. A possibly
surprising consequence of our result is that, in order to guarantee equilibrium
existence in all games with any fixed local welfare functions, it is necessary
to work within the class of potential games.


Prices and Subsidies in the Sharing Economy

  The growth of the sharing economy is driven by the emergence of sharing
platforms, e.g., Uber and Lyft, that match owners looking to share their
resources with customers looking to rent them. The design of such platforms is
a complex mixture of economics and engineering, and how to "optimally" design
such platforms is still an open problem. In this paper, we focus on the design
of prices and subsidies in sharing platforms. Our results provide insights into
the tradeoff between revenue maximizing prices and social welfare maximizing
prices. Specifically, we introduce a novel model of sharing platforms and
characterize the profit and social welfare maximizing prices in this model.
Further, we bound the efficiency loss under profit maximizing prices, showing
that there is a strong alignment between profit and efficiency in practical
settings. Our results highlight that the revenue of platforms may be limited in
practice due to supply shortages; thus platforms have a strong incentive to
encourage sharing via subsidies. We provide an analytic characterization of
when such subsidies are valuable and show how to optimize the size of the
subsidy provided. Finally, we validate the insights from our analysis using
data from Didi Chuxing, the largest ridesharing platform in China.


Failure Localization in Power Systems via Tree Partitions

  Cascading failures in power systems propagate non-locally, making the control
and mitigation of outages extremely hard. In this work, we use the emerging
concept of the tree partition of transmission networks to provide an analytical
characterization of line failure localizability in transmission systems. Our
results rigorously establish the well perceived intuition in power community
that failures cannot cross bridges, and reveal a finer-grained concept that
encodes more precise information on failure propagations within tree-partition
regions. Specifically, when a non-bridge line is tripped, the impact of this
failure only propagates within well-defined components, which we refer to as
cells, of the tree partition defined by the bridges. In contrast, when a bridge
line is tripped, the impact of this failure propagates globally across the
network, affecting the power flow on all remaining transmission lines. This
characterization suggests that it is possible to improve the system robustness
by temporarily switching off certain transmission lines, so as to create more,
smaller components in the tree partition; thus spatially localizing line
failures and making the grid less vulnerable to large-scale outages. We
illustrate this approach using the IEEE 118-bus test system and demonstrate
that switching off a negligible portion of transmission lines allows the impact
of line failures to be significantly more localized without substantial changes
in line congestion.


Newton Polytopes and Relative Entropy Optimization

  Newton polytopes play a prominent role in the study of sparse polynomial
systems, where they help formalize the idea that the root structure underlying
sparse polynomials of possibly high degree ought to still be "simple." In this
paper we consider sparse polynomial optimization problems, and we seek a deeper
understanding of the role played by Newton polytopes in this context. Our
investigation proceeds by reparametrizing polynomials as signomials -- which
are linear combinations of exponentials of linear functions in the decision
variable -- and studying the resulting signomial optimization problems.
Signomial programs represent an interesting (and generally intractable) class
of problems in their own right. We build on recent efforts that provide
tractable relative entropy convex relaxations to obtain bounds on signomial
programs. We describe several new structural results regarding these
relaxations as well as a range of conditions under which they solve signomial
programs exactly. The facial structure of the associated Newton polytopes plays
a prominent role in our analysis. Our results have consequences in two
directions, thus highlighting the utility of the signomial perspective. In one
direction, signomials have no notion of "degree"; therefore, techniques
developed for signomial programs depend only on the particular terms that
appear in a signomial. When specialized to the context of polynomials, we
obtain analysis and computational tools that only depend on the particular
monomials that constitute a sparse polynomial. In the other direction,
signomials represent a natural generalization of polynomials for which Newton
polytopes continue to yield valuable insights. In particular, a number of
invariance properties of Newton polytopes in the context of optimization are
only revealed by adopting the viewpoint of signomials.


Online Inventory Management with Application to Energy Procurement in
  Data Centers

  Motivated by the application of energy storage management in electricity
markets, this paper considers the problem of online linear programming with
inventory management constraints. Specifically, a decision maker should satisfy
some units of an asset as her demand, either form a market with time-varying
price or from her own inventory. The decision maker is presented a price in
slot-by-slot manner, and must immediately decide the purchased amount with the
current price to cover the demand or to store in inventory for covering the
future demand. The inventory has a limited capacity and its critical role is to
buy and store assets at low price and use the stored assets to cover the demand
at high price. The ultimate goal of the decision maker is to cover the demands
while minimizing the cost of buying assets from the market. We propose BatMan,
an online algorithm for simple inventory models, and BatManRate, an extended
version for the case with rate constraints. Both BatMan and BatManRate achieve
optimal competitive ratios, meaning that no other online algorithm can achieve
a better theoretical guarantee. To illustrate the results, we use the proposed
algorithms to design and evaluate energy procurement and storage management
strategies for data centers with a portfolio of energy sources including the
electric grid, local renewable generation, and energy storage systems.


Competitive Online Optimization under Inventory Constraints

  This paper studies online optimization under inventory (budget) constraints.
While online optimization is a well-studied topic, versions with inventory
constraints have proven difficult. We consider a formulation of
inventory-constrained optimization that is a generalization of the classic
one-way trading problem and has a wide range of applications. We present a new
algorithmic framework, \textsf{CR-Pursuit}, and prove that it achieves the
minimal competitive ratio among all deterministic algorithms (up to a
problem-dependent constant factor) for inventory-constrained online
optimization. Our algorithm and its analysis not only simplify and unify the
state-of-the-art results for the standard one-way trading problem, but they
also establish novel bounds for generalizations including concave revenue
functions. For example, for one-way trading with price elasticity, the
\textsf{CR-Pursuit} algorithm achieves a competitive ratio that is within a
small additive constant (i.e., 1/3) to the lower bound of $\ln \theta+1$, where
$\theta$ is the ratio between the maximum and minimum base prices.


Less is More: Real-time Failure Localization in Power Systems

  Cascading failures in power systems exhibit non-local propagation patterns
which make the analysis and mitigation of failures difficult. In this work, we
propose a distributed control framework inspired by the recently proposed
concepts of unified controller and network tree-partition that offers strong
guarantees in both the mitigation and localization of cascading failures in
power systems. In this framework, the transmission network is partitioned into
several control areas which are connected in a tree structure, and the unified
controller is adopted by generators or controllable loads for fast timescale
disturbance response. After an initial failure, the proposed strategy always
prevents successive failures from happening, and regulates the system to the
desired steady state where the impact of initial failures are localized as much
as possible. For extreme failures that cannot be localized, the proposed
framework has a configurable design, that progressively involves and
coordinates more control areas for failure mitigation and, as a last resort,
imposes minimal load shedding. We compare the proposed control framework with
Automatic Generation Control (AGC) on the IEEE 118-bus test system. Simulation
results show that our novel framework greatly improves the system robustness in
terms of the N-1 security standard, and localizes the impact of initial
failures in majority of the load profiles that are examined. Moreover, the
proposed framework incurs significantly less load loss, if any, compared to
AGC, in all of our case studies.


Routing and Staffing when Servers are Strategic

  Traditionally, research focusing on the design of routing and staffing
policies for service systems has modeled servers as having fixed (possibly
heterogeneous) service rates. However, service systems are generally staffed by
people. Furthermore, people respond to workload incentives; that is, how hard a
person works can depend both on how much work there is, and how the work is
divided between the people responsible for it. In a service system, the routing
and staffing policies control such workload incentives; and so the rate servers
work will be impacted by the system's routing and staffing policies. This
observation has consequences when modeling service system performance, and our
objective is to investigate those consequences.
  We do this in the context of the M/M/N queue, which is the canonical model
for large service systems. First, we present a model for "strategic" servers
that choose their service rate in order to maximize a trade-off between an
"effort cost", which captures the idea that servers exert more effort when
working at a faster rate, and a "value of idleness", which assumes that servers
value having idle time. Next, we characterize the symmetric Nash equilibrium
service rate under any routing policy that routes based on the server idle
time. We find that the system must operate in a quality-driven regime, in which
servers have idle time, in order for an equilibrium to exist, which implies
that the staffing must have a first-order term that strictly exceeds that of
the common square-root staffing policy. Then, within the class of policies that
admit an equilibrium, we (asymptotically) solve the problem of minimizing the
total cost, when there are linear staffing costs and linear waiting costs.
Finally, we end by exploring the question of whether routing policies that are
based on the service rate, instead of the server idle time, can improve system
performance.


Ionization Electron Signal Processing in Single Phase LArTPCs I.
  Algorithm Description and Quantitative Evaluation with MicroBooNE Simulation

  We describe the concept and procedure of drifted-charge extraction developed
in the MicroBooNE experiment, a single-phase liquid argon time projection
chamber (LArTPC). This technique converts the raw digitized TPC waveform to the
number of ionization electrons passing through a wire plane at a given time. A
robust recovery of the number of ionization electrons from both induction and
collection anode wire planes will augment the 3D reconstruction, and is
particularly important for tomographic reconstruction algorithms. A number of
building blocks of the overall procedure are described. The performance of the
signal processing is quantitatively evaluated by comparing extracted charge
with the true charge through a detailed TPC detector simulation taking into
account position-dependent induced current inside a single wire region and
across multiple wires. Some areas for further improvement of the performance of
the charge extraction procedure are also discussed.


Ionization Electron Signal Processing in Single Phase LArTPCs II.
  Data/Simulation Comparison and Performance in MicroBooNE

  The single-phase liquid argon time projection chamber (LArTPC) provides a
large amount of detailed information in the form of fine-grained drifted
ionization charge from particle traces. To fully utilize this information, the
deposited charge must be accurately extracted from the raw digitized waveforms
via a robust signal processing chain. Enabled by the ultra-low noise levels
associated with cryogenic electronics in the MicroBooNE detector, the precise
extraction of ionization charge from the induction wire planes in a
single-phase LArTPC is qualitatively demonstrated on MicroBooNE data with event
display images, and quantitatively demonstrated via waveform-level and
track-level metrics. Improved performance of induction plane calorimetry is
demonstrated through the agreement of extracted ionization charge measurements
across different wire planes for various event topologies. In addition to the
comprehensive waveform-level comparison of data and simulation, a calibration
of the cryogenic electronics response is presented and solutions to various
MicroBooNE-specific TPC issues are discussed. This work presents an important
improvement in LArTPC signal processing, the foundation of reconstruction and
therefore physics analyses in MicroBooNE.


Comparison of νμ-Ar multiplicity distributions observed by
  MicroBooNE to GENIE model predictions

  We measure a large set of observables in inclusive charged current muon
neutrino scattering on argon with the MicroBooNE liquid argon time projection
chamber operating at Fermilab. We evaluate three neutrino interaction models
based on the widely used GENIE event generator using these observables. The
measurement uses a data set consisting of neutrino interactions with a final
state muon candidate fully contained within the MicroBooNE detector. These data
were collected in 2016 with the Fermilab Booster Neutrino Beam, which has an
average neutrino energy of 800 MeV, using an exposure corresponding to 5E19
protons-on-target. The analysis employs fully automatic event selection and
charged particle track reconstruction and uses a data-driven technique to
separate neutrino interactions from cosmic ray background events. We find that
GENIE models consistently describe the shapes of a large number of kinematic
distributions for fixed observed multiplicity.


A Deep Neural Network for Pixel-Level Electromagnetic Particle
  Identification in the MicroBooNE Liquid Argon Time Projection Chamber

  We have developed a convolutional neural network (CNN) that can make a
pixel-level prediction of objects in image data recorded by a liquid argon time
projection chamber (LArTPC) for the first time. We describe the network design,
training techniques, and software tools developed to train this network. The
goal of this work is to develop a complete deep neural network based data
reconstruction chain for the MicroBooNE detector. We show the first
demonstration of a network's validity on real LArTPC data using MicroBooNE
collection plane images. The demonstration is performed for stopping muon and a
$\nu_\mu$ charged current neutral pion data samples.


First Measurement of $ν_μ$ Charged-Current $π^{0}$ Production on
  Argon with a LArTPC

  We report the first measurement of the flux-integrated cross section of
$\nu_{\mu}$ charged-current single $\pi^{0}$ production on argon. This
measurement is performed with the MicroBooNE detector, an 85 ton active mass
liquid argon time projection chamber exposed to the Booster Neutrino Beam at
Fermilab. This result on argon is compared to past measurements on lighter
nuclei to investigate the scaling assumptions used in models of the production
and transport of pions in neutrino-nucleus scattering. The techniques used are
an important demonstration of the successful reconstruction and analysis of
neutrino interactions producing electromagnetic final states using a liquid
argon time projection chamber operating at the earth's surface.


Rejecting cosmic background for exclusive neutrino interaction studies
  with Liquid Argon TPCs; a case study with the MicroBooNE detector

  Cosmic ray (CR) interactions can be a challenging source of background for
neutrino oscillation and cross-section measurements in surface detectors. We
present methods for CR rejection in measurements of charged-current
quasielastic-like (CCQE-like) neutrino interactions, with a muon and a proton
in the final state, measured using liquid argon time projection chambers
(LArTPCs). Using a sample of cosmic data collected with the MicroBooNE
detector, mixed with simulated neutrino scattering events, a set of event
selection criteria is developed that produces an event sample with minimal
contribution from CR background. Depending on the selection criteria used a
purity between 50% and 80% can be achieved with a signal selection efficiency
between 50% and 25%, with higher purity coming at the expense of lower
efficiency. While using a specific dataset from the MicroBooNE detector and
selection criteria values optimized for CCQE-like events, the concepts
presented here are generic and can be adapted for various studies of exclusive
{\nu}{\mu} interactions in LArTPCs.


Design and construction of the MicroBooNE Cosmic Ray Tagger system

  The MicroBooNE detector utilizes a liquid argon time projection chamber
(LArTPC) with an 85 t active mass to study neutrino interactions along the
Booster Neutrino Beam (BNB) at Fermilab. With a deployment location near ground
level, the detector records many cosmic muon tracks in each beam-related
detector trigger that can be misidentified as signals of interest. To reduce
these cosmogenic backgrounds, we have designed and constructed a TPC-external
Cosmic Ray Tagger (CRT). This sub-system was developed by the Laboratory for
High Energy Physics (LHEP), Albert Einstein center for fundamental physics,
University of Bern. The system utilizes plastic scintillation modules to
provide precise time and position information for TPC-traversing particles.
Successful matching of TPC tracks and CRT data will allow us to reduce
cosmogenic background and better characterize the light collection system and
LArTPC data using cosmic muons. In this paper we describe the design and
installation of the MicroBooNE CRT system and provide an overview of a series
of tests done to verify the proper operation of the system and its components
during installation, commissioning, and physics data-taking.


The DUNE Far Detector Interim Design Report, Volume 2: Single-Phase
  Module

  The DUNE IDR describes the proposed physics program and technical designs of
the DUNE far detector modules in preparation for the full TDR to be published
in 2019. It is intended as an intermediate milestone on the path to a full TDR,
justifying the technical choices that flow down from the high-level physics
goals through requirements at all levels of the Project. These design choices
will enable the DUNE experiment to make the ground-breaking discoveries that
will help to answer fundamental physics questions. Volume 2 describes the
single-phase module's subsystems, the technical coordination required for its
design, construction, installation, and integration, and its organizational
structure.


The DUNE Far Detector Interim Design Report Volume 1: Physics,
  Technology and Strategies

  The DUNE IDR describes the proposed physics program and technical designs of
the DUNE Far Detector modules in preparation for the full TDR to be published
in 2019. It is intended as an intermediate milestone on the path to a full TDR,
justifying the technical choices that flow down from the high-level physics
goals through requirements at all levels of the Project. These design choices
will enable the DUNE experiment to make the ground-breaking discoveries that
will help to answer fundamental physics questions. Volume 1 contains an
executive summary that describes the general aims of this document. The
remainder of this first volume provides a more detailed description of the DUNE
physics program that drives the choice of detector technologies. It also
includes concise outlines of two overarching systems that have not yet evolved
to consortium structures: computing and calibration. Volumes 2 and 3 of this
IDR describe, for the single-phase and dual-phase technologies, respectively,
each detector module's subsystems, the technical coordination required for its
design, construction, installation, and integration, and its organizational
structure.


The DUNE Far Detector Interim Design Report, Volume 3: Dual-Phase Module

  The DUNE IDR describes the proposed physics program and technical designs of
the DUNE far detector modules in preparation for the full TDR to be published
in 2019. It is intended as an intermediate milestone on the path to a full TDR,
justifying the technical choices that flow down from the high-level physics
goals through requirements at all levels of the Project. These design choices
will enable the DUNE experiment to make the ground-breaking discoveries that
will help to answer fundamental physics questions. Volume 3 describes the
dual-phase module's subsystems, the technical coordination required for its
design, construction, installation, and integration, and its organizational
structure.


