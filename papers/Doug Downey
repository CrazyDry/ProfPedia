Definition Modeling: Learning to define word embeddings in natural
  language

  Distributed representations of words have been shown to capture lexical
semantics, as demonstrated by their effectiveness in word similarity and
analogical relation tasks. But, these tasks only evaluate lexical semantics
indirectly. In this paper, we study whether it is possible to utilize
distributed representations to generate dictionary definitions of words, as a
more direct and transparent representation of the embeddings' semantics. We
introduce definition modeling, the task of generating a definition for a given
word and its embedding. We present several definition model architectures based
on recurrent neural networks, and experiment with the models over multiple data
sets. Our results show that a model that controls dependencies between the word
being defined and the definition words performs significantly better, and that
a character-level convolution layer designed to leverage morphology can
complement word-level embeddings. Finally, an error analysis suggests that the
errors made by a definition model may provide insight into the shortcomings of
word embeddings.


Construction of the Literature Graph in Semantic Scholar

  We describe a deployed scalable system for organizing published scientific
literature into a heterogeneous graph to facilitate algorithmic manipulation
and discovery. The resulting literature graph consists of more than 280M nodes,
representing papers, authors, entities and various interactions between them
(e.g., authorships, citations, entity mentions). We reduce literature graph
construction into familiar NLP tasks (e.g., entity extraction and linking),
point out research challenges due to differences from standard formulations of
these tasks, and report empirical results for each task. The methods described
in this paper are used to enable semantic features in www.semanticscholar.org


A new evaluation framework for topic modeling algorithms based on
  synthetic corpora

  Topic models are in widespread use in natural language processing and beyond.
Here, we propose a new framework for the evaluation of probabilistic topic
modeling algorithms based on synthetic corpora containing an unambiguously
defined ground truth topic structure. The major innovation of our approach is
the ability to quantify the agreement between the planted and inferred topic
structures by comparing the assigned topic labels at the level of the tokens.
In experiments, our approach yields novel insights about the relative strengths
of topic models as corpus characteristics vary, and the first evidence of an
"undetectable phase" for topic models when the planted structure is weak. We
also establish the practical relevance of the insights gained for synthetic
corpora by predicting the performance of topic modeling algorithms in
classification tasks in real-world corpora.


AQuA: An Adversarially Authored Question-Answer Dataset for Common Sense

  Commonsense reasoning is a critical AI capability, but it is difficult to
construct challenging datasets that test common sense. Recent neural
question-answering systems, based on large pre-trained models of language, have
already achieved near-human-level performance on commonsense knowledge
benchmarks. These systems do not possess human-level common sense, but are able
to exploit limitations of the datasets to achieve human-level scores.
  We introduce the AQuA dataset, an adversarially-constructed evaluation
dataset for testing common sense. AQuA forms a challenging extension to the
recently-proposed SWAG dataset, which tests commonsense knowledge using
sentence-completion questions that describe situations observed in video. To
produce a more difficult dataset, we introduce a novel procedure for question
acquisition in which workers author questions designed to target weaknesses of
state-of-the-art neural question answering systems. Workers are rewarded for
submissions that models fail to answer correctly both before and after
fine-tuning (in cross-validation). We create 2.8k questions via this procedure
and evaluate the performance of multiple state-of-the-art question answering
systems on our dataset. We observe a significant gap between human performance,
which is 95.3%, and the performance of the best baseline accuracy of 65.3% by
the OpenAI GPT model.


