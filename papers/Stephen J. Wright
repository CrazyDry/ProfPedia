Coordinate Descent Algorithms

  Coordinate descent algorithms solve optimization problems by successivelyperforming approximate minimization along coordinate directions or coordinatehyperplanes. They have been used in applications for many years, and theirpopularity continues to grow because of their usefulness in data analysis,machine learning, and other areas of current interest. This paper describes thefundamentals of the coordinate descent approach, together with variants andextensions and their convergence properties, mostly with reference to convexobjectives. We pay particular attention to a certain problem structure thatarises frequently in machine learning applications, showing that efficientimplementations of accelerated coordinate descent algorithms are possible forproblems of this type. We also present some parallel variants and discuss theirconvergence properties under several models of parallel execution.

A proximal method for composite minimization

  We consider minimization of functions that are compositions of convex orprox-regular functions (possibly extended-valued) with smooth vector functions.A wide variety of important optimization problems fall into this framework. Wedescribe an algorithmic framework based on a subproblem constructed from alinearized approximation to the objective and a regularization term. Propertiesof local solutions of this subproblem underlie both a global convergence resultand an identification property of the active manifold containing the solutionof the original problem. Preliminary computational results on both convex andnonconvex examples are promising.

$k$-Dependence and Domination in Kings Graphs

  We study k-dependence and half domination problems for king's graphs indimension n (n>1). Various sharp bounds are provided and a few conjectures areformulated in the cases the estimates are not the best possible.

An Asynchronous Parallel Randomized Kaczmarz Algorithm

  We describe an asynchronous parallel variant of the randomized Kaczmarz (RK)algorithm for solving the linear system $Ax=b$. The analysis shows linearconvergence and indicates that nearly linear speedup can be expected if thenumber of processors is bounded by a multiple of the number of rows in $A$.

Constraint Identification and Algorithm Stabilization for Degenerate  Nonlinear Programs

  In the vicinity of a solution of a nonlinear programming problem at whichboth strict complementarity and linear independence of the active constraintsmay fail to hold, we describe a technique for distinguishing weakly active fromstrongly active constraints. We show that this information can be used tomodify the sequential quadratic programming algorithm so that it exhibitssuperlinear convergence to the solution under assumptions weaker than thosemade in previous analyses.

Packing ellipsoids with overlap

  The problem of packing ellipsoids of different sizes and shapes into anellipsoidal container so as to minimize a measure of overlap between ellipsoidsis considered. A bilevel optimization formulation is given, together with analgorithm for the general case and a simpler algorithm for the special case inwhich all ellipsoids are in fact spheres. Convergence results are proved andcomputational experience is described and illustrated. The motivatingapplication - chromosome organization in the human cell nucleus - is discussedbriefly, and some illustrative results are presented.

Solving the continuous nonlinear resource allocation problem with an  interior point method

  Resource allocation problems are usually solved with specialized methodsexploiting their general sparsity and problem-specific algebraic structure. Weshow that the sparsity structure alone yields a closed-form Newton searchdirection for the generic primal-dual interior point method. Computationaltests show that the interior point method consistently outperforms the bestspecialized methods when no additional algebraic structure is available.

On GROUSE and Incremental SVD

  GROUSE (Grassmannian Rank-One Update Subspace Estimation) is an incrementalalgorithm for identifying a subspace of Rn from a sequence of vectors in thissubspace, where only a subset of components of each vector is revealed at eachiteration. Recent analysis has shown that GROUSE converges locally at anexpected linear rate, under certain assumptions. GROUSE has a similar flavor tothe incremental singular value decomposition algorithm, which updates the SVDof a matrix following addition of a single column. In this paper, we modify theincremental SVD approach to handle missing data, and demonstrate that thismodified approach is equivalent to GROUSE, for a certain choice of analgorithmic parameter.

An Asynchronous Parallel Stochastic Coordinate Descent Algorithm

  We describe an asynchronous parallel stochastic coordinate descent algorithmfor minimizing smooth unconstrained or separably constrained functions. Themethod achieves a linear convergence rate on functions that satisfy anessential strong convexity property and a sublinear rate ($1/K$) on generalconvex functions. Near-linear speedup on a multicore system can be expected ifthe number of processors is $O(n^{1/2})$ in unconstrained optimization and$O(n^{1/4})$ in the separable-constrained case, where $n$ is the number ofvariables. We describe results from implementation on 40-core processors.

Analyzing Random Permutations for Cyclic Coordinate Descent

  We consider coordinate descent methods on convex quadratic problems, in whichexact line searches are performed at each iteration. (This algorithm isidentical to Gauss-Seidel on the equivalent symmetric positive definite linearsystem.) We describe a class of convex quadratic problems for which therandom-permutations version of cyclic coordinate descent (RPCD) outperforms thestandard cyclic coordinate descent (CCD) approach, yielding convergencebehavior similar to the fully-random variant (RCD). A convergence analysis isdeveloped to explain the empirical observations.

Randomized sampling for basis functions construction in generalized  finite element methods

  In the framework of generalized finite element methods for elliptic equationswith rough coefficients, efficiency and accuracy of the numerical method dependcritically on the use of appropriate basis functions. This work exploresseveral random sampling strategies that construct approximations to the optimalset of basis functions of a given dimension, and proposes a quantitativecriterion to analyze and compare these sampling strategies. Numerical evidenceshows that the best results are achieved by two strategies, Random Gaussian andSmooth boundary sampling.

Random Sampling and Efficient Algorithms for Multiscale PDEs

  We describe an efficient framework for multiscale PDE problems that usesrandom sampling to capture low-rank local solution spaces arising in a domaindecomposition framework. In contrast to existing techniques, our method doesnot rely on detailed analytical understanding of specific multiscale PDEs, inparticular, their asymptotic limits. Our framework is applied to two specificproblems - a linear kinetic equation and an elliptic equation with oscillatorymedia - for which recover the asymptotic preserving scheme and numericalhomogenization, respectively. Numerical results confirm the efficacy of ourapproach.

First-order algorithms converge faster than $O(1/k)$ on convex problems

  It has been known for many years that both gradient descent and stochasticcoordinate descent achieve a global convergence rate of $O(1/k)$ in theobjective value, when applied to a scheme for minimizing aLipschitz-continuously differentiable, unconstrained convex function. In thiswork, we improve this rate to $o(1/k)$. We extend the result to proximalgradient and proximal coordinate descent on regularized problems to showsimilar $o(1/k)$ convergence rates. The result is tight in the sense that an$O(1/k^{1+\epsilon})$ rate is not generally attainable for any $\epsilon>0$,for any of these methods.

A Log-Barrier Newton-CG Method for Bound Constrained Optimization with  Complexity Guarantees

  We describe an algorithm based on a logarithmic barrier function, Newton'smethod, and linear conjugate gradients, that obtains an approximate minimizerof a smooth function over the nonnegative orthant. We develop a bound on thecomplexity of the approach, stated in terms of the required accuracy and thecost of a single gradient evaluation of the objective function and/or amatrix-vector multiplication involving the Hessian of the objective. Theapproach can be implemented without explicit calculation or storage of theHessian.

Robustness of superconductivity to competing magnetic phases in  tetragonal FeS

  We have determined the superconducting and magnetic properties of ahydrothermally synthesized powder sample of tetragonal FeS using muon spinrotation ({\mu}SR). The superconducting properties are entirely consistent withthose of a recently published study, showing fully gapped behavior and giving apenetration depth of {\lambda}_{ab} = 204(3) nm. However, our zero-field{\mu}SR data are rather different and indicate the presence of a small,non-superconducting magnetic phase within the sample. These results highlightthat sample-to-sample variations in magnetism can arise in hydrothermallyprepared phases, but interestingly the superconducting behavior is remarkablyinsensitive to these variations.

CRATES: An All-Sky Survey of Flat-Spectrum Radio Sources

  We have assembled an 8.4 GHz survey of bright, flat-spectrum (alpha > -0.5)radio sources with nearly uniform extragalactic (|b|>10 deg) coverage forsources brighter than S_{4.8 GHz} = 65 mJy. The catalog is assembled fromexisting observations (especially CLASS and the Wright et al. PMN-CA survey),augmented by reprocessing of archival VLA and ATCA data and by new observationsto fill in coverage gaps. We refer to this program as CRATES, the CombinedRadio All-sky Targeted Eight GHz Survey. The resulting catalog provides precisepositions, sub-arcsecond structures, and spectral indices for some 11,000sources. We describe the morphology and spectral index distribution of thesample and comment on the survey's power to select several classes ofinteresting sources, especially high energy blazars. Comparison of CRATES withother high-frequency surveys also provides unique opportunities foridentification of high-power radio sources.

Magnetic fluctuations and spin freezing in nonsuperconducting LiFeAs  derivatives

  We present detailed magnetometry and muon-spin rotation data onpolycrystalline samples of overdoped, non-superconducting LiFe$_{1-x}$Ni$_x$As($x = 0.1,\,0.2$) and Li$_{1-y}$Fe$_{1+y}$As ($0\leq y\leq 0.04$) as well assuperconducting LiFeAs. While LiFe$_{1-x}$Ni$_x$As exhibits weakantiferromagnetic fluctuations down to $1.5\,{\rm K}$, Li$_{1-y}$Fe$_{1+y}$Assamples, which have a much smaller deviation from the $1:1:1$ stoichiometry,show a crossover from ferromagnetic to antiferromagnetic fluctuations oncooling and a freezing of dynamically fluctuating moments at low temperatures.We do not find any signatures of time-reversal symmetry breaking instoichiometric LiFeAs that would support recent predictions of triplet pairing.

G10/COSMOS: 38 band (far-UV to far-IR) panchromatic photometry using  LAMBDAR

  We present a consistent total flux catalogue for a $\sim$1 deg$^2$ subset ofthe COSMOS region (R.A. $\in [149.55\degr, 150.65\degr]$, DEC $\in [1.80\degr,2.73\degr]$) with near-complete coverage in 38 bands from the far-ultravioletto the far-infrared. We produce aperture matched photometry for 128,304 objectswith i < 24.5 in a manner that is equivalent to the Wright et al. (2016)catalogue from the low-redshift (z < 0.4) Galaxy and Mass Assembly (GAMA)survey. This catalogue is based on publicly available imaging from GALEX, CFHT,Subaru, VISTA, Spitzer and Herschel, contains a robust total flux measurementor upper limit for every object in every waveband and complements ourre-reduction of publicly available spectra in the same region. We perform anumber of consistency checks, demonstrating that our catalogue is comparable toexisting data sets, including the recent COSMOS2015 catalogue (Laigle et al.2016). We also release an updated Davies et al. (2015) spectroscopic cataloguethat folds in new spectroscopic and photometric redshift data sets. Thecatalogues are available for download athttp://cutout.icrar.org/G10/dataRelease.php. Our analysis is optimised for bothpanchromatic analysis over the full wavelength range and for direct comparisonto GAMA, thus permitting measurements of galaxy evolution for 0 < z < 1 whileminimising the systematic error resulting from disparate data reductionmethods.

A Bayesian approach to magnetic moment determination using muSR

  A significant challenge in zero-field muSR experiments arises from theuncertainty in the muon site. It is possible to calculate the dipole field (andhence precession frequency nu) at any particular site given the magnetic momentmu and magnetic structure. One can also evaluate f(nu), the probabilitydistribution function of nu assuming that the muon site can be anywhere withinthe unit cell with equal probability, excluding physically forbidden sites.Since nu is obtained from experiment, what we would like to know is g(mu|nu),the probability density function of mu given the observed nu. This can beobtained from our calculated f(nu/mu) using Bayes' theorem. We describe anapproach to this problem which we have used to extract information about realsystems including a low-moment osmate compound, a family of molecular magnets,and an iron-arsenide compound.

Online Algorithms for Factorization-Based Structure from Motion

  We present a family of online algorithms for real-time factorization-basedstructure from motion, leveraging a relationship between incremental singularvalue decomposition and recently proposed methods for online matrix completion.Our methods are orders of magnitude faster than previous state of the art, canhandle missing data and a variable number of feature points, and are robust tonoise and sparse outliers. We demonstrate our methods on both real andsynthetic sequences and show that they perform well in both online and batchsettings. We also provide an implementation which is able to produce 3D modelsin real time using a laptop with a webcam.

Effects of Finite-Precision Arithmetic on Interior-Point Methods for  Nonlinear Programming

  We show that the effects of finite-precision arithmetic in forming andsolving the linear system that arises at each iteration of primal-dualinterior-point algorithms for nonlinear programming are benign, provided thatthe iterates satisfy centrality and feasibility conditions of the type usuallyassociated with path-following methods. When we replace the standard assumptionthat the active constraint gradients are independent by the weakerMangasarian-Fromovitz constraint qualification, rapid convergence usually isattainable, even when cancellation and roundoff errors occur during thecalculations. In deriving our main results, we prove a key technical resultabout the size of the exact primal-dual step. This result can be used to modifyexisting analysis of primal-dual interior-point methods for convex programming,making it possible to extend the superlinear local convergence results to thenonconvex case.

Identifying Activity

  Identification of active constraints in constrained optimization is ofinterest from both practical and theoretical viewpoints, as it holds thepromise of reducing an inequality-constrained problem to anequality-constrained problem, in a neighborhood of a solution. We study thisissue in the more general setting of composite nonsmooth minimization, in whichthe objective is a composition of a smooth vector function c with a lowersemicontinuous function h, typically nonsmooth but structured. In this setting,the graph of the generalized gradient of h can often be decomposed into a union(nondisjoint) of simpler subsets. "Identification" amounts to deciding whichsubsets of the graph are "active" in the criticality conditions at a givensolution. We give conditions under which any convergent sequence of approximatecritical points finitely identifies the activity. Prominent among theseproperties is a condition akin to the Mangasarian-Fromovitz constraintqualification, which ensures boundedness of the set of multiplier vectors thatsatisfy the optimality conditions at the solution.

Convex Approaches to Model Wavelet Sparsity Patterns

  Statistical dependencies among wavelet coefficients are commonly representedby graphical models such as hidden Markov trees(HMTs). However, in linearinverse problems such as deconvolution, tomography, and compressed sensing, thepresence of a sensing or observation matrix produces a linear mixing of thesimple Markovian dependency structure. This leads to reconstruction problemsthat are non-convex optimizations. Past work has dealt with this issue byresorting to greedy or suboptimal iterative reconstruction methods. In thispaper, we propose new modeling approaches based on group-sparsity penaltiesthat leads to convex optimizations that can be solved exactly and efficiently.We show that the methods we develop perform significantly better indeconvolution and compressed sensing applications, while being ascomputationally efficient as standard coefficient-wise approaches such aslasso.

HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient  Descent

  Stochastic Gradient Descent (SGD) is a popular algorithm that can achievestate-of-the-art performance on a variety of machine learning tasks. Severalresearchers have recently proposed schemes to parallelize SGD, but all requireperformance-destroying memory locking and synchronization. This work aims toshow using novel theoretical analysis, algorithms, and implementation that SGDcan be implemented without any locking. We present an update scheme calledHOGWILD! which allows processors access to shared memory with the possibilityof overwriting each other's work. We show that when the associated optimizationproblem is sparse, meaning most gradient updates only modify small parts of thedecision variable, then HOGWILD! achieves a nearly optimal rate of convergence.We demonstrate experimentally that HOGWILD! outperforms alternative schemesthat use locking by an order of magnitude.

Approximate Stochastic Subgradient Estimation Training for Support  Vector Machines

  Subgradient algorithms for training support vector machines have been quitesuccessful for solving large-scale and online learning problems. However, theyhave been restricted to linear kernels and strongly convex formulations. Thispaper describes efficient subgradient approaches without such limitations. Ourapproaches make use of randomized low-dimensional approximations to nonlinearkernels, and minimization of a reduced primal formulation using an algorithmbased on robust stochastic approximation, which do not require strongconvexity. Experiments illustrate that our approaches produce solutions ofcomparable prediction accuracy with the solutions acquired from existing SVMsolvers, but often in much shorter time. We also suggest efficient predictionschemes that depend only on the dimension of kernel approximation, not on thenumber of support vectors.

Robust Dequantized Compressive Sensing

  We consider the reconstruction problem in compressed sensing in which theobservations are recorded in a finite number of bits. They may thus containquantization errors (from being rounded to the nearest representable value) andsaturation errors (from being outside the range of representable values). Ourformulation has an objective of weighted $\ell_2$-$\ell_1$ type, along withconstraints that account explicitly for quantization and saturation errors, andis solved with an augmented Lagrangian method. We prove a consistency resultfor the recovered solution, stronger than those that have appeared to date inthe literature, showing in particular that asymptotic consistency can beobtained without oversampling. We present extensive computational comparisonswith formulations proposed previously, and variants thereof.

Local Convergence of an Algorithm for Subspace Identification from  Partial Data

  GROUSE (Grassmannian Rank-One Update Subspace Estimation) is an iterativealgorithm for identifying a linear subspace of R^n from data consisting ofpartial observations of random vectors from that subspace. This paper examineslocal convergence properties of GROUSE, under assumptions on the randomness ofthe observed vectors, the randomness of the subset of elements observed at eachiteration, and incoherence of the subspace with the coordinate directions.Convergence at an expected linear rate is demonstrated under certainassumptions. The case in which the full random vector is revealed at eachiteration allows for much simpler analysis, and is also described. GROUSE isrelated to incremental SVD methods and to gradient projection algorithms inoptimization.

An Accelerated Randomized Kaczmarz Algorithm

  The randomized Kaczmarz ($\RK$) algorithm is a simple but powerful approachfor solving consistent linear systems $Ax=b$. This paper proposes anaccelerated randomized Kaczmarz ($\ARK$) algorithm with better convergence thanthe standard $\RK$ algorithm on ill conditioned problems. The per-iterationcost of $\RK$ and $\ARK$ are similar if $A$ is dense, but $\RK$ is much moreable to exploit sparsity in $A$ than is $\ARK$. To deal with the sparse case,an efficient implementation for $\ARK$, called $\SARK$, is proposed. Acomparison of convergence rates and average per-iteration complexities among$\RK$, $\ARK$, and $\SARK$ is given, taking into account different levels ofsparseness and conditioning. Comparisons with the leading deterministicalgorithm --- conjugate gradient applied to the normal equations --- are alsogiven. Finally, the analysis is validated via computational testing.

An Approximate, Efficient Solver for LP Rounding

  Many problems in machine learning can be solved by rounding the solution ofan appropriate linear program (LP). This paper shows that we can recoversolutions of comparable quality by rounding an approximate LP solution insteadof the ex- act one. These approximate LP solutions can be computed efficientlyby applying a parallel stochastic-coordinate-descent method to aquadratic-penalty formulation of the LP. We derive worst-case runtime andsolution quality guarantees of this scheme using novel perturbation andconvergence analysis. Our experiments demonstrate that on such combinatorialproblems as vertex cover, independent set and multiway-cut, our approximaterounding scheme is up to an order of magnitude faster than Cplex (a commercialLP solver) while producing solutions of similar quality.

Asynchronous Stochastic Coordinate Descent: Parallelism and Convergence  Properties

  We describe an asynchronous parallel stochastic proximal coordinate descentalgorithm for minimizing a composite objective function, which consists of asmooth convex function plus a separable convex function. In contrast toprevious analyses, our model of asynchronous computation accounts for the factthat components of the unknown vector may be written by some coressimultaneously with being read by others. Despite the complications arisingfrom this possibility, the method achieves a linear convergence rate onfunctions that satisfy an optimal strong convexity property and a sublinearrate ($1/k$) on general convex functions. Near-linear speedup on a multicoresystem can be expected if the number of processors is $O(n^{1/4})$. We describeresults from implementation on ten cores of a multicore processor.

An S$\ell_1$LP-Active Set Approach for Feasibility Restoration in Power  Systems

  We consider power networks in which it is not possible to satisfy all loadsat the demand nodes, due to some attack or disturbance to the network. Weformulate a model, based on AC power flow equations, to restore the network tofeasibility by shedding load at demand nodes, but doing so in a way thatminimizes a weighted measure of the total load shed, and affects as few demandnodes as possible. Besides suggesting an optimal response to a given attack,our approach can be used to quantify disruption, thereby enabling "stresstesting" to be performed and vulnerabilities to be identified. Optimizationtechniques including nonsmooth penalty functions, sequential linearprogramming, and active-set heuristics are used to solve this model. Wedescribe an algorithmic framework and present convergence results, including aquadratic convergence result for the case in which the solution is fullydetermined by its constraints, a situation that arises frequently in the powersystems application.

Sorting Network Relaxations for Vector Permutation Problems

  The Birkhoff polytope (the convex hull of the set of permutation matrices) isfrequently invoked in formulating relaxations of optimization problems overpermutations. The Birkhoff polytope is represented using $\Theta(n^2)$variables and constraints, significantly more than the $n$ variables one coulduse to represent a permutation as a vector. Using a recent construction ofGoemans (2010), we show that when optimizing over the convex hull of thepermutation vectors (the permutahedron), we can reduce the number of variablesand constraints to $\Theta(n \log n)$ in theory and $\Theta(n \log^2 n)$ inpractice. We modify the recent convex formulation of the 2-SUM problemintroduced by Fogel et al. (2013) to use this polytope, and demonstrate how wecan attain results of similar quality in significantly less computational timefor large $n$. To our knowledge, this is the first usage of Goemans' compactformulation of the permutahedron in a convex optimization problem. We alsointroduce a simpler regularization scheme for this convex formulation of the2-SUM problem that yields good empirical results.

PMU Placement for Line Outage Identification via Multiclass Logistic  Regression

  We consider the problem of identifying a single line outage in a power gridby using data from phasor measurement units (PMUs). When a line outage occurs,the voltage phasor of each bus node changes in response to the change innetwork topology. Each individual line outage has a consistent "signature," anda multiclass logistic regression (MLR) classifier can be trained to distinguishbetween these signatures reliably. We consider first the ideal case in whichPMUs are attached to every bus, but phasor data alone is used to detect outagesignatures. We then describe techniques for placing PMUs selectively on asubset of buses, with the subset being chosen to allow discrimination betweenas many outage events as possible. We also discuss extensions of the MLRtechnique that incorporate explicit information about identification of outagesby PMUs measuring line current flow in or out of a bus. Experimental resultswith synthetic 24-hour demand profile data generated for 14, 30, 57 and 118-bussystems are presented.

Vulnerability Analysis of Power Systems

  Potential vulnerabilities in a power grid can be exposed by identifying thosetransmission lines on which attacks (in the form of interference with theirtransmission capabilities) causes maximum disruption to the grid. In thisstudy, we model the grid by (nonlinear) AC power flow equations, and assumethat attacks take the form of increased impedance along transmission lines. Wequantify disruption in several different ways, including (a) overall deviationof the voltages at the buses from 1.0 per unit (p.u.), and (b) the minimalamount of load that must be shed in order to restore the grid to stableoperation. We describe optimization formulations of the problem of finding themost disruptive attack, which are either nonlinear programing problems ornonlinear bilevel optimization problems, and describe customized algorithms forsolving these problems. Experimental results on the IEEE 118-Bus system and aPolish 2383-Bus system are presented.

Random Permutations Fix a Worst Case for Cyclic Coordinate Descent

  Variants of the coordinate descent approach for minimizing a nonlinearfunction are distinguished in part by the order in which coordinates areconsidered for relaxation. Three common orderings are cyclic (CCD), in which wecycle through the components of $x$ in order; randomized (RCD), in which thecomponent to update is selected randomly and independently at each iteration;and random-permutations cyclic (RPCD), which differs from CCD only in that arandom permutation is applied to the variables at the start of each cycle.Known convergence guarantees are weaker for CCD and RPCD than for RCD, thoughin most practical cases, computational performance is similar among all thesevariants. There is a certain type of quadratic function for which CCD issignificantly slower than for RCD; a recent paper by \cite{SunY16a} hasexplored the poor behavior of CCD on functions of this type. The RPCD approachperforms well on these functions, even better than RCD in a certain regime.This paper explains the good behavior of RPCD with a tight analysis.

Complexity analysis of second-order line-search algorithms for smooth  nonconvex optimization

  There has been much recent interest in finding unconstrained local minima ofsmooth functions, due in part of the prevalence of such problems in machinelearning and robust statistics. A particular focus is algorithms with goodcomplexity guarantees. Second-order Newton-type methods that make use ofregularization and trust regions have been analyzed from such a perspective.More recent proposals, based chiefly on first-order methodology, have also beenshown to enjoy optimal iteration complexity rates, while providing additionalguarantees on computational cost.  In this paper, we present an algorithm with favorable complexity propertiesthat differs in two significant ways from other recently proposed methods.First, it is based on line searches only: Each step involves computation of asearch direction, followed by a backtracking line search along that direction.Second, its analysis is rather straightforward, relying for the most part onthe standard technique for demonstrating sufficient decrease in the objectivefrom backtracking. In the latter part of the paper, we consider inexactcomputation of the search directions, using iterative methods in linearalgebra: the conjugate gradient and Lanczos methods. We derive modifiedconvergence and complexity results for these more practical methods.

Behavior of Accelerated Gradient Methods Near Critical Points of  Nonconvex Functions

  We examine the behavior of accelerated gradient methods in smooth nonconvexunconstrained optimization, focusing in particular on their behavior nearstrict saddle points. Accelerated methods are iterative methods that typicallystep along a direction that is a linear combination of the previous step andthe gradient of the function evaluated at a point at or near the currentiterate. (The previous step encodes gradient information from earlier stages inthe iterative process.) We show by means of the stable manifold theorem thatthe heavy-ball method method is unlikely to converge to strict saddle points,which are points at which the gradient of the objective is zero but the Hessianhas at least one negative eigenvalue. We then examine the behavior of theheavy-ball method and other accelerated gradient methods in the vicinity of astrict saddle point of a nonconvex quadratic function, showing that bothmethods can diverge from this point more rapidly than the steepest-descentmethod.

Training Set Debugging Using Trusted Items

  Training set bugs are flaws in the data that adversely affect machinelearning. The training set is usually too large for man- ual inspection, butone may have the resources to verify a few trusted items. The set of trusteditems may not by itself be adequate for learning, so we propose an algorithmthat uses these items to identify bugs in the training set and thus im- proveslearning. Specifically, our approach seeks the smallest set of changes to thetraining set labels such that the model learned from this corrected trainingset predicts labels of the trusted items correctly. We flag the items whoselabels are changed as potential bugs, whose labels can be checked for veracityby human experts. To find the bugs in this way is a challenging combinatorialbilevel optimization problem, but it can be relaxed into a continuousoptimization problem. Ex- periments on toy and real data demonstrate that ourapproach can identify training set bugs effectively and suggest appro- priatechanges to the labels. Our algorithm is a step toward trustworthy machinelearning.

A Distributed Quasi-Newton Algorithm for Empirical Risk Minimization  with Nonsmooth Regularization

  We propose a communication- and computation-efficient distributedoptimization algorithm using second-order information for solving ERM problemswith a nonsmooth regularization term. Current second-order and quasi-Newtonmethods for this problem either do not work well in the distributed setting orwork only for specific regularizers. Our algorithm uses successive quadraticapproximations, and we describe how to maintain an approximation of the Hessianand solve subproblems efficiently in a distributed manner. The proposed methodenjoys global linear convergence for a broad range of non-strongly convexproblems that includes the most commonly used ERMs, thus requiring lowercommunication complexity. It also converges on non-convex problems, so has thepotential to be used on applications such as deep learning. Initialcomputational results on convex problems demonstrate that our methodsignificantly improves on communication cost and running time over the currentstate-of-the-art methods.

A Newton-CG Algorithm with Complexity Guarantees for Smooth  Unconstrained Optimization

  We consider minimization of a smooth nonconvex objective function using aniterative algorithm based on Newton's method and the linear conjugate gradientalgorithm, with explicit detection and use of negative curvature directions forthe Hessian of the objective function. The algorithm tracks Newton-conjugategradient procedures developed in the 1980s closely, but includes enhancementsthat allow worst-case complexity results to be proved for convergence to pointsthat satisfy approximate first-order and second-order optimality conditions.The complexity results match the best known results in the literature forsecond-order methods.

Randomness and Permutations in Coordinate Descent Methods

  We consider coordinate descent (CD) methods with exact line search on convexquadratic problems. Our main focus is to study the performance of the CD methodthat use random permutations in each epoch and compare it to the performance ofthe CD methods that use deterministic orders and random sampling withreplacement. We focus on a class of convex quadratic problems with a diagonallydominant Hessian matrix, for which we show that using random permutationsinstead of random with-replacement sampling improves the performance of the CDmethod in the worst-case. Furthermore, we prove that as the Hessian matrixbecomes more diagonally dominant, the performance improvement attained by usingrandom permutations increases. We also show that for this problem class, usingany fixed deterministic order yields a superior performance than using randompermutations. We present detailed theoretical analyses with respect to threedifferent convergence criteria that are used in the literature and support ourtheoretical results with numerical experiments.

Inexact Variable Metric Stochastic Block-Coordinate Descent for  Regularized Optimization

  Block-coordinate descent (BCD) is a popular method for large-scaleregularized optimization problems with block-separable structure. However,existing analyses require either a fixed second-order approximation of thesmooth part, or restrictions on the subproblem solutions, such as exactness ortermination conditions that are difficult to verify except in simple cases.These assumptions essentially confine the quadratic term in the approximationof the smooth part to being diagonal, so the benefits of second-orderapproximation are mostly lost. Moreover, in contrast to the smooth case,non-uniform sampling has not yet been shown to improve the convergence rate forregularized problems. In this work, we propose an inexact randomized BCD methodbased on a regularized quadratic subproblem, in which the quadratic term canvary from iteration to iteration (and is thus known as a `variable metric'). Weprovide a detailed convergence analysis. When specialized to thenon-regularized case, Nesterov's proposal to improve convergence rate bysampling proportional to the blockwise Lipschitz constants is covered in ourframework. Empirical results also show that significant benefits are attainablewhen a variable quadratic term is used in the subproblem, rather than a fixedterm.

Galaxy And Mass Assembly: the evolution of the cosmic spectral energy  distribution from z = 1 to z = 0

  We present the evolution of the Cosmic Spectral Energy Distribution (CSED)from $z = 1 - 0$. Our CSEDs originate from stacking individual spectral energydistribution fits based on panchromatic photometry from the Galaxy and MassAssembly (GAMA) and COSMOS datasets in ten redshift intervals with completenesscorrections applied. Below $z = 0.45$, we have credible SED fits from 100 nm to1 mm. Due to the relatively low sensitivity of the far-infrared data, ourfar-infrared CSEDs contain a mix of predicted and measured fluxes above $z =0.45$. Our results include appropriate errors to highlight the impact of thesecorrections. We show that the bolometric energy output of the Universe hasdeclined by a factor of roughly four -- from $5.1 \pm 1.0$ at $z \sim 1$ to$1.3 \pm 0.3 \times 10^{35}~h_{70}$~W~Mpc$^{-3}$ at the current epoch. We showthat this decrease is robust to cosmic variance, SED modelling and othervarious types of error. Our CSEDs are also consistent with an increase in themean age of stellar populations. We also show that dust attenuation hasdecreased over the same period, with the photon escape fraction at 150~nmincreasing from $16 \pm 3$ at $z \sim 1$ to $24 \pm 5$ per cent at the currentepoch, equivalent to a decrease in $A_\mathrm{FUV}$ of 0.4~mag. Our CSEDsaccount for $68 \pm 12$ and $61 \pm 13$ per cent of the cosmic optical andinfrared backgrounds respectively as defined from integrated galaxy counts andare consistent with previous estimates of the cosmic infrared background withredshift.

A transient search using combined human and machine classifications

  Large modern surveys require efficient review of data in order to findtransient sources such as supernovae, and to distinguish such sources fromartefacts and noise. Much effort has been put into the development of automaticalgorithms, but surveys still rely on human review of targets. This paperpresents an integrated system for the identification of supernovae in data fromPan-STARRS1, combining classifications from volunteers participating in acitizen science project with those from a convolutional neural network. Theunique aspect of this work is the deployment, in combination, of both human andmachine classifications for near real-time discovery in an astronomicalproject. We show that the combination of the two methods outperforms either oneused individually. This result has important implications for the futuredevelopment of transient searches, especially in the era of LSST and otherlarge-throughput surveys.

Enhancement of superconducting transition temperature of FeSe by  intercalation of a molecular spacer layer

  The recent discovery of high temperature superconductivity in a layered ironarsenide has led to an intensive search to optimize the superconductingproperties of iron-based superconductors by changing the chemical compositionof the spacer layer that is inserted between adjacent anionic iron arsenidelayers. Until now, superconductivity has only been found in compounds with acationic spacer layer consisting of metal ions: Li+, Na+, K+, Ba2+ or aPbO-type or perovskite-type oxide layer. Electronic doping is usually necessaryto control the fine balance between antiferromagnetism and superconductivity.Superconductivity has also been reported in FeSe, which contains neutral layerssimilar in structure to those found in the iron arsenides but without thespacer layer. Here we demonstrate the synthesis of Lix(NH2)y(NH3)1-yFe2Se2 (x~0.6 ; y ~ 0.2), with lithium ions, lithium amide and ammonia acting as thespacer layer, which exhibits superconductivity at 43(1) K, higher than in anyFeSe-derived compound reported so far and four times higher at ambient pressurethan the transition temperature, Tc, of the parent Fe1.01Se. We have determinedthe crystal structure using neutron powder diffraction and used magnetometryand muon-spin rotation data to determine the superconducting properties. Thisnew synthetic route opens up the possibility of further exploitation of relatedmolecular intercalations in this and other systems in order to greatly optimizethe superconducting properties in this family.

Microflare Heating of a Solar Active Region Observed with NuSTAR,  Hinode/XRT, and SDO/AIA

  NuSTAR is a highly sensitive focusing hard X-ray (HXR) telescope and hasobserved several small microflares in its initial solar pointings. In thispaper, we present the first joint observation of a microflare with NuSTAR andHinode/XRT on 2015 April 29 at ~11:29 UT. This microflare shows heating ofmaterial to several million Kelvin, observed in Soft X-rays (SXRs) withHinode/XRT, and was faintly visible in Extreme Ultraviolet (EUV) with SDO/AIA.For three of the four NuSTAR observations of this region (pre-, decay, and postphases) the spectrum is well fitted by a single thermal model of 3.2-3.5 MK,but the spectrum during the impulsive phase shows additional emission up to 10MK, emission equivalent to A0.1 GOES class. We recover the differentialemission measure (DEM) using SDO/AIA, Hinode/XRT, and NuSTAR, givingunprecedented coverage in temperature. We find the pre-flare DEM peaks at ~3 MKand falls off sharply by 5 MK; but during the microflare's impulsive phase theemission above 3 MK is brighter and extends to 10 MK, giving a heating rate ofabout $2.5 \times 10^{25}$ erg s$^{-1}$. As the NuSTAR spectrum is purelythermal we determined upper-limits on the possible non-thermal bremsstrahlungemission. We find that for the accelerated electrons to be the source of theheating requires a power-law spectrum of $\delta \ge 7$ with a low energycut-off $E_{c} \lesssim 7$ keV. In summary, this first NuSTAR microflarestrongly resembles much more powerful flares.

Accurate high speed single-electron quantum dot preparation

  Using standard microfabrication techniques it is now possible to constructdevices, which appear to reliably manipulate electrons one at a time. Thesedevices have potential use as building blocks in quantum computing devices, oras a standard of electrical current derived only from a frequency and thefundamental charge. To date the error rate in semiconductor 'tuneable-barrier'pump devices, those which show most promise for high frequency operation, havenot been tested in detail. We present high accuracy measurements of the currentfrom an etched GaAs quantum dot pump, operated at zero source-drain biasvoltage with a single AC-modulated gate driving the pump cycle. By comparisonwith a reference current derived from primary standards, we show that theelectron transfer accuracy is better than 15 parts per million. High-resolutionstudies of the dependence of the pump current on the quantum dot tuningparameters also reveal possible deviations from a model used to describe thepumping cycle.

The New Galaxy Evolution Paradigm Revealed by the Herschel Surveys

  The Herschel Space Observatory has revealed a very different galaxyscape fromthat shown by optical surveys which presents a challenge for galaxy-evolutionmodels. The Herschel surveys reveal (1) that there was rapid galaxy evolutionin the very recent past and (2) that galaxies lie on a a single Galaxy Sequence(GS) rather than a star-forming `main sequence' and a separate region of`passive' or `red-and-dead' galaxies. The form of the GS is now clearer becausefar-infrared surveys such as the Herschel ATLAS pick up a population ofoptically-red star-forming galaxies that would have been classified as passiveusing most optical criteria. The space-density of this population is at leastas high as the traditional star-forming population. By stacking spectra ofH-ATLAS galaxies over the redshift range 0.001 < z < 0.4, we show that thegalaxies responsible for the rapid low-redshift evolution have high stellarmasses, high star-formation rates but, even several billion years in the past,old stellar populations - they are thus likely to be relatively recentancestors of early-type galaxies in the Universe today. The form of the GS isinconsistent with rapid quenching models and neither the analytic bathtub modelnor the hydrodynamical EAGLE simulation can reproduce the rapid cosmicevolution. We propose a new gentler model of galaxy evolution that can explainthe new Herschel results and other key properties of the galaxy population.

The evolutionary status of Sher25 - implications for blue supergiants  and the progenitor of SN1987A

  The blue supergiant Sher25 in the massive Galactic cluster NGC3603 issurrounded by a striking emission line nebula. The nebula contains anequatorial ring and probable bi-polar outflows, and is similar in morphology,mass and kinematics to the structure visible around SN1987A. It has beensuggested that both nebulae were ejected while Sher25 and the progenitor ofSN1987A were in previous red supergiant phases. We present opticalhigh-resolution spectra of Sher25 and a model photosphere and unified stellarwind analysis which determines atmospheric parameters, mass-loss rate andphotospheric abundances. We compare CNO abundances to other Galactic B-typesupergiants and find that Sher25 does not appear extreme or abnormal in termsof its photospheric nitrogen and helium abundances. The C/N and N/O ratios arecompared to surface abundances predicted by stellar evolutionary calculationsand are incompatible with the star having a previous red-supergiant phase. Thenebula is likely to have been ejected while the star was a blue supergiant. Theresults are compatible with some degree of rotationally induced mixing havingoccurred while the star was on or near the main-sequence. Our analsysissuggests the star has a relatively normal stellar wind and mass-loss rate, andsits comfortably within the wind momentum-luminosity relationship. In light ofthe evidence regarding massive evolved early-type stars in the Galaxy wesuggest there is no object which shows clear evidence of having had a previousred supergiant phase and hence of undergoing blue loops in the HR diagram.[ABRIDGED]

A Community-Developed Open-Source Computational Ecosystem for Big Neuro  Data

  Big imaging data is becoming more prominent in brain sciences acrossspatiotemporal scales and phylogenies. We have developed a computationalecosystem that enables storage, visualization, and analysis of these data inthe cloud, thusfar spanning 20+ publications and 100+ terabytes includingnanoscale ultrastructure, microscale synaptogenetic diversity, and mesoscalewhole brain connectivity, making NeuroData the largest and most diverse openrepository of brain data.

