Hierarchically-Attentive RNN for Album Summarization and Storytelling

  We address the problem of end-to-end visual storytelling. Given a photoalbum, our model first selects the most representative (summary) photos, andthen composes a natural language story for the album. For this task, we makeuse of the Visual Storytelling dataset and a model composed of threehierarchically-attentive Recurrent Neural Nets (RNNs) to: encode the albumphotos, select representative (summary) photos, and compose the story.Automatic and human evaluations show our model achieves better performance onselection, generation, and retrieval than baselines.

Visual Madlibs: Fill in the blank Image Generation and Question  Answering

  In this paper, we introduce a new dataset consisting of 360,001 focusednatural language descriptions for 10,738 images. This dataset, the VisualMadlibs dataset, is collected using automatically produced fill-in-the-blanktemplates designed to gather targeted descriptions about: people and objects,their appearances, activities, and interactions, as well as inferences aboutthe general scene or its broader context. We provide several analyses of theVisual Madlibs dataset and demonstrate its applicability to two new descriptiongeneration tasks: focused description generation, and multiple-choicequestion-answering for images. Experiments using joint-embedding and deeplearning methods show promising results on these tasks.

Modeling Context in Referring Expressions

  Humans refer to objects in their environments all the time, especially indialogue with other people. We explore generating and comprehending naturallanguage referring expressions for objects in images. In particular, we focuson incorporating better measures of visual context into referring expressionmodels and find that visual comparison to other objects within an image helpsimprove performance significantly. We also develop methods to tie the languagegeneration process together, so that we generate expressions for all objects ofa particular category jointly. Evaluation on three recent datasets - RefCOCO,RefCOCO+, and RefCOCOg, shows the advantages of our methods for both referringexpression generation and comprehension.

Solving Visual Madlibs with Multiple Cues

  This paper focuses on answering fill-in-the-blank style multiple choicequestions from the Visual Madlibs dataset. Previous approaches to VisualQuestion Answering (VQA) have mainly used generic image features from networkstrained on the ImageNet dataset, despite the wide scope of questions. Incontrast, our approach employs features derived from networks trained forspecialized tasks of scene classification, person activity prediction, andperson and object attribute prediction. We also present a method for selectingsub-regions of an image that are relevant for evaluating the appropriateness ofa putative answer. Visual features are computed both from the whole image andfrom local regions, while sentences are mapped to a common space using a simplenormalized canonical correlation analysis (CCA) model. Our results show asignificant improvement over the previous state of the art, and indicate thatanswering different question types benefits from examining a variety of imagecues and carefully choosing informative image sub-regions.

When was that made?

  In this paper, we explore deep learning methods for estimating when objectswere made. Automatic methods for this task could potentially be useful forhistorians, collectors, or any individual interested in estimating when theirartifact was created. Direct applications include large-scale data organizationor retrieval. Toward this goal, we utilize features from existing deep networksand also fine-tune new networks for temporal estimation. In addition, we createtwo new datasets of 67,771 dated clothing items from Flickr and museumcollections. Our method outperforms both a color-based baseline and previousstate of the art methods for temporal estimation. We also provide severalanalyses of what our networks have learned, and demonstrate applications toidentifying temporal inspiration in fashion collections.

Combining Multiple Cues for Visual Madlibs Question Answering

  This paper presents an approach for answering fill-in-the-blank multiplechoice questions from the Visual Madlibs dataset. Instead of generic andcommonly used representations trained on the ImageNet classification task, ourapproach employs a combination of networks trained for specialized tasks suchas scene recognition, person activity classification, and attribute prediction.We also present a method for localizing phrases from candidate answers in orderto provide spatial support for feature extraction. We map each of thesefeatures, together with candidate answers, to a joint embedding space throughnormalized canonical correlation analysis (nCCA). Finally, we solve anoptimization problem to learn to combine scores from nCCA models trained onmultiple cues to select the best answer. Extensive experimental results show asignificant improvement over the previous state of the art and confirm thatanswering questions from a wide range of types benefits from examining avariety of image cues and carefully choosing the spatial support for featureextraction.

Learning Temporal Transformations From Time-Lapse Videos

  Based on life-long observations of physical, chemical, and biologic phenomenain the natural world, humans can often easily picture in their minds what anobject will look like in the future. But, what about computers? In this paper,we learn computational models of object transformations from time-lapse videos.In particular, we explore the use of generative models to create depictions ofobjects at future times. These models explore several different predictiontasks: generating a future state given a single depiction of an object,generating a future state given two depictions of an object at different times,and generating future states recursively in a recurrent framework. We provideboth qualitative and quantitative evaluations of the generated results, andalso conduct a human evaluation to compare variations of our models.

A Joint Speaker-Listener-Reinforcer Model for Referring Expressions

  Referring expressions are natural language constructions used to identifyparticular objects within a scene. In this paper, we propose a unifiedframework for the tasks of referring expression comprehension and generation.Our model is composed of three modules: speaker, listener, and reinforcer. Thespeaker generates referring expressions, the listener comprehends referringexpressions, and the reinforcer introduces a reward function to guide samplingof more discriminative expressions. The listener-speaker modules are trainedjointly in an end-to-end learning framework, allowing the modules to be awareof one another during learning while also benefiting from the discriminativereinforcer's feedback. We demonstrate that this unified framework and trainingachieves state-of-the-art results for both comprehension and generation onthree referring expression datasets. Project and demo page:https://vision.cs.unc.edu/refer

Visual to Sound: Generating Natural Sound for Videos in the Wild

  As two of the five traditional human senses (sight, hearing, taste, smell,and touch), vision and sound are basic sources through which humans understandthe world. Often correlated during natural events, these two modalities combineto jointly affect human perception. In this paper, we pose the task ofgenerating sound given visual input. Such capabilities could help enableapplications in virtual reality (generating sound for virtual scenesautomatically) or provide additional accessibility to images or videos forpeople with visual impairments. As a first step in this direction, we applylearning-based methods to generate raw waveform samples given input videoframes. We evaluate our models on a dataset of videos containing a variety ofsounds (such as ambient sounds and sounds from people/animals). Our experimentsshow that the generated sounds are fairly realistic and have good temporalsynchronization with the visual inputs.

MAttNet: Modular Attention Network for Referring Expression  Comprehension

  In this paper, we address referring expression comprehension: localizing animage region described by a natural language expression. While most recent worktreats expressions as a single unit, we propose to decompose them into threemodular components related to subject appearance, location, and relationship toother objects. This allows us to flexibly adapt to expressions containingdifferent types of information in an end-to-end framework. In our model, whichwe call the Modular Attention Network (MAttNet), two types of attention areutilized: language-based attention that learns the module weights as well asthe word/phrase attention that each module should focus on; and visualattention that allows the subject and relationship modules to focus on relevantimage components. Module weights combine scores from all three modulesdynamically to output an overall score. Experiments show that MAttNetoutperforms previous state-of-art methods by a large margin on bothbounding-box-level and pixel-level comprehension tasks. Demo and code areprovided.

Image2GIF: Generating Cinemagraphs using Recurrent Deep Q-Networks

  Given a still photograph, one can imagine how dynamic objects might moveagainst a static background. This idea has been actualized in the form ofcinemagraphs, where the motion of particular objects within a still image isrepeated, giving the viewer a sense of animation. In this paper, we learncomputational models that can generate cinemagraph sequences automaticallygiven a single image. To generate cinemagraphs, we explore combining generativemodels with a recurrent neural network and deep Q-networks to enhance the powerof sequence generation. To enable and evaluate these models we make use of twodatasets, one synthetically generated and the other containing real videogenerated cinemagraphs. Both qualitative and quantitative evaluationsdemonstrate the effectiveness of our models on the synthetic and real datasets.

TVQA: Localized, Compositional Video Question Answering

  Recent years have witnessed an increasing interest in image-basedquestion-answering (QA) tasks. However, due to data limitations, there has beenmuch less work on video-based QA. In this paper, we present TVQA, a large-scalevideo QA dataset based on 6 popular TV shows. TVQA consists of 152,545 QA pairsfrom 21,793 clips, spanning over 460 hours of video. Questions are designed tobe compositional in nature, requiring systems to jointly localize relevantmoments within a clip, comprehend subtitle-based dialogue, and recognizerelevant visual concepts. We provide analyses of this new dataset as well asseveral baselines and a multi-stream end-to-end trainable neural networkframework for the TVQA task. The dataset is publicly available athttp://tvqa.cs.unc.edu.

Dance Dance Generation: Motion Transfer for Internet Videos

  This work presents computational methods for transferring body movements fromone person to another with videos collected in the wild. Specifically, we traina personalized model on a single video from the Internet which can generatevideos of this target person driven by the motions of other people. Our modelis built on two generative networks: a human (foreground) synthesis net whichgenerates photo-realistic imagery of the target person in a novel pose, and afusion net which combines the generated foreground with the scene (background),adding shadows or reflections as needed to enhance realism. We validate the theefficacy of our proposed models over baselines with qualitative andquantitative evaluations as well as a subjective test.

Multi-Target Embodied Question Answering

  Embodied Question Answering (EQA) is a relatively new task where an agent isasked to answer questions about its environment from egocentric perception. EQAmakes the fundamental assumption that every question, e.g., "what color is thecar?", has exactly one target ("car") being inquired about. This assumptionputs a direct limitation on the abilities of the agent. We present ageneralization of EQA - Multi-Target EQA (MT-EQA). Specifically, we studyquestions that have multiple targets in them, such as "Is the dresser in thebedroom bigger than the oven in the kitchen?", where the agent has to navigateto multiple locations ("dresser in bedroom", "oven in kitchen") and performcomparative reasoning ("dresser" bigger than "oven") before it can answer aquestion. Such questions require the development of entirely new modules orcomponents in the agent. To address this, we propose a modular architecturecomposed of a program generator, a controller, a navigator, and a VQA module.The program generator converts the given question into sequential executablesub-programs; the navigator guides the agent to multiple locations pertinent tothe navigation-related sub-programs; and the controller learns to selectrelevant observations along its path. These observations are then fed to theVQA module to predict the answer. We perform detailed analysis for each of themodel components and show that our joint model can outperform previous methodsand strong baselines by a significant margin.

