Designing Informative Securities

  We create a formal framework for the design of informative securities in
prediction markets. These securities allow a market organizer to infer the
likelihood of events of interest as well as if he knew all of the traders'
private signals. We consider the design of markets that are always informative,
markets that are informative for a particular signal structure of the
participants, and informative markets constructed from a restricted selection
of securities. We find that to achieve informativeness, it can be necessary to
allow participants to express information that may not be directly of interest
to the market organizer, and that understanding the participants' signal
structure is important for designing informative prediction markets.


Neyman-Pearson Criterion (NPC): A Model Selection Criterion for
  Asymmetric Binary Classification

  We propose a new model selection criterion, the Neyman-Pearson criterion
(NPC), for asymmetric binary classification problems such as cancer diagnosis,
where the two types of classification errors have vastly different priorities.
The NPC is a general prediction-based criterion that works for most
classification methods including logistic regression, support vector machines,
and random forests. We study the theoretical model selection properties of the
NPC for nonparametric plug-in methods. Simulation studies show that the NPC
outperforms the classical prediction-based criterion that minimizes the overall
classification error under various asymmetric classification scenarios. A real
data case study of breast cancer suggests that the NPC is a practical criterion
that leads to the discovery of novel gene markers with both high sensitivity
and specificity for breast cancer diagnosis. The NPC is available in an R
package NPcriterion.


A Utility Framework for Bounded-Loss Market Makers

  We introduce a class of utility-based market makers that always accept orders
at their risk-neutral prices. We derive necessary and sufficient conditions for
such market makers to have bounded loss. We prove that hyperbolic absolute risk
aversion utility market makers are equivalent to weighted pseudospherical
scoring rule market makers. In particular, Hanson's logarithmic scoring rule
market maker corresponds to a negative exponential utility market maker in our
framework. We describe a third equivalent formulation based on maintaining a
cost function that seems most natural for implementation purposes, and we
illustrate how to translate among the three equivalent formulations. We examine
the tradeoff between the market's liquidity and the market maker's worst-case
loss. For a fixed bound on worst-case loss, some market makers exhibit greater
liquidity near uniform prices and some exhibit greater liquidity near extreme
prices, but no market maker can exhibit uniformly greater liquidity in all
regimes. For a fixed minimum liquidity level, we give the lower bound of market
maker's worst-case loss under some regularity conditions.


Quantitative Analysis of Automatic Image Cropping Algorithms: A Dataset
  and Comparative Study

  Automatic photo cropping is an important tool for improving visual quality of
digital photos without resorting to tedious manual selection. Traditionally,
photo cropping is accomplished by determining the best proposal window through
visual quality assessment or saliency detection. In essence, the performance of
an image cropper highly depends on the ability to correctly rank a number of
visually similar proposal windows. Despite the ranking nature of automatic
photo cropping, little attention has been paid to learning-to-rank algorithms
in tackling such a problem. In this work, we conduct an extensive study on
traditional approaches as well as ranking-based croppers trained on various
image features. In addition, a new dataset consisting of high quality cropping
and pairwise ranking annotations is presented to evaluate the performance of
various baselines. The experimental results on the new dataset provide useful
insights into the design of better photo cropping algorithms.


Learning to Compose with Professional Photographs on the Web

  Photo composition is an important factor affecting the aesthetics in
photography. However, it is a highly challenging task to model the aesthetic
properties of good compositions due to the lack of globally applicable rules to
the wide variety of photographic styles. Inspired by the thinking process of
photo taking, we formulate the photo composition problem as a view finding
process which successively examines pairs of views and determines their
aesthetic preferences. We further exploit the rich professional photographs on
the web to mine unlimited high-quality ranking samples and demonstrate that an
aesthetics-aware deep ranking network can be trained without explicitly
modeling any photographic rules. The resulting model is simple and effective in
terms of its architectural design and data sampling method. It is also generic
since it naturally learns any photographic rules implicitly encoded in
professional photographs. The experiments show that the proposed view finding
network achieves state-of-the-art performance with sliding window search
strategy on two image cropping datasets.


Complexity of Combinatorial Market Makers

  We analyze the computational complexity of market maker pricing algorithms
for combinatorial prediction markets. We focus on Hanson's popular logarithmic
market scoring rule market maker (LMSR). Our goal is to implicitly maintain
correct LMSR prices across an exponentially large outcome space. We examine
both permutation combinatorics, where outcomes are permutations of objects, and
Boolean combinatorics, where outcomes are combinations of binary events. We
look at three restrictive languages that limit what traders can bet on. Even
with severely limited languages, we find that LMSR pricing is $\SP$-hard, even
when the same language admits polynomial-time matching without the market
maker. We then propose an approximation technique for pricing permutation
markets based on a recent algorithm for online permutation learning. The
connections we draw between LMSR pricing and the vast literature on online
learning with expert advice may be of independent interest.


A New Understanding of Prediction Markets Via No-Regret Learning

  We explore the striking mathematical connections that exist between market
scoring rules, cost function based prediction markets, and no-regret learning.
We show that any cost function based prediction market can be interpreted as an
algorithm for the commonly studied problem of learning from expert advice by
equating trades made in the market with losses observed by the learning
algorithm. If the loss of the market organizer is bounded, this bound can be
used to derive an O(sqrt(T)) regret bound for the corresponding learning
algorithm. We then show that the class of markets with convex cost functions
exactly corresponds to the class of Follow the Regularized Leader learning
algorithms, with the choice of a cost function in the market corresponding to
the choice of a regularizer in the learning problem. Finally, we show an
equivalence between market scoring rules and prediction markets with convex
cost functions. This implies that market scoring rules can also be interpreted
naturally as Follow the Regularized Leader algorithms, and may be of
independent interest. These connections provide new insight into how it is that
commonly studied markets, such as the Logarithmic Market Scoring Rule, can
aggregate opinions into accurate estimates of the likelihood of future events.


Truthful Mechanisms for Agents that Value Privacy

  Recent work has constructed economic mechanisms that are both truthful and
differentially private. In these mechanisms, privacy is treated separately from
the truthfulness; it is not incorporated in players' utility functions (and
doing so has been shown to lead to non-truthfulness in some cases). In this
work, we propose a new, general way of modelling privacy in players' utility
functions. Specifically, we only assume that if an outcome $o$ has the property
that any report of player $i$ would have led to $o$ with approximately the same
probability, then $o$ has small privacy cost to player $i$. We give three
mechanisms that are truthful with respect to our modelling of privacy: for an
election between two candidates, for a discrete version of the facility
location problem, and for a general social choice problem with discrete
utilities (via a VCG-like mechanism). As the number $n$ of players increases,
the social welfare achieved by our mechanisms approaches optimal (as a fraction
of $n$).


Capturing Variation and Uncertainty in Human Judgment

  The well-studied problem of statistical rank aggregation has been applied to
comparing sports teams, information retrieval, and most recently to data
generated by human judgment. Such human-generated rankings may be substantially
different from traditional statistical ranking data. In this work, we show that
a recently proposed generalized random utility model reveals distinctive
patterns in human judgment across three different domains, and provides a
succinct representation of variance in both population preferences and
imperfect perception. In contrast, we also show that classical statistical
ranking models fail to capture important features from human-generated input.
Our work motivates the use of more flexible ranking models for representing and
describing the collective preferences or decision-making of human participants.


Learning to Incentivize: Eliciting Effort via Output Agreement

  In crowdsourcing when there is a lack of verification for contributed
answers, output agreement mechanisms are often used to incentivize participants
to provide truthful answers when the correct answer is hold by the majority. In
this paper, we focus on using output agreement mechanisms to elicit effort, in
addition to eliciting truthful answers, from a population of workers. We
consider a setting where workers have heterogeneous cost of effort exertion and
examine the data requester's problem of deciding the reward level in output
agreement for optimal elicitation. In particular, when the requester knows the
cost distribution, we derive the optimal reward level for output agreement
mechanisms. This is achieved by first characterizing Bayesian Nash equilibria
of output agreement mechanisms for a given reward level. When the requester
does not know the cost distribution, we develop sequential mechanisms that
combine learning the cost distribution with incentivizing effort exertion to
approximately determine the optimal reward level.


Sequential Peer Prediction: Learning to Elicit Effort using Posted
  Prices

  Peer prediction mechanisms are often adopted to elicit truthful contributions
from crowd workers when no ground-truth verification is available. Recently,
mechanisms of this type have been developed to incentivize effort exertion, in
addition to truthful elicitation. In this paper, we study a sequential peer
prediction problem where a data requester wants to dynamically determine the
reward level to optimize the trade-off between the quality of information
elicited from workers and the total expected payment. In this problem, workers
have homogeneous expertise and heterogeneous cost for exerting effort, both
unknown to the requester. We propose a sequential posted-price mechanism to
dynamically learn the optimal reward level from workers' contributions and to
incentivize effort exertion and truthful reporting. We show that (1) in our
mechanism, workers exerting effort according to a non-degenerate threshold
policy and then reporting truthfully is an equilibrium that returns highest
utility for each worker, and (2) The regret of our learning mechanism w.r.t.
offering the optimal reward (price) is upper bounded by $\tilde{O}(T^{3/4})$
where $T$ is the learning horizon. We further show the power of our learning
approach when the reports of workers do not necessarily follow the
game-theoretic equilibrium.


Informational Substitutes

  We propose definitions of substitutes and complements for pieces of
information ("signals") in the context of a decision or optimization problem,
with game-theoretic and algorithmic applications. In a game-theoretic context,
substitutes capture diminishing marginal value of information to a rational
decision maker. We use the definitions to address the question of how and when
information is aggregated in prediction markets. Substitutes characterize
"best-possible" equilibria with immediate information aggregation, while
complements characterize "worst-possible", delayed aggregation. Game-theoretic
applications also include settings such as crowdsourcing contests and Q\&A
forums. In an algorithmic context, where substitutes capture diminishing
marginal improvement of information to an optimization problem, substitutes
imply efficient approximation algorithms for a very general class of (adaptive)
information acquisition problems.
  In tandem with these broad applications, we examine the structure and design
of informational substitutes and complements. They have equivalent, intuitive
definitions from disparate perspectives: submodularity, geometry, and
information theory. We also consider the design of scoring rules or
optimization problems so as to encourage substitutability or complementarity,
with positive and negative results. Taken as a whole, the results give some
evidence that, in parallel with substitutable items, informational substitutes
play a natural conceptual and formal role in game theory and algorithms.


Fairness at Equilibrium in the Labor Market

  Recent literature on computational notions of fairness has been broadly
divided into two distinct camps, supporting interventions that address either
individual-based or group-based fairness. Rather than privilege a single
definition, we seek to resolve both within the particular domain of employment
discrimination. To this end, we construct a dual labor market model composed of
a Temporary Labor Market, in which firm strategies are constrained to ensure
group-level fairness, and a Permanent Labor Market, in which individual worker
fairness is guaranteed. We show that such restrictions on hiring practices
induces an equilibrium that Pareto-dominates those arising from strategies that
employ statistical discrimination or a "group-blind" criterion. Individual
worker reputations produce externalities for collective reputation, generating
a feedback loop termed a "self-fulfilling prophecy." Our model produces its own
feedback loop, raising the collective reputation of an initially disadvantaged
group via a fairness intervention that need not be permanent. Moreover, we show
that, contrary to popular assumption, the asymmetric equilibria resulting from
hiring practices that disregard group-fairness may be immovable without
targeted intervention. The enduring nature of such equilibria that are both
inequitable and Pareto inefficient suggest that fairness interventions are of
critical importance in moving the labor market to be more socially just and
efficient.


Optimal Data Acquisition for Statistical Estimation

  We consider a data analyst's problem of purchasing data from strategic agents
to compute an unbiased estimate of a statistic of interest. Agents incur
private costs to reveal their data and the costs can be arbitrarily correlated
with their data. Once revealed, data are verifiable. This paper focuses on
linear unbiased estimators. We design an individually rational and incentive
compatible mechanism that optimizes the worst-case mean-squared error of the
estimation, where the worst-case is over the unknown correlation between costs
and data, subject to a budget constraint in expectation. We characterize the
form of the optimal mechanism in closed-form. We further extend our results to
acquiring data for estimating a parameter in regression analysis, where private
costs can correlate with the values of the dependent variable but not with the
values of the independent variables.


Strategyproof Linear Regression in High Dimensions

  This paper is part of an emerging line of work at the intersection of machine
learning and mechanism design, which aims to avoid noise in training data by
correctly aligning the incentives of data sources. Specifically, we focus on
the ubiquitous problem of linear regression, where strategyproof mechanisms
have previously been identified in two dimensions. In our setting, agents have
single-peaked preferences and can manipulate only their response variables. Our
main contribution is the discovery of a family of group strategyproof linear
regression mechanisms in any number of dimensions, which we call generalized
resistant hyperplane mechanisms. The game-theoretic properties of these
mechanisms -- and, in fact, their very existence -- are established through a
connection to a discrete version of the Ham Sandwich Theorem.


Welfare and Distributional Impacts of Fair Classification

  Current methodologies in machine learning analyze the effects of various
statistical parity notions of fairness primarily in light of their impacts on
predictive accuracy and vendor utility loss. In this paper, we propose a new
framework for interpreting the effects of fairness criteria by converting the
constrained loss minimization problem into a social welfare maximization
problem. This translation moves a classifier and its output into utility space
where individuals, groups, and society at-large experience different welfare
changes due to classification assignments. Under this characterization,
predictions and fairness constraints are seen as shaping societal welfare and
distribution and revealing individuals' implied welfare weights in
society--weights that may then be interpreted through a fairness lens. The
social welfare formulation of the fairness problem brings to the fore concerns
of distributive justice that have always had a central albeit more implicit
role in standard algorithmic fairness approaches.


Randomized Wagering Mechanisms

  Wagering mechanisms are one-shot betting mechanisms that elicit agents'
predictions of an event. For deterministic wagering mechanisms, an existing
impossibility result has shown incompatibility of some desirable theoretical
properties. In particular, Pareto optimality (no profitable side bet before
allocation) can not be achieved together with weak incentive compatibility,
weak budget balance and individual rationality. In this paper, we expand the
design space of wagering mechanisms to allow randomization and ask whether
there are randomized wagering mechanisms that can achieve all previously
considered desirable properties, including Pareto optimality. We answer this
question positively with two classes of randomized wagering mechanisms: i) one
simple randomized lottery-type implementation of existing deterministic
wagering mechanisms, and ii) another family of simple and randomized wagering
mechanisms which we call surrogate wagering mechanisms, which are robust to
noisy ground truth. This family of mechanisms builds on the idea of learning
with noisy labels (Natarajan et al. 2013) as well as a recent extension of this
idea to the information elicitation without verification setting (Liu and Chen
2018). We show that a broad family of randomized wagering mechanisms satisfy
all desirable theoretical properties.


Elicitation for Aggregation

  We study the problem of eliciting and aggregating probabilistic information
from multiple agents. In order to successfully aggregate the predictions of
agents, the principal needs to elicit some notion of confidence from agents,
capturing how much experience or knowledge led to their predictions. To
formalize this, we consider a principal who wishes to elicit predictions about
a random variable from a group of Bayesian agents, each of whom have privately
observed some independent samples of the random variable, and hopes to
aggregate the predictions as if she had directly observed the samples of all
agents. Leveraging techniques from Bayesian statistics, we represent confidence
as the number of samples an agent has observed, which is quantified by a
hyperparameter from a conjugate family of prior distributions. This then allows
us to show that if the principal has access to a few samples, she can achieve
her aggregation goal by eliciting predictions from agents using proper scoring
rules. In particular, if she has access to one sample, she can successfully
aggregate the agents' predictions if and only if every posterior predictive
distribution corresponds to a unique value of the hyperparameter. Furthermore,
this uniqueness holds for many common distributions of interest. When this
uniqueness property does not hold, we construct a novel and intuitive mechanism
where a principal with two samples can elicit and optimally aggregate the
agents' predictions.


Privacy Games

  The problem of analyzing the effect of privacy concerns on the behavior of
selfish utility-maximizing agents has received much attention lately. Privacy
concerns are often modeled by altering the utility functions of agents to
consider also their privacy loss. Such privacy aware agents prefer to take a
randomized strategy even in very simple games in which non-privacy aware agents
play pure strategies. In some cases, the behavior of privacy aware agents
follows the framework of Randomized Response, a well-known mechanism that
preserves differential privacy.
  Our work is aimed at better understanding the behavior of agents in settings
where their privacy concerns are explicitly given. We consider a toy setting
where agent A, in an attempt to discover the secret type of agent B, offers B a
gift that one type of B agent likes and the other type dislikes. As opposed to
previous works, B's incentive to keep her type a secret isn't the result of
"hardwiring" B's utility function to consider privacy, but rather takes the
form of a payment between B and A. We investigate three different types of
payment functions and analyze B's behavior in each of the resulting games. As
we show, under some payments, B's behavior is very different than the behavior
of agents with hardwired privacy concerns and might even be deterministic.
Under a different payment we show that B's BNE strategy does fall into the
framework of Randomized Response.


TROM: A Testing-based Method for Finding Transcriptomic Similarity of
  Biological Samples

  Comparative transcriptomics has gained increasing popularity in genomic
research thanks to the development of high-throughput technologies including
microarray and next-generation RNA sequencing that have generated numerous
transcriptomic data. An important question is to understand the conservation
and differentiation of biological processes in different species. We propose a
testing-based method TROM (Transcriptome Overlap Measure) for comparing
transcriptomes within or between different species, and provide a different
perspective to interpret transcriptomic similarity in contrast to traditional
correlation analyses. Specifically, the TROM method focuses on identifying
associated genes that capture molecular characteristics of biological samples,
and subsequently comparing the biological samples by testing the overlap of
their associated genes. We use simulation and real data studies to demonstrate
that TROM is more powerful in identifying similar transcriptomes and more
robust to stochastic gene expression noise than Pearson and Spearman
correlations. We apply TROM to compare the developmental stages of six
Drosophila species, C. elegans, S. purpuratus, D. rerio and mouse liver, and
find interesting correspondence patterns that imply conserved gene expression
programs in the development of these species. The TROM method is available as
an R package on CRAN (http://cran.r-project.org/) with manuals and source codes
available at http://www.stat.ucla.edu/ jingyi.li/software-and-data/trom.html.


Plasma Diagnostics Using K-Line Emission Profiles of Argon

  K-line profiles emitted from a warm dense plasma environment are used for
diagnostics of Ar droplet plasmas created by high energy laser pulses. We
observe temperature gradients within the Ar droplet from cold temperatures of
the order of some 10 eV up to higher temperatures of about 170 eV.
Non-perturbative wave functions are calculated as well as ionization energies,
binding energies and relevant emission energies using a chemical {\it ab
initio} code. The plasma screening is considered within a perturbative approach
to the Hamiltonian. The plasma effect influences the many-particle system
resulting in energy shifts due to electron-ion and electron-electron
interaction. With this approach we get a good reproduction of spectral features
that are strongly influenced by ionization and excitation processes within the
plasma. Comparing with the widely known FLYCHK code, counting for internal
degrees of freedom (bound states) and treating pressure ionization within our
quantum statistical approach leads to different results for the inferred
temperature distribution.


Low-Cost Learning via Active Data Procurement

  We design mechanisms for online procurement of data held by strategic agents
for machine learning tasks. The challenge is to use past data to actively price
future data and give learning guarantees even when an agent's cost for
revealing her data may depend arbitrarily on the data itself. We achieve this
goal by showing how to convert a large class of no-regret algorithms into
online posted-price and learning mechanisms. Our results in a sense parallel
classic sample complexity guarantees, but with the key resource being money
rather than quantity of data: With a budget constraint $B$, we give robust risk
(predictive error) bounds on the order of $1/\sqrt{B}$. Because we use an
active approach, we can often guarantee to do significantly better by
leveraging correlations between costs and data.
  Our algorithms and analysis go through a model of no-regret learning with $T$
arriving pairs (cost, data) and a budget constraint of $B$. Our regret bounds
for this model are on the order of $T/\sqrt{B}$ and we give lower bounds on the
same order.


Active Information Acquisition for Linear Optimization

  We consider partially-specified optimization problems where the goal is to
actively, but efficiently, acquire missing information about the problem in
order to solve it. An algorithm designer wishes to solve a linear program (LP),
$\max \mathbf{c}^T \mathbf{x}$ s.t. $\mathbf{A}\mathbf{x} \leq \mathbf{b},
\mathbf{x} \ge \mathbf{0}$, but does not initially know some of the parameters.
The algorithm can iteratively choose an unknown parameter and gather
information in the form of a noisy sample centered at the parameter's (unknown)
value. The goal is to find an approximately feasible and optimal solution to
the underlying LP with high probability while drawing a small number of
samples. We focus on two cases. (1) When the parameters $\mathbf{c}$ of the
objective are initially unknown, we take an information-theoretic approach and
give roughly matching upper and lower sample complexity bounds, with an
(inefficient) successive-elimination algorithm. (2) When the parameters
$\mathbf{b}$ of the constraints are initially unknown, we propose an efficient
algorithm combining techniques from the ellipsoid method for LP and
confidence-bound approaches from bandit algorithms. The algorithm adaptively
gathers information about constraints only as needed in order to make progress.
We give sample complexity bounds for the algorithm and demonstrate its
improvement over a naive approach via simulation.


A Short-term Intervention for Long-term Fairness in the Labor Market

  The persistence of racial inequality in the U.S. labor market against a
general backdrop of formal equality of opportunity is a troubling phenomenon
that has significant ramifications on the design of hiring policies. In this
paper, we show that current group disparate outcomes may be immovable even when
hiring decisions are bound by an input-output notion of "individual fairness."
Instead, we construct a dynamic reputational model of the labor market that
illustrates the reinforcing nature of asymmetric outcomes resulting from
groups' divergent accesses to resources and as a result, investment choices. To
address these disparities, we adopt a dual labor market composed of a Temporary
Labor Market (TLM), in which firms' hiring strategies are constrained to ensure
statistical parity of workers granted entry into the pipeline, and a Permanent
Labor Market (PLM), in which firms hire top performers as desired. Individual
worker reputations produce externalities for their group; the corresponding
feedback loop raises the collective reputation of the initially disadvantaged
group via a TLM fairness intervention that need not be permanent. We show that
such a restriction on hiring practices induces an equilibrium that, under
particular market conditions, Pareto-dominates those arising from strategies
that statistically discriminate or employ a "group-blind" criterion. The
enduring nature of equilibria that are both inequitable and Pareto suboptimal
suggests that fairness interventions beyond procedural checks of hiring
decisions will be of critical importance in a world where machines play a
greater role in the employment process.


Prior-free Data Acquisition for Accurate Statistical Estimation

  We study a data analyst's problem of acquiring data from self-interested
individuals to obtain an accurate estimation of some statistic of a population,
subject to an expected budget constraint. Each data holder incurs a cost, which
is unknown to the data analyst, to acquire and report his data. The cost can be
arbitrarily correlated with the data. The data analyst has an expected budget
that she can use to incentivize individuals to provide their data. The goal is
to design a joint acquisition-estimation mechanism to optimize the performance
of the produced estimator, without any prior information on the underlying
distribution of cost and data. We investigate two types of estimations:
unbiased point estimation and confidence interval estimation.
  Unbiased estimators: We design a truthful, individually rational, online
mechanism to acquire data from individuals and output an unbiased estimator of
the population mean when the data analyst has no prior information on the
cost-data distribution and individuals arrive in a random order. The
performance of this mechanism matches that of the optimal mechanism, which
knows the true cost distribution, within a constant factor. The performance of
an estimator is evaluated by its variance under the worst-case cost-data
correlation.
  Confidence intervals: We characterize an approximately optimal (within a
factor $2$) mechanism for obtaining a confidence interval of the population
mean when the data analyst knows the true cost distribution at the beginning.
This mechanism is efficiently computable. We then design a truthful,
individually rational, online algorithm that is only worse than the
approximately optimal mechanism by a constant factor. The performance of an
estimator is evaluated by its expected length under the worst-case cost-data
correlation.


An Optimization-Based Framework for Automated Market-Making

  Building on ideas from online convex optimization, we propose a general
framework for the design of efficient securities markets over very large
outcome spaces. The challenge here is computational. In a complete market, in
which one security is offered for each outcome, the market institution can not
efficiently keep track of the transaction history or calculate security prices
when the outcome space is large. The natural solution is to restrict the space
of securities to be much smaller than the outcome space in such a way that
securities can be priced efficiently. Recent research has focused on searching
for spaces of securities that can be priced efficiently by existing mechanisms
designed for complete markets. While there have been some successes, much of
this research has led to hardness results. In this paper, we take a drastically
different approach. We start with an arbitrary space of securities with bounded
payoff, and establish a framework to design markets tailored to this space. We
prove that any market satisfying a set of intuitive conditions must price
securities via a convex potential function and that the space of reachable
prices must be precisely the convex hull of the security payoffs. We then show
how the convex potential function can be defined in terms of an optimization
over the convex hull of the security payoffs. The optimal solution to the
optimization problem gives the security prices. Using this framework, we
provide an efficient market for predicting the landing location of an object on
a sphere. In addition, we show that we can relax our "no-arbitrage" condition
to design a new efficient market maker for pair betting, which is known to be
#P-hard to price using existing mechanisms. This relaxation also allows the
market maker to charge transaction fees so that the depth of the market can be
dynamically increased as the number of trades increases.


Group buying with bundle discounts: computing efficient, stable and fair
  solutions

  We model a market in which nonstrategic vendors sell items of different types
and offer bundles at discounted prices triggered by demand volumes. Each buyer
acts strategically in order to maximize her utility, given by the difference
between product valuation and price paid. Buyers report their valuations in
terms of reserve prices on sets of items, and might be willing to pay prices
different than the market price in order to subsidize other buyers and to
trigger discounts. The resulting price discrimination can be interpreted as a
redistribution of the total discount. We consider a notion of stability that
looks at unilateral deviations, and show that efficient allocations - the ones
maximizing the social welfare - can be stabilized by prices that enjoy
desirable properties of rationality and fairness. These dictate that buyers pay
higher prices only to subsidize others who contribute to the activation of the
desired discounts, and that they pay premiums over the discounted price
proportionally to their surplus - the difference between their current utility
and the utility of their best alternative. Therefore, the resulting price
discrimination appears to be desirable to buyers. Building on this existence
result, and letting N, M and c be the numbers of buyers, vendors and product
types, we propose a O(N^2+NM^c) algorithm that, given an efficient allocation,
computes prices that are rational and fair and that stabilize the market. The
algorithm first determines the redistribution of the discount between groups of
buyers with an equal product choice, and then computes single buyers' prices.
Our results show that if a desirable form of price discrimination is
implemented then social efficiency and stability can coexists in a market
presenting subtle externalities, and computing individual prices from market
prices is tractable.


Surrogate Scoring Rules and a Uniform Dominant Truth Serum

  Strictly proper scoring rules (SPSR) are widely used when designing incentive
mechanisms to elicit private information from strategic agents using realized
ground truth signals, and they can help quantify the value of elicited
information. In this paper, we extend such scoring rules to settings where a
mechanism designer does not have access to ground truth. We consider two such
settings: (i) a setting when the mechanism designer has access to a noisy proxy
version of the ground truth, with {\em known} biases; and (ii) the standard
peer prediction setting where agents' reports, and possibly some limited prior
knowledge of ground truth, are the only source of information that the
mechanism designer has.
  We introduce {\em surrogate scoring rules} (SSR) for the first setting, which
use the noisy ground truth to evaluate quality of elicited information. We show
that SSR preserves the strict properness of SPSR. Using SSR, we then develop a
multi-task scoring mechanism -- called \emph{uniform dominant truth serum}
(DTS) -- to achieve strict properness when there are sufficiently many tasks
and agents, and when the mechanism designer only has access to agents' reports
and one bit information about the marginal of the entire set of tasks' ground
truth. In comparison to standard equilibrium concepts in peer prediction, we
show that DTS can achieve truthfulness in \emph{uniform dominant strategy} in a
multi-task setting when agents adopt the same strategy for all the tasks that
they are assigned (hence the term uniform). A salient feature of SSR and DTS is
that they quantify the quality of information despite lack of ground truth,
just as proper scoring rules do for the {\em with} verification setting. Our
method is verified both theoretically and empirically using data collected from
real human participants.


