Designing Informative Securities

  We create a formal framework for the design of informative securities inprediction markets. These securities allow a market organizer to infer thelikelihood of events of interest as well as if he knew all of the traders'private signals. We consider the design of markets that are always informative,markets that are informative for a particular signal structure of theparticipants, and informative markets constructed from a restricted selectionof securities. We find that to achieve informativeness, it can be necessary toallow participants to express information that may not be directly of interestto the market organizer, and that understanding the participants' signalstructure is important for designing informative prediction markets.

Neyman-Pearson Criterion (NPC): A Model Selection Criterion for  Asymmetric Binary Classification

  We propose a new model selection criterion, the Neyman-Pearson criterion(NPC), for asymmetric binary classification problems such as cancer diagnosis,where the two types of classification errors have vastly different priorities.The NPC is a general prediction-based criterion that works for mostclassification methods including logistic regression, support vector machines,and random forests. We study the theoretical model selection properties of theNPC for nonparametric plug-in methods. Simulation studies show that the NPCoutperforms the classical prediction-based criterion that minimizes the overallclassification error under various asymmetric classification scenarios. A realdata case study of breast cancer suggests that the NPC is a practical criterionthat leads to the discovery of novel gene markers with both high sensitivityand specificity for breast cancer diagnosis. The NPC is available in an Rpackage NPcriterion.

A Utility Framework for Bounded-Loss Market Makers

  We introduce a class of utility-based market makers that always accept ordersat their risk-neutral prices. We derive necessary and sufficient conditions forsuch market makers to have bounded loss. We prove that hyperbolic absolute riskaversion utility market makers are equivalent to weighted pseudosphericalscoring rule market makers. In particular, Hanson's logarithmic scoring rulemarket maker corresponds to a negative exponential utility market maker in ourframework. We describe a third equivalent formulation based on maintaining acost function that seems most natural for implementation purposes, and weillustrate how to translate among the three equivalent formulations. We examinethe tradeoff between the market's liquidity and the market maker's worst-caseloss. For a fixed bound on worst-case loss, some market makers exhibit greaterliquidity near uniform prices and some exhibit greater liquidity near extremeprices, but no market maker can exhibit uniformly greater liquidity in allregimes. For a fixed minimum liquidity level, we give the lower bound of marketmaker's worst-case loss under some regularity conditions.

Quantitative Analysis of Automatic Image Cropping Algorithms: A Dataset  and Comparative Study

  Automatic photo cropping is an important tool for improving visual quality ofdigital photos without resorting to tedious manual selection. Traditionally,photo cropping is accomplished by determining the best proposal window throughvisual quality assessment or saliency detection. In essence, the performance ofan image cropper highly depends on the ability to correctly rank a number ofvisually similar proposal windows. Despite the ranking nature of automaticphoto cropping, little attention has been paid to learning-to-rank algorithmsin tackling such a problem. In this work, we conduct an extensive study ontraditional approaches as well as ranking-based croppers trained on variousimage features. In addition, a new dataset consisting of high quality croppingand pairwise ranking annotations is presented to evaluate the performance ofvarious baselines. The experimental results on the new dataset provide usefulinsights into the design of better photo cropping algorithms.

Learning to Compose with Professional Photographs on the Web

  Photo composition is an important factor affecting the aesthetics inphotography. However, it is a highly challenging task to model the aestheticproperties of good compositions due to the lack of globally applicable rules tothe wide variety of photographic styles. Inspired by the thinking process ofphoto taking, we formulate the photo composition problem as a view findingprocess which successively examines pairs of views and determines theiraesthetic preferences. We further exploit the rich professional photographs onthe web to mine unlimited high-quality ranking samples and demonstrate that anaesthetics-aware deep ranking network can be trained without explicitlymodeling any photographic rules. The resulting model is simple and effective interms of its architectural design and data sampling method. It is also genericsince it naturally learns any photographic rules implicitly encoded inprofessional photographs. The experiments show that the proposed view findingnetwork achieves state-of-the-art performance with sliding window searchstrategy on two image cropping datasets.

Complexity of Combinatorial Market Makers

  We analyze the computational complexity of market maker pricing algorithmsfor combinatorial prediction markets. We focus on Hanson's popular logarithmicmarket scoring rule market maker (LMSR). Our goal is to implicitly maintaincorrect LMSR prices across an exponentially large outcome space. We examineboth permutation combinatorics, where outcomes are permutations of objects, andBoolean combinatorics, where outcomes are combinations of binary events. Welook at three restrictive languages that limit what traders can bet on. Evenwith severely limited languages, we find that LMSR pricing is $\SP$-hard, evenwhen the same language admits polynomial-time matching without the marketmaker. We then propose an approximation technique for pricing permutationmarkets based on a recent algorithm for online permutation learning. Theconnections we draw between LMSR pricing and the vast literature on onlinelearning with expert advice may be of independent interest.

A New Understanding of Prediction Markets Via No-Regret Learning

  We explore the striking mathematical connections that exist between marketscoring rules, cost function based prediction markets, and no-regret learning.We show that any cost function based prediction market can be interpreted as analgorithm for the commonly studied problem of learning from expert advice byequating trades made in the market with losses observed by the learningalgorithm. If the loss of the market organizer is bounded, this bound can beused to derive an O(sqrt(T)) regret bound for the corresponding learningalgorithm. We then show that the class of markets with convex cost functionsexactly corresponds to the class of Follow the Regularized Leader learningalgorithms, with the choice of a cost function in the market corresponding tothe choice of a regularizer in the learning problem. Finally, we show anequivalence between market scoring rules and prediction markets with convexcost functions. This implies that market scoring rules can also be interpretednaturally as Follow the Regularized Leader algorithms, and may be ofindependent interest. These connections provide new insight into how it is thatcommonly studied markets, such as the Logarithmic Market Scoring Rule, canaggregate opinions into accurate estimates of the likelihood of future events.

Truthful Mechanisms for Agents that Value Privacy

  Recent work has constructed economic mechanisms that are both truthful anddifferentially private. In these mechanisms, privacy is treated separately fromthe truthfulness; it is not incorporated in players' utility functions (anddoing so has been shown to lead to non-truthfulness in some cases). In thiswork, we propose a new, general way of modelling privacy in players' utilityfunctions. Specifically, we only assume that if an outcome $o$ has the propertythat any report of player $i$ would have led to $o$ with approximately the sameprobability, then $o$ has small privacy cost to player $i$. We give threemechanisms that are truthful with respect to our modelling of privacy: for anelection between two candidates, for a discrete version of the facilitylocation problem, and for a general social choice problem with discreteutilities (via a VCG-like mechanism). As the number $n$ of players increases,the social welfare achieved by our mechanisms approaches optimal (as a fractionof $n$).

Capturing Variation and Uncertainty in Human Judgment

  The well-studied problem of statistical rank aggregation has been applied tocomparing sports teams, information retrieval, and most recently to datagenerated by human judgment. Such human-generated rankings may be substantiallydifferent from traditional statistical ranking data. In this work, we show thata recently proposed generalized random utility model reveals distinctivepatterns in human judgment across three different domains, and provides asuccinct representation of variance in both population preferences andimperfect perception. In contrast, we also show that classical statisticalranking models fail to capture important features from human-generated input.Our work motivates the use of more flexible ranking models for representing anddescribing the collective preferences or decision-making of human participants.

Learning to Incentivize: Eliciting Effort via Output Agreement

  In crowdsourcing when there is a lack of verification for contributedanswers, output agreement mechanisms are often used to incentivize participantsto provide truthful answers when the correct answer is hold by the majority. Inthis paper, we focus on using output agreement mechanisms to elicit effort, inaddition to eliciting truthful answers, from a population of workers. Weconsider a setting where workers have heterogeneous cost of effort exertion andexamine the data requester's problem of deciding the reward level in outputagreement for optimal elicitation. In particular, when the requester knows thecost distribution, we derive the optimal reward level for output agreementmechanisms. This is achieved by first characterizing Bayesian Nash equilibriaof output agreement mechanisms for a given reward level. When the requesterdoes not know the cost distribution, we develop sequential mechanisms thatcombine learning the cost distribution with incentivizing effort exertion toapproximately determine the optimal reward level.

Sequential Peer Prediction: Learning to Elicit Effort using Posted  Prices

  Peer prediction mechanisms are often adopted to elicit truthful contributionsfrom crowd workers when no ground-truth verification is available. Recently,mechanisms of this type have been developed to incentivize effort exertion, inaddition to truthful elicitation. In this paper, we study a sequential peerprediction problem where a data requester wants to dynamically determine thereward level to optimize the trade-off between the quality of informationelicited from workers and the total expected payment. In this problem, workershave homogeneous expertise and heterogeneous cost for exerting effort, bothunknown to the requester. We propose a sequential posted-price mechanism todynamically learn the optimal reward level from workers' contributions and toincentivize effort exertion and truthful reporting. We show that (1) in ourmechanism, workers exerting effort according to a non-degenerate thresholdpolicy and then reporting truthfully is an equilibrium that returns highestutility for each worker, and (2) The regret of our learning mechanism w.r.t.offering the optimal reward (price) is upper bounded by $\tilde{O}(T^{3/4})$where $T$ is the learning horizon. We further show the power of our learningapproach when the reports of workers do not necessarily follow thegame-theoretic equilibrium.

Informational Substitutes

  We propose definitions of substitutes and complements for pieces ofinformation ("signals") in the context of a decision or optimization problem,with game-theoretic and algorithmic applications. In a game-theoretic context,substitutes capture diminishing marginal value of information to a rationaldecision maker. We use the definitions to address the question of how and wheninformation is aggregated in prediction markets. Substitutes characterize"best-possible" equilibria with immediate information aggregation, whilecomplements characterize "worst-possible", delayed aggregation. Game-theoreticapplications also include settings such as crowdsourcing contests and Q\&Aforums. In an algorithmic context, where substitutes capture diminishingmarginal improvement of information to an optimization problem, substitutesimply efficient approximation algorithms for a very general class of (adaptive)information acquisition problems.  In tandem with these broad applications, we examine the structure and designof informational substitutes and complements. They have equivalent, intuitivedefinitions from disparate perspectives: submodularity, geometry, andinformation theory. We also consider the design of scoring rules oroptimization problems so as to encourage substitutability or complementarity,with positive and negative results. Taken as a whole, the results give someevidence that, in parallel with substitutable items, informational substitutesplay a natural conceptual and formal role in game theory and algorithms.

Fairness at Equilibrium in the Labor Market

  Recent literature on computational notions of fairness has been broadlydivided into two distinct camps, supporting interventions that address eitherindividual-based or group-based fairness. Rather than privilege a singledefinition, we seek to resolve both within the particular domain of employmentdiscrimination. To this end, we construct a dual labor market model composed ofa Temporary Labor Market, in which firm strategies are constrained to ensuregroup-level fairness, and a Permanent Labor Market, in which individual workerfairness is guaranteed. We show that such restrictions on hiring practicesinduces an equilibrium that Pareto-dominates those arising from strategies thatemploy statistical discrimination or a "group-blind" criterion. Individualworker reputations produce externalities for collective reputation, generatinga feedback loop termed a "self-fulfilling prophecy." Our model produces its ownfeedback loop, raising the collective reputation of an initially disadvantagedgroup via a fairness intervention that need not be permanent. Moreover, we showthat, contrary to popular assumption, the asymmetric equilibria resulting fromhiring practices that disregard group-fairness may be immovable withouttargeted intervention. The enduring nature of such equilibria that are bothinequitable and Pareto inefficient suggest that fairness interventions are ofcritical importance in moving the labor market to be more socially just andefficient.

Optimal Data Acquisition for Statistical Estimation

  We consider a data analyst's problem of purchasing data from strategic agentsto compute an unbiased estimate of a statistic of interest. Agents incurprivate costs to reveal their data and the costs can be arbitrarily correlatedwith their data. Once revealed, data are verifiable. This paper focuses onlinear unbiased estimators. We design an individually rational and incentivecompatible mechanism that optimizes the worst-case mean-squared error of theestimation, where the worst-case is over the unknown correlation between costsand data, subject to a budget constraint in expectation. We characterize theform of the optimal mechanism in closed-form. We further extend our results toacquiring data for estimating a parameter in regression analysis, where privatecosts can correlate with the values of the dependent variable but not with thevalues of the independent variables.

Strategyproof Linear Regression in High Dimensions

  This paper is part of an emerging line of work at the intersection of machinelearning and mechanism design, which aims to avoid noise in training data bycorrectly aligning the incentives of data sources. Specifically, we focus onthe ubiquitous problem of linear regression, where strategyproof mechanismshave previously been identified in two dimensions. In our setting, agents havesingle-peaked preferences and can manipulate only their response variables. Ourmain contribution is the discovery of a family of group strategyproof linearregression mechanisms in any number of dimensions, which we call generalizedresistant hyperplane mechanisms. The game-theoretic properties of thesemechanisms -- and, in fact, their very existence -- are established through aconnection to a discrete version of the Ham Sandwich Theorem.

Welfare and Distributional Impacts of Fair Classification

  Current methodologies in machine learning analyze the effects of variousstatistical parity notions of fairness primarily in light of their impacts onpredictive accuracy and vendor utility loss. In this paper, we propose a newframework for interpreting the effects of fairness criteria by converting theconstrained loss minimization problem into a social welfare maximizationproblem. This translation moves a classifier and its output into utility spacewhere individuals, groups, and society at-large experience different welfarechanges due to classification assignments. Under this characterization,predictions and fairness constraints are seen as shaping societal welfare anddistribution and revealing individuals' implied welfare weights insociety--weights that may then be interpreted through a fairness lens. Thesocial welfare formulation of the fairness problem brings to the fore concernsof distributive justice that have always had a central albeit more implicitrole in standard algorithmic fairness approaches.

Randomized Wagering Mechanisms

  Wagering mechanisms are one-shot betting mechanisms that elicit agents'predictions of an event. For deterministic wagering mechanisms, an existingimpossibility result has shown incompatibility of some desirable theoreticalproperties. In particular, Pareto optimality (no profitable side bet beforeallocation) can not be achieved together with weak incentive compatibility,weak budget balance and individual rationality. In this paper, we expand thedesign space of wagering mechanisms to allow randomization and ask whetherthere are randomized wagering mechanisms that can achieve all previouslyconsidered desirable properties, including Pareto optimality. We answer thisquestion positively with two classes of randomized wagering mechanisms: i) onesimple randomized lottery-type implementation of existing deterministicwagering mechanisms, and ii) another family of simple and randomized wageringmechanisms which we call surrogate wagering mechanisms, which are robust tonoisy ground truth. This family of mechanisms builds on the idea of learningwith noisy labels (Natarajan et al. 2013) as well as a recent extension of thisidea to the information elicitation without verification setting (Liu and Chen2018). We show that a broad family of randomized wagering mechanisms satisfyall desirable theoretical properties.

Elicitation for Aggregation

  We study the problem of eliciting and aggregating probabilistic informationfrom multiple agents. In order to successfully aggregate the predictions ofagents, the principal needs to elicit some notion of confidence from agents,capturing how much experience or knowledge led to their predictions. Toformalize this, we consider a principal who wishes to elicit predictions abouta random variable from a group of Bayesian agents, each of whom have privatelyobserved some independent samples of the random variable, and hopes toaggregate the predictions as if she had directly observed the samples of allagents. Leveraging techniques from Bayesian statistics, we represent confidenceas the number of samples an agent has observed, which is quantified by ahyperparameter from a conjugate family of prior distributions. This then allowsus to show that if the principal has access to a few samples, she can achieveher aggregation goal by eliciting predictions from agents using proper scoringrules. In particular, if she has access to one sample, she can successfullyaggregate the agents' predictions if and only if every posterior predictivedistribution corresponds to a unique value of the hyperparameter. Furthermore,this uniqueness holds for many common distributions of interest. When thisuniqueness property does not hold, we construct a novel and intuitive mechanismwhere a principal with two samples can elicit and optimally aggregate theagents' predictions.

Privacy Games

  The problem of analyzing the effect of privacy concerns on the behavior ofselfish utility-maximizing agents has received much attention lately. Privacyconcerns are often modeled by altering the utility functions of agents toconsider also their privacy loss. Such privacy aware agents prefer to take arandomized strategy even in very simple games in which non-privacy aware agentsplay pure strategies. In some cases, the behavior of privacy aware agentsfollows the framework of Randomized Response, a well-known mechanism thatpreserves differential privacy.  Our work is aimed at better understanding the behavior of agents in settingswhere their privacy concerns are explicitly given. We consider a toy settingwhere agent A, in an attempt to discover the secret type of agent B, offers B agift that one type of B agent likes and the other type dislikes. As opposed toprevious works, B's incentive to keep her type a secret isn't the result of"hardwiring" B's utility function to consider privacy, but rather takes theform of a payment between B and A. We investigate three different types ofpayment functions and analyze B's behavior in each of the resulting games. Aswe show, under some payments, B's behavior is very different than the behaviorof agents with hardwired privacy concerns and might even be deterministic.Under a different payment we show that B's BNE strategy does fall into theframework of Randomized Response.

Plasma Diagnostics Using K-Line Emission Profiles of Argon

  K-line profiles emitted from a warm dense plasma environment are used fordiagnostics of Ar droplet plasmas created by high energy laser pulses. Weobserve temperature gradients within the Ar droplet from cold temperatures ofthe order of some 10 eV up to higher temperatures of about 170 eV.Non-perturbative wave functions are calculated as well as ionization energies,binding energies and relevant emission energies using a chemical {\it abinitio} code. The plasma screening is considered within a perturbative approachto the Hamiltonian. The plasma effect influences the many-particle systemresulting in energy shifts due to electron-ion and electron-electroninteraction. With this approach we get a good reproduction of spectral featuresthat are strongly influenced by ionization and excitation processes within theplasma. Comparing with the widely known FLYCHK code, counting for internaldegrees of freedom (bound states) and treating pressure ionization within ourquantum statistical approach leads to different results for the inferredtemperature distribution.

Low-Cost Learning via Active Data Procurement

  We design mechanisms for online procurement of data held by strategic agentsfor machine learning tasks. The challenge is to use past data to actively pricefuture data and give learning guarantees even when an agent's cost forrevealing her data may depend arbitrarily on the data itself. We achieve thisgoal by showing how to convert a large class of no-regret algorithms intoonline posted-price and learning mechanisms. Our results in a sense parallelclassic sample complexity guarantees, but with the key resource being moneyrather than quantity of data: With a budget constraint $B$, we give robust risk(predictive error) bounds on the order of $1/\sqrt{B}$. Because we use anactive approach, we can often guarantee to do significantly better byleveraging correlations between costs and data.  Our algorithms and analysis go through a model of no-regret learning with $T$arriving pairs (cost, data) and a budget constraint of $B$. Our regret boundsfor this model are on the order of $T/\sqrt{B}$ and we give lower bounds on thesame order.

TROM: A Testing-based Method for Finding Transcriptomic Similarity of  Biological Samples

  Comparative transcriptomics has gained increasing popularity in genomicresearch thanks to the development of high-throughput technologies includingmicroarray and next-generation RNA sequencing that have generated numeroustranscriptomic data. An important question is to understand the conservationand differentiation of biological processes in different species. We propose atesting-based method TROM (Transcriptome Overlap Measure) for comparingtranscriptomes within or between different species, and provide a differentperspective to interpret transcriptomic similarity in contrast to traditionalcorrelation analyses. Specifically, the TROM method focuses on identifyingassociated genes that capture molecular characteristics of biological samples,and subsequently comparing the biological samples by testing the overlap oftheir associated genes. We use simulation and real data studies to demonstratethat TROM is more powerful in identifying similar transcriptomes and morerobust to stochastic gene expression noise than Pearson and Spearmancorrelations. We apply TROM to compare the developmental stages of sixDrosophila species, C. elegans, S. purpuratus, D. rerio and mouse liver, andfind interesting correspondence patterns that imply conserved gene expressionprograms in the development of these species. The TROM method is available asan R package on CRAN (http://cran.r-project.org/) with manuals and source codesavailable at http://www.stat.ucla.edu/ jingyi.li/software-and-data/trom.html.

Active Information Acquisition for Linear Optimization

  We consider partially-specified optimization problems where the goal is toactively, but efficiently, acquire missing information about the problem inorder to solve it. An algorithm designer wishes to solve a linear program (LP),$\max \mathbf{c}^T \mathbf{x}$ s.t. $\mathbf{A}\mathbf{x} \leq \mathbf{b},\mathbf{x} \ge \mathbf{0}$, but does not initially know some of the parameters.The algorithm can iteratively choose an unknown parameter and gatherinformation in the form of a noisy sample centered at the parameter's (unknown)value. The goal is to find an approximately feasible and optimal solution tothe underlying LP with high probability while drawing a small number ofsamples. We focus on two cases. (1) When the parameters $\mathbf{c}$ of theobjective are initially unknown, we take an information-theoretic approach andgive roughly matching upper and lower sample complexity bounds, with an(inefficient) successive-elimination algorithm. (2) When the parameters$\mathbf{b}$ of the constraints are initially unknown, we propose an efficientalgorithm combining techniques from the ellipsoid method for LP andconfidence-bound approaches from bandit algorithms. The algorithm adaptivelygathers information about constraints only as needed in order to make progress.We give sample complexity bounds for the algorithm and demonstrate itsimprovement over a naive approach via simulation.

A Short-term Intervention for Long-term Fairness in the Labor Market

  The persistence of racial inequality in the U.S. labor market against ageneral backdrop of formal equality of opportunity is a troubling phenomenonthat has significant ramifications on the design of hiring policies. In thispaper, we show that current group disparate outcomes may be immovable even whenhiring decisions are bound by an input-output notion of "individual fairness."Instead, we construct a dynamic reputational model of the labor market thatillustrates the reinforcing nature of asymmetric outcomes resulting fromgroups' divergent accesses to resources and as a result, investment choices. Toaddress these disparities, we adopt a dual labor market composed of a TemporaryLabor Market (TLM), in which firms' hiring strategies are constrained to ensurestatistical parity of workers granted entry into the pipeline, and a PermanentLabor Market (PLM), in which firms hire top performers as desired. Individualworker reputations produce externalities for their group; the correspondingfeedback loop raises the collective reputation of the initially disadvantagedgroup via a TLM fairness intervention that need not be permanent. We show thatsuch a restriction on hiring practices induces an equilibrium that, underparticular market conditions, Pareto-dominates those arising from strategiesthat statistically discriminate or employ a "group-blind" criterion. Theenduring nature of equilibria that are both inequitable and Pareto suboptimalsuggests that fairness interventions beyond procedural checks of hiringdecisions will be of critical importance in a world where machines play agreater role in the employment process.

Prior-free Data Acquisition for Accurate Statistical Estimation

  We study a data analyst's problem of acquiring data from self-interestedindividuals to obtain an accurate estimation of some statistic of a population,subject to an expected budget constraint. Each data holder incurs a cost, whichis unknown to the data analyst, to acquire and report his data. The cost can bearbitrarily correlated with the data. The data analyst has an expected budgetthat she can use to incentivize individuals to provide their data. The goal isto design a joint acquisition-estimation mechanism to optimize the performanceof the produced estimator, without any prior information on the underlyingdistribution of cost and data. We investigate two types of estimations:unbiased point estimation and confidence interval estimation.  Unbiased estimators: We design a truthful, individually rational, onlinemechanism to acquire data from individuals and output an unbiased estimator ofthe population mean when the data analyst has no prior information on thecost-data distribution and individuals arrive in a random order. Theperformance of this mechanism matches that of the optimal mechanism, whichknows the true cost distribution, within a constant factor. The performance ofan estimator is evaluated by its variance under the worst-case cost-datacorrelation.  Confidence intervals: We characterize an approximately optimal (within afactor $2$) mechanism for obtaining a confidence interval of the populationmean when the data analyst knows the true cost distribution at the beginning.This mechanism is efficiently computable. We then design a truthful,individually rational, online algorithm that is only worse than theapproximately optimal mechanism by a constant factor. The performance of anestimator is evaluated by its expected length under the worst-case cost-datacorrelation.

An Optimization-Based Framework for Automated Market-Making

  Building on ideas from online convex optimization, we propose a generalframework for the design of efficient securities markets over very largeoutcome spaces. The challenge here is computational. In a complete market, inwhich one security is offered for each outcome, the market institution can notefficiently keep track of the transaction history or calculate security priceswhen the outcome space is large. The natural solution is to restrict the spaceof securities to be much smaller than the outcome space in such a way thatsecurities can be priced efficiently. Recent research has focused on searchingfor spaces of securities that can be priced efficiently by existing mechanismsdesigned for complete markets. While there have been some successes, much ofthis research has led to hardness results. In this paper, we take a drasticallydifferent approach. We start with an arbitrary space of securities with boundedpayoff, and establish a framework to design markets tailored to this space. Weprove that any market satisfying a set of intuitive conditions must pricesecurities via a convex potential function and that the space of reachableprices must be precisely the convex hull of the security payoffs. We then showhow the convex potential function can be defined in terms of an optimizationover the convex hull of the security payoffs. The optimal solution to theoptimization problem gives the security prices. Using this framework, weprovide an efficient market for predicting the landing location of an object ona sphere. In addition, we show that we can relax our "no-arbitrage" conditionto design a new efficient market maker for pair betting, which is known to be#P-hard to price using existing mechanisms. This relaxation also allows themarket maker to charge transaction fees so that the depth of the market can bedynamically increased as the number of trades increases.

Group buying with bundle discounts: computing efficient, stable and fair  solutions

  We model a market in which nonstrategic vendors sell items of different typesand offer bundles at discounted prices triggered by demand volumes. Each buyeracts strategically in order to maximize her utility, given by the differencebetween product valuation and price paid. Buyers report their valuations interms of reserve prices on sets of items, and might be willing to pay pricesdifferent than the market price in order to subsidize other buyers and totrigger discounts. The resulting price discrimination can be interpreted as aredistribution of the total discount. We consider a notion of stability thatlooks at unilateral deviations, and show that efficient allocations - the onesmaximizing the social welfare - can be stabilized by prices that enjoydesirable properties of rationality and fairness. These dictate that buyers payhigher prices only to subsidize others who contribute to the activation of thedesired discounts, and that they pay premiums over the discounted priceproportionally to their surplus - the difference between their current utilityand the utility of their best alternative. Therefore, the resulting pricediscrimination appears to be desirable to buyers. Building on this existenceresult, and letting N, M and c be the numbers of buyers, vendors and producttypes, we propose a O(N^2+NM^c) algorithm that, given an efficient allocation,computes prices that are rational and fair and that stabilize the market. Thealgorithm first determines the redistribution of the discount between groups ofbuyers with an equal product choice, and then computes single buyers' prices.Our results show that if a desirable form of price discrimination isimplemented then social efficiency and stability can coexists in a marketpresenting subtle externalities, and computing individual prices from marketprices is tractable.

Surrogate Scoring Rules and a Uniform Dominant Truth Serum

  Strictly proper scoring rules (SPSR) are widely used when designing incentivemechanisms to elicit private information from strategic agents using realizedground truth signals, and they can help quantify the value of elicitedinformation. In this paper, we extend such scoring rules to settings where amechanism designer does not have access to ground truth. We consider two suchsettings: (i) a setting when the mechanism designer has access to a noisy proxyversion of the ground truth, with {\em known} biases; and (ii) the standardpeer prediction setting where agents' reports, and possibly some limited priorknowledge of ground truth, are the only source of information that themechanism designer has.  We introduce {\em surrogate scoring rules} (SSR) for the first setting, whichuse the noisy ground truth to evaluate quality of elicited information. We showthat SSR preserves the strict properness of SPSR. Using SSR, we then develop amulti-task scoring mechanism -- called \emph{uniform dominant truth serum}(DTS) -- to achieve strict properness when there are sufficiently many tasksand agents, and when the mechanism designer only has access to agents' reportsand one bit information about the marginal of the entire set of tasks' groundtruth. In comparison to standard equilibrium concepts in peer prediction, weshow that DTS can achieve truthfulness in \emph{uniform dominant strategy} in amulti-task setting when agents adopt the same strategy for all the tasks thatthey are assigned (hence the term uniform). A salient feature of SSR and DTS isthat they quantify the quality of information despite lack of ground truth,just as proper scoring rules do for the {\em with} verification setting. Ourmethod is verified both theoretically and empirically using data collected fromreal human participants.

