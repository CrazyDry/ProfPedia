NELA-GT-2018: A Large Multi-Labelled News Dataset for The Study of
  Misinformation in News Articles

  In this paper, we present a dataset of 713k articles collected between
02/2018-11/2018. These articles are collected directly from 194 news and media
outlets including mainstream, hyper-partisan, and conspiracy sources. We
incorporate ground truth ratings of the sources from 8 different assessment
sites covering multiple dimensions of veracity, including reliability, bias,
transparency, adherence to journalistic standards, and consumer trust. The
NELA-GT-2018 dataset can be found at https://doi.org/10.7910/DVN/ULHLCB.


Mechanism Design for Multi-Type Housing Markets

  We study multi-type housing markets, where there are $p\ge 2$ types of items,
each agent is initially endowed one item of each type, and the goal is to
design mechanisms without monetary transfer to (re)allocate items to the agents
based on their preferences over bundles of items, such that each agent gets one
item of each type. In sharp contrast to classical housing markets, previous
studies in multi-type housing markets have been hindered by the lack of natural
solution concepts, because the strict core might be empty.
  We break the barrier in the literature by leveraging AI techniques and making
natural assumptions on agents' preferences. We show that when agents'
preferences are lexicographic, even with different importance orders, the
classical top-trading-cycles mechanism can be extended while preserving most of
its nice properties. We also investigate computational complexity of checking
whether an allocation is in the strict core and checking whether the strict
core is empty. Our results convey an encouragingly positive message: it is
possible to design good mechanisms for multi-type housing markets under natural
assumptions on preferences.


The Impact of Crowds on News Engagement: A Reddit Case Study

  Today, users are reading the news through social platforms. These platforms
are built to facilitate crowd engagement, but not necessarily disseminate
useful news to inform the masses. Hence, the news that is highly engaged with
may not be the news that best informs. While predicting news popularity has
been well studied, it has not been studied in the context of crowd
manipulations. In this paper, we provide some preliminary results to a longer
term project on crowd and platform manipulations of news and news popularity.
In particular, we choose to study known features for predicting news popularity
and how those features may change on reddit.com, a social platform used
commonly for news aggregation. Along with this, we explore ways in which users
can alter the perception of news through changing the title of an article. We
find that news on reddit is predictable using previously studied sentiment and
content features and that posts with titles changed by reddit users tend to be
more popular than posts with the original article title.


An Exploration of Unreliable News Classification in Brazil and The U.S

  The propagation of unreliable information is on the rise in many places
around the world. This expansion is facilitated by the rapid spread of
information and anonymity granted by the Internet. The spread of unreliable
information is a wellstudied issue and it is associated with negative social
impacts. In a previous work, we have identified significant differences in the
structure of news articles from reliable and unreliable sources in the US
media. Our goal in this work was to explore such differences in the Brazilian
media. We found significant features in two data sets: one with Brazilian news
in Portuguese and another one with US news in English. Our results show that
features related to the writing style were prominent in both data sets and,
despite the language difference, some features have a universal behavior, being
significant to both US and Brazilian news articles. Finally, we combined both
data sets and used the universal features to build a machine learning
classifier to predict the source type of a news article as reliable or
unreliable.


Is Uncertainty Always Bad?: Effect of Topic Competence on Uncertain
  Opinions

  The proliferation of information disseminated by public/social media has made
decision-making highly challenging due to the wide availability of noisy,
uncertain, or unverified information. Although the issue of uncertainty in
information has been studied for several decades, little work has investigated
how noisy (or uncertain) or valuable (or credible) information can be
formulated into people's opinions, modeling uncertainty both in the quantity
and quality of evidence leading to a specific opinion. In this work, we model
and analyze an opinion and information model by using Subjective Logic where
the initial set of evidence is mixed with different types of evidence (i.e.,
pro vs. con or noisy vs. valuable) which is incorporated into the opinions of
original propagators, who propagate information over a network. With the help
of an extensive simulation study, we examine how the different ratios of
information types or agents' prior belief or topic competence affect the
overall information diffusion. Based on our findings, agents' high uncertainty
is not necessarily always bad in making a right decision as long as they are
competent enough not to be at least biased towards false information (e.g.,
neutral between two extremes).


Rating Reliability and Bias in News Articles: Does AI Assistance Help
  Everyone?

  With the spread of false and misleading information in current news, many
algorithmic tools have been introduced with the aim of assessing bias and
reliability in written content. However, there has been little work exploring
how effective these tools are at changing human perceptions of content. To this
end, we conduct a study with 654 participants to understand if algorithmic
assistance improves the accuracy of reliability and bias perceptions, and
whether there is a difference in the effectiveness of the AI assistance for
different types of news consumers. We find that AI assistance with
feature-based explanations improves the accuracy of news perceptions. However,
some consumers are helped more than others. Specifically, we find that
participants who read and share news often on social media are worse at
recognizing bias and reliability issues in news articles than those who do not,
while frequent news readers and those familiar with politics perform much
better. We discuss these differences and their implication to offer insights
for future research.


Different Spirals of Sameness: A Study of Content Sharing in Mainstream
  and Alternative Media

  In this paper, we analyze content sharing between news sources in the
alternative and mainstream media using a dataset of 713K articles and 194
sources. We find that content sharing happens in tightly formed communities,
and these communities represent relatively homogeneous portions of the media
landscape. Through a mix-method analysis, we find several primary content
sharing behaviors. First, we find that the vast majority of shared articles are
only shared with similar news sources (i.e. same community). Second, we find
that despite these echo-chambers of sharing, specific sources, such as The
Drudge Report, mix content from both mainstream and conspiracy communities.
Third, we show that while these differing communities do not always share news
articles, they do report on the same events, but often with competing and
counter-narratives. Overall, we find that the news is homogeneous within
communities and diverse in between, creating different spirals of sameness.


An Analysis of Optimal Link Bombs

  We analyze the phenomenon of collusion for the purpose of boosting the
pagerank of a node in an interlinked environment. We investigate the optimal
attack pattern for a group of nodes (attackers) attempting to improve the
ranking of a specific node (the victim). We consider attacks where the
attackers can only manipulate their own outgoing links. We show that the
optimal attacks in this scenario are uncoordinated, i.e. the attackers link
directly to the victim and no one else. nodes do not link to each other. We
also discuss optimal attack patterns for a group that wants to hide itself by
not pointing directly to the victim. In these disguised attacks, the attackers
link to nodes $l$ hops away from the victim. We show that an optimal disguised
attack exists and how it can be computed. The optimal disguised attack also
allows us to find optimal link farm configurations. A link farm can be
considered a special case of our approach: the target page of the link farm is
the victim and the other nodes in the link farm are the attackers for the
purpose of improving the rank of the victim. The target page can however
control its own outgoing links for the purpose of improving its own rank, which
can be modeled as an optimal disguised attack of 1-hop on itself. Our results
are unique in the literature as we show optimality not only in the pagerank
score, but also in the rank based on the pagerank score. We further validate
our results with experiments on a variety of random graph models.


This Just In: Fake News Packs a Lot in Title, Uses Simpler, Repetitive
  Content in Text Body, More Similar to Satire than Real News

  The problem of fake news has gained a lot of attention as it is claimed to
have had a significant impact on 2016 US Presidential Elections. Fake news is
not a new problem and its spread in social networks is well-studied. Often an
underlying assumption in fake news discussion is that it is written to look
like real news, fooling the reader who does not check for reliability of the
sources or the arguments in its content. Through a unique study of three data
sets and features that capture the style and the language of articles, we show
that this assumption is not true. Fake news in most cases is more similar to
satire than to real news, leading us to conclude that persuasion in fake news
is achieved through heuristics rather than the strength of arguments. We show
overall title structure and the use of proper nouns in titles are very
significant in differentiating fake from real. This leads us to conclude that
fake news is targeted for audiences who are not likely to read beyond titles
and is aimed at creating mental associations between entities and claims.


Identifying the social signals that drive online discussions: A case
  study of Reddit communities

  Increasingly people form opinions based on information they consume on online
social media. As a result, it is crucial to understand what type of content
attracts people's attention on social media and drive discussions. In this
paper we focus on online discussions. Can we predict which comments and what
content gets the highest attention in an online discussion? How does this
content differ from community to community? To accomplish this, we undertake a
unique study of Reddit involving a large sample comments from 11 popular
subreddits with different properties. We introduce a large number of sentiment,
relevance, content analysis features including some novel features customized
to reddit. Through a comparative analysis of the chosen subreddits, we show
that our models are correctly able to retrieve top replies under a post with
great precision. In addition, we explain our findings with a detailed analysis
of what distinguishes high scoring posts in different communities that differ
along the dimensions of the specificity of topic and style, audience and level
of moderation.


Sampling the News Producers: A Large News and Feature Data Set for the
  Study of the Complex Media Landscape

  The complexity and diversity of today's media landscape provides many
challenges for researchers studying news producers. These producers use many
different strategies to get their message believed by readers through the
writing styles they employ, by repetition across different media sources with
or without attribution, as well as other mechanisms that are yet to be studied
deeply. To better facilitate systematic studies in this area, we present a
large political news data set, containing over 136K news articles, from 92 news
sources, collected over 7 months of 2017. These news sources are carefully
chosen to include well-established and mainstream sources, maliciously fake
sources, satire sources, and hyper-partisan political blogs. In addition to
each article we compute 130 content-based and social media engagement features
drawn from a wide range of literature on political bias, persuasion, and
misinformation. With the release of the data set, we also provide the source
code for feature computation. In this paper, we discuss the first release of
the data set and demonstrate 4 use cases of the data and features: news
characterization, engagement characterization, news attribution and content
copying, and discovering news narratives.


An Exploration of Verbatim Content Republishing by News Producers

  In today's news ecosystem, news sources emerge frequently and can vary widely
in intent. This intent can range from benign to malicious, with many tactics
being used to achieve their goals. One lesser studied tactic is content
republishing, which can be used to make specific stories seem more important,
create uncertainty around an event, or create a perception of credibility for
unreliable news sources. In this paper, we take a first step in understanding
this tactic by exploring verbatim content copying across 92 news producers of
various characteristics. We find that content copying occurs more frequently
between like-audience sources (eg. alternative news, mainstream news, etc.),
but there consistently exists sparse connections between these communities. We
also find that despite articles being verbatim, the headlines are often
changed. Specifically, we find that mainstream sources change more structural
features, while alternative sources change many more content features, often
changing the emotional tone and bias of the titles. We conclude that content
republishing networks can help identify and label the intent of brand-new news
sources using the tight-knit community they belong to. In addition, it is
possible to use the network to find important content producers in each
community, producers that are used to amplify messages of other sources, and
producers that distort the messages of other sources.


Models for Predicting Community-Specific Interest in News Articles

  In this work, we ask two questions: 1. Can we predict the type of community
interested in a news article using only features from the article content? and
2. How well do these models generalize over time? To answer these questions, we
compute well-studied content-based features on over 60K news articles from 4
communities on reddit.com. We train and test models over three different time
periods between 2015 and 2017 to demonstrate which features degrade in
performance the most due to concept drift. Our models can classify news
articles into communities with high accuracy, ranging from 0.81 ROC AUC to 1.0
ROC AUC. However, while we can predict the community-specific popularity of
news articles with high accuracy, practitioners should approach these models
carefully. Predictions are both community-pair dependent and feature group
dependent. Moreover, these feature groups generalize over time differently,
with some only degrading slightly over time, but others degrading greatly.
Therefore, we recommend that community-interest predictions are done in a
hierarchical structure, where multiple binary classifiers can be used to
separate community pairs, rather than a traditional multi-class model. Second,
these models should be retrained over time based on accuracy goals and the
availability of training data.


