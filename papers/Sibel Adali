NELA-GT-2018: A Large Multi-Labelled News Dataset for The Study of  Misinformation in News Articles

  In this paper, we present a dataset of 713k articles collected between02/2018-11/2018. These articles are collected directly from 194 news and mediaoutlets including mainstream, hyper-partisan, and conspiracy sources. Weincorporate ground truth ratings of the sources from 8 different assessmentsites covering multiple dimensions of veracity, including reliability, bias,transparency, adherence to journalistic standards, and consumer trust. TheNELA-GT-2018 dataset can be found at https://doi.org/10.7910/DVN/ULHLCB.

Mechanism Design for Multi-Type Housing Markets

  We study multi-type housing markets, where there are $p\ge 2$ types of items,each agent is initially endowed one item of each type, and the goal is todesign mechanisms without monetary transfer to (re)allocate items to the agentsbased on their preferences over bundles of items, such that each agent gets oneitem of each type. In sharp contrast to classical housing markets, previousstudies in multi-type housing markets have been hindered by the lack of naturalsolution concepts, because the strict core might be empty.  We break the barrier in the literature by leveraging AI techniques and makingnatural assumptions on agents' preferences. We show that when agents'preferences are lexicographic, even with different importance orders, theclassical top-trading-cycles mechanism can be extended while preserving most ofits nice properties. We also investigate computational complexity of checkingwhether an allocation is in the strict core and checking whether the strictcore is empty. Our results convey an encouragingly positive message: it ispossible to design good mechanisms for multi-type housing markets under naturalassumptions on preferences.

The Impact of Crowds on News Engagement: A Reddit Case Study

  Today, users are reading the news through social platforms. These platformsare built to facilitate crowd engagement, but not necessarily disseminateuseful news to inform the masses. Hence, the news that is highly engaged withmay not be the news that best informs. While predicting news popularity hasbeen well studied, it has not been studied in the context of crowdmanipulations. In this paper, we provide some preliminary results to a longerterm project on crowd and platform manipulations of news and news popularity.In particular, we choose to study known features for predicting news popularityand how those features may change on reddit.com, a social platform usedcommonly for news aggregation. Along with this, we explore ways in which userscan alter the perception of news through changing the title of an article. Wefind that news on reddit is predictable using previously studied sentiment andcontent features and that posts with titles changed by reddit users tend to bemore popular than posts with the original article title.

An Exploration of Unreliable News Classification in Brazil and The U.S

  The propagation of unreliable information is on the rise in many placesaround the world. This expansion is facilitated by the rapid spread ofinformation and anonymity granted by the Internet. The spread of unreliableinformation is a wellstudied issue and it is associated with negative socialimpacts. In a previous work, we have identified significant differences in thestructure of news articles from reliable and unreliable sources in the USmedia. Our goal in this work was to explore such differences in the Brazilianmedia. We found significant features in two data sets: one with Brazilian newsin Portuguese and another one with US news in English. Our results show thatfeatures related to the writing style were prominent in both data sets and,despite the language difference, some features have a universal behavior, beingsignificant to both US and Brazilian news articles. Finally, we combined bothdata sets and used the universal features to build a machine learningclassifier to predict the source type of a news article as reliable orunreliable.

Is Uncertainty Always Bad?: Effect of Topic Competence on Uncertain  Opinions

  The proliferation of information disseminated by public/social media has madedecision-making highly challenging due to the wide availability of noisy,uncertain, or unverified information. Although the issue of uncertainty ininformation has been studied for several decades, little work has investigatedhow noisy (or uncertain) or valuable (or credible) information can beformulated into people's opinions, modeling uncertainty both in the quantityand quality of evidence leading to a specific opinion. In this work, we modeland analyze an opinion and information model by using Subjective Logic wherethe initial set of evidence is mixed with different types of evidence (i.e.,pro vs. con or noisy vs. valuable) which is incorporated into the opinions oforiginal propagators, who propagate information over a network. With the helpof an extensive simulation study, we examine how the different ratios ofinformation types or agents' prior belief or topic competence affect theoverall information diffusion. Based on our findings, agents' high uncertaintyis not necessarily always bad in making a right decision as long as they arecompetent enough not to be at least biased towards false information (e.g.,neutral between two extremes).

Rating Reliability and Bias in News Articles: Does AI Assistance Help  Everyone?

  With the spread of false and misleading information in current news, manyalgorithmic tools have been introduced with the aim of assessing bias andreliability in written content. However, there has been little work exploringhow effective these tools are at changing human perceptions of content. To thisend, we conduct a study with 654 participants to understand if algorithmicassistance improves the accuracy of reliability and bias perceptions, andwhether there is a difference in the effectiveness of the AI assistance fordifferent types of news consumers. We find that AI assistance withfeature-based explanations improves the accuracy of news perceptions. However,some consumers are helped more than others. Specifically, we find thatparticipants who read and share news often on social media are worse atrecognizing bias and reliability issues in news articles than those who do not,while frequent news readers and those familiar with politics perform muchbetter. We discuss these differences and their implication to offer insightsfor future research.

Different Spirals of Sameness: A Study of Content Sharing in Mainstream  and Alternative Media

  In this paper, we analyze content sharing between news sources in thealternative and mainstream media using a dataset of 713K articles and 194sources. We find that content sharing happens in tightly formed communities,and these communities represent relatively homogeneous portions of the medialandscape. Through a mix-method analysis, we find several primary contentsharing behaviors. First, we find that the vast majority of shared articles areonly shared with similar news sources (i.e. same community). Second, we findthat despite these echo-chambers of sharing, specific sources, such as TheDrudge Report, mix content from both mainstream and conspiracy communities.Third, we show that while these differing communities do not always share newsarticles, they do report on the same events, but often with competing andcounter-narratives. Overall, we find that the news is homogeneous withincommunities and diverse in between, creating different spirals of sameness.

An Analysis of Optimal Link Bombs

  We analyze the phenomenon of collusion for the purpose of boosting thepagerank of a node in an interlinked environment. We investigate the optimalattack pattern for a group of nodes (attackers) attempting to improve theranking of a specific node (the victim). We consider attacks where theattackers can only manipulate their own outgoing links. We show that theoptimal attacks in this scenario are uncoordinated, i.e. the attackers linkdirectly to the victim and no one else. nodes do not link to each other. Wealso discuss optimal attack patterns for a group that wants to hide itself bynot pointing directly to the victim. In these disguised attacks, the attackerslink to nodes $l$ hops away from the victim. We show that an optimal disguisedattack exists and how it can be computed. The optimal disguised attack alsoallows us to find optimal link farm configurations. A link farm can beconsidered a special case of our approach: the target page of the link farm isthe victim and the other nodes in the link farm are the attackers for thepurpose of improving the rank of the victim. The target page can howevercontrol its own outgoing links for the purpose of improving its own rank, whichcan be modeled as an optimal disguised attack of 1-hop on itself. Our resultsare unique in the literature as we show optimality not only in the pagerankscore, but also in the rank based on the pagerank score. We further validateour results with experiments on a variety of random graph models.

This Just In: Fake News Packs a Lot in Title, Uses Simpler, Repetitive  Content in Text Body, More Similar to Satire than Real News

  The problem of fake news has gained a lot of attention as it is claimed tohave had a significant impact on 2016 US Presidential Elections. Fake news isnot a new problem and its spread in social networks is well-studied. Often anunderlying assumption in fake news discussion is that it is written to looklike real news, fooling the reader who does not check for reliability of thesources or the arguments in its content. Through a unique study of three datasets and features that capture the style and the language of articles, we showthat this assumption is not true. Fake news in most cases is more similar tosatire than to real news, leading us to conclude that persuasion in fake newsis achieved through heuristics rather than the strength of arguments. We showoverall title structure and the use of proper nouns in titles are verysignificant in differentiating fake from real. This leads us to conclude thatfake news is targeted for audiences who are not likely to read beyond titlesand is aimed at creating mental associations between entities and claims.

Identifying the social signals that drive online discussions: A case  study of Reddit communities

  Increasingly people form opinions based on information they consume on onlinesocial media. As a result, it is crucial to understand what type of contentattracts people's attention on social media and drive discussions. In thispaper we focus on online discussions. Can we predict which comments and whatcontent gets the highest attention in an online discussion? How does thiscontent differ from community to community? To accomplish this, we undertake aunique study of Reddit involving a large sample comments from 11 popularsubreddits with different properties. We introduce a large number of sentiment,relevance, content analysis features including some novel features customizedto reddit. Through a comparative analysis of the chosen subreddits, we showthat our models are correctly able to retrieve top replies under a post withgreat precision. In addition, we explain our findings with a detailed analysisof what distinguishes high scoring posts in different communities that differalong the dimensions of the specificity of topic and style, audience and levelof moderation.

Sampling the News Producers: A Large News and Feature Data Set for the  Study of the Complex Media Landscape

  The complexity and diversity of today's media landscape provides manychallenges for researchers studying news producers. These producers use manydifferent strategies to get their message believed by readers through thewriting styles they employ, by repetition across different media sources withor without attribution, as well as other mechanisms that are yet to be studieddeeply. To better facilitate systematic studies in this area, we present alarge political news data set, containing over 136K news articles, from 92 newssources, collected over 7 months of 2017. These news sources are carefullychosen to include well-established and mainstream sources, maliciously fakesources, satire sources, and hyper-partisan political blogs. In addition toeach article we compute 130 content-based and social media engagement featuresdrawn from a wide range of literature on political bias, persuasion, andmisinformation. With the release of the data set, we also provide the sourcecode for feature computation. In this paper, we discuss the first release ofthe data set and demonstrate 4 use cases of the data and features: newscharacterization, engagement characterization, news attribution and contentcopying, and discovering news narratives.

An Exploration of Verbatim Content Republishing by News Producers

  In today's news ecosystem, news sources emerge frequently and can vary widelyin intent. This intent can range from benign to malicious, with many tacticsbeing used to achieve their goals. One lesser studied tactic is contentrepublishing, which can be used to make specific stories seem more important,create uncertainty around an event, or create a perception of credibility forunreliable news sources. In this paper, we take a first step in understandingthis tactic by exploring verbatim content copying across 92 news producers ofvarious characteristics. We find that content copying occurs more frequentlybetween like-audience sources (eg. alternative news, mainstream news, etc.),but there consistently exists sparse connections between these communities. Wealso find that despite articles being verbatim, the headlines are oftenchanged. Specifically, we find that mainstream sources change more structuralfeatures, while alternative sources change many more content features, oftenchanging the emotional tone and bias of the titles. We conclude that contentrepublishing networks can help identify and label the intent of brand-new newssources using the tight-knit community they belong to. In addition, it ispossible to use the network to find important content producers in eachcommunity, producers that are used to amplify messages of other sources, andproducers that distort the messages of other sources.

Models for Predicting Community-Specific Interest in News Articles

  In this work, we ask two questions: 1. Can we predict the type of communityinterested in a news article using only features from the article content? and2. How well do these models generalize over time? To answer these questions, wecompute well-studied content-based features on over 60K news articles from 4communities on reddit.com. We train and test models over three different timeperiods between 2015 and 2017 to demonstrate which features degrade inperformance the most due to concept drift. Our models can classify newsarticles into communities with high accuracy, ranging from 0.81 ROC AUC to 1.0ROC AUC. However, while we can predict the community-specific popularity ofnews articles with high accuracy, practitioners should approach these modelscarefully. Predictions are both community-pair dependent and feature groupdependent. Moreover, these feature groups generalize over time differently,with some only degrading slightly over time, but others degrading greatly.Therefore, we recommend that community-interest predictions are done in ahierarchical structure, where multiple binary classifiers can be used toseparate community pairs, rather than a traditional multi-class model. Second,these models should be retrained over time based on accuracy goals and theavailability of training data.

