Non-Destructive Discrimination of arbitrary set of orthogonal quantum  states by NMR using Quantum Phase Estimation

  An algorithm based on quantum phase estimation, which discriminates quantumstates nondestructively within a set of arbitrary orthogonal states, isdescribed and experimentally verified by a NMR quantum information processor.The procedure is scalable and can be applied to any set of orthogonal states.Scalability is demonstrated through Matlab simulation.

Singlet state creation and Universal quantum computation in NMR using  Genetic Algorithm

  Experimental implementation of a quantum algorithm requires unitary operatordecomposition. Here we treat the unitary operator decomposition as anoptimization problem and use Genetic Algorithm, a global optimization methodinspired by nature's evolutionary process for operator decomposition. As anapplication, we apply this to NMR Quantum Information Processing and find aprobabilistic way of doing universal quantum computation using global hardpulses. We also demonstrate efficient creation of singlet state (as a specialcase of Bell state) directly from thermal equilibrium using an optimum sequenceof pulses.

Quantum Simulation of Dzyaloshinsky-Moriya Interaction

  Quantum simulation of a Hamiltonian H requires unitary operator decomposition(UOD) of its evolution operator, ($U=exp(-i H t)$) in terms of experimentallypreferable unitaries. Here, using Genetic Algorithm optimization, wenumerically evaluate the most generic UOD for the Hamiltonian, DM interactionin the presence of Heisenberg XY interaction, $H_{DH}$. Using thesedecompositions, we studied the entanglement dynamics of Bell state in theHamiltonian $H_{DH}$ and verified the entanglement preservation procedure byHou et al. [Annals of Physics 327, 292 (2012)].

Empirical Probabilities in Monadic Deductive Databases

  We address the problem of supporting empirical probabilities in monadic logicdatabases. Though the semantics of multivalued logic programs has been studiedextensively, the treatment of probabilities as results of statistical findingshas not been studied in logic programming/deductive databases. We develop amodel-theoretic characterization of logic databases that facilitates such atreatment. We present an algorithm for checking consistency of such databasesand prove its total correctness. We develop a sound and complete queryprocessing procedure for handling queries to such databases.

Non-monotonic Negation in Probabilistic Deductive Databases

  In this paper we study the uses and the semantics of non-monotonic negationin probabilistic deductive data bases. Based on the stable semantics forclassical logic programming, we introduce the notion of stable formula,functions. We show that stable formula, functions are minimal fixpoints ofoperators associated with probabilistic deductive databases with negation.Furthermore, since a. probabilistic deductive database may not necessarily havea stable formula function, we provide a stable class semantics for suchdatabases. Finally, we demonstrate that the proposed semantics can handledefault reasoning naturally in the context of probabilistic deduction.

ChoiceGAPs: Competitive Diffusion as a Massive Multi-Player Game in  Social Networks

  We consider the problem of modeling competitive diffusion in real worldsocial networks via the notion of ChoiceGAPs which combine choice logicprograms due to Sacca` and Zaniolo and Generalized Annotated Programs due toKifer and Subrahmanian. We assume that each vertex in a social network is aplayer in a multi-player game (with a huge number of players) - the choice partof the ChoiceGAPs describe utilities of players for acting in various waysbased on utilities of their neighbors in those and other situations. We definemulti-player Nash equilibrium for such programs - but because they require someconditions that are hard to satisfy in the real world, we introduce a newmodel-theoretic concept of strong equilibrium. We show that stable equilibriacan capture all Nash equilibria. We prove a host of complexity (intractability)results for checking existence of strong equilibria (as well as relatedcounting complexity results), together with algorithms to find them. We thenidentify a class of ChoiceGAPs for which stable equilibria can be polynomiallycomputed. We develop algorithms for computing these equilibria under variousrestrictions. We come up with the important concept of an estimation querywhich can compute quantities w.r.t. a given strong equilibrium, and approximateranges of values (answers) across the space of strong equilibria. Even thoughwe show that computing range answers to estimation queries exactly isintractable, we are able to identify classes of estimation queries that can beanswered in polynomial time. We report on experiments we conducted with areal-world FaceBook data set surrounding the 2013 Italian election showing thatour algorithms have good predictive accuracy with an Area Under a ROC Curvethat, on average, is over 0.76.

Hybrid Probabilistic Programs: Algorithms and Complexity

  Hybrid Probabilistic Programs (HPPs) are logic programs that allow theprogrammer to explicitly encode his knowledge of the dependencies betweenevents being described in the program. In this paper, we classify HPPs intothree classes called HPP_1,HPP_2 and HPP_r,r>= 3. For these classes, we providethree types of results for HPPs. First, we develop algorithms to compute theset of all ground consequences of an HPP. Then we provide algorithms andcomplexity results for the problems of entailment ("Given an HPP P and a queryQ as input, is Q a logical consequence of P?") and consistency ("Given an HPP Pas input, is P consistent?"). Our results provide a fine characterization ofwhen polynomial algorithms exist for the above problems, and when theseproblems become intractable.

Ensemble-Based Algorithms to Detect Disjoint and Overlapping Communities  in Networks

  Given a set ${\cal AL}$ of community detection algorithms and a graph $G$ asinputs, we propose two ensemble methods $\mathtt{EnDisCO}$ and $\mathtt{MeDOC}$that (respectively) identify disjoint and overlapping communities in $G$.$\mathtt{EnDisCO}$ transforms a graph into a latent feature space by leveragingmultiple base solutions and discovers disjoint community structure.$\mathtt{MeDOC}$ groups similar base communities into a meta-community anddetects both disjoint and overlapping community structures. Experiments areconducted at different scales on both synthetically generated networks as wellas on several real-world networks for which the underlying ground-truthcommunity structure is available. Our extensive experiments show that bothalgorithms outperform state-of-the-art non-ensemble algorithms by a significantmargin. Moreover, we compare $\mathtt{EnDisCO}$ and $\mathtt{MeDOC}$ with arecent ensemble method for disjoint community detection and show that ourapproaches achieve superior performance. To the best of our knowledge,$\mathtt{MeDOC}$ is the first ensemble approach for overlapping communitydetection.

Deception Detection in Videos

  We present a system for covert automated deception detection in real-lifecourtroom trial videos. We study the importance of different modalities likevision, audio and text for this task. On the vision side, our system usesclassifiers trained on low level video features which predict humanmicro-expressions. We show that predictions of high-level micro-expressions canbe used as features for deception prediction. Surprisingly, IDT (Improved DenseTrajectory) features which have been widely used for action recognition, arealso very good at predicting deception in videos. We fuse the score ofclassifiers trained on IDT features and high-level micro-expressions to improveperformance. MFCC (Mel-frequency Cepstral Coefficients) features from the audiodomain also provide a significant boost in performance, while information fromtranscripts is not very beneficial for our system. Using various classifiers,our automated system obtains an AUC of 0.877 (10-fold cross-validation) whenevaluated on subjects which were not part of the training set. Even thoughstate-of-the-art methods use human annotations of micro-expressions fordeception detection, our fully automated approach outperforms them by 5%. Whencombined with human annotations of micro-expressions, our AUC improves to0.922. We also present results of a user-study to analyze how well do averagehumans perform on this task, what modalities they use for deception detectionand how they perform if only one modality is accessible. Our project page canbe found at \url{https://doubaibai.github.io/DARE/}.

Geospatial Optimization Problems

  There are numerous applications which require the ability to take certainactions (e.g. distribute money, medicines, people etc.) over a geographicregion. A disaster relief organization must allocate people and supplies toparts of a region after a disaster. A public health organization must allocatelimited vaccine to people across a region. In both cases, the organization istrying to optimize something (e.g. minimize expected number of people with adisease). We introduce "geospatial optimization problems" (GOPs) where anorganization has limited resources and budget to take actions in a geographicarea. The actions result in one or more properties changing for one or morelocations. There are also certain constraints on the combinations of actionsthat can be taken. We study two types of GOPs - goal-based andbenefit-maximizing (GBGOP and BMGOP respectively). A GBGOP ensures that certainproperties must be true at specified locations after the actions are takenwhile a BMGOP optimizes a linear benefit function. We show both problems to beNP-hard (with membership in NP for the associated decision problems).Additionally, we prove limits on approximation for both problems. We presentinteger programs for both GOPs that provide exact solutions. We also correctlyreduce the number of variables in for the GBGOP integer constraints. For BMGOP,we present the BMGOP-Compute algorithm that runs in PTIME and provides areasonable approximation guarantee in most cases.

VEWS: A Wikipedia Vandal Early Warning System

  We study the problem of detecting vandals on Wikipedia before any human orknown vandalism detection system reports flagging potential vandals so thatsuch users can be presented early to Wikipedia administrators. We leveragemultiple classical ML approaches, but develop 3 novel sets of features. OurWikipedia Vandal Behavior (WVB) approach uses a novel set of user editingpatterns as features to classify some users as vandals. Our WikipediaTransition Probability Matrix (WTPM) approach uses a set of features derivedfrom a transition probability matrix and then reduces it via a neural netauto-encoder to classify some users as vandals. The VEWS approach merges theprevious two approaches. Without using any information (e.g. reverts) providedby other users, these algorithms each have over 85% classification accuracy.Moreover, when temporal recency is considered, accuracy goes to almost 90%. Wecarry out detailed experiments on a new data set we have created consisting ofabout 33K Wikipedia users (including both a black list and a white list ofeditors) and containing 770K edits. We describe specific behaviors thatdistinguish between vandals and non-vandals. We show that VEWS beats ClueBot NGand STiki, the best known algorithms today for vandalism detection. Moreover,VEWS detects far more vandals than ClueBot NG and on average, detects them 2.39edits before ClueBot NG when both detect the vandal. However, we show that thecombination of VEWS and ClueBot NG can give a fully automated vandal earlywarning system with even higher accuracy.

The DARPA Twitter Bot Challenge

  A number of organizations ranging from terrorist groups such as ISIS topoliticians and nation states reportedly conduct explicit campaigns toinfluence opinion on social media, posing a risk to democratic processes. Thereis thus a growing need to identify and eliminate "influence bots" - realistic,automated identities that illicitly shape discussion on sites like Twitter andFacebook - before they get too influential. Spurred by such events, DARPA helda 4-week competition in February/March 2015 in which multiple teams supportedby the DARPA Social Media in Strategic Communications program competed toidentify a set of previously identified "influence bots" serving as groundtruth on a specific topic within Twitter. Past work regarding influence botsoften has difficulty supporting claims about accuracy, since there is limitedground truth (though some exceptions do exist [3,7]). However, with theexception of [3], no past work has looked specifically at identifying influencebots on a specific topic. This paper describes the DARPA Challenge anddescribes the methods used by the three top-ranked teams.

An Army of Me: Sockpuppets in Online Discussion Communities

  In online discussion communities, users can interact and share informationand opinions on a wide variety of topics. However, some users may createmultiple identities, or sockpuppets, and engage in undesired behavior bydeceiving others or manipulating discussions. In this work, we studysockpuppetry across nine discussion communities, and show that sockpuppetsdiffer from ordinary users in terms of their posting behavior, linguistictraits, as well as social network structure. Sockpuppets tend to start fewerdiscussions, write shorter posts, use more personal pronouns such as "I", andhave more clustered ego-networks. Further, pairs of sockpuppets controlled bythe same individual are more likely to interact on the same discussion at thesame time than pairs of ordinary users. Our analysis suggests a taxonomy ofdeceptive behavior in discussion communities. Pairs of sockpuppets can vary intheir deceptiveness, i.e., whether they pretend to be different users, or theirsupportiveness, i.e., if they support arguments of other sockpuppets controlledby the same user. We apply these findings to a series of prediction tasks,notably, to identify whether a pair of accounts belongs to the same underlyinguser or not. Altogether, this work presents a data-driven view of deception inonline discussion communities and paves the way towards the automatic detectionof sockpuppets.

