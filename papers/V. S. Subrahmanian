Non-Destructive Discrimination of arbitrary set of orthogonal quantum
  states by NMR using Quantum Phase Estimation

  An algorithm based on quantum phase estimation, which discriminates quantum
states nondestructively within a set of arbitrary orthogonal states, is
described and experimentally verified by a NMR quantum information processor.
The procedure is scalable and can be applied to any set of orthogonal states.
Scalability is demonstrated through Matlab simulation.


Singlet state creation and Universal quantum computation in NMR using
  Genetic Algorithm

  Experimental implementation of a quantum algorithm requires unitary operator
decomposition. Here we treat the unitary operator decomposition as an
optimization problem and use Genetic Algorithm, a global optimization method
inspired by nature's evolutionary process for operator decomposition. As an
application, we apply this to NMR Quantum Information Processing and find a
probabilistic way of doing universal quantum computation using global hard
pulses. We also demonstrate efficient creation of singlet state (as a special
case of Bell state) directly from thermal equilibrium using an optimum sequence
of pulses.


Quantum Simulation of Dzyaloshinsky-Moriya Interaction

  Quantum simulation of a Hamiltonian H requires unitary operator decomposition
(UOD) of its evolution operator, ($U=exp(-i H t)$) in terms of experimentally
preferable unitaries. Here, using Genetic Algorithm optimization, we
numerically evaluate the most generic UOD for the Hamiltonian, DM interaction
in the presence of Heisenberg XY interaction, $H_{DH}$. Using these
decompositions, we studied the entanglement dynamics of Bell state in the
Hamiltonian $H_{DH}$ and verified the entanglement preservation procedure by
Hou et al. [Annals of Physics 327, 292 (2012)].


Empirical Probabilities in Monadic Deductive Databases

  We address the problem of supporting empirical probabilities in monadic logic
databases. Though the semantics of multivalued logic programs has been studied
extensively, the treatment of probabilities as results of statistical findings
has not been studied in logic programming/deductive databases. We develop a
model-theoretic characterization of logic databases that facilitates such a
treatment. We present an algorithm for checking consistency of such databases
and prove its total correctness. We develop a sound and complete query
processing procedure for handling queries to such databases.


Non-monotonic Negation in Probabilistic Deductive Databases

  In this paper we study the uses and the semantics of non-monotonic negation
in probabilistic deductive data bases. Based on the stable semantics for
classical logic programming, we introduce the notion of stable formula,
functions. We show that stable formula, functions are minimal fixpoints of
operators associated with probabilistic deductive databases with negation.
Furthermore, since a. probabilistic deductive database may not necessarily have
a stable formula function, we provide a stable class semantics for such
databases. Finally, we demonstrate that the proposed semantics can handle
default reasoning naturally in the context of probabilistic deduction.


ChoiceGAPs: Competitive Diffusion as a Massive Multi-Player Game in
  Social Networks

  We consider the problem of modeling competitive diffusion in real world
social networks via the notion of ChoiceGAPs which combine choice logic
programs due to Sacca` and Zaniolo and Generalized Annotated Programs due to
Kifer and Subrahmanian. We assume that each vertex in a social network is a
player in a multi-player game (with a huge number of players) - the choice part
of the ChoiceGAPs describe utilities of players for acting in various ways
based on utilities of their neighbors in those and other situations. We define
multi-player Nash equilibrium for such programs - but because they require some
conditions that are hard to satisfy in the real world, we introduce a new
model-theoretic concept of strong equilibrium. We show that stable equilibria
can capture all Nash equilibria. We prove a host of complexity (intractability)
results for checking existence of strong equilibria (as well as related
counting complexity results), together with algorithms to find them. We then
identify a class of ChoiceGAPs for which stable equilibria can be polynomially
computed. We develop algorithms for computing these equilibria under various
restrictions. We come up with the important concept of an estimation query
which can compute quantities w.r.t. a given strong equilibrium, and approximate
ranges of values (answers) across the space of strong equilibria. Even though
we show that computing range answers to estimation queries exactly is
intractable, we are able to identify classes of estimation queries that can be
answered in polynomial time. We report on experiments we conducted with a
real-world FaceBook data set surrounding the 2013 Italian election showing that
our algorithms have good predictive accuracy with an Area Under a ROC Curve
that, on average, is over 0.76.


Hybrid Probabilistic Programs: Algorithms and Complexity

  Hybrid Probabilistic Programs (HPPs) are logic programs that allow the
programmer to explicitly encode his knowledge of the dependencies between
events being described in the program. In this paper, we classify HPPs into
three classes called HPP_1,HPP_2 and HPP_r,r>= 3. For these classes, we provide
three types of results for HPPs. First, we develop algorithms to compute the
set of all ground consequences of an HPP. Then we provide algorithms and
complexity results for the problems of entailment ("Given an HPP P and a query
Q as input, is Q a logical consequence of P?") and consistency ("Given an HPP P
as input, is P consistent?"). Our results provide a fine characterization of
when polynomial algorithms exist for the above problems, and when these
problems become intractable.


Ensemble-Based Algorithms to Detect Disjoint and Overlapping Communities
  in Networks

  Given a set ${\cal AL}$ of community detection algorithms and a graph $G$ as
inputs, we propose two ensemble methods $\mathtt{EnDisCO}$ and $\mathtt{MeDOC}$
that (respectively) identify disjoint and overlapping communities in $G$.
$\mathtt{EnDisCO}$ transforms a graph into a latent feature space by leveraging
multiple base solutions and discovers disjoint community structure.
$\mathtt{MeDOC}$ groups similar base communities into a meta-community and
detects both disjoint and overlapping community structures. Experiments are
conducted at different scales on both synthetically generated networks as well
as on several real-world networks for which the underlying ground-truth
community structure is available. Our extensive experiments show that both
algorithms outperform state-of-the-art non-ensemble algorithms by a significant
margin. Moreover, we compare $\mathtt{EnDisCO}$ and $\mathtt{MeDOC}$ with a
recent ensemble method for disjoint community detection and show that our
approaches achieve superior performance. To the best of our knowledge,
$\mathtt{MeDOC}$ is the first ensemble approach for overlapping community
detection.


Deception Detection in Videos

  We present a system for covert automated deception detection in real-life
courtroom trial videos. We study the importance of different modalities like
vision, audio and text for this task. On the vision side, our system uses
classifiers trained on low level video features which predict human
micro-expressions. We show that predictions of high-level micro-expressions can
be used as features for deception prediction. Surprisingly, IDT (Improved Dense
Trajectory) features which have been widely used for action recognition, are
also very good at predicting deception in videos. We fuse the score of
classifiers trained on IDT features and high-level micro-expressions to improve
performance. MFCC (Mel-frequency Cepstral Coefficients) features from the audio
domain also provide a significant boost in performance, while information from
transcripts is not very beneficial for our system. Using various classifiers,
our automated system obtains an AUC of 0.877 (10-fold cross-validation) when
evaluated on subjects which were not part of the training set. Even though
state-of-the-art methods use human annotations of micro-expressions for
deception detection, our fully automated approach outperforms them by 5%. When
combined with human annotations of micro-expressions, our AUC improves to
0.922. We also present results of a user-study to analyze how well do average
humans perform on this task, what modalities they use for deception detection
and how they perform if only one modality is accessible. Our project page can
be found at \url{https://doubaibai.github.io/DARE/}.


Geospatial Optimization Problems

  There are numerous applications which require the ability to take certain
actions (e.g. distribute money, medicines, people etc.) over a geographic
region. A disaster relief organization must allocate people and supplies to
parts of a region after a disaster. A public health organization must allocate
limited vaccine to people across a region. In both cases, the organization is
trying to optimize something (e.g. minimize expected number of people with a
disease). We introduce "geospatial optimization problems" (GOPs) where an
organization has limited resources and budget to take actions in a geographic
area. The actions result in one or more properties changing for one or more
locations. There are also certain constraints on the combinations of actions
that can be taken. We study two types of GOPs - goal-based and
benefit-maximizing (GBGOP and BMGOP respectively). A GBGOP ensures that certain
properties must be true at specified locations after the actions are taken
while a BMGOP optimizes a linear benefit function. We show both problems to be
NP-hard (with membership in NP for the associated decision problems).
Additionally, we prove limits on approximation for both problems. We present
integer programs for both GOPs that provide exact solutions. We also correctly
reduce the number of variables in for the GBGOP integer constraints. For BMGOP,
we present the BMGOP-Compute algorithm that runs in PTIME and provides a
reasonable approximation guarantee in most cases.


The DARPA Twitter Bot Challenge

  A number of organizations ranging from terrorist groups such as ISIS to
politicians and nation states reportedly conduct explicit campaigns to
influence opinion on social media, posing a risk to democratic processes. There
is thus a growing need to identify and eliminate "influence bots" - realistic,
automated identities that illicitly shape discussion on sites like Twitter and
Facebook - before they get too influential. Spurred by such events, DARPA held
a 4-week competition in February/March 2015 in which multiple teams supported
by the DARPA Social Media in Strategic Communications program competed to
identify a set of previously identified "influence bots" serving as ground
truth on a specific topic within Twitter. Past work regarding influence bots
often has difficulty supporting claims about accuracy, since there is limited
ground truth (though some exceptions do exist [3,7]). However, with the
exception of [3], no past work has looked specifically at identifying influence
bots on a specific topic. This paper describes the DARPA Challenge and
describes the methods used by the three top-ranked teams.


VEWS: A Wikipedia Vandal Early Warning System

  We study the problem of detecting vandals on Wikipedia before any human or
known vandalism detection system reports flagging potential vandals so that
such users can be presented early to Wikipedia administrators. We leverage
multiple classical ML approaches, but develop 3 novel sets of features. Our
Wikipedia Vandal Behavior (WVB) approach uses a novel set of user editing
patterns as features to classify some users as vandals. Our Wikipedia
Transition Probability Matrix (WTPM) approach uses a set of features derived
from a transition probability matrix and then reduces it via a neural net
auto-encoder to classify some users as vandals. The VEWS approach merges the
previous two approaches. Without using any information (e.g. reverts) provided
by other users, these algorithms each have over 85% classification accuracy.
Moreover, when temporal recency is considered, accuracy goes to almost 90%. We
carry out detailed experiments on a new data set we have created consisting of
about 33K Wikipedia users (including both a black list and a white list of
editors) and containing 770K edits. We describe specific behaviors that
distinguish between vandals and non-vandals. We show that VEWS beats ClueBot NG
and STiki, the best known algorithms today for vandalism detection. Moreover,
VEWS detects far more vandals than ClueBot NG and on average, detects them 2.39
edits before ClueBot NG when both detect the vandal. However, we show that the
combination of VEWS and ClueBot NG can give a fully automated vandal early
warning system with even higher accuracy.


An Army of Me: Sockpuppets in Online Discussion Communities

  In online discussion communities, users can interact and share information
and opinions on a wide variety of topics. However, some users may create
multiple identities, or sockpuppets, and engage in undesired behavior by
deceiving others or manipulating discussions. In this work, we study
sockpuppetry across nine discussion communities, and show that sockpuppets
differ from ordinary users in terms of their posting behavior, linguistic
traits, as well as social network structure. Sockpuppets tend to start fewer
discussions, write shorter posts, use more personal pronouns such as "I", and
have more clustered ego-networks. Further, pairs of sockpuppets controlled by
the same individual are more likely to interact on the same discussion at the
same time than pairs of ordinary users. Our analysis suggests a taxonomy of
deceptive behavior in discussion communities. Pairs of sockpuppets can vary in
their deceptiveness, i.e., whether they pretend to be different users, or their
supportiveness, i.e., if they support arguments of other sockpuppets controlled
by the same user. We apply these findings to a series of prediction tasks,
notably, to identify whether a pair of accounts belongs to the same underlying
user or not. Altogether, this work presents a data-driven view of deception in
online discussion communities and paves the way towards the automatic detection
of sockpuppets.


