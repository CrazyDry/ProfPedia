Text Coherence Analysis Based on Deep Neural Network

  In this paper, we propose a novel deep coherence model (DCM) using aconvolutional neural network architecture to capture the text coherence. Thetext coherence problem is investigated with a new perspective of learningsentence distributional representation and text coherence modelingsimultaneously. In particular, the model captures the interactions betweensentences by computing the similarities of their distributionalrepresentations. Further, it can be easily trained in an end-to-end fashion.The proposed model is evaluated on a standard Sentence Ordering task. Theexperimental results demonstrate its effectiveness and promise in coherenceassessment showing a significant improvement over the state-of-the-art by awide margin.

Non-Abelian gauge field optics

  The concept of gauge field is a cornerstone of modern physics and thesynthetic gauge field has emerged as a new way to manipulate neutral particlesin many disciplines. In optics, several schemes have been proposed to realizeAbelian synthetic gauge fields. Here, we introduce a new platform for realizingsynthetic $SU(2)$ non-Abelian gauge fields acting on two-dimensional opticalwaves in a wide class of anisotropic materials and discover new phenomena. Weshow that a virtual non-Abelian Lorentz force can arise from the materialanisotropy, which can induce wave packets to travel along wavy "Zitterbewegung"trajectories even in homogeneous media. We further propose an interferometryscheme to realize the non-Abelian Aharonov--Bohm effect of light, whichhighlights the essential non-Abelian nature of the system. We also show thatthe Wilson loop of an arbitrary closed optical path can be extracted from aseries of gauge fixed points in the interference fringes.

Pyramid Person Matching Network for Person Re-identification

  In this work, we present a deep convolutional pyramid person matching network(PPMN) with specially designed Pyramid Matching Module to address the problemof person re-identification. The architecture takes a pair of RGB images asinput, and outputs a similiarity value indicating whether the two input imagesrepresent the same person or not. Based on deep convolutional neural networks,our approach first learns the discriminative semantic representation with thesemantic-component-aware features for persons and then employs the PyramidMatching Module to match the common semantic-components of persons, which isrobust to the variation of spatial scales and misalignment of locations posedby viewpoint changes. The above two processes are jointly optimized via aunified end-to-end deep learning scheme. Extensive experiments on severalbenchmark datasets demonstrate the effectiveness of our approach against thestate-of-the-art approaches, especially on the rank-1 recognition rate.

Multi-Channel Pyramid Person Matching Network for Person  Re-Identification

  In this work, we present a Multi-Channel deep convolutional Pyramid PersonMatching Network (MC-PPMN) based on the combination of the semantic-componentsand the color-texture distributions to address the problem of personre-identification. In particular, we learn separate deep representations forsemantic-components and color-texture distributions from two person images andthen employ pyramid person matching network (PPMN) to obtain correspondencerepresentations. These correspondence representations are fused to perform there-identification task. Further, the proposed framework is optimized via aunified end-to-end deep learning scheme. Extensive experiments on severalbenchmark datasets demonstrate the effectiveness of our approach against thestate-of-the-art literature, especially on the rank-1 recognition rate.

Context-Aware Hypergraph Construction for Robust Spectral Clustering

  Spectral clustering is a powerful tool for unsupervised data analysis. Inthis paper, we propose a context-aware hypergraph similarity measure (CAHSM),which leads to robust spectral clustering in the case of noisy data. Weconstruct three types of hypergraph---the pairwise hypergraph, thek-nearest-neighbor (kNN) hypergraph, and the high-order over-clusteringhypergraph. The pairwise hypergraph captures the pairwise similarity of datapoints; the kNN hypergraph captures the neighborhood of each point; and theclustering hypergraph encodes high-order contexts within the dataset. Bycombining the affinity information from these three hypergraphs, the CAHSMalgorithm is able to explore the intrinsic topological information of thedataset. Therefore, data clustering using CAHSM tends to be more robust.Considering the intra-cluster compactness and the inter-cluster separability ofvertices, we further design a discriminative hypergraph partitioning criterion(DHPC). Using both CAHSM and DHPC, a robust spectral clustering algorithm isdeveloped. Theoretical analysis and experimental evaluation demonstrate theeffectiveness and robustness of the proposed algorithm.

Online Metric-Weighted Linear Representations for Robust Visual Tracking

  In this paper, we propose a visual tracker based on a metric-weighted linearrepresentation of appearance. In order to capture the interdependence ofdifferent feature dimensions, we develop two online distance metric learningmethods using proximity comparison information and structured output learning.The learned metric is then incorporated into a linear representation ofappearance.  We show that online distance metric learning significantly improves therobustness of the tracker, especially on those sequences exhibiting drasticappearance changes. In order to bound growth in the number of training samples,we design a time-weighted reservoir sampling method.  Moreover, we enable our tracker to automatically perform objectidentification during the process of object tracking, by introducing acollection of static template samples belonging to several object classes ofinterest. Object identification results for an entire video sequence areachieved by systematically combining the tracking information and visualrecognition at each frame. Experimental results on challenging video sequencesdemonstrate the effectiveness of the method for both inter-frame tracking andobject identification.

Multimodal Skip-gram Using Convolutional Pseudowords

  This work studies the representational mapping across multimodal data suchthat given a piece of the raw data in one modality the corresponding semanticdescription in terms of the raw data in another modality is immediatelyobtained. Such a representational mapping can be found in a wide spectrum ofreal-world applications including image/video retrieval, object recognition,action/behavior recognition, and event understanding and prediction. To thatend, we introduce a simplified training objective for learning multimodalembeddings using the skip-gram architecture by introducing convolutional"pseudowords:" embeddings composed of the additive combination of distributedword representations and image features from convolutional neural networksprojected into the multimodal space. We present extensive results of therepresentational properties of these embeddings on various word similaritybenchmarks to show the promise of this approach.

Manifold Regularized Discriminative Neural Networks

  Unregularized deep neural networks (DNNs) can be easily overfit with alimited sample size. We argue that this is mostly due to the disriminativenature of DNNs which directly model the conditional probability (or score) oflabels given the input. The ignorance of input distribution makes DNNsdifficult to generalize to unseen data. Recent advances in regularizationtechniques, such as pretraining and dropout, indicate that modeling input datadistribution (either explicitly or implicitly) greatly improves thegeneralization ability of a DNN. In this work, we explore the manifoldhypothesis which assumes that instances within the same class lie in a smoothmanifold. We accordingly propose two simple regularizers to a standarddiscriminative DNN. The first one, named Label-Aware Manifold Regularization,assumes the availability of labels and penalizes large norms of the lossfunction w.r.t. data points. The second one, named Label-Independent ManifoldRegularization, does not use label information and instead penalizes theFrobenius norm of the Jacobian matrix of prediction scores w.r.t. data points,which makes semi-supervised learning possible. We perform extensive controlexperiments on fully supervised and semi-supervised tasks using the MNIST,CIFAR10 and SVHN datasets and achieve excellent results.

Semisupervised Autoencoder for Sentiment Analysis

  In this paper, we investigate the usage of autoencoders in modeling textualdata. Traditional autoencoders suffer from at least two aspects: scalabilitywith the high dimensionality of vocabulary size and dealing withtask-irrelevant words. We address this problem by introducing supervision viathe loss function of autoencoders. In particular, we first train a linearclassifier on the labeled data, then define a loss for the autoencoder with theweights learned from the linear classifier. To reduce the bias brought by onesingle classifier, we define a posterior probability distribution on theweights of the classifier, and derive the marginalized loss of the autoencoderwith Laplace approximation. We show that our choice of loss function can berationalized from the perspective of Bregman Divergence, which justifies thesoundness of our model. We evaluate the effectiveness of our model on sixsentiment analysis datasets, and show that our model significantly outperformsall the competing methods with respect to classification accuracy. We also showthat our model is able to take advantage of unlabeled dataset and get improvedperformance. We further show that our model successfully learns highlydiscriminative feature maps, which explains its superior performance.

Dropout Training of Matrix Factorization and Autoencoder for Link  Prediction in Sparse Graphs

  Matrix factorization (MF) and Autoencoder (AE) are among the most successfulapproaches of unsupervised learning. While MF based models have beenextensively exploited in the graph modeling and link prediction literature, theAE family has not gained much attention. In this paper we investigate both MFand AE's application to the link prediction problem in sparse graphs. We showthe connection between AE and MF from the perspective of multiview learning,and further propose MF+AE: a model training MF and AE jointly with sharedparameters. We apply dropout to training both the MF and AE parts, and showthat it can significantly prevent overfitting by acting as an adaptiveregularization. We conduct experiments on six real world sparse graph datasets,and show that MF+AE consistently outperforms the competing methods, especiallyon datasets that demonstrate strong non-cohesive structures.

Deep Structured Energy Based Models for Anomaly Detection

  In this paper, we attack the anomaly detection problem by directly modelingthe data distribution with deep architectures. We propose deep structuredenergy based models (DSEBMs), where the energy function is the output of adeterministic deep neural network with structure. We develop novel modelarchitectures to integrate EBMs with different types of data such as staticdata, sequential data, and spatial data, and apply appropriate modelarchitectures to adapt to the data structure. Our training algorithm is builtupon the recent development of score matching \cite{sm}, which connects an EBMwith a regularized autoencoder, eliminating the need for complicated samplingmethod. Statistically sound decision criterion can be derived for anomalydetection purpose from the perspective of the energy landscape of the datadistribution. We investigate two decision criteria for performing anomalydetection: the energy score and the reconstruction error. Extensive empiricalstudies on benchmark tasks demonstrate that our proposed model consistentlymatches or outperforms all the competing methods.

Zero-Shot Learning with Multi-Battery Factor Analysis

  Zero-shot learning (ZSL) extends the conventional image classificationtechnique to a more challenging situation where the test image categories arenot seen in the training samples. Most studies on ZSL utilize side informationsuch as attributes or word vectors to bridge the relations between the seenclasses and the unseen classes. However, existing approaches on ZSL typicallyexploit a shared space for each type of side information independently, whichcannot make full use of the complementary knowledge of different types of sideinformation. To this end, this paper presents an MBFA-ZSL approach to embeddifferent types of side information as well as the visual feature into oneshared space. Specifically, we first develop an algorithm named Multi-BatteryFactor Analysis (MBFA) to build a unified semantic space, and then employmultiple types of side information in it to achieve the ZSL. The close-formsolution makes MBFA-ZSL simple to implement and efficient to run on largedatasets. Extensive experiments on the popular AwA, CUB, and SUN datasets showits significant superiority over the state-of-the-art approaches.

Doubly Convolutional Neural Networks

  Building large models with parameter sharing accounts for most of the successof deep convolutional neural networks (CNNs). In this paper, we propose doublyconvolutional neural networks (DCNNs), which significantly improve theperformance of CNNs by further exploring this idea. In stead of allocating aset of convolutional filters that are independently learned, a DCNN maintainsgroups of filters where filters within each group are translated versions ofeach other. Practically, a DCNN can be easily implemented by a two-stepconvolution procedure, which is supported by most modern deep learninglibraries. We perform extensive experiments on three image classificationbenchmarks: CIFAR-10, CIFAR-100 and ImageNet, and show that DCNNs consistentlyoutperform other competing architectures. We have also verified that replacinga convolutional layer with a doubly convolutional layer at any depth of a CNNcan improve its performance. Moreover, various design choices of DCNNs aredemonstrated, which shows that DCNN can serve the dual purpose of building moreaccurate models and/or reducing the memory footprint without sacrificing theaccuracy.

Generative Adversarial Networks as Variational Training of Energy Based  Models

  In this paper, we study deep generative models for effective unsupervisedlearning. We propose VGAN, which works by minimizing a variational lower boundof the negative log likelihood (NLL) of an energy based model (EBM), where themodel density $p(\mathbf{x})$ is approximated by a variational distribution$q(\mathbf{x})$ that is easy to sample from. The training of VGAN takes a twostep procedure: given $p(\mathbf{x})$, $q(\mathbf{x})$ is updated to maximizethe lower bound; $p(\mathbf{x})$ is then updated one step with samples drawnfrom $q(\mathbf{x})$ to decrease the lower bound. VGAN is inspired by thegenerative adversarial networks (GANs), where $p(\mathbf{x})$ corresponds tothe discriminator and $q(\mathbf{x})$ corresponds to the generator, but withseveral notable differences. We hence name our model variational GANs (VGANs).VGAN provides a practical solution to training deep EBMs in high dimensionalspace, by eliminating the need of MCMC sampling. From this view, we are alsoable to identify causes to the difficulty of training GANs and propose viablesolutions. \footnote{Experimental code is available athttps://github.com/Shuangfei/vgan}

Structural Correspondence Learning for Cross-lingual Sentiment  Classification with One-to-many Mappings

  Structural correspondence learning (SCL) is an effective method forcross-lingual sentiment classification. This approach uses unlabeled documentsalong with a word translation oracle to automatically induce task specific,cross-lingual correspondences. It transfers knowledge through identifyingimportant features, i.e., pivot features. For simplicity, however, it assumesthat the word translation oracle maps each pivot feature in source language toexactly only one word in target language. This one-to-one mapping between wordsin different languages is too strict. Also the context is not considered atall. In this paper, we propose a cross-lingual SCL based on distributedrepresentation of words; it can learn meaningful one-to-many mappings for pivotwords using large amounts of monolingual data and a small dictionary. Weconduct experiments on NLP\&CC 2013 cross-lingual sentiment analysis dataset,employing English as source language, and Chinese as target language. Ourmethod does not rely on the parallel corpora and the experimental results showthat our approach is more competitive than the state-of-the-art methods incross-lingual sentiment classification.

Boosted Zero-Shot Learning with Semantic Correlation Regularization

  We study zero-shot learning (ZSL) as a transfer learning problem, and focuson the two key aspects of ZSL, model effectiveness and model adaptation. Foreffective modeling, we adopt the boosting strategy to learn a zero-shotclassifier from weak models to a strong model. For adaptable knowledgetransfer, we devise a Semantic Correlation Regularization (SCR) approach toregularize the boosted model to be consistent with the inter-class semanticcorrelations. With SCR embedded in the boosting objective, and with aself-controlled sample selection for learning robustness, we propose a unifiedframework, Boosted Zero-shot classification with Semantic CorrelationRegularization (BZ-SCR). By balancing the SCR-regularized boosted modelselection and the self-controlled sample selection, BZ-SCR is capable ofcapturing both discriminative and adaptable feature-to-class semanticalignments, while ensuring the reliability and adaptability of the learnedsamples. The experiments on two ZSL datasets show the superiority of BZ-SCRover the state-of-the-arts.

A Deep Learning Approach for Expert Identification in Question Answering  Communities

  In this paper, we describe an effective convolutional neural networkframework for identifying the expert in question answering community. Thisapproach uses the convolutional neural network and combines user featurerepresentations with question feature representations to compute scores thatthe user who gets the highest score is the expert on this question. Unlikeprior work, this method does not measure expert based on measure answer contentquality to identify the expert but only require question sentence and userembedding feature to identify the expert. Remarkably, Our model can be appliedto different languages and different domains. The proposed framework is trainedon two datasets, The first dataset is Stack Overflow and the second one isZhihu. The Top-1 accuracy results of our experiments show that our frameworkoutperforms the best baseline framework for expert identification.

GNAS: A Greedy Neural Architecture Search Method for Multi-Attribute  Learning

  A key problem in deep multi-attribute learning is to effectively discover theinter-attribute correlation structures. Typically, the conventional deepmulti-attribute learning approaches follow the pipeline of manually designingthe network architectures based on task-specific expertise prior knowledge andcareful network tunings, leading to the inflexibility for various complicatedscenarios in practice. Motivated by addressing this problem, we propose anefficient greedy neural architecture search approach (GNAS) to automaticallydiscover the optimal tree-like deep architecture for multi-attribute learning.In a greedy manner, GNAS divides the optimization of global architecture intothe optimizations of individual connections step by step. By iterativelyupdating the local architectures, the global tree-like architecture getsconverged where the bottom layers are shared across relevant attributes and thebranches in top layers more encode attribute-specific features. Experiments onthree benchmark multi-attribute datasets show the effectiveness and compactnessof neural architectures derived by GNAS, and also demonstrate the efficiency ofGNAS in searching neural architectures.

Stacked Semantic-Guided Attention Model for Fine-Grained Zero-Shot  Learning

  Zero-Shot Learning (ZSL) is achieved via aligning the semantic relationshipsbetween the global image feature vector and the corresponding class semanticdescriptions. However, using the global features to represent fine-grainedimages may lead to sub-optimal results since they neglect the discriminativedifferences of local regions. Besides, different regions contain distinctdiscriminative information. The important regions should contribute more to theprediction. To this end, we propose a novel stacked semantics-guided attention(S2GA) model to obtain semantic relevant features by using individual classsemantic features to progressively guide the visual features to generate anattention map for weighting the importance of different local regions. Feedingboth the integrated visual features and the class semantic features into amulti-class classification architecture, the proposed framework can be trainedend-to-end. Extensive experimental results on CUB and NABird datasets show thatthe proposed approach has a consistent improvement on both fine-grainedzero-shot classification and retrieval tasks.

Stacked Pooling: Improving Crowd Counting by Boosting Scale Invariance

  In this work, we explore the cross-scale similarity in crowd countingscenario, in which the regions of different scales often exhibit high visualsimilarity. This feature is universal both within an image and across differentimages, indicating the importance of scale invariance of a crowd countingmodel. Motivated by this, in this paper we propose simple but effectivevariants of pooling module, i.e., multi-kernel pooling and stacked pooling, toboost the scale invariance of convolutional neural networks (CNNs), benefitingmuch the crowd density estimation and counting. Specifically, the multi-kernelpooling comprises of pooling kernels with multiple receptive fields to capturethe responses at multi-scale local ranges. The stacked pooling is an equivalentform of multi-kernel pooling, while, it reduces considerable computing cost.Our proposed pooling modules do not introduce extra parameters into model andcan easily take place of the vanilla pooling layer in implementation. Inempirical study on two benchmark crowd counting datasets, the stacked poolingbeats the vanilla pooling layer in most cases.

Bi-Adversarial Auto-Encoder for Zero-Shot Learning

  Existing generative Zero-Shot Learning (ZSL) methods only consider theunidirectional alignment from the class semantics to the visual features whileignoring the alignment from the visual features to the class semantics, whichfails to construct the visual-semantic interactions well. In this paper, wepropose to synthesize visual features based on an auto-encoder framework pairedwith bi-adversarial networks respectively for visual and semantic modalities toreinforce the visual-semantic interactions with a bi-directional alignment,which ensures the synthesized visual features to fit the real visualdistribution and to be highly related to the semantics. The encoder aims atsynthesizing real-like visual features while the decoder forces both the realand the synthesized visual features to be more related to the class semantics.To further capture the discriminative information of the synthesized visualfeatures, both the real and synthesized visual features are forced to beclassified into the correct classes via a classification network. Experimentalresults on four benchmark datasets show that the proposed approach isparticularly competitive on both the traditional ZSL and the generalized ZSLtasks.

Perceiving Physical Equation by Observing Visual Scenarios

  Inferring universal laws of the environment is an important ability of humanintelligence as well as a symbol of general AI. In this paper, we take a steptoward this goal such that we introduce a new challenging problem of inferringinvariant physical equation from visual scenarios. For instance, teaching amachine to automatically derive the gravitational acceleration formula bywatching a free-falling object. To tackle this challenge, we present a novelpipeline comprised of an Observer Engine and a Physicist Engine by respectivelyimitating the actions of an observer and a physicist in the real world.Generally, the Observer Engine watches the visual scenarios and then extractingthe physical properties of objects. The Physicist Engine analyses these dataand then summarizing the inherent laws of object dynamics. Specifically, thelearned laws are expressed by mathematical equations such that they are moreinterpretable than the results given by common probabilistic models.Experiments on synthetic videos have shown that our pipeline is able todiscover physical equations on various physical worlds with different visualappearances.

Cross-relation Cross-bag Attention for Distantly-supervised Relation  Extraction

  Distant supervision leverages knowledge bases to automatically labelinstances, thus allowing us to train relation extractor without humanannotations. However, the generated training data typically contain massivenoise, and may result in poor performances with the vanilla supervisedlearning. In this paper, we propose to conduct multi-instance learning with anovel Cross-relation Cross-bag Selective Attention (C$^2$SA), which leads tonoise-robust training for distant supervised relation extractor. Specifically,we employ the sentence-level selective attention to reduce the effect of noisyor mismatched sentences, while the correlation among relations were captured toimprove the quality of attention weights. Moreover, instead of treating allentity-pairs equally, we try to pay more attention to entity-pairs with ahigher quality. Similarly, we adopt the selective attention mechanism toachieve this goal. Experiments with two types of relation extractor demonstratethe superiority of the proposed approach over the state-of-the-art, whilefurther ablation studies verify our intuitions and demonstrate theeffectiveness of our proposed two techniques.

Text Guided Person Image Synthesis

  This paper presents a novel method to manipulate the visual appearance (poseand attribute) of a person image according to natural language descriptions.Our method can be boiled down to two stages: 1) text guided pose generation and2) visual appearance transferred image synthesis. In the first stage, ourmethod infers a reasonable target human pose based on the text. In the secondstage, our method synthesizes a realistic and appearance transferred personimage according to the text in conjunction with the target pose. Our methodextracts sufficient information from the text and establishes a mapping betweenthe image space and the language space, making generating and editing imagescorresponding to the description possible. We conduct extensive experiments toreveal the effectiveness of our method, as well as using the VQA PerceptualScore as a metric for evaluating the method. It shows for the first time thatwe can automatically edit the person image from the natural languagedescriptions.

A Survey of Appearance Models in Visual Object Tracking

  Visual object tracking is a significant computer vision task which can beapplied to many domains such as visual surveillance, human computerinteraction, and video compression. In the literature, researchers haveproposed a variety of 2D appearance models. To help readers swiftly learn therecent advances in 2D appearance models for visual object tracking, wecontribute this survey, which provides a detailed review of the existing 2Dappearance models. In particular, this survey takes a module-based architecturethat enables readers to easily grasp the key points of visual object tracking.In this survey, we first decompose the problem of appearance modeling into twodifferent processing stages: visual representation and statistical modeling.Then, different 2D appearance models are categorized and discussed with respectto their composition modules. Finally, we address several issues of interest aswell as the remaining challenges for future research on this topic. Thecontributions of this survey are four-fold. First, we review the literature ofvisual representations according to their feature-construction mechanisms(i.e., local and global). Second, the existing statistical modeling schemes fortracking-by-detection are reviewed according to their model-constructionmechanisms: generative, discriminative, and hybrid generative-discriminative.Third, each type of visual representations or statistical modeling techniquesis analyzed and discussed from a theoretical or practical viewpoint. Fourth,the existing benchmark resources (e.g., source code and video datasets) areexamined in this survey.

Deep Learning Driven Visual Path Prediction from a Single Image

  Capabilities of inference and prediction are significant components of visualsystems. In this paper, we address an important and challenging task of them:visual path prediction. Its goal is to infer the future path for a visualobject in a static scene. This task is complicated as it needs high-levelsemantic understandings of both the scenes and motion patterns underlying videosequences. In practice, cluttered situations have also raised higher demands onthe effectiveness and robustness of the considered models. Motivated by theseobservations, we propose a deep learning framework which simultaneouslyperforms deep feature learning for visual representation in conjunction withspatio-temporal context modeling. After that, we propose a unified pathplanning scheme to make accurate future path prediction based on the analyticresults of the context models. The highly effective visual representation anddeep context models ensure that our framework makes a deep semanticunderstanding of the scene and motion pattern, consequently improving theperformance of the visual path prediction task. In order to comprehensivelyevaluate the model's performance on the visual path prediction task, weconstruct two large benchmark datasets from the adaptation of video trackingdatasets. The qualitative and quantitative experimental results show that ourapproach outperforms the existing approaches and owns a better generalizationcapability.

A Survey of Multi-View Representation Learning

  Recently, multi-view representation learning has become a rapidly growingdirection in machine learning and data mining areas. This paper introduces twocategories for multi-view representation learning: multi-view representationalignment and multi-view representation fusion. Consequently, we first reviewthe representative methods and theories of multi-view representation learningbased on the perspective of alignment, such as correlation-based alignment.Representative examples are canonical correlation analysis (CCA) and itsseveral extensions. Then from the perspective of representation fusion weinvestigate the advancement of multi-view representation learning that rangesfrom generative methods including multi-modal topic learning, multi-view sparsecoding, and multi-view latent space Markov networks, to neural network-basedmethods including multi-modal autoencoders, multi-view convolutional neuralnetworks, and multi-modal recurrent neural networks. Further, we alsoinvestigate several important applications of multi-view representationlearning. Overall, this survey aims to provide an insightful overview oftheoretical foundation and state-of-the-art developments in the field ofmulti-view representation learning and to help researchers find the mostappropriate tools for particular applications.

S3Pool: Pooling with Stochastic Spatial Sampling

  Feature pooling layers (e.g., max pooling) in convolutional neural networks(CNNs) serve the dual purpose of providing increasingly abstractrepresentations as well as yielding computational savings in subsequentconvolutional layers. We view the pooling operation in CNNs as a two-stepprocedure: first, a pooling window (e.g., $2\times 2$) slides over the featuremap with stride one which leaves the spatial resolution intact, and second,downsampling is performed by selecting one pixel from each non-overlappingpooling window in an often uniform and deterministic (e.g., top-left) manner.Our starting point in this work is the observation that this regularly spaceddownsampling arising from non-overlapping windows, although intuitive from asignal processing perspective (which has the goal of signal reconstruction), isnot necessarily optimal for \emph{learning} (where the goal is to generalize).We study this aspect and propose a novel pooling strategy with stochasticspatial sampling (S3Pool), where the regular downsampling is replaced by a moregeneral stochastic version. We observe that this general stochasticity acts asa strong regularizer, and can also be seen as doing implicit data augmentationby introducing distortions in the feature maps. We further introduce amechanism to control the amount of distortion to suit different datasets andarchitectures. To demonstrate the effectiveness of the proposed approach, weperform extensive experiments on several popular image classificationbenchmarks, observing excellent improvements over baseline models. Experimentalcode is available at https://github.com/Shuangfei/s3pool.

Transductive Zero-Shot Learning with a Self-training dictionary approach

  As an important and challenging problem in computer vision, zero-shotlearning (ZSL) aims at automatically recognizing the instances from unseenobject classes without training data. To address this problem, ZSL is usuallycarried out in the following two aspects: 1) capturing the domain distributionconnections between seen classes data and unseen classes data; and 2) modelingthe semantic interactions between the image feature space and the labelembedding space. Motivated by these observations, we propose a bidirectionalmapping based semantic relationship modeling scheme that seeks for crossmodalknowledge transfer by simultaneously projecting the image features and labelembeddings into a common latent space. Namely, we have a bidirectionalconnection relationship that takes place from the image feature space to thelatent space as well as from the label embedding space to the latent space. Todeal with the domain shift problem, we further present a transductive learningapproach that formulates the class prediction problem in an iterative refiningprocess, where the object classification capacity is progressively reinforcedthrough bootstrapping-based model updating over highly reliable instances.Experimental results on three benchmark datasets (AwA, CUB and SUN) demonstratethe effectiveness of the proposed approach against the state-of-the-artapproaches.

Deep Air Learning: Interpolation, Prediction, and Feature Analysis of  Fine-grained Air Quality

  The interpolation, prediction, and feature analysis of fine-gained airquality are three important topics in the area of urban air computing. Thesolutions to these topics can provide extremely useful information to supportair pollution control, and consequently generate great societal and technicalimpacts. Most of the existing work solves the three problems separately bydifferent models. In this paper, we propose a general and effective approach tosolve the three problems in one model called the Deep Air Learning (DAL). Themain idea of DAL lies in embedding feature selection and semi-supervisedlearning in different layers of the deep learning network. The proposedapproach utilizes the information pertaining to the unlabeled spatio-temporaldata to improve the performance of the interpolation and the prediction, andperforms feature selection and association analysis to reveal the main relevantfeatures to the variation of the air quality. We evaluate our approach withextensive experiments based on real data sources obtained in Beijing, China.Experiments show that DAL is superior to the peer models from the recentliterature when solving the topics of interpolation, prediction, and featureanalysis of fine-gained air quality.

Zero-Shot Learning via Latent Space Encoding

  Zero-Shot Learning (ZSL) is typically achieved by resorting to a classsemantic embedding space to transfer the knowledge from the seen classes tounseen ones. Capturing the common semantic characteristics between the visualmodality and the class semantic modality (e.g., attributes or word vector) is akey to the success of ZSL. In this paper, we propose a novel encoder-decoderapproach, namely Latent Space Encoding (LSE), to connect the semantic relationsof different modalities. Instead of requiring a projection function to transferinformation across different modalities like most previous work, LSE per- formsthe interactions of different modalities via a feature aware latent space,which is learned in an implicit way. Specifically, different modalities aremodeled separately but optimized jointly. For each modality, an encoder-decoderframework is performed to learn a feature aware latent space via jointlymaximizing the recoverability of the original space from the latent space andthe predictability of the latent space from the original space. To relatedifferent modalities together, their features referring to the same concept areenforced to share the same latent codings. In this way, the common semanticcharacteristics of different modalities are generalized with the latentrepresentations. Another property of the proposed approach is that it is easilyextended to more modalities. Extensive experimental results on four benchmarkdatasets (AwA, CUB, aPY, and ImageNet) clearly demonstrate the superiority ofthe proposed approach on several ZSL tasks, including traditional ZSL,generalized ZSL, and zero-shot retrieval (ZSR).

