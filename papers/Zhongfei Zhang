Text Coherence Analysis Based on Deep Neural Network

  In this paper, we propose a novel deep coherence model (DCM) using a
convolutional neural network architecture to capture the text coherence. The
text coherence problem is investigated with a new perspective of learning
sentence distributional representation and text coherence modeling
simultaneously. In particular, the model captures the interactions between
sentences by computing the similarities of their distributional
representations. Further, it can be easily trained in an end-to-end fashion.
The proposed model is evaluated on a standard Sentence Ordering task. The
experimental results demonstrate its effectiveness and promise in coherence
assessment showing a significant improvement over the state-of-the-art by a
wide margin.


Non-Abelian gauge field optics

  The concept of gauge field is a cornerstone of modern physics and the
synthetic gauge field has emerged as a new way to manipulate neutral particles
in many disciplines. In optics, several schemes have been proposed to realize
Abelian synthetic gauge fields. Here, we introduce a new platform for realizing
synthetic $SU(2)$ non-Abelian gauge fields acting on two-dimensional optical
waves in a wide class of anisotropic materials and discover new phenomena. We
show that a virtual non-Abelian Lorentz force can arise from the material
anisotropy, which can induce wave packets to travel along wavy "Zitterbewegung"
trajectories even in homogeneous media. We further propose an interferometry
scheme to realize the non-Abelian Aharonov--Bohm effect of light, which
highlights the essential non-Abelian nature of the system. We also show that
the Wilson loop of an arbitrary closed optical path can be extracted from a
series of gauge fixed points in the interference fringes.


Pyramid Person Matching Network for Person Re-identification

  In this work, we present a deep convolutional pyramid person matching network
(PPMN) with specially designed Pyramid Matching Module to address the problem
of person re-identification. The architecture takes a pair of RGB images as
input, and outputs a similiarity value indicating whether the two input images
represent the same person or not. Based on deep convolutional neural networks,
our approach first learns the discriminative semantic representation with the
semantic-component-aware features for persons and then employs the Pyramid
Matching Module to match the common semantic-components of persons, which is
robust to the variation of spatial scales and misalignment of locations posed
by viewpoint changes. The above two processes are jointly optimized via a
unified end-to-end deep learning scheme. Extensive experiments on several
benchmark datasets demonstrate the effectiveness of our approach against the
state-of-the-art approaches, especially on the rank-1 recognition rate.


Multi-Channel Pyramid Person Matching Network for Person
  Re-Identification

  In this work, we present a Multi-Channel deep convolutional Pyramid Person
Matching Network (MC-PPMN) based on the combination of the semantic-components
and the color-texture distributions to address the problem of person
re-identification. In particular, we learn separate deep representations for
semantic-components and color-texture distributions from two person images and
then employ pyramid person matching network (PPMN) to obtain correspondence
representations. These correspondence representations are fused to perform the
re-identification task. Further, the proposed framework is optimized via a
unified end-to-end deep learning scheme. Extensive experiments on several
benchmark datasets demonstrate the effectiveness of our approach against the
state-of-the-art literature, especially on the rank-1 recognition rate.


Context-Aware Hypergraph Construction for Robust Spectral Clustering

  Spectral clustering is a powerful tool for unsupervised data analysis. In
this paper, we propose a context-aware hypergraph similarity measure (CAHSM),
which leads to robust spectral clustering in the case of noisy data. We
construct three types of hypergraph---the pairwise hypergraph, the
k-nearest-neighbor (kNN) hypergraph, and the high-order over-clustering
hypergraph. The pairwise hypergraph captures the pairwise similarity of data
points; the kNN hypergraph captures the neighborhood of each point; and the
clustering hypergraph encodes high-order contexts within the dataset. By
combining the affinity information from these three hypergraphs, the CAHSM
algorithm is able to explore the intrinsic topological information of the
dataset. Therefore, data clustering using CAHSM tends to be more robust.
Considering the intra-cluster compactness and the inter-cluster separability of
vertices, we further design a discriminative hypergraph partitioning criterion
(DHPC). Using both CAHSM and DHPC, a robust spectral clustering algorithm is
developed. Theoretical analysis and experimental evaluation demonstrate the
effectiveness and robustness of the proposed algorithm.


Semisupervised Autoencoder for Sentiment Analysis

  In this paper, we investigate the usage of autoencoders in modeling textual
data. Traditional autoencoders suffer from at least two aspects: scalability
with the high dimensionality of vocabulary size and dealing with
task-irrelevant words. We address this problem by introducing supervision via
the loss function of autoencoders. In particular, we first train a linear
classifier on the labeled data, then define a loss for the autoencoder with the
weights learned from the linear classifier. To reduce the bias brought by one
single classifier, we define a posterior probability distribution on the
weights of the classifier, and derive the marginalized loss of the autoencoder
with Laplace approximation. We show that our choice of loss function can be
rationalized from the perspective of Bregman Divergence, which justifies the
soundness of our model. We evaluate the effectiveness of our model on six
sentiment analysis datasets, and show that our model significantly outperforms
all the competing methods with respect to classification accuracy. We also show
that our model is able to take advantage of unlabeled dataset and get improved
performance. We further show that our model successfully learns highly
discriminative feature maps, which explains its superior performance.


Dropout Training of Matrix Factorization and Autoencoder for Link
  Prediction in Sparse Graphs

  Matrix factorization (MF) and Autoencoder (AE) are among the most successful
approaches of unsupervised learning. While MF based models have been
extensively exploited in the graph modeling and link prediction literature, the
AE family has not gained much attention. In this paper we investigate both MF
and AE's application to the link prediction problem in sparse graphs. We show
the connection between AE and MF from the perspective of multiview learning,
and further propose MF+AE: a model training MF and AE jointly with shared
parameters. We apply dropout to training both the MF and AE parts, and show
that it can significantly prevent overfitting by acting as an adaptive
regularization. We conduct experiments on six real world sparse graph datasets,
and show that MF+AE consistently outperforms the competing methods, especially
on datasets that demonstrate strong non-cohesive structures.


Zero-Shot Learning with Multi-Battery Factor Analysis

  Zero-shot learning (ZSL) extends the conventional image classification
technique to a more challenging situation where the test image categories are
not seen in the training samples. Most studies on ZSL utilize side information
such as attributes or word vectors to bridge the relations between the seen
classes and the unseen classes. However, existing approaches on ZSL typically
exploit a shared space for each type of side information independently, which
cannot make full use of the complementary knowledge of different types of side
information. To this end, this paper presents an MBFA-ZSL approach to embed
different types of side information as well as the visual feature into one
shared space. Specifically, we first develop an algorithm named Multi-Battery
Factor Analysis (MBFA) to build a unified semantic space, and then employ
multiple types of side information in it to achieve the ZSL. The close-form
solution makes MBFA-ZSL simple to implement and efficient to run on large
datasets. Extensive experiments on the popular AwA, CUB, and SUN datasets show
its significant superiority over the state-of-the-art approaches.


Multimodal Skip-gram Using Convolutional Pseudowords

  This work studies the representational mapping across multimodal data such
that given a piece of the raw data in one modality the corresponding semantic
description in terms of the raw data in another modality is immediately
obtained. Such a representational mapping can be found in a wide spectrum of
real-world applications including image/video retrieval, object recognition,
action/behavior recognition, and event understanding and prediction. To that
end, we introduce a simplified training objective for learning multimodal
embeddings using the skip-gram architecture by introducing convolutional
"pseudowords:" embeddings composed of the additive combination of distributed
word representations and image features from convolutional neural networks
projected into the multimodal space. We present extensive results of the
representational properties of these embeddings on various word similarity
benchmarks to show the promise of this approach.


Manifold Regularized Discriminative Neural Networks

  Unregularized deep neural networks (DNNs) can be easily overfit with a
limited sample size. We argue that this is mostly due to the disriminative
nature of DNNs which directly model the conditional probability (or score) of
labels given the input. The ignorance of input distribution makes DNNs
difficult to generalize to unseen data. Recent advances in regularization
techniques, such as pretraining and dropout, indicate that modeling input data
distribution (either explicitly or implicitly) greatly improves the
generalization ability of a DNN. In this work, we explore the manifold
hypothesis which assumes that instances within the same class lie in a smooth
manifold. We accordingly propose two simple regularizers to a standard
discriminative DNN. The first one, named Label-Aware Manifold Regularization,
assumes the availability of labels and penalizes large norms of the loss
function w.r.t. data points. The second one, named Label-Independent Manifold
Regularization, does not use label information and instead penalizes the
Frobenius norm of the Jacobian matrix of prediction scores w.r.t. data points,
which makes semi-supervised learning possible. We perform extensive control
experiments on fully supervised and semi-supervised tasks using the MNIST,
CIFAR10 and SVHN datasets and achieve excellent results.


Online Metric-Weighted Linear Representations for Robust Visual Tracking

  In this paper, we propose a visual tracker based on a metric-weighted linear
representation of appearance. In order to capture the interdependence of
different feature dimensions, we develop two online distance metric learning
methods using proximity comparison information and structured output learning.
The learned metric is then incorporated into a linear representation of
appearance.
  We show that online distance metric learning significantly improves the
robustness of the tracker, especially on those sequences exhibiting drastic
appearance changes. In order to bound growth in the number of training samples,
we design a time-weighted reservoir sampling method.
  Moreover, we enable our tracker to automatically perform object
identification during the process of object tracking, by introducing a
collection of static template samples belonging to several object classes of
interest. Object identification results for an entire video sequence are
achieved by systematically combining the tracking information and visual
recognition at each frame. Experimental results on challenging video sequences
demonstrate the effectiveness of the method for both inter-frame tracking and
object identification.


Deep Structured Energy Based Models for Anomaly Detection

  In this paper, we attack the anomaly detection problem by directly modeling
the data distribution with deep architectures. We propose deep structured
energy based models (DSEBMs), where the energy function is the output of a
deterministic deep neural network with structure. We develop novel model
architectures to integrate EBMs with different types of data such as static
data, sequential data, and spatial data, and apply appropriate model
architectures to adapt to the data structure. Our training algorithm is built
upon the recent development of score matching \cite{sm}, which connects an EBM
with a regularized autoencoder, eliminating the need for complicated sampling
method. Statistically sound decision criterion can be derived for anomaly
detection purpose from the perspective of the energy landscape of the data
distribution. We investigate two decision criteria for performing anomaly
detection: the energy score and the reconstruction error. Extensive empirical
studies on benchmark tasks demonstrate that our proposed model consistently
matches or outperforms all the competing methods.


Doubly Convolutional Neural Networks

  Building large models with parameter sharing accounts for most of the success
of deep convolutional neural networks (CNNs). In this paper, we propose doubly
convolutional neural networks (DCNNs), which significantly improve the
performance of CNNs by further exploring this idea. In stead of allocating a
set of convolutional filters that are independently learned, a DCNN maintains
groups of filters where filters within each group are translated versions of
each other. Practically, a DCNN can be easily implemented by a two-step
convolution procedure, which is supported by most modern deep learning
libraries. We perform extensive experiments on three image classification
benchmarks: CIFAR-10, CIFAR-100 and ImageNet, and show that DCNNs consistently
outperform other competing architectures. We have also verified that replacing
a convolutional layer with a doubly convolutional layer at any depth of a CNN
can improve its performance. Moreover, various design choices of DCNNs are
demonstrated, which shows that DCNN can serve the dual purpose of building more
accurate models and/or reducing the memory footprint without sacrificing the
accuracy.


Generative Adversarial Networks as Variational Training of Energy Based
  Models

  In this paper, we study deep generative models for effective unsupervised
learning. We propose VGAN, which works by minimizing a variational lower bound
of the negative log likelihood (NLL) of an energy based model (EBM), where the
model density $p(\mathbf{x})$ is approximated by a variational distribution
$q(\mathbf{x})$ that is easy to sample from. The training of VGAN takes a two
step procedure: given $p(\mathbf{x})$, $q(\mathbf{x})$ is updated to maximize
the lower bound; $p(\mathbf{x})$ is then updated one step with samples drawn
from $q(\mathbf{x})$ to decrease the lower bound. VGAN is inspired by the
generative adversarial networks (GANs), where $p(\mathbf{x})$ corresponds to
the discriminator and $q(\mathbf{x})$ corresponds to the generator, but with
several notable differences. We hence name our model variational GANs (VGANs).
VGAN provides a practical solution to training deep EBMs in high dimensional
space, by eliminating the need of MCMC sampling. From this view, we are also
able to identify causes to the difficulty of training GANs and propose viable
solutions. \footnote{Experimental code is available at
https://github.com/Shuangfei/vgan}


Structural Correspondence Learning for Cross-lingual Sentiment
  Classification with One-to-many Mappings

  Structural correspondence learning (SCL) is an effective method for
cross-lingual sentiment classification. This approach uses unlabeled documents
along with a word translation oracle to automatically induce task specific,
cross-lingual correspondences. It transfers knowledge through identifying
important features, i.e., pivot features. For simplicity, however, it assumes
that the word translation oracle maps each pivot feature in source language to
exactly only one word in target language. This one-to-one mapping between words
in different languages is too strict. Also the context is not considered at
all. In this paper, we propose a cross-lingual SCL based on distributed
representation of words; it can learn meaningful one-to-many mappings for pivot
words using large amounts of monolingual data and a small dictionary. We
conduct experiments on NLP\&CC 2013 cross-lingual sentiment analysis dataset,
employing English as source language, and Chinese as target language. Our
method does not rely on the parallel corpora and the experimental results show
that our approach is more competitive than the state-of-the-art methods in
cross-lingual sentiment classification.


Boosted Zero-Shot Learning with Semantic Correlation Regularization

  We study zero-shot learning (ZSL) as a transfer learning problem, and focus
on the two key aspects of ZSL, model effectiveness and model adaptation. For
effective modeling, we adopt the boosting strategy to learn a zero-shot
classifier from weak models to a strong model. For adaptable knowledge
transfer, we devise a Semantic Correlation Regularization (SCR) approach to
regularize the boosted model to be consistent with the inter-class semantic
correlations. With SCR embedded in the boosting objective, and with a
self-controlled sample selection for learning robustness, we propose a unified
framework, Boosted Zero-shot classification with Semantic Correlation
Regularization (BZ-SCR). By balancing the SCR-regularized boosted model
selection and the self-controlled sample selection, BZ-SCR is capable of
capturing both discriminative and adaptable feature-to-class semantic
alignments, while ensuring the reliability and adaptability of the learned
samples. The experiments on two ZSL datasets show the superiority of BZ-SCR
over the state-of-the-arts.


A Deep Learning Approach for Expert Identification in Question Answering
  Communities

  In this paper, we describe an effective convolutional neural network
framework for identifying the expert in question answering community. This
approach uses the convolutional neural network and combines user feature
representations with question feature representations to compute scores that
the user who gets the highest score is the expert on this question. Unlike
prior work, this method does not measure expert based on measure answer content
quality to identify the expert but only require question sentence and user
embedding feature to identify the expert. Remarkably, Our model can be applied
to different languages and different domains. The proposed framework is trained
on two datasets, The first dataset is Stack Overflow and the second one is
Zhihu. The Top-1 accuracy results of our experiments show that our framework
outperforms the best baseline framework for expert identification.


GNAS: A Greedy Neural Architecture Search Method for Multi-Attribute
  Learning

  A key problem in deep multi-attribute learning is to effectively discover the
inter-attribute correlation structures. Typically, the conventional deep
multi-attribute learning approaches follow the pipeline of manually designing
the network architectures based on task-specific expertise prior knowledge and
careful network tunings, leading to the inflexibility for various complicated
scenarios in practice. Motivated by addressing this problem, we propose an
efficient greedy neural architecture search approach (GNAS) to automatically
discover the optimal tree-like deep architecture for multi-attribute learning.
In a greedy manner, GNAS divides the optimization of global architecture into
the optimizations of individual connections step by step. By iteratively
updating the local architectures, the global tree-like architecture gets
converged where the bottom layers are shared across relevant attributes and the
branches in top layers more encode attribute-specific features. Experiments on
three benchmark multi-attribute datasets show the effectiveness and compactness
of neural architectures derived by GNAS, and also demonstrate the efficiency of
GNAS in searching neural architectures.


Stacked Semantic-Guided Attention Model for Fine-Grained Zero-Shot
  Learning

  Zero-Shot Learning (ZSL) is achieved via aligning the semantic relationships
between the global image feature vector and the corresponding class semantic
descriptions. However, using the global features to represent fine-grained
images may lead to sub-optimal results since they neglect the discriminative
differences of local regions. Besides, different regions contain distinct
discriminative information. The important regions should contribute more to the
prediction. To this end, we propose a novel stacked semantics-guided attention
(S2GA) model to obtain semantic relevant features by using individual class
semantic features to progressively guide the visual features to generate an
attention map for weighting the importance of different local regions. Feeding
both the integrated visual features and the class semantic features into a
multi-class classification architecture, the proposed framework can be trained
end-to-end. Extensive experimental results on CUB and NABird datasets show that
the proposed approach has a consistent improvement on both fine-grained
zero-shot classification and retrieval tasks.


Stacked Pooling: Improving Crowd Counting by Boosting Scale Invariance

  In this work, we explore the cross-scale similarity in crowd counting
scenario, in which the regions of different scales often exhibit high visual
similarity. This feature is universal both within an image and across different
images, indicating the importance of scale invariance of a crowd counting
model. Motivated by this, in this paper we propose simple but effective
variants of pooling module, i.e., multi-kernel pooling and stacked pooling, to
boost the scale invariance of convolutional neural networks (CNNs), benefiting
much the crowd density estimation and counting. Specifically, the multi-kernel
pooling comprises of pooling kernels with multiple receptive fields to capture
the responses at multi-scale local ranges. The stacked pooling is an equivalent
form of multi-kernel pooling, while, it reduces considerable computing cost.
Our proposed pooling modules do not introduce extra parameters into model and
can easily take place of the vanilla pooling layer in implementation. In
empirical study on two benchmark crowd counting datasets, the stacked pooling
beats the vanilla pooling layer in most cases.


Bi-Adversarial Auto-Encoder for Zero-Shot Learning

  Existing generative Zero-Shot Learning (ZSL) methods only consider the
unidirectional alignment from the class semantics to the visual features while
ignoring the alignment from the visual features to the class semantics, which
fails to construct the visual-semantic interactions well. In this paper, we
propose to synthesize visual features based on an auto-encoder framework paired
with bi-adversarial networks respectively for visual and semantic modalities to
reinforce the visual-semantic interactions with a bi-directional alignment,
which ensures the synthesized visual features to fit the real visual
distribution and to be highly related to the semantics. The encoder aims at
synthesizing real-like visual features while the decoder forces both the real
and the synthesized visual features to be more related to the class semantics.
To further capture the discriminative information of the synthesized visual
features, both the real and synthesized visual features are forced to be
classified into the correct classes via a classification network. Experimental
results on four benchmark datasets show that the proposed approach is
particularly competitive on both the traditional ZSL and the generalized ZSL
tasks.


Perceiving Physical Equation by Observing Visual Scenarios

  Inferring universal laws of the environment is an important ability of human
intelligence as well as a symbol of general AI. In this paper, we take a step
toward this goal such that we introduce a new challenging problem of inferring
invariant physical equation from visual scenarios. For instance, teaching a
machine to automatically derive the gravitational acceleration formula by
watching a free-falling object. To tackle this challenge, we present a novel
pipeline comprised of an Observer Engine and a Physicist Engine by respectively
imitating the actions of an observer and a physicist in the real world.
Generally, the Observer Engine watches the visual scenarios and then extracting
the physical properties of objects. The Physicist Engine analyses these data
and then summarizing the inherent laws of object dynamics. Specifically, the
learned laws are expressed by mathematical equations such that they are more
interpretable than the results given by common probabilistic models.
Experiments on synthetic videos have shown that our pipeline is able to
discover physical equations on various physical worlds with different visual
appearances.


Cross-relation Cross-bag Attention for Distantly-supervised Relation
  Extraction

  Distant supervision leverages knowledge bases to automatically label
instances, thus allowing us to train relation extractor without human
annotations. However, the generated training data typically contain massive
noise, and may result in poor performances with the vanilla supervised
learning. In this paper, we propose to conduct multi-instance learning with a
novel Cross-relation Cross-bag Selective Attention (C$^2$SA), which leads to
noise-robust training for distant supervised relation extractor. Specifically,
we employ the sentence-level selective attention to reduce the effect of noisy
or mismatched sentences, while the correlation among relations were captured to
improve the quality of attention weights. Moreover, instead of treating all
entity-pairs equally, we try to pay more attention to entity-pairs with a
higher quality. Similarly, we adopt the selective attention mechanism to
achieve this goal. Experiments with two types of relation extractor demonstrate
the superiority of the proposed approach over the state-of-the-art, while
further ablation studies verify our intuitions and demonstrate the
effectiveness of our proposed two techniques.


Text Guided Person Image Synthesis

  This paper presents a novel method to manipulate the visual appearance (pose
and attribute) of a person image according to natural language descriptions.
Our method can be boiled down to two stages: 1) text guided pose generation and
2) visual appearance transferred image synthesis. In the first stage, our
method infers a reasonable target human pose based on the text. In the second
stage, our method synthesizes a realistic and appearance transferred person
image according to the text in conjunction with the target pose. Our method
extracts sufficient information from the text and establishes a mapping between
the image space and the language space, making generating and editing images
corresponding to the description possible. We conduct extensive experiments to
reveal the effectiveness of our method, as well as using the VQA Perceptual
Score as a metric for evaluating the method. It shows for the first time that
we can automatically edit the person image from the natural language
descriptions.


A Survey of Appearance Models in Visual Object Tracking

  Visual object tracking is a significant computer vision task which can be
applied to many domains such as visual surveillance, human computer
interaction, and video compression. In the literature, researchers have
proposed a variety of 2D appearance models. To help readers swiftly learn the
recent advances in 2D appearance models for visual object tracking, we
contribute this survey, which provides a detailed review of the existing 2D
appearance models. In particular, this survey takes a module-based architecture
that enables readers to easily grasp the key points of visual object tracking.
In this survey, we first decompose the problem of appearance modeling into two
different processing stages: visual representation and statistical modeling.
Then, different 2D appearance models are categorized and discussed with respect
to their composition modules. Finally, we address several issues of interest as
well as the remaining challenges for future research on this topic. The
contributions of this survey are four-fold. First, we review the literature of
visual representations according to their feature-construction mechanisms
(i.e., local and global). Second, the existing statistical modeling schemes for
tracking-by-detection are reviewed according to their model-construction
mechanisms: generative, discriminative, and hybrid generative-discriminative.
Third, each type of visual representations or statistical modeling techniques
is analyzed and discussed from a theoretical or practical viewpoint. Fourth,
the existing benchmark resources (e.g., source code and video datasets) are
examined in this survey.


Deep Learning Driven Visual Path Prediction from a Single Image

  Capabilities of inference and prediction are significant components of visual
systems. In this paper, we address an important and challenging task of them:
visual path prediction. Its goal is to infer the future path for a visual
object in a static scene. This task is complicated as it needs high-level
semantic understandings of both the scenes and motion patterns underlying video
sequences. In practice, cluttered situations have also raised higher demands on
the effectiveness and robustness of the considered models. Motivated by these
observations, we propose a deep learning framework which simultaneously
performs deep feature learning for visual representation in conjunction with
spatio-temporal context modeling. After that, we propose a unified path
planning scheme to make accurate future path prediction based on the analytic
results of the context models. The highly effective visual representation and
deep context models ensure that our framework makes a deep semantic
understanding of the scene and motion pattern, consequently improving the
performance of the visual path prediction task. In order to comprehensively
evaluate the model's performance on the visual path prediction task, we
construct two large benchmark datasets from the adaptation of video tracking
datasets. The qualitative and quantitative experimental results show that our
approach outperforms the existing approaches and owns a better generalization
capability.


A Survey of Multi-View Representation Learning

  Recently, multi-view representation learning has become a rapidly growing
direction in machine learning and data mining areas. This paper introduces two
categories for multi-view representation learning: multi-view representation
alignment and multi-view representation fusion. Consequently, we first review
the representative methods and theories of multi-view representation learning
based on the perspective of alignment, such as correlation-based alignment.
Representative examples are canonical correlation analysis (CCA) and its
several extensions. Then from the perspective of representation fusion we
investigate the advancement of multi-view representation learning that ranges
from generative methods including multi-modal topic learning, multi-view sparse
coding, and multi-view latent space Markov networks, to neural network-based
methods including multi-modal autoencoders, multi-view convolutional neural
networks, and multi-modal recurrent neural networks. Further, we also
investigate several important applications of multi-view representation
learning. Overall, this survey aims to provide an insightful overview of
theoretical foundation and state-of-the-art developments in the field of
multi-view representation learning and to help researchers find the most
appropriate tools for particular applications.


S3Pool: Pooling with Stochastic Spatial Sampling

  Feature pooling layers (e.g., max pooling) in convolutional neural networks
(CNNs) serve the dual purpose of providing increasingly abstract
representations as well as yielding computational savings in subsequent
convolutional layers. We view the pooling operation in CNNs as a two-step
procedure: first, a pooling window (e.g., $2\times 2$) slides over the feature
map with stride one which leaves the spatial resolution intact, and second,
downsampling is performed by selecting one pixel from each non-overlapping
pooling window in an often uniform and deterministic (e.g., top-left) manner.
Our starting point in this work is the observation that this regularly spaced
downsampling arising from non-overlapping windows, although intuitive from a
signal processing perspective (which has the goal of signal reconstruction), is
not necessarily optimal for \emph{learning} (where the goal is to generalize).
We study this aspect and propose a novel pooling strategy with stochastic
spatial sampling (S3Pool), where the regular downsampling is replaced by a more
general stochastic version. We observe that this general stochasticity acts as
a strong regularizer, and can also be seen as doing implicit data augmentation
by introducing distortions in the feature maps. We further introduce a
mechanism to control the amount of distortion to suit different datasets and
architectures. To demonstrate the effectiveness of the proposed approach, we
perform extensive experiments on several popular image classification
benchmarks, observing excellent improvements over baseline models. Experimental
code is available at https://github.com/Shuangfei/s3pool.


Transductive Zero-Shot Learning with a Self-training dictionary approach

  As an important and challenging problem in computer vision, zero-shot
learning (ZSL) aims at automatically recognizing the instances from unseen
object classes without training data. To address this problem, ZSL is usually
carried out in the following two aspects: 1) capturing the domain distribution
connections between seen classes data and unseen classes data; and 2) modeling
the semantic interactions between the image feature space and the label
embedding space. Motivated by these observations, we propose a bidirectional
mapping based semantic relationship modeling scheme that seeks for crossmodal
knowledge transfer by simultaneously projecting the image features and label
embeddings into a common latent space. Namely, we have a bidirectional
connection relationship that takes place from the image feature space to the
latent space as well as from the label embedding space to the latent space. To
deal with the domain shift problem, we further present a transductive learning
approach that formulates the class prediction problem in an iterative refining
process, where the object classification capacity is progressively reinforced
through bootstrapping-based model updating over highly reliable instances.
Experimental results on three benchmark datasets (AwA, CUB and SUN) demonstrate
the effectiveness of the proposed approach against the state-of-the-art
approaches.


Deep Air Learning: Interpolation, Prediction, and Feature Analysis of
  Fine-grained Air Quality

  The interpolation, prediction, and feature analysis of fine-gained air
quality are three important topics in the area of urban air computing. The
solutions to these topics can provide extremely useful information to support
air pollution control, and consequently generate great societal and technical
impacts. Most of the existing work solves the three problems separately by
different models. In this paper, we propose a general and effective approach to
solve the three problems in one model called the Deep Air Learning (DAL). The
main idea of DAL lies in embedding feature selection and semi-supervised
learning in different layers of the deep learning network. The proposed
approach utilizes the information pertaining to the unlabeled spatio-temporal
data to improve the performance of the interpolation and the prediction, and
performs feature selection and association analysis to reveal the main relevant
features to the variation of the air quality. We evaluate our approach with
extensive experiments based on real data sources obtained in Beijing, China.
Experiments show that DAL is superior to the peer models from the recent
literature when solving the topics of interpolation, prediction, and feature
analysis of fine-gained air quality.


Zero-Shot Learning via Latent Space Encoding

  Zero-Shot Learning (ZSL) is typically achieved by resorting to a class
semantic embedding space to transfer the knowledge from the seen classes to
unseen ones. Capturing the common semantic characteristics between the visual
modality and the class semantic modality (e.g., attributes or word vector) is a
key to the success of ZSL. In this paper, we propose a novel encoder-decoder
approach, namely Latent Space Encoding (LSE), to connect the semantic relations
of different modalities. Instead of requiring a projection function to transfer
information across different modalities like most previous work, LSE per- forms
the interactions of different modalities via a feature aware latent space,
which is learned in an implicit way. Specifically, different modalities are
modeled separately but optimized jointly. For each modality, an encoder-decoder
framework is performed to learn a feature aware latent space via jointly
maximizing the recoverability of the original space from the latent space and
the predictability of the latent space from the original space. To relate
different modalities together, their features referring to the same concept are
enforced to share the same latent codings. In this way, the common semantic
characteristics of different modalities are generalized with the latent
representations. Another property of the proposed approach is that it is easily
extended to more modalities. Extensive experimental results on four benchmark
datasets (AwA, CUB, aPY, and ImageNet) clearly demonstrate the superiority of
the proposed approach on several ZSL tasks, including traditional ZSL,
generalized ZSL, and zero-shot retrieval (ZSR).


