Evaluating Design Tradeoffs in Numeric Static Analysis for Java

  Numeric static analysis for Java has a broad range of potentially usefulapplications, including array bounds checking and resource usage estimation.However, designing a scalable numeric static analysis for real-world Javaprograms presents a multitude of design choices, each of which may interactwith others. For example, an analysis could handle method calls via either atop-down or bottom-up interprocedural analysis. Moreover, this choice couldinteract with how we choose to represent aliasing in the heap and/or whether weuse a relational numeric domain, e.g., convex polyhedra. In this paper, wepresent a family of abstract interpretation-based numeric static analyses forJava and systematically evaluate the impact of 162 analysis configurations onthe DaCapo benchmark suite. Our experiment considered the precision andperformance of the analyses for discharging array bounds checks. We found thattop-down analysis is generally a better choice than bottom-up analysis, andthat using access paths to describe heap objects is better than using summaryobjects corresponding to points-to analysis locations. Moreover, these twochoices are the most significant, while choices about the numeric domain,representation of abstract objects, and context-sensitivity make much lessdifference to the precision/performance tradeoff.

Evaluating Fuzz Testing

  Fuzz testing has enjoyed great success at discovering security critical bugsin real software. Recently, researchers have devoted significant effort todevising new fuzzing techniques, strategies, and algorithms. Such new ideas areprimarily evaluated experimentally so an important question is: Whatexperimental setup is needed to produce trustworthy results? We surveyed therecent research literature and assessed the experimental evaluations carriedout by 32 fuzzing papers. We found problems in every evaluation we considered.We then performed our own extensive experimental evaluation using an existingfuzzer. Our results showed that the general problems we found in existingexperimental evaluations can indeed translate to actual wrong or misleadingassessments. We conclude with some guidelines that we hope will help improveexperimental evaluations of fuzz testing algorithms, making reported resultsmore robust.

