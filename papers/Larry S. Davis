Parameterizing Region Covariance: An Efficient Way To Apply Sparse Codes  On Second Order Statistics

  Sparse representations have been successfully applied to signal processing,computer vision and machine learning. Currently there is a trend to learnsparse models directly on structure data, such as region covariance. However,such methods when combined with region covariance often require complexcomputation. We present an approach to transform a structured sparse modellearning problem to a traditional vectorized sparse modeling problem byconstructing a Euclidean space representation for region covariance matrices.Our new representation has multiple advantages. Experiments on several visiontasks demonstrate competitive performance with the state-of-the-art methods.

Scalable Gaussian Processes for Supervised Hashing

  We propose a flexible procedure for large-scale image search by hashfunctions with kernels. Our method treats binary codes and pairwise semanticsimilarity as latent and observed variables, respectively, in a probabilisticmodel based on Gaussian processes for binary classification. We present anefficient inference algorithm with the sparse pseudo-input Gaussian process(SPGP) model and parallelization. Experiments on three large-scale imagedataset demonstrate the effectiveness of the proposed hashing method, GaussianProcess Hashing (GPH), for short binary codes and the datasets withoutpredefined classes in comparison to the state-of-the-art supervised hashingmethods.

Mining Discriminative Triplets of Patches for Fine-Grained  Classification

  Fine-grained classification involves distinguishing between similarsub-categories based on subtle differences in highly localized regions;therefore, accurate localization of discriminative regions remains a majorchallenge. We describe a patch-based framework to address this problem. Weintroduce triplets of patches with geometric constraints to improve theaccuracy of patch localization, and automatically mine discriminativegeometrically-constrained triplets for classification. The resulting approachonly requires object bounding boxes. Its effectiveness is demonstrated usingfour publicly available fine-grained datasets, on which it outperforms orachieves comparable performance to the state-of-the-art in classification.

Two-Stream Neural Networks for Tampered Face Detection

  We propose a two-stream network for face tampering detection. We trainGoogLeNet to detect tampering artifacts in a face classification stream, andtrain a patch based triplet network to leverage features capturing local noiseresiduals and camera characteristics as a second stream. In addition, we usetwo different online face swapping applications to create a new dataset thatconsists of 2010 tampered images, each of which contains a tampered face. Weevaluate the proposed two-stream network on our newly collected dataset.Experimental results demonstrate the effectiveness of our method.

Comparing apples to apples in the evaluation of binary coding methods

  We discuss methodological issues related to the evaluation of unsupervisedbinary code construction methods for nearest neighbor search. These issues havebeen widely ignored in literature. These coding methods attempt to preserveeither Euclidean distance or angular (cosine) distance in the binary embeddingspace. We explain why when comparing a method whose goal is preserving cosinesimilarity to one designed for preserving Euclidean distance, the originalfeatures should be normalized by mapping them to the unit hypersphere beforelearning the binary mapping functions. To compare a method whose goal is topreserves Euclidean distance to one that preserves cosine similarity, theoriginal feature data must be mapped to a higher dimension by including a biasterm in binary mapping functions. These conditions ensure the fair comparisonbetween different binary code methods for the task of nearest neighbor search.Our experiments show under these conditions the very simple methods (e.g. LSHand ITQ) often outperform recent state-of-the-art methods (e.g. MDSH andOK-means).

SHOE: Supervised Hashing with Output Embeddings

  We present a supervised binary encoding scheme for image retrieval thatlearns projections by taking into account similarity between classes obtainedfrom output embeddings. Our motivation is that binary hash codes learned inthis way improve both the visual quality of retrieval results and existingsupervised hashing schemes. We employ a sequential greedy optimization thatlearns relationship aware projections by minimizing the difference betweeninner products of binary codes and output embedding vectors. We develop a jointoptimization framework to learn projections which improve the accuracy ofsupervised hashing over the current state of the art with respect to standardand sibling evaluation metrics. We further boost performance by applying thesupervised dimensionality reduction technique on kernelized input CNN features.Experiments are performed on three datasets: CUB-2011, SUN-Attribute andImageNet ILSVRC 2010. As a by-product of our method, we show that using asimple k-nn pooling classifier with our discriminative codes improves over thecomplex classification models on fine grained datasets like CUB and offer animpressive compression ratio of 1024 on CNN features.

Exploiting Local Features from Deep Networks for Image Retrieval

  Deep convolutional neural networks have been successfully applied to imageclassification tasks. When these same networks have been applied to imageretrieval, the assumption has been made that the last layers would give thebest performance, as they do in classification. We show that for instance-levelimage retrieval, lower layers often perform better than the last layers inconvolutional neural networks. We present an approach for extractingconvolutional features from different layers of the networks, and adopt VLADencoding to encode features into a single vector for each image. We investigatethe effect of different layers and scales of input images on the performance ofconvolutional features using the recent deep networks OxfordNet and GoogLeNet.Experiments demonstrate that intermediate layers or higher layers with finerscales produce better results for image retrieval, compared to the last layer.When using compressed 128-D VLAD descriptors, our method obtainsstate-of-the-art results and outperforms other VLAD and CNN based approaches ontwo out of three test datasets. Our work provides guidance for transferringdeep networks trained on image classification to image retrieval tasks.

On Large-Scale Retrieval: Binary or n-ary Coding?

  The growing amount of data available in modern-day datasets makes the need toefficiently search and retrieve information. To make large-scale searchfeasible, Distance Estimation and Subset Indexing are the main approaches.Although binary coding has been popular for implementing both techniques, n-arycoding (known as Product Quantization) is also very effective for DistanceEstimation. However, their relative performance has not been studied for SubsetIndexing. We investigate whether binary or n-ary coding works better underdifferent retrieval strategies. This leads to the design of a new n-ary codingmethod, "Linear Subspace Quantization (LSQ)" which, unlike other n-aryencoders, can be used as a similarity-preserving embedding. Experiments onimage retrieval show that when Distance Estimation is used, n-ary LSQoutperforms other methods. However, when Subset Indexing is applied,interestingly, binary codings are more effective and binary LSQ achieves thebest accuracy.

Searching for Objects using Structure in Indoor Scenes

  To identify the location of objects of a particular class, a passive computervision system generally processes all the regions in an image to finally outputfew regions. However, we can use structure in the scene to search for objectswithout processing the entire image. We propose a search technique thatsequentially processes image regions such that the regions that are more likelyto correspond to the query class object are explored earlier. We frame theproblem as a Markov decision process and use an imitation learning algorithm tolearn a search strategy. Since structure in the scene is essential for search,we work with indoor scene images as they contain both unary scene contextinformation and object-object context in the scene. We perform experiments onthe NYU-depth v2 dataset and show that the unary scene context features alonecan achieve a significantly high average precision while processing only20-25\% of the regions for classes like bed and sofa. By consideringobject-object context along with the scene context features, the performance isfurther improved for classes like counter, lamp, pillow and sofa.

G-CNN: an Iterative Grid Based Object Detector

  We introduce G-CNN, an object detection technique based on CNNs which workswithout proposal algorithms. G-CNN starts with a multi-scale grid of fixedbounding boxes. We train a regressor to move and scale elements of the gridtowards objects iteratively. G-CNN models the problem of object detection asfinding a path from a fixed grid to boxes tightly surrounding the objects.G-CNN with around 180 boxes in a multi-scale grid performs comparably to FastR-CNN which uses around 2K bounding boxes generated with a proposal technique.This strategy makes detection faster by removing the object proposal stage aswell as reducing the number of boxes to be processed.

Generating Discriminative Object Proposals via Submodular Ranking

  A multi-scale greedy-based object proposal generation approach is presented.Based on the multi-scale nature of objects in images, our approach is built ontop of a hierarchical segmentation. We first identify the representative anddiverse exemplar clusters within each scale by using a diversity rankingalgorithm. Object proposals are obtained by selecting a subset from themulti-scale segment pool via maximizing a submodular objective function, whichconsists of a weighted coverage term, a single-scale diversity term and amulti-scale reward term. The weighted coverage term forces the selected set ofobject proposals to be representative and compact; the single-scale diversityterm encourages choosing segments from different exemplar clusters so that theywill cover as many object patterns as possible; the multi-scale reward termencourages the selected proposals to be discriminative and selected frommultiple layers generated by the hierarchical image segmentation. Theexperimental results on the Berkeley Segmentation Dataset and PASCAL VOC2012segmentation dataset demonstrate the accuracy and efficiency of our objectproposal model. Additionally, we validate our object proposals in simultaneoussegmentation and detection and outperform the state-of-art performance.

Learning Temporal Regularity in Video Sequences

  Perceiving meaningful activities in a long video sequence is a challengingproblem due to ambiguous definition of 'meaningfulness' as well as clutters inthe scene. We approach this problem by learning a generative model for regularmotion patterns, termed as regularity, using multiple sources with very limitedsupervision. Specifically, we propose two methods that are built upon theautoencoders for their ability to work with little to no supervision. We firstleverage the conventional handcrafted spatio-temporal local features and learna fully connected autoencoder on them. Second, we build a fully convolutionalfeed-forward autoencoder to learn both the local features and the classifiersas an end-to-end learning framework. Our model can capture the regularitiesfrom multiple datasets. We evaluate our methods in both qualitative andquantitative ways - showing the learned regularity of videos in various aspectsand demonstrating competitive performance on anomaly detection datasets as anapplication.

Supervised Incremental Hashing

  We propose an incremental strategy for learning hash functions with kernelsfor large-scale image search. Our method is based on a two-stage classificationframework that treats binary codes as intermediate variables between thefeature space and the semantic space. In the first stage of classification,binary codes are considered as class labels by a set of binary SVMs; eachcorresponds to one bit. In the second stage, binary codes become the inputspace of a multi-class SVM. Hash functions are learned by an efficientalgorithm where the NP-hard problem of finding optimal binary codes is solvedvia cyclic coordinate descent and SVMs are trained in a parallelizedincremental manner. For modifications like adding images from a previouslyunseen class, we describe an incremental procedure for effective and efficientupdates to the previous hash functions. Experiments on three large-scale imagedatasets demonstrate the effectiveness of the proposed hashing method,Supervised Incremental Hashing (SIH), over the state-of-the-art supervisedhashing methods.

Modeling Context Between Objects for Referring Expression Understanding

  Referring expressions usually describe an object using properties of theobject and relationships of the object with other objects. We propose atechnique that integrates context between objects to understand referringexpressions. Our approach uses an LSTM to learn the probability of a referringexpression, with input features from a region and a context region. The contextregions are discovered using multiple-instance learning (MIL) since annotationsfor context objects are generally not available for training. We utilizemax-margin based MIL objective functions for training the LSTM. Experiments onthe Google RefExp and UNC RefExp datasets show that modeling context betweenobjects provides better performance than modeling only object properties. Wealso qualitatively show that our technique can ground a referring expression toits referred region along with the supporting context region.

The Role of Context Selection in Object Detection

  We investigate the reasons why context in object detection has limitedutility by isolating and evaluating the predictive power of different contextcues under ideal conditions in which context provided by an oracle. Based onthis study, we propose a region-based context re-scoring method with dynamiccontext selection to remove noise and emphasize informative context. Weintroduce latent indicator variables to select (or ignore) potential contextualregions, and learn the selection strategy with latent-SVM. We conductexperiments to evaluate the performance of the proposed context selectionmethod on the SUN RGB-D dataset. The method achieves a significant improvementin terms of mean average precision (mAP), compared with both appearance baseddetectors and a conventional context model without the selection scheme.

Fused DNN: A deep neural network fusion approach to fast and robust  pedestrian detection

  We propose a deep neural network fusion architecture for fast and robustpedestrian detection. The proposed network fusion architecture allows forparallel processing of multiple networks for speed. A single shot deepconvolutional network is trained as a object detector to generate all possiblepedestrian candidates of different sizes and occlusions. This network outputs alarge variety of pedestrian candidates to cover the majority of ground-truthpedestrians while also introducing a large number of false positives. Next,multiple deep neural networks are used in parallel for further refinement ofthese pedestrian candidates. We introduce a soft-rejection based network fusionmethod to fuse the soft metrics from all networks together to generate thefinal confidence scores. Our method performs better than existingstate-of-the-arts, especially when detecting small-size and occludedpedestrians. Furthermore, we propose a method for integrating pixel-wisesemantic segmentation network into the network fusion architecture as areinforcement to the pedestrian detector. The approach outperformsstate-of-the-art methods on most protocols on Caltech Pedestrian dataset, withsignificant boosts on several protocols. It is also faster than all othermethods.

Learning a Discriminative Filter Bank within a CNN for Fine-grained  Recognition

  Compared to earlier multistage frameworks using CNN features, recentend-to-end deep approaches for fine-grained recognition essentially enhance themid-level learning capability of CNNs. Previous approaches achieve this byintroducing an auxiliary network to infuse localization information into themain classification network, or a sophisticated feature encoding method tocapture higher order feature statistics. We show that mid-level representationlearning can be enhanced within the CNN framework, by learning a bank ofconvolutional filters that capture class-specific discriminative patcheswithout extra part or bounding box annotations. Such a filter bank is wellstructured, properly initialized and discriminatively learned through a novelasymmetric multi-stream architecture with convolutional filter supervision anda non-random layer initialization. Experimental results show that our approachachieves state-of-the-art on three publicly available fine-grained recognitiondatasets (CUB-200-2011, Stanford Cars and FGVC-Aircraft). Ablation studies andvisualizations are provided to understand our approach.

ActionFlowNet: Learning Motion Representation for Action Recognition

  Even with the recent advances in convolutional neural networks (CNN) invarious visual recognition tasks, the state-of-the-art action recognitionsystem still relies on hand crafted motion feature such as optical flow toachieve the best performance. We propose a multitask learning modelActionFlowNet to train a single stream network directly from raw pixels tojointly estimate optical flow while recognizing actions with convolutionalneural networks, capturing both appearance and motion in a single model. Weadditionally provide insights to how the quality of the learned optical flowaffects the action recognition. Our model significantly improves actionrecognition accuracy by a large margin 31% compared to state-of-the-artCNN-based action recognition models trained without external large scale dataand additional optical flow input. Without pretraining on large externallabeled datasets, our model, by well exploiting the motion information,achieves competitive recognition accuracy to the models trained with largelabeled datasets such as ImageNet and Sport-1M.

Generalized Deep Image to Image Regression

  We present a Deep Convolutional Neural Network architecture which serves as ageneric image-to-image regressor that can be trained end-to-end without anyfurther machinery. Our proposed architecture: the Recursively BranchedDeconvolutional Network (RBDN) develops a cheap multi-context imagerepresentation very early on using an efficient recursive branching scheme withextensive parameter sharing and learnable upsampling. This multi-contextrepresentation is subjected to a highly non-linear locality preservingtransformation by the remainder of our network comprising of a series ofconvolutions/deconvolutions without any spatial downsampling. The RBDNarchitecture is fully convolutional and can handle variable sized images duringinference. We provide qualitative/quantitative results on $3$ diverse tasks:relighting, denoising and colorization and show that our proposed RBDNarchitecture obtains comparable results to the state-of-the-art on each ofthese tasks when used off-the-shelf without any post processing ortask-specific architectural modifications.

Fast-AT: Fast Automatic Thumbnail Generation using Deep Neural Networks

  Fast-AT is an automatic thumbnail generation system based on deep neuralnetworks. It is a fully-convolutional deep neural network, which learnsspecific filters for thumbnails of different sizes and aspect ratios. Duringinference, the appropriate filter is selected depending on the dimensions ofthe target thumbnail. Unlike most previous work, Fast-AT does not utilizesaliency but addresses the problem directly. In addition, it eliminates theneed to conduct region search on the saliency map. The model generalizes tothumbnails of different sizes including those with extreme aspect ratios andcan generate thumbnails in real time. A data set of more than 70,000 thumbnailannotations was collected to train Fast-AT. We show competitive results incomparison to existing techniques.

Weakly-Supervised Spatial Context Networks

  We explore the power of spatial context as a self-supervisory signal forlearning visual representations. In particular, we propose spatial contextnetworks that learn to predict a representation of one image patch from anotherimage patch, within the same image, conditioned on their real-valued relativespatial offset. Unlike auto-encoders, that aim to encode and reconstructoriginal image patches, our network aims to encode and reconstruct intermediaterepresentations of the spatially offset patches. As such, the network learns aspatially conditioned contextual representation. By testing performance withvarious patch selection mechanisms we show that focusing on object-centricpatches is important, and that using object proposal as a patch selectionmechanism leads to the highest improvement in performance. Further, unlikeauto-encoders, context encoders [21], or other forms of unsupervised featurelearning, we illustrate that contextual supervision (with pre-trained modelinitialization) can improve on existing pre-trained model performance. We buildour spatial context networks on top of standard VGG_19 and CNN_M architecturesand, among other things, show that we can achieve improvements (with noadditional explicit supervision) over the original ImageNet pre-trained VGG_19and CNN_M models in object categorization and detection on VOC2007.

Visual Relationship Detection with Internal and External Linguistic  Knowledge Distillation

  Understanding visual relationships involves identifying the subject, theobject, and a predicate relating them. We leverage the strong correlationsbetween the predicate and the (subj,obj) pair (both semantically and spatially)to predict the predicates conditioned on the subjects and the objects. Modelingthe three entities jointly more accurately reflects their relationships, butcomplicates learning since the semantic space of visual relationships is hugeand the training data is limited, especially for the long-tail relationshipsthat have few instances. To overcome this, we use knowledge of linguisticstatistics to regularize visual model learning. We obtain linguistic knowledgeby mining from both training annotations (internal knowledge) and publiclyavailable text, e.g., Wikipedia (external knowledge), computing the conditionalprobability distribution of a predicate given a (subj,obj) pair. Then, wedistill the knowledge into a deep model to achieve better generalization. Ourexperimental results on the Visual Relationship Detection (VRD) and VisualGenome datasets suggest that with this linguistic knowledge distillation, ourmodel outperforms the state-of-the-art methods significantly, especially whenpredicting unseen relationships (e.g., recall improved from 8.45% to 19.17% onVRD zero-shot testing set).

Automatic Spatially-aware Fashion Concept Discovery

  This paper proposes an automatic spatially-aware concept discovery approachusing weakly labeled image-text data from shopping websites. We first fine-tuneGoogleNet by jointly modeling clothing images and their correspondingdescriptions in a visual-semantic embedding space. Then, for each attribute(word), we generate its spatially-aware representation by combining itssemantic word vector representation with its spatial representation derivedfrom the convolutional maps of the fine-tuned network. The resultingspatially-aware representations are further used to cluster attributes intomultiple groups to form spatially-aware concepts (e.g., the neckline conceptmight consist of attributes like v-neck, round-neck, etc). Finally, wedecompose the visual-semantic embedding space into multiple concept-specificsubspaces, which facilitates structured browsing and attribute-feedback productretrieval by exploiting multimodal linguistic regularities. We conductedextensive experiments on our newly collected Fashion200K dataset, and resultson clustering quality evaluation and attribute-feedback product retrieval taskdemonstrate the effectiveness of our automatically discovered spatially-awareconcepts.

Temporal Context Network for Activity Localization in Videos

  We present a Temporal Context Network (TCN) for precise temporal localizationof human activities. Similar to the Faster-RCNN architecture, proposals areplaced at equal intervals in a video which span multiple temporal scales. Wepropose a novel representation for ranking these proposals. Since poolingfeatures only inside a segment is not sufficient to predict activityboundaries, we construct a representation which explicitly captures contextaround a proposal for ranking it. For each temporal segment inside a proposal,features are uniformly sampled at a pair of scales and are input to a temporalconvolutional neural network for classification. After ranking proposals,non-maximum suppression is applied and classification is performed to obtainfinal detections. TCN outperforms state-of-the-art methods on the ActivityNetdataset and the THUMOS14 dataset.

On Encoding Temporal Evolution for Real-time Action Prediction

  Anticipating future actions is a key component of intelligence, specificallywhen it applies to real-time systems, such as robots or autonomous cars. Whilerecent works have addressed prediction of raw RGB pixel values, we focus onanticipating the motion evolution in future video frames. To this end, weconstruct dynamic images (DIs) by summarising moving pixels through a sequenceof future frames. We train a convolutional LSTMs to predict the next DIs basedon an unsupervised learning process, and then recognise the activity associatedwith the predicted DI. We demonstrate the effectiveness of our approach on 3benchmark action datasets showing that despite running on videos with complexactivities, our approach is able to anticipate the next human action with highaccuracy and obtain better results than the state-of-the-art methods.

Dynamic Zoom-in Network for Fast Object Detection in Large Images

  We introduce a generic framework that reduces the computational cost ofobject detection while retaining accuracy for scenarios where objects withvaried sizes appear in high resolution images. Detection progresses in acoarse-to-fine manner, first on a down-sampled version of the image and then ona sequence of higher resolution regions identified as likely to improve thedetection accuracy. Built upon reinforcement learning, our approach consists ofa model (R-net) that uses coarse detection results to predict the potentialaccuracy gain for analyzing a region at a higher resolution and another model(Q-net) that sequentially selects regions to zoom in. Experiments on theCaltech Pedestrians dataset show that our approach reduces the number ofprocessed pixels by over 50% without a drop in detection accuracy. The meritsof our approach become more significant on a high resolution test set collectedfrom YFCC100M dataset, where our approach maintains high detection performancewhile reducing the number of processed pixels by about 70% and the detectiontime by over 50%.

C-WSL: Count-guided Weakly Supervised Localization

  We introduce count-guided weakly supervised localization (C-WSL), an approachthat uses per-class object count as a new form of supervision to improve weaklysupervised localization (WSL). C-WSL uses a simple count-based region selectionalgorithm to select high-quality regions, each of which covers a single objectinstance during training, and improves existing WSL methods by training withthe selected regions. To demonstrate the effectiveness of C-WSL, we integrateit into two WSL architectures and conduct extensive experiments on VOC2007 andVOC2012. Experimental results show that C-WSL leads to large improvements inWSL and that the proposed approach significantly outperforms thestate-of-the-art methods. The results of annotation experiments on VOC2007suggest that a modest extra time is needed to obtain per-class object countscompared to labeling only object categories in an image. Furthermore, we reducethe annotation time by more than $2\times$ and $38\times$ compared tocenter-click and bounding-box annotations.

An Analysis of Scale Invariance in Object Detection - SNIP

  An analysis of different techniques for recognizing and detecting objectsunder extreme scale variation is presented. Scale specific and scale invariantdesign of detectors are compared by training them with different configurationsof input data. By evaluating the performance of different network architecturesfor classifying small objects on ImageNet, we show that CNNs are not robust tochanges in scale. Based on this analysis, we propose to train and testdetectors on the same scales of an image-pyramid. Since small and large objectsare difficult to recognize at smaller and larger scales respectively, wepresent a novel training scheme called Scale Normalization for Image Pyramids(SNIP) which selectively back-propagates the gradients of object instances ofdifferent sizes as a function of the image scale. On the COCO dataset, oursingle model performance is 45.7% and an ensemble of 3 networks obtains an mAPof 48.3%. We use off-the-shelf ImageNet-1000 pre-trained models and only trainwith bounding box supervision. Our submission won the Best Student Entry in theCOCO 2017 challenge. Code will be made available at\url{http://bit.ly/2yXVg4c}.

VITON: An Image-based Virtual Try-on Network

  We present an image-based VIirtual Try-On Network (VITON) without using 3Dinformation in any form, which seamlessly transfers a desired clothing itemonto the corresponding region of a person using a coarse-to-fine strategy.Conditioned upon a new clothing-agnostic yet descriptive person representation,our framework first generates a coarse synthesized image with the targetclothing item overlaid on that same person in the same pose. We further enhancethe initial blurry clothing area with a refinement network. The network istrained to learn how much detail to utilize from the target clothing item, andwhere to apply to the person in order to synthesize a photo-realistic image inwhich the target item deforms naturally with clear visual patterns. Experimentson our newly collected Zalando dataset demonstrate its promise in theimage-based virtual try-on task over state-of-the-art generative models.

R-FCN-3000 at 30fps: Decoupling Detection and Classification

  We present R-FCN-3000, a large-scale real-time object detector in whichobjectness detection and classification are decoupled. To obtain the detectionscore for an RoI, we multiply the objectness score with the fine-grainedclassification score. Our approach is a modification of the R-FCN architecturein which position-sensitive filters are shared across different object classesfor performing localization. For fine-grained classification, theseposition-sensitive filters are not needed. R-FCN-3000 obtains an mAP of 34.9%on the ImageNet detection dataset and outperforms YOLO-9000 by 18% whileprocessing 30 images per second. We also show that the objectness learned byR-FCN-3000 generalizes to novel classes and the performance increases with thenumber of training object classes - supporting the hypothesis that it ispossible to learn a universal objectness detector. Code will be made available.

Layout-induced Video Representation for Recognizing Agent-in-Place  Actions

  We address the recognition of agent-in-place actions, which are associatedwith agents who perform them and places where they occur, in the context ofoutdoor home surveillance. We introduce a representation of the geometry andtopology of scene layouts so that a network can generalize from the layoutsobserved in the training set to unseen layouts in the test set. ThisLayout-Induced Video Representation (LIVR) abstracts away low-level appearancevariance and encodes geometric and topological relationships of places in aspecific scene layout. LIVR partitions the semantic features of a video clipinto different places to force the network to learn place-based featuredescriptions; to predict the confidence of each action, LIVR aggregatesfeatures from the place associated with an action and its adjacent places onthe scene layout. We introduce the Agent-in-Place Action dataset to show thatour method allows neural network models to generalize significantly better tounseen scenes.

Stacked U-Nets: A No-Frills Approach to Natural Image Segmentation

  Many imaging tasks require global information about all pixels in an image.Conventional bottom-up classification networks globalize information bydecreasing resolution; features are pooled and downsampled into a singleoutput. But for semantic segmentation and object detection tasks, a networkmust provide higher-resolution pixel-level outputs. To globalize informationwhile preserving resolution, many researchers propose the inclusion ofsophisticated auxiliary blocks, but these come at the cost of a considerableincrease in network size and computational cost. This paper proposes stackedu-nets (SUNets), which iteratively combine features from different resolutionscales while maintaining resolution. SUNets leverage the informationglobalization power of u-nets in a deeper network architectures that is capableof handling the complexity of natural images. SUNets perform extremely well onsemantic segmentation tasks using a small number of parameters.

Learning Rich Features for Image Manipulation Detection

  Image manipulation detection is different from traditional semantic objectdetection because it pays more attention to tampering artifacts than to imagecontent, which suggests that richer features need to be learned. We propose atwo-stream Faster R-CNN network and train it endto- end to detect the tamperedregions given a manipulated image. One of the two streams is an RGB streamwhose purpose is to extract features from the RGB image input to find tamperingartifacts like strong contrast difference, unnatural tampered boundaries, andso on. The other is a noise stream that leverages the noise features extractedfrom a steganalysis rich model filter layer to discover the noise inconsistencybetween authentic and tampered regions. We then fuse features from the twostreams through a bilinear pooling layer to further incorporate spatialco-occurrence of these two modalities. Experiments on four standard imagemanipulation datasets demonstrate that our two-stream framework outperformseach individual stream, and also achieves state-of-the-art performance comparedto alternative methods with robustness to resizing and compression.

Soft Sampling for Robust Object Detection

  We study the robustness of object detection under the presence of missingannotations. In this setting, the unlabeled object instances will be treated asbackground, which will generate an incorrect training signal for the detector.Interestingly, we observe that after dropping 30% of the annotations (andlabeling them as background), the performance of CNN-based object detectorslike Faster-RCNN only drops by 5% on the PASCAL VOC dataset. We provide adetailed explanation for this result. To further bridge the performance gap, wepropose a simple yet effective solution, called Soft Sampling. Soft Samplingre-weights the gradients of RoIs as a function of overlap with positiveinstances. This ensures that the uncertain background regions are given asmaller weight compared to the hardnegatives. Extensive experiments on curatedPASCAL VOC datasets demonstrate the effectiveness of the proposed Soft Samplingmethod at different annotation drop rates. Finally, we show that onOpenImagesV3, which is a real-world dataset with missing annotations, SoftSampling outperforms standard detection baselines by over 3%.

Improving Annotation for 3D Pose Dataset of Fine-Grained Object  Categories

  Existing 3D pose datasets of object categories are limited to generic objecttypes and lack of fine-grained information. In this work, we introduce a newlarge-scale dataset that consists of 409 fine-grained categories and 31,881images with accurate 3D pose annotation. Specifically, we augment threeexisting fine-grained object recognition datasets (StanfordCars, CompCars andFGVC-Aircraft) by finding a specific 3D model for each sub-category fromShapeNet and manually annotating each 2D image by adjusting a full set of 7continuous perspective parameters. Since the fine-grained shapes allow 3Dmodels to better fit the images, we further improve the annotation quality byinitializing from the human annotation and conducting local search of the poseparameters with the objective of maximizing the IoUs between the projected maskand the segmentation reference estimated from state-of-the-art deepConvolutional Neural Networks (CNNs). We provide full statistics of theannotations with qualitative and quantitative comparisons suggesting that ourdataset can be a complementary source for studying 3D pose estimation. Thedataset can be downloaded at http://users.umiacs.umd.edu/~wym/3dpose.html.

Temporal Recurrent Networks for Online Action Detection

  Most work on temporal action detection is formulated as an offline problem,in which the start and end times of actions are determined after the entirevideo is fully observed. However, important real-time applications includingsurveillance and driver assistance systems require identifying actions as soonas each video frame arrives, based only on current and historical observations.In this paper, we propose a novel framework, Temporal Recurrent Network (TRN),to model greater temporal context of a video frame by simultaneously performingonline action detection and anticipation of the immediate future. At eachmoment in time, our approach makes use of both accumulated historical evidenceand predicted future information to better recognize the action that iscurrently occurring, and integrates both of these into a unified end-to-endarchitecture. We evaluate our approach on two popular online action detectiondatasets, HDD and TVSeries, as well as another widely used dataset, THUMOS'14.The results show that TRN significantly outperforms the state-of-the-art.

Modeling Local Geometric Structure of 3D Point Clouds using Geo-CNN

  Recent advances in deep convolutional neural networks (CNNs) have motivatedresearchers to adapt CNNs to directly model points in 3D point clouds. Modelinglocal structure has been proven to be important for the success ofconvolutional architectures, and researchers exploited the modeling of localpoint sets in the feature extraction hierarchy. However, limited attention hasbeen paid to explicitly model the geometric structure amongst points in a localregion. To address this problem, we propose Geo-CNN, which applies a genericconvolution-like operation dubbed as GeoConv to each point and its localneighborhood. Local geometric relationships among points are captured whenextracting edge features between the center and its neighboring points. Wefirst decompose the edge feature extraction process onto three orthogonalbases, and then aggregate the extracted features based on the angles betweenthe edge vector and the bases. This encourages the network to preserve thegeometric structure in Euclidean space throughout the feature extractionhierarchy. GeoConv is a generic and efficient operation that can be easilyintegrated into 3D point cloud analysis pipelines for multiple applications. Weevaluate Geo-CNN on ModelNet40 and KITTI and achieve state-of-the-artperformance.

Explicit Bias Discovery in Visual Question Answering Models

  Researchers have observed that Visual Question Answering (VQA) models tend toanswer questions by learning statistical biases in the data. For example, theiranswer to the question "What is the color of the grass?" is usually "Green",whereas a question like "What is the title of the book?" cannot be answered byinferring statistical biases. It is of interest to the community to explicitlydiscover such biases, both for understanding the behavior of such models, andtowards debugging them. Our work address this problem. In a database, we storethe words of the question, answer and visual words corresponding to regions ofinterest in attention maps. By running simple rule mining algorithms on thisdatabase, we discover human-interpretable rules which give us unique insightinto the behavior of such models. Our results also show examples of unusualbehaviors learned by models in attempting VQA tasks.

Generate, Segment and Replace: Towards Generic Manipulation Segmentation

  It has been witnessed an emerging demand for image manipulation segmentationto distinguish between fake images produced by advanced photo editing softwareand authentic ones. In this paper, we describe an approach based on semanticsegmentation for detecting image manipulation. The approach consists of threestages. A generation stage generates hard manipulated images from authenticimages using a Generative Adversarial Network (GAN) based model by cutting aregion out of a training sample, pasting it into an authentic image and thenpassing the image through a GAN to generate harder true positive tamperedregion. A segmentation stage and a replacement stage, sharing weights with eachother, then collaboratively construct dense predictions of tampered regions. Weachieve state-of-the-art performance on four public image manipulationdetection benchmarks while maintaining robustness to various attacks.

Universal Adversarial Training

  Standard adversarial attacks change the predicted class label of an image byadding specially tailored small perturbations to its pixels. In contrast, auniversal perturbation is an update that can be added to any image in a broadclass of images, while still changing the predicted class label. We study theefficient generation of universal adversarial perturbations, and also efficientmethods for hardening networks to these attacks. We propose a simpleoptimization-based universal attack that reduces the top-1 accuracy of variousnetwork architectures on ImageNet to less than 20%, while learning theuniversal perturbation 13X faster than the standard method. To defend againstthese perturbations, we propose universal adversarial training, which modelsthe problem of robust classifier generation as a two-player min-max game. Thismethod is much faster and more scalable than conventional adversarial trainingwith a strong adversary (PGD), and yet yields models that are extremelyresistant to universal attacks, and comparably resistant to standard(per-instance) black box attacks. We also discover a rather fascinatingside-effect of universal adversarial training: attacks built for universallyrobust models transfer better to other (black box) models than those built withconventional adversarial training.

AdaFrame: Adaptive Frame Selection for Fast Video Recognition

  We present AdaFrame, a framework that adaptively selects relevant frames on aper-input basis for fast video recognition. AdaFrame contains a Long Short-TermMemory network augmented with a global memory that provides context informationfor searching which frames to use over time. Trained with policy gradientmethods, AdaFrame generates a prediction, determines which frame to observenext, and computes the utility, i.e., expected future rewards, of seeing moreframes at each time step. At testing time, AdaFrame exploits predictedutilities to achieve adaptive lookahead inference such that the overallcomputational costs are reduced without incurring a decrease in accuracy.Extensive experiments are conducted on two large-scale video benchmarks, FCVIDand ActivityNet. AdaFrame matches the performance of using all frames with only8.21 and 8.65 frames on FCVID and ActivityNet, respectively. We furtherqualitatively demonstrate learned frame usage can indicate the difficulty ofmaking classification decisions; easier samples need fewer frames while harderones require more, both at instance-level within the same class and atclass-level among different categories.

MAN: Moment Alignment Network for Natural Language Moment Retrieval via  Iterative Graph Adjustment

  This research strives for natural language moment retrieval in long,untrimmed video streams. The problem nevertheless is not trivial especiallywhen a video contains multiple moments of interests and the language describescomplex temporal dependencies, which often happens in real scenarios. Weidentify two crucial challenges: semantic misalignment and structuralmisalignment. However, existing approaches treat different moments separatelyand do not explicitly model complex moment-wise temporal relations. In thispaper, we present Moment Alignment Network (MAN), a novel framework thatunifies the candidate moment encoding and temporal structural reasoning in asingle-shot feed-forward network. MAN naturally assigns candidate momentrepresentations aligned with language semantics over different temporallocations and scales. Most importantly, we propose to explicitly modelmoment-wise temporal relations as a structured graph and devise an iterativegraph adjustment network to jointly learn the best structure in an end-to-endmanner. We evaluate the proposed approach on two challenging public benchmarksCharades-STA and DiDeMo, where our MAN significantly outperforms thestate-of-the-art by a large margin.

FA-RPN: Floating Region Proposals for Face Detection

  We propose a novel approach for generating region proposals for performingface-detection. Instead of classifying anchor boxes using features from a pixelin the convolutional feature map, we adopt a pooling-based approach forgenerating region proposals. However, pooling hundreds of thousands of anchorswhich are evaluated for generating proposals becomes a computational bottleneckduring inference. To this end, an efficient anchor placement strategy forreducing the number of anchor-boxes is proposed. We then show that proposalsgenerated by our network (Floating Anchor Region Proposal Network, FA-RPN) arebetter than RPN for generating region proposals for face detection. We discussseveral beneficial features of FA-RPN proposals like iterative refinement,placement of fractional anchors and changing anchors which can be enabledwithout making any changes to the trained model. Our face detector based onFA-RPN obtains 89.4% mAP with a ResNet-50 backbone on the WIDER dataset.

TAN: Temporal Aggregation Network for Dense Multi-label Action  Recognition

  We present Temporal Aggregation Network (TAN) which decomposes 3Dconvolutions into spatial and temporal aggregation blocks. By stacking spatialand temporal convolutions repeatedly, TAN forms a deep hierarchicalrepresentation for capturing spatio-temporal information in videos. Since we donot apply 3D convolutions in each layer but only apply temporal aggregationblocks once after each spatial downsampling layer in the network, wesignificantly reduce the model complexity. The use of dilated convolutions atdifferent resolutions of the network helps in aggregating multi-scalespatio-temporal information efficiently. Experiments show that our model iswell suited for dense multi-label action recognition, which is a challengingsub-topic of action recognition that requires predicting multiple action labelsin each frame. We outperform state-of-the-art methods by 5% and 3% on theCharades and Multi-THUMOS dataset respectively.

Compatible and Diverse Fashion Image Inpainting

  Visual compatibility is critical for fashion analysis, yet is missing inexisting fashion image synthesis systems. In this paper, we propose toexplicitly model visual compatibility through fashion image inpainting. To thisend, we present Fashion Inpainting Networks (FiNet), a two-stage image-to-imagegeneration framework that is able to perform compatible and diverse inpainting.Disentangling the generation of shape and appearance to ensure photorealisticresults, our framework consists of a shape generation network and an appearancegeneration network. More importantly, for each generation network, we introducetwo encoders interacting with one another to learn latent code in a sharedcompatibility space. The latent representations are jointly optimized with thecorresponding generation network to condition the synthesis process,encouraging a diverse set of generated results that are visually compatiblewith existing fashion garments. In addition, our framework is readily extendedto clothing reconstruction and fashion transfer, with impressive results.Extensive experiments with comparisons with state-of-the-art approaches onfashion synthesis task quantitatively and qualitatively demonstrate theeffectiveness of our method.

StartNet: Online Detection of Action Start in Untrimmed Videos

  We propose StartNet to address Online Detection of Action Start (ODAS) whereaction starts and their associated categories are detected in untrimmed,streaming videos. Previous methods aim to localize action starts by learningfeature representations that can directly separate the start point from itspreceding background. It is challenging due to the subtle appearance differencenear the action starts and the lack of training data. Instead, StartNetdecomposes ODAS into two stages: action classification (using ClsNet) and startpoint localization (using LocNet). ClsNet focuses on per-frame labeling andpredicts action score distributions online. Based on the predicted actionscores of the past and current frames, LocNet conducts class-agnostic startdetection by optimizing long-term localization rewards using policy gradientmethods. The proposed framework is validated on two large-scale datasets,THUMOS'14 and ActivityNet. The experimental results show that StartNetsignificantly outperforms the state-of-the-art by 15%-30% p-mAP under theoffset tolerance of 1-10 seconds on THUMOS'14, and achieves comparableperformance on ActivityNet with 10 times smaller time offset.

M2KD: Multi-model and Multi-level Knowledge Distillation for Incremental  Learning

  Incremental learning targets at achieving good performance on new categorieswithout forgetting old ones. Knowledge distillation has been shown critical inpreserving the performance on old classes. Conventional methods, however,sequentially distill knowledge only from the last model, leading to performancedegradation on the old classes in later incremental learning steps. In thispaper, we propose a multi-model and multi-level knowledge distillationstrategy. Instead of sequentially distilling knowledge only from the lastmodel, we directly leverage all previous model snapshots. In addition, weincorporate an auxiliary distillation to further preserve knowledge encoded atthe intermediate feature levels. To make the model more memory efficient, weadapt mask based pruning to reconstruct all previous models with a small memoryfootprint. Experiments on standard incremental learning benchmarks show thatour method preserves the knowledge on old classes better and improves theoverall performance over standard distillation techniques.

Referring to Objects in Videos using Spatio-Temporal Identifying  Descriptions

  This paper presents a new task, the grounding of spatio-temporal identifyingdescriptions in videos. Previous work suggests potential bias in existingdatasets and emphasizes the need for a new data creation schema to better modellinguistic structure. We introduce a new data collection scheme based ongrammatical constraints for surface realization to enable us to investigate theproblem of grounding spatio-temporal identifying descriptions in videos. Wethen propose a two-stream modular attention network that learns and groundsspatio-temporal identifying descriptions based on appearance and motion. Weshow that motion modules help to ground motion-related words and also help tolearn in appearance modules because modular neural networks resolve taskinterference between modules. Finally, we propose a future challenge and a needfor a robust system arising from replacing ground truth visual annotations withautomatic video object detector and temporal event localization.

Deception Detection in Videos

  We present a system for covert automated deception detection in real-lifecourtroom trial videos. We study the importance of different modalities likevision, audio and text for this task. On the vision side, our system usesclassifiers trained on low level video features which predict humanmicro-expressions. We show that predictions of high-level micro-expressions canbe used as features for deception prediction. Surprisingly, IDT (Improved DenseTrajectory) features which have been widely used for action recognition, arealso very good at predicting deception in videos. We fuse the score ofclassifiers trained on IDT features and high-level micro-expressions to improveperformance. MFCC (Mel-frequency Cepstral Coefficients) features from the audiodomain also provide a significant boost in performance, while information fromtranscripts is not very beneficial for our system. Using various classifiers,our automated system obtains an AUC of 0.877 (10-fold cross-validation) whenevaluated on subjects which were not part of the training set. Even thoughstate-of-the-art methods use human annotations of micro-expressions fordeception detection, our fully automated approach outperforms them by 5%. Whencombined with human annotations of micro-expressions, our AUC improves to0.922. We also present results of a user-study to analyze how well do averagehumans perform on this task, what modalities they use for deception detectionand how they perform if only one modality is accessible. Our project page canbe found at \url{https://doubaibai.github.io/DARE/}.

Multi-Directional Multi-Level Dual-Cross Patterns for Robust Face  Recognition

  To perform unconstrained face recognition robust to variations inillumination, pose and expression, this paper presents a new scheme to extract"Multi-Directional Multi-Level Dual-Cross Patterns" (MDML-DCPs) from faceimages. Specifically, the MDMLDCPs scheme exploits the first derivative ofGaussian operator to reduce the impact of differences in illumination and thencomputes the DCP feature at both the holistic and component levels. DCP is anovel face image descriptor inspired by the unique textural structure of humanfaces. It is computationally efficient and only doubles the cost of computinglocal binary patterns, yet is extremely robust to pose and expressionvariations. MDML-DCPs comprehensively yet efficiently encodes the invariantcharacteristics of a face image from multiple levels into patterns that arehighly discriminative of inter-personal differences but robust tointra-personal variations. Experimental results on the FERET, CAS-PERL-R1, FRGC2.0, and LFW databases indicate that DCP outperforms the state-of-the-art localdescriptors (e.g. LBP, LTP, LPQ, POEM, tLBP, and LGXP) for both faceidentification and face verification tasks. More impressively, the bestperformance is achieved on the challenging LFW and FRGC 2.0 databases bydeploying MDML-DCPs in a simple recognition scheme.

