Determining the implied volatility in the Dupire equation for vanilla  European call options

  The Black-Scholes model gives vanilla Europen call option prices as afunction of the volatility. We prove Lipschitz stability in the inverse problemof determining the implied volatility, which is a function of the underlyingasset, from a collection of quoted option prices with different strikes.

Stable reconstruction of the volatility in a regime-switching local  volatility model

  Prices of European call options in a regime-switching local volatility modelcan be computed by solving a parabolic system which generalises the classicalBlack and Scholes equation, giving these prices as functionals of the localvolatilities. We prove Lipschitz stability for the inverse problem ofdetermining the local volatilities from quoted call option prices for a rangeof strikes, if the calls are indexed by the different states of the continuousMarkov chain which governs the regime switches.

Lead, Follow, or Go Your Own Way: Empirical Evidence Against  Leader-Follower Behavior in Electronic Markets

  Low search costs in Internet markets can be used by consumers to find lowprices, but can also be used by retailers to monitor competitors' prices. Thisprice monitoring can lead to price matching, resulting in dampened pricecompetition and higher prices in some cases. This paper analyzes price data for316 bestselling, computer, and random book titles gathered from 32 retailersbetween August 1999 and January 2000. In contrast to previous studies we findno evidence of leader-follow behavior for the vast majority of retailers westudy. Further, the few cases of leader-follow behavior we observe seem to beassociated with managerial convenience as opposed to anti-competitive behavior.We offer a methodology that can be used by future academic researchers orgovernment regulators to check for anti-competitive price matching behavior infuture time periods or in additional product categories.

Self-Confirming Price Prediction Strategies for Simultaneous One-Shot  Auctions

  Bidding in simultaneous auctions is challenging because an agent's value fora good in one auction may depend on the uncertain outcome of other auctions:the so-called exposure problem. Given the gap in understanding of generalsimultaneous auction games, previous works have tackled this problem withheuristic strategies that employ probabilistic price predictions. We define aconcept of self-confirming prices, and show that within an independent privatevalue model, Bayes-Nash equilibrium can be fully characterized as a profile ofoptimal price prediction strategies with self-confirming predictions. Weexhibit practical procedures to compute approximately optimal bids given aprobabilistic price prediction, and near self-confirming price predictionsgiven a price-prediction strategy. An extensive empirical game-theoreticanalysis demonstrates that self-confirming price prediction strategies areeffective in simultaneous auction games with both complementary andsubstitutable preference structures.

Price and Quantity Trajectories: Second-order Dynamics

  In two previous papers the author developed a second-order price adjustment(t\^atonnement) process. This paper extends the approach to include bothquantity and price adjustments. We demonstrate three results: a analogue tophysical energy, called "activity" arises naturally in the model, and is notconserved in general; price and quantity trajectories must either end at alocal minimum of a scalar potential or circulate endlessly; and disturbancesinto a subspace of substitutable commodities decay over time. From this weargue, although we do not prove, that the model features global stability,combined with local instability, a characteristic of many real markets.Following these observations and a brief survey of empirical results forprice-setting and consumption behavior in markets for "real" goods (as opposedto financial markets), we conjecture that Stigler and Becker's well-knowntheory of consumer preference opens the possibility of substantial degeneracyin commodity space, and therefore that price and quantity trajectories couldlie on a relatively low-dimensional subspace within the full commodity space.

Price Jump Prediction in Limit Order Book

  A limit order book provides information on available limit order prices andtheir volumes. Based on these quantities, we give an empirical result on therelationship between the bid-ask liquidity balance and trade sign and we showthat liquidity balance on best bid/best ask is quite informative for predictingthe future market order's direction. Moreover, we define price jump as a sell(buy) market order arrival which is executed at a price which is smaller(larger) than the best bid (best ask) price at the moment just after theprecedent market order arrival. Features are then extracted related to limitorder volumes, limit order price gaps, market order information and limit orderevent information. Logistic regression is applied to predict the price jumpfrom the limit order book's feature. LASSO logistic regression is introduced tohelp us make variable selection from which we are capable to highlight theimportance of different features in predicting the future price jump. In orderto get rid of the intraday data seasonality, the analysis is based on twoseparated datasets: morning dataset and afternoon dataset. Based on an analysison forty largest French stocks of CAC40, we find that trade sign and marketorder size as well as the liquidity on the best bid (best ask) are consistentlyinformative for predicting the incoming price jump.

Second-order Price Dynamics: Approach to Equilibrium with Perpetual  Arbitrage

  The notion that economies should normally be in equilibrium is by nowwell-established; equally well-established is that economies are almost neverprecisely in equilibrium. Using a very general formulation, we show that underdynamics that are second-order in time a price system can remain away fromequilibrium with permanent and repeating opportunities for arbitrage, even whena damping term drives the system towards equilibrium. We also argue thatsecond-order dynamic equations emerge naturally when there are heterogeneouseconomic actors, some behaving as active and knowledgeable arbitrageurs, andothers using heuristics. The essential mechanism is that active arbitrageursare able to repeatedly benefit from the suboptimal heuristics that govern mosteconomic behavior.

Optimal Lower Bound for Itemset Frequency Indicator Sketches

  Given a database, a common problem is to find the pairs or $k$-tuples ofitems that frequently co-occur. One specific problem is to create a small space"sketch" of the data that records which $k$-tuples appear in more than an$\epsilon$ fraction of rows of the database.  We improve the lower bound of Liberty, Mitzenmacher, and Thaler [LMT14],showing that $\Omega(\frac{1}{\epsilon}d \log (\epsilon d))$ bits are necessaryeven in the case of $k=2$. This matches the sampling upper bound for all$\epsilon \geq 1/d^{.99}$, and (in the case of $k=2$) another trivial upperbound for $\epsilon = 1/d$.

VCG Payments for Portfolio Allocations in Online Advertising

  Some online advertising offers pay only when an ad elicits a response.Randomness and uncertainty about response rates make showing those ads a riskyinvestment for online publishers. Like financial investors, publishers can useportfolio allocation over multiple advertising offers to pursue revenue whilecontrolling risk. Allocations over multiple offers do not have a distinctwinner and runner-up, so the usual second-price mechanism does not apply. Thispaper develops a pricing mechanism for portfolio allocations. The mechanism isefficient, truthful, and rewards offers that reduce risk.

A Merton-Like Approach to Pricing Debt based on a non-Gaussian Asset  Model

  This paper is a contribution to the Proceedings of the Workshop  Complexity, Metastability and Nonextensivity held in Erice 20-26 July 2004,to be published by World Scientific. We propose a generalization to Merton'smodel for evaluating credit spreads. In his original work, a company's assetswere assumed to follow a log-normal process. We introduce fat tails and skewinto this model, along the same lines as in the option pricing model of Borlandand Bouchaud (2004, Quantitative Finance 4) and illustrate the effects of eachcomponent. Preliminary empirical results indicate that this model fits well toempirically observed credit spreads with a parameterization that also matchedobserved stock return distributions and option prices.

A quantitative model of trading and price formation in financial markets

  We use standard physics techniques to model trading and price formation in amarket under the assumption that order arrival and cancellations are Poissonrandom processes. This model makes testable predictions for the most basicproperties of a market, such as the diffusion rate of prices, which is thestandard measure of financial risk, and the spread and price impact functions,which are the main determinants of transaction cost. Guided by dimensionalanalysis, simulation, and mean field theory, we find scaling relations in termsof order flow rates. We show that even under completely random order flow theneed to store supply and demand to facilitate trading induces anomalousdiffusion and temporal structure in prices.

Bayesian Budget Feasibility with Posted Pricing

  We consider the problem of budget feasible mechanism design proposed bySinger (2010), but in a Bayesian setting. A principal has a public value forhiring a subset of the agents and a budget, while the agents have private costsfor being hired. We consider both additive and submodular value functions ofthe principal. We show that there are simple, practical, ex post budgetbalanced posted pricing mechanisms that approximate the value obtained by theBayesian optimal mechanism that is budget balanced only in expectation. A mainmotivating application for this work is the crowdsourcing large projects, e.g.,on Mechanical Turk, where workers are drawn from a large population and postedpricing is standard. Our analysis methods relate to contention resolutionschemes in submodular optimization of Vondrak et al. (2011) and the correlationgap analysis of Yan (2011).

Quantifying Volatility Reduction in German Day-ahead Spot Market in the  Period 2006 through 2016

  In Europe, Germany is taking the lead in the switch from the conventional torenewable energy. This poses new challenges as wind and solar energy arefundamentally intermittent, weather-dependent and less predictable. It istherefore of considerable interest to investigate the evolution of pricevolatility in this post-transition era. There are a number of reasons, however,that makes the practical studies difficult. For instance, EPEX prices can bezero or negative. Consequently, the standard approach in financial time seriesanalysis to switch to logarithmic measures is inapplicable. Furthermore, incontrast to the stock market prices which are only available for trading days,EPEX prices cover the whole year, including weekends and holidays. Accordingly,there is a lot of underlying variability in the data which has nothing to dowith volatility, but simply reflects diurnal activity patterns. An importantdistinction of the present work is the application of matrix decompositiontechniques, namely the singular value decomposition (SVD), for defining analternative notion of volatility. This approach is systematically more robusttoward outliers and also the diurnal patterns. Our observations show that theday-ahead market is becoming less volatile in recent years.

Cumulative Prospect Theory Based Dynamic Pricing for Shared Mobility on  Demand Services

  Cumulative Prospect Theory (CPT) is a modeling tool widely used in behavioraleconomics and cognitive psychology that captures subjective decision making ofindividuals under risk or uncertainty. In this paper, we propose a dynamicpricing strategy for Shared Mobility on Demand Services (SMoDS) using abehavioral model based on CPT. This dynamic pricing strategy together withdynamic routing via an optimization algorithm that we have developed earlier,provide a complete solution customized for SMoDS of multi-passenger transport.The basic principles of CPT and the derivation of the passenger behavioralmodel in the SMoDS context are described in detail. The implications of CPT ondynamic pricing of the SMoDS are delineated using computational experimentsinvolving passenger preferences. These implications include interpretation ofthe classic fourfold pattern of risk attitudes, strong risk aversion over mixedprospects, and behavioral preferences of self reference. Overall, it is arguedthat the use of the CPT framework corresponds to a crucial building block indesigning socio-technical systems by allowing quantification of subjectivedecision making that is perceived to be otherwise qualitative.

Smart expansion and fast calibration for jump diffusion

  Using Malliavin calculus techniques, we derive an analytical formula for theprice of European options, for any model including local volatility and Poissonjump process. We show that the accuracy of the formula depends on thesmoothness of the payoff function. Our approach relies on an asymptoticexpansion related to small diffusion and small jump frequency/size. Our formulahas excellent accuracy (the error on implied Black-Scholes volatilities forcall option is smaller than 2 bp for various strikes and maturities).Additionally, model calibration becomes very rapid.

Non-Markovian Beables vs. Massive Parallelism

  A simple dynamical model over a discrete classical state space is presented.In a certain limit, it reduces to one in a class of models subsuming Bell'sfield-theoretic version of Bohmian mechanics. But it exhibits the massiveparallelism native to quantum mechanics only as an emergent phenomenon, incontrast with Bell's and other hidden variable theories. While still non-localin its dynamics, the model thus restores our ability to regard a system as acombination of separate, localized parts, at the price of admittingnon-Markovian dynamics.

Ensemble Validation: Selectivity has a Price, but Variety is Free

  Suppose some classifiers are selected from a set of hypothesis classifiers toform an equally-weighted ensemble that selects a member classifier at randomfor each input example. Then the ensemble has an error bound consisting of theaverage error bound for the member classifiers, a term for selectivity thatvaries from zero (if all hypothesis classifiers are selected) to a standarduniform error bound (if only a single classifier is selected), and smallconstants. There is no penalty for using a richer hypothesis set if the samefraction of the hypothesis classifiers are selected for the ensemble.

Radiative falloff of a scalar field in a weakly curved spacetime without  symmetries

  We consider a massless scalar field propagating in a weakly curved spacetimewhose metric is a solution to the linearized Einstein field equations. Thespacetime is assumed to be stationary and asymptotically flat, but no othersymmetries are imposed -- the spacetime can rotate and deviate strongly fromspherical symmetry. We prove that the late-time behavior of the scalar field isidentical to what it would be in a spherically-symmetric spacetime: it decaysin time according to an inverse power-law, with a power determined by theangular profile of the initial wave packet (Price falloff theorem). The field'slate-time dynamics is insensitive to the nonspherical aspects of the metric,and it is governed entirely by the spacetime's total gravitational mass; othermultipole moments, and in particular the spacetime's total angular momentum, donot enter in the description of the field's late-time behavior. This extendedformulation of Price's falloff theorem appears to be at odds with previousstudies of radiative decay in the spacetime of a Kerr black hole. We show,however, that the contradiction is only apparent, and that it is largely anartifact of the Boyer-Lindquist coordinates adopted in these studies.

Efficient Sketches for the Set Query Problem

  We develop an algorithm for estimating the values of a vector x in R^n over asupport S of size k from a randomized sparse binary linear sketch Ax of sizeO(k). Given Ax and S, we can recover x' with ||x' - x_S||_2 <= eps ||x -x_S||_2 with probability at least 1 - k^{-\Omega(1)}. The recovery takes O(k)time.  While interesting in its own right, this primitive also has a number ofapplications. For example, we can:  1. Improve the linear k-sparse recovery of heavy hitters in Zipfiandistributions with O(k log n) space from a (1+eps) approximation to a (1 +o(1)) approximation, giving the first such approximation in O(k log n) spacewhen k <= O(n^{1-eps}).  2. Recover block-sparse vectors with O(k) space and a (1+eps) approximation.Previous algorithms required either omega(k) space or omega(1) approximation.

K-Median Clustering, Model-Based Compressive Sensing, and Sparse  Recovery for Earth Mover Distance

  We initiate the study of sparse recovery problems under the Earth-MoverDistance (EMD). Specifically, we design a distribution over m x n matrices Asuch that for any x, given Ax, we can recover a k-sparse approximation to xunder the EMD distance. One construction yields m = O(k log(n/k)) and a 1 +epsilon approximation factor, which matches the best achievable bound for othererror measures, such as the L_1 norm. Our algorithms are obtained by exploitingnovel connections to other problems and areas, such as streaming algorithms fork-median clustering and model-based compressive sensing. We also provide novelalgorithms and results for the latter problems.

Lower Bounds for Sparse Recovery

  We consider the following k-sparse recovery problem: design an m x n matrixA, such that for any signal x, given Ax we can efficiently recover x'satisfying  ||x-x'||_1 <= C min_{k-sparse} x"} ||x-x"||_1.  It is known that there exist matrices A with this property that have only O(klog (n/k)) rows.  In this paper we show that this bound is tight. Our bound holds even for themore general /randomized/ version of the problem, where A is a random variableand the recovery algorithm is required to work for any fixed x with constantprobability (over A).

Nearly Optimal Sparse Fourier Transform

  We consider the problem of computing the k-sparse approximation to thediscrete Fourier transform of an n-dimensional signal. We show:  * An O(k log n)-time randomized algorithm for the case where the input signalhas at most k non-zero Fourier coefficients, and  * An O(k log n log(n/k))-time randomized algorithm for general input signals.  Both algorithms achieve o(n log n) time, and thus improve over the FastFourier Transform, for any k = o(n). They are the first known algorithms thatsatisfy this property. Also, if one assumes that the Fast Fourier Transform isoptimal, the algorithm for the exactly k-sparse case is optimal for any k =n^{\Omega(1)}.  We complement our algorithmic results by showing that any algorithm forcomputing the sparse Fourier transform of a general signal must use at least\Omega(k log(n/k)/ log log n) signal samples, even if it is allowed to performadaptive sampling.

Sample-Optimal Average-Case Sparse Fourier Transform in Two Dimensions

  We present the first sample-optimal sublinear time algorithms for the sparseDiscrete Fourier Transform over a two-dimensional sqrt{n} x sqrt{n} grid. Ouralgorithms are analyzed for /average case/ signals. For signals whose spectrumis exactly sparse, our algorithms use O(k) samples and run in O(k log k) time,where k is the expected sparsity of the signal. For signals whose spectrum isapproximately sparse, our algorithm uses O(k log n) samples and runs in O(klog^2 n) time; the latter algorithm works for k=Theta(sqrt{n}). The number ofsamples used by our algorithms matches the known lower bounds for therespective signal models.  By a known reduction, our algorithms give similar results for theone-dimensional sparse Discrete Fourier Transform when n is a power of a smallcomposite number (e.g., n = 6^t).

Binary Embedding: Fundamental Limits and Fast Algorithm

  Binary embedding is a nonlinear dimension reduction methodology where highdimensional data are embedded into the Hamming cube while preserving thestructure of the original space. Specifically, for an arbitrary $N$ distinctpoints in $\mathbb{S}^{p-1}$, our goal is to encode each point using$m$-dimensional binary strings such that we can reconstruct their geodesicdistance up to $\delta$ uniform distortion. Existing binary embeddingalgorithms either lack theoretical guarantees or suffer from running time$O\big(mp\big)$. We make three contributions: (1) we establish a lower boundthat shows any binary embedding oblivious to the set of points requires $m =\Omega(\frac{1}{\delta^2}\log{N})$ bits and a similar lower bound fornon-oblivious embeddings into Hamming distance; (2) [DELETED, see comment]; (3)we also provide an analytic result about embedding a general set of points $K\subseteq \mathbb{S}^{p-1}$ with even infinite size. Our theoretical findingsare supported through experiments on both synthetic and real data sets.

Trend without hiccups: a Kalman filter approach

  Have you ever felt miserable because of a sudden whipsaw in the price thattriggered an unfortunate trade? In an attempt to remove this noise, technicalanalysts have used various types of moving averages (simple, exponential,adaptive one or using Nyquist criterion). These tools may have performeddecently but we show in this paper that this can be improved dramaticallythanks to the optimal filtering theory of Kalman filters (KF). We explain thebasic concepts of KF and its optimum criterion. We provide a pseudo code forthis new technical indicator that demystifies its complexity. We show that thisnew smoothing device can be used to better forecast price moves as lag isreduced. We provide 4 Kalman filter models and their performance on the SP500mini-future contract. Results are quite illustrative of the efficiency of KFmodels with better net performance achieved by the KF model combining smoothingand extremum position.

Smooth solutions to portfolio liquidation problems under price-sensitive  market impact

  We consider the stochastic control problem of a financial trader that needsto unwind a large asset portfolio within a short period of time. The trader cansimultaneously submit active orders to a primary market and passive orders to adark pool. Our framework is flexible enough to allow for price-dependent impactfunctions describing the trading costs in the primary market andprice-dependent adverse selection costs associated with dark pool trading. Weprove that the value function can be characterized in terms of the uniquesmooth solution to a PDE with singular terminal value, establish its explicitasymptotic behavior at the terminal time, and give the optimal trading strategyin feedback form.

Computing large market equilibria using abstractions

  Computing market equilibria is an important practical problem for marketdesign (e.g. fair division, item allocation). However, computing equilibriarequires large amounts of information (e.g. all valuations for all buyers forall items) and compute power. We consider ameliorating these issues by applyinga method used for solving complex games: constructing a coarsened abstractionof a given market, solving for the equilibrium in the abstraction, and liftingthe prices and allocations back to the original market. We show how to boundimportant quantities such as regret, envy, Nash social welfare, Paretooptimality, and maximin share when the abstracted prices and allocations areused in place of the real equilibrium. We then study two abstraction methods ofinterest for practitioners: 1) filling in unknown valuations using techniquesfrom matrix completion, 2) reducing the problem size by aggregating groups ofbuyers/items into smaller numbers of representative buyers/items and solvingfor equilibrium in this coarsened market. We find that in real dataallocations/prices that are relatively close to equilibria can be computed fromeven very coarse abstractions.

fVSS: A New Secure and Cost-Efficient Scheme for Cloud Data Warehouses

  Cloud business intelligence is an increasingly popular choice to deliverdecision support capabilities via elastic, pay-per-use resources. However, datasecurity issues are one of the top concerns when dealing with sensitive data.In this pa-per, we propose a novel approach for securing cloud data warehousesby flexible verifiable secret sharing, fVSS. Secret sharing encrypts anddistributes data over several cloud ser-vice providers, thus enforcing dataprivacy and availability. fVSS addresses four shortcomings in existing secretsharing-based approaches. First, it allows refreshing the data ware-house whensome service providers fail. Second, it allows on-line analysis processing.Third, it enforces data integrity with the help of both inner and outersignatures. Fourth, it helps users control the cost of cloud warehousing bybalanc-ing the load among service providers with respect to their pricingpolicies. To illustrate fVSS' efficiency, we thoroughly compare it withexisting secret sharing-based approaches with respect to security features,querying power and data storage and computing costs.

Statistical theory of the continuous double auction

  Most modern financial markets use a continuous double auction mechanism tostore and match orders and facilitate trading. In this paper we develop amicroscopic dynamical statistical model for the continuous double auction underthe assumption of IID random order flow, and analyze it using simulation,dimensional analysis, and theoretical tools based on mean field approximations.The model makes testable predictions for basic properties of markets, such asprice volatility, the depth of stored supply and demand vs. price, the bid-askspread, the price impact function, and the time and probability of fillingorders. These predictions are based on properties of order flow and the limitorder book, such as share volume of market and limit orders, cancellations,typical order size, and tick size. Because these quantities can all be measureddirectly there are no free parameters. We show that the order size, which canbe cast as a nondimensional granularity parameter, is in most cases a moresignificant determinant of market behavior than tick size. We also provide anexplanation for the observed highly concave nature of the price impactfunction. On a broader level, this work suggests how stochastic models based onzero-intelligence agents may be useful to probe the structure of marketinstitutions. Like the model of perfect rationality, a stochastic-zerointelligence model can be used to make strong predictions based on a compactset of assumptions, even if these assumptions are not fully believable.

Symmetry restoration by pricing in a duopoly of perishable goods

  Competition is a main tenet of economics, and the reason is that a perfectlycompetitive equilibrium is Pareto-efficient in the absence of externalities andpublic goods. Whether a product is selected in a market crucially relates toits competitiveness, but the selection in turn affects the landscape ofcompetition. Such a feedback mechanism has been illustrated in a duopoly modelby Lambert et al., in which a buyer's satisfaction is updated depending on the{\em freshness} of a purchased product. The probability for buyer $n$ to selectseller $i$ is assumed to be $p_{n,i} \propto e^{ S_{n,i}/T}$, where $S_{n,i}$is the buyer's satisfaction and $T$ is an effective temperature to introducestochasticity. If $T$ decreases below a critical point $T_c$, the systemundergoes a transition from a symmetric phase to an asymmetric one, in whichonly one of the two sellers is selected. In this work, we extend the model byincorporating a simple price system. By considering a greed factor $g$ tocontrol how the satisfaction depends on the price, we argue the existence of anoscillatory phase in addition to the symmetric and asymmetric ones in the$(T,g)$ plane, and estimate the phase boundaries through mean-fieldapproximations. The analytic results show that the market preserves theinherent symmetry between the sellers for lower $T$ in the presence of theprice system, which is confirmed by our numerical simulations.

Pacing Equilibrium in First-Price Auction Markets

  In ad auctions--the prevalent monetization mechanism of Internetcompanies--advertisers compete for online impressions in a sequential auctionmarket. Since advertisers are typically budget-constrained, a common toolemployed to improve their ROI is that of pacing, i.e., uniform scaling of theirbids to preserve their budget for a longer duration. If the advertisers areexcessively paced, they end up not spending their budget, while if they are notsufficiently paced, they use up their budget too soon. Therefore, it isimportant that they are paced at just the right amount, a solution concept thatwe call a pacing equilibrium. In this paper, we study pacing equilibria in thecontext of first-price auctions, which are popular in the theory of admechanisms. We show existence, uniqueness, and efficient computability offirst-price pacing equilibria (FPPE), while also establishing several othersalient features of this solution concept. In the process, we uncover a sharpcontrast between these solutions and second price pacing equilibria (SPPE), thelatter being known to produce non-unique, fragile solutions that are alsocomputationally hard to obtain. Simulations show that FPPE have better revenueproperties than SPPE, that bidders have lower ex-post regret, and thatincentives to misreport budgets for thick markets are smaller.

Matters of Gravity, the newsletter of the APS Topical Group on  Gravitation

  Research Briefs:  Cosmic microwave background anisotropy experiments, by Sean Carroll  LISA Project Update by Bill Folkner  An update on the r-mode instability, by Nils Andersson  Laboratory experiments: news from MG9, by Riley Newman  Progress toward Commissioning the LIGO detectors, by Stan Whitcomb  160 Hours of Data Taken on TAMA300, by Seiji Kawamura  Conference reports:  Kipfest, by Richard Price  Third Capra meeting, by Eric Poisson  GR at the XIIIth Congress on Mathematical Physics, by Abhay Ashtekar  3rd International LISA Symposium, by Curt Cutler

Parallel Computing for QCD on a Pentium Cluster

  Motivated by the computational demands of our research and budgetaryconstraints which are common to many research institutions, we built a ``poorman's supercomputer'', a cluster of PC nodes which together can performparallel calculations at a fraction of the price of a commercial supercomputer.We describe the construction, cost, and performance of our cluster.

Light-Cone Gauge for 1+1 Strings

  Explicit construction of the light-cone gauge quantum theory of bosonicstrings in 1+1 spacetime dimensions reveals unexpected structures. One is theexistence of a gauge choice that gives a free action at the price ofpropagating ghosts and a nontrivial BRST charge. Fixing this gauge leaves aU(1) Kac-Moody algebra of residual symmetry, generated by a conformal tensor ofrank two and a conformal scalar. Another is that the BRST charge made fromthese currents is nilpotent when the action includes a linear dilatonbackground, independent of the particular value of the dilaton gradient.Spacetime Lorentz invariance in this theory is still elusive, however, becauseof the linear dilaton background and the nature of the gauge symmetries.

A probabilistic max-plus numerical method for solving stochastic control  problems

  We consider fully nonlinear Hamilton-Jacobi-Bellman equations associated todiffusion control problems involving a finite set-valued (or switching) controland possibly a continuum-valued control. We construct a lower complexityprobabilistic numerical algorithm by combining the idempotent expansionproperties obtained by McEneaney, Kaise and Han (2011) for solving suchproblems with a numerical probabilistic method such as the one proposed byFahim, Touzi and Warin (2011) for solving some fully nonlinear parabolicpartial differential equations. Numerical tests on a small example of pricingand hedging an option are presented.

Sequential Bayesian Learning for Merton's Jump Model with Stochastic  Volatility

  Jump stochastic volatility models are central to financial econometrics forvolatility forecasting, portfolio risk management, and derivatives pricing.Markov Chain Monte Carlo (MCMC) algorithms are computationally unfeasible forthe sequential learning of volatility state variables and parameters, wherebythe investor must update all posterior and predictive densities as newinformation arrives. We develop a particle filtering and learning algorithm tosample posterior distribution in Merton's jump stochastic volatility. Thisallows to filter spot volatilities and jump times, together with sequentiallyupdating (learning) of jump and volatility parameters. We illustrate ourmethodology on Google's stock return. We conclude with directions for futureresearch.

Robust polynomial regression up to the information theoretic limit

  We consider the problem of robust polynomial regression, where one receivessamples $(x_i, y_i)$ that are usually within $\sigma$ of a polynomial $y =p(x)$, but have a $\rho$ chance of being arbitrary adversarial outliers.Previously, it was known how to efficiently estimate $p$ only when $\rho <\frac{1}{\log d}$. We give an algorithm that works for the entire feasiblerange of $\rho < 1/2$, while simultaneously improving other parameters of theproblem. We complement our algorithm, which gives a factor 2 approximation,with impossibility results that show, for example, that a $1.09$ approximationis impossible even with infinitely many samples.

Stochastic Multi-armed Bandits in Constant Space

  We consider the stochastic bandit problem in the sublinear space setting,where one cannot record the win-loss record for all $K$ arms. We give analgorithm using $O(1)$ words of space with regret \[  \sum_{i=1}^{K}\frac{1}{\Delta_i}\log \frac{\Delta_i}{\Delta}\log T \] where$\Delta_i$ is the gap between the best arm and arm $i$ and $\Delta$ is the gapbetween the best and the second-best arms. If the rewards are bounded away from$0$ and $1$, this is within an $O(\log 1/\Delta)$ factor of the optimum regretpossible without space constraints.

The Supersingularity of Hurwitz Curves

  We study when Hurwitz curves are supersingular. Specifically, we show thatthe curve $H_{n,\ell}: X^nY^\ell + Y^nZ^\ell + Z^nX^\ell = 0$, with $n$ and$\ell$ relatively prime, is supersingular over the finite field$\mathbb{F}_{p}$ if and only if there exists an integer $i$ such that $p^i\equiv -1 \bmod (n^2 - n\ell + \ell^2)$. If this holds, we prove that it isalso true that the curve is maximal over $\mathbb{F}_{p^{2i}}$. Further, weprovide a complete table of supersingular Hurwitz curves of genus less than 5for characteristic less than 37.

On the Power of Adaptivity in Sparse Recovery

  The goal of (stable) sparse recovery is to recover a $k$-sparse approximation$x*$ of a vector $x$ from linear measurements of $x$. Specifically, the goal isto recover $x*$ such that ||x-x*||_p <= C min_{k-sparse x'} ||x-x'||_q for someconstant $C$ and norm parameters $p$ and $q$. It is known that, for $p=q=1$ or$p=q=2$, this task can be accomplished using $m=O(k \log (n/k))$ non-adaptivemeasurements [CRT06] and that this bound is tight [DIPW10,FPRU10,PW11].  In this paper we show that if one is allowed to perform measurements that areadaptive, then the number of measurements can be considerably reduced.Specifically, for $C=1+eps$ and $p=q=2$ we show - A scheme with $m=O((1/eps)klog log (n eps/k))$ measurements that uses $O(log* k \log \log (n eps/k))$rounds. This is a significant improvement over the best possible non-adaptivebound. - A scheme with $m=O((1/eps) k log (k/eps) + k \log (n/k))$ measurementsthat uses /two/ rounds. This improves over the best possible non-adaptivebound. To the best of our knowledge, these are the first results of this type.As an independent application, we show how to solve the problem of finding aduplicate in a data stream of $n$ items drawn from ${1, 2, ..., n-1}$ using$O(log n)$ bits of space and $O(log log n)$ passes, improving over the bestpossible space complexity achievable using a single pass.

(1+eps)-approximate Sparse Recovery

  The problem central to sparse recovery and compressive sensing is that ofstable sparse recovery: we want a distribution of matrices A in R^{m\times n}such that, for any x \in R^n and with probability at least 2/3 over A, there isan algorithm to recover x* from Ax with  ||x* - x||_p <= C min_{k-sparse x'} ||x - x'||_p for some constant C > 1 andnorm p. The measurement complexity of this problem is well understood forconstant C > 1. However, in a variety of applications it is important to obtainC = 1 + eps for a small eps > 0, and this complexity is not well understood. Weresolve the dependence on eps in the number of measurements required of ak-sparse recovery algorithm, up to polylogarithmic factors for the centralcases of p = 1 and p = 2. Namely, we give new algorithms and lower bounds thatshow the number of measurements required is (1/eps^{p/2})k polylog(n). For p =2, our bound of (1/eps) k log(n/k) is tight up to constant factors. We alsogive matching bounds when the output is required to be k-sparse, in which casewe achieve (1/eps^p) k polylog(n). This shows the distinction between thecomplexity of sparse and non-sparse outputs is fundamental.

Lower Bounds for Adaptive Sparse Recovery

  We give lower bounds for the problem of stable sparse recovery from/adaptive/ linear measurements. In this problem, one would like to estimate avector $x \in \R^n$ from $m$ linear measurements $A_1x,..., A_mx$. One maychoose each vector $A_i$ based on $A_1x,..., A_{i-1}x$, and must output $x*$satisfying |x* - x|_p \leq (1 + \epsilon) \min_{k\text{-sparse} x'} |x - x'|_pwith probability at least $1-\delta>2/3$, for some $p \in \{1,2\}$. For $p=2$,it was recently shown that this is possible with $m = O(\frac{1}{\epsilon}k\log \log (n/k))$, while nonadaptively it requires $\Theta(\frac{1}{\epsilon}k\log (n/k))$. It is also known that even adaptively, it takes $m =\Omega(k/\epsilon)$ for $p = 2$. For $p = 1$, there is a non-adaptive upperbound of $\tilde{O}(\frac{1}{\sqrt{\epsilon}} k\log n)$. We show:  * For $p=2$, $m = \Omega(\log \log n)$. This is tight for $k = O(1)$ andconstant $\epsilon$, and shows that the $\log \log n$ dependence is correct.  * If the measurement vectors are chosen in $R$ "rounds", then $m = \Omega(R\log^{1/R} n)$. For constant $\epsilon$, this matches the previously knownupper bound up to an O(1) factor in $R$.  * For $p=1$, $m = \Omega(k/(\sqrt{\epsilon} \cdot \log k/\epsilon))$. Thisshows that adaptivity cannot improve more than logarithmic factors, providingthe analog of the $m = \Omega(k/\epsilon)$ bound for $p = 2$.

Tight bounds for learning a mixture of two gaussians

  We consider the problem of identifying the parameters of an unknown mixtureof two arbitrary $d$-dimensional gaussians from a sequence of independentrandom samples. Our main results are upper and lower bounds giving acomputationally efficient moment-based estimator with an optimal convergencerate, thus resolving a problem introduced by Pearson (1894). Denoting by$\sigma^2$ the variance of the unknown mixture, we prove that$\Theta(\sigma^{12})$ samples are necessary and sufficient to estimate eachparameter up to constant additive error when $d=1.$ Our upper bound extends toarbitrary dimension $d>1$ up to a (provably necessary) logarithmic loss in $d$using a novel---yet simple---dimensionality reduction technique. We furtheridentify several interesting special cases where the sample complexity isnotably smaller than our optimal worst-case bound. For instance, if the meansof the two components are separated by $\Omega(\sigma)$ the sample complexityreduces to $O(\sigma^2)$ and this is again optimal.  Our results also apply to learning each component of the mixture up to smallerror in total variation distance, where our algorithm gives strongimprovements in sample complexity over previous work. We also extend our lowerbound to mixtures of $k$ Gaussians, showing that $\Omega(\sigma^{6k-2})$samples are necessary to estimate each parameter up to constant additive error.

Modelling Information Incorporation in Markets, with Application to  Detecting and Explaining Events

  We develop a model of how information flows into a market, and derivealgorithms for automatically detecting and explaining relevant events. Weanalyze data from twenty-two "political stock markets" (i.e., betting marketson political outcomes) on the Iowa Electronic Market (IEM). We prove that,under certain efficiency assumptions, prices in such betting markets will onaverage approach the correct outcomes over time, and show that IEM dataconforms closely to the theory. We present a simple model of a betting marketwhere information is revealed over time, and show a qualitative correspondencebetween the model and real market data. We also present an algorithm forautomatically detecting significant events and generating semantic explanationsof their origin. The algorithm operates by discovering significant changes invocabulary on online news sources (using expected entropy loss) that align withmajor price spikes in related betting markets.

Adversarial Examples from Cryptographic Pseudo-Random Generators

  In our recent work (Bubeck, Price, Razenshteyn, arXiv:1805.10204) we arguedthat adversarial examples in machine learning might be due to an inherentcomputational hardness of the problem. More precisely, we constructed a binaryclassification task for which (i) a robust classifier exists; yet nonon-trivial accuracy can be obtained with an efficient algorithm in (ii) thestatistical query model. In the present paper we significantly strengthen both(i) and (ii): we now construct a task which admits (i') a maximally robustclassifier (that is it can tolerate perturbations of size comparable to thesize of the examples themselves); and moreover we prove computational hardnessof learning this task under (ii') a standard cryptographic assumption.

Curved Space or Curved Vacuum?

  While the simple picture of a spatially flat, matter plus cosmologicalconstant universe fits current observation of the accelerated expansion, strongconsideration has also been given to models with dynamical vacuum energy. Weexamine the tradeoff of ``curving'' the vacuum but retaining spatial flatness,vs. curving space but retaining the cosmological constant. These differentbreakdowns in the simple picture could readily be distinguished by combinedhigh accuracy supernovae and cosmic microwave background distance measurements.If we allow the uneasy situation of both breakdowns, the curvature can still bemeasured to 1%, but at the price of degrading estimation of the equation ofstate time variation by 60% or more, unless additional information (such asweak lensing data or a tight matter density prior) is included.

Gravitational perturbations of the Schwarzschild spacetime: A practical  covariant and gauge-invariant formalism

  We present a formalism to study the metric perturbations of the Schwarzschildspacetime. The formalism is gauge invariant, and it is also covariant undertwo-dimensional coordinate transformations that leave the angular coordinatesunchanged. The formalism is applied to the typical problem of calculating thegravitational waves produced by material sources moving in the Schwarzschildspacetime. We examine the radiation escaping to future null infinity as well asthe radiation crossing the event horizon. The waveforms, the energy radiated,and the angular-momentum radiated can all be expressed in terms of twogauge-invariant scalar functions that satisfy one-dimensional wave equations.The first is the Zerilli-Moncrief function, which satisfies the Zerilliequation, and which represents the even-parity sector of the perturbation. Thesecond is the Cunningham-Price-Moncrief function, which satisfies theRegge-Wheeler equation, and which represents the odd-parity sector of theperturbation. The covariant forms of these wave equations are presented here,complete with covariant source terms that are derived from the stress-energytensor of the matter responsible for the perturbation. Our presentation of theformalism is concluded with a separate examination of the monopole and dipolecomponents of the metric perturbation.

Integral Field Spectrographs: a user's view

  We easily tend to think of Integral-Field Spectrographs (IFS) along twoopposing trends: as either the beautiful combination between photometry andspectroscopy, or as our worst nightmare including the dark side of both worlds.I favour a view where each IFS is considered individually, as one instrumentwith specific performances which can be used optimally for a certain range ofscientific programs. It is indeed true that data-wise, IFS do sometime mergethe characteristics of classic (e.g., long-slit) spectrographs with annoyingissues associated with Imagers. This is in fact the price to pay to access adrastically different perspective of our favourite targets. The challenge isthen to provide the necessary tools to properly handle the corresponding data.However, this should certainly not be thought as something specific to IFS:such a challenge should be accepted for any instrument, and most importantlysolved prior to its delivery at the telescope.

Second-Order, Dissipative Tâtonnement: Economic Interpretation and  2-Point Limit Cycles

  This paper proposes an alternative to the classical price-adjustmentmechanism (called "t\^{a}tonnement" after Walras) that is second-order in time.The proposed mechanism, an analogue to the damped harmonic oscillator, providesa dynamic equilibration process that depends only on local information. We showhow such a process can result from simple behavioural rules. The discrete-timeform of the model can result in two-step limit cycles, but as the distancecovered by the cycle depends on the size of the damping, the proposed mechanismcan lead to both highly unstable and relatively stable behaviour, as observedin real economies.

Post-periapsis pancakes: sustenance for self-gravity in tidal disruption  events

  A tidal disruption event, which occurs when a star is destroyed by thegravitational field of a supermassive black hole, produces a stream of debris,the evolution of which ultimately determines the observational properties ofthe event. Here we show that a post-periapsis caustic -- a location where thelocus of gas parcels comprising the stream would collapse into atwo-dimensional surface if they evolved solely in the gravitational field ofthe hole -- occurs when the pericenter distance of the star is on the order ofthe tidal radius of the hole. It is demonstrated that this "pancake" inducessignificant density perturbations in the debris stream, and, for stifferequations of state (adiabatic index $\gamma \gtrsim 5/3$), these fluctuationsare sufficient to gravitationally destabilize the stream, resulting in itsfragmentation into bound clumps. The results of our findings are discussed inthe context of the observational properties of tidal disruption events.

