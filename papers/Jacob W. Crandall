Non-myopic learning in repeated stochastic games

  In repeated stochastic games (RSGs), an agent must quickly adapt to thebehavior of previously unknown associates, who may themselves be learning. Thismachine-learning problem is particularly challenging due, in part, to thepresence of multiple (even infinite) equilibria and inherently large strategyspaces. In this paper, we introduce a method to reduce the strategy space oftwo-player general-sum RSGs to a handful of expert strategies. This process,called Mega, effectually reduces an RSG to a bandit problem. We show that theresulting strategy space preserves several important properties of the originalRSG, thus enabling a learner to produce robust strategies within a reasonablysmall number of interactions. To better establish strengths and weaknesses ofthis approach, we empirically evaluate the resulting learning system againstother algorithms in three different RSGs.

Cooperating with Machines

  Since Alan Turing envisioned Artificial Intelligence (AI) [1], a majordriving force behind technical progress has been competition with humancognition. Historical milestones have been frequently associated with computersmatching or outperforming humans in difficult cognitive tasks (e.g. facerecognition [2], personality classification [3], driving cars [4], or playingvideo games [5]), or defeating humans in strategic zero-sum encounters (e.g.Chess [6], Checkers [7], Jeopardy! [8], Poker [9], or Go [10]). In contrast,less attention has been given to developing autonomous machines that establishmutually cooperative relationships with people who may not share the machine'spreferences. A main challenge has been that human cooperation does not requiresheer computational power, but rather relies on intuition [11], cultural norms[12], emotions and signals [13, 14, 15, 16], and pre-evolved dispositionstoward cooperation [17], common-sense mechanisms that are difficult to encodein machines for arbitrary contexts. Here, we combine a state-of-the-artmachine-learning algorithm with novel mechanisms for generating and acting onsignals to produce a new learning algorithm that cooperates with people andother machines at levels that rival human cooperation in a variety oftwo-player repeated stochastic games. This is the first general-purposealgorithm that is capable, given a description of a previously unseen gameenvironment, of learning to cooperate with people within short timescales inscenarios previously unanticipated by algorithm designers. This is achievedwithout complex opponent modeling or higher-order theories of mind, thusshowing that flexible, fast, and general human-machine cooperation iscomputationally achievable using a non-trivial, but ultimately simple, set ofalgorithmic mechanisms.

A Critical Assessment of Cost-Based Nash Methods for Demand Scheduling  in Smart Grids

  Demand-side management (DSM) is becoming an increasingly important componentof the envisioned smart grid. The ability to improve the efficiency of energyuse in the power system by altering demand is widely viewed as being not merelypromising but in fact essential. However, while the advantages of DSM areclear, arriving at an efficient implementation has so far proven to be lessstraightforward. There have recently been many proposals put forth in theliterature to tackle the demand scheduling aspect of DSM. One particularapproach based on a game-theoretic treatment of the day-ahead load-schedulingproblem has recently gained tremendous popularity in the DSM literature. Inthis letter, an assessment of this approach is conducted, and its main resultis challenged.

An Online Mechanism for Ridesharing in Autonomous Mobility-on-Demand  Systems

  With proper management, Autonomous Mobility-on-Demand (AMoD) systems havegreat potential to satisfy the transport demands of urban populations byproviding safe, convenient, and affordable ridesharing services. Meanwhile,such systems can substantially decrease private car ownership and use, and thussignificantly reduce traffic congestion, energy consumption, and carbonemissions. To achieve this objective, an AMoD system requires privateinformation about the demand from passengers. However, due toself-interestedness, passengers are unlikely to cooperate with the serviceproviders in this regard. Therefore, an online mechanism is desirable if itincentivizes passengers to truthfully report their actual demand. For thepurpose of promoting ridesharing, we hereby introduce a posted-price,integrated online ridesharing mechanism (IORS) that satisfies desirableproperties such as ex-post incentive compatibility, individual rationality, andbudget-balance. Numerical results indicate the competitiveness of IORS comparedwith two benchmarks, namely the optimal assignment and an offline,auction-based mechanism.

Information Design in Crowdfunding under Thresholding Policies

  Crowdfunding has emerged as a prominent way for entrepreneurs to securefunding without sophisticated intermediation. In crowdfunding, an entrepreneuroften has to decide how to disclose the campaign status in order to collect asmany contributions as possible. Such decisions are difficult to make primarilydue to incomplete information. We propose information design as a tool to helpthe entrepreneur to improve revenue by influencing backers' beliefs. Weintroduce a heuristic algorithm to dynamically compute information-disclosurepolicies for the entrepreneur, followed by an empirical evaluation todemonstrate its competitiveness over the widely-adopted immediate-disclosurepolicy. Our results demonstrate that the immediate-disclosure policy is notoptimal when backers follow thresholding policies despite its ease ofimplementation. With appropriate heuristics, an entrepreneur can benefit fromdynamic information disclosure. Our work sheds light on information design in adynamic setting where agents make decisions using thresholding policies.

Regulating Highly Automated Robot Ecologies: Insights from Three User  Studies

  Highly automated robot ecologies (HARE), or societies of independentautonomous robots or agents, are rapidly becoming an important part of much ofthe world's critical infrastructure. As with human societies, regulation,wherein a governing body designs rules and processes for the society, plays animportant role in ensuring that HARE meet societal objectives. However, todate, a careful study of interactions between a regulator and HARE is lacking.In this paper, we report on three user studies which give insights into how todesign systems that allow people, acting as the regulatory authority, toeffectively interact with HARE. As in the study of political systems in whichgovernments regulate human societies, our studies analyze how interactionsbetween HARE and regulators are impacted by regulatory power and individual(robot or agent) autonomy. Our results show that regulator power, decisionsupport, and adaptive autonomy can each diminish the social welfare of HARE,and hint at how these seemingly desirable mechanisms can be designed so thatthey become part of successful HARE.

Belief and Truth in Hypothesised Behaviours

  There is a long history in game theory on the topic of Bayesian or "rational"learning, in which each player maintains beliefs over a set of alternativebehaviours, or types, for the other players. This idea has gained increasinginterest in the artificial intelligence (AI) community, where it is used as amethod to control a single agent in a system composed of multiple agents withunknown behaviours. The idea is to hypothesise a set of types, each specifyinga possible behaviour for the other agents, and to plan our own actions withrespect to those types which we believe are most likely, given the observedactions of the agents. The game theory literature studies this idea primarilyin the context of equilibrium attainment. In contrast, many AI applicationshave a focus on task completion and payoff maximisation. With this perspectivein mind, we identify and address a spectrum of questions pertaining to beliefand truth in hypothesised types. We formulate three basic ways to incorporateevidence into posterior beliefs and show when the resulting beliefs arecorrect, and when they may fail to be correct. Moreover, we demonstrate thatprior beliefs can have a significant impact on our ability to maximise payoffsin the long-term, and that they can be computed automatically with consistentperformance effects. Furthermore, we analyse the conditions under which we areable complete our task optimally, despite inaccuracies in the hypothesisedtypes. Finally, we show how the correctness of hypothesised types can beascertained during the interaction via an automated statistical analysis.

