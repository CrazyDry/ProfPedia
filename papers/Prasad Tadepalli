Event Detection with Neural Networks: A Rigorous Empirical Evaluation

  Detecting events and classifying them into predefined types is an importantstep in knowledge extraction from natural language texts. While the neuralnetwork models have generally led the state-of-the-art, the differences inperformance between different architectures have not been rigorously studied.In this paper we present a novel GRU-based model that combines syntacticinformation along with temporal structure through an attention mechanism. Weshow that it is competitive with other neural network architectures throughempirical evaluations under different random initializations andtraining-validation-test splits of ACE2005 dataset.

Conservative Agency via Attainable Utility Preservation

  Reward functions are often misspecified. An agent optimizing an incorrectreward function can change its environment in large, undesirable, andpotentially irreversible ways. Work on impact measurement seeks a means ofidentifying (and thereby avoiding) large changes to the environment. We proposea novel impact measure which induces conservative, effective behavior across arange of situations. The approach attempts to preserve the attainable utilityof auxiliary objectives. We evaluate our proposal on an array of benchmarktasks and show that it matches or outperforms relative reachability, thestate-of-the-art in impact measurement.

Output Space Search for Structured Prediction

  We consider a framework for structured prediction based on search in thespace of complete structured outputs. Given a structured input, an output isproduced by running a time-bounded search procedure guided by a learned costfunction, and then returning the least cost output uncovered during the search.This framework can be instantiated for a wide range of search spaces and searchprocedures, and easily incorporates arbitrary structured-prediction lossfunctions. In this paper, we make two main technical contributions. First, wedefine the limited-discrepancy search space over structured outputs, which isable to leverage powerful classification learning algorithms to improve thesearch space quality. Second, we give a generic cost function learningapproach, where the key idea is to learn a cost function that attempts to mimicthe behavior of conducting searches guided by the true loss function. Ourexperiments on six benchmark domains demonstrate that using our framework withonly a small amount of search is sufficient for significantly improving onstate-of-the-art structured-prediction performance.

Coactive Learning for Locally Optimal Problem Solving

  Coactive learning is an online problem solving setting where the solutionsprovided by a solver are interactively improved by a domain expert, which inturn drives learning. In this paper we extend the study of coactive learning toproblems where obtaining a globally optimal or near-optimal solution may beintractable or where an expert can only be expected to make small, localimprovements to a candidate solution. The goal of learning in this new settingis to minimize the cost as measured by the expert effort over time. We firstestablish theoretical bounds on the average cost of the existing coactivePerceptron algorithm. In addition, we consider new online algorithms that usecost-sensitive and Passive-Aggressive (PA) updates, showing similar or improvedtheoretical bounds. We provide an empirical evaluation of the learners invarious domains, which show that the Perceptron based algorithms are quiteeffective and that unlike the case for online classification, the PA algorithmsdo not yield significant performance gains.

Event Nugget Detection with Forward-Backward Recurrent Neural Networks

  Traditional event detection methods heavily rely on manually engineered richfeatures. Recent deep learning approaches alleviate this problem by automaticfeature engineering. But such efforts, like tradition methods, have so far onlyfocused on single-token event mentions, whereas in practice events can also bea phrase. We instead use forward-backward recurrent neural networks (FBRNNs) todetect events that can be either words or phrases. To the best our knowledge,this is one of the first efforts to handle multi-word events and also the firstattempt to use RNNs for event detection. Experimental results demonstrate thatFBRNN is competitive with the state-of-the-art methods on the ACE 2005 and theRich ERE 2015 event detection tasks.

Dependent Gated Reading for Cloze-Style Question Answering

  We present a novel deep learning architecture to address the cloze-stylequestion answering task. Existing approaches employ reading mechanisms that donot fully exploit the interdependency between the document and the query. Inthis paper, we propose a novel \emph{dependent gated reading} bidirectional GRUnetwork (DGR) to efficiently model the relationship between the document andthe query during encoding and decision making. Our evaluation shows that DGRobtains highly competitive performance on well-known machine comprehensionbenchmarks such as the Children's Book Test (CBT-NE and CBT-CN) and Who DiDWhat (WDW, Strict and Relaxed). Finally, we extensively analyze and validateour model by ablation and attention studies.

Joint Neural Entity Disambiguation with Output Space Search

  In this paper, we present a novel model for entity disambiguation thatcombines both local contextual information and global evidences through LimitedDiscrepancy Search (LDS). Given an input document, we start from a completesolution constructed by a local model and conduct a search in the space ofpossible corrections to improve the local solution from a global view point.Our search utilizes a heuristic function to focus more on the least confidentlocal decisions and a pruning function to score the global solutions based ontheir local fitness and the global coherences among the predicted entities.Experimental results on CoNLL 2003 and TAC 2010 benchmarks verify theeffectiveness of our model.

Interpreting Recurrent and Attention-Based Neural Models: a Case Study  on Natural Language Inference

  Deep learning models have achieved remarkable success in natural languageinference (NLI) tasks. While these models are widely explored, they are hard tointerpret and it is often unclear how and why they actually work. In thispaper, we take a step toward explaining such deep learning based models througha case study on a popular neural model for NLI. In particular, we propose tointerpret the intermediate layers of NLI models by visualizing the saliency ofattention and LSTM gating signals. We present several examples for which ourmethods are able to reveal interesting insights and identify the criticalinformation contributing to the model decisions.

Attentional Multi-Reading Sarcasm Detection

  Recognizing sarcasm often requires a deep understanding of multiple sourcesof information, including the utterance, the conversational context, and realworld facts. Most of the current sarcasm detection systems consider only theutterance in isolation. There are some limited attempts toward taking intoaccount the conversational context. In this paper, we propose an interpretableend-to-end model that combines information from both the utterance and theconversational context to detect sarcasm, and demonstrate its effectivenessthrough empirical evaluations. We also study the behavior of the proposed modelto provide explanations for the model's decisions. Importantly, our model iscapable of determining the impact of utterance and conversational context onthe model's decisions. Finally, we provide an ablation study to illustrate theimpact of different components of the proposed model.

Learning Scripts as Hidden Markov Models

  Scripts have been proposed to model the stereotypical event sequences foundin narratives. They can be applied to make a variety of inferences includingfilling gaps in the narratives and resolving ambiguous references. This paperproposes the first formal framework for scripts based on Hidden Markov Models(HMMs). Our framework supports robust inference and learning algorithms, whichare lacking in previous clustering models. We develop an algorithm forstructure and parameter learning based on Expectation Maximization and evaluateit on a number of natural datasets. The results show that our algorithm issuperior to several informed baselines for predicting missing events in partialobservation sequences.

Saliency Learning: Teaching the Model Where to Pay Attention

  Deep learning has emerged as a compelling solution to many NLP tasks withremarkable performances. However, due to their opacity, such models are hard tointerpret and trust. Recent work on explaining deep models has introducedapproaches to provide insights toward the model's behaviour and predictions,which are helpful for assessing the reliability of the model's predictions.However, such methods do not improve the model's reliability. In this paper, weaim to teach the model to make the right prediction for the right reason byproviding explanation training and ensuring the alignment of the model'sexplanation with the ground truth explanation. Our experimental results onmultiple tasks and datasets demonstrate the effectiveness of the proposedmethod, which produces more reliable predictions while delivering betterresults compared to traditionally trained models.

Interactive Naming for Explaining Deep Neural Networks: A Formative  Study

  We consider the problem of explaining the decisions of deep neural networksfor image recognition in terms of human-recognizable visual concepts. Inparticular, given a test set of images, we aim to explain each classificationin terms of a small number of image regions, or activation maps, which havebeen associated with semantic concepts by a human annotator. This allows forgenerating summary views of the typical reasons for classifications, which canhelp build trust in a classifier and/or identify example types for which theclassifier may not be trusted. For this purpose, we developed a user interfacefor "interactive naming," which allows a human annotator to manually clustersignificant activation maps in a test set into meaningful groups called "visualconcepts". The main contribution of this paper is a systematic study of thevisual concepts produced by five human annotators using the interactive naminginterface. In particular, we consider the adequacy of the concepts forexplaining the classification of test-set images, correspondence of theconcepts to activations of individual neurons, and the inter-annotatoragreement of visual concepts. We find that a large fraction of the activationmaps have recognizable visual concepts, and that there is significant agreementbetween the different annotators about their denotations. Our work is anexploratory study of the interplay between machine learning and humanrecognition mediated by visualizations of the results of learning.

The SeaQuest Spectrometer at Fermilab

  The SeaQuest spectrometer at Fermilab was designed to detectoppositely-charged pairs of muons (dimuons) produced by interactions between a120 GeV proton beam and liquid hydrogen, liquid deuterium and solid nucleartargets. The primary physics program uses the Drell-Yan process to probeantiquark distributions in the target nucleon. The spectrometer consists of atarget system, two dipole magnets and four detector stations. The upstreammagnet is a closed-aperture solid iron magnet which also serves as the beamdump, while the second magnet is an open aperture magnet. Each of the detectorstations consists of scintillator hodoscopes and a high-resolution trackingdevice. The FPGA-based trigger compares the hodoscope signals to a set ofpre-programmed roads to determine if the event contains oppositely-signed,high-mass muon pairs.

