IllinoisSL: A JAVA Library for Structured Prediction

  IllinoisSL is a Java library for learning structured prediction models. It
supports structured Support Vector Machines and structured Perceptron. The
library consists of a core learning module and several applications, which can
be executed from command-lines. Documentation is provided to guide users. In
Comparison to other structured learning libraries, IllinoisSL is efficient,
general, and easy to use.


Learning to Search for Dependencies

  We demonstrate that a dependency parser can be built using a credit
assignment compiler which removes the burden of worrying about low-level
machine learning details from the parser implementation. The result is a simple
parser which robustly applies to many languages that provides similar
statistical and computational performance with best-to-date transition-based
parsing approaches, while avoiding various downsides including randomization,
extra feature requirements, and custom learning algorithms.


A Credit Assignment Compiler for Joint Prediction

  Many machine learning applications involve jointly predicting multiple
mutually dependent output variables. Learning to search is a family of methods
where the complex decision problem is cast into a sequence of decisions via a
search space. Although these methods have shown promise both in theory and in
practice, implementing them has been burdensomely awkward. In this paper, we
show the search space can be defined by an arbitrary imperative program,
turning learning to search into a credit assignment compiler. Altogether with
the algorithmic improvements for the compiler, we radically reduce the
complexity of programming and the running time. We demonstrate the feasibility
of our approach on multiple joint prediction tasks. In all cases, we obtain
accuracies as high as alternative approaches, at drastically reduced execution
and programming time.


Learning to Search Better Than Your Teacher

  Methods for learning to search for structured prediction typically imitate a
reference policy, with existing theoretical guarantees demonstrating low regret
compared to that reference. This is unsatisfactory in many applications where
the reference policy is suboptimal and the goal of learning is to improve upon
it. Can learning to search work even when the reference is poor?
  We provide a new learning to search algorithm, LOLS, which does well relative
to the reference policy, but additionally guarantees low regret compared to
deviations from the learned policy: a local-optimality guarantee. Consequently,
LOLS can improve upon the reference policy, unlike previous algorithms. This
enables us to develop structured contextual bandits, a partial information
structured prediction setting with many potential applications.


Multi-task Learning for Universal Sentence Embeddings: A Thorough
  Evaluation using Transfer and Auxiliary Tasks

  Learning distributed sentence representations is one of the key challenges in
natural language processing. Previous work demonstrated that a recurrent neural
network (RNNs) based sentence encoder trained on a large collection of
annotated natural language inference data, is efficient in the transfer
learning to facilitate other related tasks. In this paper, we show that joint
learning of multiple tasks results in better generalizable sentence
representations by conducting extensive experiments and analysis comparing the
multi-task and single-task learned sentence encoders. The quantitative analysis
using auxiliary tasks show that multi-task learning helps to embed better
semantic information in the sentence representations compared to single-task
learning. In addition, we compare multi-task sentence encoders with
contextualized word representations and show that combining both of them can
further boost the performance of transfer learning.


LearningWord Embeddings for Low-resource Languages by PU Learning

  Word embedding is a key component in many downstream applications in
processing natural languages. Existing approaches often assume the existence of
a large collection of text for learning effective word embedding. However, such
a corpus may not be available for some low-resource languages. In this paper,
we study how to effectively learn a word embedding model on a corpus with only
a few million tokens. In such a situation, the co-occurrence matrix is sparse
as the co-occurrences of many word pairs are unobserved. In contrast to
existing approaches often only sample a few unobserved word pairs as negative
samples, we argue that the zero entries in the co-occurrence matrix also
provide valuable information. We then design a Positive-Unlabeled Learning
(PU-Learning) approach to factorize the co-occurrence matrix and validate the
proposed approaches in four different languages.


Men Also Like Shopping: Reducing Gender Bias Amplification using
  Corpus-level Constraints

  Language is increasingly being used to define rich visual recognition
problems with supporting image collections sourced from the web. Structured
prediction models are used in these tasks to take advantage of correlations
between co-occurring labels and visual input but risk inadvertently encoding
social biases found in web corpora. In this work, we study data and models
associated with multilabel object classification and visual semantic role
labeling. We find that (a) datasets for these tasks contain significant gender
bias and (b) models trained on these datasets further amplify existing bias.
For example, the activity cooking is over 33% more likely to involve females
than males in a training set, and a trained model further amplifies the
disparity to 68% at test time. We propose to inject corpus-level constraints
for calibrating existing structured prediction models and design an algorithm
based on Lagrangian relaxation for collective inference. Our method results in
almost no performance loss for the underlying recognition task but decreases
the magnitude of bias amplification by 47.5% and 40.5% for multilabel
classification and visual semantic role labeling, respectively.


