IllinoisSL: A JAVA Library for Structured Prediction

  IllinoisSL is a Java library for learning structured prediction models. Itsupports structured Support Vector Machines and structured Perceptron. Thelibrary consists of a core learning module and several applications, which canbe executed from command-lines. Documentation is provided to guide users. InComparison to other structured learning libraries, IllinoisSL is efficient,general, and easy to use.

Learning to Search for Dependencies

  We demonstrate that a dependency parser can be built using a creditassignment compiler which removes the burden of worrying about low-levelmachine learning details from the parser implementation. The result is a simpleparser which robustly applies to many languages that provides similarstatistical and computational performance with best-to-date transition-basedparsing approaches, while avoiding various downsides including randomization,extra feature requirements, and custom learning algorithms.

A Credit Assignment Compiler for Joint Prediction

  Many machine learning applications involve jointly predicting multiplemutually dependent output variables. Learning to search is a family of methodswhere the complex decision problem is cast into a sequence of decisions via asearch space. Although these methods have shown promise both in theory and inpractice, implementing them has been burdensomely awkward. In this paper, weshow the search space can be defined by an arbitrary imperative program,turning learning to search into a credit assignment compiler. Altogether withthe algorithmic improvements for the compiler, we radically reduce thecomplexity of programming and the running time. We demonstrate the feasibilityof our approach on multiple joint prediction tasks. In all cases, we obtainaccuracies as high as alternative approaches, at drastically reduced executionand programming time.

Learning to Search Better Than Your Teacher

  Methods for learning to search for structured prediction typically imitate areference policy, with existing theoretical guarantees demonstrating low regretcompared to that reference. This is unsatisfactory in many applications wherethe reference policy is suboptimal and the goal of learning is to improve uponit. Can learning to search work even when the reference is poor?  We provide a new learning to search algorithm, LOLS, which does well relativeto the reference policy, but additionally guarantees low regret compared todeviations from the learned policy: a local-optimality guarantee. Consequently,LOLS can improve upon the reference policy, unlike previous algorithms. Thisenables us to develop structured contextual bandits, a partial informationstructured prediction setting with many potential applications.

Multi-task Learning for Universal Sentence Embeddings: A Thorough  Evaluation using Transfer and Auxiliary Tasks

  Learning distributed sentence representations is one of the key challenges innatural language processing. Previous work demonstrated that a recurrent neuralnetwork (RNNs) based sentence encoder trained on a large collection ofannotated natural language inference data, is efficient in the transferlearning to facilitate other related tasks. In this paper, we show that jointlearning of multiple tasks results in better generalizable sentencerepresentations by conducting extensive experiments and analysis comparing themulti-task and single-task learned sentence encoders. The quantitative analysisusing auxiliary tasks show that multi-task learning helps to embed bettersemantic information in the sentence representations compared to single-tasklearning. In addition, we compare multi-task sentence encoders withcontextualized word representations and show that combining both of them canfurther boost the performance of transfer learning.

LearningWord Embeddings for Low-resource Languages by PU Learning

  Word embedding is a key component in many downstream applications inprocessing natural languages. Existing approaches often assume the existence ofa large collection of text for learning effective word embedding. However, sucha corpus may not be available for some low-resource languages. In this paper,we study how to effectively learn a word embedding model on a corpus with onlya few million tokens. In such a situation, the co-occurrence matrix is sparseas the co-occurrences of many word pairs are unobserved. In contrast toexisting approaches often only sample a few unobserved word pairs as negativesamples, we argue that the zero entries in the co-occurrence matrix alsoprovide valuable information. We then design a Positive-Unlabeled Learning(PU-Learning) approach to factorize the co-occurrence matrix and validate theproposed approaches in four different languages.

Men Also Like Shopping: Reducing Gender Bias Amplification using  Corpus-level Constraints

  Language is increasingly being used to define rich visual recognitionproblems with supporting image collections sourced from the web. Structuredprediction models are used in these tasks to take advantage of correlationsbetween co-occurring labels and visual input but risk inadvertently encodingsocial biases found in web corpora. In this work, we study data and modelsassociated with multilabel object classification and visual semantic rolelabeling. We find that (a) datasets for these tasks contain significant genderbias and (b) models trained on these datasets further amplify existing bias.For example, the activity cooking is over 33% more likely to involve femalesthan males in a training set, and a trained model further amplifies thedisparity to 68% at test time. We propose to inject corpus-level constraintsfor calibrating existing structured prediction models and design an algorithmbased on Lagrangian relaxation for collective inference. Our method results inalmost no performance loss for the underlying recognition task but decreasesthe magnitude of bias amplification by 47.5% and 40.5% for multilabelclassification and visual semantic role labeling, respectively.

