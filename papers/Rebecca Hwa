An Empirical Evaluation of Probabilistic Lexicalized Tree Insertion
  Grammars

  We present an empirical study of the applicability of Probabilistic
Lexicalized Tree Insertion Grammars (PLTIG), a lexicalized counterpart to
Probabilistic Context-Free Grammars (PCFG), to problems in stochastic
natural-language processing. Comparing the performance of PLTIGs with
non-hierarchical N-gram models and PCFGs, we show that PLTIG combines the best
aspects of both, with language modeling capability comparable to N-grams, and
improved parsing performance over its non-lexicalized counterpart. Furthermore,
training of PLTIGs displays faster convergence than PCFGs.


Supervised Grammar Induction Using Training Data with Limited
  Constituent Information

  Corpus-based grammar induction generally relies on hand-parsed training data
to learn the structure of the language. Unfortunately, the cost of building
large annotated corpora is prohibitively expensive. This work aims to improve
the induction strategy when there are few labels in the training data. We show
that the most informative linguistic constituents are the higher nodes in the
parse trees, typically denoting complex noun phrases and sentential clauses.
They account for only 20% of all constituents. For inducing grammars from
sparsely labeled training data (e.g., only higher-level constituent labels), we
propose an adaptation strategy, which produces grammars that parse almost as
well as grammars induced from fully labeled corpora. Our results suggest that
for a partial parser to replace human annotators, it must be able to
automatically extract higher-level constituents rather than base noun phrases.


Equal But Not The Same: Understanding the Implicit Relationship Between
  Persuasive Images and Text

  Images and text in advertisements interact in complex, non-literal ways. The
two channels are usually complementary, with each channel telling a different
part of the story. Current approaches, such as image captioning methods, only
examine literal, redundant relationships, where image and text show exactly the
same content. To understand more complex relationships, we first collect a
dataset of advertisement interpretations for whether the image and slogan in
the same visual advertisement form a parallel (conveying the same message
without literally saying the same thing) or non-parallel relationship, with the
help of workers recruited on Amazon Mechanical Turk. We develop a variety of
features that capture the creativity of images and the specificity or ambiguity
of text, as well as methods that analyze the semantics within and across
channels. We show that our method outperforms standard image-text alignment
approaches on predicting the parallel/non-parallel relationship between image
and text.


An Interactive Tool for Natural Language Processing on Clinical Text

  Natural Language Processing (NLP) systems often make use of machine learning
techniques that are unfamiliar to end-users who are interested in analyzing
clinical records. Although NLP has been widely used in extracting information
from clinical text, current systems generally do not support model revision
based on feedback from domain experts.
  We present a prototype tool that allows end users to visualize and review the
outputs of an NLP system that extracts binary variables from clinical text. Our
tool combines multiple visualizations to help the users understand these
results and make any necessary corrections, thus forming a feedback loop and
helping improve the accuracy of the NLP models. We have tested our prototype in
a formative think-aloud user study with clinicians and researchers involved in
colonoscopy research. Results from semi-structured interviews and a System
Usability Scale (SUS) analysis show that the users are able to quickly start
refining NLP models, despite having very little or no experience with machine
learning. Observations from these sessions suggest revisions to the interface
to better support review workflow and interpretation of results.


