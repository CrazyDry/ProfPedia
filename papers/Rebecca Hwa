An Empirical Evaluation of Probabilistic Lexicalized Tree Insertion  Grammars

  We present an empirical study of the applicability of ProbabilisticLexicalized Tree Insertion Grammars (PLTIG), a lexicalized counterpart toProbabilistic Context-Free Grammars (PCFG), to problems in stochasticnatural-language processing. Comparing the performance of PLTIGs withnon-hierarchical N-gram models and PCFGs, we show that PLTIG combines the bestaspects of both, with language modeling capability comparable to N-grams, andimproved parsing performance over its non-lexicalized counterpart. Furthermore,training of PLTIGs displays faster convergence than PCFGs.

Supervised Grammar Induction Using Training Data with Limited  Constituent Information

  Corpus-based grammar induction generally relies on hand-parsed training datato learn the structure of the language. Unfortunately, the cost of buildinglarge annotated corpora is prohibitively expensive. This work aims to improvethe induction strategy when there are few labels in the training data. We showthat the most informative linguistic constituents are the higher nodes in theparse trees, typically denoting complex noun phrases and sentential clauses.They account for only 20% of all constituents. For inducing grammars fromsparsely labeled training data (e.g., only higher-level constituent labels), wepropose an adaptation strategy, which produces grammars that parse almost aswell as grammars induced from fully labeled corpora. Our results suggest thatfor a partial parser to replace human annotators, it must be able toautomatically extract higher-level constituents rather than base noun phrases.

Equal But Not The Same: Understanding the Implicit Relationship Between  Persuasive Images and Text

  Images and text in advertisements interact in complex, non-literal ways. Thetwo channels are usually complementary, with each channel telling a differentpart of the story. Current approaches, such as image captioning methods, onlyexamine literal, redundant relationships, where image and text show exactly thesame content. To understand more complex relationships, we first collect adataset of advertisement interpretations for whether the image and slogan inthe same visual advertisement form a parallel (conveying the same messagewithout literally saying the same thing) or non-parallel relationship, with thehelp of workers recruited on Amazon Mechanical Turk. We develop a variety offeatures that capture the creativity of images and the specificity or ambiguityof text, as well as methods that analyze the semantics within and acrosschannels. We show that our method outperforms standard image-text alignmentapproaches on predicting the parallel/non-parallel relationship between imageand text.

An Interactive Tool for Natural Language Processing on Clinical Text

  Natural Language Processing (NLP) systems often make use of machine learningtechniques that are unfamiliar to end-users who are interested in analyzingclinical records. Although NLP has been widely used in extracting informationfrom clinical text, current systems generally do not support model revisionbased on feedback from domain experts.  We present a prototype tool that allows end users to visualize and review theoutputs of an NLP system that extracts binary variables from clinical text. Ourtool combines multiple visualizations to help the users understand theseresults and make any necessary corrections, thus forming a feedback loop andhelping improve the accuracy of the NLP models. We have tested our prototype ina formative think-aloud user study with clinicians and researchers involved incolonoscopy research. Results from semi-structured interviews and a SystemUsability Scale (SUS) analysis show that the users are able to quickly startrefining NLP models, despite having very little or no experience with machinelearning. Observations from these sessions suggest revisions to the interfaceto better support review workflow and interpretation of results.

