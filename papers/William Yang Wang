Learning to Explain Non-Standard English Words and Phrases

  We describe a data-driven approach for automatically explaining new,non-standard English expressions in a given sentence, building on a largedataset that includes 15 years of crowdsourced examples fromUrbanDictionary.com. Unlike prior studies that focus on matching keywords froma slang dictionary, we investigate the possibility of learning a neuralsequence-to-sequence model that generates explanations of unseen non-standardEnglish expressions given context. We propose a dual encoder approach---aword-level encoder learns the representation of context, and a secondcharacter-level encoder to learn the hidden representation of the targetnon-standard expression. Our model can produce reasonable definitions of newnon-standard English expressions given their context with certain confidence.

Programming with Personalized PageRank: A Locally Groundable First-Order  Probabilistic Logic

  In many probabilistic first-order representation systems, inference isperformed by "grounding"---i.e., mapping it to a propositional representation,and then performing propositional inference. With a large database of facts,groundings can be very large, making inference and learning computationallyexpensive. Here we present a first-order probabilistic language which iswell-suited to approximate "local" grounding: every query $Q$ can beapproximately grounded with a small graph. The language is an extension ofstochastic logic programs where inference is performed by a variant ofpersonalized PageRank. Experimentally, we show that the approach performs wellwithout weight learning on an entity resolution task; that supervisedweight-learning improves accuracy; and that grounding time is independent of DBsize. We also show that order-of-magnitude speedups are possible byparallelizing learning.

Recurrence Relations for Strongly q-Log-Convex Polynomials

  We consider a class of strongly q-log-convex polynomials based on atriangular recurrence relation with linear coefficients, and we show that theBell polynomials, the Bessel polynomials, the Ramanujan polynomials and theDowling polynomials are strongly q-log-convex. We also prove that the Besseltransformation preserves log-convexity.

Spin-orbit torque in Pt/CoNiCo/Pt symmetric devices

  Current induced magnetization switching by spin-orbit torques offers anenergy-efficient means of writing information in heavy metal/ferromagnet (FM)multilayer systems. The relative contributions of field-like torques anddamping-like torques to the magnetization switching induced by the electricalcurrent are still under debate. Here, we describe a device based on a symmetricPt/FM/Pt structure, in which we demonstrate a strong damping-like torque fromthe spin Hall effect and unmeasurable field-like torque from Rashba effect. Thespin-orbit effective fields due to the spin Hall effect were investigatedquantitatively and were found to be consistent with the switching effectivefields after accounting for the switching current reduction due to thermalfluctuations from the current pulse. A non-linear dependence of deterministicswitching of average Mz on the in-plane magnetic field was revealed, whichcould be explained and understood by micromagnetic simulation.

"Liar, Liar Pants on Fire": A New Benchmark Dataset for Fake News  Detection

  Automatic fake news detection is a challenging problem in deceptiondetection, and it has tremendous real-world political and social impacts.However, statistical approaches to combating fake news has been dramaticallylimited by the lack of labeled benchmark datasets. In this paper, we presentliar: a new, publicly available dataset for fake news detection. We collected adecade-long, 12.8K manually labeled short statements in various contexts fromPolitiFact.com, which provides detailed analysis report and links to sourcedocuments for each case. This dataset can be used for fact-checking research aswell. Notably, this new dataset is an order of magnitude larger than previouslylargest public fake news datasets of similar type. Empirically, we investigateautomatic fake news detection based on surface-level linguistic patterns. Wehave designed a novel, hybrid convolutional neural network to integratemeta-data with text. We show that this hybrid approach can improve a text-onlydeep learning model.

The q-Log-convexity of the Generating Functions of the Squares of  Binomial Coefficients

  We prove a conjecture of Liu and Wang on the q-log-convexity of thepolynomial sequence $\{\sum_{k=0}^n{n\choose k}^2q^k\}_{n\geq 0}$. By usingPieri's rule and the Jacobi-Trudi identity for Schur functions, we obtain anexpansion of a sum of products of elementary symmetric functions in terms ofSchur functions with nonnegative coefficients. Then the principalspecialization leads to the q-log-convexity. We also prove that a technicalcondition of Liu and Wang holds for the squares of the binomial coefficients.Hence we deduce that the linear transformation with respect to the triangulararray $\{{n\choose k}^2\}_{0\leq k\leq n}$ is log-convexity preserving.

Efficient Inference and Learning in a Large Knowledge Base: Reasoning  with Extracted Information using a Locally Groundable First-Order  Probabilistic Logic

  One important challenge for probabilistic logics is reasoning with very largeknowledge bases (KBs) of imperfect information, such as those produced bymodern web-scale information extraction systems. One scalability problem sharedby many probabilistic logics is that answering queries involves "grounding" thequery---i.e., mapping it to a propositional representation---and the size of a"grounding" grows with database size. To address this bottleneck, we present afirst-order probabilistic language called ProPPR in which that approximate"local groundings" can be constructed in time independent of database size.Technically, ProPPR is an extension to stochastic logic programs (SLPs) that isbiased towards short derivations; it is also closely related to an earlierrelational learning algorithm called the path ranking algorithm (PRA). We showthat the problem of constructing proofs for this logic is related tocomputation of personalized PageRank (PPR) on a linearized version of the proofspace, and using on this connection, we develop a proveably-correct approximategrounding scheme, based on the PageRank-Nibble algorithm. Building on this, wedevelop a fast and easily-parallelized weight-learning algorithm for ProPPR. Inexperiments, we show that learning for ProPPR is orders magnitude faster thanlearning for Markov logic networks; that allowing mutual recursion (jointlearning) in KB inference leads to improvements in performance; and that ProPPRcan learn weights for a mutually recursive program with hundreds of clauses,which define scores of interrelated predicates, over a KB containing onemillion entities.

Schur Positivity and the $q$-Log-convexity of the Narayana Polynomials

  Using Schur positivity and the principal specialization of Schur functions,we provide a proof of a recent conjecture of Liu and Wang on the$q$-log-convexity of the Narayana polynomials, and a proof of the secondconjecture that the Narayana transformation preserves the log-convexity. Basedon a formula of Br\"and$\mathrm{\acute{e}}$n which expresses the $q$-Narayananumbers as the specializations of Schur functions, we derive several symmetricfunction identities using the Littlewood-Richardson rule for the product ofSchur functions, and obtain the strong $q$-log-convexity of the Narayanapolynomials and the strong $q$-log-concavity of the $q$-Narayana numbers.

Video Captioning via Hierarchical Reinforcement Learning

  Video captioning is the task of automatically generating a textualdescription of the actions in a video. Although previous work (e.g.sequence-to-sequence model) has shown promising results in abstracting a coarsedescription of a short video, it is still very challenging to caption a videocontaining multiple fine-grained actions with a detailed description. Thispaper aims to address the challenge by proposing a novel hierarchicalreinforcement learning framework for video captioning, where a high-levelManager module learns to design sub-goals and a low-level Worker modulerecognizes the primitive actions to fulfill the sub-goal. With thiscompositional framework to reinforce video captioning at different levels, ourapproach significantly outperforms all the baseline methods on a newlyintroduced large-scale dataset for fine-grained video captioning. Furthermore,our non-ensemble model has already achieved the state-of-the-art results on thewidely-used MSR-VTT dataset.

Look Before You Leap: Bridging Model-Free and Model-Based Reinforcement  Learning for Planned-Ahead Vision-and-Language Navigation

  Existing research studies on vision and language grounding for robotnavigation focus on improving model-free deep reinforcement learning (DRL)models in synthetic environments. However, model-free DRL models do notconsider the dynamics in the real-world environments, and they often fail togeneralize to new scenes. In this paper, we take a radical approach to bridgethe gap between synthetic studies and real-world practices---We propose anovel, planned-ahead hybrid reinforcement learning model that combinesmodel-free and model-based reinforcement learning to solve a real-worldvision-language navigation task. Our look-ahead module tightly integrates alook-ahead policy model with an environment model that predicts the next stateand the reward. Experimental results suggest that our proposed methodsignificantly outperforms the baselines and achieves the best on the real-worldRoom-to-Room dataset. Moreover, our scalable method is more generalizable whentransferring to unseen environments.

Watch, Listen, and Describe: Globally and Locally Aligned Cross-Modal  Attentions for Video Captioning

  A major challenge for video captioning is to combine audio and visual cues.Existing multi-modal fusion methods have shown encouraging results in videounderstanding. However, the temporal structures of multiple modalities atdifferent granularities are rarely explored, and how to selectively fuse themulti-modal representations at different levels of details remains uncharted.In this paper, we propose a novel hierarchically aligned cross-modal attention(HACA) framework to learn and selectively fuse both global and local temporaldynamics of different modalities. Furthermore, for the first time, we validatethe superior performance of the deep audio features on the video captioningtask. Finally, our HACA model significantly outperforms the previous bestsystems and achieves new state-of-the-art results on the widely used MSR-VTTdataset.

No Metrics Are Perfect: Adversarial Reward Learning for Visual  Storytelling

  Though impressive results have been achieved in visual captioning, the taskof generating abstract stories from photo streams is still a little-tappedproblem. Different from captions, stories have more expressive language stylesand contain many imaginary concepts that do not appear in the images. Thus itposes challenges to behavioral cloning algorithms. Furthermore, due to thelimitations of automatic metrics on evaluating story quality, reinforcementlearning methods with hand-crafted rewards also face difficulties in gaining anoverall performance boost. Therefore, we propose an Adversarial REward Learning(AREL) framework to learn an implicit reward function from humandemonstrations, and then optimize policy search with the learned rewardfunction. Though automatic eval- uation indicates slight performance boost overstate-of-the-art (SOTA) methods in cloning expert behaviors, human evaluationshows that our approach achieves significant improvement in generating morehuman-like stories than SOTA systems.

Simple Models for Word Formation in English Slang

  We propose generative models for three types of extra-grammatical wordformation phenomena abounding in English slang: Blends, Clippings, andReduplicatives. Adopting a data-driven approach coupled with linguisticknowledge, we propose simple models with state of the art performance on humanannotated gold standard datasets. Overall, our models reveal insights into thegenerative processes of word formation in slang -- insights which areincreasingly relevant in the context of the rising prevalence of slang andnon-standard varieties on the Internet.

Hierarchical CVAE for Fine-Grained Hate Speech Classification

  Existing work on automated hate speech detection typically focuses on binaryclassification or on differentiating among a small set of categories. In thispaper, we propose a novel method on a fine-grained hate speech classificationtask, which focuses on differentiating among 40 hate groups of 13 differenthate group categories. We first explore the Conditional Variational Autoencoder(CVAE) as a discriminative model and then extend it to a hierarchicalarchitecture to utilize the additional hate category information for moreaccurate prediction. Experimentally, we show that incorporating the hatecategory information for training can significantly improve the classificationperformance and our proposed model outperforms commonly-used discriminativemodels.

Analyzing and Interpreting Convolutional Neural Networks in NLP

  Convolutional neural networks have been successfully applied to various NLPtasks. However, it is not obvious whether they model different linguisticpatterns such as negation, intensification, and clause compositionality to helpthe decision-making process. In this paper, we apply visualization techniquesto observe how the model can capture different linguistic features and howthese features can affect the performance of the model. Later on, we try toidentify the model errors and their sources. We believe that interpreting CNNsis the first step to understand the underlying semantic features which canraise awareness to further improve the performance and explainability of CNNmodels.

Dirichlet Variational Autoencoder for Text Modeling

  We introduce an improved variational autoencoder (VAE) for text modeling withtopic information explicitly modeled as a Dirichlet latent variable. Byproviding the proposed model topic awareness, it is more superior atreconstructing input texts. Furthermore, due to the inherent interactionsbetween the newly introduced Dirichlet variable and the conventionalmultivariate Gaussian variable, the model is less prone to KL divergencevanishing. We derive the variational lower bound for the new model and conductexperiments on four different data sets. The results show that the proposedmodel is superior at text reconstruction across the latent space andclassifications on learned representations have higher test accuracies.

Quantifying Uncertainties in Natural Language Processing Tasks

  Reliable uncertainty quantification is a first step towards buildingexplainable, transparent, and accountable artificial intelligent systems.Recent progress in Bayesian deep learning has made such quantificationrealizable. In this paper, we propose novel methods to study the benefits ofcharacterizing model and data uncertainties for natural language processing(NLP) tasks. With empirical experiments on sentiment analysis, named entityrecognition, and language modeling using convolutional and recurrent neuralnetwork models, we show that explicitly modeling uncertainties is not onlynecessary to measure output confidence levels, but also useful at enhancingmodel performances in various NLP tasks.

DOLORES: Deep Contextualized Knowledge Graph Embeddings

  We introduce a new method DOLORES for learning knowledge graph embeddingsthat effectively captures contextual cues and dependencies among entities andrelations. First, we note that short paths on knowledge graphs comprising ofchains of entities and relations can encode valuable information regardingtheir contextual usage. We operationalize this notion by representing knowledgegraphs not as a collection of triples but as a collection of entity-relationchains, and learn embeddings for entities and relations using deep neuralmodels that capture such contextual usage. In particular, our model is based onBi-Directional LSTMs and learn deep representations of entities and relationsfrom constructed entity-relation chains. We show that these representations canvery easily be incorporated into existing models to significantly advance thestate of the art on several knowledge graph prediction tasks like linkprediction, triple classification, and missing relation type prediction (insome cases by at least 9.5%).

Sentence Embedding Alignment for Lifelong Relation Extraction

  Conventional approaches to relation extraction usually require a fixed set ofpre-defined relations. Such requirement is hard to meet in many realapplications, especially when new data and relations are emerging incessantlyand it is computationally expensive to store all data and re-train the wholemodel every time new data and relations come in. We formulate such achallenging problem as lifelong relation extraction and investigatememory-efficient incremental learning methods without catastrophicallyforgetting knowledge learned from previous tasks. We first investigate amodified version of the stochastic gradient methods with a replay memory, whichsurprisingly outperforms recent state-of-the-art lifelong learning methods. Wefurther propose to improve this approach to alleviate the forgetting problem byanchoring the sentence embedding space. Specifically, we utilize an explicitalignment model to mitigate the sentence embedding distortion of the learnedmodel when training on new data and new relations. Experiment results onmultiple benchmarks show that our proposed method significantly outperforms thestate-of-the-art lifelong learning approaches.

Riemannian Normalizing Flow on Variational Wasserstein Autoencoder for  Text Modeling

  Recurrent Variational Autoencoder has been widely used for language modelingand text generation tasks. These models often face a difficult optimizationproblem, also known as the Kullback-Leibler (KL) term vanishing issue, wherethe posterior easily collapses to the prior, and the model will ignore latentcodes in generative tasks. To address this problem, we introduce an improvedWasserstein Variational Autoencoder (WAE) with Riemannian Normalizing Flow(RNF) for text modeling. The RNF transforms a latent variable into a space thatrespects the geometric characteristics of input space, which makes posteriorimpossible to collapse to the non-informative prior. The Wasserstein objectiveminimizes the distance between the marginal distribution and the prior directlyand therefore does not force the posterior to match the prior. Empiricalexperiments show that our model avoids KL vanishing over a range of datasetsand has better performances in tasks such as language modeling, likelihoodapproximation, and text generation. Through a series of experiments andanalysis over latent space, we show that our model learns latent distributionsthat respect latent space geometry and is able to generate sentences that aremore diverse.

The physical constraints on a new LoBAL QSO at z=4.82

  Very few low-ionization broad absorption line (LoBAL) QSOs have been found athigh redshifts to date. One high-redshift LoBAL QSO, J0122+1216, was recentlydiscovered at the Lijiang 2.4-m Telescope with an initial redshiftdetermination of 4.76. Aiming to investigate its physical properties, wecarried out follow-up observations in the optical and near-IR spectroscopy.Near-IR spectra from UKIRT and P200 confirms that it is a LoBAL, with a newredshift determination of $4.82\pm0.01$ based on the \mgii~ emission-line. Thenew \mgii~ redshift determination reveals strong blueshifts and asymmetry ofthe high-ionization emission lines. We estimated a black hole mass of $\sim2.3\times 10^9 M_\odot$ and Eddington ratio of $\sim 1.0$ according to theempirical \mgii-based single-epoch relation and bolometric correction factor.It is possible that strong outflows are the result of an extreme quasarenvironment driven by the high Eddington ratio. A lower limit on the outflowingkinetic power ($>0.9\% L_{Edd}$) was derived from both emission and absorptionlines, indicating these outflows play a significant role in the feedbackprocess to regulate the growth of its black hole as well as host galaxyevolution.

Supernova Remnant Crossing a Density Jump: A Thin Shell Model

  The environments of supernova explosion are often inhomogeneous and there maybe jumps in their density structure. We have developed a semi-analytic modelunder the thin-shell approximation for supernova remnants that evolve crossinga density jump in the ambient medium. The generic evolutionary relations arepresented for the blast wave after impacting on a cavity wall, which may beproduced by the energetic stellar wind from the supernova progenitor. Therelations can also be extended to the case that the blast waves break out froma dense cloud if different density contrast is used. This model is applied toN132D, a well-known cavity-born supernova remnant whose evolution has not yetbeen quantitively estimated in a cavity scenario due to lack of model formulae,and self-consistent physical parameters are obtained.

Rough matroids based on coverings

  The introduction of covering-based rough sets has made a substantialcontribution to the classical rough sets. However, many vital problems in roughsets, including attribution reduction, are NP-hard and therefore the algorithmsfor solving them are usually greedy. Matroid, as a generalization of linearindependence in vector spaces, it has a variety of applications in many fieldssuch as algorithm design and combinatorial optimization. An excellentintroduction to the topic of rough matroids is due to Zhu and Wang. On thebasis of their work, we study the rough matroids based on coverings in thispaper. First, we investigate some properties of the definable sets with respectto a covering. Specifically, it is interesting that the set of all definablesets with respect to a covering, equipped with the binary relation of inclusion$\subseteq$, constructs a lattice. Second, we propose the rough matroids basedon coverings, which are a generalization of the rough matroids based onrelations. Finally, some properties of rough matroids based on coverings areexplored. Moreover, an equivalent formulation of rough matroids based oncoverings is presented. These interesting and important results exhibit manypotential connections between rough sets and matroids.

Electric field control of deterministic current-induced magnetization  switching in a hybrid ferromagnetic/ferroelectric structure

  All-electrical and programmable manipulations of ferromagnetic bits arehighly pursued for the aim of high integration and low energy consumption inmodern information technology. Methods based on the spin-orbit torque switchingin heavy metal/ferromagnet structures have been proposed with magnetic field,and recently are heading toward deterministic switching without externalmagnetic field. Here we demonstrate that an in-plane effective magnetic fieldcan be induced by an electric field without breaking the symmetry of thestructure of the thin film, and realize the deterministic magnetizationswitching in a hybrid ferromagnetic/ferroelectric structure with Pt/Co/Ni/Co/Ptlayers on PMN-PT substrate. The effective magnetic field can be reversed bychanging the direction of the applied electric field on the PMN-PT substrate,which fully replaces the controllability function of the external magneticfield. The electric field is found to generate an additional spin-orbit torqueon the CoNiCo magnets, which is confirmed by macrospin calculations.

Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning  for Vision-Language Navigation

  Vision-language navigation (VLN) is the task of navigating an embodied agentto carry out natural language instructions inside real 3D environments. In thispaper, we study how to address three critical challenges for this task: thecross-modal grounding, the ill-posed feedback, and the generalization problems.First, we propose a novel Reinforced Cross-Modal Matching (RCM) approach thatenforces cross-modal grounding both locally and globally via reinforcementlearning (RL). Particularly, a matching critic is used to provide an intrinsicreward to encourage global matching between instructions and trajectories, anda reasoning navigator is employed to perform cross-modal grounding in the localvisual scene. Evaluation on a VLN benchmark dataset shows that our RCM modelsignificantly outperforms previous methods by 10% on SPL and achieves the newstate-of-the-art performance. To improve the generalizability of the learnedpolicy, we further introduce a Self-Supervised Imitation Learning (SIL) methodto explore unseen environments by imitating its own past, good decisions. Wedemonstrate that SIL can approximate a better and more efficient policy, whichtremendously minimizes the success rate performance gap between seen and unseenenvironments (from 30.7% to 11.7%).

VATEX: A Large-Scale, High-Quality Multilingual Dataset for  Video-and-Language Research

  We present a new large-scale multilingual video description dataset, VATEX,which contains over 41,250 videos and 825,000 captions in both English andChinese. Among the captions, there are over 206,000 English-Chinese paralleltranslation pairs. Compared to the widely-used MSR-VTT dataset, VATEX ismultilingual, larger, linguistically complex, and more diverse in terms of bothvideo and natural language descriptions. We also introduce two tasks forvideo-and-language research based on VATEX: (1) Multilingual Video Captioning,aimed at describing a video in various languages with a compact unifiedcaptioning model, and (2) Video-guided Machine Translation, to translate asource language description into the target language using the videoinformation as additional spatiotemporal context. Extensive experiments on theVATEX dataset show that, first, the unified multilingual model can not onlyproduce both English and Chinese descriptions for a video more efficiently, butalso offer improved performance over the monolingual models. Furthermore, wedemonstrate that the spatiotemporal video context can be effectively utilizedto align source and target languages and thus assist machine translation. Inthe end, we discuss the potentials of using VATEX for other video-and-languageresearch.

IcoRating: A Deep-Learning System for Scam ICO Identification

  Cryptocurrencies (or digital tokens, digital currencies, e.g., BTC, ETH, XRP,NEO) have been rapidly gaining ground in use, value, and understanding amongthe public, bringing astonishing profits to investors. Unlike other money andbanking systems, most digital tokens do not require central authorities. Beingdecentralized poses significant challenges for credit rating. Most ICOs arecurrently not subject to government regulations, which makes a reliable creditrating system for ICO projects necessary and urgent.  In this paper, we introduce IcoRating, the first learning--basedcryptocurrency rating system. We exploit natural-language processing techniquesto analyze various aspects of 2,251 digital currencies to date, such as whitepaper content, founding teams, Github repositories, websites, etc. Supervisedlearning models are used to correlate the life span and the price change ofcryptocurrencies with these features. For the best setting, the proposed systemis able to identify scam ICO projects with 0.83 precision.  We hope this work will help investors identify scam ICOs and attract moreefforts in automatically evaluating and analyzing ICO projects.

Extract and Edit: An Alternative to Back-Translation for Unsupervised  Neural Machine Translation

  The overreliance on large parallel corpora significantly limits theapplicability of machine translation systems to the majority of language pairs.Back-translation has been dominantly used in previous approaches forunsupervised neural machine translation, where pseudo sentence pairs aregenerated to train the models with a reconstruction loss. However, the pseudosentences are usually of low quality as translation errors accumulate duringtraining. To avoid this fundamental issue, we propose an alternative but moreeffective approach, extract-edit, to extract and then edit real sentences fromthe target monolingual corpora. Furthermore, we introduce a comparativetranslation loss to evaluate the translated target sentences and thus train theunsupervised translation systems. Experiments show that the proposed approachconsistently outperforms the previous state-of-the-art unsupervised machinetranslation systems across two benchmarks (English-French and English-German)and two low-resource language pairs (English-Romanian and English-Russian) bymore than 2 (up to 3.63) BLEU points.

SDSS J013127.34$-$032100.1: A newly discovered radio-loud quasar at  $z=5.18$ with extremely high luminosity

  Only very few z>5 quasars discovered to date are radio-loud, with aradio-to-optical flux ratio (radio-loudness parameter) higher than 10. Here wereport the discovery of an optically luminous radio-loud quasar, SDSSJ013127.34-032100.1 (J0131-0321 in short), at z=5.18+-0.01 using the Lijiang2.4m and Magellan telescopes. J0131-0321 has a spectral energy distributionconsistent with that of radio-loud quasars. With an i-band magnitude of 18.47and radio flux density of 33 mJy, its radio-loudness parameter is ~100. Theoptical and near-infrared spectra taken by Magellan enable us to estimate itsbolometric luminosity to be L_bol ~ 1.1E48 erg/s, approximately 4.5 timesgreater than that of the most distant quasar known to date. The black hole massof J0131-0321 is estimated to be 2.7E9 solar masses, with an uncertainty up to0.4 dex. Detailed physical properties of this high-redshift, radio-loud,potentially super-Eddington quasar can be probed in the future with morededicated and intensive follow-up observations using multi-wavelengthfacilities.

Mapping Dirac Quasiparticles near a Single Coulomb Impurity on Graphene

  The response of Dirac fermions to a Coulomb potential is predicted to differsignificantly from the behavior of non-relativistic electrons seen intraditional atomic and impurity systems. Surprisingly, many key theoreticalpredictions for this ultra-relativistic regime have yet to be tested in alaboratory. Graphene, a 2D material in which electrons behave like masslessDirac fermions, provides a unique opportunity to experimentally test suchpredictions. The response of Dirac fermions to a Coulomb potential in grapheneis central to a wide range of electronic phenomena and can serve as a sensitiveprobe of graphene's intrinsic dielectric constant, the primary factordetermining the strength of electron-electron interactions in this material.Here we present a direct measurement of the nanoscale response of Diracfermions to a single Coulomb potential placed on a gated graphene device.Scanning tunneling microscopy and spectroscopy were used to fabricate tunablecharge impurities on graphene and to measure how they are screened by Diracfermions for a Q = +1|e| impurity charge state. Electron-like and hole-likeDirac fermions were observed to respond very differently to tunable Coulombpotentials. Comparison of this electron-hole asymmetry to theoreticalsimulations has allowed us to test basic predictions for the behavior of Diracfermions near a Coulomb potential and to extract the intrinsic dielectricconstant of graphene: {\epsilon}_g= 3.0 \pm 1.0. This small value of{\epsilon}_g indicates that microscopic electron-electron interactions cancontribute significantly to graphene properties.

Mapping Circumstellar Matter with Polarized Light - The Case of  Supernova 2014J in M82

  Optical polarimetry is an effective way of probing the environment ofsupernova for dust. We acquired linear HST ACS/WFC polarimetry in bands$F475W$, $F606W$, and $F775W$ of the supernova (SN) 2014J in M82 at six epochsfrom $\sim$277 days to $\sim$1181 days after the $B$-band maximum. Thepolarization measured at day 277 shows conspicuous deviations from otherepochs. These differences can be attributed to at least $\sim$ 10$^{-6}M_{\odot}$ of circumstellar dust located at a distance of $\sim5\times10^{17}$cm from the SN. The scattering dust grains revealed by these observations seemto be aligned with the dust in the interstellar medium that is responsible forthe large reddening towards the supernova. The presence of this circumstellardust sets strong constraints on the progenitor system that led to the explosionof SN\,2014J; however, it cannot discriminate between single- anddouble-degenerate models.

Quantifying Inactive Lithium in Lithium Metal Batteries

  Inactive lithium (Li) formation is the immediate cause of capacity loss andcatastrophic failure of Li metal batteries. However, the chemical component andthe atomic level structure of inactive Li have rarely been studied due to thelack of effective diagnosis tools to accurately differentiate and quantify Li+in solid electrolyte interphase (SEI) components and the electrically isolatedunreacted metallic Li0, which together comprise the inactive Li. Here, byintroducing a new analytical method, Titration Gas Chromatography (TGC), we canaccurately quantify the contribution from metallic Li0 to the total amount ofinactive Li. We uncover that the Li0, rather than the electrochemically formedSEI, dominates the inactive Li and capacity loss. Using cryogenic electronmicroscopies to further study the microstructure and nanostructure of inactiveLi, we find that the Li0 is surrounded by insulating SEI, losing the electronicconductive pathway to the bulk electrode. Coupling the measurements of the Li0global content to observations of its local atomic structure, we reveal theformation mechanism of inactive Li in different types of electrolytes, andidentify the true underlying cause of low Coulombic efficiency in Li metaldeposition and stripping. We ultimately propose strategies to enable the highlyefficient Li deposition and stripping to enable Li metal anode for nextgeneration high energy batteries.

Letter of Intent: Jinping Neutrino Experiment

  Jinping Neutrino Experiment (Jinping) is proposed to significantly improvemeasurements on solar neutrinos and geoneutrinos in China Jinping Laboratory -a lab with a number of unparalleled features, thickest overburden, lowestreactor neutrino background, etc., which identify it as the world-bestlow-energy neutrino laboratory. The proposed experiment will have target massof 4 kilotons of liquid scintillator or water-based liquid scintillator, with afiducial mass of 2 kilotons for neutrino-electron scattering events and 3kilotons for inverse-beta interaction events. A number of initial sensitivitiesstudies have been carried out, including on the transition phase for the solarneutrinos oscillation from the vacuum to the matter effect, the discovery ofsolar neutrinos from the carbon-nitrogen-oxygen (CNO) cycle, the resolution ofthe high and low metallicity hypotheses, and the unambiguous separation on Uand Th cascade decays from the dominant crustal anti-electron neutrinos inChina.

Learning to Compose Topic-Aware Mixture of Experts for Zero-Shot Video  Captioning

  Although promising results have been achieved in video captioning, existingmodels are limited to the fixed inventory of activities in the training corpus,and do not generalize to open vocabulary scenarios. Here we introduce a noveltask, zero-shot video captioning, that aims at describing out-of-domain videosof unseen activities. Videos of different activities usually require differentcaptioning strategies in many aspects, i.e. word selection, semanticconstruction, and style expression etc, which poses a great challenge to depictnovel activities without paired training data. But meanwhile, similaractivities share some of those aspects in common. Therefore, We propose aprincipled Topic-Aware Mixture of Experts (TAMoE) model for zero-shot videocaptioning, which learns to compose different experts based on different topicembeddings, implicitly transferring the knowledge learned from seen activitiesto unseen ones. Besides, we leverage external topic-related text corpus toconstruct the topic embedding for each activity, which embodies the mostrelevant semantic vectors within the topic. Empirical results not only validatethe effectiveness of our method in utilizing semantic knowledge for videocaptioning, but also show its strong generalization ability when describingnovel activities.

Late-time Flattening of Type Ia Supernova Light Curves: Constraints From  SN 2014J in M82

  The very nearby Type Ia supernova 2014J in M82 offers a rare opportunity tostudy the physics of thermonuclear supernovae at extremely late phases($\gtrsim$800 days). Using the Hubble Space Telescope (HST), we obtained sixepochs of high precision photometry for SN 2014J from 277 days to 1181 dayspast the $B-$band maximum light. The reprocessing of electrons and X-raysemitted by the radioactive decay chain $^{57}$Co$\rightarrow ^{57}$Fe areneeded to explain the significant flattening of both the $F606W$-band and thepseudo-bolometric light curves. The flattening confirms previous predictionsthat the late-time evolution of type Ia supernova luminosities requiresadditional energy input from the decay of $^{57}$Co (Seitenzahl et al. 2009).By assuming the $F606W$-band luminosity scales with the bolometric luminosityat $\sim$500 days after the $B-$band maximum light, a mass ratio$^{57}$Ni/$^{56}$Ni$\sim$0.065$_{-0.004}^{+0.005}$ is required. This mass ratiois roughly $\sim$3 times the solar ratio and favors a progenitor white dwarfwith a mass near the Chandrasekhar limit. A similar fit using the constructedpseudo-bolometric luminosity gives a mass ratio$^{57}$Ni/$^{56}$Ni$\sim$0.066$_{-0.008}^{+0.009}$. Astrometric tests based onthe multi-epoch HST ACS/WFC images reveal no significant circumstellar lightechoes in between 0.3 pc and 100 pc (Yang et al. 2017) from the supernova.

Waterproofed Photomultiplier Tube Assemblies for the Daya Bay Reactor  Neutrino Experiment

  In the Daya Bay Reactor Neutrino Experiment 960 20-cm-diameter waterproofphotomultiplier tubes are used to instrument three water pools as Cherenkovdetectors for detecting cosmic-ray muons. Of these 960 photomultiplier tubes,341 are recycled from the MACRO experiment. A systematic program was undertakento refurbish them as waterproof assemblies. In the context of passing the waterleakage check, a success rate better than 97% was achieved. Details of thedesign, fabrication, testing, operation, and performance of these waterproofedphotomultiplier-tube assemblies are presented.

Observing Atomic Collapse Resonances in Artificial Nuclei on Graphene

  Relativistic quantum mechanics predicts that when the charge of a superheavyatomic nucleus surpasses a certain threshold, the resulting strong Coulombfield causes an unusual atomic collapse state; this state exhibits an electronwave function component that falls toward the nucleus, as well as a positroncomponent that escapes to infinity. In graphene, where charge carriers behaveas massless relativistic particles, it has been predicted that highly chargedimpurities should exhibit resonances corresponding to these atomic collapsestates. We have observed the formation of such resonances around artificialnuclei (clusters of charged calcium dimers) fabricated on gated graphenedevices via atomic manipulation with a scanning tunneling microscope. Theenergy and spatial dependence of the atomic collapse state measured withscanning tunneling microscopy revealed unexpected behavior when occupied byelectrons.

DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning

  We study the problem of learning to reason in large scale knowledge graphs(KGs). More specifically, we describe a novel reinforcement learning frameworkfor learning multi-hop relational paths: we use a policy-based agent withcontinuous states based on knowledge graph embeddings, which reasons in a KGvector space by sampling the most promising relation to extend its path. Incontrast to prior work, our approach includes a reward function that takes theaccuracy, diversity, and efficiency into consideration. Experimentally, we showthat our proposed method outperforms a path-ranking based algorithm andknowledge graph embedding methods on Freebase and Never-Ending LanguageLearning datasets.

Deep Residual Learning for Weakly-Supervised Relation Extraction

  Deep residual learning (ResNet) is a new method for training very deep neuralnetworks using identity map-ping for shortcut connections. ResNet has won theImageNet ILSVRC 2015 classification task, and achieved state-of-the-artperformances in many computer vision tasks. However, the effect of residuallearning on noisy natural language processing tasks is still not wellunderstood. In this paper, we design a novel convolutional neural network (CNN)with residual learning, and investigate its impacts on the task of distantlysupervised noisy relation extraction. In contradictory to popular beliefs thatResNet only works well for very deep networks, we found that even with 9 layersof CNNs, using identity mapping could significantly improve the performance fordistantly-supervised relation extraction.

Experimental unconditionally secure covert communication in dense  wavelength-division multiplexing networks

  Covert communication offers a method to transmit messages in such a way thatit is not possible to detect that the communication is happening at all. Inthis work, we report an experimental demonstration of covert communication thatis provably secure against unbounded quantum adversaries. The covertcommunication is carried out over 10 km of optical fiber, addressing thechallenges associated with transmission over metropolitan distances. We deploythe protocol in a dense wavelength-division multiplexing infrastructure, whereour system has to coexist with a co-propagating C-band classical channel. Thenoise from the classical channel allows us to perform covert communication in aneighbouring channel. We perform an optimization of all protocol parameters andreport the transmission of three different messages with varying levels ofsecurity. Our results showcase the feasibility of secure covert communicationin a practical setting, with several possible future improvements from boththeory and experiment.

MojiTalk: Generating Emotional Responses at Scale

  Generating emotional language is a key step towards building empatheticnatural language processing agents. However, a major challenge for this line ofresearch is the lack of large-scale labeled training data, and previous studiesare limited to only small sets of human annotated sentiment labels.Additionally, explicitly controlling the emotion and sentiment of generatedtext is also difficult. In this paper, we take a more radical approach: weexploit the idea of leveraging Twitter data that are naturally labeled withemojis. More specifically, we collect a large corpus of Twitter conversationsthat include emojis in the response, and assume the emojis convey theunderlying emotions of the sentence. We then introduce a reinforced conditionalvariational encoder approach to train a deep generative model on theseconversations, which allows us to use emojis to control the emotion of thegenerated text. Experimentally, we show in our quantitative and qualitativeanalyses that the proposed models can successfully generate high-qualityabstractive conversation responses in accordance with designated emotions.

Ray: A Distributed Framework for Emerging AI Applications

  The next generation of AI applications will continuously interact with theenvironment and learn from these interactions. These applications impose newand demanding systems requirements, both in terms of performance andflexibility. In this paper, we consider these requirements and present Ray---adistributed system to address them. Ray implements a unified interface that canexpress both task-parallel and actor-based computations, supported by a singledynamic execution engine. To meet the performance requirements, Ray employs adistributed scheduler and a distributed and fault-tolerant store to manage thesystem's control state. In our experiments, we demonstrate scaling beyond 1.8million tasks per second and better performance than existing specializedsystems for several challenging reinforcement learning applications.

Leveraging Intra-User and Inter-User Representation Learning for  Automated Hate Speech Detection

  Hate speech detection is a critical, yet challenging problem in NaturalLanguage Processing (NLP). Despite the existence of numerous studies dedicatedto the development of NLP hate speech detection approaches, the accuracy isstill poor. The central problem is that social media posts are short and noisy,and most existing hate speech detection solutions take each post as an isolatedinput instance, which is likely to yield high false positive and negativerates. In this paper, we radically improve automated hate speech detection bypresenting a novel model that leverages intra-user and inter-userrepresentation learning for robust hate speech detection on Twitter. Inaddition to the target Tweet, we collect and analyze the user's historicalposts to model intra-user Tweet representations. To suppress the noise in asingle Tweet, we also model the similar Tweets posted by all other users withreinforced inter-user representation learning techniques. Experimentally, weshow that leveraging these two representations can significantly improve thef-score of a strong bidirectional LSTM baseline model by 10.1%.

Reinforced Co-Training

  Co-training is a popular semi-supervised learning framework to utilize alarge amount of unlabeled data in addition to a small labeled set. Co-trainingmethods exploit predicted labels on the unlabeled data and select samples basedon prediction confidence to augment the training. However, the selection ofsamples in existing co-training methods is based on a predetermined policy,which ignores the sampling bias between the unlabeled and the labeled subsets,and fails to explore the data space. In this paper, we propose a novel method,Reinforced Co-Training, to select high-quality unlabeled samples to betterco-train on. More specifically, our approach uses Q-learning to learn a dataselection policy with a small labeled dataset, and then exploits this policy totrain the co-training classifiers automatically. Experimental results onclickbait detection and generic text classification tasks demonstrate that ourproposed method can obtain more accurate text classification results.

Robust Distant Supervision Relation Extraction via Deep Reinforcement  Learning

  Distant supervision has become the standard method for relation extraction.However, even though it is an efficient method, it does not come at nocost---The resulted distantly-supervised training samples are often very noisy.To combat the noise, most of the recent state-of-the-art approaches focus onselecting one-best sentence or calculating soft attention weights over the setof the sentences of one specific entity pair. However, these methods aresuboptimal, and the false positive problem is still a key stumbling bottleneckfor the performance. We argue that those incorrectly-labeled candidatesentences must be treated with a hard decision, rather than being dealt withsoft attention weights. To do this, our paper describes a radical solution---Weexplore a deep reinforcement learning strategy to generate the false-positiveindicator, where we automatically recognize false positives for each relationtype without any supervised information. Unlike the removal operation in theprevious studies, we redistribute them into the negative examples. Theexperimental results show that the proposed strategy significantly improves theperformance of distant supervision comparing to state-of-the-art systems.

DSGAN: Generative Adversarial Training for Distant Supervision Relation  Extraction

  Distant supervision can effectively label data for relation extraction, butsuffers from the noise labeling problem. Recent works mainly perform softbag-level noise reduction strategies to find the relatively better samples in asentence bag, which is suboptimal compared with making a hard decision of falsepositive samples in sentence level. In this paper, we introduce an adversariallearning framework, which we named DSGAN, to learn a sentence-leveltrue-positive generator. Inspired by Generative Adversarial Networks, we regardthe positive samples generated by the generator as the negative samples totrain the discriminator. The optimal generator is obtained until thediscrimination ability of the discriminator has the greatest decline. We adoptthe generator to filter distant supervision training dataset and redistributethe false positive instances into the negative set, in which way to provide acleaned dataset for relation classification. The experimental results show thatthe proposed strategy significantly improves the performance of distantsupervision relation extraction comparing to state-of-the-art systems.

Long short-term memory networks in memristor crossbars

  Recent breakthroughs in recurrent deep neural networks with long short-termmemory (LSTM) units has led to major advances in artificial intelligence.State-of-the-art LSTM models with significantly increased complexity and alarge number of parameters, however, have a bottleneck in computing powerresulting from limited memory capacity and data communication bandwidth. Herewe demonstrate experimentally that LSTM can be implemented with a memristorcrossbar, which has a small circuit footprint to store a large number ofparameters and in-memory computing capability that circumvents the 'von Neumannbottleneck'. We illustrate the capability of our system by solving real-worldproblems in regression and classification, which shows that memristor LSTM is apromising low-power and low-latency hardware platform for edge inference.

Deep Reinforcement Learning for Chinese Zero pronoun Resolution

  Deep neural network models for Chinese zero pronoun resolution learn semanticinformation for zero pronoun and candidate antecedents, but tend to beshort-sighted---they often make local decisions. They typically predictcoreference chains between the zero pronoun and one single candidate antecedentone link at a time, while overlooking their long-term influence on futuredecisions. Ideally, modeling useful information of preceding potentialantecedents is critical when later predicting zero pronoun-candidate antecedentpairs. In this study, we show how to integrate local and global decision-makingby exploiting deep reinforcement learning models. With the help of thereinforcement learning agent, our model learns the policy of selectingantecedents in a sequential manner, where useful information provided byearlier predicted antecedents could be utilized for making later coreferencedecisions. Experimental results on OntoNotes 5.0 dataset show that ourtechnique surpasses the state-of-the-art models.

Scheduled Policy Optimization for Natural Language Communication with  Intelligent Agents

  We investigate the task of learning to follow natural language instructionsby jointly reasoning with visual observations and language inputs. In contrastto existing methods which start with learning from demonstrations (LfD) andthen use reinforcement learning (RL) to fine-tune the model parameters, wepropose a novel policy optimization algorithm which dynamically schedulesdemonstration learning and RL. The proposed training paradigm providesefficient exploration and better generalization beyond existing methods.Comparing to existing ensemble models, the best single model based on ourproposed method tremendously decreases the execution error by over 50% on ablock-world environment. To further illustrate the exploration strategy of ourRL algorithm, We also include systematic studies on the evolution of policyentropy during training.

One-Shot Relational Learning for Knowledge Graphs

  Knowledge graphs (KGs) are the key components of various natural languageprocessing applications. To further expand KGs' coverage, previous studies onknowledge graph completion usually require a large number of training instancesfor each relation. However, we observe that long-tail relations are actuallymore common in KGs and those newly added relations often do not have many knowntriples for training. In this work, we aim at predicting new facts under achallenging setting where only one training instance is available. We propose aone-shot relational learning framework, which utilizes the knowledge extractedby embedding models and learns a matching metric by considering both thelearned embeddings and one-hop graph structures. Empirically, our model yieldsconsiderable performance improvements over existing embedding models, and alsoeliminates the need of re-training the embedding models when dealing with newlyadded relations.

