Learning to Explain Non-Standard English Words and Phrases

  We describe a data-driven approach for automatically explaining new,
non-standard English expressions in a given sentence, building on a large
dataset that includes 15 years of crowdsourced examples from
UrbanDictionary.com. Unlike prior studies that focus on matching keywords from
a slang dictionary, we investigate the possibility of learning a neural
sequence-to-sequence model that generates explanations of unseen non-standard
English expressions given context. We propose a dual encoder approach---a
word-level encoder learns the representation of context, and a second
character-level encoder to learn the hidden representation of the target
non-standard expression. Our model can produce reasonable definitions of new
non-standard English expressions given their context with certain confidence.


Programming with Personalized PageRank: A Locally Groundable First-Order
  Probabilistic Logic

  In many probabilistic first-order representation systems, inference is
performed by "grounding"---i.e., mapping it to a propositional representation,
and then performing propositional inference. With a large database of facts,
groundings can be very large, making inference and learning computationally
expensive. Here we present a first-order probabilistic language which is
well-suited to approximate "local" grounding: every query $Q$ can be
approximately grounded with a small graph. The language is an extension of
stochastic logic programs where inference is performed by a variant of
personalized PageRank. Experimentally, we show that the approach performs well
without weight learning on an entity resolution task; that supervised
weight-learning improves accuracy; and that grounding time is independent of DB
size. We also show that order-of-magnitude speedups are possible by
parallelizing learning.


Recurrence Relations for Strongly q-Log-Convex Polynomials

  We consider a class of strongly q-log-convex polynomials based on a
triangular recurrence relation with linear coefficients, and we show that the
Bell polynomials, the Bessel polynomials, the Ramanujan polynomials and the
Dowling polynomials are strongly q-log-convex. We also prove that the Bessel
transformation preserves log-convexity.


Spin-orbit torque in Pt/CoNiCo/Pt symmetric devices

  Current induced magnetization switching by spin-orbit torques offers an
energy-efficient means of writing information in heavy metal/ferromagnet (FM)
multilayer systems. The relative contributions of field-like torques and
damping-like torques to the magnetization switching induced by the electrical
current are still under debate. Here, we describe a device based on a symmetric
Pt/FM/Pt structure, in which we demonstrate a strong damping-like torque from
the spin Hall effect and unmeasurable field-like torque from Rashba effect. The
spin-orbit effective fields due to the spin Hall effect were investigated
quantitatively and were found to be consistent with the switching effective
fields after accounting for the switching current reduction due to thermal
fluctuations from the current pulse. A non-linear dependence of deterministic
switching of average Mz on the in-plane magnetic field was revealed, which
could be explained and understood by micromagnetic simulation.


"Liar, Liar Pants on Fire": A New Benchmark Dataset for Fake News
  Detection

  Automatic fake news detection is a challenging problem in deception
detection, and it has tremendous real-world political and social impacts.
However, statistical approaches to combating fake news has been dramatically
limited by the lack of labeled benchmark datasets. In this paper, we present
liar: a new, publicly available dataset for fake news detection. We collected a
decade-long, 12.8K manually labeled short statements in various contexts from
PolitiFact.com, which provides detailed analysis report and links to source
documents for each case. This dataset can be used for fact-checking research as
well. Notably, this new dataset is an order of magnitude larger than previously
largest public fake news datasets of similar type. Empirically, we investigate
automatic fake news detection based on surface-level linguistic patterns. We
have designed a novel, hybrid convolutional neural network to integrate
meta-data with text. We show that this hybrid approach can improve a text-only
deep learning model.


The q-Log-convexity of the Generating Functions of the Squares of
  Binomial Coefficients

  We prove a conjecture of Liu and Wang on the q-log-convexity of the
polynomial sequence $\{\sum_{k=0}^n{n\choose k}^2q^k\}_{n\geq 0}$. By using
Pieri's rule and the Jacobi-Trudi identity for Schur functions, we obtain an
expansion of a sum of products of elementary symmetric functions in terms of
Schur functions with nonnegative coefficients. Then the principal
specialization leads to the q-log-convexity. We also prove that a technical
condition of Liu and Wang holds for the squares of the binomial coefficients.
Hence we deduce that the linear transformation with respect to the triangular
array $\{{n\choose k}^2\}_{0\leq k\leq n}$ is log-convexity preserving.


Efficient Inference and Learning in a Large Knowledge Base: Reasoning
  with Extracted Information using a Locally Groundable First-Order
  Probabilistic Logic

  One important challenge for probabilistic logics is reasoning with very large
knowledge bases (KBs) of imperfect information, such as those produced by
modern web-scale information extraction systems. One scalability problem shared
by many probabilistic logics is that answering queries involves "grounding" the
query---i.e., mapping it to a propositional representation---and the size of a
"grounding" grows with database size. To address this bottleneck, we present a
first-order probabilistic language called ProPPR in which that approximate
"local groundings" can be constructed in time independent of database size.
Technically, ProPPR is an extension to stochastic logic programs (SLPs) that is
biased towards short derivations; it is also closely related to an earlier
relational learning algorithm called the path ranking algorithm (PRA). We show
that the problem of constructing proofs for this logic is related to
computation of personalized PageRank (PPR) on a linearized version of the proof
space, and using on this connection, we develop a proveably-correct approximate
grounding scheme, based on the PageRank-Nibble algorithm. Building on this, we
develop a fast and easily-parallelized weight-learning algorithm for ProPPR. In
experiments, we show that learning for ProPPR is orders magnitude faster than
learning for Markov logic networks; that allowing mutual recursion (joint
learning) in KB inference leads to improvements in performance; and that ProPPR
can learn weights for a mutually recursive program with hundreds of clauses,
which define scores of interrelated predicates, over a KB containing one
million entities.


Schur Positivity and the $q$-Log-convexity of the Narayana Polynomials

  Using Schur positivity and the principal specialization of Schur functions,
we provide a proof of a recent conjecture of Liu and Wang on the
$q$-log-convexity of the Narayana polynomials, and a proof of the second
conjecture that the Narayana transformation preserves the log-convexity. Based
on a formula of Br\"and$\mathrm{\acute{e}}$n which expresses the $q$-Narayana
numbers as the specializations of Schur functions, we derive several symmetric
function identities using the Littlewood-Richardson rule for the product of
Schur functions, and obtain the strong $q$-log-convexity of the Narayana
polynomials and the strong $q$-log-concavity of the $q$-Narayana numbers.


Video Captioning via Hierarchical Reinforcement Learning

  Video captioning is the task of automatically generating a textual
description of the actions in a video. Although previous work (e.g.
sequence-to-sequence model) has shown promising results in abstracting a coarse
description of a short video, it is still very challenging to caption a video
containing multiple fine-grained actions with a detailed description. This
paper aims to address the challenge by proposing a novel hierarchical
reinforcement learning framework for video captioning, where a high-level
Manager module learns to design sub-goals and a low-level Worker module
recognizes the primitive actions to fulfill the sub-goal. With this
compositional framework to reinforce video captioning at different levels, our
approach significantly outperforms all the baseline methods on a newly
introduced large-scale dataset for fine-grained video captioning. Furthermore,
our non-ensemble model has already achieved the state-of-the-art results on the
widely-used MSR-VTT dataset.


Look Before You Leap: Bridging Model-Free and Model-Based Reinforcement
  Learning for Planned-Ahead Vision-and-Language Navigation

  Existing research studies on vision and language grounding for robot
navigation focus on improving model-free deep reinforcement learning (DRL)
models in synthetic environments. However, model-free DRL models do not
consider the dynamics in the real-world environments, and they often fail to
generalize to new scenes. In this paper, we take a radical approach to bridge
the gap between synthetic studies and real-world practices---We propose a
novel, planned-ahead hybrid reinforcement learning model that combines
model-free and model-based reinforcement learning to solve a real-world
vision-language navigation task. Our look-ahead module tightly integrates a
look-ahead policy model with an environment model that predicts the next state
and the reward. Experimental results suggest that our proposed method
significantly outperforms the baselines and achieves the best on the real-world
Room-to-Room dataset. Moreover, our scalable method is more generalizable when
transferring to unseen environments.


Watch, Listen, and Describe: Globally and Locally Aligned Cross-Modal
  Attentions for Video Captioning

  A major challenge for video captioning is to combine audio and visual cues.
Existing multi-modal fusion methods have shown encouraging results in video
understanding. However, the temporal structures of multiple modalities at
different granularities are rarely explored, and how to selectively fuse the
multi-modal representations at different levels of details remains uncharted.
In this paper, we propose a novel hierarchically aligned cross-modal attention
(HACA) framework to learn and selectively fuse both global and local temporal
dynamics of different modalities. Furthermore, for the first time, we validate
the superior performance of the deep audio features on the video captioning
task. Finally, our HACA model significantly outperforms the previous best
systems and achieves new state-of-the-art results on the widely used MSR-VTT
dataset.


No Metrics Are Perfect: Adversarial Reward Learning for Visual
  Storytelling

  Though impressive results have been achieved in visual captioning, the task
of generating abstract stories from photo streams is still a little-tapped
problem. Different from captions, stories have more expressive language styles
and contain many imaginary concepts that do not appear in the images. Thus it
poses challenges to behavioral cloning algorithms. Furthermore, due to the
limitations of automatic metrics on evaluating story quality, reinforcement
learning methods with hand-crafted rewards also face difficulties in gaining an
overall performance boost. Therefore, we propose an Adversarial REward Learning
(AREL) framework to learn an implicit reward function from human
demonstrations, and then optimize policy search with the learned reward
function. Though automatic eval- uation indicates slight performance boost over
state-of-the-art (SOTA) methods in cloning expert behaviors, human evaluation
shows that our approach achieves significant improvement in generating more
human-like stories than SOTA systems.


Simple Models for Word Formation in English Slang

  We propose generative models for three types of extra-grammatical word
formation phenomena abounding in English slang: Blends, Clippings, and
Reduplicatives. Adopting a data-driven approach coupled with linguistic
knowledge, we propose simple models with state of the art performance on human
annotated gold standard datasets. Overall, our models reveal insights into the
generative processes of word formation in slang -- insights which are
increasingly relevant in the context of the rising prevalence of slang and
non-standard varieties on the Internet.


Hierarchical CVAE for Fine-Grained Hate Speech Classification

  Existing work on automated hate speech detection typically focuses on binary
classification or on differentiating among a small set of categories. In this
paper, we propose a novel method on a fine-grained hate speech classification
task, which focuses on differentiating among 40 hate groups of 13 different
hate group categories. We first explore the Conditional Variational Autoencoder
(CVAE) as a discriminative model and then extend it to a hierarchical
architecture to utilize the additional hate category information for more
accurate prediction. Experimentally, we show that incorporating the hate
category information for training can significantly improve the classification
performance and our proposed model outperforms commonly-used discriminative
models.


Analyzing and Interpreting Convolutional Neural Networks in NLP

  Convolutional neural networks have been successfully applied to various NLP
tasks. However, it is not obvious whether they model different linguistic
patterns such as negation, intensification, and clause compositionality to help
the decision-making process. In this paper, we apply visualization techniques
to observe how the model can capture different linguistic features and how
these features can affect the performance of the model. Later on, we try to
identify the model errors and their sources. We believe that interpreting CNNs
is the first step to understand the underlying semantic features which can
raise awareness to further improve the performance and explainability of CNN
models.


Dirichlet Variational Autoencoder for Text Modeling

  We introduce an improved variational autoencoder (VAE) for text modeling with
topic information explicitly modeled as a Dirichlet latent variable. By
providing the proposed model topic awareness, it is more superior at
reconstructing input texts. Furthermore, due to the inherent interactions
between the newly introduced Dirichlet variable and the conventional
multivariate Gaussian variable, the model is less prone to KL divergence
vanishing. We derive the variational lower bound for the new model and conduct
experiments on four different data sets. The results show that the proposed
model is superior at text reconstruction across the latent space and
classifications on learned representations have higher test accuracies.


Quantifying Uncertainties in Natural Language Processing Tasks

  Reliable uncertainty quantification is a first step towards building
explainable, transparent, and accountable artificial intelligent systems.
Recent progress in Bayesian deep learning has made such quantification
realizable. In this paper, we propose novel methods to study the benefits of
characterizing model and data uncertainties for natural language processing
(NLP) tasks. With empirical experiments on sentiment analysis, named entity
recognition, and language modeling using convolutional and recurrent neural
network models, we show that explicitly modeling uncertainties is not only
necessary to measure output confidence levels, but also useful at enhancing
model performances in various NLP tasks.


DOLORES: Deep Contextualized Knowledge Graph Embeddings

  We introduce a new method DOLORES for learning knowledge graph embeddings
that effectively captures contextual cues and dependencies among entities and
relations. First, we note that short paths on knowledge graphs comprising of
chains of entities and relations can encode valuable information regarding
their contextual usage. We operationalize this notion by representing knowledge
graphs not as a collection of triples but as a collection of entity-relation
chains, and learn embeddings for entities and relations using deep neural
models that capture such contextual usage. In particular, our model is based on
Bi-Directional LSTMs and learn deep representations of entities and relations
from constructed entity-relation chains. We show that these representations can
very easily be incorporated into existing models to significantly advance the
state of the art on several knowledge graph prediction tasks like link
prediction, triple classification, and missing relation type prediction (in
some cases by at least 9.5%).


Sentence Embedding Alignment for Lifelong Relation Extraction

  Conventional approaches to relation extraction usually require a fixed set of
pre-defined relations. Such requirement is hard to meet in many real
applications, especially when new data and relations are emerging incessantly
and it is computationally expensive to store all data and re-train the whole
model every time new data and relations come in. We formulate such a
challenging problem as lifelong relation extraction and investigate
memory-efficient incremental learning methods without catastrophically
forgetting knowledge learned from previous tasks. We first investigate a
modified version of the stochastic gradient methods with a replay memory, which
surprisingly outperforms recent state-of-the-art lifelong learning methods. We
further propose to improve this approach to alleviate the forgetting problem by
anchoring the sentence embedding space. Specifically, we utilize an explicit
alignment model to mitigate the sentence embedding distortion of the learned
model when training on new data and new relations. Experiment results on
multiple benchmarks show that our proposed method significantly outperforms the
state-of-the-art lifelong learning approaches.


Riemannian Normalizing Flow on Variational Wasserstein Autoencoder for
  Text Modeling

  Recurrent Variational Autoencoder has been widely used for language modeling
and text generation tasks. These models often face a difficult optimization
problem, also known as the Kullback-Leibler (KL) term vanishing issue, where
the posterior easily collapses to the prior, and the model will ignore latent
codes in generative tasks. To address this problem, we introduce an improved
Wasserstein Variational Autoencoder (WAE) with Riemannian Normalizing Flow
(RNF) for text modeling. The RNF transforms a latent variable into a space that
respects the geometric characteristics of input space, which makes posterior
impossible to collapse to the non-informative prior. The Wasserstein objective
minimizes the distance between the marginal distribution and the prior directly
and therefore does not force the posterior to match the prior. Empirical
experiments show that our model avoids KL vanishing over a range of datasets
and has better performances in tasks such as language modeling, likelihood
approximation, and text generation. Through a series of experiments and
analysis over latent space, we show that our model learns latent distributions
that respect latent space geometry and is able to generate sentences that are
more diverse.


The physical constraints on a new LoBAL QSO at z=4.82

  Very few low-ionization broad absorption line (LoBAL) QSOs have been found at
high redshifts to date. One high-redshift LoBAL QSO, J0122+1216, was recently
discovered at the Lijiang 2.4-m Telescope with an initial redshift
determination of 4.76. Aiming to investigate its physical properties, we
carried out follow-up observations in the optical and near-IR spectroscopy.
Near-IR spectra from UKIRT and P200 confirms that it is a LoBAL, with a new
redshift determination of $4.82\pm0.01$ based on the \mgii~ emission-line. The
new \mgii~ redshift determination reveals strong blueshifts and asymmetry of
the high-ionization emission lines. We estimated a black hole mass of $\sim
2.3\times 10^9 M_\odot$ and Eddington ratio of $\sim 1.0$ according to the
empirical \mgii-based single-epoch relation and bolometric correction factor.
It is possible that strong outflows are the result of an extreme quasar
environment driven by the high Eddington ratio. A lower limit on the outflowing
kinetic power ($>0.9\% L_{Edd}$) was derived from both emission and absorption
lines, indicating these outflows play a significant role in the feedback
process to regulate the growth of its black hole as well as host galaxy
evolution.


Supernova Remnant Crossing a Density Jump: A Thin Shell Model

  The environments of supernova explosion are often inhomogeneous and there may
be jumps in their density structure. We have developed a semi-analytic model
under the thin-shell approximation for supernova remnants that evolve crossing
a density jump in the ambient medium. The generic evolutionary relations are
presented for the blast wave after impacting on a cavity wall, which may be
produced by the energetic stellar wind from the supernova progenitor. The
relations can also be extended to the case that the blast waves break out from
a dense cloud if different density contrast is used. This model is applied to
N132D, a well-known cavity-born supernova remnant whose evolution has not yet
been quantitively estimated in a cavity scenario due to lack of model formulae,
and self-consistent physical parameters are obtained.


Rough matroids based on coverings

  The introduction of covering-based rough sets has made a substantial
contribution to the classical rough sets. However, many vital problems in rough
sets, including attribution reduction, are NP-hard and therefore the algorithms
for solving them are usually greedy. Matroid, as a generalization of linear
independence in vector spaces, it has a variety of applications in many fields
such as algorithm design and combinatorial optimization. An excellent
introduction to the topic of rough matroids is due to Zhu and Wang. On the
basis of their work, we study the rough matroids based on coverings in this
paper. First, we investigate some properties of the definable sets with respect
to a covering. Specifically, it is interesting that the set of all definable
sets with respect to a covering, equipped with the binary relation of inclusion
$\subseteq$, constructs a lattice. Second, we propose the rough matroids based
on coverings, which are a generalization of the rough matroids based on
relations. Finally, some properties of rough matroids based on coverings are
explored. Moreover, an equivalent formulation of rough matroids based on
coverings is presented. These interesting and important results exhibit many
potential connections between rough sets and matroids.


Electric field control of deterministic current-induced magnetization
  switching in a hybrid ferromagnetic/ferroelectric structure

  All-electrical and programmable manipulations of ferromagnetic bits are
highly pursued for the aim of high integration and low energy consumption in
modern information technology. Methods based on the spin-orbit torque switching
in heavy metal/ferromagnet structures have been proposed with magnetic field,
and recently are heading toward deterministic switching without external
magnetic field. Here we demonstrate that an in-plane effective magnetic field
can be induced by an electric field without breaking the symmetry of the
structure of the thin film, and realize the deterministic magnetization
switching in a hybrid ferromagnetic/ferroelectric structure with Pt/Co/Ni/Co/Pt
layers on PMN-PT substrate. The effective magnetic field can be reversed by
changing the direction of the applied electric field on the PMN-PT substrate,
which fully replaces the controllability function of the external magnetic
field. The electric field is found to generate an additional spin-orbit torque
on the CoNiCo magnets, which is confirmed by macrospin calculations.


Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning
  for Vision-Language Navigation

  Vision-language navigation (VLN) is the task of navigating an embodied agent
to carry out natural language instructions inside real 3D environments. In this
paper, we study how to address three critical challenges for this task: the
cross-modal grounding, the ill-posed feedback, and the generalization problems.
First, we propose a novel Reinforced Cross-Modal Matching (RCM) approach that
enforces cross-modal grounding both locally and globally via reinforcement
learning (RL). Particularly, a matching critic is used to provide an intrinsic
reward to encourage global matching between instructions and trajectories, and
a reasoning navigator is employed to perform cross-modal grounding in the local
visual scene. Evaluation on a VLN benchmark dataset shows that our RCM model
significantly outperforms previous methods by 10% on SPL and achieves the new
state-of-the-art performance. To improve the generalizability of the learned
policy, we further introduce a Self-Supervised Imitation Learning (SIL) method
to explore unseen environments by imitating its own past, good decisions. We
demonstrate that SIL can approximate a better and more efficient policy, which
tremendously minimizes the success rate performance gap between seen and unseen
environments (from 30.7% to 11.7%).


VATEX: A Large-Scale, High-Quality Multilingual Dataset for
  Video-and-Language Research

  We present a new large-scale multilingual video description dataset, VATEX,
which contains over 41,250 videos and 825,000 captions in both English and
Chinese. Among the captions, there are over 206,000 English-Chinese parallel
translation pairs. Compared to the widely-used MSR-VTT dataset, VATEX is
multilingual, larger, linguistically complex, and more diverse in terms of both
video and natural language descriptions. We also introduce two tasks for
video-and-language research based on VATEX: (1) Multilingual Video Captioning,
aimed at describing a video in various languages with a compact unified
captioning model, and (2) Video-guided Machine Translation, to translate a
source language description into the target language using the video
information as additional spatiotemporal context. Extensive experiments on the
VATEX dataset show that, first, the unified multilingual model can not only
produce both English and Chinese descriptions for a video more efficiently, but
also offer improved performance over the monolingual models. Furthermore, we
demonstrate that the spatiotemporal video context can be effectively utilized
to align source and target languages and thus assist machine translation. In
the end, we discuss the potentials of using VATEX for other video-and-language
research.


IcoRating: A Deep-Learning System for Scam ICO Identification

  Cryptocurrencies (or digital tokens, digital currencies, e.g., BTC, ETH, XRP,
NEO) have been rapidly gaining ground in use, value, and understanding among
the public, bringing astonishing profits to investors. Unlike other money and
banking systems, most digital tokens do not require central authorities. Being
decentralized poses significant challenges for credit rating. Most ICOs are
currently not subject to government regulations, which makes a reliable credit
rating system for ICO projects necessary and urgent.
  In this paper, we introduce IcoRating, the first learning--based
cryptocurrency rating system. We exploit natural-language processing techniques
to analyze various aspects of 2,251 digital currencies to date, such as white
paper content, founding teams, Github repositories, websites, etc. Supervised
learning models are used to correlate the life span and the price change of
cryptocurrencies with these features. For the best setting, the proposed system
is able to identify scam ICO projects with 0.83 precision.
  We hope this work will help investors identify scam ICOs and attract more
efforts in automatically evaluating and analyzing ICO projects.


Extract and Edit: An Alternative to Back-Translation for Unsupervised
  Neural Machine Translation

  The overreliance on large parallel corpora significantly limits the
applicability of machine translation systems to the majority of language pairs.
Back-translation has been dominantly used in previous approaches for
unsupervised neural machine translation, where pseudo sentence pairs are
generated to train the models with a reconstruction loss. However, the pseudo
sentences are usually of low quality as translation errors accumulate during
training. To avoid this fundamental issue, we propose an alternative but more
effective approach, extract-edit, to extract and then edit real sentences from
the target monolingual corpora. Furthermore, we introduce a comparative
translation loss to evaluate the translated target sentences and thus train the
unsupervised translation systems. Experiments show that the proposed approach
consistently outperforms the previous state-of-the-art unsupervised machine
translation systems across two benchmarks (English-French and English-German)
and two low-resource language pairs (English-Romanian and English-Russian) by
more than 2 (up to 3.63) BLEU points.


SDSS J013127.34$-$032100.1: A newly discovered radio-loud quasar at
  $z=5.18$ with extremely high luminosity

  Only very few z>5 quasars discovered to date are radio-loud, with a
radio-to-optical flux ratio (radio-loudness parameter) higher than 10. Here we
report the discovery of an optically luminous radio-loud quasar, SDSS
J013127.34-032100.1 (J0131-0321 in short), at z=5.18+-0.01 using the Lijiang
2.4m and Magellan telescopes. J0131-0321 has a spectral energy distribution
consistent with that of radio-loud quasars. With an i-band magnitude of 18.47
and radio flux density of 33 mJy, its radio-loudness parameter is ~100. The
optical and near-infrared spectra taken by Magellan enable us to estimate its
bolometric luminosity to be L_bol ~ 1.1E48 erg/s, approximately 4.5 times
greater than that of the most distant quasar known to date. The black hole mass
of J0131-0321 is estimated to be 2.7E9 solar masses, with an uncertainty up to
0.4 dex. Detailed physical properties of this high-redshift, radio-loud,
potentially super-Eddington quasar can be probed in the future with more
dedicated and intensive follow-up observations using multi-wavelength
facilities.


Mapping Dirac Quasiparticles near a Single Coulomb Impurity on Graphene

  The response of Dirac fermions to a Coulomb potential is predicted to differ
significantly from the behavior of non-relativistic electrons seen in
traditional atomic and impurity systems. Surprisingly, many key theoretical
predictions for this ultra-relativistic regime have yet to be tested in a
laboratory. Graphene, a 2D material in which electrons behave like massless
Dirac fermions, provides a unique opportunity to experimentally test such
predictions. The response of Dirac fermions to a Coulomb potential in graphene
is central to a wide range of electronic phenomena and can serve as a sensitive
probe of graphene's intrinsic dielectric constant, the primary factor
determining the strength of electron-electron interactions in this material.
Here we present a direct measurement of the nanoscale response of Dirac
fermions to a single Coulomb potential placed on a gated graphene device.
Scanning tunneling microscopy and spectroscopy were used to fabricate tunable
charge impurities on graphene and to measure how they are screened by Dirac
fermions for a Q = +1|e| impurity charge state. Electron-like and hole-like
Dirac fermions were observed to respond very differently to tunable Coulomb
potentials. Comparison of this electron-hole asymmetry to theoretical
simulations has allowed us to test basic predictions for the behavior of Dirac
fermions near a Coulomb potential and to extract the intrinsic dielectric
constant of graphene: {\epsilon}_g= 3.0 \pm 1.0. This small value of
{\epsilon}_g indicates that microscopic electron-electron interactions can
contribute significantly to graphene properties.


Mapping Circumstellar Matter with Polarized Light - The Case of
  Supernova 2014J in M82

  Optical polarimetry is an effective way of probing the environment of
supernova for dust. We acquired linear HST ACS/WFC polarimetry in bands
$F475W$, $F606W$, and $F775W$ of the supernova (SN) 2014J in M82 at six epochs
from $\sim$277 days to $\sim$1181 days after the $B$-band maximum. The
polarization measured at day 277 shows conspicuous deviations from other
epochs. These differences can be attributed to at least $\sim$ 10$^{-6}
M_{\odot}$ of circumstellar dust located at a distance of $\sim5\times10^{17}$
cm from the SN. The scattering dust grains revealed by these observations seem
to be aligned with the dust in the interstellar medium that is responsible for
the large reddening towards the supernova. The presence of this circumstellar
dust sets strong constraints on the progenitor system that led to the explosion
of SN\,2014J; however, it cannot discriminate between single- and
double-degenerate models.


Quantifying Inactive Lithium in Lithium Metal Batteries

  Inactive lithium (Li) formation is the immediate cause of capacity loss and
catastrophic failure of Li metal batteries. However, the chemical component and
the atomic level structure of inactive Li have rarely been studied due to the
lack of effective diagnosis tools to accurately differentiate and quantify Li+
in solid electrolyte interphase (SEI) components and the electrically isolated
unreacted metallic Li0, which together comprise the inactive Li. Here, by
introducing a new analytical method, Titration Gas Chromatography (TGC), we can
accurately quantify the contribution from metallic Li0 to the total amount of
inactive Li. We uncover that the Li0, rather than the electrochemically formed
SEI, dominates the inactive Li and capacity loss. Using cryogenic electron
microscopies to further study the microstructure and nanostructure of inactive
Li, we find that the Li0 is surrounded by insulating SEI, losing the electronic
conductive pathway to the bulk electrode. Coupling the measurements of the Li0
global content to observations of its local atomic structure, we reveal the
formation mechanism of inactive Li in different types of electrolytes, and
identify the true underlying cause of low Coulombic efficiency in Li metal
deposition and stripping. We ultimately propose strategies to enable the highly
efficient Li deposition and stripping to enable Li metal anode for next
generation high energy batteries.


Letter of Intent: Jinping Neutrino Experiment

  Jinping Neutrino Experiment (Jinping) is proposed to significantly improve
measurements on solar neutrinos and geoneutrinos in China Jinping Laboratory -
a lab with a number of unparalleled features, thickest overburden, lowest
reactor neutrino background, etc., which identify it as the world-best
low-energy neutrino laboratory. The proposed experiment will have target mass
of 4 kilotons of liquid scintillator or water-based liquid scintillator, with a
fiducial mass of 2 kilotons for neutrino-electron scattering events and 3
kilotons for inverse-beta interaction events. A number of initial sensitivities
studies have been carried out, including on the transition phase for the solar
neutrinos oscillation from the vacuum to the matter effect, the discovery of
solar neutrinos from the carbon-nitrogen-oxygen (CNO) cycle, the resolution of
the high and low metallicity hypotheses, and the unambiguous separation on U
and Th cascade decays from the dominant crustal anti-electron neutrinos in
China.


Learning to Compose Topic-Aware Mixture of Experts for Zero-Shot Video
  Captioning

  Although promising results have been achieved in video captioning, existing
models are limited to the fixed inventory of activities in the training corpus,
and do not generalize to open vocabulary scenarios. Here we introduce a novel
task, zero-shot video captioning, that aims at describing out-of-domain videos
of unseen activities. Videos of different activities usually require different
captioning strategies in many aspects, i.e. word selection, semantic
construction, and style expression etc, which poses a great challenge to depict
novel activities without paired training data. But meanwhile, similar
activities share some of those aspects in common. Therefore, We propose a
principled Topic-Aware Mixture of Experts (TAMoE) model for zero-shot video
captioning, which learns to compose different experts based on different topic
embeddings, implicitly transferring the knowledge learned from seen activities
to unseen ones. Besides, we leverage external topic-related text corpus to
construct the topic embedding for each activity, which embodies the most
relevant semantic vectors within the topic. Empirical results not only validate
the effectiveness of our method in utilizing semantic knowledge for video
captioning, but also show its strong generalization ability when describing
novel activities.


Late-time Flattening of Type Ia Supernova Light Curves: Constraints From
  SN 2014J in M82

  The very nearby Type Ia supernova 2014J in M82 offers a rare opportunity to
study the physics of thermonuclear supernovae at extremely late phases
($\gtrsim$800 days). Using the Hubble Space Telescope (HST), we obtained six
epochs of high precision photometry for SN 2014J from 277 days to 1181 days
past the $B-$band maximum light. The reprocessing of electrons and X-rays
emitted by the radioactive decay chain $^{57}$Co$\rightarrow ^{57}$Fe are
needed to explain the significant flattening of both the $F606W$-band and the
pseudo-bolometric light curves. The flattening confirms previous predictions
that the late-time evolution of type Ia supernova luminosities requires
additional energy input from the decay of $^{57}$Co (Seitenzahl et al. 2009).
By assuming the $F606W$-band luminosity scales with the bolometric luminosity
at $\sim$500 days after the $B-$band maximum light, a mass ratio
$^{57}$Ni/$^{56}$Ni$\sim$0.065$_{-0.004}^{+0.005}$ is required. This mass ratio
is roughly $\sim$3 times the solar ratio and favors a progenitor white dwarf
with a mass near the Chandrasekhar limit. A similar fit using the constructed
pseudo-bolometric luminosity gives a mass ratio
$^{57}$Ni/$^{56}$Ni$\sim$0.066$_{-0.008}^{+0.009}$. Astrometric tests based on
the multi-epoch HST ACS/WFC images reveal no significant circumstellar light
echoes in between 0.3 pc and 100 pc (Yang et al. 2017) from the supernova.


Waterproofed Photomultiplier Tube Assemblies for the Daya Bay Reactor
  Neutrino Experiment

  In the Daya Bay Reactor Neutrino Experiment 960 20-cm-diameter waterproof
photomultiplier tubes are used to instrument three water pools as Cherenkov
detectors for detecting cosmic-ray muons. Of these 960 photomultiplier tubes,
341 are recycled from the MACRO experiment. A systematic program was undertaken
to refurbish them as waterproof assemblies. In the context of passing the water
leakage check, a success rate better than 97% was achieved. Details of the
design, fabrication, testing, operation, and performance of these waterproofed
photomultiplier-tube assemblies are presented.


Observing Atomic Collapse Resonances in Artificial Nuclei on Graphene

  Relativistic quantum mechanics predicts that when the charge of a superheavy
atomic nucleus surpasses a certain threshold, the resulting strong Coulomb
field causes an unusual atomic collapse state; this state exhibits an electron
wave function component that falls toward the nucleus, as well as a positron
component that escapes to infinity. In graphene, where charge carriers behave
as massless relativistic particles, it has been predicted that highly charged
impurities should exhibit resonances corresponding to these atomic collapse
states. We have observed the formation of such resonances around artificial
nuclei (clusters of charged calcium dimers) fabricated on gated graphene
devices via atomic manipulation with a scanning tunneling microscope. The
energy and spatial dependence of the atomic collapse state measured with
scanning tunneling microscopy revealed unexpected behavior when occupied by
electrons.


DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning

  We study the problem of learning to reason in large scale knowledge graphs
(KGs). More specifically, we describe a novel reinforcement learning framework
for learning multi-hop relational paths: we use a policy-based agent with
continuous states based on knowledge graph embeddings, which reasons in a KG
vector space by sampling the most promising relation to extend its path. In
contrast to prior work, our approach includes a reward function that takes the
accuracy, diversity, and efficiency into consideration. Experimentally, we show
that our proposed method outperforms a path-ranking based algorithm and
knowledge graph embedding methods on Freebase and Never-Ending Language
Learning datasets.


Deep Residual Learning for Weakly-Supervised Relation Extraction

  Deep residual learning (ResNet) is a new method for training very deep neural
networks using identity map-ping for shortcut connections. ResNet has won the
ImageNet ILSVRC 2015 classification task, and achieved state-of-the-art
performances in many computer vision tasks. However, the effect of residual
learning on noisy natural language processing tasks is still not well
understood. In this paper, we design a novel convolutional neural network (CNN)
with residual learning, and investigate its impacts on the task of distantly
supervised noisy relation extraction. In contradictory to popular beliefs that
ResNet only works well for very deep networks, we found that even with 9 layers
of CNNs, using identity mapping could significantly improve the performance for
distantly-supervised relation extraction.


Experimental unconditionally secure covert communication in dense
  wavelength-division multiplexing networks

  Covert communication offers a method to transmit messages in such a way that
it is not possible to detect that the communication is happening at all. In
this work, we report an experimental demonstration of covert communication that
is provably secure against unbounded quantum adversaries. The covert
communication is carried out over 10 km of optical fiber, addressing the
challenges associated with transmission over metropolitan distances. We deploy
the protocol in a dense wavelength-division multiplexing infrastructure, where
our system has to coexist with a co-propagating C-band classical channel. The
noise from the classical channel allows us to perform covert communication in a
neighbouring channel. We perform an optimization of all protocol parameters and
report the transmission of three different messages with varying levels of
security. Our results showcase the feasibility of secure covert communication
in a practical setting, with several possible future improvements from both
theory and experiment.


MojiTalk: Generating Emotional Responses at Scale

  Generating emotional language is a key step towards building empathetic
natural language processing agents. However, a major challenge for this line of
research is the lack of large-scale labeled training data, and previous studies
are limited to only small sets of human annotated sentiment labels.
Additionally, explicitly controlling the emotion and sentiment of generated
text is also difficult. In this paper, we take a more radical approach: we
exploit the idea of leveraging Twitter data that are naturally labeled with
emojis. More specifically, we collect a large corpus of Twitter conversations
that include emojis in the response, and assume the emojis convey the
underlying emotions of the sentence. We then introduce a reinforced conditional
variational encoder approach to train a deep generative model on these
conversations, which allows us to use emojis to control the emotion of the
generated text. Experimentally, we show in our quantitative and qualitative
analyses that the proposed models can successfully generate high-quality
abstractive conversation responses in accordance with designated emotions.


Ray: A Distributed Framework for Emerging AI Applications

  The next generation of AI applications will continuously interact with the
environment and learn from these interactions. These applications impose new
and demanding systems requirements, both in terms of performance and
flexibility. In this paper, we consider these requirements and present Ray---a
distributed system to address them. Ray implements a unified interface that can
express both task-parallel and actor-based computations, supported by a single
dynamic execution engine. To meet the performance requirements, Ray employs a
distributed scheduler and a distributed and fault-tolerant store to manage the
system's control state. In our experiments, we demonstrate scaling beyond 1.8
million tasks per second and better performance than existing specialized
systems for several challenging reinforcement learning applications.


Leveraging Intra-User and Inter-User Representation Learning for
  Automated Hate Speech Detection

  Hate speech detection is a critical, yet challenging problem in Natural
Language Processing (NLP). Despite the existence of numerous studies dedicated
to the development of NLP hate speech detection approaches, the accuracy is
still poor. The central problem is that social media posts are short and noisy,
and most existing hate speech detection solutions take each post as an isolated
input instance, which is likely to yield high false positive and negative
rates. In this paper, we radically improve automated hate speech detection by
presenting a novel model that leverages intra-user and inter-user
representation learning for robust hate speech detection on Twitter. In
addition to the target Tweet, we collect and analyze the user's historical
posts to model intra-user Tweet representations. To suppress the noise in a
single Tweet, we also model the similar Tweets posted by all other users with
reinforced inter-user representation learning techniques. Experimentally, we
show that leveraging these two representations can significantly improve the
f-score of a strong bidirectional LSTM baseline model by 10.1%.


Reinforced Co-Training

  Co-training is a popular semi-supervised learning framework to utilize a
large amount of unlabeled data in addition to a small labeled set. Co-training
methods exploit predicted labels on the unlabeled data and select samples based
on prediction confidence to augment the training. However, the selection of
samples in existing co-training methods is based on a predetermined policy,
which ignores the sampling bias between the unlabeled and the labeled subsets,
and fails to explore the data space. In this paper, we propose a novel method,
Reinforced Co-Training, to select high-quality unlabeled samples to better
co-train on. More specifically, our approach uses Q-learning to learn a data
selection policy with a small labeled dataset, and then exploits this policy to
train the co-training classifiers automatically. Experimental results on
clickbait detection and generic text classification tasks demonstrate that our
proposed method can obtain more accurate text classification results.


Robust Distant Supervision Relation Extraction via Deep Reinforcement
  Learning

  Distant supervision has become the standard method for relation extraction.
However, even though it is an efficient method, it does not come at no
cost---The resulted distantly-supervised training samples are often very noisy.
To combat the noise, most of the recent state-of-the-art approaches focus on
selecting one-best sentence or calculating soft attention weights over the set
of the sentences of one specific entity pair. However, these methods are
suboptimal, and the false positive problem is still a key stumbling bottleneck
for the performance. We argue that those incorrectly-labeled candidate
sentences must be treated with a hard decision, rather than being dealt with
soft attention weights. To do this, our paper describes a radical solution---We
explore a deep reinforcement learning strategy to generate the false-positive
indicator, where we automatically recognize false positives for each relation
type without any supervised information. Unlike the removal operation in the
previous studies, we redistribute them into the negative examples. The
experimental results show that the proposed strategy significantly improves the
performance of distant supervision comparing to state-of-the-art systems.


DSGAN: Generative Adversarial Training for Distant Supervision Relation
  Extraction

  Distant supervision can effectively label data for relation extraction, but
suffers from the noise labeling problem. Recent works mainly perform soft
bag-level noise reduction strategies to find the relatively better samples in a
sentence bag, which is suboptimal compared with making a hard decision of false
positive samples in sentence level. In this paper, we introduce an adversarial
learning framework, which we named DSGAN, to learn a sentence-level
true-positive generator. Inspired by Generative Adversarial Networks, we regard
the positive samples generated by the generator as the negative samples to
train the discriminator. The optimal generator is obtained until the
discrimination ability of the discriminator has the greatest decline. We adopt
the generator to filter distant supervision training dataset and redistribute
the false positive instances into the negative set, in which way to provide a
cleaned dataset for relation classification. The experimental results show that
the proposed strategy significantly improves the performance of distant
supervision relation extraction comparing to state-of-the-art systems.


Long short-term memory networks in memristor crossbars

  Recent breakthroughs in recurrent deep neural networks with long short-term
memory (LSTM) units has led to major advances in artificial intelligence.
State-of-the-art LSTM models with significantly increased complexity and a
large number of parameters, however, have a bottleneck in computing power
resulting from limited memory capacity and data communication bandwidth. Here
we demonstrate experimentally that LSTM can be implemented with a memristor
crossbar, which has a small circuit footprint to store a large number of
parameters and in-memory computing capability that circumvents the 'von Neumann
bottleneck'. We illustrate the capability of our system by solving real-world
problems in regression and classification, which shows that memristor LSTM is a
promising low-power and low-latency hardware platform for edge inference.


Deep Reinforcement Learning for Chinese Zero pronoun Resolution

  Deep neural network models for Chinese zero pronoun resolution learn semantic
information for zero pronoun and candidate antecedents, but tend to be
short-sighted---they often make local decisions. They typically predict
coreference chains between the zero pronoun and one single candidate antecedent
one link at a time, while overlooking their long-term influence on future
decisions. Ideally, modeling useful information of preceding potential
antecedents is critical when later predicting zero pronoun-candidate antecedent
pairs. In this study, we show how to integrate local and global decision-making
by exploiting deep reinforcement learning models. With the help of the
reinforcement learning agent, our model learns the policy of selecting
antecedents in a sequential manner, where useful information provided by
earlier predicted antecedents could be utilized for making later coreference
decisions. Experimental results on OntoNotes 5.0 dataset show that our
technique surpasses the state-of-the-art models.


Scheduled Policy Optimization for Natural Language Communication with
  Intelligent Agents

  We investigate the task of learning to follow natural language instructions
by jointly reasoning with visual observations and language inputs. In contrast
to existing methods which start with learning from demonstrations (LfD) and
then use reinforcement learning (RL) to fine-tune the model parameters, we
propose a novel policy optimization algorithm which dynamically schedules
demonstration learning and RL. The proposed training paradigm provides
efficient exploration and better generalization beyond existing methods.
Comparing to existing ensemble models, the best single model based on our
proposed method tremendously decreases the execution error by over 50% on a
block-world environment. To further illustrate the exploration strategy of our
RL algorithm, We also include systematic studies on the evolution of policy
entropy during training.


One-Shot Relational Learning for Knowledge Graphs

  Knowledge graphs (KGs) are the key components of various natural language
processing applications. To further expand KGs' coverage, previous studies on
knowledge graph completion usually require a large number of training instances
for each relation. However, we observe that long-tail relations are actually
more common in KGs and those newly added relations often do not have many known
triples for training. In this work, we aim at predicting new facts under a
challenging setting where only one training instance is available. We propose a
one-shot relational learning framework, which utilizes the knowledge extracted
by embedding models and learns a matching metric by considering both the
learned embeddings and one-hop graph structures. Empirically, our model yields
considerable performance improvements over existing embedding models, and also
eliminates the need of re-training the embedding models when dealing with newly
added relations.


