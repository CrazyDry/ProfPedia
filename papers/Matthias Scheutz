The MacGyver Test - A Framework for Evaluating Machine Resourcefulness
  and Creative Problem Solving

  Current measures of machine intelligence are either difficult to evaluate or
lack the ability to test a robot's problem-solving capacity in open worlds. We
propose a novel evaluation framework based on the formal notion of MacGyver
Test which provides a practical way for assessing the resilience and
resourcefulness of artificial agents.


Interpretable Apprenticeship Learning with Temporal Logic Specifications

  Recent work has addressed using formulas in linear temporal logic (LTL) as
specifications for agents planning in Markov Decision Processes (MDPs). We
consider the inverse problem: inferring an LTL specification from demonstrated
behavior trajectories in MDPs. We formulate this as a multiobjective
optimization problem, and describe state-based ("what actually happened") and
action-based ("what the agent expected to happen") objective functions based on
a notion of "violation cost". We demonstrate the efficacy of the approach by
employing genetic programming to solve this problem in two simple domains.


Quasi-Dilemmas for Artificial Moral Agents

  In this paper we describe moral quasi-dilemmas (MQDs): situations similar to
moral dilemmas, but in which an agent is unsure whether exploring the plan
space or the world may reveal a course of action that satisfies all moral
requirements. We argue that artificial moral agents (AMAs) should be built to
handle MQDs (in particular, by exploring the plan space rather than immediately
accepting the inevitability of the moral dilemma), and that MQDs may be useful
for evaluating AMA architectures.


Enabling Basic Normative HRI in a Cognitive Robotic Architecture

  Collaborative human activities are grounded in social and moral norms, which
humans consciously and subconsciously use to guide and constrain their
decision-making and behavior, thereby strengthening their interactions and
preventing emotional and physical harm. This type of norm-based processing is
also critical for robots in many human-robot interaction scenarios (e.g., when
helping elderly and disabled persons in assisted living facilities, or
assisting humans in assembly tasks in factories or even the space station). In
this position paper, we will briefly describe how several components in an
integrated cognitive architecture can be used to implement processes that are
required for normative human-robot interactions, especially in collaborative
tasks where actions and situations could potentially be perceived as
threatening and thus need a change in course of action to mitigate the
perceived threats.


Norm Conflict Resolution in Stochastic Domains

  Artificial agents will need to be aware of human moral and social norms, and
able to use them in decision-making. In particular, artificial agents will need
a principled approach to managing conflicting norms, which are common in human
social interactions. Existing logic-based approaches suffer from normative
explosion and are typically designed for deterministic environments;
reward-based approaches lack principled ways of determining which normative
alternatives exist in a given environment. We propose a hybrid approach, using
Linear Temporal Logic (LTL) representations in Markov Decision Processes
(MDPs), that manages norm conflicts in a systematic manner while accommodating
domain stochasticity. We provide a proof-of-concept implementation in a
simulated vacuum cleaning domain.


AI Challenges in Human-Robot Cognitive Teaming

  Among the many anticipated roles for robots in the future is that of being a
human teammate. Aside from all the technological hurdles that have to be
overcome with respect to hardware and control to make robots fit to work with
humans, the added complication here is that humans have many conscious and
subconscious expectations of their teammates - indeed, we argue that teaming is
mostly a cognitive rather than physical coordination activity. This introduces
new challenges for the AI and robotics community and requires fundamental
changes to the traditional approach to the design of autonomy. With this in
mind, we propose an update to the classical view of the intelligent agent
architecture, highlighting the requirements for mental modeling of the human in
the deliberative process of the autonomous agent. In this article, we outline
briefly the recent efforts of ours, and others in the community, towards
developing cognitive teammates along these guidelines.


Augmenting Robot Knowledge Consultants with Distributed Short Term
  Memory

  Human-robot communication in situated environments involves a complex
interplay between knowledge representations across a wide variety of
modalities. Crucially, linguistic information must be associated with
representations of objects, locations, people, and goals, which may be
represented in very different ways. In previous work, we developed a Consultant
Framework that facilitates modality-agnostic access to information distributed
across a set of heterogeneously represented knowledge sources. In this work, we
draw inspiration from cognitive science to augment these distributed knowledge
sources with Short Term Memory Buffers to create an STM-augmented algorithm for
referring expression generation. We then discuss the potential performance
benefits of this approach and insights from cognitive science that may inform
future refinements in the design of our approach.


When Exceptions are the Norm: Exploring the Role of Consent in HRI

  HRI researchers have made major strides in developing robotic architectures
that are capable of reading a limited set of social cues and producing
behaviors that enhance their likeability and feeling of comfort amongst humans.
However, the cues in these models are fairly direct and the interactions
largely dyadic. To capture the normative qualities of interaction more
robustly, we propose consent as a distinct, critical area for HRI research.
Convening important insights in existing HRI work around topics like touch,
proxemics, gaze, and moral norms, the notion of consent reveals key
expectations that can shape how a robot acts in social space. By sorting
various kinds of consent through social and legal doctrine, we delineate
empirical and technical questions to meet consent challenges faced in major
application domains and robotic roles. Attention to consent could show, for
example, how extraordinary, norm-violating actions can be justified by agents
and accepted by those around them. We argue that operationalizing ideas from
legal scholarship can better guide how robotic systems might cultivate and
sustain proper forms of consent.


