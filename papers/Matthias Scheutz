The MacGyver Test - A Framework for Evaluating Machine Resourcefulness  and Creative Problem Solving

  Current measures of machine intelligence are either difficult to evaluate orlack the ability to test a robot's problem-solving capacity in open worlds. Wepropose a novel evaluation framework based on the formal notion of MacGyverTest which provides a practical way for assessing the resilience andresourcefulness of artificial agents.

Interpretable Apprenticeship Learning with Temporal Logic Specifications

  Recent work has addressed using formulas in linear temporal logic (LTL) asspecifications for agents planning in Markov Decision Processes (MDPs). Weconsider the inverse problem: inferring an LTL specification from demonstratedbehavior trajectories in MDPs. We formulate this as a multiobjectiveoptimization problem, and describe state-based ("what actually happened") andaction-based ("what the agent expected to happen") objective functions based ona notion of "violation cost". We demonstrate the efficacy of the approach byemploying genetic programming to solve this problem in two simple domains.

Quasi-Dilemmas for Artificial Moral Agents

  In this paper we describe moral quasi-dilemmas (MQDs): situations similar tomoral dilemmas, but in which an agent is unsure whether exploring the planspace or the world may reveal a course of action that satisfies all moralrequirements. We argue that artificial moral agents (AMAs) should be built tohandle MQDs (in particular, by exploring the plan space rather than immediatelyaccepting the inevitability of the moral dilemma), and that MQDs may be usefulfor evaluating AMA architectures.

Enabling Basic Normative HRI in a Cognitive Robotic Architecture

  Collaborative human activities are grounded in social and moral norms, whichhumans consciously and subconsciously use to guide and constrain theirdecision-making and behavior, thereby strengthening their interactions andpreventing emotional and physical harm. This type of norm-based processing isalso critical for robots in many human-robot interaction scenarios (e.g., whenhelping elderly and disabled persons in assisted living facilities, orassisting humans in assembly tasks in factories or even the space station). Inthis position paper, we will briefly describe how several components in anintegrated cognitive architecture can be used to implement processes that arerequired for normative human-robot interactions, especially in collaborativetasks where actions and situations could potentially be perceived asthreatening and thus need a change in course of action to mitigate theperceived threats.

Norm Conflict Resolution in Stochastic Domains

  Artificial agents will need to be aware of human moral and social norms, andable to use them in decision-making. In particular, artificial agents will needa principled approach to managing conflicting norms, which are common in humansocial interactions. Existing logic-based approaches suffer from normativeexplosion and are typically designed for deterministic environments;reward-based approaches lack principled ways of determining which normativealternatives exist in a given environment. We propose a hybrid approach, usingLinear Temporal Logic (LTL) representations in Markov Decision Processes(MDPs), that manages norm conflicts in a systematic manner while accommodatingdomain stochasticity. We provide a proof-of-concept implementation in asimulated vacuum cleaning domain.

AI Challenges in Human-Robot Cognitive Teaming

  Among the many anticipated roles for robots in the future is that of being ahuman teammate. Aside from all the technological hurdles that have to beovercome with respect to hardware and control to make robots fit to work withhumans, the added complication here is that humans have many conscious andsubconscious expectations of their teammates - indeed, we argue that teaming ismostly a cognitive rather than physical coordination activity. This introducesnew challenges for the AI and robotics community and requires fundamentalchanges to the traditional approach to the design of autonomy. With this inmind, we propose an update to the classical view of the intelligent agentarchitecture, highlighting the requirements for mental modeling of the human inthe deliberative process of the autonomous agent. In this article, we outlinebriefly the recent efforts of ours, and others in the community, towardsdeveloping cognitive teammates along these guidelines.

Augmenting Robot Knowledge Consultants with Distributed Short Term  Memory

  Human-robot communication in situated environments involves a complexinterplay between knowledge representations across a wide variety ofmodalities. Crucially, linguistic information must be associated withrepresentations of objects, locations, people, and goals, which may berepresented in very different ways. In previous work, we developed a ConsultantFramework that facilitates modality-agnostic access to information distributedacross a set of heterogeneously represented knowledge sources. In this work, wedraw inspiration from cognitive science to augment these distributed knowledgesources with Short Term Memory Buffers to create an STM-augmented algorithm forreferring expression generation. We then discuss the potential performancebenefits of this approach and insights from cognitive science that may informfuture refinements in the design of our approach.

When Exceptions are the Norm: Exploring the Role of Consent in HRI

  HRI researchers have made major strides in developing robotic architecturesthat are capable of reading a limited set of social cues and producingbehaviors that enhance their likeability and feeling of comfort amongst humans.However, the cues in these models are fairly direct and the interactionslargely dyadic. To capture the normative qualities of interaction morerobustly, we propose consent as a distinct, critical area for HRI research.Convening important insights in existing HRI work around topics like touch,proxemics, gaze, and moral norms, the notion of consent reveals keyexpectations that can shape how a robot acts in social space. By sortingvarious kinds of consent through social and legal doctrine, we delineateempirical and technical questions to meet consent challenges faced in majorapplication domains and robotic roles. Attention to consent could show, forexample, how extraordinary, norm-violating actions can be justified by agentsand accepted by those around them. We argue that operationalizing ideas fromlegal scholarship can better guide how robotic systems might cultivate andsustain proper forms of consent.

