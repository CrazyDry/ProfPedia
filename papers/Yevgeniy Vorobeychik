Simulation-Based Game Theoretic Analysis of Keyword Auctions with
  Low-Dimensional Bidding Strategies

  We perform a simulation-based analysis of keyword auctions modeled as
one-shot games of incomplete information to study a series of mechanism design
questions. Our first question addresses the degree to which incentive
compatibility fails in generalized second-price (GSP) auctions. Our results
suggest that sincere bidding in GSP auctions is a strikingly poor strategy and
a poor predictor of equilibrium outcomes. We next show that the rank-by-revenue
mechanism is welfare optimal, corroborating past results. Finally, we analyze
profit as a function of auction mechanism under a series of alternative
settings. Our conclusions coincide with those of Lahaie and Pennock [2007] when
values and quality scores are strongly positively correlated: in such a case,
rank-by-bid rules are clearly superior. We diverge, however, in showing that
auctions that put little weight on quality scores almost universally dominate
the pure rank-by-revenue scheme.


Constrained Automated Mechanism Design for Infinite Games of Incomplete
  Information

  We present a functional framework for automated mechanism design based on a
two-stage game model of strategic interaction between the designer and the
mechanism participants, and apply it to several classes of two-player infinite
games of incomplete information. At the core of our framework is a black-box
optimization algorithm which guides the selection process of candidate
mechanisms. Our approach yields optimal or nearly optimal mechanisms in several
application domains using various objective functions. By comparing our results
with known optimal mechanisms, and in some cases improving on the best known
mechanisms, we provide evidence that ours is a promising approach to parametric
design of indirect mechanisms.


Adversarial Regression for Detecting Attacks in Cyber-Physical Systems

  Attacks in cyber-physical systems (CPS) which manipulate sensor readings can
cause enormous physical damage if undetected. Detection of attacks on sensors
is crucial to mitigate this issue. We study supervised regression as a means to
detect anomalous sensor readings, where each sensor's measurement is predicted
as a function of other sensors. We show that several common learning approaches
in this context are still vulnerable to \emph{stealthy attacks}, which
carefully modify readings of compromised sensors to cause desired damage while
remaining undetected. Next, we model the interaction between the CPS defender
and attacker as a Stackelberg game in which the defender chooses detection
thresholds, while the attacker deploys a stealthy attack in response. We
present a heuristic algorithm for finding an approximately optimal threshold
for the defender in this game, and show that it increases system resilience to
attacks without significantly increasing the false alarm rate.


Optical Neural Networks

  We develop a novel optical neural network (ONN) framework which introduces a
degree of scalar invariance to image classification estima- tion. Taking a hint
from the human eye, which has higher resolution near the center of the retina,
images are broken out into multiple levels of varying zoom based on a focal
point. Each level is passed through an identical convolutional neural network
(CNN) in a Siamese fashion, and the results are recombined to produce a high
accuracy estimate of the object class. ONNs act as a wrapper around existing
CNNs, and can thus be applied to many existing algorithms to produce notable
accuracy improvements without having to change the underlying architecture.


Noncooperatively Optimized Tolerance: Decentralized Strategic
  Optimization in Complex Systems

  We introduce noncooperatively optimized tolerance (NOT), a generalization of
highly optimized tolerance (HOT) that involves strategic (game theoretic)
interactions between parties in a complex system. We illustrate our model in
the forest fire (percolation) framework. As the number of players increases,
our model retains features of HOT, such as robustness, high yield combined with
high density, and self-dissimilar landscapes, but also develops features of
self-organized criticality (SOC) when the number of players is large enough.
For example, the forest landscape becomes increasingly homogeneous and
protection from adverse events (lightning strikes) becomes less closely
correlated with the spatial distribution of these events. While HOT is a
special case of our model, the resemblance to SOC is only partial; for example,
the distribution of cascades, while becoming increasingly heavy-tailed as the
number of players increases, also deviates more significantly from a power law
in this regime. Surprisingly, the system retains considerable robustness even
as it becomes fractured, due in part to emergent cooperation between
neighboring players. At the same time, increasing homogeneity promotes
resilience against changes in the lightning distribution, giving rise to
intermediate regimes where the system is robust to a particular distribution of
adverse events, yet not very fragile to changes.


Influence and Dynamic Behavior in Random Boolean Networks

  We present a rigorous mathematical framework for analyzing dynamics of a
broad class of Boolean network models. We use this framework to provide the
first formal proof of many of the standard critical transition results in
Boolean network analysis, and offer analogous characterizations for novel
classes of random Boolean networks. We precisely connect the short-run dynamic
behavior of a Boolean network to the average influence of the transfer
functions. We show that some of the assumptions traditionally made in the more
common mean-field analysis of Boolean networks do not hold in general.
  For example, we offer some evidence that imbalance, or expected internal
inhomogeneity, of transfer functions is a crucial feature that tends to drive
quiescent behavior far more strongly than previously observed.


Computing Optimal Security Strategies for Interdependent Assets

  We introduce a novel framework for computing optimal randomized security
policies in networked domains which extends previous approaches in several
ways. First, we extend previous linear programming techniques for Stackelberg
security games to incorporate benefits and costs of arbitrary security
configurations on individual assets. Second, we offer a principled model of
failure cascades that allows us to capture both the direct and indirect value
of assets, and extend this model to capture uncertainty about the structure of
the interdependency network. Third, we extend the linear programming
formulation to account for exogenous (random) failures in addition to targeted
attacks. The goal of our work is two-fold. First, we aim to develop techniques
for computing optimal security strategies in realistic settings involving
interdependent security. To this end, we evaluate the value of our technical
contributions in comparison with previous approaches, and show that our
approach yields much better defense policies and scales to realistic graphs.
Second, our computational framework enables us to attain theoretical insights
about security on networks. As an example, we study how allowing security to be
endogenous impacts the relative resilience of different network topologies.


Characterizing short-term stability for Boolean networks over any
  distribution of transfer functions

  We present a characterization of short-term stability of random Boolean
networks under \emph{arbitrary} distributions of transfer functions. Given any
distribution of transfer functions for a random Boolean network, we present a
formula that decides whether short-term chaos (damage spreading) will happen.
We provide a formal proof for this formula, and empirically show that its
predictions are accurate. Previous work only works for special cases of
balanced families. It has been observed that these characterizations fail for
unbalanced families, yet such families are widespread in real biological
networks.


Vulnerability of Fixed-Time Control of Signalized Intersections to
  Cyber-Tampering

  Recent experimental studies have shown that traffic management systems are
vulnerable to cyber-attacks on sensor data. This paper studies the
vulnerability of fixed-time control of signalized intersections when sensors
measuring traffic flow information are compromised and perturbed by an
adversary. The problems are formulated by considering three malicious
objectives: 1) worst-case network accumulation, which aims to destabilize the
overall network as much as possible; 2) worst-case lane accumulation, which
aims to cause worst-case accumulation on some target lanes; and 3) risk-averse
target accumulation, which aims to reach a target accumulation by making the
minimum perturbation to sensor data. The problems are solved using bilevel
programming optimization methods. Finally, a case study of a real network is
used to illustrate the results.


Mechanism Design for Team Formation

  Team formation is a core problem in AI. Remarkably, little prior work has
addressed the problem of mechanism design for team formation, accounting for
the need to elicit agents' preferences over potential teammates. Coalition
formation in the related hedonic games has received much attention, but only
from the perspective of coalition stability, with little emphasis on the
mechanism design objectives of true preference elicitation, social welfare, and
equity. We present the first formal mechanism design framework for team
formation, building on recent combinatorial matching market design literature.
We exhibit four mechanisms for this problem, two novel, two simple extensions
of known mechanisms from other domains. Two of these (one new, one known) have
desirable theoretical properties. However, we use extensive experiments to show
our second novel mechanism, despite having no theoretical guarantees,
empirically achieves good incentive compatibility, welfare, and fairness.


A General Retraining Framework for Scalable Adversarial Classification

  Traditional classification algorithms assume that training and test data come
from similar distributions. This assumption is violated in adversarial
settings, where malicious actors modify instances to evade detection. A number
of custom methods have been developed for both adversarial evasion attacks and
robust learning. We propose the first systematic and general-purpose retraining
framework which can: a) boost robustness of an \emph{arbitrary} learning
algorithm, in the face of b) a broader class of adversarial models than any
prior methods. We show that, under natural conditions, the retraining framework
minimizes an upper bound on optimal adversarial risk, and show how to extend
this result to account for approximations of evasion attacks. Extensive
experimental evaluation demonstrates that our retraining methods are nearly
indistinguishable from state-of-the-art algorithms for optimizing adversarial
risk, but are more general and far more scalable. The experiments also confirm
that without retraining, our adversarial framework dramatically reduces the
effectiveness of learning. In contrast, retraining significantly boosts
robustness to evasion attacks without significantly compromising overall
accuracy.


Robust High-Dimensional Linear Regression

  The effectiveness of supervised learning techniques has made them ubiquitous
in research and practice. In high-dimensional settings, supervised learning
commonly relies on dimensionality reduction to improve performance and identify
the most important factors in predicting outcomes. However, the economic
importance of learning has made it a natural target for adversarial
manipulation of training data, which we term poisoning attacks. Prior
approaches to dealing with robust supervised learning rely on strong
assumptions about the nature of the feature matrix, such as feature
independence and sub-Gaussian noise with low variance. We propose an integrated
method for robust regression that relaxes these assumptions, assuming only that
the feature matrix can be well approximated by a low-rank matrix. Our
techniques integrate improved robust low-rank matrix approximation and robust
principle component regression, and yield strong performance guarantees.
Moreover, we experimentally show that our methods significantly outperform
state of the art both in running time and prediction error.


Data Poisoning Attacks on Factorization-Based Collaborative Filtering

  Recommendation and collaborative filtering systems are important in modern
information and e-commerce applications. As these systems are becoming
increasingly popular in the industry, their outputs could affect business
decision making, introducing incentives for an adversarial party to compromise
the availability or integrity of such systems. We introduce a data poisoning
attack on collaborative filtering systems. We demonstrate how a powerful
attacker with full knowledge of the learner can generate malicious data so as
to maximize his/her malicious objectives, while at the same time mimicking
normal user behavior to avoid being detected. While the complete knowledge
assumption seems extreme, it enables a robust assessment of the vulnerability
of collaborative filtering schemes to highly motivated attacks. We present
efficient solutions for two popular factorization-based collaborative filtering
algorithms: the \emph{alternative minimization} formulation and the
\emph{nuclear norm minimization} method. Finally, we test the effectiveness of
our proposed algorithms on real-world data and discuss potential defensive
strategies.


Empirically Grounded Agent-Based Models of Innovation Diffusion: A
  Critical Review

  Innovation diffusion has been studied extensively in a variety of
disciplines, including sociology, economics, marketing, ecology, and computer
science. Traditional literature on innovation diffusion has been dominated by
models of aggregate behavior and trends. However, the agent-based modeling
(ABM) paradigm is gaining popularity as it captures agent heterogeneity and
enables fine-grained modeling of interactions mediated by social and geographic
networks. While most ABM work on innovation diffusion is theoretical,
empirically grounded models are increasingly important, particularly in guiding
policy decisions. We present a critical review of empirically grounded
agent-based models of innovation diffusion, developing a categorization of this
research based on types of agent models as well as applications. By connecting
the modeling methodologies in the fields of information and innovation
diffusion, we suggest that the maximum likelihood estimation framework widely
used in the former is a promising paradigm for calibration of agent-based
models for innovation diffusion. Although many advances have been made to
standardize ABM methodology, we identify four major issues in model calibration
and validation, and suggest potential solutions.


Controlling Elections through Social Influence

  Election control considers the problem of an adversary who attempts to tamper
with a voting process, in order to either ensure that their favored candidate
wins (constructive control) or another candidate loses (destructive control).
As online social networks have become significant sources of information for
potential voters, a new tool in an attacker's arsenal is to effect control by
harnessing social influence, for example, by spreading fake news and other
forms of misinformation through online social media.
  We consider the computational problem of election control via social
influence, studying the conditions under which finding good adversarial
strategies is computationally feasible. We consider two objectives for the
adversary in both the constructive and destructive control settings:
probability and margin of victory (POV and MOV, respectively). We present
several strong negative results, showing, for example, that the problem of
maximizing POV is inapproximable for any constant factor. On the other hand, we
present approximation algorithms which provide somewhat weaker approximation
guarantees, such as bicriteria approximations for the POV objective and
constant-factor approximations for MOV. Finally, we present mixed integer
programming formulations for these problems. Experimental results show that our
approximation algorithms often find near-optimal control strategies, indicating
that election control through social influence is a salient threat to election
integrity.


Adversarial Classification on Social Networks

  The spread of unwanted or malicious content through social media has become a
major challenge. Traditional examples of this include social network spam, but
an important new concern is the propagation of fake news through social media.
A common approach for mitigating this problem is by using standard statistical
classification to distinguish malicious (e.g., fake news) instances from benign
(e.g., actual news stories). However, such an approach ignores the fact that
malicious instances propagate through the network, which is consequential both
in quantifying consequences (e.g., fake news diffusing through the network),
and capturing detection redundancy (bad content can be detected at different
nodes). An additional concern is evasion attacks, whereby the generators of
malicious instances modify the nature of these to escape detection. We model
this problem as a Stackelberg game between the defender who is choosing
parameters of the detection model, and an attacker, who is choosing both the
node at which to initiate malicious spread, and the nature of malicious
entities. We develop a novel bi-level programming approach for this problem, as
well as a novel solution approach based on implicit function gradients, and
experimentally demonstrate the advantage of our approach over alternatives
which ignore network structure.


Adversarial Task Assignment

  The problem of assigning tasks to workers is of long-standing fundamental
importance. Examples of this include the classical problem of assigning
computing tasks to nodes in a distributed computing environment, assigning jobs
to robots, and crowdsourcing. Extensive research into this problem generally
addresses important issues such as uncertainty and incentives. However, the
problem of adversarial tampering with the task assignment process has not
received as much attention.
  We are concerned with a particular adversarial setting where an attacker may
target a set of workers in order to prevent the tasks assigned to these workers
from being completed. When all tasks are homogeneous, we provide an efficient
algorithm for computing the optimal assignment. When tasks are heterogeneous,
we show that the adversarial assignment problem is NP-Hard, and present an
algorithm for solving it approximately. Our theoretical results are accompanied
by extensive experiments showing the effectiveness of our algorithms.


Community Detection by Information Flow Simulation

  Community detection remains an important problem in data mining, owing to the
lack of scalable algorithms that exploit all aspects of available data - namely
the directionality of flow of information and the dynamics thereof. Most
existing methods use measures of connectedness in the graphical structure. In
this paper, we present a fast, scalable algorithm to detect communities in
directed, weighted graph representations of social networks by simulating flow
of information through them. By design, our algorithm naturally handles
undirected or unweighted networks as well. Our algorithm runs in
$\mathcal{O}(|E|)$ time, which is better than most existing work and uses
$\mathcal{O}(|E|)$ space and hence scales easily to very large datasets.
Finally, we show that our algorithm outperforms the state-of-the-art Markov
Clustering Algorithm (MCL) in both accuracy and scalability on ground truth
data (in a number of cases, we can find communities in graphs too large for
MCL).


Adversarial Regression with Multiple Learners

  Despite the considerable success enjoyed by machine learning techniques in
practice, numerous studies demonstrated that many approaches are vulnerable to
attacks. An important class of such attacks involves adversaries changing
features at test time to cause incorrect predictions. Previous investigations
of this problem pit a single learner against an adversary. However, in many
situations an adversary's decision is aimed at a collection of learners, rather
than specifically targeted at each independently. We study the problem of
adversarial linear regression with multiple learners. We approximate the
resulting game by exhibiting an upper bound on learner loss functions, and show
that the resulting game has a unique symmetric equilibrium. We present an
algorithm for computing this equilibrium, and show through extensive
experiments that equilibrium models are significantly more robust than
conventional regularized linear regression.


Adversarial Coordination on Social Networks

  Decentralized coordination is one of the fundamental challenges for societies
and organizations. While extensively explored from a variety of perspectives,
one issue which has received limited attention is human coordination in the
presence of adversarial agents. We study this problem by situating human
subjects as nodes on a network, and endowing each with a role, either regular
(with the goal of achieving consensus among all regular players), or
adversarial (aiming to prevent consensus among regular players). We show that
adversarial nodes are, indeed, quite successful in preventing consensus.
However, we demonstrate that having the ability to communicate among network
neighbors can considerably improve coordination success, as well as resilience
to adversarial nodes. Our analysis of communication suggests that adversarial
nodes attempt to exploit this capability for their ends, but do so in a
somewhat limited way, perhaps to prevent regular nodes from recognizing their
intent. In addition, we show that the presence of trusted nodes generally has
limited value, but does help when many adversarial nodes are present and
players can communicate. Finally, we use experimental data to develop a
computational model of human behavior, and explore a number of additional
parametric variations, such as features of network topologies, using the
resulting data-driven agent-based model.


Synergistic Security for the Industrial Internet of Things: Integrating
  Redundancy, Diversity, and Hardening

  As the Industrial Internet of Things (IIot) becomes more prevalent in
critical application domains, ensuring security and resilience in the face of
cyber-attacks is becoming an issue of paramount importance. Cyber-attacks
against critical infrastructures, for example, against smart water-distribution
and transportation systems, pose serious threats to public health and safety.
Owing to the severity of these threats, a variety of security techniques are
available. However, no single technique can address the whole spectrum of
cyber-attacks that may be launched by a determined and resourceful attacker. In
light of this, we consider a multi-pronged approach for designing secure and
resilient IIoT systems, which integrates redundancy, diversity, and hardening
techniques. We introduce a framework for quantifying cyber-security risks and
optimizing IIoT design by determining security investments in redundancy,
diversity, and hardening. To demonstrate the applicability of our framework, we
present two case studies in water distribution and transportation a case study
in water-distribution systems. Our numerical evaluation shows that integrating
redundancy, diversity, and hardening can lead to reduced security risk at the
same cost.


Defending Elections Against Malicious Spread of Misinformation

  The integrity of democratic elections depends on voters' access to accurate
information. However, modern media environments, which are dominated by social
media, provide malicious actors with unprecedented ability to manipulate
elections via misinformation, such as fake news. We study a zero-sum game
between an attacker, who attempts to subvert an election by propagating a fake
new story or other misinformation over a set of advertising channels, and a
defender who attempts to limit the attacker's impact. Computing an equilibrium
in this game is challenging as even the pure strategy sets of players are
exponential. Nevertheless, we give provable polynomial-time approximation
algorithms for computing the defender's minimax optimal strategy across a range
of settings, encompassing different population structures as well as models of
the information available to each player. Experimental results confirm that our
algorithms provide near-optimal defender strategies and showcase variations in
the difficulty of defending elections depending on the resources and knowledge
available to the defender.


Attacking Similarity-Based Link Prediction in Social Networks

  Link prediction is one of the fundamental problems in computational social
science. A particularly common means to predict existence of unobserved links
is via structural similarity metrics, such as the number of common neighbors;
node pairs with higher similarity are thus deemed more likely to be linked.
However, a number of applications of link prediction, such as predicting links
in gang or terrorist networks, are adversarial, with another party incentivized
to minimize its effectiveness by manipulating observed information about the
network. We offer a comprehensive algorithmic investigation of the problem of
attacking similarity-based link prediction through link deletion, focusing on
two broad classes of such approaches, one which uses only local information
about target links, and another which uses global network information. While we
show several variations of the general problem to be NP-Hard for both local and
global metrics, we exhibit a number of well-motivated special cases which are
tractable. Additionally, we provide principled and empirically effective
algorithms for the intractable cases, in some cases proving worst-case
approximation guarantees.


Plan Interdiction Games

  We propose a framework for cyber risk assessment and mitigation which models
attackers as formal planners and defenders as interdicting such plans. We
illustrate the value of plan interdiction problems by first modeling network
cyber risk through the use of formal planning, and subsequently formalizing an
important question of prioritizing vulnerabilities for patching in the plan
interdiction framework. In particular, we show that selectively patching
relatively few vulnerabilities allows a network administrator to significantly
reduce exposure to cyber risk. More broadly, we have developed a number of
scalable approaches for plan interdiction problems, making especially
significant advances when attack plans involve uncertainty about system
dynamics. However, important open problems remain, including how to effectively
capture information asymmetry between the attacker and defender, how to best
model dynamics in the attacker-defender interaction, and how to develop
scalable algorithms for solving associated plan interdiction games.


Regularized Ensembles and Transferability in Adversarial Learning

  Despite the considerable success of convolutional neural networks in a broad
array of domains, recent research has shown these to be vulnerable to small
adversarial perturbations, commonly known as adversarial examples. Moreover,
such examples have shown to be remarkably portable, or transferable, from one
model to another, enabling highly successful black-box attacks. We explore this
issue of transferability and robustness from two dimensions: first, considering
the impact of conventional $l_p$ regularization as well as replacing the top
layer with a linear support vector machine (SVM), and second, the value of
combining regularized models into an ensemble. We show that models trained with
different regularizers present barriers to transferability, as does partial
information about the models comprising the ensemble.


Distributionally Robust Removal of Malicious Nodes from Networks

  An important problem in networked systems is detection and removal of
suspected malicious nodes. A crucial consideration in such settings is the
uncertainty endemic in detection, coupled with considerations of network
connectivity, which impose indirect costs from mistakely removing benign nodes
as well as failing to remove malicious nodes. A recent approach proposed to
address this problem directly tackles these considerations, but has a
significant limitation: it assumes that the decision maker has accurate
knowledge of the joint maliciousness probability of the nodes on the network.
This is clearly not the case in practice, where such a distribution is at best
an estimate from limited evidence. To address this problem, we propose a
distributionally robust framework for optimal node removal. While the problem
is NP-Hard, we propose a principled algorithmic technique for solving it
approximately based on duality combined with Semidefinite Programming
relaxation. A combination of both theoretical and empirical analysis, the
latter using both synthetic and real data, provide strong evidence that our
algorithmic approach is highly effective and, in particular, is significantly
more robust than the state of the art.


Simple Physical Adversarial Examples against End-to-End Autonomous
  Driving Models

  Recent advances in machine learning, especially techniques such as deep
neural networks, are promoting a range of high-stakes applications, including
autonomous driving, which often relies on deep learning for perception. While
deep learning for perception has been shown to be vulnerable to a host of
subtle adversarial manipulations of images, end-to-end demonstrations of
successful attacks, which manipulate the physical environment and result in
physical consequences, are scarce. Moreover, attacks typically involve
carefully constructed adversarial examples at the level of pixels. We
demonstrate the first end-to-end attacks on autonomous driving in simulation,
using simple physically realizable attacks: the painting of black lines on the
road. These attacks target deep neural network models for end-to-end autonomous
driving control. A systematic investigation shows that such attacks are
surprisingly easy to engineer, and we describe scenarios (e.g., right turns) in
which they are highly effective, and others that are less vulnerable (e.g.,
driving straight). Further, we use network deconvolution to demonstrate that
the attacks succeed by inducing activation patterns similar to entirely
different scenarios used in training.


Multidefender Security Games

  Stackelberg security game models and associated computational tools have seen
deployment in a number of high-consequence security settings, such as LAX
canine patrols and Federal Air Marshal Service. These models focus on isolated
systems with only one defender, despite being part of a more complex system
with multiple players. Furthermore, many real systems such as transportation
networks and the power grid exhibit interdependencies between targets and,
consequently, between decision makers jointly charged with protecting them. To
understand such multidefender strategic interactions present in security, we
investigate game theoretic models of security games with multiple defenders.
Unlike most prior analysis, we focus on the situations in which each defender
must protect multiple targets, so that even a single defender's best response
decision is, in general, highly non-trivial. We start with an analytical
investigation of multidefender security games with independent targets,
offering an equilibrium and price-of-anarchy analysis of three models with
increasing generality. In all models, we find that defenders have the incentive
to over-protect targets, at times significantly. Additionally, in the simpler
models, we find that the price of anarchy is unbounded, linearly increasing
both in the number of defenders and the number of targets per defender.
Considering interdependencies among targets, we develop a novel mixed-integer
linear programming formulation to compute a defender's best response, and make
use of this formulation in approximating Nash equilibria of the game. We apply
this approach towards computational strategic analysis of several models of
networks representing interdependencies, including real-world power networks.
Our analysis shows how network structure and the probability of failure spread
determine the propensity of defenders to over- or under-invest in security.


Predicting Human Cooperation

  The Prisoner's Dilemma has been a subject of extensive research due to its
importance in understanding the ever-present tension between individual
self-interest and social benefit. A strictly dominant strategy in a Prisoner's
Dilemma (defection), when played by both players, is mutually harmful.
Repetition of the Prisoner's Dilemma can give rise to cooperation as an
equilibrium, but defection is as well, and this ambiguity is difficult to
resolve. The numerous behavioral experiments investigating the Prisoner's
Dilemma highlight that players often cooperate, but the level of cooperation
varies significantly with the specifics of the experimental predicament. We
present the first computational model of human behavior in repeated Prisoner's
Dilemma games that unifies the diversity of experimental observations in a
systematic and quantitatively reliable manner. Our model relies on data we
integrated from many experiments, comprising 168,386 individual decisions. The
computational model is composed of two pieces: the first predicts the
first-period action using solely the structural game parameters, while the
second predicts dynamic actions using both game parameters and history of play.
Our model is extremely successful not merely at fitting the data, but in
predicting behavior at multiple scales in experimental designs not used for
calibration, using only information about the game structure. We demonstrate
the power of our approach through a simulation analysis revealing how to best
promote human cooperation.


Scheduling Resource-Bounded Monitoring Devices for Event Detection and
  Isolation in Networks

  In networked systems, monitoring devices such as sensors are typically
deployed to monitor various target locations. Targets are the points in the
physical space at which events of some interest, such as random faults or
attacks, can occur. Most often, these devices have limited energy supplies, and
they can operate for a limited duration. As a result, energy-efficient
monitoring of various target locations through a set of monitoring devices with
limited energy supplies is a crucial problem in networked systems. In this
paper, we study optimal scheduling of monitoring devices to maximize network
coverage for detecting and isolating events on targets for a given network
lifetime. The monitoring devices considered could remain active only for a
fraction of the overall network lifetime. We formulate the problem of
scheduling of monitoring devices as a graph labeling problem, which unlike
other existing solutions, allows us to directly utilize the underlying network
structure to explore the trade-off between coverage and network lifetime. In
this direction, first we propose a greedy heuristic to solve the graph labeling
problem, and then provide a game-theoretic solution to achieve near optimal
graph labeling. Moreover, the proposed setup can be used to simultaneously
solve the scheduling and placement of monitoring devices, which yields improved
performance as compared to separately solving the placement and scheduling
problems. Finally, we illustrate our results on various networks, including
real-world water distribution networks.


How Robust is Robust ML? Evaluating Models of Classifier Evasion in PDF
  Malware Detection

  Machine learning (ML) techniques are increasingly common in security
applications, such as malware and intrusion detection. However, ML models are
often susceptible to evasion attacks, in which an adversary makes changes to
the input (such as malware) in order to avoid being detected. A conventional
approach to evaluate ML robustness to such attacks, as well as to design robust
ML, is by considering simplified feature-space models of attacks, where the
attacker changes ML features directly to effect evasion, while minimizing or
constraining the magnitude of this change. We investigate the effectiveness of
this approach to designing robust ML in the face of attacks that can be
realized in actual malware (realizable attacks). We demonstrate that in the
context of structure-based PDF malware detection, such techniques appear to
have limited effectiveness, but they are effective with content-based
detectors. In either case, we show that augmenting the feature space models
with conserved features (those that cannot be unilaterally modified without
compromising malicious functionality) significantly improves performance.
Finally, we show that feature space models enable generalized robustness when
faced with a variety of realizable attacks, as compared to classifiers which
are tuned to be robust to a specific realizable attack.


Adversarial Task Allocation

  The problem of allocating tasks to workers is of long standing fundamental
importance. Examples of this include the classical problem of assigning
computing tasks to nodes in a distributed computing environment, as well as the
more recent problem of crowdsourcing where a broad array of tasks are slated to
be completed by human workers. Extensive research into this problem generally
addresses important issues such as uncertainty and, in crowdsourcing,
incentives. However, the problem of adversarial tampering with the task
allocation process has not received as much attention. We are concerned with a
particular adversarial setting in task allocation where an attacker may target
a specific worker in order to prevent the tasks assigned to this worker from
being completed. We consider two attack models: one in which the adversary
observes only the allocation policy (which may be randomized), and the second
in which the attacker observes the actual allocation decision. For the case
when all tasks are homogeneous, we provide polynomial-time algorithms for both
settings. When tasks are heterogeneous, however, we show the adversarial
allocation problem to be NP-Hard, and present algorithms for solving it when
the defender is restricted to assign only a single worker per task. Our
experiments show, surprisingly, that the difference between the two attack
models is minimal: deterministic allocation can achieve nearly as much utility
as randomized.


Get Your Workload in Order: Game Theoretic Prioritization of Database
  Auditing

  For enhancing the privacy protections of databases, where the increasing
amount of detailed personal data is stored and processed, multiple mechanisms
have been developed, such as audit logging and alert triggers, which notify
administrators about suspicious activities; however, the two main limitations
in common are: 1) the volume of such alerts is often substantially greater than
the capabilities of resource-constrained organizations, and 2) strategic
attackers may disguise their actions or carefully choosing which records they
touch, making incompetent the statistical detection models. For solving them,
we introduce a novel approach to database auditing that explicitly accounts for
adversarial behavior by 1) prioritizing the order in which types of alerts are
investigated and 2) providing an upper bound on how much resource to allocate
for each type. We model the interaction between a database auditor and
potential attackers as a Stackelberg game in which the auditor chooses an
auditing policy and attackers choose which records to target. A corresponding
approach combining linear programming, column generation, and heuristic search
is proposed to derive an auditing policy. For testing the policy-searching
performance, a publicly available credit card application dataset are adopted,
on which it shows that our methods produce high-quality mixed strategies as
database audit policies, and our general approach significantly outperforms
non-game-theoretic baselines.


Detection and Mitigation of Attacks on Transportation Networks as a
  Multi-Stage Security Game

  In recent years, state-of-the-art traffic-control devices have evolved from
standalone hardware to networked smart devices. Smart traffic control enables
operators to decrease traffic congestion and environmental impact by acquiring
real-time traffic data and changing traffic signals from fixed to adaptive
schedules. However, these capabilities have inadvertently exposed traffic
control to a wide range of cyber-attacks, which adversaries can easily mount
through wireless networks or even through the Internet. Indeed, recent studies
have found that a large number of traffic signals that are deployed in practice
suffer from exploitable vulnerabilities, which adversaries may use to take
control of the devices. Thanks to hardware-based failsafes, adversaries cannot
cause traffic accidents directly by setting compromised signals to dangerous
configurations. Nonetheless, an adversary could cause disastrous traffic
congestion by changing the schedule of compromised traffic signals, thereby
effectively crippling the transportation network. To provide theoretical
foundations for the protection of transportation networks from these attacks,
we introduce a game-theoretic model of launching, detecting, and mitigating
attacks that tamper with traffic-signal schedules. We show that finding optimal
strategies is a computationally challenging problem, and we propose efficient
heuristic algorithms for finding near optimal strategies. We also introduce a
Gaussian-process based anomaly detector, which can alert operators to ongoing
attacks. Finally, we evaluate our algorithms and the proposed detector using
numerical experiments based on the SUMO traffic simulator.


Attack Tolerance of Link Prediction Algorithms: How to Hide Your
  Relations in a Social Network

  Link prediction is one of the fundamental research problems in network
analysis. Intuitively, it involves identifying the edges that are most likely
to be added to a given network, or the edges that appear to be missing from the
network when in fact they are present. Various algorithms have been proposed to
solve this problem over the past decades. For all their benefits, such
algorithms raise serious privacy concerns, as they could be used to expose a
connection between two individuals who wish to keep their relationship private.
With this in mind, we investigate the ability of such individuals to evade link
prediction algorithms. More precisely, we study their ability to strategically
alter their connections so as to increase the probability that some of their
connections remain unidentified by link prediction algorithms. We formalize
this question as an optimization problem, and prove that finding an optimal
solution is NP-complete. Despite this hardness, we show that the situation is
not bleak in practice. In particular, we propose two heuristics that can easily
be applied by members of the general public on existing social media. We
demonstrate the effectiveness of those heuristics on a wide variety of networks
and against a plethora of link prediction algorithms.


Removing Malicious Nodes from Networks

  A fundamental challenge in networked systems is detection and removal of
suspected malicious nodes. In reality, detection is always imperfect, and the
decision about which potentially malicious nodes to remove must trade off false
positives (erroneously removing benign nodes) and false negatives (mistakenly
failing to remove malicious nodes). However, in network settings this
conventional tradeoff must now account for node connectivity. In particular,
malicious nodes may exert malicious influence, so that mistakenly leaving some
of these in the network may cause damage to spread. On the other hand, removing
benign nodes causes direct harm to these, and indirect harm to their benign
neighbors who would wish to communicate with them.
  We formalize the problem of removing potentially malicious nodes from a
network under uncertainty through an objective that takes connectivity into
account. We show that optimally solving the resulting problem is NP-Hard. We
then propose a tractable solution approach based on a convex relaxation of the
objective. Finally, we experimentally demonstrate that our approach
significantly outperforms both a simple baseline that ignores network
structure, as well as a state-of-the-art approach for a related problem, on
both synthetic and real-world datasets.


An Online Decision-Theoretic Pipeline for Responder Dispatch

  The problem of dispatching emergency responders to service traffic accidents,
fire, distress calls and crimes plagues urban areas across the globe. While
such problems have been extensively looked at, most approaches are offline.
Such methodologies fail to capture the dynamically changing environments under
which critical emergency response occurs, and therefore, fail to be implemented
in practice. Any holistic approach towards creating a pipeline for effective
emergency response must also look at other challenges that it subsumes -
predicting when and where incidents happen and understanding the changing
environmental dynamics. We describe a system that collectively deals with all
these problems in an online manner, meaning that the models get updated with
streaming data sources. We highlight why such an approach is crucial to the
effectiveness of emergency response, and present an algorithmic framework that
can compute promising actions for a given decision-theoretic model for
responder dispatch. We argue that carefully crafted heuristic measures can
balance the trade-off between computational time and the quality of solutions
achieved and highlight why such an approach is more scalable and tractable than
traditional approaches. We also present an online mechanism for incident
prediction, as well as an approach based on recurrent neural networks for
learning and predicting environmental features that affect responder dispatch.
We compare our methodology with prior state-of-the-art and existing dispatch
strategies in the field, which show that our approach results in a reduction in
response time with a drastic reduction in computational time.


Optimal Thresholds for Anomaly-Based Intrusion Detection in Dynamical
  Environments

  In cyber-physical systems, malicious and resourceful attackers could
penetrate the system through cyber means and cause significant physical damage.
Consequently, detection of such attacks becomes integral towards making these
systems resilient to attacks. To achieve this objective, intrusion detection
systems (IDS) that are able to detect malicious behavior can be deployed.
However, practical IDS are imperfect and sometimes they may produce false
alarms for a normal system behavior. Since alarms need to be investigated for
any potential damage, a large number of false alarms may increase the
operational costs significantly. Thus, IDS need to be configured properly, as
oversensitive IDS could detect attacks early but at the cost of a higher number
of false alarms. Similarly, IDS with low sensitivity could reduce the false
alarms while increasing the time to detect the attacks. The configuration of
IDS to strike the right balance between time to detecting attacks and the rate
of false positives is a challenging task, especially in dynamic environments,
in which the damage incurred by a successful attack is time-varying.
  In this paper, we study the problem of finding optimal detection thresholds
for anomaly-based detectors implemented in dynamical systems in the face of
strategic attacks. We formulate the problem as an attacker-defender security
game, and determine thresholds for the detector to achieve an optimal trade-off
between the detection delay and the false positive rates. In this direction,
first, we provide an algorithm that computes optimal fixed threshold that
remains fixed throughout. Second, we allow detector's threshold to change with
time to further minimize the defender's loss and provide an algorithm to
compute time-varying thresholds, which we call adaptive thresholds. Finally, we
numerically evaluate our results using a water distribution network as a
case-study.


