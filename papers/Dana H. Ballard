Atari-HEAD: Atari Human Eye-Tracking and Demonstration Dataset

  We introduce a large-scale dataset of human actions and eye movements while
playing Atari videos games. The dataset currently has 44 hours of gameplay data
from 16 games and a total of 2.97 million demonstrated actions. Human subjects
played games in a frame-by-frame manner to allow enough decision time in order
to obtain near-optimal decisions. This dataset could be potentially used for
research in imitation learning, reinforcement learning, and visual saliency.


AGIL: Learning Attention from Human for Visuomotor Tasks

  When intelligent agents learn visuomotor behaviors from human demonstrations,
they may benefit from knowing where the human is allocating visual attention,
which can be inferred from their gaze. A wealth of information regarding
intelligent decision making is conveyed by human gaze allocation; hence,
exploiting such information has the potential to improve the agents'
performance. With this motivation, we propose the AGIL (Attention Guided
Imitation Learning) framework. We collect high-quality human action and gaze
data while playing Atari games in a carefully controlled experimental setting.
Using these data, we first train a deep neural network that can predict human
gaze positions and visual attention with high accuracy (the gaze network) and
then train another network to predict human actions (the policy network).
Incorporating the learned attention model from the gaze network into the policy
network significantly improves the action prediction accuracy and task
performance.


An Initial Attempt of Combining Visual Selective Attention with Deep
  Reinforcement Learning

  Visual attention serves as a means of feature selection mechanism in the
perceptual system. Motivated by Broadbent's leaky filter model of selective
attention, we evaluate how such mechanism could be implemented and affect the
learning process of deep reinforcement learning. We visualize and analyze the
feature maps of DQN on a toy problem Catch, and propose an approach to combine
visual selective attention with deep reinforcement learning. We experiment with
optical flow-based attention and A2C on Atari games. Experiment results show
that visual selective attention could lead to improvements in terms of sample
efficiency on tested games. An intriguing relation between attention and batch
normalization is also discovered.


