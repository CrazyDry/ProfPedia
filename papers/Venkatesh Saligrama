Deterministic Designs with Deterministic Guarantees: Toeplitz Compressed  Sensing Matrices, Sequence Designs and System Identification

  In this paper we present a new family of discrete sequences having "randomlike" uniformly decaying auto-correlation properties. The new class of infinitelength sequences are higher order chirps constructed using irrational numbers.Exploiting results from the theory of continued fractions and diophantineapproximations, we show that the class of sequences so formed has the propertythat the worst-case auto-correlation coefficients for every finite lengthsequence decays at a polynomial rate. These sequences display doppler immunityas well. We also show that Toeplitz matrices formed from such sequences satisfyrestricted-isometry-property (RIP), a concept that has played a central rolerecently in Compressed Sensing applications. Compressed sensing hasconventionally dealt with sensing matrices with arbitrary components.Nevertheless, such arbitrary sensing matrices are not appropriate for linearsystem identification and one must employ Toeplitz structured sensing matrices.Linear system identification plays a central role in a wide variety ofapplications such as channel estimation for multipath wireless systems as wellas control system applications. Toeplitz matrices are also desirable on accountof their filtering structure, which allows for fast implementation togetherwith reduced storage requirements.

RAPID: Rapidly Accelerated Proximal Gradient Algorithms for Convex  Minimization

  In this paper, we propose a new algorithm to speed-up the convergence ofaccelerated proximal gradient (APG) methods. In order to minimize a convexfunction $f(\mathbf{x})$, our algorithm introduces a simple line search stepafter each proximal gradient step in APG so that a biconvex function$f(\theta\mathbf{x})$ is minimized over scalar variable $\theta>0$ while fixingvariable $\mathbf{x}$. We propose two new ways of constructing the auxiliaryvariables in APG based on the intermediate solutions of the proximal gradientand the line search steps. We prove that at arbitrary iteration step $t(t\geq1)$, our algorithm can achieve a smaller upper-bound for the gap betweenthe current and optimal objective values than those in the traditional APGmethods such as FISTA, making it converge faster in practice. In fact, ouralgorithm can be potentially applied to many important convex optimizationproblems, such as sparse linear regression and kernel SVMs. Our experimentalresults clearly demonstrate that our algorithm converges faster than APG in allof the applications above, even comparable to some sophisticated solvers.

Learning Efficient Anomaly Detectors from $K$-NN Graphs

  We propose a non-parametric anomaly detection algorithm for high dimensionaldata. We score each datapoint by its average $K$-NN distance, and rank themaccordingly. We then train limited complexity models to imitate these scoresbased on the max-margin learning-to-rank framework. A test-point is declared asan anomaly at $\alpha$-false alarm level if the predicted score is in the$\alpha$-percentile. The resulting anomaly detector is shown to beasymptotically optimal in that for any false alarm rate $\alpha$, its decisionregion converges to the $\alpha$-percentile minimum volume level set of theunknown underlying density. In addition, we test both the statisticalperformance and computational efficiency of our algorithm on a number ofsynthetic and real-data experiments. Our results demonstrate the superiority ofour algorithm over existing $K$-NN based anomaly detection algorithms, withsignificant computational savings.

Learning Minimum Volume Sets and Anomaly Detectors from KNN Graphs

  We propose a non-parametric anomaly detection algorithm for high dimensionaldata. We first rank scores derived from nearest neighbor graphs on $n$-pointnominal training data. We then train limited complexity models to imitate thesescores based on the max-margin learning-to-rank framework. A test-point isdeclared as an anomaly at $\alpha$-false alarm level if the predicted score isin the $\alpha$-percentile. The resulting anomaly detector is shown to beasymptotically optimal in that for any false alarm rate $\alpha$, its decisionregion converges to the $\alpha$-percentile minimum volume level set of theunknown underlying density. In addition, we test both the statisticalperformance and computational efficiency of our algorithm on a number ofsynthetic and real-data experiments. Our results demonstrate the superiority ofour algorithm over existing $K$-NN based anomaly detection algorithms, withsignificant computational savings.

Sequential Learning without Feedback

  In many security and healthcare systems a sequence of features/sensors/testsare used for detection and diagnosis. Each test outputs a prediction of thelatent state, and carries with it inherent costs. Our objective is to {\itlearn} strategies for selecting tests to optimize accuracy \& costs.Unfortunately it is often impossible to acquire in-situ ground truthannotations and we are left with the problem of unsupervised sensor selection(USS). We pose USS as a version of stochastic partial monitoring problem withan {\it unusual} reward structure (even noisy annotations are unavailable).Unsurprisingly no learner can achieve sublinear regret without furtherassumptions. To this end we propose the notion of weak-dominance. This is acondition on the joint probability distribution of test outputs and latentstate and says that whenever a test is accurate on an example, a later test inthe sequence is likely to be accurate as well. We empirically verify that weakdominance holds on real datasets and prove that it is a maximal condition forachieving sublinear regret. We reduce USS to a special case of multi-armedbandit problem with side information and develop polynomial time algorithmsthat achieve sublinear regret.

Crowdsourcing with Sparsely Interacting Workers

  We consider estimation of worker skills from worker-task interaction data(with unknown labels) for the single-coin crowd-sourcing binary classificationmodel in symmetric noise. We define the (worker) interaction graph whose nodesare workers and an edge between two nodes indicates whether or not the twoworkers participated in a common task. We show that skills are asymptoticallyidentifiable if and only if an appropriate limiting version of the interactiongraph is irreducible and has odd-cycles. We then formulate a weighted rank-oneoptimization problem to estimate skills based on observations on anirreducible, aperiodic interaction graph. We propose a gradient descent schemeand show that for such interaction graphs estimates converge asymptotically tothe global minimum. We characterize noise robustness of the gradient scheme interms of spectral properties of signless Laplacians of the interaction graph.We then demonstrate that a plug-in estimator based on the estimated skillsachieves state-of-art performance on a number of real-world datasets. Ourresults have implications for rank-one matrix completion problem in thatgradient descent can provably recover $W \times W$ rank-one matrices based on$W+1$ off-diagonal observations of a connected graph with a single odd-cycle.

On the Non-Existence of Unbiased Estimators in Constrained Estimation  Problems

  We address the problem of existence of unbiased constrained parameterestimators. We show that if the constrained set of parameters is compact andthe hypothesized distributions are absolutely continuous with respect to oneanother, then there exists no unbiased estimator. Weaker conditions for theabsence of unbiased constrained estimators are also specified. We provideseveral examples which demonstrate the utility of these conditions.

Comments on the proof of adaptive submodular function minimization

  We point out an issue with Theorem 5 appearing in "Group-based active queryselection for rapid diagnosis in time-critical situations". Theorem 5 boundsthe expected number of queries for a greedy algorithm to identify the class ofan item within a constant factor of optimal. The Theorem is based oncorrectness of a result on minimization of adaptive submodular functions. Wepresent an example that shows that a critical step in Theorem A.11 of "AdaptiveSubmodularity: Theory and Applications in Active Learning and StochasticOptimization" is incorrect.

A Token Based Algorithm to Distributed Computation in Sensor Networks

  We consider distributed algorithms for data aggregation and functioncomputation in sensor networks. The algorithms perform pairwise computationsalong edges of an underlying communication graph. A token is associated witheach sensor node, which acts as a transmission permit. Nodes with active tokenshave transmission permits; they generate messages at a constant rate and sendeach message to a randomly selected neighbor. By using different strategies tocontrol the transmission permits we can obtain tradeoffs between message andtime complexity. Gossip corresponds to the case when all nodes have permits allthe time. We study algorithms where permits are revoked after transmission andrestored upon reception. Examples of such algorithms include Simple-RandomWalk(SRW), Coalescent-Random-Walk(CRW) and Controlled Flooding(CFLD) and theirhybrid variants. SRW has a single node permit, which is passed on in thenetwork. CRW, initially initially has a permit for each node but these permitsare revoked gradually. The final result for SRW and CRW resides at a single(orfew) random node(s) making a direct comparison with GOSSIP difficult. A hybridtwo-phase algorithm switching from CRW to CFLD at a suitable pre-determinedtime can be employed to achieve consensus. We show that such hybrid variantsachieve significant gains in both message and time complexity. The per-nodemessage complexity for n-node graphs, such as 2D mesh, torii, and Randomgeometric graphs, scales as $O(polylog(n))$ and the corresponding timecomplexity scales as O(n). The reduced per-node message complexity leads toreduced energy utilization in sensor networks.

Distributed Detection in Sensor Networks with Limited Range Multi-Modal  Sensors

  We consider a multi-object detection problem over a sensor network (SNET)with limited range multi-modal sensors. Limited range sensing environmentarises in a sensing field prone to signal attenuation and path losses. Thegeneral problem complements the widely considered decentralized detectionproblem where all sensors observe the same object. In this paper we develop adistributed detection approach based on recent development of the falsediscovery rate (FDR) and the associated BH test procedure. The BH procedure isbased on rank ordering of scalar test statistics. We first develop scalar teststatistics for multidimensional data to handle multi-modal sensor observationsand establish its optimality in terms of the BH procedure. We then propose adistributed algorithm in the ideal case of infinite attenuation foridentification of sensors that are in the immediate vicinity of an object. Wedemonstrate communication message scalability to large SNETs by showing thatthe upper bound on the communication message complexity scales linearly withthe number of sensors that are in the vicinity of objects and is independent ofthe total number of sensors in the SNET. This brings forth an importantprinciple for evaluating the performance of an SNET, namely, the need forscalability of communications and performance with respect to the number ofobjects or events in an SNET irrespective of the network size. We then accountfor finite attenuation by modeling sensor observations as corrupted byuncertain interference arising from distant objects and developing robustextensions to our idealized distributed scheme. The robustness propertiesensure that both the error performance and communication message complexitydegrade gracefully with interference.

Thresholded Basis Pursuit: An LP Algorithm for Achieving Optimal Support  Recovery for Sparse and Approximately Sparse Signals from Noisy Random  Measurements

  In this paper we present a linear programming solution for sign patternrecovery of a sparse signal from noisy random projections of the signal. Weconsider two types of noise models, input noise, where noise enters before therandom projection; and output noise, where noise enters after the randomprojection. Sign pattern recovery involves the estimation of sign pattern of asparse signal. Our idea is to pretend that no noise exists and solve thenoiseless $\ell_1$ problem, namely, $\min \|\beta\|_1 ~ s.t. ~ y=G \beta$ andquantizing the resulting solution. We show that the quantized solutionperfectly reconstructs the sign pattern of a sufficiently sparse signal.Specifically, we show that the sign pattern of an arbitrary k-sparse,n-dimensional signal $x$ can be recovered with $SNR=\Omega(\log n)$ andmeasurements scaling as $m= \Omega(k \log{n/k})$ for all sparsity levels $k$satisfying $0< k \leq \alpha n$, where $\alpha$ is a sufficiently smallpositive constant. Surprisingly, this bound matches the optimal\emph{Max-Likelihood} performance bounds in terms of $SNR$, required number ofmeasurements, and admissible sparsity level in an order-wise sense. In contrastto our results, previous results based on LASSO and Max-Correlation techniqueseither assume significantly larger $SNR$, sublinear sparsity levels orrestrictive assumptions on signal sets. Our proof technique is based on noisyperturbation of the noiseless $\ell_1$ problem, in that, we estimate themaximum admissible noise level before sign pattern recovery fails.

Compressed Blind De-convolution

  Suppose the signal x is realized by driving a k-sparse signal u through anarbitrary unknown stable discrete-linear time invariant system H. These typesof processes arise naturally in Reflection Seismology. In this paper we areinterested in several problems: (a) Blind-Deconvolution: Can we recover boththe filter $H$ and the sparse signal $u$ from noisy measurements? (b)Compressive Sensing: Is x compressible in the conventional sense of compressedsensing? Namely, can x, u and H be reconstructed from a sparse set ofmeasurements. We develop novel L1 minimization methods to solve both cases andestablish sufficient conditions for exact recovery for the case when theunknown system H is auto-regressive (i.e. all pole) of a known order. In thecompressed sensing/sampling setting it turns out that both H and x can bereconstructed from O(k log(n)) measurements under certain technical conditionson the support structure of u. Our main idea is to pass x through a linear timeinvariant system G and collect O(k log(n)) sequential measurements. The filterG is chosen suitably, namely, its associated Toeplitz matrix satisfies the RIPproperty. We develop a novel LP optimization algorithm and show that both theunknown filter H and the sparse input u can be reliably estimated.

Behavior Subtraction

  Background subtraction has been a driving engine for many computer vision andvideo analytics tasks. Although its many variants exist, they all share theunderlying assumption that photometric scene properties are either static orexhibit temporal stationarity. While this works in some applications, the modelfails when one is interested in discovering {\it changes in scene dynamics}rather than those in a static background; detection of unusual pedestrian andmotor traffic patterns is but one example. We propose a new model andcomputational framework that address this failure by considering stationaryscene dynamics as a ``background'' with which observed scene dynamics arecompared. Central to our approach is the concept of an {\it event}, that wedefine as short-term scene dynamics captured over a time window at a specificspatial location in the camera field of view. We compute events bytime-aggregating motion labels, obtained by background subtraction, as well asobject descriptors (e.g., object size). Subsequently, we characterize eventsprobabilistically, but use a low-memory, low-complexity surrogates in practicalimplementation. Using these surrogates amounts to {\it behavior subtraction}, anew algorithm with some surprising properties. As demonstrated here, behaviorsubtraction is an effective tool in anomaly detection and localization. It isresilient to spurious background motion, such as one due to camera jitter, andis content-blind, i.e., it works equally well on humans, cars, animals, andother objects in both uncluttered and highly-cluttered scenes. Clearly,treating video as a collection of events rather than colored pixels opens newpossibilities for video analytics.

Boolean Compressed Sensing and Noisy Group Testing

  The fundamental task of group testing is to recover a small distinguishedsubset of items from a large population while efficiently reducing the totalnumber of tests (measurements). The key contribution of this paper is inadopting a new information-theoretic perspective on group testing problems. Weformulate the group testing problem as a channel coding/decoding problem andderive a single-letter characterization for the total number of tests used toidentify the defective set. Although the focus of this paper is primarily ongroup testing, our main result is generally applicable to other compressivesensing models.  The single letter characterization is shown to be order-wise tight for manyinteresting noisy group testing scenarios. Specifically, we consider anadditive Bernoulli($q$) noise model where we show that, for $N$ items and $K$defectives, the number of tests $T$ is $O(\frac{K\log N}{1-q})$ for arbitrarilysmall average error probability and $O(\frac{K^2\log N}{1-q})$ for a worst caseerror criterion. We also consider dilution effects whereby a defective item ina positive pool might get diluted with probability $u$ and potentially missed.In this case, it is shown that $T$ is $O(\frac{K\log N}{(1-u)^2})$ and$O(\frac{K^2\log N}{(1-u)^2})$ for the average and the worst case errorcriteria, respectively. Furthermore, our bounds allow us to verify existingknown bounds for noiseless group testing including the deterministic noise-freecase and approximate reconstruction with bounded distortion. Our proof ofachievability is based on random coding and the analysis of a MaximumLikelihood Detector, and our information theoretic lower bound is based onFano's inequality.

Testing Changes in Communities for the Stochastic Block Model

  We introduce the problems of goodness-of-fit and two-sample testing of thelatent community structure in a 2-community, symmetric, stochastic block model(SBM), in the regime where recovery of the structure is difficult. The latterproblem may be described as follows: let $x,y$ be two latent communitypartitions. Given graphs $G,H$ drawn according to SBMs with partitions $x,y$,respectively, we wish to test the hypothesis $x = y$ against $d(x,y) \ge s,$for a given Hamming distortion parameter $s \ll n$. Prior work showed that`partial' recovery of these partitions up to distortion $s$ with vanishingerror probability requires that the signal-to-noise ratio $(\mathrm{SNR})$ is$\gtrsim C \log (n/s).$ We prove by constructing simple schemes that if $s \gg\sqrt{n \log n},$ then these testing problems can be solved even if$\mathrm{SNR} = O(1).$ For $s = o(\sqrt{n}),$ and constant order degrees, weshow via an information-theoretic lower bound that both testing problemsrequire $\mathrm{SNR} = \Omega(\log(n)),$ and thus at this scale the na\"{i}vescheme of learning the communities and comparing them is minimax optimal up toconstant factors. These results are augmented by simulations of goodness-of-fitand two-sample testing for standard SBMs as well as for Gaussian Markov randomfields with underlying SBM structure.

Wireless ad-hoc networks: Strategies and Scaling laws for the fixed SNR  regime

  This paper deals with throughput scaling laws for random ad-hoc wirelessnetworks in a rich scattering environment. We develop schemes to optimize theratio, $\rho(n)$ of achievable network sum capacity to the sum of thepoint-to-point capacities of source-destinations pairs operating in isolation.For fixed SNR networks, i.e., where the worst case SNR over thesource-destination pairs is fixed independent of $n$, we show thatcollaborative strategies yield a scaling law of $\rho(n) = {\calO}(\frac{1}{n^{1/3}})$ in contrast to multi-hop strategies which yield ascaling law of $\rho(n) = {\cal O}(\frac{1}{\sqrt{n}})$. While, networks whereworst case SNR goes to zero, do not preclude the possibility of collaboration,multi-hop strategies achieve optimal throughput. The plausible reason is thatthe gains due to collaboration cannot offset the effect of vanishing receiveSNR. This suggests that for fixed SNR networks, a network designer should lookfor network protocols that exploit collaboration. The fact that most currentnetworks operate in a fixed SNR interference limited environment providesfurther motivation for considering this regime.

Anomaly Detection with Score functions based on Nearest Neighbor Graphs

  We propose a novel non-parametric adaptive anomaly detection algorithm forhigh dimensional data based on score functions derived from nearest neighborgraphs on $n$-point nominal data. Anomalies are declared whenever the score ofa test sample falls below $\alpha$, which is supposed to be the desired falsealarm level. The resulting anomaly detector is shown to be asymptoticallyoptimal in that it is uniformly most powerful for the specified false alarmlevel, $\alpha$, for the case when the anomaly density is a mixture of thenominal and a known density. Our algorithm is computationally efficient, beinglinear in dimension and quadratic in data size. It does not require choosingcomplicated tuning parameters or function approximation classes and it canadapt to local structure such as local change in dimensionality. We demonstratethe algorithm on both artificial and real data sets in high dimensional featurespaces.

Non-adaptive probabilistic group testing with noisy measurements:  Near-optimal bounds with efficient algorithms

  We consider the problem of detecting a small subset of defective items from alarge set via non-adaptive "random pooling" group tests. We consider both thecase when the measurements are noiseless, and the case when the measurementsare noisy (the outcome of each group test may be independently faulty withprobability q). Order-optimal results for these scenarios are known in theliterature. We give information-theoretic lower bounds on the query complexityof these problems, and provide corresponding computationally efficientalgorithms that match the lower bounds up to a constant factor. To the best ofour knowledge this work is the first to explicitly estimate such a constantthat characterizes the gap between the upper and lower bounds for theseproblems.

Structural Similarity and Distance in Learning

  We propose a novel method of introducing structure into existing machinelearning techniques by developing structure-based similarity and distancemeasures. To learn structural information, low-dimensional structure of thedata is captured by solving a non-linear, low-rank representation problem. Weshow that this low-rank representation can be kernelized, has a closed-formsolution, allows for separation of independent manifolds, and is robust tonoise. From this representation, similarity between observations based onnon-linear structure is computed and can be incorporated into existing featuretransformations, dimensionality reduction techniques, and machine learningmethods. Experimental results on both synthetic and real data sets showperformance improvements for clustering, and anomaly detection through the useof structural similarity.

Graph Construction for Learning with Unbalanced Data

  Unbalanced data arises in many learning tasks such as clustering ofmulti-class data, hierarchical divisive clustering and semisupervised learning.Graph-based approaches are popular tools for these problems. Graph constructionis an important aspect of graph-based learning. We show that graph-basedalgorithms can fail for unbalanced data for many popular graphs such as k-NN,\epsilon-neighborhood and full-RBF graphs. We propose a novel graphconstruction technique that encodes global statistical information into nodedegrees through a ranking scheme. The rank of a data sample is an estimate ofits p-value and is proportional to the total number of data samples withsmaller density. This ranking scheme serves as a surrogate for density; can bereliably estimated; and indicates whether a data sample is close tovalleys/modes. This rank-modulated degree(RMD) scheme is able to significantlysparsify the graph near valleys and provides an adaptive way to cope withunbalanced data. We then theoretically justify our method through limit cutanalysis. Unsupervised and semi-supervised experiments on synthetic and realdata sets demonstrate the superiority of our method.

Graph-based Learning with Unbalanced Clusters

  Graph construction is a crucial step in spectral clustering (SC) andgraph-based semi-supervised learning (SSL). Spectral methods applied onstandard graphs such as full-RBF, $\epsilon$-graphs and $k$-NN graphs can leadto poor performance in the presence of proximal and unbalanced data. This isbecause spectral methods based on minimizing RatioCut or normalized cut onthese graphs tend to put more importance on balancing cluster sizes overreducing cut values. We propose a novel graph construction technique and showthat the RatioCut solution on this new graph is able to handle proximal andunbalanced data. Our method is based on adaptively modulating the neighborhooddegrees in a $k$-NN graph, which tends to sparsify neighborhoods in low densityregions. Our method adapts to data with varying levels of unbalancedness andcan be naturally used for small cluster detection. We justify our ideas throughlimit cut analysis. Unsupervised and semi-supervised experiments on syntheticand real data sets demonstrate the superiority of our method.

A New Geometric Approach to Latent Topic Modeling and Discovery

  A new geometrically-motivated algorithm for nonnegative matrix factorizationis developed and applied to the discovery of latent "topics" for text and image"document" corpora. The algorithm is based on robustly finding and clusteringextreme points of empirical cross-document word-frequencies that correspond tonovel "words" unique to each topic. In contrast to related approaches that arebased on solving non-convex optimization problems using suboptimalapproximations, locally-optimal methods, or heuristics, the new algorithm isconvex, has polynomial complexity, and has competitive qualitative andquantitative performance compared to the current state-of-the-art approaches onsynthetic and real-world datasets.

Spectral Clustering with Unbalanced Data

  Spectral clustering (SC) and graph-based semi-supervised learning (SSL)algorithms are sensitive to how graphs are constructed from data. In particularif the data has proximal and unbalanced clusters these algorithms can lead topoor performance on well-known graphs such as $k$-NN, full-RBF,$\epsilon$-graphs. This is because the objectives such as Ratio-Cut (RCut) ornormalized cut (NCut) attempt to tradeoff cut values with cluster sizes, whichare not tailored to unbalanced data. We propose a novel graph partitioningframework, which parameterizes a family of graphs by adaptively modulating nodedegrees in a $k$-NN graph. We then propose a model selection scheme to choosesizable clusters which are separated by smallest cut values. Our framework isable to adapt to varying levels of unbalancedness of data and can be naturallyused for small cluster detection. We theoretically justify our ideas throughlimit cut analysis. Unsupervised and semi-supervised experiments on syntheticand real data sets demonstrate the superiority of our method.

Topic Discovery through Data Dependent and Random Projections

  We present algorithms for topic modeling based on the geometry ofcross-document word-frequency patterns. This perspective gains significanceunder the so called separability condition. This is a condition on existence ofnovel-words that are unique to each topic. We present a suite of highlyefficient algorithms based on data-dependent and random projections ofword-frequency patterns to identify novel words and associated topics. We willalso discuss the statistical guarantees of the data-dependent projectionsmethod based on two mild assumptions on the prior density of topic documentmatrix. Our key insight here is that the maximum and minimum values ofcross-document frequency patterns projected along any direction are associatedwith novel words. While our sample complexity bounds for topic recovery aresimilar to the state-of-art, the computational complexity of our randomprojection scheme scales linearly with the number of documents and the numberof words per document. We present several experiments on synthetic andreal-world datasets to demonstrate qualitative and quantitative merits of ourscheme.

Near-Optimal Stochastic Threshold Group Testing

  We formulate and analyze a stochastic threshold group testing problemmotivated by biological applications. Here a set of $n$ items contains a subsetof $d \ll n$ defective items. Subsets (pools) of the $n$ items are tested --the test outcomes are negative, positive, or stochastic (negative or positivewith certain probabilities that might depend on the number of defectives beingtested in the pool), depending on whether the number of defective items in thepool being tested are fewer than the {\it lower threshold} $l$, greater thanthe {\it upper threshold} $u$, or in between. The goal of a {\it stochasticthreshold group testing} scheme is to identify the set of $d$ defective itemsvia a "small" number of such tests. In the regime that $l = o(d)$ we presentschemes that are computationally feasible to design and implement, and requirenear-optimal number of tests (significantly improving on existing schemes). Ourschemes are robust to a variety of models for probabilistic threshold grouptesting.

Spectral Clustering with Imbalanced Data

  Spectral clustering is sensitive to how graphs are constructed from dataparticularly when proximal and imbalanced clusters are present. We show thatRatio-Cut (RCut) or normalized cut (NCut) objectives are not tailored toimbalanced data since they tend to emphasize cut sizes over cut values. Wepropose a graph partitioning problem that seeks minimum cut partitions underminimum size constraints on partitions to deal with imbalanced data. Ourapproach parameterizes a family of graphs, by adaptively modulating nodedegrees on a fixed node set, to yield a set of parameter dependent cutsreflecting varying levels of imbalance. The solution to our problem is thenobtained by optimizing over these parameters. We present rigorous limit cutanalysis results to justify our approach. We demonstrate the superiority of ourmethod through unsupervised and semi-supervised experiments on synthetic andreal data sets.

Necessary and Sufficient Conditions for Novel Word Detection in  Separable Topic Models

  The simplicial condition and other stronger conditions that imply it haverecently played a central role in developing polynomial time algorithms withprovable asymptotic consistency and sample complexity guarantees for topicestimation in separable topic models. Of these algorithms, those that relysolely on the simplicial condition are impractical while the practical onesneed stronger conditions. In this paper, we demonstrate, for the first time,that the simplicial condition is a fundamental, algorithm-independent,information-theoretic necessary condition for consistent separable topicestimation. Furthermore, under solely the simplicial condition, we present apractical quadratic-complexity algorithm based on random projections whichconsistently detects all novel words of all topics using only up tosecond-order empirical word moments. This algorithm is amenable to distributedimplementation making it attractive for 'big-data' scenarios involving anetwork of large distributed databases.

Sensing-Aware Kernel SVM

  We propose a novel approach for designing kernels for support vector machines(SVMs) when the class label is linked to the observation through a latent stateand the likelihood function of the observation given the state (the sensingmodel) is available. We show that the Bayes-optimum decision boundary is ahyperplane under a mapping defined by the likelihood function. Combining thiswith the maximum margin principle yields kernels for SVMs that leverageknowledge of the sensing model in an optimal way. We derive the optimum kernelfor the bag-of-words (BoWs) sensing model and demonstrate its superiorperformance over other kernels in document and image classification tasks.These results indicate that such optimum sensing-aware kernel SVMs can matchthe performance of rather sophisticated state-of-the-art approaches.

Information-Theoretic Bounds for Adaptive Sparse Recovery

  We derive an information-theoretic lower bound for sample complexity insparse recovery problems where inputs can be chosen sequentially andadaptively. This lower bound is in terms of a simple mutual informationexpression and unifies many different linear and nonlinear observation models.Using this formula we derive bounds for adaptive compressive sensing (CS),group testing and 1-bit CS problems. We show that adaptivity cannot decreasesample complexity in group testing, 1-bit CS and CS with linear sparsity. Incontrast, we show there might be mild performance gains for CS in the sublinearregime. Our unified analysis also allows characterization of gains due toadaptivity from a wider perspective on sparse problems.

Sparse Recovery with Linear and Nonlinear Observations: Dependent and  Noisy Data

  We formulate sparse support recovery as a salient set identification problemand use information-theoretic analyses to characterize the recovery performanceand sample complexity. We consider a very general model where we are notrestricted to linear models or specific distributions. We state non-asymptoticbounds on recovery probability and a tight mutual information formula forsample complexity. We evaluate our bounds for applications such as sparselinear regression and explicitly characterize effects of correlation or noisyfeatures on recovery performance. We show improvements upon previous work andidentify gaps between the performance of recovery algorithms and fundamentalinformation.

A Rank-SVM Approach to Anomaly Detection

  We propose a novel non-parametric adaptive anomaly detection algorithm forhigh dimensional data based on rank-SVM. Data points are first ranked based onscores derived from nearest neighbor graphs on n-point nominal data. We thentrain a rank-SVM using this ranked data. A test-point is declared as an anomalyat alpha-false alarm level if the predicted score is in the alpha-percentile.The resulting anomaly detector is shown to be asymptotically optimal andadaptive in that for any false alarm rate alpha, its decision region convergesto the alpha-percentile level set of the unknown underlying density. Inaddition we illustrate through a number of synthetic and real-data experimentsboth the statistical performance and computational efficiency of our anomalydetector.

PRISM: Person Re-Identification via Structured Matching

  Person re-identification (re-id), an emerging problem in visual surveillance,deals with maintaining entities of individuals whilst they traverse variouslocations surveilled by a camera network. From a visual perspective re-id ischallenging due to significant changes in visual appearance of individuals incameras with different pose, illumination and calibration. Globally thechallenge arises from the need to maintain structurally consistent matchesamong all the individual entities across different camera views. We proposePRISM, a structured matching method to jointly account for these challenges. Weview the global problem as a weighted graph matching problem and estimate edgeweights by learning to predict them based on the co-occurrences of visualpatterns in the training examples. These co-occurrence based scores in turnaccount for appearance changes by inferring likely and unlikely visualco-occurrences appearing in training instances. We implement PRISM on singleshot and multi-shot scenarios. PRISM uniformly outperforms state-of-the-art interms of matching rate while being computationally efficient.

A Novel Visual Word Co-occurrence Model for Person Re-identification

  Person re-identification aims to maintain the identity of an individual indiverse locations through different non-overlapping camera views. The problemis fundamentally challenging due to appearance variations resulting fromdiffering poses, illumination and configurations of camera views. To deal withthese difficulties, we propose a novel visual word co-occurrence model. Wefirst map each pixel of an image to a visual word using a codebook, which islearned in an unsupervised manner. The appearance transformation between cameraviews is encoded by a co-occurrence matrix of visual word joint distributionsin probe and gallery images. Our appearance model naturally accounts forspatial similarities and variations caused by pose, illumination &configuration change across camera views. Linear SVMs are then trained asclassifiers using these co-occurrence descriptors. On the VIPeR and CUHK Campusbenchmark datasets, our method achieves 83.86% and 85.49% at rank-15 on theCumulative Match Characteristic (CMC) curves, and beats the state-of-the-artresults by 10.44% and 22.27%.

Efficient Minimax Signal Detection on Graphs

  Several problems such as network intrusion, community detection, and diseaseoutbreak can be described by observations attributed to nodes or edges of agraph. In these applications presence of intrusion, community or diseaseoutbreak is characterized by novel observations on some unknown connectedsubgraph. These problems can be formulated in terms of optimization of suitableobjectives on connected subgraphs, a problem which is generally computationallydifficult. We overcome the combinatorics of connectivity by embedding connectedsubgraphs into linear matrix inequalities (LMI). Computationally efficienttests are then realized by optimizing convex objective functions subject tothese LMI constraints. We prove, by means of a novel Euclidean embeddingargument, that our tests are minimax optimal for exponential family ofdistributions on 1-D and 2-D lattices. We show that internal conductance of theconnected subgraph family plays a fundamental role in characterizingdetectability.

A Topic Modeling Approach to Ranking

  We propose a topic modeling approach to the prediction of preferences inpairwise comparisons. We develop a new generative model for pairwisecomparisons that accounts for multiple shared latent rankings that areprevalent in a population of users. This new model also captures inconsistentuser behavior in a natural way. We show how the estimation of latent rankingsin the new generative model can be formally reduced to the estimation of topicsin a statistically equivalent topic modeling problem. We leverage recentadvances in the topic modeling literature to develop an algorithm that canlearn shared latent rankings with provable consistency as well as sample andcomputational complexity guarantees. We demonstrate that the new approach isempirically competitive with the current state-of-the-art approaches inpredicting preferences on some semi-synthetic and real world datasets.

Max-Cost Discrete Function Evaluation Problem under a Budget

  We propose novel methods for max-cost Discrete Function Evaluation Problem(DFEP) under budget constraints. We are motivated by applications such asclinical diagnosis where a patient is subjected to a sequence of (possiblyexpensive) tests before a decision is made. Our goal is to develop strategiesfor minimizing max-costs. The problem is known to be NP hard and greedy methodsbased on specialized impurity functions have been proposed. We develop a broadclass of \emph{admissible} impurity functions that admit monomials, classes ofpolynomials, and hinge-loss functions that allow for flexible impurity designwith provably optimal approximation bounds. This flexibility is important fordatasets when max-cost can be overly sensitive to "outliers." Outliers biasmax-cost to a few examples that require a large number of tests forclassification. We design admissible functions that allow for accuracy-costtrade-off and result in $O(\log n)$ guarantees of the optimal cost among treeswith corresponding classification accuracy levels.

Minimax Optimal Sparse Signal Recovery with Poisson Statistics

  We are motivated by problems that arise in a number of applications such asOnline Marketing and Explosives detection, where the observations are usuallymodeled using Poisson statistics. We model each observation as a Poisson randomvariable whose mean is a sparse linear superposition of known patterns. Unlikemany conventional problems observations here are not identically distributedsince they are associated with different sensing modalities. We analyze theperformance of a Maximum Likelihood (ML) decoder, which for our Poisson settinginvolves a non-linear optimization but yet is computationally tractable. Wederive fundamental sample complexity bounds for sparse recovery when themeasurements are contaminated with Poisson noise. In contrast to theleast-squares linear regression setting with Gaussian noise, we observe that inaddition to sparsity, the scale of the parameters also fundamentally impacts$\ell_2$ error in the Poisson setting. We show tightness of our upper boundsboth theoretically and experimentally. In particular, we derive a minimaxmatching lower bound on the mean-squared error and show that our constrained MLdecoder is minimax optimal for this regime.

Feature-Budgeted Random Forest

  We seek decision rules for prediction-time cost reduction, where completedata is available for training, but during prediction-time, each feature canonly be acquired for an additional cost. We propose a novel random forestalgorithm to minimize prediction error for a user-specified {\it average}feature acquisition budget. While random forests yield strong generalizationperformance, they do not explicitly account for feature costs and furthermorerequire low correlation among trees, which amplifies costs. Our random forestgrows trees with low acquisition cost and high strength based on greedy minimaxcost-weighted-impurity splits. Theoretically, we establish near-optimalacquisition cost guarantees for our algorithm. Empirically, on a number ofbenchmark datasets we demonstrate superior accuracy-cost curves againststate-of-the-art prediction-time algorithms.

Learning Mixed Membership Mallows Models from Pairwise Comparisons

  We propose a novel parameterized family of Mixed Membership Mallows Models(M4) to account for variability in pairwise comparisons generated by aheterogeneous population of noisy and inconsistent users. M4 models individualpreferences as a user-specific probabilistic mixture of shared latent Mallowscomponents. Our key algorithmic insight for estimation is to establish astatistical connection between M4 and topic models by viewing pairwisecomparisons as words, and users as documents. This key insight leads us toexplore Mallows components with a separable structure and leverage recentadvances in separable topic discovery. While separability appears to be overlyrestrictive, we nevertheless show that it is an inevitable outcome of arelatively small number of latent Mallows components in a world of large numberof items. We then develop an algorithm based on robust extreme-pointidentification of convex polygons to learn the reference rankings, and isprovably consistent with polynomial sample complexity guarantees. Wedemonstrate that our new model is empirically competitive with the currentstate-of-the-art approaches in predicting real-world preferences.

Cheap Bandits

  We consider stochastic sequential learning problems where the learner canobserve the \textit{average reward of several actions}. Such a setting isinteresting in many applications involving monitoring and surveillance, wherethe set of the actions to observe represent some (geographical) area. Theimportance of this setting is that in these applications, it is actually\textit{cheaper} to observe average reward of a group of actions rather thanthe reward of a single action. We show that when the reward is \textit{smooth}over a given graph representing the neighboring actions, we can maximize thecumulative reward of learning while \textit{minimizing the sensing cost}. Inthis paper we propose CheapUCB, an algorithm that matches the regret guaranteesof the known algorithms for this setting and at the same time guarantees alinear cost again over them. As a by-product of our analysis, we establish a$\Omega(\sqrt{dT})$ lower bound on the cumulative regret of spectral banditsfor a class of graphs with effective dimension $d$.

Necessary and Sufficient Conditions and a Provably Efficient Algorithm  for Separable Topic Discovery

  We develop necessary and sufficient conditions and a novel provablyconsistent and efficient algorithm for discovering topics (latent factors) fromobservations (documents) that are realized from a probabilistic mixture ofshared latent factors that have certain properties. Our focus is on the classof topic models in which each shared latent factor contains a novel word thatis unique to that factor, a property that has come to be known as separability.Our algorithm is based on the key insight that the novel words correspond tothe extreme points of the convex hull formed by the row-vectors of a suitablynormalized word co-occurrence matrix. We leverage this geometric insight toestablish polynomial computation and sample complexity bounds based on a fewisotropic random projections of the rows of the normalized word co-occurrencematrix. Our proposed random-projections-based algorithm is naturally amenableto an efficient distributed implementation and is attractive for modernweb-scale distributed data mining applications.

Sensor Selection by Linear Programming

  We learn sensor trees from training data to minimize sensor acquisition costsduring test time. Our system adaptively selects sensors at each stage ifnecessary to make a confident classification. We pose the problem as empiricalrisk minimization over the choice of trees and node decision rules. Wedecompose the problem, which is known to be intractable, into combinatorial(tree structures) and continuous parts (node decision rules) and propose tosolve them separately. Using training data we greedily solve for thecombinatorial tree structures and for the continuous part, which is anon-convex multilinear objective function, we derive convex surrogate lossfunctions that are piecewise linear. The resulting problem can be cast as alinear program and has the advantage of guaranteed convergence, globaloptimality, repeatability and computational efficiency. We show that ourproposed approach outperforms the state-of-art on a number of benchmarkdatasets.

Zero-Shot Learning via Semantic Similarity Embedding

  In this paper we consider a version of the zero-shot learning problem whereseen class source and target domain data are provided. The goal duringtest-time is to accurately predict the class label of an unseen target domaininstance based on revealed source domain side information (\eg attributes) forunseen classes. Our method is based on viewing each source or target data as amixture of seen class proportions and we postulate that the mixture patternshave to be similar if the two instances belong to the same unseen class. Thisperspective leads us to learning source/target embedding functions that map anarbitrary source/target domain data into a same semantic space where similaritycan be readily measured. We develop a max-margin framework to learn thesesimilarity functions and jointly optimize parameters by means of crossvalidation. Our test results are compelling, leading to significant improvementin terms of accuracy on most benchmark datasets for zero-shot recognition.

Group Membership Prediction

  The group membership prediction (GMP) problem involves predicting whether ornot a collection of instances share a certain semantic property. For instance,in kinship verification given a collection of images, the goal is to predictwhether or not they share a {\it familial} relationship. In this context wepropose a novel probability model and introduce latent {\em view-specific} and{\em view-shared} random variables to jointly account for the view-specificappearance and cross-view similarities among data instances. Our model positsthat data from each view is independent conditioned on the shared variables.This postulate leads to a parametric probability model that decomposes groupmembership likelihood into a tensor product of data-independent parameters anddata-dependent factors. We propose learning the data-independent parameters ina discriminative way with bilinear classifiers, and test our predictionalgorithm on challenging visual recognition tasks such as multi-camera personre-identification and kinship verification. On most benchmark datasets, ourmethod can significantly outperform the current state-of-the-art.

Efficient Learning by Directed Acyclic Graph For Resource Constrained  Prediction

  We study the problem of reducing test-time acquisition costs inclassification systems. Our goal is to learn decision rules that adaptivelyselect sensors for each example as necessary to make a confident prediction. Wemodel our system as a directed acyclic graph (DAG) where internal nodescorrespond to sensor subsets and decision functions at each node choose whetherto acquire a new sensor or classify using the available measurements. Thisproblem can be naturally posed as an empirical risk minimization over trainingdata. Rather than jointly optimizing such a highly coupled and non-convexproblem over all decision nodes, we propose an efficient algorithm motivated bydynamic programming. We learn node policies in the DAG by reducing the globalobjective to a series of cost sensitive learning problems. Our approach iscomputationally efficient and has proven guarantees of convergence to theoptimal system for a fixed architecture. In addition, we present an extensionto map other budgeted learning problems with large number of sensors to our DAGarchitecture and demonstrate empirical performance exceeding state-of-the-artalgorithms for data composed of both few and many sensors.

Zero-Shot Learning via Joint Latent Similarity Embedding

  Zero-shot recognition (ZSR) deals with the problem of predicting class labelsfor target domain instances based on source domain side information (e.g.attributes) of unseen classes. We formulate ZSR as a binary prediction problem.Our resulting classifier is class-independent. It takes an arbitrary pair ofsource and target domain instances as input and predicts whether or not theycome from the same class, i.e. whether there is a match. We model the posteriorprobability of a match since it is a sufficient statistic and propose a latentprobabilistic model in this context. We develop a joint discriminative learningframework based on dictionary learning to jointly learn the parameters of ourmodel for both domains, which ultimately leads to our class-independentclassifier. Many of the existing embedding methods can be viewed as specialcases of our probabilistic model. On ZSR our method shows 4.90\% improvementover the state-of-the-art in accuracy averaged across four benchmark datasets.We also adapt ZSR method for zero-shot retrieval and show 22.45\% improvementaccordingly in mean average precision (mAP).

Efficient Training of Very Deep Neural Networks for Supervised Hashing

  In this paper, we propose training very deep neural networks (DNNs) forsupervised learning of hash codes. Existing methods in this context trainrelatively "shallow" networks limited by the issues arising in back propagation(e.e. vanishing gradients) as well as computational efficiency. We propose anovel and efficient training algorithm inspired by alternating direction methodof multipliers (ADMM) that overcomes some of these limitations. Our methoddecomposes the training process into independent layer-wise local updatesthrough auxiliary variables. Empirically we observe that our training algorithmalways converges and its computational complexity is linearly proportional tothe number of edges in the networks. Empirically we manage to train DNNs with64 hidden layers and 1024 nodes per layer for supervised hashing in about 3hours using a single GPU. Our proposed very deep supervised hashing (VDSH)method significantly outperforms the state-of-the-art on several benchmarkdatasets.

Optimally Pruning Decision Tree Ensembles With Feature Cost

  We consider the problem of learning decision rules for prediction withfeature budget constraint. In particular, we are interested in pruning anensemble of decision trees to reduce expected feature cost while maintaininghigh prediction accuracy for any test example. We propose a novel 0-1 integerprogram formulation for ensemble pruning. Our pruning formulation is general -it takes any ensemble of decision trees as input. By explicitly accounting forfeature-sharing across trees together with accuracy/cost trade-off, our methodis able to significantly reduce feature cost by pruning subtrees that introducemore loss in terms of feature cost than benefit in terms of prediction accuracygain. Theoretically, we prove that a linear programming relaxation produces theexact solution of the original integer program. This allows us to use efficientconvex optimization tools to obtain an optimally pruned ensemble for any givenbudget. Empirically, we see that our pruning algorithm significantly improvesthe performance of the state of the art ensemble method BudgetRF.

Resource Constrained Structured Prediction

  We study the problem of structured prediction under test-time budgetconstraints. We propose a novel approach applicable to a wide range ofstructured prediction problems in computer vision and natural languageprocessing. Our approach seeks to adaptively generate computationally costlyfeatures during test-time in order to reduce the computational cost ofprediction while maintaining prediction performance. We show that training theadaptive feature generation system can be reduced to a series of structuredlearning problems, resulting in efficient training using existing structuredlearning algorithms. This framework provides theoretical justification forseveral existing heuristic approaches found in literature. We evaluate ourproposed adaptive system on two structured prediction tasks, optical characterrecognition (OCR) and dependency parsing and show strong performance inreduction of the feature costs without degrading accuracy.

Pruning Random Forests for Prediction on a Budget

  We propose to prune a random forest (RF) for resource-constrained prediction.We first construct a RF and then prune it to optimize expected feature cost &accuracy. We pose pruning RFs as a novel 0-1 integer program with linearconstraints that encourages feature re-use. We establish total unimodularity ofthe constraint set to prove that the corresponding LP relaxation solves theoriginal integer program. We then exploit connections to combinatorialoptimization and develop an efficient primal-dual algorithm, scalable to largedatasets. In contrast to our bottom-up approach, which benefits from good RFinitialization, conventional methods are top-down acquiring features based ontheir utility value and is generally intractable, requiring heuristics.Empirically, our pruning algorithm outperforms existing state-of-the-artresource-constrained algorithms.

