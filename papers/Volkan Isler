Capturing an Omnidirectional Evader in Convex Environments using a  Differential Drive Robot

  We study the problem of capturing an Omnidirectional Evader in convexenvironments using a Differential Drive Robot (DDR). The DDR wins the game ifat any time instant it captures (collides with) the evader. The evader wins ifit can avoid capture forever. Both players are unit disks with the same maximum(bounded) speed, but the DDR can only change its motion direction at a boundedrate. We show that despite this limitation, the DDR can capture the evader.

A Novel Method for the Extrinsic Calibration of a 2D Laser Rangefinder  and a Camera

  We present a novel method for extrinsically calibrating a camera and a 2DLaser Rangefinder (LRF) whose beams are invisible from the camera image. Weshow that point-to-plane constraints from a single observation of a V-shapedcalibration pattern composed of two non-coplanar triangles suffice to uniquelyconstrain the relative pose between two sensors. Next, we present an approachto obtain analytical solutions using point-to-plane constraints from single ormultiple observations. Along the way, we also show that previous solutions, incontrast to our method, have inherent ambiguities and therefore must rely on agood initial estimate. Real and synthetic experiments validate our method andshow that it achieves better accuracy than previous methods.

Tree Morphology for Phenotyping from Semantics-Based Mapping in Orchard  Environments

  Measuring tree morphology for phenotyping is an essential but labor-intensiveactivity in horticulture. Researchers often rely on manual measurements whichmay not be accurate for example when measuring tree volume. Recent approacheson automating the measurement process rely on LIDAR measurements coupled withhigh-accuracy GPS. Usually each side of a row is reconstructed independentlyand then merged using GPS information. Such approaches have two disadvantages:(1) they rely on specialized and expensive equipment, and (2) since thereconstruction process does not simultaneously use information from both sides,side reconstructions may not be accurate. We also show that standard loopclosure methods do not necessarily align tree trunks well. In this paper, wepresent a novel vision system that employs only an RGB-D camera to estimatemorphological parameters. A semantics-based mapping algorithm merges thetwo-sides 3D models of tree rows, where integrated semantic information isobtained and refined by robust fitting algorithms. We focus on measuring treeheight, canopy volume and trunk diameter from the optimized 3D model.Experiments conducted in real orchards quantitatively demonstrate the accuracyof our method.

Adaptive View Planning for Aerial 3D Reconstruction of Complex Scenes

  With the proliferation of small aerial vehicles, acquiring close up aerialimagery for high quality reconstruction of complex scenes is gainingimportance. We present an adaptive view planning method to collect such imagesin an automated fashion. We start by sampling a small set of views to build acoarse proxy to the scene. We then present (i)~a method that builds a viewmanifold for view selection, and (ii) an algorithm to select a sparse set ofviews. The vehicle then visits these viewpoints to cover the scene, and theprocedure is repeated until reconstruction quality converges or a desired levelof quality is achieved. The view manifold provides an effectiveefficiency/quality compromise between using the entire 6 degree of freedom posespace and using a single view hemisphere to select the views.  Our results show that, in contrast to existing "explore and exploit" methodswhich collect only two sets of views, reconstruction quality can be drasticallyimproved by adding a third set. They also indicate that three rounds of datacollection is sufficient even for very complex scenes. We compare our algorithmto existing methods in three challenging scenes. We require each algorithm toselect the same number of views. Our algorithm generates views which producethe least reconstruction error.

Vision-Based Preharvest Yield Mapping for Apple Orchards

  We present an end-to-end computer vision system for mapping yield in an appleorchard using images captured from a single camera. Our proposed system isplatform independent and does not require any specific lighting conditions. Ourmain technical contributions are 1)~a semi-supervised clustering algorithm thatutilizes colors to identify apples and 2)~an unsupervised clustering methodthat utilizes spatial properties to estimate fruit counts from apple clustershaving arbitrarily complex geometry. Additionally, we utilize camera motion tomerge the counts across multiple views. We verified the performance of ouralgorithms by conducting multiple field trials on three tree rows consisting of$252$ trees at the University of Minnesota Horticultural Research Center.Results indicate that the detection method achieves $F_1$-measure $.95 -.97$for multiple color varieties and lighting conditions. The counting methodachieves an accuracy of $89\%-98\%$. Additionally, we report merged fruitcounts from both sides of the tree rows. Our yield estimation method achievesan overall accuracy of $91.98\% - 94.81\%$ across different datasets.

A Comparative Study of Fruit Detection and Counting Methods for Yield  Mapping in Apple Orchards

  We present new methods for apple detection and counting based on recent deeplearning approaches and compare them with state-of-the-art results based onclassical methods. Our goal is to quantify performance improvements by neuralnetwork-based methods compared to methods based on classical approaches.Additionally, we introduce a complete system for counting apples in an entirerow. This task is challenging as it requires tracking fruits in images fromboth sides of the row. We evaluate the performances of three fruit detectionmethods and two fruit counting methods on six datasets. Results indicate thatthe classical detection approach still outperforms the deep learning basedmethods in the majority of the datasets. For fruit counting though, the deeplearning based approach performs better for all of the datasets. Combining theclassical detection method together with the neural network based countingapproach, we achieve remarkable yield accuracies ranging from 95.56% to 97.83%.

Semantics-Aware Image to Image Translation and Domain Transfer

  Image to image translation is the problem of transferring an image from asource domain to a target domain. We present a new method to transfer theunderlying semantics of an image even when there are geometric changes acrossthe two domains. Specifically, we present a Generative Adversarial Network(GAN) that can transfer semantic information presented as segmentation masks.Our main technical contribution is an encoder-decoder based generatorarchitecture that jointly encodes the image and its underlying semantics andtranslates both simultaneously to the target domain. Additionally, we proposeobject transfiguration and cross-domain semantic consistency losses thatpreserve the underlying semantic labels maps. We demonstrate the effectivenessof our approach in multiple object transfiguration and domain transfer tasksthrough qualitative and quantitative experiments. The results show that ourmethod is better at transferring image semantics than state of the art image toimage translation methods.

View Selection with Geometric Uncertainty Modeling

  Estimating positions of world points from features observed in images is akey problem in 3D reconstruction, image mosaicking,simultaneous localizationand mapping and structure from motion. We consider a special instance in whichthere is a dominant ground plane $\mathcal{G}$ viewed from a parallel viewingplane $\mathcal{S}$ above it. Such instances commonly arise, for example, inaerial photography. Consider a world point $g \in \mathcal{G}$ and its worstcase reconstruction uncertainty $\varepsilon(g,\mathcal{S})$ obtained bymerging \emph{all} possible views of $g$ chosen from $\mathcal{S}$. We firstshow that one can pick two views $s_p$ and $s_q$ such that the uncertainty$\varepsilon(g,\{s_p,s_q\})$ obtained using only these two views is almost asgood as (i.e. within a small constant factor of) $\varepsilon(g,\mathcal{S})$.Next, we extend the result to the entire ground plane $\mathcal{G}$ and showthat one can pick a small subset of $\mathcal{S'} \subseteq \mathcal{S}$ (whichgrows only linearly with the area of $\mathcal{G}$) and still obtain a constantfactor approximation, for every point $g \in \mathcal{G}$, to the minimum worstcase estimate obtained by merging all views in $\mathcal{S}$. Finally, wepresent a multi-resolution view selection method which extends our techniquesto non-planar scenes. We show that the method can produce rich and accuratedense reconstructions with a small number of views. Our results provide a viewselection mechanism with provable performance guarantees which can drasticallyincrease the speed of scene reconstruction algorithms. In addition totheoretical results, we demonstrate their effectiveness in an application whereaerial imagery is used for monitoring farms and orchards.

Design and Evaluation of a Novel Cable-Driven Gripper with Perception  Capabilities for Strawberry Picking Robots

  This paper presents a novel cable-driven gripper with perception capabilitiesfor autonomous harvesting of strawberries. Experiments show that the gripperallows for more accurate and faster picking of strawberries compared toexisting systems. The gripper consists of four functional parts for sensing,picking, transmission, and storing. It has six fingers that open to form aclosed space to swallow a target strawberry and push other surrounding berriesaway from the target. Equipped with three IR sensors, the gripper controls amanipulator arm to correct for positional error, and can thus pick strawberriesthat are not exactly localized by the vision algorithm, improving therobustness. Experiments show that the gripper is gentle on the berries as itmerely cuts the stem and there is no physical interaction with the berriesduring the cutting process. We show that the gripper has close-to-perfectsuccessful picking rate when addressing isolated strawberries. By includinginternal perception, we get high positional error tolerance, and avoid usingslow, high-level closed-loop control. Moreover, the gripper can store severalberries, which reduces the overall travel distance for the manipulator, anddecreases the time needed to pick a single strawberry substantially. Theexperiments show that the gripper design decreased picking execution timenoticeably compared to results found in literature.

Semantic Mapping for Orchard Environments by Merging Two-Sides  Reconstructions of Tree Rows

  Measuring semantic traits for phenotyping is an essential but labor-intensiveactivity in horticulture. Researchers often rely on manual measurements whichmay not be accurate for tasks such as measuring tree volume. To improve theaccuracy of such measurements and to automate the process, we consider theproblem of building coherent three dimensional (3D) reconstructions of orchardrows. Even though 3D reconstructions of side views can be obtained usingstandard mapping techniques, merging the two side-views is difficult due to thelack of overlap between the two partial reconstructions. Our first maincontribution in this paper is a novel method that utilizes global features andsemantic information to obtain an initial solution aligning the two sides. Ourmapping approach then refines the 3D model of the entire tree row byintegrating semantic information common to both sides, and extracted using ournovel robust detection and fitting algorithms. Next, we present a vision systemto measure semantic traits from the optimized 3D model that is built from theRGB or RGB-D data captured by only a camera. Specifically, we show how canopyvolume, trunk diameter, tree height and fruit count can be automaticallyobtained in real orchard environments. The experiment results from multipledatasets quantitatively demonstrate the high accuracy and robustness of ourmethod.

Pixels to Plans: Learning Non-Prehensile Manipulation by Imitating a  Planner

  We present a novel method enabling robots to quickly learn to manipulateobjects by leveraging a motion planner to generate "expert" trainingtrajectories from a small amount of human-labeled data. In contrast to thetraditional sense-plan-act cycle, we propose a deep learning architecture andtraining regimen called PtPNet that can estimate effective end-effectortrajectories for manipulation directly from a single RGB-D image of an object.Additionally, we present a data collection and augmentation pipeline thatenables the automatic generation of large numbers (millions) of training imageand trajectory examples with almost no human labeling effort.  We demonstrate our approach in a non-prehensile tool-based manipulation task,specifically picking up shoes with a hook. In hardware experiments, PtPNetgenerates motion plans (open-loop trajectories) that reliably (89% success over189 trials) pick up four very different shoes from a range of positions andorientations, and reliably picks up a shoe it has never seen before. Comparedwith a traditional sense-plan-act paradigm, our system has the advantages ofoperating on sparse information (single RGB-D frame), producing high-qualitytrajectories much faster than the "expert" planner (300ms versus severalseconds), and generalizing effectively to previously unseen shoes.

A Leapfrog Strategy for Pursuit-Evasion in a Polygonal Environment

  We study pursuit-evasion in a polygonal environment with polygonal obstacles.In this turn based game, an evader $e$ is chased by pursuers $p_1, p_2, ...,p_{\ell}$. The players have full information about the environment and thelocation of the other players. The pursuers are allowed to coordinate theiractions. On the pursuer turn, each $p_i$ can move to any point at distance atmost 1 from his current location. On the evader turn, he moves similarly. Thepursuers win if some pursuer becomes co-located with the evader in finite time.The evader wins if he can evade capture forever.  It is known that one pursuer can capture the evader in any simply-connectedpolygonal environment, and that three pursuers are always sufficient in anypolygonal environment (possibly with polygonal obstacles). We contribute twonew results to this field. First, we fully characterize when an environmentwith a single obstacles is one-pursuer-win or two-pursuer-win. Second, we givesufficient (but not necessary) conditions for an environment to have a winningstrategy for two pursuers. Such environments can be swept by a \emph{leapfrogstrategy} in which the two cops alternately guard/increase the currentlycontrolled area. The running time of this algorithm is $O(n \cdot h \cdot{diam}(P))$ where $n$ is the number of vertices, $h$ is the number of obstaclesand ${diam}(P)$ is the diameter of $P$.  More concretely, for an environment with $n$ vertices, we describe an$O(n^2)$ algorithm that (1) determines whether the obstacles arewell-separated, and if so, (2) constructs the required partition for a leapfrogstrategy.

Line-of-Sight Pursuit in Strictly Sweepable Polygons

  We study a turn-based game in a simply connected polygonal environment $Q$between a pursuer $P$ and an adversarial evader $E$. Both players can move in astraight line to any point within unit distance during their turn. The pursuer$P$ wins by capturing the evader, meaning that their distance satisfies $d(P,E) \leq 1$, while the evader wins by eluding capture forever. Both players havea map of the environment, but they have different sensing capabilities. Theevader $E$ always knows the location of $P$. Meanwhile, $P$ only hasline-of-sight visibility: $P$ observes the evader's position only when the linesegment connecting them lies entirely within the polygon. Therefore $P$ mustsearch for $E$ when the evader is hidden from view.  We provide a winning strategy for $P$ in the family of strictly sweepablepolygons, meaning that a straight line $L$ can be moved continuously over $Q$so that (1) $L \cap Q$ is always convex and (2) every point on the boundary$\partial Q$ is swept exactly once. This strict sweeping requires that $L$moves along $Q$ via a sequence of translations and rotations. We develop ourmain result by first considering pursuit in the subfamilies of monotonepolygons (where $L$ moves by translation) and scallop polygons (where $L$ movesby a single rotation). Our algorithm uses rook strategy during its activepursuit phase, rather than the well-known lion strategy. The rook strategy iscrucial for obtaining a capture time that is linear in the area of $Q$. Formonotone and scallop polygons, our algorithm has a capture time of $O(n(Q) +\mbox{area}(Q))$, where $n(Q)$ is the number of polygon vertices. The capturetime bound for strictly sweepable polygons is $O( n(Q) \cdot \mbox{area}(Q) )$.

Minimizing Uncertainty through Sensor Placement with Angle Constraints

  We study the problem of sensor placement in environments in whichlocalization is a necessity, such as ad-hoc wireless sensor networks that allowthe placement of a few anchors that know their location or sensor arrays thatare tracking a target. In most of these situations, the quality of localizationdepends on the relative angle between the target and the pair of sensorsobserving it. In this paper, we consider placing a small number of sensorswhich ensure good angular $\alpha$-coverage: given $\alpha$ in $[0,\pi/2]$, foreach target location $t$, there must be at least two sensors $s_1$ and $s_2$such that the $\angle(s_1 t s_2)$ is in the interval $[\alpha, \pi-\alpha]$.One of the main difficulties encountered in such problems is that since theconstraints depend on at least two sensors, building a solution must accountfor the inherent dependency between selected sensors, a feature that genericSet Cover techniques do not account for. We introduce a general framework thatguarantees an angular coverage that is arbitrarily close to $\alpha$ for any$\alpha <= \pi/3$ and apply it to a variety of problems to get bi-criteriaapproximations. When the angular coverage is required to be at least a constantfraction of $\alpha$, we obtain results that are strictly better than whatstandard geometric Set Cover methods give. When the angular coverage isrequired to be at least $(1-1/\delta)\cdot\alpha$, we obtain a$\mathcal{O}(\log \delta)$- approximation for sensor placement with$\alpha$-coverage on the plane. In the presence of additional distance orvisibility constraints, the framework gives a $\mathcal{O}(\log\delta\cdot\logk_{OPT})$-approximation, where $k_{OPT}$ is the size of the optimal solution.We also use our framework to give a $\mathcal{O}(\log \delta)$-approximationthat ensures $(1-1/\delta)\cdot \alpha$-coverage and covers every target withindistance $3R$.

