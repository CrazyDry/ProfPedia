Capturing an Omnidirectional Evader in Convex Environments using a
  Differential Drive Robot

  We study the problem of capturing an Omnidirectional Evader in convex
environments using a Differential Drive Robot (DDR). The DDR wins the game if
at any time instant it captures (collides with) the evader. The evader wins if
it can avoid capture forever. Both players are unit disks with the same maximum
(bounded) speed, but the DDR can only change its motion direction at a bounded
rate. We show that despite this limitation, the DDR can capture the evader.


A Novel Method for the Extrinsic Calibration of a 2D Laser Rangefinder
  and a Camera

  We present a novel method for extrinsically calibrating a camera and a 2D
Laser Rangefinder (LRF) whose beams are invisible from the camera image. We
show that point-to-plane constraints from a single observation of a V-shaped
calibration pattern composed of two non-coplanar triangles suffice to uniquely
constrain the relative pose between two sensors. Next, we present an approach
to obtain analytical solutions using point-to-plane constraints from single or
multiple observations. Along the way, we also show that previous solutions, in
contrast to our method, have inherent ambiguities and therefore must rely on a
good initial estimate. Real and synthetic experiments validate our method and
show that it achieves better accuracy than previous methods.


Tree Morphology for Phenotyping from Semantics-Based Mapping in Orchard
  Environments

  Measuring tree morphology for phenotyping is an essential but labor-intensive
activity in horticulture. Researchers often rely on manual measurements which
may not be accurate for example when measuring tree volume. Recent approaches
on automating the measurement process rely on LIDAR measurements coupled with
high-accuracy GPS. Usually each side of a row is reconstructed independently
and then merged using GPS information. Such approaches have two disadvantages:
(1) they rely on specialized and expensive equipment, and (2) since the
reconstruction process does not simultaneously use information from both sides,
side reconstructions may not be accurate. We also show that standard loop
closure methods do not necessarily align tree trunks well. In this paper, we
present a novel vision system that employs only an RGB-D camera to estimate
morphological parameters. A semantics-based mapping algorithm merges the
two-sides 3D models of tree rows, where integrated semantic information is
obtained and refined by robust fitting algorithms. We focus on measuring tree
height, canopy volume and trunk diameter from the optimized 3D model.
Experiments conducted in real orchards quantitatively demonstrate the accuracy
of our method.


Adaptive View Planning for Aerial 3D Reconstruction of Complex Scenes

  With the proliferation of small aerial vehicles, acquiring close up aerial
imagery for high quality reconstruction of complex scenes is gaining
importance. We present an adaptive view planning method to collect such images
in an automated fashion. We start by sampling a small set of views to build a
coarse proxy to the scene. We then present (i)~a method that builds a view
manifold for view selection, and (ii) an algorithm to select a sparse set of
views. The vehicle then visits these viewpoints to cover the scene, and the
procedure is repeated until reconstruction quality converges or a desired level
of quality is achieved. The view manifold provides an effective
efficiency/quality compromise between using the entire 6 degree of freedom pose
space and using a single view hemisphere to select the views.
  Our results show that, in contrast to existing "explore and exploit" methods
which collect only two sets of views, reconstruction quality can be drastically
improved by adding a third set. They also indicate that three rounds of data
collection is sufficient even for very complex scenes. We compare our algorithm
to existing methods in three challenging scenes. We require each algorithm to
select the same number of views. Our algorithm generates views which produce
the least reconstruction error.


Vision-Based Preharvest Yield Mapping for Apple Orchards

  We present an end-to-end computer vision system for mapping yield in an apple
orchard using images captured from a single camera. Our proposed system is
platform independent and does not require any specific lighting conditions. Our
main technical contributions are 1)~a semi-supervised clustering algorithm that
utilizes colors to identify apples and 2)~an unsupervised clustering method
that utilizes spatial properties to estimate fruit counts from apple clusters
having arbitrarily complex geometry. Additionally, we utilize camera motion to
merge the counts across multiple views. We verified the performance of our
algorithms by conducting multiple field trials on three tree rows consisting of
$252$ trees at the University of Minnesota Horticultural Research Center.
Results indicate that the detection method achieves $F_1$-measure $.95 -.97$
for multiple color varieties and lighting conditions. The counting method
achieves an accuracy of $89\%-98\%$. Additionally, we report merged fruit
counts from both sides of the tree rows. Our yield estimation method achieves
an overall accuracy of $91.98\% - 94.81\%$ across different datasets.


A Comparative Study of Fruit Detection and Counting Methods for Yield
  Mapping in Apple Orchards

  We present new methods for apple detection and counting based on recent deep
learning approaches and compare them with state-of-the-art results based on
classical methods. Our goal is to quantify performance improvements by neural
network-based methods compared to methods based on classical approaches.
Additionally, we introduce a complete system for counting apples in an entire
row. This task is challenging as it requires tracking fruits in images from
both sides of the row. We evaluate the performances of three fruit detection
methods and two fruit counting methods on six datasets. Results indicate that
the classical detection approach still outperforms the deep learning based
methods in the majority of the datasets. For fruit counting though, the deep
learning based approach performs better for all of the datasets. Combining the
classical detection method together with the neural network based counting
approach, we achieve remarkable yield accuracies ranging from 95.56% to 97.83%.


Semantics-Aware Image to Image Translation and Domain Transfer

  Image to image translation is the problem of transferring an image from a
source domain to a target domain. We present a new method to transfer the
underlying semantics of an image even when there are geometric changes across
the two domains. Specifically, we present a Generative Adversarial Network
(GAN) that can transfer semantic information presented as segmentation masks.
Our main technical contribution is an encoder-decoder based generator
architecture that jointly encodes the image and its underlying semantics and
translates both simultaneously to the target domain. Additionally, we propose
object transfiguration and cross-domain semantic consistency losses that
preserve the underlying semantic labels maps. We demonstrate the effectiveness
of our approach in multiple object transfiguration and domain transfer tasks
through qualitative and quantitative experiments. The results show that our
method is better at transferring image semantics than state of the art image to
image translation methods.


View Selection with Geometric Uncertainty Modeling

  Estimating positions of world points from features observed in images is a
key problem in 3D reconstruction, image mosaicking,simultaneous localization
and mapping and structure from motion. We consider a special instance in which
there is a dominant ground plane $\mathcal{G}$ viewed from a parallel viewing
plane $\mathcal{S}$ above it. Such instances commonly arise, for example, in
aerial photography. Consider a world point $g \in \mathcal{G}$ and its worst
case reconstruction uncertainty $\varepsilon(g,\mathcal{S})$ obtained by
merging \emph{all} possible views of $g$ chosen from $\mathcal{S}$. We first
show that one can pick two views $s_p$ and $s_q$ such that the uncertainty
$\varepsilon(g,\{s_p,s_q\})$ obtained using only these two views is almost as
good as (i.e. within a small constant factor of) $\varepsilon(g,\mathcal{S})$.
Next, we extend the result to the entire ground plane $\mathcal{G}$ and show
that one can pick a small subset of $\mathcal{S'} \subseteq \mathcal{S}$ (which
grows only linearly with the area of $\mathcal{G}$) and still obtain a constant
factor approximation, for every point $g \in \mathcal{G}$, to the minimum worst
case estimate obtained by merging all views in $\mathcal{S}$. Finally, we
present a multi-resolution view selection method which extends our techniques
to non-planar scenes. We show that the method can produce rich and accurate
dense reconstructions with a small number of views. Our results provide a view
selection mechanism with provable performance guarantees which can drastically
increase the speed of scene reconstruction algorithms. In addition to
theoretical results, we demonstrate their effectiveness in an application where
aerial imagery is used for monitoring farms and orchards.


Design and Evaluation of a Novel Cable-Driven Gripper with Perception
  Capabilities for Strawberry Picking Robots

  This paper presents a novel cable-driven gripper with perception capabilities
for autonomous harvesting of strawberries. Experiments show that the gripper
allows for more accurate and faster picking of strawberries compared to
existing systems. The gripper consists of four functional parts for sensing,
picking, transmission, and storing. It has six fingers that open to form a
closed space to swallow a target strawberry and push other surrounding berries
away from the target. Equipped with three IR sensors, the gripper controls a
manipulator arm to correct for positional error, and can thus pick strawberries
that are not exactly localized by the vision algorithm, improving the
robustness. Experiments show that the gripper is gentle on the berries as it
merely cuts the stem and there is no physical interaction with the berries
during the cutting process. We show that the gripper has close-to-perfect
successful picking rate when addressing isolated strawberries. By including
internal perception, we get high positional error tolerance, and avoid using
slow, high-level closed-loop control. Moreover, the gripper can store several
berries, which reduces the overall travel distance for the manipulator, and
decreases the time needed to pick a single strawberry substantially. The
experiments show that the gripper design decreased picking execution time
noticeably compared to results found in literature.


Semantic Mapping for Orchard Environments by Merging Two-Sides
  Reconstructions of Tree Rows

  Measuring semantic traits for phenotyping is an essential but labor-intensive
activity in horticulture. Researchers often rely on manual measurements which
may not be accurate for tasks such as measuring tree volume. To improve the
accuracy of such measurements and to automate the process, we consider the
problem of building coherent three dimensional (3D) reconstructions of orchard
rows. Even though 3D reconstructions of side views can be obtained using
standard mapping techniques, merging the two side-views is difficult due to the
lack of overlap between the two partial reconstructions. Our first main
contribution in this paper is a novel method that utilizes global features and
semantic information to obtain an initial solution aligning the two sides. Our
mapping approach then refines the 3D model of the entire tree row by
integrating semantic information common to both sides, and extracted using our
novel robust detection and fitting algorithms. Next, we present a vision system
to measure semantic traits from the optimized 3D model that is built from the
RGB or RGB-D data captured by only a camera. Specifically, we show how canopy
volume, trunk diameter, tree height and fruit count can be automatically
obtained in real orchard environments. The experiment results from multiple
datasets quantitatively demonstrate the high accuracy and robustness of our
method.


Pixels to Plans: Learning Non-Prehensile Manipulation by Imitating a
  Planner

  We present a novel method enabling robots to quickly learn to manipulate
objects by leveraging a motion planner to generate "expert" training
trajectories from a small amount of human-labeled data. In contrast to the
traditional sense-plan-act cycle, we propose a deep learning architecture and
training regimen called PtPNet that can estimate effective end-effector
trajectories for manipulation directly from a single RGB-D image of an object.
Additionally, we present a data collection and augmentation pipeline that
enables the automatic generation of large numbers (millions) of training image
and trajectory examples with almost no human labeling effort.
  We demonstrate our approach in a non-prehensile tool-based manipulation task,
specifically picking up shoes with a hook. In hardware experiments, PtPNet
generates motion plans (open-loop trajectories) that reliably (89% success over
189 trials) pick up four very different shoes from a range of positions and
orientations, and reliably picks up a shoe it has never seen before. Compared
with a traditional sense-plan-act paradigm, our system has the advantages of
operating on sparse information (single RGB-D frame), producing high-quality
trajectories much faster than the "expert" planner (300ms versus several
seconds), and generalizing effectively to previously unseen shoes.


A Leapfrog Strategy for Pursuit-Evasion in a Polygonal Environment

  We study pursuit-evasion in a polygonal environment with polygonal obstacles.
In this turn based game, an evader $e$ is chased by pursuers $p_1, p_2, ...,
p_{\ell}$. The players have full information about the environment and the
location of the other players. The pursuers are allowed to coordinate their
actions. On the pursuer turn, each $p_i$ can move to any point at distance at
most 1 from his current location. On the evader turn, he moves similarly. The
pursuers win if some pursuer becomes co-located with the evader in finite time.
The evader wins if he can evade capture forever.
  It is known that one pursuer can capture the evader in any simply-connected
polygonal environment, and that three pursuers are always sufficient in any
polygonal environment (possibly with polygonal obstacles). We contribute two
new results to this field. First, we fully characterize when an environment
with a single obstacles is one-pursuer-win or two-pursuer-win. Second, we give
sufficient (but not necessary) conditions for an environment to have a winning
strategy for two pursuers. Such environments can be swept by a \emph{leapfrog
strategy} in which the two cops alternately guard/increase the currently
controlled area. The running time of this algorithm is $O(n \cdot h \cdot
{diam}(P))$ where $n$ is the number of vertices, $h$ is the number of obstacles
and ${diam}(P)$ is the diameter of $P$.
  More concretely, for an environment with $n$ vertices, we describe an
$O(n^2)$ algorithm that (1) determines whether the obstacles are
well-separated, and if so, (2) constructs the required partition for a leapfrog
strategy.


Line-of-Sight Pursuit in Strictly Sweepable Polygons

  We study a turn-based game in a simply connected polygonal environment $Q$
between a pursuer $P$ and an adversarial evader $E$. Both players can move in a
straight line to any point within unit distance during their turn. The pursuer
$P$ wins by capturing the evader, meaning that their distance satisfies $d(P,
E) \leq 1$, while the evader wins by eluding capture forever. Both players have
a map of the environment, but they have different sensing capabilities. The
evader $E$ always knows the location of $P$. Meanwhile, $P$ only has
line-of-sight visibility: $P$ observes the evader's position only when the line
segment connecting them lies entirely within the polygon. Therefore $P$ must
search for $E$ when the evader is hidden from view.
  We provide a winning strategy for $P$ in the family of strictly sweepable
polygons, meaning that a straight line $L$ can be moved continuously over $Q$
so that (1) $L \cap Q$ is always convex and (2) every point on the boundary
$\partial Q$ is swept exactly once. This strict sweeping requires that $L$
moves along $Q$ via a sequence of translations and rotations. We develop our
main result by first considering pursuit in the subfamilies of monotone
polygons (where $L$ moves by translation) and scallop polygons (where $L$ moves
by a single rotation). Our algorithm uses rook strategy during its active
pursuit phase, rather than the well-known lion strategy. The rook strategy is
crucial for obtaining a capture time that is linear in the area of $Q$. For
monotone and scallop polygons, our algorithm has a capture time of $O(n(Q) +
\mbox{area}(Q))$, where $n(Q)$ is the number of polygon vertices. The capture
time bound for strictly sweepable polygons is $O( n(Q) \cdot \mbox{area}(Q) )$.


Minimizing Uncertainty through Sensor Placement with Angle Constraints

  We study the problem of sensor placement in environments in which
localization is a necessity, such as ad-hoc wireless sensor networks that allow
the placement of a few anchors that know their location or sensor arrays that
are tracking a target. In most of these situations, the quality of localization
depends on the relative angle between the target and the pair of sensors
observing it. In this paper, we consider placing a small number of sensors
which ensure good angular $\alpha$-coverage: given $\alpha$ in $[0,\pi/2]$, for
each target location $t$, there must be at least two sensors $s_1$ and $s_2$
such that the $\angle(s_1 t s_2)$ is in the interval $[\alpha, \pi-\alpha]$.
One of the main difficulties encountered in such problems is that since the
constraints depend on at least two sensors, building a solution must account
for the inherent dependency between selected sensors, a feature that generic
Set Cover techniques do not account for. We introduce a general framework that
guarantees an angular coverage that is arbitrarily close to $\alpha$ for any
$\alpha <= \pi/3$ and apply it to a variety of problems to get bi-criteria
approximations. When the angular coverage is required to be at least a constant
fraction of $\alpha$, we obtain results that are strictly better than what
standard geometric Set Cover methods give. When the angular coverage is
required to be at least $(1-1/\delta)\cdot\alpha$, we obtain a
$\mathcal{O}(\log \delta)$- approximation for sensor placement with
$\alpha$-coverage on the plane. In the presence of additional distance or
visibility constraints, the framework gives a $\mathcal{O}(\log\delta\cdot\log
k_{OPT})$-approximation, where $k_{OPT}$ is the size of the optimal solution.
We also use our framework to give a $\mathcal{O}(\log \delta)$-approximation
that ensures $(1-1/\delta)\cdot \alpha$-coverage and covers every target within
distance $3R$.


