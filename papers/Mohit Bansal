Photovoltage Bleaching in Bulk Heterojunction Solar Cells through
  Occupation of the Charge Transfer State

  We observe a strong peak in the capacitive photocurrent of a MDMO-PPV / PCBM
bulk heterojunction solar cell for excitation below the absorbance threshold
energy. Illumination at the peak energy blocks charge capture at other
wavelengths, and causes the photovoltage to drop dramatically. These results
suggest that the new peak is due to a charge transfer state, which provides a
pathway for charge separation and photocurrent generation in the solar cell.


Web-scale Surface and Syntactic n-gram Features for Dependency Parsing

  We develop novel first- and second-order features for dependency parsing
based on the Google Syntactic Ngrams corpus, a collection of subtree counts of
parsed sentences from scanned books. We also extend previous work on surface
$n$-gram features from Web1T to the Google Books corpus and from first-order to
second-order, comparing and analysing performance over newswire and web
treebanks.
  Surface and syntactic $n$-grams both produce substantial and complementary
gains in parsing accuracy across domains. Our best system combines the two
feature sets, achieving up to 0.8% absolute UAS improvements on newswire and
1.4% on web text.


Charagram: Embedding Words and Sentences via Character n-grams

  We present Charagram embeddings, a simple approach for learning
character-based compositional models to embed textual sequences. A word or
sentence is represented using a character n-gram count vector, followed by a
single nonlinear transformation to yield a low-dimensional embedding. We use
three tasks for evaluation: word similarity, sentence similarity, and
part-of-speech tagging. We demonstrate that Charagram embeddings outperform
more complex architectures based on character-level recurrent and convolutional
neural networks, achieving new state-of-the-art performance on several
similarity tasks.


Video Highlight Prediction Using Audience Chat Reactions

  Sports channel video portals offer an exciting domain for research on
multimodal, multilingual analysis. We present methods addressing the problem of
automatic video highlight prediction based on joint visual features and textual
analysis of the real-world audience discourse with complex slang, in both
English and traditional Chinese. We present a novel dataset based on League of
Legends championships recorded from North American and Taiwanese Twitch.tv
channels (will be released for further research), and demonstrate strong
results on these using multimodal, character-level CNN-RNN model architectures.


Hierarchically-Attentive RNN for Album Summarization and Storytelling

  We address the problem of end-to-end visual storytelling. Given a photo
album, our model first selects the most representative (summary) photos, and
then composes a natural language story for the album. For this task, we make
use of the Visual Storytelling dataset and a model composed of three
hierarchically-attentive Recurrent Neural Nets (RNNs) to: encode the album
photos, select representative (summary) photos, and compose the story.
Automatic and human evaluations show our model achieves better performance on
selection, generation, and retrieval than baselines.


Efficient Algorithms for Discrepancy Minimization in Convex Sets

  A result of Spencer states that every collection of $n$ sets over a universe
of size $n$ has a coloring of the ground set with $\{-1,+1\}$ of discrepancy
$O(\sqrt{n})$. A geometric generalization of this result was given by Gluskin
(see also Giannopoulos) who showed that every symmetric convex body $K\subseteq
R^n$ with Gaussian measure at least $e^{-\epsilon n}$, for a small
$\epsilon>0$, contains a point $y\in K$ where a constant fraction of
coordinates of $y$ are in $\{-1,1\}$. This is often called a partial coloring
result. While both these results were inherently non-algorithmic, recently
Bansal (see also Lovett-Meka) gave a polynomial time algorithm for Spencer's
setting and Rothvo\ss gave a randomized polynomial time algorithm obtaining the
same guarantee as the result of Gluskin and Giannopoulos.
  This paper has several related results. First we prove another constructive
version of the result of Gluskin and Giannopoulos via an optimization of a
linear function. This implies a linear programming based algorithm for
combinatorial discrepancy obtaining the same result as Spencer. Our second
result gives a new approach to obtains partial colorings and shows that every
convex body $K\subseteq R^n$, possibly non-symmetric, with Gaussian measure at
least $e^{-\epsilon n}$, for a small $\epsilon>0$, contains a point $y\in K$
where a constant fraction of coordinates of $y$ are in $\{-1,1\}$. Finally, we
give a simple proof that shows that for any $\delta >0$ there exists a constant
$c>0$ such that given a body $K$ with $\gamma_n(K)\geq \delta$, a uniformly
random $x$ from $\{-1,1\}^n$ is in $cK$ with constant probability. This gives
an algorithmic version of a special case of the result of Banaszczyk.


Question Relevance in VQA: Identifying Non-Visual And False-Premise
  Questions

  Visual Question Answering (VQA) is the task of answering natural-language
questions about images. We introduce the novel problem of determining the
relevance of questions to images in VQA. Current VQA models do not reason about
whether a question is even related to the given image (e.g. What is the capital
of Argentina?) or if it requires information from external resources to answer
correctly. This can break the continuity of a dialogue in human-machine
interaction. Our approaches for determining relevance are composed of two
stages. Given an image and a question, (1) we first determine whether the
question is visual or not, (2) if visual, we determine whether the question is
relevant to the given image or not. Our approaches, based on LSTM-RNNs, VQA
model uncertainty, and caption-question similarity, are able to outperform
strong baselines on both relevance tasks. We also present human studies showing
that VQA models augmented with such question relevance reasoning are perceived
as more intelligent, reasonable, and human-like.


Sort Story: Sorting Jumbled Images and Captions into Stories

  Temporal common sense has applications in AI tasks such as QA, multi-document
summarization, and human-AI communication. We propose the task of sequencing --
given a jumbled set of aligned image-caption pairs that belong to a story, the
task is to sort them such that the output sequence forms a coherent story. We
present multiple approaches, via unary (position) and pairwise (order)
predictions, and their ensemble-based combinations, achieving strong results on
this task. We use both text-based and image-based features, which depict
complementary improvements. Using qualitative examples, we demonstrate that our
models have learnt interesting aspects of temporal common sense.


End-to-End Relation Extraction using LSTMs on Sequences and Tree
  Structures

  We present a novel end-to-end neural model to extract entities and relations
between them. Our recurrent neural network based model captures both word
sequence and dependency tree substructure information by stacking bidirectional
tree-structured LSTM-RNNs on bidirectional sequential LSTM-RNNs. This allows
our model to jointly represent both entities and relations with shared
parameters in a single model. We further encourage detection of entities during
training and use of entity information in relation extraction via entity
pretraining and scheduled sampling. Our model improves over the
state-of-the-art feature-based model on end-to-end relation extraction,
achieving 12.1% and 5.7% relative error reductions in F1-score on ACE2005 and
ACE2004, respectively. We also show that our LSTM-RNN based model compares
favorably to the state-of-the-art CNN based model (in F1-score) on nominal
relation classification (SemEval-2010 Task 8). Finally, we present an extensive
ablation analysis of several model components.


The Role of Context Types and Dimensionality in Learning Word Embeddings

  We provide the first extensive evaluation of how using different types of
context to learn skip-gram word embeddings affects performance on a wide range
of intrinsic and extrinsic NLP tasks. Our results suggest that while intrinsic
tasks tend to exhibit a clear preference to particular types of contexts and
higher dimensionality, more careful tuning is required for finding the optimal
settings for most of the extrinsic tasks that we considered. Furthermore, for
these extrinsic tasks, we find that once the benefit from increasing the
embedding dimensionality is mostly exhausted, simple concatenation of word
embeddings, learned with different context types, can yield further performance
gains. As an additional contribution, we propose a new variant of the skip-gram
model that learns word embeddings from weighted contexts of substitute words.


Parsing Speech: A Neural Approach to Integrating Lexical and
  Acoustic-Prosodic Information

  In conversational speech, the acoustic signal provides cues that help
listeners disambiguate difficult parses. For automatically parsing spoken
utterances, we introduce a model that integrates transcribed text and
acoustic-prosodic features using a convolutional neural network over energy and
pitch trajectories coupled with an attention-based recurrent neural network
that accepts text and prosodic features. We find that different types of
acoustic-prosodic features are individually helpful, and together give
statistically significant improvements in parse and disfluency detection F1
scores over a strong text-only baseline. For this study with known sentence
boundaries, error analyses show that the main benefit of acoustic-prosodic
features is in sentences with disfluencies, attachment decisions are most
improved, and transcription errors obscure gains from prosody.


Multi-Task Video Captioning with Video and Entailment Generation

  Video captioning, the task of describing the content of a video, has seen
some promising improvements in recent years with sequence-to-sequence models,
but accurately learning the temporal and logical dynamics involved in the task
still remains a challenge, especially given the lack of sufficient annotated
data. We improve video captioning by sharing knowledge with two related
directed-generation tasks: a temporally-directed unsupervised video prediction
task to learn richer context-aware video encoder representations, and a
logically-directed language entailment generation task to learn better
video-entailed caption decoder representations. For this, we present a
many-to-many multi-task learning model that shares parameters across the
encoders and decoders of the three tasks. We achieve significant improvements
and the new state-of-the-art on several standard video captioning datasets
using diverse automatic and human evaluations. We also show mutual multi-task
improvements on the entailment generation task.


Punny Captions: Witty Wordplay in Image Descriptions

  Wit is a form of rich interaction that is often grounded in a specific
situation (e.g., a comment in response to an event). In this work, we attempt
to build computational models that can produce witty descriptions for a given
image. Inspired by a cognitive account of humor appreciation, we employ
linguistic wordplay, specifically puns, in image descriptions. We develop two
approaches which involve retrieving witty descriptions for a given image from a
large corpus of sentences, or generating them via an encoder-decoder neural
network architecture. We compare our approach against meaningful baseline
approaches via human studies and show substantial improvements. We find that
when a human is subject to similar constraints as the model regarding word
usage and style, people vote the image descriptions generated by our model to
be slightly wittier than human-written witty descriptions. Unsurprisingly,
humans are almost always wittier than the model when they are free to choose
the vocabulary, style, etc.


From Paraphrase Database to Compositional Paraphrase Model and Back

  The Paraphrase Database (PPDB; Ganitkevitch et al., 2013) is an extensive
semantic resource, consisting of a list of phrase pairs with (heuristic)
confidence estimates. However, it is still unclear how it can best be used, due
to the heuristic nature of the confidences and its necessarily incomplete
coverage. We propose models to leverage the phrase pairs from the PPDB to build
parametric paraphrase models that score paraphrase pairs more accurately than
the PPDB's internal scores while simultaneously improving its coverage. They
allow for learning phrase embeddings as well as improved word embeddings.
Moreover, we introduce two new, manually annotated datasets to evaluate
short-phrase paraphrasing models. Using our paraphrase model trained using
PPDB, we achieve state-of-the-art results on standard word and bigram
similarity tasks and beat strong baselines on our new short phrase paraphrase
tasks.


Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to
  Action Sequences

  We propose a neural sequence-to-sequence model for direction following, a
task that is essential to realizing effective autonomous agents. Our
alignment-based encoder-decoder model with long short-term memory recurrent
neural networks (LSTM-RNN) translates natural language instructions to action
sequences based upon a representation of the observable world state. We
introduce a multi-level aligner that empowers our model to focus on sentence
"regions" salient to the current world state by using multiple abstractions of
the input sentence. In contrast to existing methods, our model uses no
specialized linguistic resources (e.g., parsers) or task-specific annotations
(e.g., seed lexicons). It is therefore generalizable, yet still achieves the
best results reported to-date on a benchmark single-sentence dataset and
competitive results for the limited-training multi-sentence setting. We analyze
our model through a series of ablations that elucidate the contributions of the
primary components of our model.


What to talk about and how? Selective Generation using LSTMs with
  Coarse-to-Fine Alignment

  We propose an end-to-end, domain-independent neural encoder-aligner-decoder
model for selective generation, i.e., the joint task of content selection and
surface realization. Our model first encodes a full set of over-determined
database event records via an LSTM-based recurrent neural network, then
utilizes a novel coarse-to-fine aligner to identify the small subset of salient
records to talk about, and finally employs a decoder to generate free-form
descriptions of the aligned, selected records. Our model achieves the best
selection and generation results reported to-date (with 59% relative
improvement in generation) on the benchmark WeatherGov dataset, despite using
no specialized features or linguistic resources. Using an improved k-nearest
neighbor beam filter helps further. We also perform a series of ablations and
visualizations to elucidate the contributions of our key model components.
Lastly, we evaluate the generalizability of our model on the RoboCup dataset,
and get results that are competitive with or better than the state-of-the-art,
despite being severely data-starved.


Mapping Unseen Words to Task-Trained Embedding Spaces

  We consider the supervised training setting in which we learn task-specific
word embeddings. We assume that we start with initial embeddings learned from
unlabelled data and update them to learn task-specific embeddings for words in
the supervised training data. However, for new words in the test set, we must
use either their initial embeddings or a single unknown embedding, which often
leads to errors. We address this by learning a neural network to map from
initial embeddings to the task-specific embedding space, via a multi-loss
objective function. The technique is general, but here we demonstrate its use
for improved dependency parsing (especially for sentences with
out-of-vocabulary words), as well as for downstream improvements on sentiment
analysis.


Accurate Vision-based Vehicle Localization using Satellite Imagery

  We propose a method for accurately localizing ground vehicles with the aid of
satellite imagery. Our approach takes a ground image as input, and outputs the
location from which it was taken on a georeferenced satellite image. We perform
visual localization by estimating the co-occurrence probabilities between the
ground and satellite images based on a ground-satellite feature dictionary. The
method is able to estimate likelihoods over arbitrary locations without the
need for a dense ground image database. We present a ranking-loss based
algorithm that learns location-discriminative feature projection matrices that
result in further improvements in accuracy. We evaluate our method on the
Malaga and KITTI public datasets and demonstrate significant improvements over
a baseline that performs exhaustive search.


Who did What: A Large-Scale Person-Centered Cloze Dataset

  We have constructed a new "Who-did-What" dataset of over 200,000
fill-in-the-gap (cloze) multiple choice reading comprehension problems
constructed from the LDC English Gigaword newswire corpus. The WDW dataset has
a variety of novel features. First, in contrast with the CNN and Daily Mail
datasets (Hermann et al., 2015) we avoid using article summaries for question
formation. Instead, each problem is formed from two independent articles --- an
article given as the passage to be read and a separate article on the same
events used to form the question. Second, we avoid anonymization --- each
choice is a person named entity. Third, the problems have been filtered to
remove a fraction that are easily solved by simple baselines, while remaining
84% solvable by humans. We report performance benchmarks of standard systems
and propose the WDW dataset as a challenge task for the community.


Interpreting Neural Networks to Improve Politeness Comprehension

  We present an interpretable neural network approach to predicting and
understanding politeness in natural language requests. Our models are based on
simple convolutional neural networks directly on raw text, avoiding any manual
identification of complex sentiment or syntactic features, while performing
better than such feature-based models from previous work. More importantly, we
use the challenging task of politeness prediction as a testbed to next present
a much-needed understanding of what these successful networks are actually
learning. For this, we present several network visualizations based on
activation clusters, first derivative saliency, and embedding space
transformations, helping us automatically identify several subtle linguistics
markers of politeness theories. Further, this analysis reveals multiple novel,
high-scoring politeness strategies which, when added back as new features,
reduce the accuracy gap between the original featurized system and the neural
model, thus providing a clear quantitative interpretation of the success of
these neural networks.


Navigational Instruction Generation as Inverse Reinforcement Learning
  with Neural Machine Translation

  Modern robotics applications that involve human-robot interaction require
robots to be able to communicate with humans seamlessly and effectively.
Natural language provides a flexible and efficient medium through which robots
can exchange information with their human partners. Significant advancements
have been made in developing robots capable of interpreting free-form
instructions, but less attention has been devoted to endowing robots with the
ability to generate natural language. We propose a navigational guide model
that enables robots to generate natural language instructions that allow humans
to navigate a priori unknown environments. We first decide which information to
share with the user according to their preferences, using a policy trained from
human demonstrations via inverse reinforcement learning. We then "translate"
this information into a natural language instruction using a neural
sequence-to-sequence model that learns to generate free-form instructions from
natural language corpora. We evaluate our method on a benchmark route
instruction dataset and achieve a BLEU score of 72.18% when compared to
human-generated reference instructions. We additionally conduct navigation
experiments with human participants that demonstrate that our method generates
instructions that people follow as accurately and easily as those produced by
humans.


Coherent Dialogue with Attention-based Language Models

  We model coherent conversation continuation via RNN-based dialogue models
equipped with a dynamic attention mechanism. Our attention-RNN language model
dynamically increases the scope of attention on the history as the conversation
continues, as opposed to standard attention (or alignment) models with a fixed
input scope in a sequence-to-sequence model. This allows each generated word to
be associated with the most relevant words in its corresponding conversation
history. We evaluate the model on two popular dialogue datasets, the
open-domain MovieTriples dataset and the closed-domain Ubuntu Troubleshoot
dataset, and achieve significant improvements over the state-of-the-art and
baselines on several metrics, including complementary diversity-based metrics,
human evaluation, and qualitative visualizations. We also show that a vanilla
RNN with dynamic attention outperforms more complex memory models (e.g., LSTM
and GRU) by allowing for flexible, long-distance memory. We promote further
coherence via topic modeling-based reranking.


A Joint Speaker-Listener-Reinforcer Model for Referring Expressions

  Referring expressions are natural language constructions used to identify
particular objects within a scene. In this paper, we propose a unified
framework for the tasks of referring expression comprehension and generation.
Our model is composed of three modules: speaker, listener, and reinforcer. The
speaker generates referring expressions, the listener comprehends referring
expressions, and the reinforcer introduces a reward function to guide sampling
of more discriminative expressions. The listener-speaker modules are trained
jointly in an end-to-end learning framework, allowing the modules to be aware
of one another during learning while also benefiting from the discriminative
reinforcer's feedback. We demonstrate that this unified framework and training
achieves state-of-the-art results for both comprehension and generation on
three referring expression datasets. Project and demo page:
https://vision.cs.unc.edu/refer


Efficient Generation of Motion Plans from Attribute-Based Natural
  Language Instructions Using Dynamic Constraint Mapping

  We present an algorithm for combining natural language processing (NLP) and
fast robot motion planning to automatically generate robot movements. Our
formulation uses a novel concept called Dynamic Constraint Mapping to transform
complex, attribute-based natural language instructions into appropriate cost
functions and parametric constraints for optimization-based motion planning. We
generate a factor graph from natural language instructions called the Dynamic
Grounding Graph (DGG), which takes latent parameters into account. The
coefficients of this factor graph are learned based on conditional random
fields (CRFs) and are used to dynamically generate the constraints for motion
planning. We map the cost function directly to the motion parameters of the
planner and compute smooth trajectories in dynamic scenes. We highlight the
performance of our approach in a simulated environment and via a human
interacting with a 7-DOF Fetch robot using intricate language commands
including negation, orientation specification, and distance constraints.


Source-Target Inference Models for Spatial Instruction Understanding

  Models that can execute natural language instructions for situated robotic
tasks such as assembly and navigation have several useful applications in
homes, offices, and remote scenarios. We study the semantics of
spatially-referred configuration and arrangement instructions, based on the
challenging Bisk-2016 blank-labeled block dataset. This task involves finding a
source block and moving it to the target position (mentioned via a reference
block and offset), where the blocks have no names or colors and are just
referred to via spatial location features. We present novel models for the
subtasks of source block classification and target position regression, based
on joint-loss language and spatial-world representation learning, as well as
CNN-based and dual attention models to compute the alignment between the world
blocks and the instruction phrases. For target position prediction, we compare
two inference approaches: annealed sampling via policy gradient versus
expectation inference via supervised regression. Our models achieve the new
state-of-the-art on this task, with an improvement of 47% on source block
accuracy and 22% on target position distance.


Reinforced Video Captioning with Entailment Rewards

  Sequence-to-sequence models have shown promising improvements on the temporal
task of video captioning, but they optimize word-level cross-entropy loss
during training. First, using policy gradient and mixed-loss methods for
reinforcement learning, we directly optimize sentence-level task-based metrics
(as rewards), achieving significant improvements over the baseline, based on
both automatic metrics and human evaluation on multiple datasets. Next, we
propose a novel entailment-enhanced reward (CIDEnt) that corrects
phrase-matching based metrics (such as CIDEr) to only allow for
logically-implied partial matches and avoid contradictions, achieving further
significant improvements over the CIDEr-reward model. Overall, our
CIDEnt-reward model achieves the new state-of-the-art on the MSR-VTT dataset.


Shortcut-Stacked Sentence Encoders for Multi-Domain Inference

  We present a simple sequential sentence encoder for multi-domain natural
language inference. Our encoder is based on stacked bidirectional LSTM-RNNs
with shortcut connections and fine-tuning of word embeddings. The overall
supervised model uses the above encoder to encode two input sentences into two
vectors, and then uses a classifier over the vector combination to label the
relationship between these two sentences as that of entailment, contradiction,
or neural. Our Shortcut-Stacked sentence encoders achieve strong improvements
over existing encoders on matched and mismatched multi-domain natural language
inference (top non-ensemble single-model result in the EMNLP RepEval 2017
Shared Task (Nangia et al., 2017)). Moreover, they achieve the new
state-of-the-art encoding result on the original SNLI dataset (Bowman et al.,
2015).


MAttNet: Modular Attention Network for Referring Expression
  Comprehension

  In this paper, we address referring expression comprehension: localizing an
image region described by a natural language expression. While most recent work
treats expressions as a single unit, we propose to decompose them into three
modular components related to subject appearance, location, and relationship to
other objects. This allows us to flexibly adapt to expressions containing
different types of information in an end-to-end framework. In our model, which
we call the Modular Attention Network (MAttNet), two types of attention are
utilized: language-based attention that learns the module weights as well as
the word/phrase attention that each module should focus on; and visual
attention that allows the subject and relationship modules to focus on relevant
image components. Module weights combine scores from all three modules
dynamically to output an overall score. Experiments show that MAttNet
outperforms previous state-of-art methods by a large margin on both
bounding-box-level and pixel-level comprehension tasks. Demo and code are
provided.


Detecting Linguistic Characteristics of Alzheimer's Dementia by
  Interpreting Neural Models

  Alzheimer's disease (AD) is an irreversible and progressive brain disease
that can be stopped or slowed down with medical treatment. Language changes
serve as a sign that a patient's cognitive functions have been impacted,
potentially leading to early diagnosis. In this work, we use NLP techniques to
classify and analyze the linguistic characteristics of AD patients using the
DementiaBank dataset. We apply three neural models based on CNNs, LSTM-RNNs,
and their combination, to distinguish between language samples from AD and
control patients. We achieve a new independent benchmark accuracy for the AD
classification task. More importantly, we next interpret what these neural
models have learned about the linguistic characteristics of AD patients, via
analysis based on activation clustering and first-derivative saliency
techniques. We then perform novel automatic pattern discovery inside activation
clusters, and consolidate AD patients' distinctive grammar patterns.
Additionally, we show that first derivative saliency can not only rediscover
previous language patterns of AD patients, but also shed light on the
limitations of neural models. Lastly, we also include analysis of
gender-separated AD data.


Multi-Reward Reinforced Summarization with Saliency and Entailment

  Abstractive text summarization is the task of compressing and rewriting a
long document into a short summary while maintaining saliency, directed logical
entailment, and non-redundancy. In this work, we address these three important
aspects of a good summary via a reinforcement learning approach with two novel
reward functions: ROUGESal and Entail, on top of a coverage-based baseline. The
ROUGESal reward modifies the ROUGE metric by up-weighting the salient
phrases/words detected via a keyphrase classifier. The Entail reward gives high
(length-normalized) scores to logically-entailed summaries using an entailment
classifier. Further, we show superior performance improvement when these
rewards are combined with traditional metric (ROUGE) based rewards, via our
novel and effective multi-reward approach of optimizing multiple rewards
simultaneously in alternate mini-batches. Our method achieves the new
state-of-the-art results (including human evaluation) on the CNN/Daily Mail
dataset as well as strong improvements in a test-only transfer setup on
DUC-2002.


Robust Machine Comprehension Models via Adversarial Training

  It is shown that many published models for the Stanford Question Answering
Dataset (Rajpurkar et al., 2016) lack robustness, suffering an over 50%
decrease in F1 score during adversarial evaluation based on the AddSent (Jia
and Liang, 2017) algorithm. It has also been shown that retraining models on
data generated by AddSent has limited effect on their robustness. We propose a
novel alternative adversary-generation algorithm, AddSentDiverse, that
significantly increases the variance within the adversarial training data by
providing effective examples that punish the model for making certain
superficial assumptions. Further, in order to improve robustness to AddSent's
semantic perturbations (e.g., antonyms), we jointly improve the model's
semantic-relationship learning capabilities in addition to our
AddSentDiverse-based adversarial training data augmentation. With these
additions, we show that we can make a state-of-the-art model significantly more
robust, achieving a 36.5% increase in F1 score under many different types of
adversarial evaluation while maintaining performance on the regular SQuAD task.


Object Ordering with Bidirectional Matchings for Visual Reasoning

  Visual reasoning with compositional natural language instructions, e.g.,
based on the newly-released Cornell Natural Language Visual Reasoning (NLVR)
dataset, is a challenging task, where the model needs to have the ability to
create an accurate mapping between the diverse phrases and the several objects
placed in complex arrangements in the image. Further, this mapping needs to be
processed to answer the question in the statement given the ordering and
relationship of the objects across three similar images. In this paper, we
propose a novel end-to-end neural model for the NLVR task, where we first use
joint bidirectional attention to build a two-way conditioning between the
visual information and the language phrases. Next, we use an RL-based pointer
network to sort and process the varying number of unordered objects (so as to
match the order of the statement phrases) in each of the three images and then
pool over the three decisions. Our model achieves strong improvements (of 4-6%
absolute) over the state-of-the-art on both the structured representation and
raw image versions of the dataset.


Polite Dialogue Generation Without Parallel Data

  Stylistic dialogue response generation, with valuable applications in
personality-based conversational agents, is a challenging task because the
response needs to be fluent, contextually-relevant, as well as
paralinguistically accurate. Moreover, parallel datasets for
regular-to-stylistic pairs are usually unavailable. We present three
weakly-supervised models that can generate diverse polite (or rude) dialogue
responses without parallel data. Our late fusion model (Fusion) merges the
decoder of an encoder-attention-decoder dialogue model with a language model
trained on stand-alone polite utterances. Our label-fine-tuning (LFT) model
prepends to each source sequence a politeness-score scaled label (predicted by
our state-of-the-art politeness classifier) during training, and at test time
is able to generate polite, neutral, and rude responses by simply scaling the
label embedding by the corresponding score. Our reinforcement learning model
(Polite-RL) encourages politeness generation by assigning rewards proportional
to the politeness classifier score of the sampled response. We also present two
retrieval-based polite dialogue model baselines. Human evaluation validates
that while the Fusion and the retrieval-based models achieve politeness with
poorer context-relevance, the LFT and Polite-RL models can produce
significantly more polite responses without sacrificing dialogue quality.


Soft Layer-Specific Multi-Task Summarization with Entailment and
  Question Generation

  An accurate abstractive summary of a document should contain all its salient
information and should be logically entailed by the input document. We improve
these important aspects of abstractive summarization via multi-task learning
with the auxiliary tasks of question generation and entailment generation,
where the former teaches the summarization model how to look for salient
questioning-worthy details, and the latter teaches the model how to rewrite a
summary which is a directed-logical subset of the input document. We also
propose novel multi-task architectures with high-level (semantic)
layer-specific sharing across multiple encoder and decoder layers of the three
tasks, as well as soft-sharing mechanisms (and show performance ablations and
analysis examples of each contribution). Overall, we achieve statistically
significant improvements over the state-of-the-art on both the CNN/DailyMail
and Gigaword datasets, as well as on the DUC-2002 transfer setup. We also
present several quantitative and qualitative analysis studies of our model's
learned saliency and entailment skills.


Fast Abstractive Summarization with Reinforce-Selected Sentence
  Rewriting

  Inspired by how humans summarize long documents, we propose an accurate and
fast summarization model that first selects salient sentences and then rewrites
them abstractively (i.e., compresses and paraphrases) to generate a concise
overall summary. We use a novel sentence-level policy gradient method to bridge
the non-differentiable computation between these two neural networks in a
hierarchical way, while maintaining language fluency. Empirically, we achieve
the new state-of-the-art on all metrics (including human evaluation) on the
CNN/Daily Mail dataset, as well as significantly higher abstractiveness scores.
Moreover, by first operating at the sentence-level and then the word-level, we
enable parallel decoding of our neural generative model that results in
substantially faster (10-20x) inference speed as well as 4x faster training
convergence than previous long-paragraph encoder-decoder models. We also
demonstrate the generalization of our model on the test-only DUC-2002 dataset,
where we achieve higher scores than a state-of-the-art model.


Dynamic Multi-Level Multi-Task Learning for Sentence Simplification

  Sentence simplification aims to improve readability and understandability,
based on several operations such as splitting, deletion, and paraphrasing.
However, a valid simplified sentence should also be logically entailed by its
input sentence. In this work, we first present a strong pointer-copy mechanism
based sequence-to-sequence sentence simplification model, and then improve its
entailment and paraphrasing capabilities via multi-task learning with related
auxiliary tasks of entailment and paraphrase generation. Moreover, we propose a
novel 'multi-level' layered soft sharing approach where each auxiliary task
shares different (higher versus lower) level layers of the sentence
simplification model, depending on the task's semantic versus lexico-syntactic
nature. We also introduce a novel multi-armed bandit based training approach
that dynamically learns how to effectively switch across tasks during
multi-task learning. Experiments on multiple popular datasets demonstrate that
our model outperforms competitive simplification systems in SARI and FKGL
automatic metrics, and human evaluation. Further, we present several ablation
analyses on alternative layer sharing methods, soft versus hard sharing,
dynamic multi-armed bandit sampling approaches, and our model's learned
entailment and paraphrasing skills.


TVQA: Localized, Compositional Video Question Answering

  Recent years have witnessed an increasing interest in image-based
question-answering (QA) tasks. However, due to data limitations, there has been
much less work on video-based QA. In this paper, we present TVQA, a large-scale
video QA dataset based on 6 popular TV shows. TVQA consists of 152,545 QA pairs
from 21,793 clips, spanning over 460 hours of video. Questions are designed to
be compositional in nature, requiring systems to jointly localize relevant
moments within a clip, comprehend subtitle-based dialogue, and recognize
relevant visual concepts. We provide analyses of this new dataset as well as
several baselines and a multi-stream end-to-end trainable neural network
framework for the TVQA task. The dataset is publicly available at
http://tvqa.cs.unc.edu.


Adversarial Over-Sensitivity and Over-Stability Strategies for Dialogue
  Models

  We present two categories of model-agnostic adversarial strategies that
reveal the weaknesses of several generative, task-oriented dialogue models:
Should-Not-Change strategies that evaluate over-sensitivity to small and
semantics-preserving edits, as well as Should-Change strategies that test if a
model is over-stable against subtle yet semantics-changing modifications. We
next perform adversarial training with each strategy, employing a max-margin
approach for negative generative examples. This not only makes the target
dialogue model more robust to the adversarial inputs, but also helps it perform
significantly better on the original inputs. Moreover, training on all
strategies combined achieves further improvements, achieving a new
state-of-the-art performance on the original task (also verified via human
evaluation). In addition to adversarial training, we also address the
robustness task at the model-level, by feeding it subword units as both inputs
and outputs, and show that the resulting model is equally competitive, requires
only 1/4 of the original vocabulary size, and is robust to one of the
adversarial strategies (to which the original model is vulnerable) even without
adversarial training.


Game-Based Video-Context Dialogue

  Current dialogue systems focus more on textual and speech context knowledge
and are usually based on two speakers. Some recent work has investigated static
image-based dialogue. However, several real-world human interactions also
involve dynamic visual context (similar to videos) as well as dialogue
exchanges among multiple speakers. To move closer towards such multimodal
conversational skills and visually-situated applications, we introduce a new
video-context, many-speaker dialogue dataset based on live-broadcast soccer
game videos and chats from Twitch.tv. This challenging testbed allows us to
develop visually-grounded dialogue models that should generate relevant
temporal and spatial event language from the live video, while also being
relevant to the chat history. For strong baselines, we also present several
discriminative and generative models, e.g., based on tridirectional attention
flow (TriDAF). We evaluate these models via retrieval ranking-recall, automatic
phrase-matching metrics, as well as human evaluation studies. We also present
dataset analyses, model ablations, and visualizations to understand the
contribution of different modalities and model components.


Closed-Book Training to Improve Summarization Encoder Memory

  A good neural sequence-to-sequence summarization model should have a strong
encoder that can distill and memorize the important information from long input
texts so that the decoder can generate salient summaries based on the encoder's
memory. In this paper, we aim to improve the memorization capabilities of the
encoder of a pointer-generator model by adding an additional 'closed-book'
decoder without attention and pointer mechanisms. Such a decoder forces the
encoder to be more selective in the information encoded in its memory state
because the decoder can't rely on the extra information provided by the
attention and possibly copy modules, and hence improves the entire model. On
the CNN/Daily Mail dataset, our 2-decoder model outperforms the baseline
significantly in terms of ROUGE and METEOR metrics, for both cross-entropy and
reinforced setups (and on human evaluation). Moreover, our model also achieves
higher scores in a test-only DUC-2002 generalizability setup. We further
present a memory ability test, two saliency metrics, as well as several
sanity-check ablations (based on fixed-encoder, gradient-flow cut, and model
capacity) to prove that the encoder of our 2-decoder model does in fact learn
stronger memory representations than the baseline encoder.


SafeCity: Understanding Diverse Forms of Sexual Harassment Personal
  Stories

  With the recent rise of #MeToo, an increasing number of personal stories
about sexual harassment and sexual abuse have been shared online. In order to
push forward the fight against such harassment and abuse, we present the task
of automatically categorizing and analyzing various forms of sexual harassment,
based on stories shared on the online forum SafeCity. For the labels of
groping, ogling, and commenting, our single-label CNN-RNN model achieves an
accuracy of 86.5%, and our multi-label model achieves a Hamming score of 82.5%.
Furthermore, we present analysis using LIME, first-derivative saliency
heatmaps, activation clustering, and embedding visualization to interpret
neural model predictions and demonstrate how this extracts features that can
help automatically fill out incident reports, identify unsafe areas, avoid
unsafe practices, and 'pin the creeps'.


Analyzing Compositionality-Sensitivity of NLI Models

  Success in natural language inference (NLI) should require a model to
understand both lexical and compositional semantics. However, through
adversarial evaluation, we find that several state-of-the-art models with
diverse architectures are over-relying on the former and fail to use the
latter. Further, this compositionality unawareness is not reflected via
standard evaluation on current datasets. We show that removing RNNs in existing
models or shuffling input words during training does not induce large
performance loss despite the explicit removal of compositional information.
Therefore, we propose a compositionality-sensitivity testing setup that
analyzes models on natural examples from existing datasets that cannot be
solved via lexical features alone (i.e., on which a bag-of-words model gives a
high probability to one wrong label), hence revealing the models' actual
compositionality awareness. We show that this setup not only highlights the
limited compositional ability of current NLI models, but also differentiates
model performance based on design, e.g., separating shallow bag-of-words models
from deeper, linguistically-grounded tree-based models. Our evaluation setup is
an important analysis tool: complementing currently existing adversarial and
linguistically driven diagnostic evaluations, and exposing opportunities for
future work on evaluating models' compositional understanding.


We Are Humor Beings: Understanding and Predicting Visual Humor

  Humor is an integral part of human lives. Despite being tremendously
impactful, it is perhaps surprising that we do not have a detailed
understanding of humor yet. As interactions between humans and AI systems
increase, it is imperative that these systems are taught to understand
subtleties of human expressions such as humor. In this work, we are interested
in the question - what content in a scene causes it to be funny? As a first
step towards understanding visual humor, we analyze the humor manifested in
abstract scenes and design computational models for them. We collect two
datasets of abstract scenes that facilitate the study of humor at both the
scene-level and the object-level. We analyze the funny scenes and explore the
different types of humor depicted in them via human studies. We model two tasks
that we believe demonstrate an understanding of some aspects of visual humor.
The tasks involve predicting the funniness of a scene and altering the
funniness of a scene. We show that our models perform well quantitatively, and
qualitatively through human studies. Our datasets are publicly available.


Learning Articulated Motion Models from Visual and Lingual Signals

  In order for robots to operate effectively in homes and workplaces, they must
be able to manipulate the articulated objects common within environments built
for and by humans. Previous work learns kinematic models that prescribe this
manipulation from visual demonstrations. Lingual signals, such as natural
language descriptions and instructions, offer a complementary means of
conveying knowledge of such manipulation models and are suitable to a wide
range of interactions (e.g., remote manipulation). In this paper, we present a
multimodal learning framework that incorporates both visual and lingual
information to estimate the structure and parameters that define kinematic
models of articulated objects. The visual signal takes the form of an RGB-D
image stream that opportunistically captures object motion in an unprepared
scene. Accompanying natural language descriptions of the motion constitute the
lingual signal. We present a probabilistic language model that uses word
embeddings to associate lingual verbs with their corresponding kinematic
structures. By exploiting the complementary nature of the visual and lingual
input, our method infers correct kinematic structures for various multiple-part
objects on which the previous state-of-the-art, visual-only system fails. We
evaluate our multimodal learning framework on a dataset comprised of a variety
of household objects, and demonstrate a 36% improvement in model accuracy over
the vision-only baseline.


Towards Universal Paraphrastic Sentence Embeddings

  We consider the problem of learning general-purpose, paraphrastic sentence
embeddings based on supervision from the Paraphrase Database (Ganitkevitch et
al., 2013). We compare six compositional architectures, evaluating them on
annotated textual similarity datasets drawn both from the same distribution as
the training data and from a wide range of other domains. We find that the most
complex architectures, such as long short-term memory (LSTM) recurrent neural
networks, perform best on the in-domain data. However, in out-of-domain
scenarios, simple architectures such as word averaging vastly outperform LSTMs.
Our simplest averaging model is even competitive with systems tuned for the
particular tasks while also being extremely efficient and easy to use.
  In order to better understand how these architectures compare, we conduct
further experiments on three supervised NLP tasks: sentence similarity,
entailment, and sentiment classification. We again find that the word averaging
models perform well for sentence similarity and entailment, outperforming
LSTMs. However, on sentiment classification, we find that the LSTM performs
very strongly-even recording new state-of-the-art performance on the Stanford
Sentiment Treebank.
  We then demonstrate how to combine our pretrained sentence embeddings with
these supervised tasks, using them both as a prior and as a black box feature
extractor. This leads to performance rivaling the state of the art on the SICK
similarity and entailment tasks. We release all of our resources to the
research community with the hope that they can serve as the new baseline for
further work on universal sentence embeddings.


Contextual RNN-GANs for Abstract Reasoning Diagram Generation

  Understanding, predicting, and generating object motions and transformations
is a core problem in artificial intelligence. Modeling sequences of evolving
images may provide better representations and models of motion and may
ultimately be used for forecasting, simulation, or video generation.
Diagrammatic Abstract Reasoning is an avenue in which diagrams evolve in
complex patterns and one needs to infer the underlying pattern sequence and
generate the next image in the sequence. For this, we develop a novel
Contextual Generative Adversarial Network based on Recurrent Neural Networks
(Context-RNN-GANs), where both the generator and the discriminator modules are
based on contextual history (modeled as RNNs) and the adversarial discriminator
guides the generator to produce realistic images for the particular time step
in the image sequence. We evaluate the Context-RNN-GAN model (and its variants)
on a novel dataset of Diagrammatic Abstract Reasoning, where it performs
competitively with 10th-grade human performance but there is still scope for
interesting improvements as compared to college-grade human performance. We
also evaluate our model on a standard video next-frame prediction task,
achieving improved performance over comparable state-of-the-art.


Commonsense for Generative Multi-Hop Question Answering Tasks

  Reading comprehension QA tasks have seen a recent surge in popularity, yet
most works have focused on fact-finding extractive QA. We instead focus on a
more challenging multi-hop generative task (NarrativeQA), which requires the
model to reason, gather, and synthesize disjoint pieces of information within
the context to generate an answer. This type of multi-step reasoning also often
requires understanding implicit relations, which humans resolve via external,
background commonsense knowledge. We first present a strong generative baseline
that uses a multi-attention mechanism to perform multiple hops of reasoning and
a pointer-generator decoder to synthesize the answer. This model performs
substantially better than previous generative models, and is competitive with
current state-of-the-art span prediction models. We next introduce a novel
system for selecting grounded multi-hop relational commonsense information from
ConceptNet via a pointwise mutual information and term-frequency based scoring
function. Finally, we effectively use this extracted commonsense information to
fill in gaps of reasoning between context hops, using a selectively-gated
attention mechanism. This boosts the model's performance significantly (also
verified via human evaluation), establishing a new state-of-the-art for the
task. We also show promising initial results of the generalizability of our
background knowledge enhancements by demonstrating some improvement on
QAngaroo-WikiHop, another multi-hop reasoning dataset.


Combining Fact Extraction and Verification with Neural Semantic Matching
  Networks

  The increasing concern with misinformation has stimulated research efforts on
automatic fact checking. The recently-released FEVER dataset introduced a
benchmark fact-verification task in which a system is asked to verify a claim
using evidential sentences from Wikipedia documents. In this paper, we present
a connected system consisting of three homogeneous neural semantic matching
models that conduct document retrieval, sentence selection, and claim
verification jointly for fact extraction and verification. For evidence
retrieval (document retrieval and sentence selection), unlike traditional
vector space IR models in which queries and sources are matched in some
pre-designed term vector space, we develop neural models to perform deep
semantic matching from raw textual input, assuming no intermediate term
representation and no access to structured external knowledge bases. We also
show that Pageview frequency can also help improve the performance of evidence
retrieval results, that later can be matched by using our neural semantic
matching network. For claim verification, unlike previous approaches that
simply feed upstream retrieved evidence and the claim to a natural language
inference (NLI) model, we further enhance the NLI model by providing it with
internal semantic relatedness scores (hence integrating it with the evidence
retrieval modules) and ontological WordNet features. Experiments on the FEVER
dataset indicate that (1) our neural semantic matching method outperforms
popular TF-IDF and encoder models, by significant margins on all evidence
retrieval metrics, (2) the additional relatedness score and WordNet features
improve the NLI model via better semantic awareness, and (3) by formalizing all
three subtasks as a similar semantic matching problem and improving on all
three stages, the complete model is able to achieve the state-of-the-art
results on the FEVER test set.


Sticky Brownian Rounding and its Applications to Constraint Satisfaction
  Problems

  Semidefinite programming is a powerful tool in the design and analysis of
approximation algorithms for combinatorial optimization problems. In
particular, the random hyperplane rounding method of Goemans and Williamson,
has been extensively studied for more than two decades, resulting in various
extensions to the original technique and beautiful algorithms for a wide range
of applications. Despite the fact that this approach yields tight approximation
guarantees for some problems, e.g., Max-Cut, for many others, e.g., Max-Sat and
Max-DiCut, the tight approximation ratio is still unknown. One of the main
reasons for this is the fact that very few techniques for rounding
semi-definite relaxations are known.
  In this work, we present a new general and simple method for rounding
semi-definite programs, based on Brownian motion. Our approach is inspired by
recent results in algorithmic discrepancy theory. We develop and present tools
for analyzing our new rounding algorithms, utilizing mathematical machinery
from the theory of Brownian motion, complex analysis, and partial differential
equations. Focusing on constraint satisfaction problems, we apply our method to
several classical problems, including Max-Cut, Max-2SAT, and Max-DiCut, and
derive new algorithms that are competitive with the best known results. We
further show the versatility of our approach by presenting simple and natural
variants of it, and we numerically demonstrate that they exhibit nearly optimal
approximation guarantees for some problems.


AutoSeM: Automatic Task Selection and Mixing in Multi-Task Learning

  Multi-task learning (MTL) has achieved success over a wide range of problems,
where the goal is to improve the performance of a primary task using a set of
relevant auxiliary tasks. However, when the usefulness of the auxiliary tasks
w.r.t. the primary task is not known a priori, the success of MTL models
depends on the correct choice of these auxiliary tasks and also a balanced
mixing ratio of these tasks during alternate training. These two problems could
be resolved via manual intuition or hyper-parameter tuning over all
combinatorial task choices, but this introduces inductive bias or is not
scalable when the number of candidate auxiliary tasks is very large. To address
these issues, we present AutoSeM, a two-stage MTL pipeline, where the first
stage automatically selects the most useful auxiliary tasks via a
Beta-Bernoulli multi-armed bandit with Thompson Sampling, and the second stage
learns the training mixing ratio of these selected auxiliary tasks via a
Gaussian Process based Bayesian optimization framework. We conduct several MTL
experiments on the GLUE language understanding tasks, and show that our AutoSeM
framework can successfully find relevant auxiliary tasks and automatically
learn their mixing ratio, achieving significant performance boosts on several
primary tasks. Finally, we present ablations for each stage of AutoSeM and
analyze the learned auxiliary task choices.


