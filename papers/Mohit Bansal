Photovoltage Bleaching in Bulk Heterojunction Solar Cells through  Occupation of the Charge Transfer State

  We observe a strong peak in the capacitive photocurrent of a MDMO-PPV / PCBMbulk heterojunction solar cell for excitation below the absorbance thresholdenergy. Illumination at the peak energy blocks charge capture at otherwavelengths, and causes the photovoltage to drop dramatically. These resultssuggest that the new peak is due to a charge transfer state, which provides apathway for charge separation and photocurrent generation in the solar cell.

Web-scale Surface and Syntactic n-gram Features for Dependency Parsing

  We develop novel first- and second-order features for dependency parsingbased on the Google Syntactic Ngrams corpus, a collection of subtree counts ofparsed sentences from scanned books. We also extend previous work on surface$n$-gram features from Web1T to the Google Books corpus and from first-order tosecond-order, comparing and analysing performance over newswire and webtreebanks.  Surface and syntactic $n$-grams both produce substantial and complementarygains in parsing accuracy across domains. Our best system combines the twofeature sets, achieving up to 0.8% absolute UAS improvements on newswire and1.4% on web text.

Charagram: Embedding Words and Sentences via Character n-grams

  We present Charagram embeddings, a simple approach for learningcharacter-based compositional models to embed textual sequences. A word orsentence is represented using a character n-gram count vector, followed by asingle nonlinear transformation to yield a low-dimensional embedding. We usethree tasks for evaluation: word similarity, sentence similarity, andpart-of-speech tagging. We demonstrate that Charagram embeddings outperformmore complex architectures based on character-level recurrent and convolutionalneural networks, achieving new state-of-the-art performance on severalsimilarity tasks.

Video Highlight Prediction Using Audience Chat Reactions

  Sports channel video portals offer an exciting domain for research onmultimodal, multilingual analysis. We present methods addressing the problem ofautomatic video highlight prediction based on joint visual features and textualanalysis of the real-world audience discourse with complex slang, in bothEnglish and traditional Chinese. We present a novel dataset based on League ofLegends championships recorded from North American and Taiwanese Twitch.tvchannels (will be released for further research), and demonstrate strongresults on these using multimodal, character-level CNN-RNN model architectures.

Hierarchically-Attentive RNN for Album Summarization and Storytelling

  We address the problem of end-to-end visual storytelling. Given a photoalbum, our model first selects the most representative (summary) photos, andthen composes a natural language story for the album. For this task, we makeuse of the Visual Storytelling dataset and a model composed of threehierarchically-attentive Recurrent Neural Nets (RNNs) to: encode the albumphotos, select representative (summary) photos, and compose the story.Automatic and human evaluations show our model achieves better performance onselection, generation, and retrieval than baselines.

Efficient Algorithms for Discrepancy Minimization in Convex Sets

  A result of Spencer states that every collection of $n$ sets over a universeof size $n$ has a coloring of the ground set with $\{-1,+1\}$ of discrepancy$O(\sqrt{n})$. A geometric generalization of this result was given by Gluskin(see also Giannopoulos) who showed that every symmetric convex body $K\subseteqR^n$ with Gaussian measure at least $e^{-\epsilon n}$, for a small$\epsilon>0$, contains a point $y\in K$ where a constant fraction ofcoordinates of $y$ are in $\{-1,1\}$. This is often called a partial coloringresult. While both these results were inherently non-algorithmic, recentlyBansal (see also Lovett-Meka) gave a polynomial time algorithm for Spencer'ssetting and Rothvo\ss gave a randomized polynomial time algorithm obtaining thesame guarantee as the result of Gluskin and Giannopoulos.  This paper has several related results. First we prove another constructiveversion of the result of Gluskin and Giannopoulos via an optimization of alinear function. This implies a linear programming based algorithm forcombinatorial discrepancy obtaining the same result as Spencer. Our secondresult gives a new approach to obtains partial colorings and shows that everyconvex body $K\subseteq R^n$, possibly non-symmetric, with Gaussian measure atleast $e^{-\epsilon n}$, for a small $\epsilon>0$, contains a point $y\in K$where a constant fraction of coordinates of $y$ are in $\{-1,1\}$. Finally, wegive a simple proof that shows that for any $\delta >0$ there exists a constant$c>0$ such that given a body $K$ with $\gamma_n(K)\geq \delta$, a uniformlyrandom $x$ from $\{-1,1\}^n$ is in $cK$ with constant probability. This givesan algorithmic version of a special case of the result of Banaszczyk.

From Paraphrase Database to Compositional Paraphrase Model and Back

  The Paraphrase Database (PPDB; Ganitkevitch et al., 2013) is an extensivesemantic resource, consisting of a list of phrase pairs with (heuristic)confidence estimates. However, it is still unclear how it can best be used, dueto the heuristic nature of the confidences and its necessarily incompletecoverage. We propose models to leverage the phrase pairs from the PPDB to buildparametric paraphrase models that score paraphrase pairs more accurately thanthe PPDB's internal scores while simultaneously improving its coverage. Theyallow for learning phrase embeddings as well as improved word embeddings.Moreover, we introduce two new, manually annotated datasets to evaluateshort-phrase paraphrasing models. Using our paraphrase model trained usingPPDB, we achieve state-of-the-art results on standard word and bigramsimilarity tasks and beat strong baselines on our new short phrase paraphrasetasks.

Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to  Action Sequences

  We propose a neural sequence-to-sequence model for direction following, atask that is essential to realizing effective autonomous agents. Ouralignment-based encoder-decoder model with long short-term memory recurrentneural networks (LSTM-RNN) translates natural language instructions to actionsequences based upon a representation of the observable world state. Weintroduce a multi-level aligner that empowers our model to focus on sentence"regions" salient to the current world state by using multiple abstractions ofthe input sentence. In contrast to existing methods, our model uses nospecialized linguistic resources (e.g., parsers) or task-specific annotations(e.g., seed lexicons). It is therefore generalizable, yet still achieves thebest results reported to-date on a benchmark single-sentence dataset andcompetitive results for the limited-training multi-sentence setting. We analyzeour model through a series of ablations that elucidate the contributions of theprimary components of our model.

What to talk about and how? Selective Generation using LSTMs with  Coarse-to-Fine Alignment

  We propose an end-to-end, domain-independent neural encoder-aligner-decodermodel for selective generation, i.e., the joint task of content selection andsurface realization. Our model first encodes a full set of over-determineddatabase event records via an LSTM-based recurrent neural network, thenutilizes a novel coarse-to-fine aligner to identify the small subset of salientrecords to talk about, and finally employs a decoder to generate free-formdescriptions of the aligned, selected records. Our model achieves the bestselection and generation results reported to-date (with 59% relativeimprovement in generation) on the benchmark WeatherGov dataset, despite usingno specialized features or linguistic resources. Using an improved k-nearestneighbor beam filter helps further. We also perform a series of ablations andvisualizations to elucidate the contributions of our key model components.Lastly, we evaluate the generalizability of our model on the RoboCup dataset,and get results that are competitive with or better than the state-of-the-art,despite being severely data-starved.

Mapping Unseen Words to Task-Trained Embedding Spaces

  We consider the supervised training setting in which we learn task-specificword embeddings. We assume that we start with initial embeddings learned fromunlabelled data and update them to learn task-specific embeddings for words inthe supervised training data. However, for new words in the test set, we mustuse either their initial embeddings or a single unknown embedding, which oftenleads to errors. We address this by learning a neural network to map frominitial embeddings to the task-specific embedding space, via a multi-lossobjective function. The technique is general, but here we demonstrate its usefor improved dependency parsing (especially for sentences without-of-vocabulary words), as well as for downstream improvements on sentimentanalysis.

Accurate Vision-based Vehicle Localization using Satellite Imagery

  We propose a method for accurately localizing ground vehicles with the aid ofsatellite imagery. Our approach takes a ground image as input, and outputs thelocation from which it was taken on a georeferenced satellite image. We performvisual localization by estimating the co-occurrence probabilities between theground and satellite images based on a ground-satellite feature dictionary. Themethod is able to estimate likelihoods over arbitrary locations without theneed for a dense ground image database. We present a ranking-loss basedalgorithm that learns location-discriminative feature projection matrices thatresult in further improvements in accuracy. We evaluate our method on theMalaga and KITTI public datasets and demonstrate significant improvements overa baseline that performs exhaustive search.

End-to-End Relation Extraction using LSTMs on Sequences and Tree  Structures

  We present a novel end-to-end neural model to extract entities and relationsbetween them. Our recurrent neural network based model captures both wordsequence and dependency tree substructure information by stacking bidirectionaltree-structured LSTM-RNNs on bidirectional sequential LSTM-RNNs. This allowsour model to jointly represent both entities and relations with sharedparameters in a single model. We further encourage detection of entities duringtraining and use of entity information in relation extraction via entitypretraining and scheduled sampling. Our model improves over thestate-of-the-art feature-based model on end-to-end relation extraction,achieving 12.1% and 5.7% relative error reductions in F1-score on ACE2005 andACE2004, respectively. We also show that our LSTM-RNN based model comparesfavorably to the state-of-the-art CNN based model (in F1-score) on nominalrelation classification (SemEval-2010 Task 8). Finally, we present an extensiveablation analysis of several model components.

The Role of Context Types and Dimensionality in Learning Word Embeddings

  We provide the first extensive evaluation of how using different types ofcontext to learn skip-gram word embeddings affects performance on a wide rangeof intrinsic and extrinsic NLP tasks. Our results suggest that while intrinsictasks tend to exhibit a clear preference to particular types of contexts andhigher dimensionality, more careful tuning is required for finding the optimalsettings for most of the extrinsic tasks that we considered. Furthermore, forthese extrinsic tasks, we find that once the benefit from increasing theembedding dimensionality is mostly exhausted, simple concatenation of wordembeddings, learned with different context types, can yield further performancegains. As an additional contribution, we propose a new variant of the skip-grammodel that learns word embeddings from weighted contexts of substitute words.

Question Relevance in VQA: Identifying Non-Visual And False-Premise  Questions

  Visual Question Answering (VQA) is the task of answering natural-languagequestions about images. We introduce the novel problem of determining therelevance of questions to images in VQA. Current VQA models do not reason aboutwhether a question is even related to the given image (e.g. What is the capitalof Argentina?) or if it requires information from external resources to answercorrectly. This can break the continuity of a dialogue in human-machineinteraction. Our approaches for determining relevance are composed of twostages. Given an image and a question, (1) we first determine whether thequestion is visual or not, (2) if visual, we determine whether the question isrelevant to the given image or not. Our approaches, based on LSTM-RNNs, VQAmodel uncertainty, and caption-question similarity, are able to outperformstrong baselines on both relevance tasks. We also present human studies showingthat VQA models augmented with such question relevance reasoning are perceivedas more intelligent, reasonable, and human-like.

Sort Story: Sorting Jumbled Images and Captions into Stories

  Temporal common sense has applications in AI tasks such as QA, multi-documentsummarization, and human-AI communication. We propose the task of sequencing --given a jumbled set of aligned image-caption pairs that belong to a story, thetask is to sort them such that the output sequence forms a coherent story. Wepresent multiple approaches, via unary (position) and pairwise (order)predictions, and their ensemble-based combinations, achieving strong results onthis task. We use both text-based and image-based features, which depictcomplementary improvements. Using qualitative examples, we demonstrate that ourmodels have learnt interesting aspects of temporal common sense.

Who did What: A Large-Scale Person-Centered Cloze Dataset

  We have constructed a new "Who-did-What" dataset of over 200,000fill-in-the-gap (cloze) multiple choice reading comprehension problemsconstructed from the LDC English Gigaword newswire corpus. The WDW dataset hasa variety of novel features. First, in contrast with the CNN and Daily Maildatasets (Hermann et al., 2015) we avoid using article summaries for questionformation. Instead, each problem is formed from two independent articles --- anarticle given as the passage to be read and a separate article on the sameevents used to form the question. Second, we avoid anonymization --- eachchoice is a person named entity. Third, the problems have been filtered toremove a fraction that are easily solved by simple baselines, while remaining84% solvable by humans. We report performance benchmarks of standard systemsand propose the WDW dataset as a challenge task for the community.

Interpreting Neural Networks to Improve Politeness Comprehension

  We present an interpretable neural network approach to predicting andunderstanding politeness in natural language requests. Our models are based onsimple convolutional neural networks directly on raw text, avoiding any manualidentification of complex sentiment or syntactic features, while performingbetter than such feature-based models from previous work. More importantly, weuse the challenging task of politeness prediction as a testbed to next presenta much-needed understanding of what these successful networks are actuallylearning. For this, we present several network visualizations based onactivation clusters, first derivative saliency, and embedding spacetransformations, helping us automatically identify several subtle linguisticsmarkers of politeness theories. Further, this analysis reveals multiple novel,high-scoring politeness strategies which, when added back as new features,reduce the accuracy gap between the original featurized system and the neuralmodel, thus providing a clear quantitative interpretation of the success ofthese neural networks.

Navigational Instruction Generation as Inverse Reinforcement Learning  with Neural Machine Translation

  Modern robotics applications that involve human-robot interaction requirerobots to be able to communicate with humans seamlessly and effectively.Natural language provides a flexible and efficient medium through which robotscan exchange information with their human partners. Significant advancementshave been made in developing robots capable of interpreting free-forminstructions, but less attention has been devoted to endowing robots with theability to generate natural language. We propose a navigational guide modelthat enables robots to generate natural language instructions that allow humansto navigate a priori unknown environments. We first decide which information toshare with the user according to their preferences, using a policy trained fromhuman demonstrations via inverse reinforcement learning. We then "translate"this information into a natural language instruction using a neuralsequence-to-sequence model that learns to generate free-form instructions fromnatural language corpora. We evaluate our method on a benchmark routeinstruction dataset and achieve a BLEU score of 72.18% when compared tohuman-generated reference instructions. We additionally conduct navigationexperiments with human participants that demonstrate that our method generatesinstructions that people follow as accurately and easily as those produced byhumans.

Coherent Dialogue with Attention-based Language Models

  We model coherent conversation continuation via RNN-based dialogue modelsequipped with a dynamic attention mechanism. Our attention-RNN language modeldynamically increases the scope of attention on the history as the conversationcontinues, as opposed to standard attention (or alignment) models with a fixedinput scope in a sequence-to-sequence model. This allows each generated word tobe associated with the most relevant words in its corresponding conversationhistory. We evaluate the model on two popular dialogue datasets, theopen-domain MovieTriples dataset and the closed-domain Ubuntu Troubleshootdataset, and achieve significant improvements over the state-of-the-art andbaselines on several metrics, including complementary diversity-based metrics,human evaluation, and qualitative visualizations. We also show that a vanillaRNN with dynamic attention outperforms more complex memory models (e.g., LSTMand GRU) by allowing for flexible, long-distance memory. We promote furthercoherence via topic modeling-based reranking.

A Joint Speaker-Listener-Reinforcer Model for Referring Expressions

  Referring expressions are natural language constructions used to identifyparticular objects within a scene. In this paper, we propose a unifiedframework for the tasks of referring expression comprehension and generation.Our model is composed of three modules: speaker, listener, and reinforcer. Thespeaker generates referring expressions, the listener comprehends referringexpressions, and the reinforcer introduces a reward function to guide samplingof more discriminative expressions. The listener-speaker modules are trainedjointly in an end-to-end learning framework, allowing the modules to be awareof one another during learning while also benefiting from the discriminativereinforcer's feedback. We demonstrate that this unified framework and trainingachieves state-of-the-art results for both comprehension and generation onthree referring expression datasets. Project and demo page:https://vision.cs.unc.edu/refer

Parsing Speech: A Neural Approach to Integrating Lexical and  Acoustic-Prosodic Information

  In conversational speech, the acoustic signal provides cues that helplisteners disambiguate difficult parses. For automatically parsing spokenutterances, we introduce a model that integrates transcribed text andacoustic-prosodic features using a convolutional neural network over energy andpitch trajectories coupled with an attention-based recurrent neural networkthat accepts text and prosodic features. We find that different types ofacoustic-prosodic features are individually helpful, and together givestatistically significant improvements in parse and disfluency detection F1scores over a strong text-only baseline. For this study with known sentenceboundaries, error analyses show that the main benefit of acoustic-prosodicfeatures is in sentences with disfluencies, attachment decisions are mostimproved, and transcription errors obscure gains from prosody.

Multi-Task Video Captioning with Video and Entailment Generation

  Video captioning, the task of describing the content of a video, has seensome promising improvements in recent years with sequence-to-sequence models,but accurately learning the temporal and logical dynamics involved in the taskstill remains a challenge, especially given the lack of sufficient annotateddata. We improve video captioning by sharing knowledge with two relateddirected-generation tasks: a temporally-directed unsupervised video predictiontask to learn richer context-aware video encoder representations, and alogically-directed language entailment generation task to learn bettervideo-entailed caption decoder representations. For this, we present amany-to-many multi-task learning model that shares parameters across theencoders and decoders of the three tasks. We achieve significant improvementsand the new state-of-the-art on several standard video captioning datasetsusing diverse automatic and human evaluations. We also show mutual multi-taskimprovements on the entailment generation task.

Punny Captions: Witty Wordplay in Image Descriptions

  Wit is a form of rich interaction that is often grounded in a specificsituation (e.g., a comment in response to an event). In this work, we attemptto build computational models that can produce witty descriptions for a givenimage. Inspired by a cognitive account of humor appreciation, we employlinguistic wordplay, specifically puns, in image descriptions. We develop twoapproaches which involve retrieving witty descriptions for a given image from alarge corpus of sentences, or generating them via an encoder-decoder neuralnetwork architecture. We compare our approach against meaningful baselineapproaches via human studies and show substantial improvements. We find thatwhen a human is subject to similar constraints as the model regarding wordusage and style, people vote the image descriptions generated by our model tobe slightly wittier than human-written witty descriptions. Unsurprisingly,humans are almost always wittier than the model when they are free to choosethe vocabulary, style, etc.

Efficient Generation of Motion Plans from Attribute-Based Natural  Language Instructions Using Dynamic Constraint Mapping

  We present an algorithm for combining natural language processing (NLP) andfast robot motion planning to automatically generate robot movements. Ourformulation uses a novel concept called Dynamic Constraint Mapping to transformcomplex, attribute-based natural language instructions into appropriate costfunctions and parametric constraints for optimization-based motion planning. Wegenerate a factor graph from natural language instructions called the DynamicGrounding Graph (DGG), which takes latent parameters into account. Thecoefficients of this factor graph are learned based on conditional randomfields (CRFs) and are used to dynamically generate the constraints for motionplanning. We map the cost function directly to the motion parameters of theplanner and compute smooth trajectories in dynamic scenes. We highlight theperformance of our approach in a simulated environment and via a humaninteracting with a 7-DOF Fetch robot using intricate language commandsincluding negation, orientation specification, and distance constraints.

Source-Target Inference Models for Spatial Instruction Understanding

  Models that can execute natural language instructions for situated robotictasks such as assembly and navigation have several useful applications inhomes, offices, and remote scenarios. We study the semantics ofspatially-referred configuration and arrangement instructions, based on thechallenging Bisk-2016 blank-labeled block dataset. This task involves finding asource block and moving it to the target position (mentioned via a referenceblock and offset), where the blocks have no names or colors and are justreferred to via spatial location features. We present novel models for thesubtasks of source block classification and target position regression, basedon joint-loss language and spatial-world representation learning, as well asCNN-based and dual attention models to compute the alignment between the worldblocks and the instruction phrases. For target position prediction, we comparetwo inference approaches: annealed sampling via policy gradient versusexpectation inference via supervised regression. Our models achieve the newstate-of-the-art on this task, with an improvement of 47% on source blockaccuracy and 22% on target position distance.

Reinforced Video Captioning with Entailment Rewards

  Sequence-to-sequence models have shown promising improvements on the temporaltask of video captioning, but they optimize word-level cross-entropy lossduring training. First, using policy gradient and mixed-loss methods forreinforcement learning, we directly optimize sentence-level task-based metrics(as rewards), achieving significant improvements over the baseline, based onboth automatic metrics and human evaluation on multiple datasets. Next, wepropose a novel entailment-enhanced reward (CIDEnt) that correctsphrase-matching based metrics (such as CIDEr) to only allow forlogically-implied partial matches and avoid contradictions, achieving furthersignificant improvements over the CIDEr-reward model. Overall, ourCIDEnt-reward model achieves the new state-of-the-art on the MSR-VTT dataset.

Shortcut-Stacked Sentence Encoders for Multi-Domain Inference

  We present a simple sequential sentence encoder for multi-domain naturallanguage inference. Our encoder is based on stacked bidirectional LSTM-RNNswith shortcut connections and fine-tuning of word embeddings. The overallsupervised model uses the above encoder to encode two input sentences into twovectors, and then uses a classifier over the vector combination to label therelationship between these two sentences as that of entailment, contradiction,or neural. Our Shortcut-Stacked sentence encoders achieve strong improvementsover existing encoders on matched and mismatched multi-domain natural languageinference (top non-ensemble single-model result in the EMNLP RepEval 2017Shared Task (Nangia et al., 2017)). Moreover, they achieve the newstate-of-the-art encoding result on the original SNLI dataset (Bowman et al.,2015).

MAttNet: Modular Attention Network for Referring Expression  Comprehension

  In this paper, we address referring expression comprehension: localizing animage region described by a natural language expression. While most recent worktreats expressions as a single unit, we propose to decompose them into threemodular components related to subject appearance, location, and relationship toother objects. This allows us to flexibly adapt to expressions containingdifferent types of information in an end-to-end framework. In our model, whichwe call the Modular Attention Network (MAttNet), two types of attention areutilized: language-based attention that learns the module weights as well asthe word/phrase attention that each module should focus on; and visualattention that allows the subject and relationship modules to focus on relevantimage components. Module weights combine scores from all three modulesdynamically to output an overall score. Experiments show that MAttNetoutperforms previous state-of-art methods by a large margin on bothbounding-box-level and pixel-level comprehension tasks. Demo and code areprovided.

Detecting Linguistic Characteristics of Alzheimer's Dementia by  Interpreting Neural Models

  Alzheimer's disease (AD) is an irreversible and progressive brain diseasethat can be stopped or slowed down with medical treatment. Language changesserve as a sign that a patient's cognitive functions have been impacted,potentially leading to early diagnosis. In this work, we use NLP techniques toclassify and analyze the linguistic characteristics of AD patients using theDementiaBank dataset. We apply three neural models based on CNNs, LSTM-RNNs,and their combination, to distinguish between language samples from AD andcontrol patients. We achieve a new independent benchmark accuracy for the ADclassification task. More importantly, we next interpret what these neuralmodels have learned about the linguistic characteristics of AD patients, viaanalysis based on activation clustering and first-derivative saliencytechniques. We then perform novel automatic pattern discovery inside activationclusters, and consolidate AD patients' distinctive grammar patterns.Additionally, we show that first derivative saliency can not only rediscoverprevious language patterns of AD patients, but also shed light on thelimitations of neural models. Lastly, we also include analysis ofgender-separated AD data.

Multi-Reward Reinforced Summarization with Saliency and Entailment

  Abstractive text summarization is the task of compressing and rewriting along document into a short summary while maintaining saliency, directed logicalentailment, and non-redundancy. In this work, we address these three importantaspects of a good summary via a reinforcement learning approach with two novelreward functions: ROUGESal and Entail, on top of a coverage-based baseline. TheROUGESal reward modifies the ROUGE metric by up-weighting the salientphrases/words detected via a keyphrase classifier. The Entail reward gives high(length-normalized) scores to logically-entailed summaries using an entailmentclassifier. Further, we show superior performance improvement when theserewards are combined with traditional metric (ROUGE) based rewards, via ournovel and effective multi-reward approach of optimizing multiple rewardssimultaneously in alternate mini-batches. Our method achieves the newstate-of-the-art results (including human evaluation) on the CNN/Daily Maildataset as well as strong improvements in a test-only transfer setup onDUC-2002.

Robust Machine Comprehension Models via Adversarial Training

  It is shown that many published models for the Stanford Question AnsweringDataset (Rajpurkar et al., 2016) lack robustness, suffering an over 50%decrease in F1 score during adversarial evaluation based on the AddSent (Jiaand Liang, 2017) algorithm. It has also been shown that retraining models ondata generated by AddSent has limited effect on their robustness. We propose anovel alternative adversary-generation algorithm, AddSentDiverse, thatsignificantly increases the variance within the adversarial training data byproviding effective examples that punish the model for making certainsuperficial assumptions. Further, in order to improve robustness to AddSent'ssemantic perturbations (e.g., antonyms), we jointly improve the model'ssemantic-relationship learning capabilities in addition to ourAddSentDiverse-based adversarial training data augmentation. With theseadditions, we show that we can make a state-of-the-art model significantly morerobust, achieving a 36.5% increase in F1 score under many different types ofadversarial evaluation while maintaining performance on the regular SQuAD task.

Object Ordering with Bidirectional Matchings for Visual Reasoning

  Visual reasoning with compositional natural language instructions, e.g.,based on the newly-released Cornell Natural Language Visual Reasoning (NLVR)dataset, is a challenging task, where the model needs to have the ability tocreate an accurate mapping between the diverse phrases and the several objectsplaced in complex arrangements in the image. Further, this mapping needs to beprocessed to answer the question in the statement given the ordering andrelationship of the objects across three similar images. In this paper, wepropose a novel end-to-end neural model for the NLVR task, where we first usejoint bidirectional attention to build a two-way conditioning between thevisual information and the language phrases. Next, we use an RL-based pointernetwork to sort and process the varying number of unordered objects (so as tomatch the order of the statement phrases) in each of the three images and thenpool over the three decisions. Our model achieves strong improvements (of 4-6%absolute) over the state-of-the-art on both the structured representation andraw image versions of the dataset.

Polite Dialogue Generation Without Parallel Data

  Stylistic dialogue response generation, with valuable applications inpersonality-based conversational agents, is a challenging task because theresponse needs to be fluent, contextually-relevant, as well asparalinguistically accurate. Moreover, parallel datasets forregular-to-stylistic pairs are usually unavailable. We present threeweakly-supervised models that can generate diverse polite (or rude) dialogueresponses without parallel data. Our late fusion model (Fusion) merges thedecoder of an encoder-attention-decoder dialogue model with a language modeltrained on stand-alone polite utterances. Our label-fine-tuning (LFT) modelprepends to each source sequence a politeness-score scaled label (predicted byour state-of-the-art politeness classifier) during training, and at test timeis able to generate polite, neutral, and rude responses by simply scaling thelabel embedding by the corresponding score. Our reinforcement learning model(Polite-RL) encourages politeness generation by assigning rewards proportionalto the politeness classifier score of the sampled response. We also present tworetrieval-based polite dialogue model baselines. Human evaluation validatesthat while the Fusion and the retrieval-based models achieve politeness withpoorer context-relevance, the LFT and Polite-RL models can producesignificantly more polite responses without sacrificing dialogue quality.

Soft Layer-Specific Multi-Task Summarization with Entailment and  Question Generation

  An accurate abstractive summary of a document should contain all its salientinformation and should be logically entailed by the input document. We improvethese important aspects of abstractive summarization via multi-task learningwith the auxiliary tasks of question generation and entailment generation,where the former teaches the summarization model how to look for salientquestioning-worthy details, and the latter teaches the model how to rewrite asummary which is a directed-logical subset of the input document. We alsopropose novel multi-task architectures with high-level (semantic)layer-specific sharing across multiple encoder and decoder layers of the threetasks, as well as soft-sharing mechanisms (and show performance ablations andanalysis examples of each contribution). Overall, we achieve statisticallysignificant improvements over the state-of-the-art on both the CNN/DailyMailand Gigaword datasets, as well as on the DUC-2002 transfer setup. We alsopresent several quantitative and qualitative analysis studies of our model'slearned saliency and entailment skills.

Fast Abstractive Summarization with Reinforce-Selected Sentence  Rewriting

  Inspired by how humans summarize long documents, we propose an accurate andfast summarization model that first selects salient sentences and then rewritesthem abstractively (i.e., compresses and paraphrases) to generate a conciseoverall summary. We use a novel sentence-level policy gradient method to bridgethe non-differentiable computation between these two neural networks in ahierarchical way, while maintaining language fluency. Empirically, we achievethe new state-of-the-art on all metrics (including human evaluation) on theCNN/Daily Mail dataset, as well as significantly higher abstractiveness scores.Moreover, by first operating at the sentence-level and then the word-level, weenable parallel decoding of our neural generative model that results insubstantially faster (10-20x) inference speed as well as 4x faster trainingconvergence than previous long-paragraph encoder-decoder models. We alsodemonstrate the generalization of our model on the test-only DUC-2002 dataset,where we achieve higher scores than a state-of-the-art model.

Dynamic Multi-Level Multi-Task Learning for Sentence Simplification

  Sentence simplification aims to improve readability and understandability,based on several operations such as splitting, deletion, and paraphrasing.However, a valid simplified sentence should also be logically entailed by itsinput sentence. In this work, we first present a strong pointer-copy mechanismbased sequence-to-sequence sentence simplification model, and then improve itsentailment and paraphrasing capabilities via multi-task learning with relatedauxiliary tasks of entailment and paraphrase generation. Moreover, we propose anovel 'multi-level' layered soft sharing approach where each auxiliary taskshares different (higher versus lower) level layers of the sentencesimplification model, depending on the task's semantic versus lexico-syntacticnature. We also introduce a novel multi-armed bandit based training approachthat dynamically learns how to effectively switch across tasks duringmulti-task learning. Experiments on multiple popular datasets demonstrate thatour model outperforms competitive simplification systems in SARI and FKGLautomatic metrics, and human evaluation. Further, we present several ablationanalyses on alternative layer sharing methods, soft versus hard sharing,dynamic multi-armed bandit sampling approaches, and our model's learnedentailment and paraphrasing skills.

TVQA: Localized, Compositional Video Question Answering

  Recent years have witnessed an increasing interest in image-basedquestion-answering (QA) tasks. However, due to data limitations, there has beenmuch less work on video-based QA. In this paper, we present TVQA, a large-scalevideo QA dataset based on 6 popular TV shows. TVQA consists of 152,545 QA pairsfrom 21,793 clips, spanning over 460 hours of video. Questions are designed tobe compositional in nature, requiring systems to jointly localize relevantmoments within a clip, comprehend subtitle-based dialogue, and recognizerelevant visual concepts. We provide analyses of this new dataset as well asseveral baselines and a multi-stream end-to-end trainable neural networkframework for the TVQA task. The dataset is publicly available athttp://tvqa.cs.unc.edu.

Adversarial Over-Sensitivity and Over-Stability Strategies for Dialogue  Models

  We present two categories of model-agnostic adversarial strategies thatreveal the weaknesses of several generative, task-oriented dialogue models:Should-Not-Change strategies that evaluate over-sensitivity to small andsemantics-preserving edits, as well as Should-Change strategies that test if amodel is over-stable against subtle yet semantics-changing modifications. Wenext perform adversarial training with each strategy, employing a max-marginapproach for negative generative examples. This not only makes the targetdialogue model more robust to the adversarial inputs, but also helps it performsignificantly better on the original inputs. Moreover, training on allstrategies combined achieves further improvements, achieving a newstate-of-the-art performance on the original task (also verified via humanevaluation). In addition to adversarial training, we also address therobustness task at the model-level, by feeding it subword units as both inputsand outputs, and show that the resulting model is equally competitive, requiresonly 1/4 of the original vocabulary size, and is robust to one of theadversarial strategies (to which the original model is vulnerable) even withoutadversarial training.

Game-Based Video-Context Dialogue

  Current dialogue systems focus more on textual and speech context knowledgeand are usually based on two speakers. Some recent work has investigated staticimage-based dialogue. However, several real-world human interactions alsoinvolve dynamic visual context (similar to videos) as well as dialogueexchanges among multiple speakers. To move closer towards such multimodalconversational skills and visually-situated applications, we introduce a newvideo-context, many-speaker dialogue dataset based on live-broadcast soccergame videos and chats from Twitch.tv. This challenging testbed allows us todevelop visually-grounded dialogue models that should generate relevanttemporal and spatial event language from the live video, while also beingrelevant to the chat history. For strong baselines, we also present severaldiscriminative and generative models, e.g., based on tridirectional attentionflow (TriDAF). We evaluate these models via retrieval ranking-recall, automaticphrase-matching metrics, as well as human evaluation studies. We also presentdataset analyses, model ablations, and visualizations to understand thecontribution of different modalities and model components.

Closed-Book Training to Improve Summarization Encoder Memory

  A good neural sequence-to-sequence summarization model should have a strongencoder that can distill and memorize the important information from long inputtexts so that the decoder can generate salient summaries based on the encoder'smemory. In this paper, we aim to improve the memorization capabilities of theencoder of a pointer-generator model by adding an additional 'closed-book'decoder without attention and pointer mechanisms. Such a decoder forces theencoder to be more selective in the information encoded in its memory statebecause the decoder can't rely on the extra information provided by theattention and possibly copy modules, and hence improves the entire model. Onthe CNN/Daily Mail dataset, our 2-decoder model outperforms the baselinesignificantly in terms of ROUGE and METEOR metrics, for both cross-entropy andreinforced setups (and on human evaluation). Moreover, our model also achieveshigher scores in a test-only DUC-2002 generalizability setup. We furtherpresent a memory ability test, two saliency metrics, as well as severalsanity-check ablations (based on fixed-encoder, gradient-flow cut, and modelcapacity) to prove that the encoder of our 2-decoder model does in fact learnstronger memory representations than the baseline encoder.

SafeCity: Understanding Diverse Forms of Sexual Harassment Personal  Stories

  With the recent rise of #MeToo, an increasing number of personal storiesabout sexual harassment and sexual abuse have been shared online. In order topush forward the fight against such harassment and abuse, we present the taskof automatically categorizing and analyzing various forms of sexual harassment,based on stories shared on the online forum SafeCity. For the labels ofgroping, ogling, and commenting, our single-label CNN-RNN model achieves anaccuracy of 86.5%, and our multi-label model achieves a Hamming score of 82.5%.Furthermore, we present analysis using LIME, first-derivative saliencyheatmaps, activation clustering, and embedding visualization to interpretneural model predictions and demonstrate how this extracts features that canhelp automatically fill out incident reports, identify unsafe areas, avoidunsafe practices, and 'pin the creeps'.

Analyzing Compositionality-Sensitivity of NLI Models

  Success in natural language inference (NLI) should require a model tounderstand both lexical and compositional semantics. However, throughadversarial evaluation, we find that several state-of-the-art models withdiverse architectures are over-relying on the former and fail to use thelatter. Further, this compositionality unawareness is not reflected viastandard evaluation on current datasets. We show that removing RNNs in existingmodels or shuffling input words during training does not induce largeperformance loss despite the explicit removal of compositional information.Therefore, we propose a compositionality-sensitivity testing setup thatanalyzes models on natural examples from existing datasets that cannot besolved via lexical features alone (i.e., on which a bag-of-words model gives ahigh probability to one wrong label), hence revealing the models' actualcompositionality awareness. We show that this setup not only highlights thelimited compositional ability of current NLI models, but also differentiatesmodel performance based on design, e.g., separating shallow bag-of-words modelsfrom deeper, linguistically-grounded tree-based models. Our evaluation setup isan important analysis tool: complementing currently existing adversarial andlinguistically driven diagnostic evaluations, and exposing opportunities forfuture work on evaluating models' compositional understanding.

Learning Articulated Motion Models from Visual and Lingual Signals

  In order for robots to operate effectively in homes and workplaces, they mustbe able to manipulate the articulated objects common within environments builtfor and by humans. Previous work learns kinematic models that prescribe thismanipulation from visual demonstrations. Lingual signals, such as naturallanguage descriptions and instructions, offer a complementary means ofconveying knowledge of such manipulation models and are suitable to a widerange of interactions (e.g., remote manipulation). In this paper, we present amultimodal learning framework that incorporates both visual and lingualinformation to estimate the structure and parameters that define kinematicmodels of articulated objects. The visual signal takes the form of an RGB-Dimage stream that opportunistically captures object motion in an unpreparedscene. Accompanying natural language descriptions of the motion constitute thelingual signal. We present a probabilistic language model that uses wordembeddings to associate lingual verbs with their corresponding kinematicstructures. By exploiting the complementary nature of the visual and lingualinput, our method infers correct kinematic structures for various multiple-partobjects on which the previous state-of-the-art, visual-only system fails. Weevaluate our multimodal learning framework on a dataset comprised of a varietyof household objects, and demonstrate a 36% improvement in model accuracy overthe vision-only baseline.

Towards Universal Paraphrastic Sentence Embeddings

  We consider the problem of learning general-purpose, paraphrastic sentenceembeddings based on supervision from the Paraphrase Database (Ganitkevitch etal., 2013). We compare six compositional architectures, evaluating them onannotated textual similarity datasets drawn both from the same distribution asthe training data and from a wide range of other domains. We find that the mostcomplex architectures, such as long short-term memory (LSTM) recurrent neuralnetworks, perform best on the in-domain data. However, in out-of-domainscenarios, simple architectures such as word averaging vastly outperform LSTMs.Our simplest averaging model is even competitive with systems tuned for theparticular tasks while also being extremely efficient and easy to use.  In order to better understand how these architectures compare, we conductfurther experiments on three supervised NLP tasks: sentence similarity,entailment, and sentiment classification. We again find that the word averagingmodels perform well for sentence similarity and entailment, outperformingLSTMs. However, on sentiment classification, we find that the LSTM performsvery strongly-even recording new state-of-the-art performance on the StanfordSentiment Treebank.  We then demonstrate how to combine our pretrained sentence embeddings withthese supervised tasks, using them both as a prior and as a black box featureextractor. This leads to performance rivaling the state of the art on the SICKsimilarity and entailment tasks. We release all of our resources to theresearch community with the hope that they can serve as the new baseline forfurther work on universal sentence embeddings.

We Are Humor Beings: Understanding and Predicting Visual Humor

  Humor is an integral part of human lives. Despite being tremendouslyimpactful, it is perhaps surprising that we do not have a detailedunderstanding of humor yet. As interactions between humans and AI systemsincrease, it is imperative that these systems are taught to understandsubtleties of human expressions such as humor. In this work, we are interestedin the question - what content in a scene causes it to be funny? As a firststep towards understanding visual humor, we analyze the humor manifested inabstract scenes and design computational models for them. We collect twodatasets of abstract scenes that facilitate the study of humor at both thescene-level and the object-level. We analyze the funny scenes and explore thedifferent types of humor depicted in them via human studies. We model two tasksthat we believe demonstrate an understanding of some aspects of visual humor.The tasks involve predicting the funniness of a scene and altering thefunniness of a scene. We show that our models perform well quantitatively, andqualitatively through human studies. Our datasets are publicly available.

Contextual RNN-GANs for Abstract Reasoning Diagram Generation

  Understanding, predicting, and generating object motions and transformationsis a core problem in artificial intelligence. Modeling sequences of evolvingimages may provide better representations and models of motion and mayultimately be used for forecasting, simulation, or video generation.Diagrammatic Abstract Reasoning is an avenue in which diagrams evolve incomplex patterns and one needs to infer the underlying pattern sequence andgenerate the next image in the sequence. For this, we develop a novelContextual Generative Adversarial Network based on Recurrent Neural Networks(Context-RNN-GANs), where both the generator and the discriminator modules arebased on contextual history (modeled as RNNs) and the adversarial discriminatorguides the generator to produce realistic images for the particular time stepin the image sequence. We evaluate the Context-RNN-GAN model (and its variants)on a novel dataset of Diagrammatic Abstract Reasoning, where it performscompetitively with 10th-grade human performance but there is still scope forinteresting improvements as compared to college-grade human performance. Wealso evaluate our model on a standard video next-frame prediction task,achieving improved performance over comparable state-of-the-art.

Commonsense for Generative Multi-Hop Question Answering Tasks

  Reading comprehension QA tasks have seen a recent surge in popularity, yetmost works have focused on fact-finding extractive QA. We instead focus on amore challenging multi-hop generative task (NarrativeQA), which requires themodel to reason, gather, and synthesize disjoint pieces of information withinthe context to generate an answer. This type of multi-step reasoning also oftenrequires understanding implicit relations, which humans resolve via external,background commonsense knowledge. We first present a strong generative baselinethat uses a multi-attention mechanism to perform multiple hops of reasoning anda pointer-generator decoder to synthesize the answer. This model performssubstantially better than previous generative models, and is competitive withcurrent state-of-the-art span prediction models. We next introduce a novelsystem for selecting grounded multi-hop relational commonsense information fromConceptNet via a pointwise mutual information and term-frequency based scoringfunction. Finally, we effectively use this extracted commonsense information tofill in gaps of reasoning between context hops, using a selectively-gatedattention mechanism. This boosts the model's performance significantly (alsoverified via human evaluation), establishing a new state-of-the-art for thetask. We also show promising initial results of the generalizability of ourbackground knowledge enhancements by demonstrating some improvement onQAngaroo-WikiHop, another multi-hop reasoning dataset.

Combining Fact Extraction and Verification with Neural Semantic Matching  Networks

  The increasing concern with misinformation has stimulated research efforts onautomatic fact checking. The recently-released FEVER dataset introduced abenchmark fact-verification task in which a system is asked to verify a claimusing evidential sentences from Wikipedia documents. In this paper, we presenta connected system consisting of three homogeneous neural semantic matchingmodels that conduct document retrieval, sentence selection, and claimverification jointly for fact extraction and verification. For evidenceretrieval (document retrieval and sentence selection), unlike traditionalvector space IR models in which queries and sources are matched in somepre-designed term vector space, we develop neural models to perform deepsemantic matching from raw textual input, assuming no intermediate termrepresentation and no access to structured external knowledge bases. We alsoshow that Pageview frequency can also help improve the performance of evidenceretrieval results, that later can be matched by using our neural semanticmatching network. For claim verification, unlike previous approaches thatsimply feed upstream retrieved evidence and the claim to a natural languageinference (NLI) model, we further enhance the NLI model by providing it withinternal semantic relatedness scores (hence integrating it with the evidenceretrieval modules) and ontological WordNet features. Experiments on the FEVERdataset indicate that (1) our neural semantic matching method outperformspopular TF-IDF and encoder models, by significant margins on all evidenceretrieval metrics, (2) the additional relatedness score and WordNet featuresimprove the NLI model via better semantic awareness, and (3) by formalizing allthree subtasks as a similar semantic matching problem and improving on allthree stages, the complete model is able to achieve the state-of-the-artresults on the FEVER test set.

Sticky Brownian Rounding and its Applications to Constraint Satisfaction  Problems

  Semidefinite programming is a powerful tool in the design and analysis ofapproximation algorithms for combinatorial optimization problems. Inparticular, the random hyperplane rounding method of Goemans and Williamson,has been extensively studied for more than two decades, resulting in variousextensions to the original technique and beautiful algorithms for a wide rangeof applications. Despite the fact that this approach yields tight approximationguarantees for some problems, e.g., Max-Cut, for many others, e.g., Max-Sat andMax-DiCut, the tight approximation ratio is still unknown. One of the mainreasons for this is the fact that very few techniques for roundingsemi-definite relaxations are known.  In this work, we present a new general and simple method for roundingsemi-definite programs, based on Brownian motion. Our approach is inspired byrecent results in algorithmic discrepancy theory. We develop and present toolsfor analyzing our new rounding algorithms, utilizing mathematical machineryfrom the theory of Brownian motion, complex analysis, and partial differentialequations. Focusing on constraint satisfaction problems, we apply our method toseveral classical problems, including Max-Cut, Max-2SAT, and Max-DiCut, andderive new algorithms that are competitive with the best known results. Wefurther show the versatility of our approach by presenting simple and naturalvariants of it, and we numerically demonstrate that they exhibit nearly optimalapproximation guarantees for some problems.

AutoSeM: Automatic Task Selection and Mixing in Multi-Task Learning

  Multi-task learning (MTL) has achieved success over a wide range of problems,where the goal is to improve the performance of a primary task using a set ofrelevant auxiliary tasks. However, when the usefulness of the auxiliary tasksw.r.t. the primary task is not known a priori, the success of MTL modelsdepends on the correct choice of these auxiliary tasks and also a balancedmixing ratio of these tasks during alternate training. These two problems couldbe resolved via manual intuition or hyper-parameter tuning over allcombinatorial task choices, but this introduces inductive bias or is notscalable when the number of candidate auxiliary tasks is very large. To addressthese issues, we present AutoSeM, a two-stage MTL pipeline, where the firststage automatically selects the most useful auxiliary tasks via aBeta-Bernoulli multi-armed bandit with Thompson Sampling, and the second stagelearns the training mixing ratio of these selected auxiliary tasks via aGaussian Process based Bayesian optimization framework. We conduct several MTLexperiments on the GLUE language understanding tasks, and show that our AutoSeMframework can successfully find relevant auxiliary tasks and automaticallylearn their mixing ratio, achieving significant performance boosts on severalprimary tasks. Finally, we present ablations for each stage of AutoSeM andanalyze the learned auxiliary task choices.

