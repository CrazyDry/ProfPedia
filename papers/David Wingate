The Infinite Latent Events Model

  We present the Infinite Latent Events Model, a nonparametric hierarchical
Bayesian distribution over infinite dimensional Dynamic Bayesian Networks with
binary state representations and noisy-OR-like transitions. The distribution
can be used to learn structure in discrete timeseries data by simultaneously
inferring a set of latent events, which events fired at each timestep, and how
those events are causally linked. We illustrate the model on a sound
factorization task, a network topology identification task, and a video game
task.


Round Table Discussion at the Final Session of FPCP 2008: The Future of
  Flavor Physics and CP

  The final session of FPCP 2008 consisted of a round-table discussion among
panelists and audience. The panelists included Jeffrey Appel(moderator), Martin
Beneke, George W.S. Hou, David Kirkby, Dmitri Tsybychev, Matt Wingate, and Taku
Yamanaka. What follows is an edited transcript of the session.


Automated Variational Inference in Probabilistic Programming

  We present a new algorithm for approximate inference in probabilistic
programs, based on a stochastic gradient for variational programs. This method
is efficient without restrictions on the probabilistic program; it is
particularly practical for distributions which are not analytically tractable,
including highly structured distributions that arise in probabilistic programs.
We show how to automatically derive mean-field probabilistic programs and
optimize them, and demonstrate that our perspective improves inference
efficiency over other algorithms.


Graph Neural Processes: Towards Bayesian Graph Neural Networks

  We introduce Graph Neural Processes (GNP), inspired by the recent work in
conditional and latent neural processes. A Graph Neural Process is defined as a
Conditional Neural Process that operates on arbitrary graph data. It takes
features of sparsely observed context points as input, and outputs a
distribution over target points. We demonstrate graph neural processes in edge
imputation and discuss benefits and drawbacks of the method for other
application areas. One major benefit of GNPs is the ability to quantify
uncertainty in deep learning on graph structures. An additional benefit of this
method is the ability to extend graph neural networks to inputs of dynamic
sized graphs.


Video Extrapolation with an Invertible Linear Embedding

  We predict future video frames from complex dynamic scenes, using an
invertible neural network as the encoder of a nonlinear dynamic system with
latent linear state evolution. Our invertible linear embedding (ILE)
demonstrates successful learning, prediction and latent state inference. In
contrast to other approaches, ILE does not use any explicit reconstruction loss
or simplistic pixel-space assumptions. Instead, it leverages invertibility to
optimize the likelihood of image sequences exactly, albeit indirectly.
Comparison with a state-of-the-art method demonstrates the viability of our
approach.


Bottom hadron mass splittings in the static limit from 2+1 flavour
  lattice QCD

  Dynamical 2+1 flavour lattice QCD is used to calculate the splittings between
the masses of mesons and baryons containing a single static heavy quark and
domain-wall light and strange quarks. Our calculations are based on the
dynamical domain-wall gauge field configurations generated by the RBC and UKQCD
collaborations at a spatial volume of (2.7 fm)^3 and a range of quark masses
with a lightest value corresponding to a (partially-quenched) pion mass of 275
MeV. When extrapolated to the physical values of the light quark masses, the
results of our calculations are generally in good agreement with experimental
determinations in the bottom sector. However, the static limit splittings
between the Omega_b^- baryon and other bottom hadrons tend to slightly
underestimate those obtained using the recent D-zero measurement of the
Omega_b^-.


A Bayesian Sampling Approach to Exploration in Reinforcement Learning

  We present a modular approach to reinforcement learning that uses a Bayesian
representation of the uncertainty over models. The approach, BOSS (Best of
Sampled Set), drives exploration by sampling multiple models from the posterior
and selecting actions optimistically. It extends previous work by providing a
rule for deciding when to resample and how to combine the models. We show that
our algorithm achieves nearoptimal reward with high probability with a sample
complexity that is low relative to the speed at which the posterior
distribution converges during learning. We demonstrate that BOSS performs quite
favorably compared to state-of-the-art reinforcement-learning approaches and
illustrate its flexibility by pairing it with a non-parametric model that
generalizes across states.


Form factors for Lambda_b -> Lambda transitions from lattice QCD

  The rare baryonic decays $\Lambda_b \to \Lambda \mu^+ \mu^-$ and $\Lambda_b
\to \Lambda \gamma$ can complement rare $B$ meson decays in constraining models
of new physics. In this work, we calculate the relevant $\Lambda_b \to \Lambda$
transition form factors at leading order in heavy-quark effective theory using
lattice QCD. Our analysis is based on RBC/UKQCD gauge field ensembles with 2+1
flavors of domain-wall fermions, and with lattice spacings of $a\approx 0.11$
fm and $a\approx 0.08$ fm. We compute appropriate ratios of three-point and
two-point correlation functions for a wide range of source-sink separations,
and extrapolate to infinite separation in order to eliminate excited-state
contamination. We then extrapolate the form factors to the continuum limit and
to the physical values of the light-quark masses.


Lambda_b -> Lambda l+ l- form factors and differential branching
  fraction from lattice QCD

  We present the first lattice QCD determination of the $\Lambda_b \to \Lambda$
transition form factors that govern the rare baryonic decays $\Lambda_b \to
\Lambda l^+ l^-$ at leading order in heavy-quark effective theory. Our
calculations are performed with 2+1 flavors of domain-wall fermions, at two
lattice spacings and with pion masses down to 227 MeV. Three-point functions
with a wide range of source-sink separations are used to extract the
ground-state contributions. The form factors are extrapolated to the physical
values of the light-quark masses and to the continuum limit. We use our results
to calculate the differential branching fractions for $\Lambda_b \to \Lambda
l^+ l^-$ with $l=e,\mu,\tau$ within the standard model. We find agreement with
a recent CDF measurement of the $\Lambda_b \to \Lambda \mu^+ \mu^-$
differential branching fraction.


Bottom hadrons from lattice QCD with domain wall and NRQCD fermions

  Dynamical 2+1 flavor lattice QCD is used to calculate the masses of bottom
hadrons, including B mesons, singly and doubly bottom baryons, and for the
first time also the triply-bottom baryon Omega_bbb. The domain wall action is
used for the up-, down-, and strange quarks (both valence and sea), while the
bottom quark is implemented with non-relativistic QCD. A calculation of the
bottomonium spectrum is also presented.


Predictive Linear-Gaussian Models of Stochastic Dynamical Systems

  Models of dynamical systems based on predictive state representations (PSRs)
are defined strictly in terms of observable quantities, in contrast with
traditional models (such as Hidden Markov Models) that use latent variables or
statespace representations. In addition, PSRs have an effectively infinite
memory, allowing them to model some systems that finite memory-based models
cannot. Thus far, PSR models have primarily been developed for domains with
discrete observations. Here, we develop the Predictive Linear-Gaussian (PLG)
model, a class of PSR models for domains with continuous observations. We show
that PLG models subsume Linear Dynamical System models (also called Kalman
filter models or state-space models) while using fewer parameters. We also
introduce an algorithm to estimate PLG parameters from data, and contrast it
with standard Expectation Maximization (EM) algorithms used to estimate Kalman
filter parameters. We show that our algorithm is a consistent estimation
procedure and present preliminary empirical results suggesting that our
algorithm outperforms EM, particularly as the model dimension increases.


Probabilistic programs for inferring the goals of autonomous agents

  Intelligent systems sometimes need to infer the probable goals of people,
cars, and robots, based on partial observations of their motion. This paper
introduces a class of probabilistic programs for formulating and solving these
problems. The formulation uses randomized path planning algorithms as the basis
for probabilistic models of the process by which autonomous agents plan to
achieve their goals. Because these path planning algorithms do not have
tractable likelihood functions, new inference algorithms are needed. This paper
proposes two Monte Carlo techniques for these "likelihood-free" models, one of
which can use likelihood estimates from neural networks to accelerate
inference. The paper demonstrates efficacy on three simple examples, each using
under 50 lines of probabilistic code.


What can you do with a rock? Affordance extraction via word embeddings

  Autonomous agents must often detect affordances: the set of behaviors enabled
by a situation. Affordance detection is particularly helpful in domains with
large action spaces, allowing the agent to prune its search space by avoiding
futile behaviors. This paper presents a method for affordance extraction via
word embeddings trained on a Wikipedia corpus. The resulting word vectors are
treated as a common knowledge database which can be queried using linear
algebra. We apply this method to a reinforcement learning agent in a text-only
environment and show that affordance-based action selection improves
performance most of the time. Our method increases the computational complexity
of each learning step but significantly reduces the total number of steps
needed. In addition, the agent's action selections begin to resemble those a
human would choose.


Embedding Grammars

  Classic grammars and regular expressions can be used for a variety of
purposes, including parsing, intent detection, and matching. However, the
comparisons are performed at a structural level, with constituent elements
(words or characters) matched exactly. Recent advances in word embeddings show
that semantically related words share common features in a vector-space
representation, suggesting the possibility of a hybrid grammar and word
embedding. In this paper, we blend the structure of standard context-free
grammars with the semantic generalization capabilities of word embeddings to
create hybrid semantic grammars. These semantic grammars generalize the
specific terminals used by the programmer to other words and phrases with
related meanings, allowing the construction of compact grammars that match an
entire region of the vector space rather than matching specific elements.


General machine-learning surrogate models for materials prediction

  Surrogate machine-learning models are transforming computational materials
science by predicting properties of materials with the accuracy of ab initio
methods at a fraction of the computational cost. We demonstrate surrogate
models that simultaneously interpolate energies of different materials on a
dataset of 10 binary alloys (AgCu, AlFe, AlMg, AlNi, AlTi, CoNi, CuFe, CuNi,
FeV, NbNi) with 10 different species and all possible fcc, bcc and hcp
structures up to 8 atoms in the unit cell, 15 950 structures in total. We find
that prediction errors remain unaffected when increasing the number of
simultaneously modeled alloys. Several state-of-the-art materials
representations and learning algorithms were found to qualitatively agree, with
prediction errors as low as 1 meV/atom.


Modeling Theory of Mind for Autonomous Agents with Probabilistic
  Programs

  As autonomous agents become more ubiquitous, they will eventually have to
reason about the mental state of other agents, including those agents' beliefs,
desires and goals - so-called theory of mind reasoning. We introduce a
collection of increasingly complex theory of mind models of a "chaser" pursuing
a "runner", known as the Chaser-Runner model. We show that our implementation
is a relatively straightforward theory of mind model that can capture a variety
of rich behaviors, which in turn, increase runner detection rates relative to
basic (non-theory-of-mind) models. In addition, our paper demonstrates that (1)
using a planning-as-inference formulation based on nested importance sampling
results in agents simultaneously reasoning about other agents' plans and
crafting counter-plans, (2) probabilistic programming is a natural way to
describe models in which each uses complex primitives such as path planners to
make decisions, and (3) allocating additional computation to perform nested
reasoning about agents result in lower-variance estimates of expected utility.


$Λ_b \to p l^- \barν$ form factors from lattice QCD with
  static b quarks

  We present a lattice QCD calculation of form factors for the decay $\Lambda_b
\to p \mu^- \bar{\nu}$, which is a promising channel for determining the CKM
matrix element $|V_{ub}|$ at the Large Hadron Collider. In this initial study
we work in the limit of static b quarks, where the number of independent form
factors reduces to two. We use dynamical domain-wall fermions for the light
quarks, and perform the calculation at two different lattice spacings and at
multiple values of the light-quark masses in a single large volume. Using our
form factor results, we calculate the $\Lambda_b \to p \mu^- \bar{\nu}$
differential decay rate in the range $14 GeV^2 \leq q^2 \leq q^2_{max}$, and
obtain the integral $\int_{14 GeV^2}^{q^2_{max}} [d\Gamma/dq^2] dq^2 /
|V_{ub}|^2 = 15.3 \pm 4.2 ps^{-1}$. Combined with future experimental data,
this will give a novel determination of $|V_{ub}|$ with about 15\% theoretical
uncertainty. The uncertainty is dominated by the use of the static
approximation for the b quark, and can be reduced further by performing the
lattice calculation with a more sophisticated heavy-quark action.


Estimating Human Intent for Physical Human-Robot Co-Manipulation

  Human teams can be exceptionally efficient at adapting and collaborating
during manipulation tasks using shared mental models. However, the same shared
mental models that can be used by humans to perform robust low-level force and
motion control during collaborative manipulation tasks are non-existent for
robots. For robots to perform collaborative tasks with people naturally and
efficiently, understanding and predicting human intent is necessary. However,
humans are difficult to predict and model. We have completed an exploratory
study recording motion and force for 20 human dyads moving an object in tandem
in order to better understand how they move and how their movement can be
predicted. In this paper, we show how past motion data can be used to predict
human intent. In order to predict human intent, which we equate with the human
team's velocity for a short time horizon, we used a neural network. Using the
previous 150 time steps at a rate of 200 Hz, human intent can be predicted for
the next 50 time steps with a mean squared error of 0.02 (m/s)^2. We also show
that human intent can be estimated in a human-robot dyad. This work is an
important first step in enabling future work of integrating human intent
estimation on a robot controller to execute a short-term collaborative
trajectory.


