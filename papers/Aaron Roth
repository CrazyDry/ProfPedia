Wirsing-type inequalities

  Wirsing's theorem on approximating algebraic numbers by algebraic numbers ofbounded degree is a generalization of Roth's theorem in Diophantineapproximation. We study variations of Wirsing's theorem where the inequality inthe theorem is strengthened, but one excludes a certain easily-describedspecial set of approximating algebraic points.

Differential Privacy and the Fat-Shattering Dimension of Linear Queries

  In this paper, we consider the task of answering linear queries under theconstraint of differential privacy. This is a general and well-studied class ofqueries that captures other commonly studied classes, including predicatequeries and histogram queries. We show that the accuracy to which a set oflinear queries can be answered is closely related to its fat-shatteringdimension, a property that characterizes the learnability of real-valuedfunctions in the agnostic-learning setting.

Conducting Truthful Surveys, Cheaply

  We consider the problem of conducting a survey with the goal of obtaining anunbiased estimator of some population statistic when individuals have unknowncosts (drawn from a known prior) for participating in the survey. Individualsmust be compensated for their participation and are strategic agents, and sothe payment scheme must incentivize truthful behavior. We derive optimaltruthful mechanisms for this problem for the two goals of minimizing thevariance of the estimator given a fixed budget, and minimizing the expectedcost of the survey given a fixed variance goal.

Efficiently Learning from Revealed Preference

  In this paper, we consider the revealed preferences problem from a learningperspective. Every day, a price vector and a budget is drawn from an unknowndistribution, and a rational agent buys his most preferred bundle according tosome unknown utility function, subject to the given prices and budgetconstraint. We wish not only to find a utility function which rationalizes afinite set of observations, but to produce a hypothesis valuation functionwhich accurately predicts the behavior of the agent in the future. We giveefficient algorithms with polynomial sample-complexity for agents with linearvaluation functions, as well as for agents with linearly separable, concavevaluation functions with bounded second derivative.

A generalized Schmidt subspace theorem for closed subschemes

  We prove a generalized version of Schmidt's subspace theorem for closedsubschemes in general position in terms of suitably defined Seshadri constantswith respect to a fixed ample divisor. Our proof builds on previous work byEvertse and Ferretti, Corvaja and Zannier, and others, and uses standardtechniques from algebraic geometry such as notions of positivity, blowing-upsand direct image sheaves. As an application, we recover a higher-dimensionalDiophantine approximation theorem of K.F. Roth-type due to D. McKinnon and M.Roth with a significantly shortened proof, while simultaneously extending thescope of the use of Seshadri constants in this context in a natural way.

The Frontiers of Fairness in Machine Learning

  The last few years have seen an explosion of academic and popular interest inalgorithmic fairness. Despite this interest and the volume and velocity of workthat has been produced recently, the fundamental science of fairness in machinelearning is still in a nascent state. In March 2018, we convened a group ofexperts as part of a CCC visioning workshop to assess the state of the field,and distill the most promising research directions going forward. This reportsummarizes the findings of that workshop. Along the way, it surveys recenttheoretical work in the field and points towards promising directions forresearch.

Vojta's Inequality and Rational and Integral Points of Bounded Degree on  Curves

  Let C in C_1xC_2 be a curve of type (d_1,d_2) in the product of the twocurves C_1 and C_2. Let d be a positive integer. We prove that if a certaininequality involving d_1, d_2, d, and the genera of the curves C_1, C_2, and Cis satisfied, then the set of points P in C(\kbar) with [k(P):k]<=d is finitefor any number field k. We prove a similar result for integral points ofbounded degree on C. These results are obtained as consequences of aninequality of Vojta which generalizes the Roth-Wirsing theorem to curves.

Interactive Privacy via the Median Mechanism

  We define a new interactive differentially private mechanism -- the medianmechanism -- for answering arbitrary predicate queries that arrive online.Relative to fixed accuracy and privacy constraints, this mechanism can answerexponentially more queries than the previously best known interactive privacymechanism (the Laplace mechanism, which independently perturbs each queryresult). Our guarantee is almost the best possible, even for non-interactiveprivacy mechanisms. Conceptually, the median mechanism is the first privacymechanism capable of identifying and exploiting correlations among queries inan interactive setting.  We also give an efficient implementation of the median mechanism, withrunning time polynomial in the number of queries, the database size, and thedomain size. This efficient implementation guarantees privacy for all inputdatabases, and accurate query results for almost all input databases. Thedependence of the privacy on the number of queries in this mechanism improvesover that of the best previously known efficient mechanism by asuper-polynomial factor, even in the non-interactive setting.

Fast Private Data Release Algorithms for Sparse Queries

  We revisit the problem of accurately answering large classes of statisticalqueries while preserving differential privacy. Previous approaches to thisproblem have either been very general but have not had run-time polynomial inthe size of the database, have applied only to very limited classes of queries,or have relaxed the notion of worst-case error guarantees. In this paper weconsider the large class of sparse queries, which take non-zero values on onlypolynomially many universe elements. We give efficient query release algorithmsfor this class, in both the interactive and the non-interactive setting. Ouralgorithms also achieve better accuracy bounds than previous general techniquesdo when applied to sparse queries: our bounds are independent of the universesize. In fact, even the runtime of our interactive mechanism is independent ofthe universe size, and so can be implemented in the "infinite universe" modelin which no finite universe need be specified by the data curator.

Exploiting Metric Structure for Efficient Private Query Release

  We consider the problem of privately answering queries defined on databaseswhich are collections of points belonging to some metric space. We give simple,computationally efficient algorithms for answering distance queries definedover an arbitrary metric. Distance queries are specified by points in themetric space, and ask for the average distance from the query point to thepoints contained in the database, according to the specified metric. Ouralgorithms run efficiently in the database size and the dimension of the space,and operate in both the online query release setting, and the offline settingin which they must in polynomial time generate a fixed data structure which cananswer all queries of interest. This represents one of the first subclasses oflinear queries for which efficient algorithms are known for the private queryrelease problem, circumventing known hardness results for generic linearqueries.

Constrained Signaling in Auction Design

  We consider the problem of an auctioneer who faces the task of selling a good(drawn from a known distribution) to a set of buyers, when the auctioneer doesnot have the capacity to describe to the buyers the exact identity of the goodthat he is selling. Instead, he must come up with a constrained signallingscheme: a (non injective) mapping from goods to signals, that satisfies theconstraints of his setting. For example, the auctioneer may be able tocommunicate only a bounded length message for each good, or he might be legallyconstrained in how he can advertise the item being sold. Each candidatesignaling scheme induces an incomplete-information game among the buyers, andthe goal of the auctioneer is to choose the signaling scheme and accompanyingauction format that optimizes welfare. In this paper, we use techniques fromsubmodular function maximization and no-regret learning to give algorithms forcomputing constrained signaling schemes for a variety of constrained signalingproblems.

Privacy and Mechanism Design

  This paper is a survey of recent work at the intersection of mechanism designand privacy. The connection is a natural one, but its study has beenjump-started in recent years by the advent of differential privacy, whichprovides a rigorous, quantitative way of reasoning about the costs that anagent might experience because of the loss of his privacy. Here, we surveyseveral facets of this study, and differential privacy plays a role in morethan one way. Of course, it provides us a basis for modeling agent costs forprivacy, which is essential if we are to attempt mechanism design in a settingin which agents have preferences for privacy. It also provides a toolkit forcontrolling those costs. However, perhaps more surprisingly, it provides apowerful toolkit for controlling the stability of mechanisms in general, whichyields a set of tools for designing novel mechanisms even in economic settingscompletely unrelated to privacy.

Mitigating Bias in Adaptive Data Gathering via Differential Privacy

  Data that is gathered adaptively --- via bandit algorithms, for example ---exhibits bias. This is true both when gathering simple numeric valued data ---the empirical means kept track of by stochastic bandit algorithms are biaseddownwards --- and when gathering more complicated data --- running hypothesistests on complex data gathered via contextual bandit algorithms leads to falsediscovery. In this paper, we show that this problem is mitigated if the datacollection procedure is differentially private. This lets us both bound thebias of simple numeric valued quantities (like the empirical means ofstochastic bandit algorithms), and correct the p-values of hypothesis tests runon the adaptively gathered data. Moreover, there exist differentially privatebandit algorithms with near optimal regret bounds: we apply existing theoremsin the simple stochastic case, and give a new analysis for linear contextualbandits. We complement our theoretical results with experiments validating ourtheory.

The Impact of Humanoid Affect Expression on Human Behavior in a  Game-Theoretic Setting

  With the rapid development of robot and other intelligent and autonomousagents, how a human could be influenced by a robot's expressed mood when makingdecisions becomes a crucial question in human-robot interaction. In this pilotstudy, we investigate (1) in what way a robot can express a certain mood toinfluence a human's decision making behavioral model; (2) how and to whatextent the human will be influenced in a game theoretic setting. Morespecifically, we create an NLP model to generate sentences that adhere to aspecific affective expression profile. We use these sentences for a humanoidrobot as it plays a Stackelberg security game against a human. We investigatethe behavioral model of the human player.

Take it or Leave it: Running a Survey when Privacy Comes at a Cost

  In this paper, we consider the problem of estimating a potentially sensitive(individually stigmatizing) statistic on a population. In our model,individuals are concerned about their privacy, and experience some cost as afunction of their privacy loss. Nevertheless, they would be willing toparticipate in the survey if they were compensated for their privacy cost.These cost functions are not publicly known, however, nor do we make Bayesianassumptions about their form or distribution. Individuals are rational and willmisreport their costs for privacy if doing so is in their best interest. Ghoshand Roth recently showed in this setting, when costs for privacy loss may becorrelated with private types, if individuals value differential privacy, noindividually rational direct revelation mechanism can compute any non-trivialestimate of the population statistic. In this paper, we circumvent thisimpossibility result by proposing a modified notion of how individualsexperience cost as a function of their privacy loss, and by giving a mechanismwhich does not operate by direct revelation. Instead, our mechanism has theability to randomly approach individuals from a population and offer them atake-it-or-leave-it offer. This is intended to model the abilities of asurveyor who may stand on a street corner and approach passers-by.

Counting matrices over finite fields with support on skew Young diagrams  and complements of Rothe diagrams

  We consider the problem of finding the number of matrices over a finite fieldwith a certain rank and with support that avoids a subset of the entries. Thesematrices are a q-analogue of permutations with restricted positions (i.e., rookplacements). For general sets of entries these numbers of matrices are notpolynomials in q (Stembridge 98); however, when the set of entries is a Youngdiagram, the numbers, up to a power of q-1, are polynomials with nonnegativecoefficients (Haglund 98).  In this paper, we give a number of conditions under which these numbers arepolynomials in q, or even polynomials with nonnegative integer coefficients. Weextend Haglund's result to complements of skew Young diagrams, and we applythis result to the case when the set of entries is the Rothe diagram of apermutation. In particular, we give a necessary and sufficient condition on thepermutation for its Rothe diagram to be the complement of a skew Young diagramup to rearrangement of rows and columns. We end by giving conjecturesconnecting invertible matrices whose support avoids a Rothe diagram andPoincar\'e polynomials of the strong Bruhat order.

Privately Solving Linear Programs

  In this paper, we initiate the systematic study of solving linear programsunder differential privacy. The first step is simply to define the problem: tothis end, we introduce several natural classes of private linear programs thatcapture different ways sensitive data can be incorporated into a linearprogram. For each class of linear programs we give an efficient, differentiallyprivate solver based on the multiplicative weights framework, or we give animpossibility result.

Approximately Stable, School Optimal, and Student-Truthful Many-to-One  Matchings (via Differential Privacy)

  We present a mechanism for computing asymptotically stable school optimalmatchings, while guaranteeing that it is an asymptotic dominant strategy forevery student to report their true preferences to the mechanism. Our main toolin this endeavor is differential privacy: we give an algorithm that coordinatesa stable matching using differentially private signals, which lead to ourtruthfulness guarantee. This is the first setting in which it is known how toachieve nontrivial truthfulness guarantees for students when computing schooloptimal matchings, assuming worst- case preferences (for schools and students)in large markets.

Auctions with Online Supply

  We study the problem of selling identical goods to n unit-demand bidders in asetting in which the total supply of goods is unknown to the mechanism. Itemsarrive dynamically, and the seller must make the allocation and paymentdecisions online with the goal of maximizing social welfare. We consider twomodels of unknown supply: the adversarial supply model, in which the mechanismmust produce a welfare guarantee for any arbitrary supply, and the stochasticsupply model, in which supply is drawn from a distribution known to themechanism, and the mechanism need only provide a welfare guarantee inexpectation.  Our main result is a separation between these two models. We show that alltruthful mechanisms, even randomized, achieve a diminishing fraction of theoptimal social welfare (namely, no better than a Omega(loglog n) approximation)in the adversarial setting. In sharp contrast, in the stochastic model, under astandard monotone hazard-rate condition, we present a truthful mechanism thatachieves a constant approximation. We show that the monotone hazard ratecondition is necessary, and also characterize a natural subclass of truthfulmechanisms in our setting, the set of online-envy-free mechanisms. All of themechanisms we present fall into this class, and we prove almost optimal lowerbounds for such mechanisms. Since auctions with unknown supply are regularlyrun in many online-advertising settings, our main results emphasize theimportance of considering distributional information in the design of auctionsin such environments.

A Learning Theory Approach to Non-Interactive Database Privacy

  In this paper we demonstrate that, ignoring computational constraints, it ispossible to privately release synthetic databases that are useful for largeclasses of queries -- much larger in size than the database itself.Specifically, we give a mechanism that privately releases synthetic data for aclass of queries over a discrete domain with error that grows as a function ofthe size of the smallest net approximately representing the answers to thatclass of queries. We show that this in particular implies a mechanism forcounting queries that gives error guarantees that grow only with theVC-dimension of the class of queries, which itself grows only logarithmicallywith the size of the query class.  We also show that it is not possible to privately release even simple classesof queries (such as intervals and their generalizations) over continuousdomains. Despite this, we give a privacy-preserving polynomial time algorithmthat releases information useful for all halfspace queries, given a slightrelaxation of the utility guarantee. This algorithm does not release syntheticdata, but instead another data structure capable of representing an answer foreach query. We also give an efficient algorithm for releasing synthetic datafor the class of interval queries and axis-aligned rectangles of constantdimension.  Finally, inspired by learning theory, we introduce a new notion of dataprivacy, which we call distributional privacy, and show that it is strictlystronger than the prevailing privacy notion, differential privacy.

Buying Private Data without Verification

  We consider the problem of designing a survey to aggregate non-verifiableinformation from a privacy-sensitive population: an analyst wants to computesome aggregate statistic from the private bits held by each member of apopulation, but cannot verify the correctness of the bits reported byparticipants in his survey. Individuals in the population are strategic agentswith a cost for privacy, \ie, they not only account for the payments theyexpect to receive from the mechanism, but also their privacy costs from anyinformation revealed about them by the mechanism's outcome---the computedstatistic as well as the payments---to determine their utilities. How can theanalyst design payments to obtain an accurate estimate of the populationstatistic when individuals strategically decide both whether to participate andwhether to truthfully report their sensitive information?  We design a differentially private peer-prediction mechanism that supportsaccurate estimation of the population statistic as a Bayes-Nash equilibrium insettings where agents have explicit preferences for privacy. The mechanismrequires knowledge of the marginal prior distribution on bits $b_i$, but doesnot need full knowledge of the marginal distribution on the costs $c_i$,instead requiring only an approximate upper bound. Our mechanism guarantees$\epsilon$-differential privacy to each agent $i$ against any adversary who canobserve the statistical estimate output by the mechanism, as well as thepayments made to the $n-1$ other agents $j\neq i$. Finally, we show that withslightly more structured assumptions on the privacy cost functions of eachagent, the cost of running the survey goes to $0$ as the number of agentsdiverges.

Downstream Effects of Affirmative Action

  We study a two-stage model, in which students are 1) admitted to college onthe basis of an entrance exam which is a noisy signal about theirqualifications (type), and then 2) those students who were admitted to collegecan be hired by an employer as a function of their college grades, which are anindependently drawn noisy signal of their type. Students are drawn from one oftwo populations, which might have different type distributions. We assume thatthe employer at the end of the pipeline is rational, in the sense that itcomputes a posterior distribution on student type conditional on allinformation that it has available (college admissions, grades, and groupmembership), and makes a decision based on posterior expectation. We then studywhat kinds of fairness goals can be achieved by the college by setting itsadmissions rule and grading policy. For example, the college might have thegoal of guaranteeing equal opportunity across populations: that the probabilityof passing through the pipeline and being hired by the employer should beindependent of group membership, conditioned on type. Alternately, the collegemight have the goal of incentivizing the employer to have a group blind hiringrule. We show that both goals can be achieved when the college does not reportgrades. On the other hand, we show that under reasonable conditions, thesegoals are impossible to achieve even in isolation when the college uses an(even minimally) informative grading policy.

How to Use Heuristics for Differential Privacy

  We develop theory for using heuristics to solve computationally hard problemsin differential privacy. Heuristic approaches have enjoyed tremendous successin machine learning, for which performance can be empirically evaluated.However, privacy guarantees cannot be evaluated empirically, and must be proven--- without making heuristic assumptions. We show that learning problems overbroad classes of functions can be solved privately and efficiently, assumingthe existence of a non-private oracle for solving the same problem. Our firstalgorithm yields a privacy guarantee that is contingent on the correctness ofthe oracle. We then give a reduction which applies to a class of heuristicswhich we call certifiable, which allows us to convert oracle-dependent privacyguarantees to worst-case privacy guarantee that hold even when the heuristicstanding in for the oracle might fail in adversarial ways. Finally, we considera broad class of functions that includes most classes of simple booleanfunctions studied in the PAC learning literature, including conjunctions,disjunctions, parities, and discrete halfspaces. We show that there is anefficient algorithm for privately constructing synthetic data for any suchclass, given a non-private learning oracle. This in particular gives the firstoracle-efficient algorithm for privately generating synthetic data forcontingency tables. The most intriguing question left open by our work iswhether or not every problem that can be solved differentially privately can beprivately solved with an oracle-efficient algorithm. While we do not resolvethis, we give a barrier result that suggests that any generic oracle-efficientreduction must fall outside of a natural class of algorithms (which includesthe algorithms given in this paper).

Constrained Non-Monotone Submodular Maximization: Offline and Secretary  Algorithms

  Constrained submodular maximization problems have long been studied, withnear-optimal results known under a variety of constraints when the submodularfunction is monotone. The case of non-monotone submodular maximization is lessunderstood: the first approximation algorithms even for the unconstraintedsetting were given by Feige et al. (FOCS '07). More recently, Lee et al. (STOC'09, APPROX '09) show how to approximately maximize non-monotone submodularfunctions when the constraints are given by the intersection of p matroidconstraints; their algorithm is based on local-search procedures that considerp-swaps, and hence the running time may be n^Omega(p), implying their algorithmis polynomial-time only for constantly many matroids. In this paper, we givealgorithms that work for p-independence systems (which generalize constraintsgiven by the intersection of p matroids), where the running time is poly(n,p).Our algorithm essentially reduces the non-monotone maximization problem tomultiple runs of the greedy algorithm previously used in the monotone case.  Our idea of using existing algorithms for monotone functions to solve thenon-monotone case also works for maximizing a submodular function with respectto a knapsack constraint: we get a simple greedy-based constant-factorapproximation for this problem.  With these simpler algorithms, we are able to adapt our approach toconstrained non-monotone submodular maximization to the (online) secretarysetting, where elements arrive one at a time in random order, and the algorithmmust make irrevocable decisions about whether or not to select each element asit arrives. We give constant approximations in this secretary setting when thealgorithm is constrained subject to a uniform matroid or a partition matroid,and give an O(log k) approximation when it is constrained by a general matroidof rank k.

Selling Privacy at Auction

  We initiate the study of markets for private data, though the lens ofdifferential privacy. Although the purchase and sale of private data hasalready begun on a large scale, a theory of privacy as a commodity is missing.In this paper, we propose to build such a theory. Specifically, we consider asetting in which a data analyst wishes to buy information from a populationfrom which he can estimate some statistic. The analyst wishes to obtain anaccurate estimate cheaply. On the other hand, the owners of the private dataexperience some cost for their loss of privacy, and must be compensated forthis loss. Agents are selfish, and wish to maximize their profit, so our goalis to design truthful mechanisms. Our main result is that such auctions cannaturally be viewed and optimally solved as variants of multi-unit procurementauctions. Based on this result, we derive auctions for two natural settingswhich are optimal up to small constant factors:  1. In the setting in which the data analyst has a fixed accuracy goal, weshow that an application of the classic Vickrey auction achieves the analyst'saccuracy goal while minimizing his total payment.  2. In the setting in which the data analyst has a fixed budget, we give amechanism which maximizes the accuracy of the resulting estimate whileguaranteeing that the resulting sum payments do not exceed the analysts budget.  In both cases, our comparison class is the set of envy-free mechanisms, whichcorrespond to the natural class of fixed-price mechanisms in our setting.  In both of these results, we ignore the privacy cost due to possiblecorrelations between an individuals private data and his valuation for privacyitself. We then show that generically, no individually rational mechanism cancompensate individuals for the privacy loss incurred due to their reportedvaluations for privacy.

Iterative Constructions and Private Data Release

  In this paper we study the problem of approximately releasing the cutfunction of a graph while preserving differential privacy, and give newalgorithms (and new analyses of existing algorithms) in both the interactiveand non-interactive settings.  Our algorithms in the interactive setting are achieved by revisiting theproblem of releasing differentially private, approximate answers to a largenumber of queries on a database. We show that several algorithms for thisproblem fall into the same basic framework, and are based on the existence ofobjects which we call iterative database construction algorithms. We give a newgeneric framework in which new (efficient) IDC algorithms give rise to new(efficient) interactive private query release mechanisms. Our modular analysissimplifies and tightens the analysis of previous algorithms, leading toimproved bounds. We then give a new IDC algorithm (and therefore a new private,interactive query release mechanism) based on the Frieze/Kannan low-rank matrixdecomposition. This new release mechanism gives an improvement on prior work ina range of parameters where the size of the database is comparable to the sizeof the data universe (such as releasing all cut queries on dense graphs).  We also give a non-interactive algorithm for efficiently releasing privatesynthetic data for graph cuts with error O(|V|^{1.5}). Our algorithm is basedon randomized response and a non-private implementation of the SDP-based,constant-factor approximation algorithm for cut-norm due to Alon and Naor.Finally, we give a reduction based on the IDC framework showing that anefficient, private algorithm for computing sufficiently accurate rank-1 matrixapproximations would lead to an improved efficient algorithm for releasingprivate synthetic data for graph cuts. We leave finding such an algorithm asour main open problem.

Mechanism Design in Large Games: Incentives and Privacy

  We study the problem of implementing equilibria of complete information gamesin settings of incomplete information, and address this problem using"recommender mechanisms." A recommender mechanism is one that does not have thepower to enforce outcomes or to force participation, rather it only has thepower to suggestion outcomes on the basis of voluntary participation. We showthat despite these restrictions, recommender mechanisms can implementequilibria of complete information games in settings of incomplete informationunder the condition that the game is large---i.e. that there are a large numberof players, and any player's action affects any other's payoff by at most asmall amount.  Our result follows from a novel application of differential privacy. We showthat any algorithm that computes a correlated equilibrium of a completeinformation game while satisfying a variant of differential privacy---which wecall joint differential privacy---can be used as a recommender mechanism whilesatisfying our desired incentive properties. Our main technical result is analgorithm for computing a correlated equilibrium of a large game whilesatisfying joint differential privacy.  Although our recommender mechanisms are designed to satisfy game-theoreticproperties, our solution ends up satisfying a strong privacy property as well.No group of players can learn "much" about the type of any player outside thegroup from the recommendations of the mechanism, even if these players colludein an arbitrary way. As such, our algorithm is able to implement equilibria ofcomplete information games, without revealing information about the realizedtypes.

Asymptotically Truthful Equilibrium Selection in Large Congestion Games

  Studying games in the complete information model makes them analyticallytractable. However, large $n$ player interactions are more realisticallymodeled as games of incomplete information, where players may know little tonothing about the types of other players. Unfortunately, games in incompleteinformation settings lose many of the nice properties of complete informationgames: the quality of equilibria can become worse, the equilibria lose theirex-post properties, and coordinating on an equilibrium becomes even moredifficult. Because of these problems, we would like to study games ofincomplete information, but still implement equilibria of the completeinformation game induced by the (unknown) realized player types.  This problem was recently studied by Kearns et al. and solved in large gamesby means of introducing a weak mediator: their mediator took as input reportedtypes of players, and output suggested actions which formed a correlatedequilibrium of the underlying game. Players had the option to playindependently of the mediator, or ignore its suggestions, but crucially, ifthey decided to opt-in to the mediator, they did not have the power to lieabout their type. In this paper, we rectify this deficiency in the setting oflarge congestion games. We give, in a sense, the weakest possible mediator: itcannot enforce participation, verify types, or enforce its suggestions.Moreover, our mediator implements a Nash equilibrium of the completeinformation game. We show that it is an (asymptotic) ex-post equilibrium of theincomplete information game for all players to use the mediator honestly, andthat when they do so, they end up playing an approximate Nash equilibrium ofthe induced complete information game. In particular, truthful use of themediator is a Bayes-Nash equilibrium in any Bayesian game for any prior.

Beating Randomized Response on Incoherent Matrices

  Computing accurate low rank approximations of large matrices is a fundamentaldata mining task. In many applications however the matrix contains sensitiveinformation about individuals. In such case we would like to release a low rankapproximation that satisfies a strong privacy guarantee such as differentialprivacy. Unfortunately, to date the best known algorithm for this task thatsatisfies differential privacy is based on naive input perturbation orrandomized response: Each entry of the matrix is perturbed independently by asufficiently large random noise variable, a low rank approximation is thencomputed on the resulting matrix.  We give (the first) significant improvements in accuracy over randomizedresponse under the natural and necessary assumption that the matrix has lowcoherence. Our algorithm is also very efficient and finds a constant rankapproximation of an m x n matrix in time O(mn). Note that even generating thenoise matrix required for randomized response already requires time O(mn).

Distributed Private Heavy Hitters

  In this paper, we give efficient algorithms and lower bounds for solving theheavy hitters problem while preserving differential privacy in the fullydistributed local model. In this model, there are n parties, each of whichpossesses a single element from a universe of size N. The heavy hitters problemis to find the identity of the most common element shared amongst the nparties. In the local model, there is no trusted database administrator, and sothe algorithm must interact with each of the $n$ parties separately, using adifferentially private protocol. We give tight information-theoretic upper andlower bounds on the accuracy to which this problem can be solved in the localmodel (giving a separation between the local model and the more commoncentralized model of privacy), as well as computationally efficient algorithmseven in the case where the data universe N may be exponentially large.

Differential Privacy for the Analyst via Private Equilibrium Computation

  We give new mechanisms for answering exponentially many queries from multipleanalysts on a private database, while protecting differential privacy both forthe individuals in the database and for the analysts. That is, our mechanism'sanswer to each query is nearly insensitive to changes in the queries asked byother analysts. Our mechanism is the first to offer differential privacy on thejoint distribution over analysts' answers, providing privacy for data analystseven if the other data analysts collude or register multiple accounts. In somesettings, we are able to achieve nearly optimal error rates (even compared tomechanisms which do not offer analyst privacy), and we are able to extend ourtechniques to handle non-linear queries. Our analysis is based on a novel viewof the private query-release problem as a two-player zero-sum game, which maybe of independent interest.

Beyond Worst-Case Analysis in Private Singular Vector Computation

  We consider differentially private approximate singular vector computation.Known worst-case lower bounds show that the error of any differentially privatealgorithm must scale polynomially with the dimension of the singular vector. Weare able to replace this dependence on the dimension by a natural parameterknown as the coherence of the matrix that is often observed to be significantlysmaller than the dimension both theoretically and empirically. We also prove amatching lower bound showing that our guarantee is nearly optimal for everysetting of the coherence parameter. Notably, we achieve our bounds by giving arobust analysis of the well-known power iteration algorithm, which may be ofindependent interest. Our algorithm also leads to improvements in worst-casesettings and to better low-rank approximations in the spectral norm.

Dual Query: Practical Private Query Release for High Dimensional Data

  We present a practical, differentially private algorithm for answering alarge number of queries on high dimensional datasets. Like all algorithms forthis task, ours necessarily has worst-case complexity exponential in thedimension of the data. However, our algorithm packages the computationally hardstep into a concisely defined integer program, which can be solvednon-privately using standard solvers. We prove accuracy and privacy theoremsfor our algorithm, and then demonstrate experimentally that our algorithmperforms well in practice. For example, our algorithm can efficiently andaccurately answer millions of queries on the Netflix dataset, which has over17,000 attributes; this is an improvement on the state of the art by multipleorders of magnitude.

An Anti-Folk Theorem for Large Repeated Games with Imperfect Monitoring

  We study infinitely repeated games in settings of imperfect monitoring. Wefirst prove a family of theorems that show that when the signals observed bythe players satisfy a condition known as $(\epsilon, \gamma)$-differentialprivacy, that the folk theorem has little bite: for values of $\epsilon$ and$\gamma$ sufficiently small, for a fixed discount factor, any equilibrium ofthe repeated game involve players playing approximate equilibria of the stagegame in every period. Next, we argue that in large games ($n$ player games inwhich unilateral deviations by single players have only a small impact on theutility of other players), many monitoring settings naturally lead to signalsthat satisfy $(\epsilon,\gamma)$-differential privacy, for $\epsilon$ and$\gamma$ tending to zero as the number of players $n$ grows large. We concludethat in such settings, the set of equilibria of the repeated game collapse tothe set of equilibria of the stage game.

Online Learning and Profit Maximization from Revealed Preferences

  We consider the problem of learning from revealed preferences in an onlinesetting. In our framework, each period a consumer buys an optimal bundle ofgoods from a merchant according to her (linear) utility function and currentprices, subject to a budget constraint. The merchant observes only thepurchased goods, and seeks to adapt prices to optimize his profits. We give anefficient algorithm for the merchant's problem that consists of a learningphase in which the consumer's utility function is (perhaps partially) inferred,followed by a price optimization step. We also consider an alternative onlinelearning algorithm for the setting where prices are set exogenously, but themerchant would still like to predict the bundle that will be bought by theconsumer for purposes of inventory or supply chain management. In contrast withmost prior work on the revealed preferences problem, we demonstrate that bymaking stronger assumptions on the form of utility functions, efficientalgorithms for both learning and profit maximization are possible, even inadaptive, online settings.

Jointly Private Convex Programming

  In this paper we present an extremely general method for approximatelysolving a large family of convex programs where the solution can be dividedbetween different agents, subject to joint differential privacy. This classincludes multi-commodity flow problems, general allocation problems, andmulti-dimensional knapsack problems, among other examples. The accuracy of ouralgorithm depends on the \emph{number} of constraints that bind betweenindividuals, but crucially, is \emph{nearly independent} of the number ofprimal variables and hence the number of agents who make up the problem. As thenumber of agents in a problem grows, the error we introduce often becomesnegligible.  We also consider the setting where agents are strategic and have preferencesover their part of the solution. For any convex program in this class thatmaximizes \emph{social welfare}, there is a generic reduction that makes thecorresponding optimization \emph{approximately dominant strategy truthful} bycharging agents prices for resources as a function of the approximately optimaldual variables, which are themselves computed under differential privacy. Ourresults substantially expand the class of problems that are known to besolvable under both privacy and incentive constraints.

Inducing Approximately Optimal Flow Using Truthful Mediators

  We revisit a classic coordination problem from the perspective of mechanismdesign: how can we coordinate a social welfare maximizing flow in a networkcongestion game with selfish players? The classical approach, which computestolls as a function of known demands, fails when the demands are unknown to themechanism designer, and naively eliciting them does not necessarily yield atruthful mechanism. Instead, we introduce a weak mediator that can providesuggested routes to players and set tolls as a function of reported demands.However, players can choose to ignore or misreport their type to this mediator.Using techniques from differential privacy, we show how to design a weakmediator such that it is an asymptotic ex-post Nash equilibrium for all playersto truthfully report their types to the mediator and faithfully follow itssuggestion, and that when they do, they end up playing a nearly optimal flow.Notably, our solution works in settings of incomplete information even in theabsence of a prior distribution on player types. Along the way, we develop newtechniques for privately solving convex programs which may be of independentinterest.

Privacy for the Protected (Only)

  Motivated by tensions between data privacy for individual citizens, andsocietal priorities such as counterterrorism and the containment of infectiousdisease, we introduce a computational model that distinguishes between partiesfor whom privacy is explicitly protected, and those for whom it is not (thetargeted subpopulation). The goal is the development of algorithms that caneffectively identify and take action upon members of the targeted subpopulationin a way that minimally compromises the privacy of the protected, whilesimultaneously limiting the expense of distinguishing members of the two groupsvia costly mechanisms such as surveillance, background checks, or medicaltesting. Within this framework, we provide provably privacy-preservingalgorithms for targeted search in social networks. These algorithms are naturalvariants of common graph search methods, and ensure privacy for the protectedby the careful injection of noise in the prioritization of potential targets.We validate the utility of our algorithms with extensive computationalexperiments on two large-scale social network datasets.

Robust Mediators in Large Games

  A mediator is a mechanism that can only suggest actions to players, as afunction of all agents' reported types, in a given game of incompleteinformation. We study what is achievable by two kinds of mediators, "strong"and "weak." Players can choose to opt-out of using a strong mediator but cannotmisrepresent their type if they opt-in. Such a mediator is "strong" because wecan view it as having the ability to verify player types. Weak mediators lackthis ability--- players are free to misrepresent their type to a weak mediator.We show a striking result---in a prior-free setting, assuming only that thegame is large and players have private types, strong mediators can implementapproximate equilibria of the complete-information game. If the game is acongestion game, then the same result holds using only weak mediators. Ourresult follows from a novel application of differential privacy, in particular,a variant we propose called joint differential privacy.

Fair Algorithms for Infinite and Contextual Bandits

  We study fairness in linear bandit problems. Starting from the notion ofmeritocratic fairness introduced in Joseph et al. [2016], we carry out a morerefined analysis of a more general problem, achieving better performanceguarantees with fewer modelling assumptions on the number and structure ofavailable choices as well as the number selected. We also analyze thepreviously-unstudied question of fairness in infinite linear bandit problems,obtaining instance-dependent regret upper bounds as well as lower boundsdemonstrating that this instance-dependence is necessary. The result is aframework for meritocratic fairness in an online linear setting that issubstantially more powerful, general, and realistic than the current state ofthe art.

Fairness in Reinforcement Learning

  We initiate the study of fairness in reinforcement learning, where theactions of a learning algorithm may affect its environment and future rewards.Our fairness constraint requires that an algorithm never prefers one actionover another if the long-term (discounted) reward of choosing the latter actionis higher. Our first result is negative: despite the fact that fairness isconsistent with the optimal policy, any learning algorithm satisfying fairnessmust take time exponential in the number of states to achieve non-trivialapproximation to the optimal policy. We then provide a provably fair polynomialtime algorithm under an approximate notion of fairness, thus establishing anexponential gap between exact and approximate fairness

Fairness in Criminal Justice Risk Assessments: The State of the Art

  Objectives: Discussions of fairness in criminal justice risk assessmentstypically lack conceptual precision. Rhetoric too often substitutes for carefulanalysis. In this paper, we seek to clarify the tradeoffs between differentkinds of fairness and between fairness and accuracy.  Methods: We draw on the existing literatures in criminology, computer scienceand statistics to provide an integrated examination of fairness and accuracy incriminal justice risk assessments. We also provide an empirical illustrationusing data from arraignments.  Results: We show that there are at least six kinds of fairness, some of whichare incompatible with one another and with accuracy.  Conclusions: Except in trivial cases, it is impossible to maximize accuracyand fairness at the same time, and impossible simultaneously to satisfy allkinds of fairness. In practice, a major complication is different base ratesacross different legally protected groups. There is a need to considerchallenging tradeoffs.

A Convex Framework for Fair Regression

  We introduce a flexible family of fairness regularizers for (linear andlogistic) regression problems. These regularizers all enjoy convexity,permitting fast optimization, and they span the rang from notions of groupfairness to strong individual fairness. By varying the weight on the fairnessregularizer, we can compute the efficient frontier of the accuracy-fairnesstrade-off on any given dataset, and we measure the severity of this trade-offvia a numerical quantity we call the Price of Fairness (PoF). The centerpieceof our results is an extensive comparative study of the PoF across sixdifferent datasets in which fairness is a primary consideration.

Strategic Classification from Revealed Preferences

  We study an online linear classification problem, in which the data isgenerated by strategic agents who manipulate their features in an effort tochange the classification outcome. In rounds, the learner deploys a classifier,and an adversarially chosen agent arrives, possibly manipulating her featuresto optimally respond to the learner. The learner has no knowledge of theagents' utility functions or "real" features, which may vary widely acrossagents. Instead, the learner is only able to observe their "revealedpreferences" --- i.e. the actual manipulated feature vectors they provide. Fora broad family of agent cost functions, we give a computationally efficientlearning algorithm that is able to obtain diminishing "Stackelberg regret" ---a form of policy regret that guarantees that the learner is obtaining lossnearly as small as that of the best classifier in hindsight, even allowing forthe fact that agents will best-respond differently to the optimal classifier.

A Smoothed Analysis of the Greedy Algorithm for the Linear Contextual  Bandit Problem

  Bandit learning is characterized by the tension between long-term explorationand short-term exploitation. However, as has recently been noted, in settingsin which the choices of the learning algorithm correspond to importantdecisions about individual people (such as criminal recidivism prediction,lending, and sequential drug trials), exploration corresponds to explicitlysacrificing the well-being of one individual for the potential future benefitof others. This raises a fairness concern. In such settings, one might like torun a "greedy" algorithm, which always makes the (myopically) optimal decisionfor the individuals at hand - but doing this can result in a catastrophicfailure to learn. In this paper, we consider the linear contextual banditproblem and revisit the performance of the greedy algorithm. We give a smoothedanalysis, showing that even when contexts may be chosen by an adversary, smallperturbations of the adversary's choices suffice for the algorithm to achieve"no regret", perhaps (depending on the specifics of the setting) with aconstant amount of initial training data. This suggests that "generically"(i.e. in slightly perturbed environments), exploration and exploitation neednot be in conflict in the linear setting.

Online Learning with an Unknown Fairness Metric

  We consider the problem of online learning in the linear contextual banditssetting, but in which there are also strong individual fairness constraintsgoverned by an unknown similarity metric. These constraints demand that weselect similar actions or individuals with approximately equal probability(arXiv:1104.3913), which may be at odds with optimizing reward, thus modelingsettings where profit and social policy are in tension. We assume we learnabout an unknown Mahalanobis similarity metric from only weak feedback thatidentifies fairness violations, but does not quantify their extent. This isintended to represent the interventions of a regulator who "knows unfairnesswhen he sees it" but nevertheless cannot enunciate a quantitative fairnessmetric over individuals. Our main result is an algorithm in the adversarialcontext setting that has a number of fairness violations that depends onlylogarithmically on $T$, while obtaining an optimal $O(\sqrt{T})$ regret boundto the best fair policy.

Local Differential Privacy for Evolving Data

  There are now several large scale deployments of differential privacy used tocollect statistical information about users. However, these deploymentsperiodically recollect the data and recompute the statistics using algorithmsdesigned for a single use. As a result, these systems do not provide meaningfulprivacy guarantees over long time scales. Moreover, existing techniques tomitigate this effect do not apply in the "local model" of differential privacythat these systems use.  In this paper, we introduce a new technique for local differential privacythat makes it possible to maintain up-to-date statistics over time, withprivacy guarantees that degrade only in the number of changes in the underlyingdistribution rather than the number of collection periods. We use our techniquefor tracking a changing statistic in the setting where users are partitionedinto an unknown collection of groups, and at every time period each user drawsa single bit from a common (but changing) group-specific distribution. We alsoprovide an application to frequency and heavy-hitter estimation.

Equal Opportunity in Online Classification with Partial Feedback

  We study an online classification problem with partial feedback in whichindividuals arrive one at a time from a fixed but unknown distribution, andmust be classified as positive or negative. Our algorithm only observes thetrue label of an individual if they are given a positive classification. Thissetting captures many classification problems for which fairness is a concern:for example, in criminal recidivism prediction, recidivism is only observed ifthe inmate is released; in lending applications, loan repayment is onlyobserved if the loan is granted. We require that our algorithms satisfy commonstatistical fairness constraints (such as equalizing false positive or negativerates --- introduced as "equal opportunity" in Hardt et al. (2016)) at everyround, with respect to the underlying distribution. We give upper and lowerbounds characterizing the cost of this constraint in terms of the regret rate(and show that it is mild), and give an oracle efficient algorithm thatachieves the upper bound.

Differentially Private Combinatorial Optimization

  Consider the following problem: given a metric space, some of whose pointsare "clients", open a set of at most $k$ facilities to minimize the averagedistance from the clients to these facilities. This is just the well-studied$k$-median problem, for which many approximation algorithms and hardnessresults are known. Note that the objective function encourages openingfacilities in areas where there are many clients, and given a solution, it isoften possible to get a good idea of where the clients are located. However,this poses the following quandary: what if the identity of the clients issensitive information that we would like to keep private? Is it even possibleto design good algorithms for this problem that preserve the privacy of theclients?  In this paper, we initiate a systematic study of algorithms for discreteoptimization problems in the framework of differential privacy (whichformalizes the idea of protecting the privacy of individual input elements). Weshow that many such problems indeed have good approximation algorithms thatpreserve differential privacy; this is even in cases where it is impossible topreserve cryptographic definitions of privacy while computing any non-trivialapproximation to even the_value_ of an optimal solution, let alone the entiresolution.  Apart from the $k$-median problem, we study the problems of vertex and setcover, min-cut, facility location, Steiner tree, and the recently introducedsubmodular maximization problem, "Combinatorial Public Projects" (CPP).

Differential Privacy: An Economic Method for Choosing Epsilon

  Differential privacy is becoming a gold standard for privacy research; itoffers a guaranteed bound on loss of privacy due to release of query results,even under worst-case assumptions. The theory of differential privacy is anactive research area, and there are now differentially private algorithms for awide range of interesting problems.  However, the question of when differential privacy works in practice hasreceived relatively little attention. In particular, there is still no rigorousmethod for choosing the key parameter $\epsilon$, which controls the crucialtradeoff between the strength of the privacy guarantee and the accuracy of thepublished results.  In this paper, we examine the role that these parameters play in concreteapplications, identifying the key questions that must be addressed whenchoosing specific values. This choice requires balancing the interests of twodifferent parties: the data analyst and the prospective participant, who mustdecide whether to allow their data to be included in the analysis. We propose asimple model that expresses this balance as formulas over a handful ofparameters, and we use our model to choose $\epsilon$ on a series of simplestatistical studies. We also explore a surprising insight: in somecircumstances, a differentially private study can be more accurate than anon-private study for the same cost, under our model. Finally, we discuss thesimplifying assumptions in our model and outline a research agenda for possiblerefinements.

