Wirsing-type inequalities

  Wirsing's theorem on approximating algebraic numbers by algebraic numbers of
bounded degree is a generalization of Roth's theorem in Diophantine
approximation. We study variations of Wirsing's theorem where the inequality in
the theorem is strengthened, but one excludes a certain easily-described
special set of approximating algebraic points.


Differential Privacy and the Fat-Shattering Dimension of Linear Queries

  In this paper, we consider the task of answering linear queries under the
constraint of differential privacy. This is a general and well-studied class of
queries that captures other commonly studied classes, including predicate
queries and histogram queries. We show that the accuracy to which a set of
linear queries can be answered is closely related to its fat-shattering
dimension, a property that characterizes the learnability of real-valued
functions in the agnostic-learning setting.


Efficiently Learning from Revealed Preference

  In this paper, we consider the revealed preferences problem from a learning
perspective. Every day, a price vector and a budget is drawn from an unknown
distribution, and a rational agent buys his most preferred bundle according to
some unknown utility function, subject to the given prices and budget
constraint. We wish not only to find a utility function which rationalizes a
finite set of observations, but to produce a hypothesis valuation function
which accurately predicts the behavior of the agent in the future. We give
efficient algorithms with polynomial sample-complexity for agents with linear
valuation functions, as well as for agents with linearly separable, concave
valuation functions with bounded second derivative.


Conducting Truthful Surveys, Cheaply

  We consider the problem of conducting a survey with the goal of obtaining an
unbiased estimator of some population statistic when individuals have unknown
costs (drawn from a known prior) for participating in the survey. Individuals
must be compensated for their participation and are strategic agents, and so
the payment scheme must incentivize truthful behavior. We derive optimal
truthful mechanisms for this problem for the two goals of minimizing the
variance of the estimator given a fixed budget, and minimizing the expected
cost of the survey given a fixed variance goal.


A generalized Schmidt subspace theorem for closed subschemes

  We prove a generalized version of Schmidt's subspace theorem for closed
subschemes in general position in terms of suitably defined Seshadri constants
with respect to a fixed ample divisor. Our proof builds on previous work by
Evertse and Ferretti, Corvaja and Zannier, and others, and uses standard
techniques from algebraic geometry such as notions of positivity, blowing-ups
and direct image sheaves. As an application, we recover a higher-dimensional
Diophantine approximation theorem of K.F. Roth-type due to D. McKinnon and M.
Roth with a significantly shortened proof, while simultaneously extending the
scope of the use of Seshadri constants in this context in a natural way.


The Frontiers of Fairness in Machine Learning

  The last few years have seen an explosion of academic and popular interest in
algorithmic fairness. Despite this interest and the volume and velocity of work
that has been produced recently, the fundamental science of fairness in machine
learning is still in a nascent state. In March 2018, we convened a group of
experts as part of a CCC visioning workshop to assess the state of the field,
and distill the most promising research directions going forward. This report
summarizes the findings of that workshop. Along the way, it surveys recent
theoretical work in the field and points towards promising directions for
research.


Vojta's Inequality and Rational and Integral Points of Bounded Degree on
  Curves

  Let C in C_1xC_2 be a curve of type (d_1,d_2) in the product of the two
curves C_1 and C_2. Let d be a positive integer. We prove that if a certain
inequality involving d_1, d_2, d, and the genera of the curves C_1, C_2, and C
is satisfied, then the set of points P in C(\kbar) with [k(P):k]<=d is finite
for any number field k. We prove a similar result for integral points of
bounded degree on C. These results are obtained as consequences of an
inequality of Vojta which generalizes the Roth-Wirsing theorem to curves.


Interactive Privacy via the Median Mechanism

  We define a new interactive differentially private mechanism -- the median
mechanism -- for answering arbitrary predicate queries that arrive online.
Relative to fixed accuracy and privacy constraints, this mechanism can answer
exponentially more queries than the previously best known interactive privacy
mechanism (the Laplace mechanism, which independently perturbs each query
result). Our guarantee is almost the best possible, even for non-interactive
privacy mechanisms. Conceptually, the median mechanism is the first privacy
mechanism capable of identifying and exploiting correlations among queries in
an interactive setting.
  We also give an efficient implementation of the median mechanism, with
running time polynomial in the number of queries, the database size, and the
domain size. This efficient implementation guarantees privacy for all input
databases, and accurate query results for almost all input databases. The
dependence of the privacy on the number of queries in this mechanism improves
over that of the best previously known efficient mechanism by a
super-polynomial factor, even in the non-interactive setting.


Exploiting Metric Structure for Efficient Private Query Release

  We consider the problem of privately answering queries defined on databases
which are collections of points belonging to some metric space. We give simple,
computationally efficient algorithms for answering distance queries defined
over an arbitrary metric. Distance queries are specified by points in the
metric space, and ask for the average distance from the query point to the
points contained in the database, according to the specified metric. Our
algorithms run efficiently in the database size and the dimension of the space,
and operate in both the online query release setting, and the offline setting
in which they must in polynomial time generate a fixed data structure which can
answer all queries of interest. This represents one of the first subclasses of
linear queries for which efficient algorithms are known for the private query
release problem, circumventing known hardness results for generic linear
queries.


Constrained Signaling in Auction Design

  We consider the problem of an auctioneer who faces the task of selling a good
(drawn from a known distribution) to a set of buyers, when the auctioneer does
not have the capacity to describe to the buyers the exact identity of the good
that he is selling. Instead, he must come up with a constrained signalling
scheme: a (non injective) mapping from goods to signals, that satisfies the
constraints of his setting. For example, the auctioneer may be able to
communicate only a bounded length message for each good, or he might be legally
constrained in how he can advertise the item being sold. Each candidate
signaling scheme induces an incomplete-information game among the buyers, and
the goal of the auctioneer is to choose the signaling scheme and accompanying
auction format that optimizes welfare. In this paper, we use techniques from
submodular function maximization and no-regret learning to give algorithms for
computing constrained signaling schemes for a variety of constrained signaling
problems.


Privacy and Mechanism Design

  This paper is a survey of recent work at the intersection of mechanism design
and privacy. The connection is a natural one, but its study has been
jump-started in recent years by the advent of differential privacy, which
provides a rigorous, quantitative way of reasoning about the costs that an
agent might experience because of the loss of his privacy. Here, we survey
several facets of this study, and differential privacy plays a role in more
than one way. Of course, it provides us a basis for modeling agent costs for
privacy, which is essential if we are to attempt mechanism design in a setting
in which agents have preferences for privacy. It also provides a toolkit for
controlling those costs. However, perhaps more surprisingly, it provides a
powerful toolkit for controlling the stability of mechanisms in general, which
yields a set of tools for designing novel mechanisms even in economic settings
completely unrelated to privacy.


Fast Private Data Release Algorithms for Sparse Queries

  We revisit the problem of accurately answering large classes of statistical
queries while preserving differential privacy. Previous approaches to this
problem have either been very general but have not had run-time polynomial in
the size of the database, have applied only to very limited classes of queries,
or have relaxed the notion of worst-case error guarantees. In this paper we
consider the large class of sparse queries, which take non-zero values on only
polynomially many universe elements. We give efficient query release algorithms
for this class, in both the interactive and the non-interactive setting. Our
algorithms also achieve better accuracy bounds than previous general techniques
do when applied to sparse queries: our bounds are independent of the universe
size. In fact, even the runtime of our interactive mechanism is independent of
the universe size, and so can be implemented in the "infinite universe" model
in which no finite universe need be specified by the data curator.


Mitigating Bias in Adaptive Data Gathering via Differential Privacy

  Data that is gathered adaptively --- via bandit algorithms, for example ---
exhibits bias. This is true both when gathering simple numeric valued data ---
the empirical means kept track of by stochastic bandit algorithms are biased
downwards --- and when gathering more complicated data --- running hypothesis
tests on complex data gathered via contextual bandit algorithms leads to false
discovery. In this paper, we show that this problem is mitigated if the data
collection procedure is differentially private. This lets us both bound the
bias of simple numeric valued quantities (like the empirical means of
stochastic bandit algorithms), and correct the p-values of hypothesis tests run
on the adaptively gathered data. Moreover, there exist differentially private
bandit algorithms with near optimal regret bounds: we apply existing theorems
in the simple stochastic case, and give a new analysis for linear contextual
bandits. We complement our theoretical results with experiments validating our
theory.


The Impact of Humanoid Affect Expression on Human Behavior in a
  Game-Theoretic Setting

  With the rapid development of robot and other intelligent and autonomous
agents, how a human could be influenced by a robot's expressed mood when making
decisions becomes a crucial question in human-robot interaction. In this pilot
study, we investigate (1) in what way a robot can express a certain mood to
influence a human's decision making behavioral model; (2) how and to what
extent the human will be influenced in a game theoretic setting. More
specifically, we create an NLP model to generate sentences that adhere to a
specific affective expression profile. We use these sentences for a humanoid
robot as it plays a Stackelberg security game against a human. We investigate
the behavioral model of the human player.


Take it or Leave it: Running a Survey when Privacy Comes at a Cost

  In this paper, we consider the problem of estimating a potentially sensitive
(individually stigmatizing) statistic on a population. In our model,
individuals are concerned about their privacy, and experience some cost as a
function of their privacy loss. Nevertheless, they would be willing to
participate in the survey if they were compensated for their privacy cost.
These cost functions are not publicly known, however, nor do we make Bayesian
assumptions about their form or distribution. Individuals are rational and will
misreport their costs for privacy if doing so is in their best interest. Ghosh
and Roth recently showed in this setting, when costs for privacy loss may be
correlated with private types, if individuals value differential privacy, no
individually rational direct revelation mechanism can compute any non-trivial
estimate of the population statistic. In this paper, we circumvent this
impossibility result by proposing a modified notion of how individuals
experience cost as a function of their privacy loss, and by giving a mechanism
which does not operate by direct revelation. Instead, our mechanism has the
ability to randomly approach individuals from a population and offer them a
take-it-or-leave-it offer. This is intended to model the abilities of a
surveyor who may stand on a street corner and approach passers-by.


Counting matrices over finite fields with support on skew Young diagrams
  and complements of Rothe diagrams

  We consider the problem of finding the number of matrices over a finite field
with a certain rank and with support that avoids a subset of the entries. These
matrices are a q-analogue of permutations with restricted positions (i.e., rook
placements). For general sets of entries these numbers of matrices are not
polynomials in q (Stembridge 98); however, when the set of entries is a Young
diagram, the numbers, up to a power of q-1, are polynomials with nonnegative
coefficients (Haglund 98).
  In this paper, we give a number of conditions under which these numbers are
polynomials in q, or even polynomials with nonnegative integer coefficients. We
extend Haglund's result to complements of skew Young diagrams, and we apply
this result to the case when the set of entries is the Rothe diagram of a
permutation. In particular, we give a necessary and sufficient condition on the
permutation for its Rothe diagram to be the complement of a skew Young diagram
up to rearrangement of rows and columns. We end by giving conjectures
connecting invertible matrices whose support avoids a Rothe diagram and
Poincar\'e polynomials of the strong Bruhat order.


Privately Solving Linear Programs

  In this paper, we initiate the systematic study of solving linear programs
under differential privacy. The first step is simply to define the problem: to
this end, we introduce several natural classes of private linear programs that
capture different ways sensitive data can be incorporated into a linear
program. For each class of linear programs we give an efficient, differentially
private solver based on the multiplicative weights framework, or we give an
impossibility result.


Approximately Stable, School Optimal, and Student-Truthful Many-to-One
  Matchings (via Differential Privacy)

  We present a mechanism for computing asymptotically stable school optimal
matchings, while guaranteeing that it is an asymptotic dominant strategy for
every student to report their true preferences to the mechanism. Our main tool
in this endeavor is differential privacy: we give an algorithm that coordinates
a stable matching using differentially private signals, which lead to our
truthfulness guarantee. This is the first setting in which it is known how to
achieve nontrivial truthfulness guarantees for students when computing school
optimal matchings, assuming worst- case preferences (for schools and students)
in large markets.


Auctions with Online Supply

  We study the problem of selling identical goods to n unit-demand bidders in a
setting in which the total supply of goods is unknown to the mechanism. Items
arrive dynamically, and the seller must make the allocation and payment
decisions online with the goal of maximizing social welfare. We consider two
models of unknown supply: the adversarial supply model, in which the mechanism
must produce a welfare guarantee for any arbitrary supply, and the stochastic
supply model, in which supply is drawn from a distribution known to the
mechanism, and the mechanism need only provide a welfare guarantee in
expectation.
  Our main result is a separation between these two models. We show that all
truthful mechanisms, even randomized, achieve a diminishing fraction of the
optimal social welfare (namely, no better than a Omega(loglog n) approximation)
in the adversarial setting. In sharp contrast, in the stochastic model, under a
standard monotone hazard-rate condition, we present a truthful mechanism that
achieves a constant approximation. We show that the monotone hazard rate
condition is necessary, and also characterize a natural subclass of truthful
mechanisms in our setting, the set of online-envy-free mechanisms. All of the
mechanisms we present fall into this class, and we prove almost optimal lower
bounds for such mechanisms. Since auctions with unknown supply are regularly
run in many online-advertising settings, our main results emphasize the
importance of considering distributional information in the design of auctions
in such environments.


Buying Private Data without Verification

  We consider the problem of designing a survey to aggregate non-verifiable
information from a privacy-sensitive population: an analyst wants to compute
some aggregate statistic from the private bits held by each member of a
population, but cannot verify the correctness of the bits reported by
participants in his survey. Individuals in the population are strategic agents
with a cost for privacy, \ie, they not only account for the payments they
expect to receive from the mechanism, but also their privacy costs from any
information revealed about them by the mechanism's outcome---the computed
statistic as well as the payments---to determine their utilities. How can the
analyst design payments to obtain an accurate estimate of the population
statistic when individuals strategically decide both whether to participate and
whether to truthfully report their sensitive information?
  We design a differentially private peer-prediction mechanism that supports
accurate estimation of the population statistic as a Bayes-Nash equilibrium in
settings where agents have explicit preferences for privacy. The mechanism
requires knowledge of the marginal prior distribution on bits $b_i$, but does
not need full knowledge of the marginal distribution on the costs $c_i$,
instead requiring only an approximate upper bound. Our mechanism guarantees
$\epsilon$-differential privacy to each agent $i$ against any adversary who can
observe the statistical estimate output by the mechanism, as well as the
payments made to the $n-1$ other agents $j\neq i$. Finally, we show that with
slightly more structured assumptions on the privacy cost functions of each
agent, the cost of running the survey goes to $0$ as the number of agents
diverges.


A Learning Theory Approach to Non-Interactive Database Privacy

  In this paper we demonstrate that, ignoring computational constraints, it is
possible to privately release synthetic databases that are useful for large
classes of queries -- much larger in size than the database itself.
Specifically, we give a mechanism that privately releases synthetic data for a
class of queries over a discrete domain with error that grows as a function of
the size of the smallest net approximately representing the answers to that
class of queries. We show that this in particular implies a mechanism for
counting queries that gives error guarantees that grow only with the
VC-dimension of the class of queries, which itself grows only logarithmically
with the size of the query class.
  We also show that it is not possible to privately release even simple classes
of queries (such as intervals and their generalizations) over continuous
domains. Despite this, we give a privacy-preserving polynomial time algorithm
that releases information useful for all halfspace queries, given a slight
relaxation of the utility guarantee. This algorithm does not release synthetic
data, but instead another data structure capable of representing an answer for
each query. We also give an efficient algorithm for releasing synthetic data
for the class of interval queries and axis-aligned rectangles of constant
dimension.
  Finally, inspired by learning theory, we introduce a new notion of data
privacy, which we call distributional privacy, and show that it is strictly
stronger than the prevailing privacy notion, differential privacy.


Downstream Effects of Affirmative Action

  We study a two-stage model, in which students are 1) admitted to college on
the basis of an entrance exam which is a noisy signal about their
qualifications (type), and then 2) those students who were admitted to college
can be hired by an employer as a function of their college grades, which are an
independently drawn noisy signal of their type. Students are drawn from one of
two populations, which might have different type distributions. We assume that
the employer at the end of the pipeline is rational, in the sense that it
computes a posterior distribution on student type conditional on all
information that it has available (college admissions, grades, and group
membership), and makes a decision based on posterior expectation. We then study
what kinds of fairness goals can be achieved by the college by setting its
admissions rule and grading policy. For example, the college might have the
goal of guaranteeing equal opportunity across populations: that the probability
of passing through the pipeline and being hired by the employer should be
independent of group membership, conditioned on type. Alternately, the college
might have the goal of incentivizing the employer to have a group blind hiring
rule. We show that both goals can be achieved when the college does not report
grades. On the other hand, we show that under reasonable conditions, these
goals are impossible to achieve even in isolation when the college uses an
(even minimally) informative grading policy.


How to Use Heuristics for Differential Privacy

  We develop theory for using heuristics to solve computationally hard problems
in differential privacy. Heuristic approaches have enjoyed tremendous success
in machine learning, for which performance can be empirically evaluated.
However, privacy guarantees cannot be evaluated empirically, and must be proven
--- without making heuristic assumptions. We show that learning problems over
broad classes of functions can be solved privately and efficiently, assuming
the existence of a non-private oracle for solving the same problem. Our first
algorithm yields a privacy guarantee that is contingent on the correctness of
the oracle. We then give a reduction which applies to a class of heuristics
which we call certifiable, which allows us to convert oracle-dependent privacy
guarantees to worst-case privacy guarantee that hold even when the heuristic
standing in for the oracle might fail in adversarial ways. Finally, we consider
a broad class of functions that includes most classes of simple boolean
functions studied in the PAC learning literature, including conjunctions,
disjunctions, parities, and discrete halfspaces. We show that there is an
efficient algorithm for privately constructing synthetic data for any such
class, given a non-private learning oracle. This in particular gives the first
oracle-efficient algorithm for privately generating synthetic data for
contingency tables. The most intriguing question left open by our work is
whether or not every problem that can be solved differentially privately can be
privately solved with an oracle-efficient algorithm. While we do not resolve
this, we give a barrier result that suggests that any generic oracle-efficient
reduction must fall outside of a natural class of algorithms (which includes
the algorithms given in this paper).


Selling Privacy at Auction

  We initiate the study of markets for private data, though the lens of
differential privacy. Although the purchase and sale of private data has
already begun on a large scale, a theory of privacy as a commodity is missing.
In this paper, we propose to build such a theory. Specifically, we consider a
setting in which a data analyst wishes to buy information from a population
from which he can estimate some statistic. The analyst wishes to obtain an
accurate estimate cheaply. On the other hand, the owners of the private data
experience some cost for their loss of privacy, and must be compensated for
this loss. Agents are selfish, and wish to maximize their profit, so our goal
is to design truthful mechanisms. Our main result is that such auctions can
naturally be viewed and optimally solved as variants of multi-unit procurement
auctions. Based on this result, we derive auctions for two natural settings
which are optimal up to small constant factors:
  1. In the setting in which the data analyst has a fixed accuracy goal, we
show that an application of the classic Vickrey auction achieves the analyst's
accuracy goal while minimizing his total payment.
  2. In the setting in which the data analyst has a fixed budget, we give a
mechanism which maximizes the accuracy of the resulting estimate while
guaranteeing that the resulting sum payments do not exceed the analysts budget.
  In both cases, our comparison class is the set of envy-free mechanisms, which
correspond to the natural class of fixed-price mechanisms in our setting.
  In both of these results, we ignore the privacy cost due to possible
correlations between an individuals private data and his valuation for privacy
itself. We then show that generically, no individually rational mechanism can
compensate individuals for the privacy loss incurred due to their reported
valuations for privacy.


Iterative Constructions and Private Data Release

  In this paper we study the problem of approximately releasing the cut
function of a graph while preserving differential privacy, and give new
algorithms (and new analyses of existing algorithms) in both the interactive
and non-interactive settings.
  Our algorithms in the interactive setting are achieved by revisiting the
problem of releasing differentially private, approximate answers to a large
number of queries on a database. We show that several algorithms for this
problem fall into the same basic framework, and are based on the existence of
objects which we call iterative database construction algorithms. We give a new
generic framework in which new (efficient) IDC algorithms give rise to new
(efficient) interactive private query release mechanisms. Our modular analysis
simplifies and tightens the analysis of previous algorithms, leading to
improved bounds. We then give a new IDC algorithm (and therefore a new private,
interactive query release mechanism) based on the Frieze/Kannan low-rank matrix
decomposition. This new release mechanism gives an improvement on prior work in
a range of parameters where the size of the database is comparable to the size
of the data universe (such as releasing all cut queries on dense graphs).
  We also give a non-interactive algorithm for efficiently releasing private
synthetic data for graph cuts with error O(|V|^{1.5}). Our algorithm is based
on randomized response and a non-private implementation of the SDP-based,
constant-factor approximation algorithm for cut-norm due to Alon and Naor.
Finally, we give a reduction based on the IDC framework showing that an
efficient, private algorithm for computing sufficiently accurate rank-1 matrix
approximations would lead to an improved efficient algorithm for releasing
private synthetic data for graph cuts. We leave finding such an algorithm as
our main open problem.


Constrained Non-Monotone Submodular Maximization: Offline and Secretary
  Algorithms

  Constrained submodular maximization problems have long been studied, with
near-optimal results known under a variety of constraints when the submodular
function is monotone. The case of non-monotone submodular maximization is less
understood: the first approximation algorithms even for the unconstrainted
setting were given by Feige et al. (FOCS '07). More recently, Lee et al. (STOC
'09, APPROX '09) show how to approximately maximize non-monotone submodular
functions when the constraints are given by the intersection of p matroid
constraints; their algorithm is based on local-search procedures that consider
p-swaps, and hence the running time may be n^Omega(p), implying their algorithm
is polynomial-time only for constantly many matroids. In this paper, we give
algorithms that work for p-independence systems (which generalize constraints
given by the intersection of p matroids), where the running time is poly(n,p).
Our algorithm essentially reduces the non-monotone maximization problem to
multiple runs of the greedy algorithm previously used in the monotone case.
  Our idea of using existing algorithms for monotone functions to solve the
non-monotone case also works for maximizing a submodular function with respect
to a knapsack constraint: we get a simple greedy-based constant-factor
approximation for this problem.
  With these simpler algorithms, we are able to adapt our approach to
constrained non-monotone submodular maximization to the (online) secretary
setting, where elements arrive one at a time in random order, and the algorithm
must make irrevocable decisions about whether or not to select each element as
it arrives. We give constant approximations in this secretary setting when the
algorithm is constrained subject to a uniform matroid or a partition matroid,
and give an O(log k) approximation when it is constrained by a general matroid
of rank k.


Mechanism Design in Large Games: Incentives and Privacy

  We study the problem of implementing equilibria of complete information games
in settings of incomplete information, and address this problem using
"recommender mechanisms." A recommender mechanism is one that does not have the
power to enforce outcomes or to force participation, rather it only has the
power to suggestion outcomes on the basis of voluntary participation. We show
that despite these restrictions, recommender mechanisms can implement
equilibria of complete information games in settings of incomplete information
under the condition that the game is large---i.e. that there are a large number
of players, and any player's action affects any other's payoff by at most a
small amount.
  Our result follows from a novel application of differential privacy. We show
that any algorithm that computes a correlated equilibrium of a complete
information game while satisfying a variant of differential privacy---which we
call joint differential privacy---can be used as a recommender mechanism while
satisfying our desired incentive properties. Our main technical result is an
algorithm for computing a correlated equilibrium of a large game while
satisfying joint differential privacy.
  Although our recommender mechanisms are designed to satisfy game-theoretic
properties, our solution ends up satisfying a strong privacy property as well.
No group of players can learn "much" about the type of any player outside the
group from the recommendations of the mechanism, even if these players collude
in an arbitrary way. As such, our algorithm is able to implement equilibria of
complete information games, without revealing information about the realized
types.


Asymptotically Truthful Equilibrium Selection in Large Congestion Games

  Studying games in the complete information model makes them analytically
tractable. However, large $n$ player interactions are more realistically
modeled as games of incomplete information, where players may know little to
nothing about the types of other players. Unfortunately, games in incomplete
information settings lose many of the nice properties of complete information
games: the quality of equilibria can become worse, the equilibria lose their
ex-post properties, and coordinating on an equilibrium becomes even more
difficult. Because of these problems, we would like to study games of
incomplete information, but still implement equilibria of the complete
information game induced by the (unknown) realized player types.
  This problem was recently studied by Kearns et al. and solved in large games
by means of introducing a weak mediator: their mediator took as input reported
types of players, and output suggested actions which formed a correlated
equilibrium of the underlying game. Players had the option to play
independently of the mediator, or ignore its suggestions, but crucially, if
they decided to opt-in to the mediator, they did not have the power to lie
about their type. In this paper, we rectify this deficiency in the setting of
large congestion games. We give, in a sense, the weakest possible mediator: it
cannot enforce participation, verify types, or enforce its suggestions.
Moreover, our mediator implements a Nash equilibrium of the complete
information game. We show that it is an (asymptotic) ex-post equilibrium of the
incomplete information game for all players to use the mediator honestly, and
that when they do so, they end up playing an approximate Nash equilibrium of
the induced complete information game. In particular, truthful use of the
mediator is a Bayes-Nash equilibrium in any Bayesian game for any prior.


Distributed Private Heavy Hitters

  In this paper, we give efficient algorithms and lower bounds for solving the
heavy hitters problem while preserving differential privacy in the fully
distributed local model. In this model, there are n parties, each of which
possesses a single element from a universe of size N. The heavy hitters problem
is to find the identity of the most common element shared amongst the n
parties. In the local model, there is no trusted database administrator, and so
the algorithm must interact with each of the $n$ parties separately, using a
differentially private protocol. We give tight information-theoretic upper and
lower bounds on the accuracy to which this problem can be solved in the local
model (giving a separation between the local model and the more common
centralized model of privacy), as well as computationally efficient algorithms
even in the case where the data universe N may be exponentially large.


Differential Privacy for the Analyst via Private Equilibrium Computation

  We give new mechanisms for answering exponentially many queries from multiple
analysts on a private database, while protecting differential privacy both for
the individuals in the database and for the analysts. That is, our mechanism's
answer to each query is nearly insensitive to changes in the queries asked by
other analysts. Our mechanism is the first to offer differential privacy on the
joint distribution over analysts' answers, providing privacy for data analysts
even if the other data analysts collude or register multiple accounts. In some
settings, we are able to achieve nearly optimal error rates (even compared to
mechanisms which do not offer analyst privacy), and we are able to extend our
techniques to handle non-linear queries. Our analysis is based on a novel view
of the private query-release problem as a two-player zero-sum game, which may
be of independent interest.


Beyond Worst-Case Analysis in Private Singular Vector Computation

  We consider differentially private approximate singular vector computation.
Known worst-case lower bounds show that the error of any differentially private
algorithm must scale polynomially with the dimension of the singular vector. We
are able to replace this dependence on the dimension by a natural parameter
known as the coherence of the matrix that is often observed to be significantly
smaller than the dimension both theoretically and empirically. We also prove a
matching lower bound showing that our guarantee is nearly optimal for every
setting of the coherence parameter. Notably, we achieve our bounds by giving a
robust analysis of the well-known power iteration algorithm, which may be of
independent interest. Our algorithm also leads to improvements in worst-case
settings and to better low-rank approximations in the spectral norm.


Beating Randomized Response on Incoherent Matrices

  Computing accurate low rank approximations of large matrices is a fundamental
data mining task. In many applications however the matrix contains sensitive
information about individuals. In such case we would like to release a low rank
approximation that satisfies a strong privacy guarantee such as differential
privacy. Unfortunately, to date the best known algorithm for this task that
satisfies differential privacy is based on naive input perturbation or
randomized response: Each entry of the matrix is perturbed independently by a
sufficiently large random noise variable, a low rank approximation is then
computed on the resulting matrix.
  We give (the first) significant improvements in accuracy over randomized
response under the natural and necessary assumption that the matrix has low
coherence. Our algorithm is also very efficient and finds a constant rank
approximation of an m x n matrix in time O(mn). Note that even generating the
noise matrix required for randomized response already requires time O(mn).


Robust Mediators in Large Games

  A mediator is a mechanism that can only suggest actions to players, as a
function of all agents' reported types, in a given game of incomplete
information. We study what is achievable by two kinds of mediators, "strong"
and "weak." Players can choose to opt-out of using a strong mediator but cannot
misrepresent their type if they opt-in. Such a mediator is "strong" because we
can view it as having the ability to verify player types. Weak mediators lack
this ability--- players are free to misrepresent their type to a weak mediator.
We show a striking result---in a prior-free setting, assuming only that the
game is large and players have private types, strong mediators can implement
approximate equilibria of the complete-information game. If the game is a
congestion game, then the same result holds using only weak mediators. Our
result follows from a novel application of differential privacy, in particular,
a variant we propose called joint differential privacy.


Dual Query: Practical Private Query Release for High Dimensional Data

  We present a practical, differentially private algorithm for answering a
large number of queries on high dimensional datasets. Like all algorithms for
this task, ours necessarily has worst-case complexity exponential in the
dimension of the data. However, our algorithm packages the computationally hard
step into a concisely defined integer program, which can be solved
non-privately using standard solvers. We prove accuracy and privacy theorems
for our algorithm, and then demonstrate experimentally that our algorithm
performs well in practice. For example, our algorithm can efficiently and
accurately answer millions of queries on the Netflix dataset, which has over
17,000 attributes; this is an improvement on the state of the art by multiple
orders of magnitude.


An Anti-Folk Theorem for Large Repeated Games with Imperfect Monitoring

  We study infinitely repeated games in settings of imperfect monitoring. We
first prove a family of theorems that show that when the signals observed by
the players satisfy a condition known as $(\epsilon, \gamma)$-differential
privacy, that the folk theorem has little bite: for values of $\epsilon$ and
$\gamma$ sufficiently small, for a fixed discount factor, any equilibrium of
the repeated game involve players playing approximate equilibria of the stage
game in every period. Next, we argue that in large games ($n$ player games in
which unilateral deviations by single players have only a small impact on the
utility of other players), many monitoring settings naturally lead to signals
that satisfy $(\epsilon,\gamma)$-differential privacy, for $\epsilon$ and
$\gamma$ tending to zero as the number of players $n$ grows large. We conclude
that in such settings, the set of equilibria of the repeated game collapse to
the set of equilibria of the stage game.


Online Learning and Profit Maximization from Revealed Preferences

  We consider the problem of learning from revealed preferences in an online
setting. In our framework, each period a consumer buys an optimal bundle of
goods from a merchant according to her (linear) utility function and current
prices, subject to a budget constraint. The merchant observes only the
purchased goods, and seeks to adapt prices to optimize his profits. We give an
efficient algorithm for the merchant's problem that consists of a learning
phase in which the consumer's utility function is (perhaps partially) inferred,
followed by a price optimization step. We also consider an alternative online
learning algorithm for the setting where prices are set exogenously, but the
merchant would still like to predict the bundle that will be bought by the
consumer for purposes of inventory or supply chain management. In contrast with
most prior work on the revealed preferences problem, we demonstrate that by
making stronger assumptions on the form of utility functions, efficient
algorithms for both learning and profit maximization are possible, even in
adaptive, online settings.


Jointly Private Convex Programming

  In this paper we present an extremely general method for approximately
solving a large family of convex programs where the solution can be divided
between different agents, subject to joint differential privacy. This class
includes multi-commodity flow problems, general allocation problems, and
multi-dimensional knapsack problems, among other examples. The accuracy of our
algorithm depends on the \emph{number} of constraints that bind between
individuals, but crucially, is \emph{nearly independent} of the number of
primal variables and hence the number of agents who make up the problem. As the
number of agents in a problem grows, the error we introduce often becomes
negligible.
  We also consider the setting where agents are strategic and have preferences
over their part of the solution. For any convex program in this class that
maximizes \emph{social welfare}, there is a generic reduction that makes the
corresponding optimization \emph{approximately dominant strategy truthful} by
charging agents prices for resources as a function of the approximately optimal
dual variables, which are themselves computed under differential privacy. Our
results substantially expand the class of problems that are known to be
solvable under both privacy and incentive constraints.


Inducing Approximately Optimal Flow Using Truthful Mediators

  We revisit a classic coordination problem from the perspective of mechanism
design: how can we coordinate a social welfare maximizing flow in a network
congestion game with selfish players? The classical approach, which computes
tolls as a function of known demands, fails when the demands are unknown to the
mechanism designer, and naively eliciting them does not necessarily yield a
truthful mechanism. Instead, we introduce a weak mediator that can provide
suggested routes to players and set tolls as a function of reported demands.
However, players can choose to ignore or misreport their type to this mediator.
Using techniques from differential privacy, we show how to design a weak
mediator such that it is an asymptotic ex-post Nash equilibrium for all players
to truthfully report their types to the mediator and faithfully follow its
suggestion, and that when they do, they end up playing a nearly optimal flow.
Notably, our solution works in settings of incomplete information even in the
absence of a prior distribution on player types. Along the way, we develop new
techniques for privately solving convex programs which may be of independent
interest.


Privacy for the Protected (Only)

  Motivated by tensions between data privacy for individual citizens, and
societal priorities such as counterterrorism and the containment of infectious
disease, we introduce a computational model that distinguishes between parties
for whom privacy is explicitly protected, and those for whom it is not (the
targeted subpopulation). The goal is the development of algorithms that can
effectively identify and take action upon members of the targeted subpopulation
in a way that minimally compromises the privacy of the protected, while
simultaneously limiting the expense of distinguishing members of the two groups
via costly mechanisms such as surveillance, background checks, or medical
testing. Within this framework, we provide provably privacy-preserving
algorithms for targeted search in social networks. These algorithms are natural
variants of common graph search methods, and ensure privacy for the protected
by the careful injection of noise in the prioritization of potential targets.
We validate the utility of our algorithms with extensive computational
experiments on two large-scale social network datasets.


Fair Algorithms for Infinite and Contextual Bandits

  We study fairness in linear bandit problems. Starting from the notion of
meritocratic fairness introduced in Joseph et al. [2016], we carry out a more
refined analysis of a more general problem, achieving better performance
guarantees with fewer modelling assumptions on the number and structure of
available choices as well as the number selected. We also analyze the
previously-unstudied question of fairness in infinite linear bandit problems,
obtaining instance-dependent regret upper bounds as well as lower bounds
demonstrating that this instance-dependence is necessary. The result is a
framework for meritocratic fairness in an online linear setting that is
substantially more powerful, general, and realistic than the current state of
the art.


Fairness in Reinforcement Learning

  We initiate the study of fairness in reinforcement learning, where the
actions of a learning algorithm may affect its environment and future rewards.
Our fairness constraint requires that an algorithm never prefers one action
over another if the long-term (discounted) reward of choosing the latter action
is higher. Our first result is negative: despite the fact that fairness is
consistent with the optimal policy, any learning algorithm satisfying fairness
must take time exponential in the number of states to achieve non-trivial
approximation to the optimal policy. We then provide a provably fair polynomial
time algorithm under an approximate notion of fairness, thus establishing an
exponential gap between exact and approximate fairness


Fairness in Criminal Justice Risk Assessments: The State of the Art

  Objectives: Discussions of fairness in criminal justice risk assessments
typically lack conceptual precision. Rhetoric too often substitutes for careful
analysis. In this paper, we seek to clarify the tradeoffs between different
kinds of fairness and between fairness and accuracy.
  Methods: We draw on the existing literatures in criminology, computer science
and statistics to provide an integrated examination of fairness and accuracy in
criminal justice risk assessments. We also provide an empirical illustration
using data from arraignments.
  Results: We show that there are at least six kinds of fairness, some of which
are incompatible with one another and with accuracy.
  Conclusions: Except in trivial cases, it is impossible to maximize accuracy
and fairness at the same time, and impossible simultaneously to satisfy all
kinds of fairness. In practice, a major complication is different base rates
across different legally protected groups. There is a need to consider
challenging tradeoffs.


A Convex Framework for Fair Regression

  We introduce a flexible family of fairness regularizers for (linear and
logistic) regression problems. These regularizers all enjoy convexity,
permitting fast optimization, and they span the rang from notions of group
fairness to strong individual fairness. By varying the weight on the fairness
regularizer, we can compute the efficient frontier of the accuracy-fairness
trade-off on any given dataset, and we measure the severity of this trade-off
via a numerical quantity we call the Price of Fairness (PoF). The centerpiece
of our results is an extensive comparative study of the PoF across six
different datasets in which fairness is a primary consideration.


Strategic Classification from Revealed Preferences

  We study an online linear classification problem, in which the data is
generated by strategic agents who manipulate their features in an effort to
change the classification outcome. In rounds, the learner deploys a classifier,
and an adversarially chosen agent arrives, possibly manipulating her features
to optimally respond to the learner. The learner has no knowledge of the
agents' utility functions or "real" features, which may vary widely across
agents. Instead, the learner is only able to observe their "revealed
preferences" --- i.e. the actual manipulated feature vectors they provide. For
a broad family of agent cost functions, we give a computationally efficient
learning algorithm that is able to obtain diminishing "Stackelberg regret" ---
a form of policy regret that guarantees that the learner is obtaining loss
nearly as small as that of the best classifier in hindsight, even allowing for
the fact that agents will best-respond differently to the optimal classifier.


A Smoothed Analysis of the Greedy Algorithm for the Linear Contextual
  Bandit Problem

  Bandit learning is characterized by the tension between long-term exploration
and short-term exploitation. However, as has recently been noted, in settings
in which the choices of the learning algorithm correspond to important
decisions about individual people (such as criminal recidivism prediction,
lending, and sequential drug trials), exploration corresponds to explicitly
sacrificing the well-being of one individual for the potential future benefit
of others. This raises a fairness concern. In such settings, one might like to
run a "greedy" algorithm, which always makes the (myopically) optimal decision
for the individuals at hand - but doing this can result in a catastrophic
failure to learn. In this paper, we consider the linear contextual bandit
problem and revisit the performance of the greedy algorithm. We give a smoothed
analysis, showing that even when contexts may be chosen by an adversary, small
perturbations of the adversary's choices suffice for the algorithm to achieve
"no regret", perhaps (depending on the specifics of the setting) with a
constant amount of initial training data. This suggests that "generically"
(i.e. in slightly perturbed environments), exploration and exploitation need
not be in conflict in the linear setting.


Online Learning with an Unknown Fairness Metric

  We consider the problem of online learning in the linear contextual bandits
setting, but in which there are also strong individual fairness constraints
governed by an unknown similarity metric. These constraints demand that we
select similar actions or individuals with approximately equal probability
(arXiv:1104.3913), which may be at odds with optimizing reward, thus modeling
settings where profit and social policy are in tension. We assume we learn
about an unknown Mahalanobis similarity metric from only weak feedback that
identifies fairness violations, but does not quantify their extent. This is
intended to represent the interventions of a regulator who "knows unfairness
when he sees it" but nevertheless cannot enunciate a quantitative fairness
metric over individuals. Our main result is an algorithm in the adversarial
context setting that has a number of fairness violations that depends only
logarithmically on $T$, while obtaining an optimal $O(\sqrt{T})$ regret bound
to the best fair policy.


Local Differential Privacy for Evolving Data

  There are now several large scale deployments of differential privacy used to
collect statistical information about users. However, these deployments
periodically recollect the data and recompute the statistics using algorithms
designed for a single use. As a result, these systems do not provide meaningful
privacy guarantees over long time scales. Moreover, existing techniques to
mitigate this effect do not apply in the "local model" of differential privacy
that these systems use.
  In this paper, we introduce a new technique for local differential privacy
that makes it possible to maintain up-to-date statistics over time, with
privacy guarantees that degrade only in the number of changes in the underlying
distribution rather than the number of collection periods. We use our technique
for tracking a changing statistic in the setting where users are partitioned
into an unknown collection of groups, and at every time period each user draws
a single bit from a common (but changing) group-specific distribution. We also
provide an application to frequency and heavy-hitter estimation.


Equal Opportunity in Online Classification with Partial Feedback

  We study an online classification problem with partial feedback in which
individuals arrive one at a time from a fixed but unknown distribution, and
must be classified as positive or negative. Our algorithm only observes the
true label of an individual if they are given a positive classification. This
setting captures many classification problems for which fairness is a concern:
for example, in criminal recidivism prediction, recidivism is only observed if
the inmate is released; in lending applications, loan repayment is only
observed if the loan is granted. We require that our algorithms satisfy common
statistical fairness constraints (such as equalizing false positive or negative
rates --- introduced as "equal opportunity" in Hardt et al. (2016)) at every
round, with respect to the underlying distribution. We give upper and lower
bounds characterizing the cost of this constraint in terms of the regret rate
(and show that it is mild), and give an oracle efficient algorithm that
achieves the upper bound.


Differentially Private Combinatorial Optimization

  Consider the following problem: given a metric space, some of whose points
are "clients", open a set of at most $k$ facilities to minimize the average
distance from the clients to these facilities. This is just the well-studied
$k$-median problem, for which many approximation algorithms and hardness
results are known. Note that the objective function encourages opening
facilities in areas where there are many clients, and given a solution, it is
often possible to get a good idea of where the clients are located. However,
this poses the following quandary: what if the identity of the clients is
sensitive information that we would like to keep private? Is it even possible
to design good algorithms for this problem that preserve the privacy of the
clients?
  In this paper, we initiate a systematic study of algorithms for discrete
optimization problems in the framework of differential privacy (which
formalizes the idea of protecting the privacy of individual input elements). We
show that many such problems indeed have good approximation algorithms that
preserve differential privacy; this is even in cases where it is impossible to
preserve cryptographic definitions of privacy while computing any non-trivial
approximation to even the_value_ of an optimal solution, let alone the entire
solution.
  Apart from the $k$-median problem, we study the problems of vertex and set
cover, min-cut, facility location, Steiner tree, and the recently introduced
submodular maximization problem, "Combinatorial Public Projects" (CPP).


Characterisation of deuterium spectra from laser driven multi-species
  sources by employing differentially filtered image plate detectors in Thomson
  spectrometers

  A novel method for characterising the full spectrum of deuteron ions emitted
by laser driven multi-species ion sources is discussed. The procedure is based
on using differential filtering over the detector of a Thompson parabola ion
spectrometer, which enables discrimination of deuterium ions from heavier ion
species with the same charge-to-mass ratio (such as C6+, O8+, etc.). Commonly
used Fuji Image plates were used as detectors in the spectrometer, whose
absolute response to deuterium ions over a wide range of energies was
calibrated by using slotted CR-39 nuclear track detectors. A typical deuterium
ion spectrum diagnosed in a recent experimental campaign is presented.


