Use of Weighted Finite State Transducers in Part of Speech Tagging

  This paper addresses issues in part of speech disambiguation using
finite-state transducers and presents two main contributions to the field. One
of them is the use of finite-state machines for part of speech tagging.
Linguistic and statistical information is represented in terms of weights on
transitions in weighted finite-state transducers. Another contribution is the
successful combination of techniques -- linguistic and statistical -- for word
disambiguation, compounded with the notion of word classes.


Centroid-based summarization of multiple documents: sentence extraction,
  utility-based evaluation, and user studies

  We present a multi-document summarizer, called MEAD, which generates
summaries using cluster centroids produced by a topic detection and tracking
system. We also describe two new techniques, based on sentence utility and
subsumption, which we have applied to the evaluation of both single and
multiple document summaries. Finally, we describe two user studies that test
our models of multi-document summarization.


Ranking suspected answers to natural language questions using predictive
  annotation

  In this paper, we describe a system to rank suspected answers to natural
language questions. We process both corpus and query using a new technique,
predictive annotation, which augments phrases in texts with labels anticipating
their being targets of certain kinds of questions. Given a natural language
question, an IR system returns a set of matching passages, which are then
analyzed and ranked according to various criteria described in this paper. We
provide an evaluation of the techniques based on results from the TREC Q&A
evaluation in which our system participated.


Building a Generation Knowledge Source using Internet-Accessible
  Newswire

  In this paper, we describe a method for automatic creation of a knowledge
source for text generation using information extraction over the Internet. We
present a prototype system called PROFILE which uses a client-server
architecture to extract noun-phrase descriptions of entities such as people,
places, and organizations. The system serves two purposes: as an information
extraction tool, it allows users to search for textual descriptions of
entities; as a utility to generate functional descriptions (FD), it is used in
a functional-unification based generation system. We present an evaluation of
the approach and its applications to natural language generation and
summarization.


Tagging French Without Lexical Probabilities -- Combining Linguistic
  Knowledge And Statistical Learning

  This paper explores morpho-syntactic ambiguities for French to develop a
strategy for part-of-speech disambiguation that a) reflects the complexity of
French as an inflected language, b) optimizes the estimation of probabilities,
c) allows the user flexibility in choosing a tagset. The problem in extracting
lexical probabilities from a limited training corpus is that the statistical
model may not necessarily represent the use of a particular word in a
particular context. In a highly morphologically inflected language, this
argument is particularly serious since a word can be tagged with a large number
of parts of speech. Due to the lack of sufficient training data, we argue
against estimating lexical probabilities to disambiguate parts of speech in
unrestricted texts. Instead, we use the strength of contextual probabilities
along with a feature we call ``genotype'', a set of tags associated with a
word. Using this knowledge, we have built a part-of-speech tagger that combines
linguistic and statistical approaches: contextual information is disambiguated
by linguistic rules and n-gram probabilities on parts of speech only are
estimated in order to disambiguate the remaining ambiguous tags.


Learning Correlations between Linguistic Indicators and Semantic
  Constraints: Reuse of Context-Dependent Descriptions of Entities

  This paper presents the results of a study on the semantic constraints
imposed on lexical choice by certain contextual indicators. We show how such
indicators are computed and how correlations between them and the choice of a
noun phrase description of a named entity can be automatically established
using supervised learning. Based on this correlation, we have developed a
technique for automatic lexical choice of descriptions of entities in text
generation. We discuss the underlying relationship between the pragmatics of
choosing an appropriate description that serves a specific purpose in the
automatically generated text and the semantics of the description itself. We
present our work in the framework of the more general concept of reuse of
linguistic structures that are automatically extracted from large corpora. We
present a formal evaluation of our approach and we conclude with some thoughts
on potential applications of our method.


Scientific Paper Summarization Using Citation Summary Networks

  Quickly moving to a new area of research is painful for researchers due to
the vast amount of scientific literature in each field of study. One possible
way to overcome this problem is to summarize a scientific topic. In this paper,
we propose a model of summarizing a single article, which can be further used
to summarize an entire topic. Our model is based on analyzing others' viewpoint
of the target article's contributions and the study of its citation summary
network using a clustering approach.


What Should I Learn First: Introducing LectureBank for NLP Education and
  Prerequisite Chain Learning

  Recent years have witnessed the rising popularity of Natural Language
Processing (NLP) and related fields such as Artificial Intelligence (AI) and
Machine Learning (ML). Many online courses and resources are available even for
those without a strong background in the field. Often the student is curious
about a specific topic but does not quite know where to begin studying. To
answer the question of "what should one learn first," we apply an
embedding-based method to learn prerequisite relations for course concepts in
the domain of NLP. We introduce LectureBank, a dataset containing 1,352 English
lecture files collected from university courses which are each classified
according to an existing taxonomy as well as 208 manually-labeled prerequisite
relation topics, which is publicly available. The dataset will be useful for
educational purposes such as lecture preparation and organization as well as
applications such as reading list generation. Additionally, we experiment with
neural graph-based networks and non-neural classifiers to learn these
prerequisite relations from our dataset.


A Computational Analysis of Collective Discourse

  This paper is focused on the computational analysis of collective discourse,
a collective behavior seen in non-expert content contributions in online social
media. We collect and analyze a wide range of real-world collective discourse
datasets from movie user reviews to microblogs and news headlines to scientific
citations. We show that all these datasets exhibit diversity of perspective, a
property seen in other collective systems and a criterion in wise crowds. Our
experiments also confirm that the network of different perspective
co-occurrences exhibits the small-world property with high clustering of
different perspectives. Finally, we show that non-expert contributions in
collective discourse can be used to answer simple questions that are otherwise
hard to answer.


Generating Extractive Summaries of Scientific Paradigms

  Researchers and scientists increasingly find themselves in the position of
having to quickly understand large amounts of technical material. Our goal is
to effectively serve this need by using bibliometric text mining and
summarization techniques to generate summaries of scientific literature. We
show how we can use citations to produce automatically generated, readily
consumable, technical extractive summaries. We first propose C-LexRank, a model
for summarizing single scientific articles based on citations, which employs
community detection and extracts salient information-rich sentences. Next, we
further extend our experiments to summarize a set of papers, which cover the
same scientific topic. We generate extractive summaries of a set of Question
Answering (QA) and Dependency Parsing (DP) papers, their abstracts, and their
citation sentences and show that citations have unique information amenable to
creating a summary.


Zero-shot Transfer Learning for Semantic Parsing

  While neural networks have shown impressive performance on large datasets,
applying these models to tasks where little data is available remains a
challenging problem.
  In this paper we propose to use feature transfer in a zero-shot experimental
setting on the task of semantic parsing.
  We first introduce a new method for learning the shared space between
multiple domains based on the prediction of the domain label for each example.
  Our experiments support the superiority of this method in a zero-shot
experimental setting in terms of accuracy metrics compared to state-of-the-art
techniques.
  In the second part of this paper we study the impact of individual domains
and examples on semantic parsing performance.
  We use influence functions to this aim and investigate the sensitivity of
domain-label classification loss on each example.
  Our findings reveal that cross-domain adversarial attacks identify useful
examples for training even from the domains the least similar to the target
domain. Augmenting our training data with these influential examples further
boosts our accuracy at both the token and the sequence level.


TutorialBank: A Manually-Collected Corpus for Prerequisite Chains,
  Survey Extraction and Resource Recommendation

  The field of Natural Language Processing (NLP) is growing rapidly, with new
research published daily along with an abundance of tutorials, codebases and
other online resources. In order to learn this dynamic field or stay up-to-date
on the latest research, students as well as educators and researchers must
constantly sift through multiple sources to find valuable, relevant
information. To address this situation, we introduce TutorialBank, a new,
publicly available dataset which aims to facilitate NLP education and research.
We have manually collected and categorized over 6,300 resources on NLP as well
as the related fields of Artificial Intelligence (AI), Machine Learning (ML)
and Information Retrieval (IR). Our dataset is notably the largest
manually-picked corpus of resources intended for NLP education which does not
include only academic papers. Additionally, we have created both a search
engine and a command-line tool for the resources and have annotated the corpus
to include lists of research topics, relevant resources for each topic,
prerequisite relations among topics, relevant sub-parts of individual
resources, among other annotations. We are releasing the dataset and present
several avenues for further research.


LexRank: Graph-based Lexical Centrality as Salience in Text
  Summarization

  We introduce a stochastic graph-based method for computing relative
importance of textual units for Natural Language Processing. We test the
technique on the problem of Text Summarization (TS). Extractive TS relies on
the concept of sentence salience to identify the most important sentences in a
document or set of documents. Salience is typically defined in terms of the
presence of particular important words or in terms of similarity to a centroid
pseudo-sentence. We consider a new approach, LexRank, for computing sentence
importance based on the concept of eigenvector centrality in a graph
representation of sentences. In this model, a connectivity matrix based on
intra-sentence cosine similarity is used as the adjacency matrix of the graph
representation of sentences. Our system, based on LexRank ranked in first place
in more than one task in the recent DUC 2004 evaluation. In this paper we
present a detailed analysis of our approach and apply it to a larger data set
including data from earlier DUC evaluations. We discuss several methods to
compute centrality using the similarity graph. The results show that
degree-based methods (including LexRank) outperform both centroid-based methods
and other systems participating in DUC in most of the cases. Furthermore, the
LexRank with threshold method outperforms the other degree-based techniques
including continuous LexRank. We also show that our approach is quite
insensitive to the noise in the data that may result from an imperfect topical
clustering of documents.


