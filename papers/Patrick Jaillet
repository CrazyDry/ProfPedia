Near-Optimal Online Algorithms for Dynamic Resource Allocation Problems

  In this paper, we study a general online linear programming problem whose
formulation encompasses many practical dynamic resource allocation problems,
including internet advertising display applications, revenue management,
various routing, packing, and auction problems. We propose a model, which under
mild assumptions, allows us to design near-optimal learning-based online
algorithms that do not require the a priori knowledge about the total number of
online requests to come, a first of its kind. We then consider two variants of
the problem that relax the initial assumptions imposed on the proposed model.


Log-Quadratic Bounds for the Gaussian Q-function

  We present bounds of quadratic form for the logarithm of the Gaussian
Q-function. We also show an analytical method for deriving log-quadratic
approximations of the Q-function and give an approximation with absolute error
less than $10^{-3}$.


Exponential Convergence Rates for Stochastically Ordered Markov
  Processes with Random Initial Conditions

  In this brief paper we find computable exponential convergence rates for a
large class of stochastically ordered Markov processes. We extend the result of
Lund, Meyn, and Tweedie (1996), who found exponential convergence rates for
stochastically ordered Markov processes starting from a fixed initial state, by
allowing for a random initial condition that is also stochastically ordered.
Our bounds are formulated in terms of moment-generating functions of hitting
times. To illustrate our result, we find an explicit exponential convergence
rate for an M/M/1 queue beginning in equilibrium and then experiencing a change
in its arrival or departure rates, a setting which has not been studied to our
knowledge.


Digital breadcrumbs: Detecting urban mobility patterns and transport
  mode choices from cellphone networks

  Many modern and growing cities are facing declines in public transport usage,
with few efficient methods to explain why. In this article, we show that urban
mobility patterns and transport mode choices can be derived from cellphone call
detail records coupled with public transport data recorded from smart cards.
Specifically, we present new data mining approaches to determine the spatial
and temporal variability of public and private transportation usage and
transport mode preferences across Singapore. Our results, which were validated
by Singapore's quadriennial Household Interview Travel Survey (HITS), revealed
that there are 3.5 (HITS: 3.5 million) million and 4.3 (HITS: 4.4 million)
million inter-district passengers by public and private transport,
respectively. Along with classifying which transportation connections are weak
or underserved, the analysis shows that the mode share of public transport use
increases from 38 percent in the morning to 44 percent around mid-day and 52
percent in the evening.


Robust Adaptive Routing Under Uncertainty

  We consider the problem of finding an optimal history-dependent routing
strategy on a directed graph weighted by stochastic arc costs when the
objective is to minimize the risk of spending more than a prescribed budget. To
help mitigate the impact of the lack of information on the arc cost probability
distributions, we introduce a robust counterpart where the distributions are
only known through confidence intervals on some statistics such as the mean,
the mean absolute deviation, and any quantile. Leveraging recent results in
distributionally robust optimization, we develop a general-purpose algorithm to
compute an approximate optimal strategy. To illustrate the benefits of the
robust approach, we run numerical experiments with field data from the
Singapore road network.


Kidney Exchange in Dynamic Sparse Heterogenous Pools

  Current kidney exchange pools are of moderate size and thin, as they consist
of many highly sensitized patients. Creating a thicker pool can be done by
waiting for many pairs to arrive. We analyze a simple class of matching
algorithms that search periodically for allocations. We find that if only 2-way
cycles are conducted, in order to gain a significant amount of matches over the
online scenario (matching each time a new incompatible pair joins the pool) the
waiting period should be "very long". If 3-way cycles are also allowed we find
regimes in which waiting for a short period also increases the number of
matches considerably. Finally, a significant increase of matches can be
obtained by using even one non-simultaneous chain while still matching in an
online fashion. Our theoretical findings and data-driven computational
experiments lead to policy recommendations.


Average-Case Performance of Rollout Algorithms for Knapsack Problems

  Rollout algorithms have demonstrated excellent performance on a variety of
dynamic and discrete optimization problems. Interpreted as an approximate
dynamic programming algorithm, a rollout algorithm estimates the value-to-go at
each decision stage by simulating future events while following a greedy
policy, referred to as the base policy. While in many cases rollout algorithms
are guaranteed to perform as well as their base policies, there have been few
theoretical results showing additional improvement in performance. In this
paper we perform a probabilistic analysis of the subset sum problem and
knapsack problem, giving theoretical evidence that rollout algorithms perform
strictly better than their base policies. Using a stochastic model from the
existing literature, we analyze two rollout methods that we refer to as the
consecutive rollout and exhaustive rollout, both of which employ a simple
greedy base policy. For the subset sum problem, we prove that after only a
single iteration of the rollout algorithm, both methods yield at least a 30%
reduction in the expected gap between the solution value and capacity, relative
to the base policy. Analogous results are shown for the knapsack problem.


Randomized Minmax Regret for Combinatorial Optimization Under
  Uncertainty

  The minmax regret problem for combinatorial optimization under uncertainty
can be viewed as a zero-sum game played between an optimizing player and an
adversary, where the optimizing player selects a solution and the adversary
selects costs with the intention of maximizing the regret of the player. The
existing minmax regret model considers only deterministic solutions/strategies,
and minmax regret versions of most polynomial solvable problems are NP-hard. In
this paper, we consider a randomized model where the optimizing player selects
a probability distribution (corresponding to a mixed strategy) over solutions
and the adversary selects costs with knowledge of the player's distribution,
but not its realization. We show that under this randomized model, the minmax
regret version of any polynomial solvable combinatorial problem becomes
polynomial solvable. This holds true for both the interval and discrete
scenario representations of uncertainty. Using the randomized model, we show
new proofs of existing approximation algorithms for the deterministic model
based on primal-dual approaches. Finally, we prove that minmax regret problems
are NP-hard under general convex uncertainty.


A Decomposition Algorithm for Nested Resource Allocation Problems

  We propose an exact polynomial algorithm for a resource allocation problem
with convex costs and constraints on partial sums of resource consumptions, in
the presence of either continuous or integer variables. No assumption of strict
convexity or differentiability is needed. The method solves a hierarchy of
resource allocation subproblems, whose solutions are used to convert
constraints on sums of resources into bounds for separate variables at higher
levels. The resulting time complexity for the integer problem is $O(n \log m
\log (B/n))$, and the complexity of obtaining an $\epsilon$-approximate
solution for the continuous case is $O(n \log m \log (B/\epsilon))$, $n$ being
the number of variables, $m$ the number of ascending constraints (such that $m
< n$), $\epsilon$ a desired precision, and $B$ the total resource. This
algorithm attains the best-known complexity when $m = n$, and improves it when
$\log m = o(\log n)$. Extensive experimental analyses are conducted with four
recent algorithms on various continuous problems issued from theory and
practice. The proposed method achieves a higher performance than previous
algorithms, addressing all problems with up to one million variables in less
than one minute on a modern computer.


Managing Relocation and Delay in Container Terminals with Flexible
  Service Policies

  We introduce a new model and mathematical formulation for planning crane
moves in the storage yard of container terminals. Our objective is to develop a
tool that captures customer centric elements, especially service time, and
helps operators to manage costly relocation moves. Our model incorporates
several practical details and provides port operators with expanded
capabilities including planning repositioning moves in off-peak hours,
controlling wait times of each customer as well as total service time,
optimizing the number of relocations and wait time jointly, and optimizing
simultaneously the container stacking and retrieval process. We also study a
class of flexible service policies which allow for out-of-order retrieval. We
show that under such flexible policies, we can decrease the number of
relocations and retrieval delays without creating inequities.


Gaussian Process Planning with Lipschitz Continuous Reward Functions:
  Towards Unifying Bayesian Optimization, Active Learning, and Beyond

  This paper presents a novel nonmyopic adaptive Gaussian process planning
(GPP) framework endowed with a general class of Lipschitz continuous reward
functions that can unify some active learning/sensing and Bayesian optimization
criteria and offer practitioners some flexibility to specify their desired
choices for defining new tasks/problems. In particular, it utilizes a
principled Bayesian sequential decision problem framework for jointly and
naturally optimizing the exploration-exploitation trade-off. In general, the
resulting induced GPP policy cannot be derived exactly due to an uncountable
set of candidate observations. A key contribution of our work here thus lies in
exploiting the Lipschitz continuity of the reward functions to solve for a
nonmyopic adaptive epsilon-optimal GPP (epsilon-GPP) policy. To plan in real
time, we further propose an asymptotically optimal, branch-and-bound anytime
variant of epsilon-GPP with performance guarantee. We empirically demonstrate
the effectiveness of our epsilon-GPP policy and its anytime variant in Bayesian
optimization and an energy harvesting task.


Solving Combinatorial Games using Products, Projections and
  Lexicographically Optimal Bases

  In order to find Nash-equilibria for two-player zero-sum games where each
player plays combinatorial objects like spanning trees, matchings etc, we
consider two online learning algorithms: the online mirror descent (OMD)
algorithm and the multiplicative weights update (MWU) algorithm. The OMD
algorithm requires the computation of a certain Bregman projection, that has
closed form solutions for simple convex sets like the Euclidean ball or the
simplex. However, for general polyhedra one often needs to exploit the general
machinery of convex optimization. We give a novel primal-style algorithm for
computing Bregman projections on the base polytopes of polymatroids. Next, in
the case of the MWU algorithm, although it scales logarithmically in the number
of pure strategies or experts $N$ in terms of regret, the algorithm takes time
polynomial in $N$; this especially becomes a problem when learning
combinatorial objects. We give a general recipe to simulate the multiplicative
weights update algorithm in time polynomial in their natural dimension. This is
useful whenever there exists a polynomial time generalized counting oracle
(even if approximate) over these objects. Finally, using the combinatorial
structure of symmetric Nash-equilibria (SNE) when both players play bases of
matroids, we show that these can be found with a single projection or convex
minimization (without using online learning).


Parallel Gaussian Process Regression with Low-Rank Covariance Matrix
  Approximations

  Gaussian processes (GP) are Bayesian non-parametric models that are widely
used for probabilistic regression. Unfortunately, it cannot scale well with
large data nor perform real-time predictions due to its cubic time cost in the
data size. This paper presents two parallel GP regression methods that exploit
low-rank covariance matrix approximations for distributing the computational
load among parallel machines to achieve time efficiency and scalability. We
theoretically guarantee the predictive performances of our proposed parallel
GPs to be equivalent to that of some centralized approximate GP regression
methods: The computation of their centralized counterparts can be distributed
among parallel machines, hence achieving greater time efficiency and
scalability. We analytically compare the properties of our parallel GPs such as
time, space, and communication complexity. Empirical evaluation on two
real-world datasets in a cluster of 20 computing nodes shows that our parallel
GPs are significantly more time-efficient and scalable than their centralized
counterparts and exact/full GP while achieving predictive performances
comparable to full GP.


Logarithmic regret bounds for Bandits with Knapsacks

  Optimal regret bounds for Multi-Armed Bandit problems are now well
documented. They can be classified into two categories based on the growth rate
with respect to the time horizon $T$: (i) small, distribution-dependent, bounds
of order of magnitude $\ln(T)$ and (ii) robust, distribution-free, bounds of
order of magnitude $\sqrt{T}$. The Bandits with Knapsacks model, an extension
to the framework allowing to model resource consumption, lacks this clear-cut
distinction. While several algorithms have been shown to achieve asymptotically
optimal distribution-free bounds on regret, there has been little progress
toward the development of small distribution-dependent regret bounds. We
partially bridge the gap by designing a general-purpose algorithm with
distribution-dependent regret bounds that are logarithmic in the initial
endowments of resources in several important cases that cover many practical
applications, including dynamic pricing with limited supply, bid optimization
in online advertisement auctions, and dynamic procurement.


Structured Prediction by Conditional Risk Minimization

  We propose a general approach for supervised learning with structured output
spaces, such as combinatorial and polyhedral sets, that is based on minimizing
estimated conditional risk functions. Given a loss function defined over pairs
of output labels, we first estimate the conditional risk function by solving a
(possibly infinite) collection of regularized least squares problems. A
prediction is made by solving an inference problem that minimizes the estimated
conditional risk function over the output space. We show that this approach
enables, in some cases, efficient training and inference without explicitly
introducing a convex surrogate for the original loss function, even when it is
discontinuous. Empirical evaluations on real-world and synthetic data sets
demonstrate the effectiveness of our method in adapting to a variety of loss
functions.


Separable Convex Optimization with Nested Lower and Upper Constraints

  We study a convex resource allocation problem in which lower and upper bounds
are imposed on partial sums of allocations. This model is linked to a large
range of applications, including production planning, speed optimization,
stratified sampling, support vector machines, portfolio management, and
telecommunications. We propose an efficient gradient-free divide-and-conquer
algorithm, which uses monotonicity arguments to generate valid bounds from the
recursive calls, and eliminate linking constraints based on the information
from sub-problems. This algorithm does not need strict convexity or
differentiability. It produces an $\epsilon$-approximate solution for the
continuous problem in $\mathcal{O}(n \log m \log \frac{n B}{\epsilon})$ time
and an integer solution in $\mathcal{O}(n \log m \log B)$ time, where $n$ is
the number of decision variables, $m$ is the number of constraints, and $B$ is
the resource bound. A complexity of $\mathcal{O}(n \log m)$ is also achieved
for the linear and quadratic cases. These are the best complexities known to
date for this important problem class. Our experimental analyses confirm the
good performance of the method, which produces optimal solutions for problems
with up to 1,000,000 variables in a few seconds. Promising applications to the
support vector ordinal regression problem are also investigated.


The Stochastic Container Relocation Problem

  The Container Relocation Problem (CRP) is concerned with finding a sequence
of moves of containers that minimizes the number of relocations needed to
retrieve all containers, while respecting a given order of retrieval. However,
the assumption of knowing the full retrieval order of containers is
particularly unrealistic in real operations. This paper studies the stochastic
CRP (SCRP), which relaxes this assumption. A new multi-stage stochastic model,
called the batch model, is introduced, motivated, and compared with an existing
model (the online model). The two main contributions are an optimal algorithm
called Pruning-Best-First-Search (PBFS) and a randomized approximate algorithm
called PBFS-Approximate with a bounded average error. Both algorithms,
applicable in the batch and online models, are based on a new family of lower
bounds for which we show some theoretical properties. Moreover, we introduce
two new heuristics outperforming the best existing heuristics. Algorithms,
bounds and heuristics are tested in an extensive computational section.
Finally, based on strong computational evidence, we conjecture the optimality
of the "Leveling" heuristic in a special "no information" case, where at any
retrieval stage, any of the remaining containers is equally likely to be
retrieved next.


Signaling Game-based Misbehavior Inspection in V2I-enabled Highway
  Operations

  Vehicle-to-Infrastructure (V2I) communications are increasingly supporting
highway operations such as electronic toll collection, carpooling, and vehicle
platooning. In this paper we study the incentives of strategic misbehavior by
individual vehicles who can exploit the security vulnerabilities in V2I
communications and negatively impact the highway operations. We consider a
V2I-enabled highway segment facing two classes of vehicles (agent populations),
each with an authorized access to one server (subset of lanes). Vehicles are
strategic in that they can misreport their class (type) to the system operator
and get an unauthorized access to the server dedicated to the other class. This
misbehavior causes additional congestion externality on the compliant vehicles,
and thus, needs to be deterred. We focus on an environment where the operator
is able to inspect the vehicles for misbehavior. The inspection is costly and
successful detection incurs a fine on the misbehaving vehicle. We formulate a
signaling game to study the strategic interaction between the vehicle classes
and the operator. Our equilibrium analysis provides conditions on the cost
parameters that govern the vehicles' incentive to misbehave or not. We also
determine the operator's equilibrium inspection strategy.


Maximum Weight Online Matching with Deadlines

  We study the problem of matching agents who arrive at a marketplace over time
and leave after d time periods. Agents can only be matched while they are
present in the marketplace. Each pair of agents can yield a different match
value, and the planner's goal is to maximize the total value over a finite time
horizon. First we study the case in which vertices arrive in an adversarial
order. We provide a randomized 0.25-competitive algorithm building on a result
by Feldman et al. (2009) and Lehman et al. (2006). We extend the model to the
case in which departure times are drawn independently from a distribution with
non-decreasing hazard rate, for which we establish a 1/8-competitive algorithm.
  When the arrival order is chosen uniformly at random, we show that a batching
algorithm, which computes a maximum-weighted matching every (d+1) periods, is
0.279-competitive.


Probability Distributions on Partially Ordered Sets and Network Security
  Games

  We consider the following problem: Does there exist a probability
distribution over subsets of a finite partially ordered set (poset), such that
a set of constraints involving marginal probabilities of the poset's elements
and maximal chains is satisfied? In this article, we present a combinatorial
algorithm to positively resolve this question. We show that this result plays a
crucial role in the equilibrium analysis of a generic security game on a
capacitated flow network. The game involves a routing entity that sends its
flow through the network while facing path transportation costs, and an
interdictor who simultaneously interdicts one or more edges while facing edge
interdiction costs. The first (resp. second) player seeks to maximize the value
of effective (resp. interdicted) flow net the total transportation (resp.
interdiction) cost. Using our existence result on posets and strict
complementary slackness in linear programming, we show that the equilibrium
properties of this game can be described using primal and dual solutions of a
minimum cost circulation problem. Our analysis provides a new characterization
of the critical network components.


Decentralized Data Fusion and Active Sensing with Mobile Sensors for
  Modeling and Predicting Spatiotemporal Traffic Phenomena

  The problem of modeling and predicting spatiotemporal traffic phenomena over
an urban road network is important to many traffic applications such as
detecting and forecasting congestion hotspots. This paper presents a
decentralized data fusion and active sensing (D2FAS) algorithm for mobile
sensors to actively explore the road network to gather and assimilate the most
informative data for predicting the traffic phenomenon. We analyze the time and
communication complexity of D2FAS and demonstrate that it can scale well with a
large number of observations and sensors. We provide a theoretical guarantee on
its predictive performance to be equivalent to that of a sophisticated
centralized sparse approximation for the Gaussian process (GP) model: The
computation of such a sparse approximate GP model can thus be parallelized and
distributed among the mobile sensors (in a Google-like MapReduce paradigm),
thereby achieving efficient and scalable prediction. We also theoretically
guarantee its active sensing performance that improves under various practical
environmental conditions. Empirical evaluation on real-world urban road network
data shows that our D2FAS algorithm is significantly more time-efficient and
scalable than state-oftheart centralized algorithms while achieving comparable
predictive performance.


Parallel Gaussian Process Regression with Low-Rank Covariance Matrix
  Approximations

  Gaussian processes (GP) are Bayesian non-parametric models that are widely
used for probabilistic regression. Unfortunately, it cannot scale well with
large data nor perform real-time predictions due to its cubic time cost in the
data size. This paper presents two parallel GP regression methods that exploit
low-rank covariance matrix approximations for distributing the computational
load among parallel machines to achieve time efficiency and scalability. We
theoretically guarantee the predictive performances of our proposed parallel
GPs to be equivalent to that of some centralized approximate GP regression
methods: The computation of their centralized counterparts can be distributed
among parallel machines, hence achieving greater time efficiency and
scalability. We analytically compare the properties of our parallel GPs such as
time, space, and communication complexity. Empirical evaluation on two
real-world datasets in a cluster of 20 computing nodes shows that our parallel
GPs are significantly more time-efficient and scalable than their centralized
counterparts and exact/full GP while achieving predictive performances
comparable to full GP.


Advances on Matroid Secretary Problems: Free Order Model and Laminar
  Case

  The most well-known conjecture in the context of matroid secretary problems
claims the existence of a constant-factor approximation applicable to any
matroid. Whereas this conjecture remains open, modified forms of it were shown
to be true, when assuming that the assignment of weights to the secretaries is
not adversarial but uniformly random (Soto [SODA 2011], Oveis Gharan and
Vondr\'ak [ESA 2011]). However, so far, there was no variant of the matroid
secretary problem with adversarial weight assignment for which a
constant-factor approximation was found. We address this point by presenting a
9-approximation for the \emph{free order model}, a model suggested shortly
after the introduction of the matroid secretary problem, and for which no
constant-factor approximation was known so far. The free order model is a
relaxed version of the original matroid secretary problem, with the only
difference that one can choose the order in which secretaries are interviewed.
  Furthermore, we consider the classical matroid secretary problem for the
special case of laminar matroids. Only recently, a constant-factor
approximation has been found for this case, using a clever but rather involved
method and analysis (Im and Wang, [SODA 2011]) that leads to a
16000/3-approximation. This is arguably the most involved special case of the
matroid secretary problem for which a constant-factor approximation is known.
We present a considerably simpler and stronger $3\sqrt{3}e\approx
14.12$-approximation, based on reducing the problem to a matroid secretary
problem on a partition matroid.


Greedy Online Bipartite Matching on Random Graphs

  We study the average performance of online greedy matching algorithms on
$G(n,n,p)$, the random bipartite graph with $n$ vertices on each side and edges
occurring independently with probability $p=p(n)$. In the online model,
vertices on one side of the graph are given up front while vertices on the
other side arrive sequentially; when a vertex arrives its edges are revealed
and it must be immediately matched or dropped. We begin by analyzing the
\textsc{oblivious} algorithm, which tries to match each arriving vertex to a
random neighbor, even if the neighbor has already been matched. The algorithm
is shown to have a performance ratio of at least $1-1/e$ for all monotonic
functions $p(n)$, where the performance ratio is defined asymptotically as the
ratio of the expected matching size given by the algorithm to the expected
maximum matching size. Next we show that the conventional \textsc{greedy}
algorithm, which assigns each vertex to a random unmatched neighbor, has a
performance ratio of at least 0.837 for all monotonic functions $p(n)$. Under
the $G(n,n,p)$ model, the performance of \textsc{greedy} is equivalent to the
performance of the well known \textsc{ranking} algorithm, so our results show
that \textsc{ranking} has a performance ratio of at least 0.837. We finally
consider vertex-weighted bipartite matching. Our proofs are based on simple
differential equations that describe the evolution of the matching process.


On Matching and Thickness in Heterogeneous Dynamic Markets

  We study dynamic matching in an infinite-horizon stochastic market. While all
agents are potentially compatible with each other, some are hard-to-match and
others are easy-to-match. Agents prefer to be matched as soon as possible and
matches are formed either bilaterally or indirectly through chains. We adopt an
asymptotic approach and compute tight bounds on the limit of waiting time of
agents under myopic policies that differ in matching technology and
prioritization.
  We find that the market composition is a key factor in the desired matching
technology and prioritization level. When hard-to-match agents arrive less
frequently than easy-to-match ones (i) bilateral matching is almost as
efficient as chains (waiting times scale similarly under both, though chains
always outperform bilateral matching by a constant factor), and (ii) assigning
priorities to hard-to-match agents improves their waiting times. When
hard-to-match agents arrive more frequently, chains are much more efficient
than bilateral matching and prioritization has no impact.
  We further conduct comparative statics on arrival rates. Somewhat
surprisingly, we find that in a heterogeneous market and under bilateral
matching, increasing arrival rate has a non-monotone effect on waiting times,
due to the fact that, under some market compositions, there is an adverse
effect of competition. Our comparative statics shed light on the impact of
merging markets and attracting altruistic agents (that initiate chains) or
easy-to-match agents.
  This work uncovers fundamental differences between heterogeneous and
homogeneous dynamic markets, and potentially helps policy makers to generate
insights on the operations of matching markets such as kidney exchange
programs.


Decentralized Data Fusion and Active Sensing with Mobile Sensors for
  Modeling and Predicting Spatiotemporal Traffic Phenomena

  The problem of modeling and predicting spatiotemporal traffic phenomena over
an urban road network is important to many traffic applications such as
detecting and forecasting congestion hotspots. This paper presents a
decentralized data fusion and active sensing (D2FAS) algorithm for mobile
sensors to actively explore the road network to gather and assimilate the most
informative data for predicting the traffic phenomenon. We analyze the time and
communication complexity of D2FAS and demonstrate that it can scale well with a
large number of observations and sensors. We provide a theoretical guarantee on
its predictive performance to be equivalent to that of a sophisticated
centralized sparse approximation for the Gaussian process (GP) model: The
computation of such a sparse approximate GP model can thus be parallelized and
distributed among the mobile sensors (in a Google-like MapReduce paradigm),
thereby achieving efficient and scalable prediction. We also theoretically
guarantee its active sensing performance that improves under various practical
environmental conditions. Empirical evaluation on real-world urban road network
data shows that our D2FAS algorithm is significantly more time-efficient and
scalable than state-of-the-art centralized algorithms while achieving
comparable predictive performance.


No-Regret Learnability for Piecewise Linear Losses

  In the convex optimization approach to online regret minimization, many
methods have been developed to guarantee a $O(\sqrt{T})$ bound on regret for
subdifferentiable convex loss functions with bounded subgradients, by using a
reduction to linear loss functions. This suggests that linear loss functions
tend to be the hardest ones to learn against, regardless of the underlying
decision spaces. We investigate this question in a systematic fashion looking
at the interplay between the set of possible moves for both the decision maker
and the adversarial environment. This allows us to highlight sharp distinctive
behaviors about the learnability of piecewise linear loss functions. On the one
hand, when the decision set of the decision maker is a polyhedron, we establish
$\Omega(\sqrt{T})$ lower bounds on regret for a large class of piecewise linear
loss functions with important applications in online linear optimization,
repeated zero-sum Stackelberg games, online prediction with side information,
and online two-stage optimization. On the other hand, we exhibit $o(\sqrt{T})$
learning rates, achieved by the Follow-The-Leader algorithm, in online linear
optimization when the boundary of the decision maker's decision set is curved
and when $0$ does not lie in the convex hull of the environment's decision set.
Hence, the curvature of the decision maker's decision set is a determining
factor for the optimal learning rate. These results hold in a completely
adversarial setting.


Stochastic Variational Inference for Bayesian Sparse Gaussian Process
  Regression

  This paper presents a novel variational inference framework for deriving a
family of Bayesian sparse Gaussian process regression (SGPR) models whose
approximations are variationally optimal with respect to the full-rank GPR
model enriched with various corresponding correlation structures of the
observation noises. Our variational Bayesian SGPR (VBSGPR) models jointly treat
both the distributions of the inducing variables and hyperparameters as
variational parameters, which enables the decomposability of the variational
lower bound that in turn can be exploited for stochastic optimization. Such a
stochastic optimization involves iteratively following the stochastic gradient
of the variational lower bound to improve its estimates of the optimal
variational distributions of the inducing variables and hyperparameters (and
hence the predictive distribution) of our VBSGPR models and is guaranteed to
achieve asymptotic convergence to them. We show that the stochastic gradient is
an unbiased estimator of the exact gradient and can be computed in constant
time per iteration, hence achieving scalability to big data. We empirically
evaluate the performance of our proposed framework on two real-world, massive
datasets.


Maximizing Efficiency in Dynamic Matching Markets

  We study the problem of matching agents who arrive at a marketplace over time
and leave after d time periods. Agents can only be matched while they are
present in the marketplace. Each pair of agents can yield a different match
value, and the planner's goal is to maximize the total value over a finite time
horizon. We study matching algorithms that perform well over any sequence of
arrivals when there is no a priori information about the match values or
arrival times.
  Our main contribution is a 1/4-competitive algorithm. The algorithm randomly
selects a subset of agents who will wait until right before their departure to
get matched, and maintains a maximum-weight matching with respect to the other
agents. The primal-dual analysis of the algorithm hinges on a careful
comparison between the initial dual value associated with an agent when it
first arrives, and the final value after d time steps.
  It is also shown that no algorithm is 1/2-competitive. We extend the model to
the case in which departure times are drawn i.i.d from a distribution with
non-decreasing hazard rate, and establish a 1/8-competitive algorithm in this
setting. Finally we show on real-world data that a modified version of our
algorithm performs well in practice.


Online Resource Allocation under Partially Predictable Demand

  For online resource allocation problems, we propose a new demand arrival
model where the sequence of arrivals contains both an adversarial component and
a stochastic one. Our model requires no demand forecasting; however, due to the
presence of the stochastic component, we can partially predict future demand as
the sequence of arrivals unfolds. Under the proposed model, we study the
problem of the online allocation of a single resource to two types of
customers, and design online algorithms that outperform existing ones. Our
algorithms are adjustable to the relative size of the stochastic component, and
our analysis reveals that as the portion of the stochastic component grows, the
loss due to making online decisions decreases. This highlights the value of
(even partial) predictability in online resource allocation. We impose no
conditions on how the resource capacity scales with the maximum number of
customers. However, we show that using an adaptive algorithm---which makes
online decisions based on observed data---is particularly beneficial when
capacity scales linearly with the number of customers. Our work serves as a
first step in bridging the long-standing gap between the two well-studied
approaches to the design and analysis of online algorithms based on (1)
adversarial models and (2) stochastic ones. Using novel algorithm design, we
demonstrate that even if the arrival sequence contains an adversarial
component, we can take advantage of the limited information that the data
reveals to improve allocation decisions. We also study the classical secretary
problem under our proposed arrival model, and we show that randomizing over
multiple stopping rules may increase the probability of success.


Distributed Multi-Depot Routing without Communications

  We consider and formulate a class of distributed multi-depot routing
problems, where servers are to visit a set of requests, with the aim of
minimizing the total distance travelled by all servers. These problems fall
into two categories: distributed offline routing problems where all the
requests that need to be visited are known from the start; distributed online
routing problems where the requests come to be known incrementally. A critical
and novel feature of our formulations is that communications are not allowed
among the servers, hence posing an interesting and challenging question: what
performance can be achieved in comparison to the best possible solution
obtained from an omniscience planner with perfect communication capabilities?
The worst-case (over all possible request-set instances) performance metrics
are given by the approximation ratio (offline case) and the competitive ratio
(online case).
  Our first result indicates that the online and offline problems are
effectively equivalent: for the same request-set instance, the approximation
ratio and the competitive ratio differ by at most an additive factor of 2,
irrespective of the release dates in the online case. Therefore, we can
restrict our attention to the offline problem. For the offline problem, we show
that the approximation ratio given by the Voronoi partition is m (the number of
servers). For two classes of depot configurations, when the depots form a line
and when the ratios between the distances of pairs of depots are upper bounded
by a sublinear function f(m) (i.e., f(m) = o(m)), we give partition schemes
with sublinear approximation ratios O(log m) and {\Theta}(f(m)) respectively.
We also discuss several interesting open problems in our formulations: in
particular, how our initial results (on the two deliberately chosen classes of
depots) shape our conjecture on the open problems.


Container Relocation Problem: Approximation, Asymptotic, and Incomplete
  Information

  The Container Relocation Problem (CRP) is concerned with finding a sequence
of moves of containers that minimizes the number of relocations needed to
retrieve all containers respecting a given order of retrieval. While the
problem is known to be NP-hard, certain algorithms such as the A* search and
heuristics perform reasonably well on many instances of the problem. In this
paper, we first focus on the A* search algorithm, and analyze lower and upper
bounds that are easy to compute and can be used to prune nodes. Our analysis
sheds light on which bounds result in fast computation within a given
approximation gap. We present extensive simulation results that improve upon
our theoretical analysis, and further show that our method finds the optimum
solution on most instances of medium-size bays. On "hard" instances, our method
finds an approximate solution with a small gap and within a time frame that is
fast for practical applications. We also study the average-case asymptotic
behavior of the CRP where the number of columns grows. We calculate the
expected number of relocations in the limit, and show that the optimum number
of relocations converges to a simple and intuitive lower-bound. We further
study the CRP with incomplete information by relaxing the assumption that the
order of retrieval of all containers are initially known. This assumption is
particularly unrealistic in ports without an appointment system. We assume that
the retrieval order of a subset of containers is known initially and the
retrieval order of the remaining containers is observed later at a given
specific time. Before this time, we assume a probabilistic distribution on the
retrieval order of unknown containers. We combine the A* algorithm with
sampling technique to solve this two-stage stochastic optimization problem. We
show that our algorithm is fast and the error due to sampling and pruning is
reasonably small.


Parallel Gaussian Process Regression for Big Data: Low-Rank
  Representation Meets Markov Approximation

  The expressive power of a Gaussian process (GP) model comes at a cost of poor
scalability in the data size. To improve its scalability, this paper presents a
low-rank-cum-Markov approximation (LMA) of the GP model that is novel in
leveraging the dual computational advantages stemming from complementing a
low-rank approximate representation of the full-rank GP based on a support set
of inputs with a Markov approximation of the resulting residual process; the
latter approximation is guaranteed to be closest in the Kullback-Leibler
distance criterion subject to some constraint and is considerably more refined
than that of existing sparse GP models utilizing low-rank representations due
to its more relaxed conditional independence assumption (especially with larger
data). As a result, our LMA method can trade off between the size of the
support set and the order of the Markov property to (a) incur lower
computational cost than such sparse GP models while achieving predictive
performance comparable to them and (b) accurately represent features/patterns
of any scale. Interestingly, varying the Markov order produces a spectrum of
LMAs with PIC approximation and full-rank GP at the two extremes. An advantage
of our LMA method is that it is amenable to parallelization on multiple
machines/cores, thereby gaining greater scalability. Empirical evaluation on
three real-world datasets in clusters of up to 32 computing nodes shows that
our centralized and parallel LMA methods are significantly more time-efficient
and scalable than state-of-the-art sparse and full-rank GP regression methods
while achieving comparable predictive performances.


