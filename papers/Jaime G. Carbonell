Nonparametric Neural Networks

  Automatically determining the optimal size of a neural network for a giventask without prior information currently requires an expensive global searchand training many networks from scratch. In this paper, we address the problemof automatically finding a good network size during a single training cycle. Weintroduce *nonparametric neural networks*, a non-probabilistic framework forconducting optimization over all possible network sizes and prove its soundnesswhen network growth is limited via an L_p penalty. We train networks under thisframework by continuously adding new units while eliminating redundant unitsvia an L_2 penalty. We employ a novel optimization algorithm, which we term*adaptive radial-angular gradient descent* or *AdaRad*, and obtain promisingresults.

Smoothing Proximal Gradient Method for General Structured Sparse  Learning

  We study the problem of learning high dimensional regression modelsregularized by a structured-sparsity-inducing penalty that encodes priorstructural information on either input or output sides. We consider two widelyadopted types of such penalties as our motivating examples: 1) overlappinggroup lasso penalty, based on the l1/l2 mixed-norm penalty, and 2) graph-guidedfusion penalty. For both types of penalties, due to their non-separability,developing an efficient optimization method has remained a challenging problem.In this paper, we propose a general optimization approach, called smoothingproximal gradient method, which can solve the structured sparse regressionproblems with a smooth convex loss and a wide spectrum ofstructured-sparsity-inducing penalties. Our approach is based on a generalsmoothing technique of Nesterov. It achieves a convergence rate faster than thestandard first-order method, subgradient method, and is much more scalable thanthe most widely used interior-point method. Numerical results are reported todemonstrate the efficiency and scalability of the proposed method.

Efficient Structured Matrix Rank Minimization

  We study the problem of finding structured low-rank matrices using nuclearnorm regularization where the structure is encoded by a linear map. In contrastto most known approaches for linearly structured rank minimization, we do not(a) use the full SVD, nor (b) resort to augmented Lagrangian techniques, nor(c) solve linear systems per iteration. Instead, we formulate the problemdifferently so that it is amenable to a generalized conditional gradientmethod, which results in a practical improvement with low per iterationcomputational cost. Numerical results show that our approach significantlyoutperforms state-of-the-art competitors in terms of running time, whileeffectively recovering low rank solutions in stochastic system realization andspectral compressed sensing problems.

Adapting Word Embeddings to New Languages with Morphological and  Phonological Subword Representations

  Much work in Natural Language Processing (NLP) has been for resource-richlanguages, making generalization to new, less-resourced languages challenging.We present two approaches for improving generalization to low-resourcedlanguages by adapting continuous word representations using linguisticallymotivated subword units: phonemes, morphemes and graphemes. Our method requiresneither parallel corpora nor bilingual dictionaries and provides a significantgain in performance over previous methods relying on these resources. Wedemonstrate the effectiveness of our approaches on Named Entity Recognition forfour languages, namely Uyghur, Turkish, Bengali and Hindi, of which Uyghur andBengali are low resource languages, and also perform experiments on MachineTranslation. Exploiting subwords with transfer learning gives us a boost of+15.2 NER F1 for Uyghur and +9.7 F1 for Bengali. We also show improvements inthe monolingual setting where we achieve (avg.) +3 F1 and (avg.) +1.35 BLEU.

Graph-Structured Multi-task Regression and an Efficient Optimization  Method for General Fused Lasso

  We consider the problem of learning a structured multi-task regression, wherethe output consists of multiple responses that are related by a graph and thecorrelated response variables are dependent on the common inputs in a sparsebut synergistic manner. Previous methods such as l1/l2-regularized multi-taskregression assume that all of the output variables are equally related to theinputs, although in many real-world problems, outputs are related in a complexmanner. In this paper, we propose graph-guided fused lasso (GFlasso) forstructured multi-task regression that exploits the graph structure over theoutput variables. We introduce a novel penalty function based on fusion penaltyto encourage highly correlated outputs to share a common set of relevantinputs. In addition, we propose a simple yet efficient proximal-gradient methodfor optimizing GFlasso that can also be applied to any optimization problemswith a convex smooth loss and the general class of fusion penalty defined onarbitrary graph structures. By exploiting the structure of the non-smooth''fusion penalty'', our method achieves a faster convergence rate than thestandard first-order method, sub-gradient method, and is significantly morescalable than the widely adopted second-order cone-programming andquadratic-programming formulations. In addition, we provide an analysis of theconsistency property of the GFlasso model. Experimental results not onlydemonstrate the superiority of GFlasso over the standard lasso but also showthe efficiency and scalability of our proximal-gradient method.

Smoothing proximal gradient method for general structured sparse  regression

  We study the problem of estimating high-dimensional regression modelsregularized by a structured sparsity-inducing penalty that encodes priorstructural information on either the input or output variables. We consider twowidely adopted types of penalties of this kind as motivating examples: (1) thegeneral overlapping-group-lasso penalty, generalized from the group-lassopenalty; and (2) the graph-guided-fused-lasso penalty, generalized from thefused-lasso penalty. For both types of penalties, due to their nonseparabilityand nonsmoothness, developing an efficient optimization method remains achallenging problem. In this paper we propose a general optimization approach,the smoothing proximal gradient (SPG) method, which can solve structured sparseregression problems with any smooth convex loss under a wide spectrum ofstructured sparsity-inducing penalties. Our approach combines a smoothingtechnique with an effective proximal gradient method. It achieves a convergencerate significantly faster than the standard first-order methods, subgradientmethods, and is much more scalable than the most widely used interior-pointmethods. The efficiency and scalability of our method are demonstrated on bothsimulation experiments and real genetic data sets.

The exploding gradient problem demystified - definition, prevalence,  impact, origin, tradeoffs, and solutions

  Whereas it is believed that techniques such as Adam, batch normalization and,more recently, SeLU nonlinearities "solve" the exploding gradient problem, weshow that this is not the case in general and that in a range of popular MLParchitectures, exploding gradients exist and that they limit the depth to whichnetworks can be effectively trained, both in theory and in practice. We explainwhy exploding gradients occur and highlight the *collapsing domain problem*,which can arise in architectures that avoid exploding gradients.  ResNets have significantly lower gradients and thus can circumvent theexploding gradient problem, enabling the effective training of much deepernetworks. We show this is a direct consequence of the Pythagorean equation. Bynoticing that *any neural network is a residual network*, we devise the*residual trick*, which reveals that introducing skip connections simplifiesthe network mathematically, and that this simplicity may be the major cause fortheir success.

The Nonlinearity Coefficient - Predicting Generalization in Deep Neural  Networks

  For a long time, designing neural architectures that exhibit high performancewas considered a dark art that required expert hand-tuning. One of the fewwell-known guidelines for architecture design is the avoidance of explodinggradients, though even this guideline has remained relatively vague andcircumstantial. We introduce the nonlinearity coefficient (NLC), a measurementof the complexity of the function computed by a neural network that is based onthe magnitude of the gradient. Via an extensive empirical study, we show thatthe NLC is a powerful predictor of test error and that attaining a right-sizedNLC is essential for optimal performance.  The NLC exhibits a range of intriguing and important properties. It isclosely tied to the amount of information gained from computing a singlenetwork gradient. It is tied to the error incurred when replacing thenonlinearity operations in the network with linear operations. It is notsusceptible to the confounders of multiplicative scaling, additive bias andlayer width. It is stable from layer to layer. Hence, we argue that the NLC isthe first robust predictor of overfitting in deep networks.

