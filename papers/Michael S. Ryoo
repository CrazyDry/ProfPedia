Privacy-Preserving Human Activity Recognition from Extreme Low  Resolution

  Privacy protection from surreptitious video recordings is an importantsocietal challenge. We desire a computer vision system (e.g., a robot) that canrecognize human activities and assist our daily life, yet ensure that it is notrecording video that may invade our privacy. This paper presents a fundamentalapproach to address such contradicting objectives: human activity recognitionwhile only using extreme low-resolution (e.g., 16x12) anonymized videos. Weintroduce the paradigm of inverse super resolution (ISR), the concept oflearning the optimal set of image transformations to generate multiplelow-resolution (LR) training videos from a single video. Our ISR learnsdifferent types of sub-pixel transformations optimized for the activityclassification, allowing the classifier to best take advantage of existinghigh-resolution videos (e.g., YouTube videos) by creating multiple LR trainingvideos tailored for the problem. We experimentally confirm that the paradigm ofinverse super resolution is able to benefit activity recognition from extremelow-resolution videos.

Learning Latent Sub-events in Activity Videos Using Temporal Attention  Filters

  In this paper, we newly introduce the concept of temporal attention filters,and describe how they can be used for human activity recognition from videos.Many high-level activities are often composed of multiple temporal parts (e.g.,sub-events) with different duration/speed, and our objective is to make themodel explicitly learn such temporal structure using multiple attention filtersand benefit from them. Our temporal filters are designed to be fullydifferentiable, allowing end-of-end training of the temporal filters togetherwith the underlying frame-based or segment-based convolutional neural networkarchitectures. This paper presents an approach of learning a set of optimalstatic temporal attention filters to be shared across different videos, andextends this approach to dynamically adjust attention filters per testing videousing recurrent long short-term memory networks (LSTMs). This allows ourtemporal attention filters to learn latent sub-events specific to eachactivity. We experimentally confirm that the proposed concept of temporalattention filters benefits the activity recognition, and we visualize thelearned latent sub-events.

Extreme Low Resolution Activity Recognition with Multi-Siamese Embedding  Learning

  This paper presents an approach for recognizing human activities from extremelow resolution (e.g., 16x12) videos. Extreme low resolution recognition is notonly necessary for analyzing actions at a distance but also is crucial forenabling privacy-preserving recognition of human activities. We design a newtwo-stream multi-Siamese convolutional neural network. The idea is toexplicitly capture the inherent property of low resolution (LR) videos that twoimages originated from the exact same scene often have totally different pixelvalues depending on their LR transformations. Our approach learns the sharedembedding space that maps LR videos with the same content to the same locationregardless of their transformations. We experimentally confirm that ourapproach of jointly learning such transform robust LR video representation andthe classifier outperforms the previous state-of-the-art low resolutionrecognition approaches on two public standard datasets by a meaningful margin.

Evolving Space-Time Neural Architectures for Videos

  In this paper, we present a new method for evolving video CNN models to findarchitectures that more optimally captures rich spatio-temporal information invideos. Previous work, taking advantage of 3D convolutional layers, obtainedpromising results by manually designing CNN architectures for videos. We heredevelop an evolutionary algorithm that automatically explores models withdifferent types and combinations of space-time convolutional layers to jointlycapture various spatial and temporal aspects of video representations. Wefurther propose a new key component in video model evolution, the iTGM layer,which more efficiently utilizes its parameters to allow learning of space-timeinteractions over longer time horizons. The experiments confirm the advantagesof our video CNN architecture evolution, with results outperforming previousstate-of-the-art models. Our algorithm discovers new and interesting videoarchitecture structures.

Early Recognition of Human Activities from First-Person Videos Using  Onset Representations

  In this paper, we propose a methodology for early recognition of humanactivities from videos taken with a first-person viewpoint. Early recognition,which is also known as activity prediction, is an ability to infer an ongoingactivity at its early stage. We present an algorithm to perform recognition ofactivities targeted at the camera from streaming videos, making the system topredict intended activities of the interacting person and avoid harmful eventsbefore they actually happen. We introduce the novel concept of 'onset' thatefficiently summarizes pre-activity observations, and design an approach toconsider event history in addition to ongoing video observation for earlyfirst-person recognition of activities. We propose to represent onset usingcascade histograms of time series gradients, and we describe a novelalgorithmic setup to take advantage of onset for early recognition ofactivities. The experimental results clearly illustrate that the proposedconcept of onset enables better/earlier recognition of human activities fromfirst-person videos.

Pooled Motion Features for First-Person Videos

  In this paper, we present a new feature representation for first-personvideos. In first-person video understanding (e.g., activity recognition), it isvery important to capture both entire scene dynamics (i.e., egomotion) andsalient local motion observed in videos. We describe a representation frameworkbased on time series pooling, which is designed to abstractshort-term/long-term changes in feature descriptor elements. The idea is tokeep track of how descriptor values are changing over time and summarize themto represent motion in the activity video. The framework is general, handlingany types of per-frame feature descriptors including conventional motiondescriptors like histogram of optical flows (HOF) as well as appearancedescriptors from more recent convolutional neural networks (CNN). Weexperimentally confirm that our approach clearly outperforms previous featurerepresentations including bag-of-visual-words and improved Fisher vector (IFV)when using identical underlying feature descriptors. We also confirm that ourfeature representation has superior performance to existing state-of-the-artfeatures like local spatio-temporal features and Improved Trajectory Features(originally developed for 3rd-person videos) when handling first-person videos.Multiple first-person activity datasets were tested under various settings toconfirm these findings.

Temporal Gaussian Mixture Layer for Videos

  We introduce a new convolutional layer named the Temporal Gaussian Mixture(TGM) layer and present how it can be used to efficiently capture longer-termtemporal information in continuous activity videos. The TGM layer is a temporalconvolutional layer governed by a much smaller set of parameters (e.g.,location/variance of Gaussians) that are fully differentiable. We present ourfully convolutional video models with multiple TGM layers for activitydetection. The extensive experiments on multiple datasets, including Charadesand MultiTHUMOS, confirm the effectiveness of TGM layers, significantlyoutperforming the state-of-the-arts.

Fine-grained Activity Recognition in Baseball Videos

  In this paper, we introduce a challenging new dataset, MLB-YouTube, designedfor fine-grained activity detection. The dataset contains two settings:segmented video classification as well as activity detection in continuousvideos. We experimentally compare various recognition approaches capturingtemporal structure in activity videos, by classifying segmented videos andextending those approaches to continuous videos. We also compare models on theextremely difficult task of predicting pitch speed and pitch type frombroadcast baseball videos. We find that learning temporal structure is valuablefor fine-grained activity recognition.

Unseen Action Recognition with Multimodal Learning

  In this paper, we present a method to learn a joint multimodal representationspace that allows for the recognition of unseen activities in videos. Wecompare the effect of placing various constraints on the embedding space usingpaired text and video data. Additionally, we propose a method to improve thejoint embedding space using an adversarial formulation with unpaired text andvideo data. In addition to testing on publicly available datasets, we introducea new, large-scale text/video dataset. We experimentally confirm that learningsuch shared embedding space benefits three difficult tasks (i) zero-shotactivity classification, (ii) unsupervised activity discovery, and (iii) unseenactivity captioning.

Learning Differentiable Grammars for Continuous Data

  This paper proposes a novel algorithm which learns a formal regular grammarfrom real-world continuous data, such as videos or other streaming data.Learning latent terminals, non-terminals, and productions rules directly fromstreaming data allows the construction of a generative model capturingsequential structures with multiple possibilities. Our model is fullydifferentiable, and provides easily interpretable results which are importantin order to understand the learned structures. It outperforms thestate-of-the-art on several challenging datasets and is more accurate forforecasting future activities in videos. We plan to open-source the code.

Multi-Type Activity Recognition in Robot-Centric Scenarios

  Activity recognition is very useful in scenarios where robots interact with,monitor or assist humans. In the past years many types of activities -- singleactions, two persons interactions or ego-centric activities, to name a few --have been analyzed. Whereas traditional methods treat such types of activitiesseparately, an autonomous robot should be able to detect and recognize multipletypes of activities to effectively fulfill its tasks. We propose a method thatis intrinsically able to detect and recognize activities of different typesthat happen in sequence or concurrently. We present a new unified descriptor,called Relation History Image (RHI), which can be extracted from all theactivity types we are interested in. We then formulate an optimizationprocedure to detect and recognize activities of different types. We apply ourapproach to a new dataset recorded from a robot-centric perspective andsystematically evaluate its quality compared to multiple baselines. Finally, weshow the efficacy of the RHI descriptor on publicly available datasetsperforming extensive comparisons.

Learning Social Affordance Grammar from Videos: Transferring Human  Interactions to Human-Robot Interactions

  In this paper, we present a general framework for learning social affordancegrammar as a spatiotemporal AND-OR graph (ST-AOG) from RGB-D videos of humaninteractions, and transfer the grammar to humanoids to enable a real-timemotion inference for human-robot interaction (HRI). Based on Gibbs sampling,our weakly supervised grammar learning can automatically construct ahierarchical representation of an interaction with long-term joint sub-tasks ofboth agents and short term atomic actions of individual agents. Based on a newRGB-D video dataset with rich instances of human interactions, our experimentsof Baxter simulation, human evaluation, and real Baxter test demonstrate thatthe model learned from limited training data successfully generates human-likebehaviors in unseen scenarios and outperforms both baselines.

Learning Robot Activities from First-Person Human Videos Using  Convolutional Future Regression

  We design a new approach that allows robot learning of new activities fromunlabeled human example videos. Given videos of humans executing the sameactivity from a human's viewpoint (i.e., first-person videos), our objective isto make the robot learn the temporal structure of the activity as its futureregression network, and learn to transfer such model for its own motorexecution. We present a new deep learning model: We extend the state-of-the-artconvolutional object detection network for the representation/estimation ofhuman hands in training videos, and newly introduce the concept of using afully convolutional network to regress (i.e., predict) the intermediate scenerepresentation corresponding to the future frame (e.g., 1-2 seconds later).Combining these allows direct prediction of future locations of human hands andobjects, which enables the robot to infer the motor control plan using ourmanipulation network. We experimentally confirm that our approach makeslearning of robot activities from unlabeled human interaction videos possible,and demonstrate that our robot is able to execute the learned collaborativeactivities in real-time directly based on its camera input.

Identifying First-person Camera Wearers in Third-person Videos

  We consider scenarios in which we wish to perform joint scene understanding,object tracking, activity recognition, and other tasks in environments in whichmultiple people are wearing body-worn cameras while a third-person staticcamera also captures the scene. To do this, we need to establish person-levelcorrespondences across first- and third-person videos, which is challengingbecause the camera wearer is not visible from his/her own egocentric video,preventing the use of direct feature matching. In this paper, we propose a newsemi-Siamese Convolutional Neural Network architecture to address this novelchallenge. We formulate the problem as learning a joint embedding space forfirst- and third-person videos that considers both spatial- and motion-domaincues. A new triplet loss function is designed to minimize the distance betweencorrect first- and third-person matches while maximizing the distance betweenincorrect ones. This end-to-end approach performs significantly better thanseveral baselines, in part by learning the first- and third-person featuresoptimized for matching jointly with the distance measure itself.

Forecasting Hands and Objects in Future Frames

  This paper presents an approach to forecast future presence and location ofhuman hands and objects. Given an image frame, the goal is to predict whatobjects will appear in the future frame (e.g., 5 seconds later) and where theywill be located at, even when they are not visible in the current frame. Thekey idea is that (1) an intermediate representation of a convolutional objectrecognition model abstracts scene information in its frame and that (2) we canpredict (i.e., regress) such representations corresponding to the future framesbased on that of the current frame. We design a new two-stream convolutionalneural network (CNN) architecture for videos by extending the state-of-the-artconvolutional object detection network, and present a new fully convolutionalregression network for predicting future scene representations. Our experimentsconfirm that combining the regressed future representation with our detectionnetwork allows reliable estimation of future hands and objects in videos. Weobtain much higher accuracy compared to the state-of-the-art future objectpresence forecast method on a public dataset.

Learning Latent Super-Events to Detect Multiple Activities in Videos

  In this paper, we introduce the concept of learning latent super-events fromactivity videos, and present how it benefits activity detection in continuousvideos. We define a super-event as a set of multiple events occurring togetherin videos with a particular temporal organization; it is the opposite conceptof sub-events. Real-world videos contain multiple activities and are rarelysegmented (e.g., surveillance videos), and learning latent super-events allowsthe model to capture how the events are temporally related in videos. We designtemporal structure filters that enable the model to focus on particularsub-intervals of the videos, and use them together with a soft attentionmechanism to learn representations of latent super-events. Super-eventrepresentations are combined with per-frame or per-segment CNNs to provideframe-level annotations. Our approach is designed to be fully differentiable,enabling end-to-end learning of latent super-event representations jointly withthe activity detector using them. Our experiments with multiple public videodatasets confirm that the proposed concept of latent super-event learningsignificantly benefits activity detection, advancing the state-of-the-arts.

Learning to Anonymize Faces for Privacy Preserving Action Detection

  There is an increasing concern in computer vision devices invading users'privacy by recording unwanted videos. On the one hand, we want the camerasystems to recognize important events and assist human daily lives byunderstanding its videos, but on the other hand we want to ensure that they donot intrude people's privacy. In this paper, we propose a new principledapproach for learning a video \emph{face anonymizer}. We use an adversarialtraining setting in which two competing systems fight: (1) a video anonymizerthat modifies the original video to remove privacy-sensitive information whilestill trying to maximize spatial action detection performance, and (2) adiscriminator that tries to extract privacy-sensitive information from theanonymized videos. The end result is a video anonymizer that performspixel-level modifications to anonymize each person's face, with minimal effecton action detection performance. We experimentally confirm the benefits of ourapproach compared to conventional hand-crafted anonymization methods includingmasking, blurring, and noise adding. Code, demo, and more results can be foundon our project page https://jason718.github.io/project/privacy/main.html.

Learning Real-World Robot Policies by Dreaming

  Learning to control robots directly based on images is a primary challenge inrobotics. However, many existing reinforcement learning approaches requireiteratively obtaining millions of samples to learn a policy which can takesignificant time. In this paper, we focus on the problem of learning real-worldrobot action policies solely based on a few random off-policy initial samples.We learn a realistic dreaming model that can emulate samples equivalent to asequence of images from the actual environment, and make the agent learn actionpolicies by interacting with the dreaming model rather than the real-world. Weexperimentally confirm that our dreaming model can learn realistic policiesthat transfer to the real-world.

Representation Flow for Action Recognition

  In this paper, we propose a convolutional layer inspired by optical flowalgorithms to learn motion representations. Our representation flow layer is afully-differentiable layer designed to capture the `flow' of any representationchannel within a convolutional neural network for action recognition. Itsparameters for iterative flow optimization are learned in an end-to-end fashiontogether with the other CNN model parameters, maximizing the action recognitionperformance. Furthermore, we newly introduce the concept of learning `flow offlow' representations by stacking multiple representation flow layers. Weconducted extensive experimental evaluations, confirming its advantages overprevious recognition models using traditional optical flows in bothcomputational speed and performance. Code/models available here:https://piergiaj.github.io/rep-flow-site/

Musical Chair: Efficient Real-Time Recognition Using Collaborative IoT  Devices

  The prevalence of Internet of things (IoT) devices and abundance of sensordata has created an increase in real-time data processing such as recognitionof speech, image, and video. While currently such processes are offloaded tothe computationally powerful cloud system, a localized and distributed approachis desirable because (i) it preserves the privacy of users and (ii) it omitsthe dependency on cloud services. However, IoT networks are usually composed ofresource-constrained devices, and a single device is not powerful enough toprocess real-time data. To overcome this challenge, we examine data and modelparallelism for such devices in the context of deep neural networks. We proposeMusical Chair to enable efficient, localized, and dynamic real-time recognitionby harvesting the aggregated computational power from the resource-constraineddevices in the same IoT network as input sensors. Musical chair adapts to theavailability of computing devices at runtime and adjusts to the inheritdynamics of IoT networks. To demonstrate Musical Chair, on a network ofRaspberry PIs (up to 12) each connected to a camera, we implement astate-of-the-art action recognition model for videos and two recognition modelsfor images. Compared to the Tegra TX2, an embedded low-power platform with asix-core CPU and a GPU, our distributed action recognition system achieves notonly similar energy consumption but also twice the performance of the TX2.Furthermore, in image recognition, Musical Chair achieves similar performanceand saves dynamic energy.

Joint Person Segmentation and Identification in Synchronized First- and  Third-person Videos

  In a world of pervasive cameras, public spaces are often captured frommultiple perspectives by cameras of different types, both fixed and mobile. Animportant problem is to organize these heterogeneous collections of videos byfinding connections between them, such as identifying correspondences betweenthe people appearing in the videos and the people holding or wearing thecameras. In this paper, we wish to solve two specific problems: (1) given twoor more synchronized third-person videos of a scene, produce a pixel-levelsegmentation of each visible person and identify corresponding people acrossdifferent views (i.e., determine who in camera A corresponds with whom incamera B), and (2) given one or more synchronized third-person videos as wellas a first-person video taken by a mobile or wearable camera, segment andidentify the camera wearer in the third-person videos. Unlike previous workwhich requires ground truth bounding boxes to estimate the correspondences, weperform person segmentation and identification jointly. We find that solvingthese two problems simultaneously is mutually beneficial, because betterfine-grained segmentation allows us to better perform matching across views,and information from multiple views helps us perform more accuratesegmentation. We evaluate our approach on two challenging datasets ofinteracting people captured from multiple wearable cameras, and show that ourproposed method performs significantly better than the state-of-the-art on bothperson segmentation and identification.

