Lagrangian Mean Curvature flow for entire Lipschitz graphs II

  We prove longtime existence and estimates for solutions to a fully nonlinear
Lagrangian parabolic equation with locally $C^{1,1}$ initial data $u_0$
satisfying either (1) $-(1+\eta) I_n\leq D^2u_0 \leq (1+\eta)I_n$ for some
positive dimensional constant $\eta$, (2) $u_0$ is weakly convex everywhere or
(3) $u_0$ satisfies a large supercritical Lagrangian phase condition.


Semantic See-Through Rendering on Light Fields

  We present a novel semantic light field (LF) refocusing technique that can
achieve unprecedented see-through quality. Different from prior art, our
semantic see-through (SST) differentiates rays in their semantic meaning and
depth. Specifically, we combine deep learning and stereo matching to provide
each ray a semantic label. We then design tailored weighting schemes for
blending the rays. Although simple, our solution can effectively remove
foreground residues when focusing on the background. At the same time, SST
maintains smooth transitions in varying focal depths. Comprehensive experiments
on synthetic and new real indoor and outdoor datasets demonstrate the
effectiveness and usefulness of our technique.


Hamiltonian stationary cones with isotropic links

  We show that any closed oriented immersed Hamiltonian stationary isotropic
surface $\Sigma$ with genus $g_{\Sigma}$ in $S^{5}\subset\mathbb{C}^{3}$ is (1)
Legendrian and minimal if $g_{\Sigma}=0$; (2) either Legendrian or with exactly
$2g_{\Sigma}-2$ Legendrian points if $g_{\Sigma}\geq1.$ In general, every
compact oriented immersed isotropic submanifold $L^{n-1}\subset
S^{2n-1}\subset\mathbb{C}^{n}$ such that the cone $C\left( L^{n-1}\right) $ is
Hamiltonian stationary must be Legendrian and minimal if its first Betti number
is zero. Corresponding results for non-orientable links are also provided.


Rigidity of Entire self-shrinking solutions to curvature flows

  We show that (a) any entire graphic self-shrinking solution to the Lagrangian
mean curvature flow in ${\mathbb C}^{m}$ with the Euclidean metric is flat; (b)
any space-like entire graphic self-shrinking solution to the Lagrangian mean
curvature flow in ${\mathbb C}^{m}$ with the pseudo-Euclidean metric is flat if
the Hessian of the potential is bounded below quadratically; and (c) the
Hermitian counterpart of (b) for the K\"ahler Ricci flow.


Automatic Layer Separation using Light Field Imaging

  We propose a novel approach that jointly removes reflection or translucent
layer from a scene and estimates scene depth. The input data are captured via
light field imaging. The problem is couched as minimizing the rank of the
transmitted scene layer via Robust Principle Component Analysis (RPCA). We also
impose regularization based on piecewise smoothness, gradient sparsity, and
layer independence to simultaneously recover 3D geometry of the transmitted
layer. Experimental results on synthetic and real data show that our technique
is robust and reliable, and can handle a broad range of layer separation
problems.


Zeta Function Regularization of Photon Polarization Tensor for a
  Magnetized Vacuum

  In this paper, we have developed a systematic technique to regularize double
summations of Landau levels and analytically evaluated the photon vacuum
polarization at an external magnetic field. The final results are described by
Lerch transcendent $\Phi(z,s,v)$ or its $z$-derivation. We have found that the
tensor of vacuum polarization is split into not only longitudinal and
transverse parts but also another mixture component. We have obtained a
complete expression of the magnetized photon vacuum polarization at any
kinematic regime and any strength of magnetic field for the first time. In the
weak $B$-fields, after canceling out a logarithmic counter term, all three
scalar functions are limited to the usual photon polarization tensor without
turning on magnetic field. In the strong $B$-fields, the calculations under
Lowest Landau Level approximation are only valid at the region $M^2\gg
q_{\shortparallel}^2$, but not correct while $q_{\shortparallel}^2\gg M^2$,
where, an imaginary part has been missed. It reminds us, a recalculation of the
gap equation under a full consideration of all Landau Levels is necessary in
the next future.


Robust 3D Human Motion Reconstruction Via Dynamic Template Construction

  In multi-view human body capture systems, the recovered 3D geometry or even
the acquired imagery data can be heavily corrupted due to occlusions, noise,
limited field of- view, etc. Direct estimation of 3D pose, body shape or motion
on these low-quality data has been traditionally challenging.In this paper, we
present a graph-based non-rigid shape registration framework that can
simultaneously recover 3D human body geometry and estimate pose/motion at high
fidelity.Our approach first generates a global full-body template by
registering all poses in the acquired motion sequence.We then construct a
deformable graph by utilizing the rigid components in the global template. We
directly warp the global template graph back to each motion frame in order to
fill in missing geometry. Specifically, we combine local rigidity and temporal
coherence constraints to maintain geometry and motion consistencies.
Comprehensive experiments on various scenes show that our method is accurate
and robust even in the presence of drastic motions.


Towards 3D Human Shape Recovery Under Clothing

  We present a learning-based scheme for robustly and accurately estimating
clothing fitness as well as the human shape on clothed 3D human scans. Our
approach maps the clothed human geometry to a geometry image that we call
clothed-GI. To align clothed-GI under different clothing, we extend the
parametric human model and employ skeleton detection and warping for reliable
alignment. For each pixel on the clothed-GI, we extract a feature vector
including color/texture, position, normal, etc. and train a modified
conditional GAN network for per-pixel fitness prediction using a comprehensive
3D clothing. Our technique significantly improves the accuracy of human shape
prediction, especially under loose and fitted clothing. We further demonstrate
using our results for human/clothing segmentation and virtual clothes fitting
at a high visual realism.


3D Face Reconstruction Using Color Photometric Stereo with Uncalibrated
  Near Point Lights

  We present a new color photometric stereo (CPS) method that can recover high
quality, detailed 3D face geometry in a single shot. Our system uses three
uncalibrated near point lights of different colors and a single camera. We
first utilize 3D morphable model (3DMM) and semantic segmentation of facial
parts to achieve robust self-calibration of light sources. We then address the
spectral ambiguity problem by incorporating albedo consensus, albedo
similarity, and proxy prior into a unified framework. We avoid the need for
spatial constancy of albedo and use a new measure for albedo similarity that is
based on the albedo norm profile. Experiments show that our new approach
produces state-of-the-art results in single image with high-fidelity geometry
that includes details such as wrinkles.


Non-Lambertian Surface Shape and Reflectance Reconstruction Using
  Concentric Multi-Spectral Light Field

  Recovering the shape and reflectance of non-Lambertian surfaces remains a
challenging problem in computer vision since the view-dependent appearance
invalidates traditional photo-consistency constraint. In this paper, we
introduce a novel concentric multi-spectral light field (CMSLF) design that is
able to recover the shape and reflectance of surfaces with arbitrary material
in one shot. Our CMSLF system consists of an array of cameras arranged on
concentric circles where each ring captures a specific spectrum. Coupled with a
multi-spectral ring light, we are able to sample viewpoint and lighting
variations in a single shot via spectral multiplexing. We further show that
such concentric camera/light setting results in a unique pattern of specular
changes across views that enables robust depth estimation. We formulate a
physical-based reflectance model on CMSLF to estimate depth and multi-spectral
reflectance map without imposing any surface prior. Extensive synthetic and
real experiments show that our method outperforms state-of-the-art light
field-based techniques, especially in non-Lambertian scenes.


A Precise Calculation of Delayed Coincidence Selection Efficiency and
  Accidental Coincidence Rate

  A model is proposed to address issues on the precise background evaluation
due to the complex data structure defined by the delayed coincidence method,
which is widely used in reactor electron-antineutrino oscillation experiments.
In this model, the effects from the muon veto, uncorrelated random background,
coincident signal and background are all studied with the analytical solutions,
simplifying the estimation of the systematic uncertainties of signal efficiency
and accidental background rate determined by the unstable single rate. The
result of calculation is validated numerically with a number of simulation
studies and is also applied and validated in the recent Daya Bay
hydrogen-capture based oscillation measurement.


Resolving Scale Ambiguity Via XSlit Aspect Ratio Analysis

  In perspective cameras, images of a frontal-parallel 3D object preserve its
aspect ratio invariant to its depth. Such an invariance is useful in
photography but is unique to perspective projection. In this paper, we show
that alternative non-perspective cameras such as the crossed-slit or XSlit
cameras exhibit a different depth-dependent aspect ratio (DDAR) property that
can be used to 3D recovery. We first conduct a comprehensive analysis to
characterize DDAR, infer object depth from its AR, and model recoverable depth
range, sensitivity, and error. We show that repeated shape patterns in real
Manhattan World scenes can be used for 3D reconstruction using a single XSlit
image. We also extend our analysis to model slopes of lines. Specifically,
parallel 3D lines exhibit depth-dependent slopes (DDS) on their images which
can also be used to infer their depths. We validate our analyses using real
XSlit cameras, XSlit panoramas, and catadioptric mirrors. Experiments show that
DDAR and DDS provide important depth cues and enable effective single-image
scene reconstruction.


Robust High Quality Image Guided Depth Upsampling

  Time-of-Flight (ToF) depth sensing camera is able to obtain depth maps at a
high frame rate. However, its low resolution and sensitivity to the noise are
always a concern. A popular solution is upsampling the obtained noisy low
resolution depth map with the guidance of the companion high resolution color
image. However, due to the constrains in the existing upsampling models, the
high resolution depth map obtained in such way may suffer from either texture
copy artifacts or blur of depth discontinuity. In this paper, a novel
optimization framework is proposed with the brand new data term and smoothness
term. The comprehensive experiments using both synthetic data and real data
show that the proposed method well tackles the problem of texture copy
artifacts and blur of depth discontinuity. It also demonstrates sufficient
robustness to the noise. Moreover, a data driven scheme is proposed to
adaptively estimate the parameter in the upsampling optimization framework. The
encouraging performance is maintained even in the case of large upsampling e.g.
$8\times$ and $16\times$.


Occlusion-Model Guided Anti-Occlusion Depth Estimation in Light Field

  Occlusion is one of the most challenging problems in depth estimation.
Previous work has modeled the single-occluder occlusion in light field and get
good results, however it is still difficult to obtain accurate depth for
multi-occluder occlusion. In this paper, we explore the multi-occluder
occlusion model in light field, and derive the occluder-consistency between the
spatial and angular space which is used as a guidance to select the un-occluded
views for each candidate occlusion point. Then an anti-occlusion energy
function is built to regularize depth map. The experimental results on public
light field datasets have demonstrated the advantages of the proposed algorithm
compared with other state-of-the-art light field depth estimation algorithms,
especially in multi-occluder areas.


A Learning-based Framework for Hybrid Depth-from-Defocus and Stereo
  Matching

  Depth from defocus (DfD) and stereo matching are two most studied passive
depth sensing schemes. The techniques are essentially complementary: DfD can
robustly handle repetitive textures that are problematic for stereo matching
whereas stereo matching is insensitive to defocus blurs and can handle large
depth range. In this paper, we present a unified learning-based technique to
conduct hybrid DfD and stereo matching. Our input is image triplets: a stereo
pair and a defocused image of one of the stereo views. We first apply
depth-guided light field rendering to construct a comprehensive training
dataset for such hybrid sensing setups. Next, we adopt the hourglass network
architecture to separately conduct depth inference from DfD and stereo.
Finally, we exploit different connection methods between the two separate
networks for integrating them into a unified solution to produce high fidelity
3D disparity maps. Comprehensive experiments on real and synthetic data show
that our new learning-based hybrid 3D sensing technique can significantly
improve accuracy and robustness in 3D reconstruction.


Hyperspectral Light Field Stereo Matching

  In this paper, we describe how scene depth can be extracted using a
hyperspectral light field capture (H-LF) system. Our H-LF system consists of a
5 x 6 array of cameras, with each camera sampling a different narrow band in
the visible spectrum. There are two parts to extracting scene depth. The first
part is our novel cross-spectral pairwise matching technique, which involves a
new spectral-invariant feature descriptor and its companion matching metric we
call bidirectional weighted normalized cross correlation (BWNCC). The second
part, namely, H-LF stereo matching, uses a combination of spectral-dependent
correspondence and defocus cues that rely on BWNCC. These two new cost terms
are integrated into a Markov Random Field (MRF) for disparity estimation.
Experiments on synthetic and real H-LF data show that our approach can produce
high-quality disparity maps. We also show that these results can be used to
produce the complete plenoptic cube in addition to synthesizing all-focus and
defocused color images under different sensor spectral responses.


Personalized Saliency and its Prediction

  Nearly all existing visual saliency models by far have focused on predicting
a universal saliency map across all observers. Yet psychology studies suggest
that visual attention of different observers can vary significantly under
specific circumstances, especially a scene is composed of multiple salient
objects. To study such heterogenous visual attention pattern across observers,
we first construct a personalized saliency dataset and explore correlations
between visual attention, personal preferences, and image contents.
Specifically, we propose to decompose a personalized saliency map (referred to
as PSM) into a universal saliency map (referred to as USM) predictable by
existing saliency detection models and a new discrepancy map across users that
characterizes personalized saliency. We then present two solutions towards
predicting such discrepancy maps, i.e., a multi-task convolutional neural
network (CNN) framework and an extended CNN with Person-specific Information
Encoded Filters (CNN-PIEF). Extensive experimental results demonstrate the
effectiveness of our models for PSM prediction as well their generalization
capability for unseen observers.


Deep Depth Inference using Binocular and Monocular Cues

  Human visual system relies on both binocular stereo cues and monocular
focusness cues to gain effective 3D perception. In computer vision, the two
problems are traditionally solved in separate tracks. In this paper, we present
a unified learning-based technique that simultaneously uses both types of cues
for depth inference. Specifically, we use a pair of focal stacks as input to
emulate human perception. We first construct a comprehensive focal stack
training dataset synthesized by depth-guided light field rendering. We then
construct three individual networks: a FocusNet to extract depth from a single
focal stack, a EDoFNet to obtain the extended depth of field (EDoF) image from
the focal stack, and a StereoNet to conduct stereo matching. We then integrate
them into a unified solution to obtain high quality depth maps. Comprehensive
experiments show that our approach outperforms the state-of-the-art in both
accuracy and speed and effectively emulates human vision systems.


Sparse Photometric 3D Face Reconstruction Guided by Morphable Models

  We present a novel 3D face reconstruction technique that leverages sparse
photometric stereo (PS) and latest advances on face registration/modeling from
a single image. We observe that 3D morphable faces approach provides a
reasonable geometry proxy for light position calibration. Specifically, we
develop a robust optimization technique that can calibrate per-pixel lighting
direction and illumination at a very high precision without assuming uniform
surface albedos. Next, we apply semantic segmentation on input images and the
geometry proxy to refine hairy vs. bare skin regions using tailored filters.
Experiments on synthetic and real data show that by using a very small set of
images, our technique is able to reconstruct fine geometric details such as
wrinkles, eyebrows, whelks, pores, etc, comparable to and sometimes surpassing
movie quality productions.


A Generic Multi-Projection-Center Model and Calibration Method for Light
  Field Cameras

  Light field cameras can capture both spatial and angular information of light
rays, enabling 3D reconstruction by a single exposure. The geometry of 3D
reconstruction is affected by intrinsic parameters of a light field camera
significantly. In the paper, we propose a multi-projection-center (MPC) model
with 6 intrinsic parameters to characterize light field cameras based on
traditional two-parallel-plane (TPP) representation. The MPC model can
generally parameterize light field in different imaging formations, including
conventional and focused light field cameras. By the constraints of 4D ray and
3D geometry, a 3D projective transformation is deduced to describe the
relationship between geometric structure and the MPC coordinates. Based on the
MPC model and projective transformation, we propose a calibration algorithm to
verify our light field camera model. Our calibration method includes a
close-form solution and a non-linear optimization by minimizing re-projection
errors. Experimental results on both simulated and real scene data have
verified the performance of our algorithm.


4D Human Body Correspondences from Panoramic Depth Maps

  The availability of affordable 3D full body reconstruction systems has given
rise to free-viewpoint video (FVV) of human shapes. Most existing solutions
produce temporally uncorrelated point clouds or meshes with unknown
point/vertex correspondences. Individually compressing each frame is
ineffective and still yields to ultra-large data sizes. We present an
end-to-end deep learning scheme to establish dense shape correspondences and
subsequently compress the data. Our approach uses sparse set of "panoramic"
depth maps or PDMs, each emulating an inward-viewing concentric mosaics. We
then develop a learning-based technique to learn pixel-wise feature descriptors
on PDMs. The results are fed into an autoencoder-based network for compression.
Comprehensive experiments demonstrate our solution is robust and effective on
both public and our newly captured datasets.


Deep Surface Light Fields

  A surface light field represents the radiance of rays originating from any
points on the surface in any directions. Traditional approaches require
ultra-dense sampling to ensure the rendering quality. In this paper, we present
a novel neural network based technique called deep surface light field or DSLF
to use only moderate sampling for high fidelity rendering. DSLF automatically
fills in the missing data by leveraging different sampling patterns across the
vertices and at the same time eliminates redundancies due to the network's
prediction capability. For real data, we address the image registration problem
as well as conduct texture-aware remeshing for aligning texture edges with
vertices to avoid blurring. Comprehensive experiments show that DSLF can
further achieve high data compression ratio while facilitating real-time
rendering on the GPU.


Photo-Realistic Facial Details Synthesis from Single Immage

  We present a single-image 3D face synthesis technique that can handle
challenging facial expressions while recovering fine geometric details. Our
technique employs expression analysis for proxy face geometry generation and
combines supervised and unsupervised learning for facial detail synthesis. On
proxy generation, we conduct emotion prediction to determine a new
expression-informed proxy. On detail synthesis, we present a Deep Facial Detail
Net (DFDN) based on Conditional Generative Adversarial Net (CGAN) that employs
both geometry and appearance loss functions. For geometry, we capture 366
high-quality 3D scans from 122 different subjects under 3 facial expressions.
For appearance, we use additional 20K in-the-wild face images and apply
image-based rendering to accommodate lighting variations. Comprehensive
experiments demonstrate that our framework can produce high-quality 3D faces
with realistic details under challenging facial expressions.


Generic Multiview Visual Tracking

  Recent progresses in visual tracking have greatly improved the tracking
performance. However, challenges such as occlusion and view change remain
obstacles in real world deployment. A natural solution to these challenges is
to use multiple cameras with multiview inputs, though existing systems are
mostly limited to specific targets (e.g. human), static cameras, and/or camera
calibration. To break through these limitations, we propose a generic multiview
tracking (GMT) framework that allows camera movement, while requiring neither
specific object model nor camera calibration. A key innovation in our framework
is a cross-camera trajectory prediction network (TPN), which implicitly and
dynamically encodes camera geometric relations, and hence addresses missing
target issues such as occlusion. Moreover, during tracking, we assemble
information across different cameras to dynamically update a novel
collaborative correlation filter (CCF), which is shared among cameras to
achieve robustness against view change. The two components are integrated into
a correlation filter tracking framework, where the features are trained offline
using existing single view tracking datasets. For evaluation, we first
contribute a new generic multiview tracking dataset (GMTD) with careful
annotations, and then run experiments on GMTD and the PETS2009 datasets. On
both datasets, the proposed GMT algorithm shows clear advantages over
state-of-the-art ones.


A Co-Prime Blur Scheme for Data Security in Video Surveillance

  This paper presents a novel Coprime Blurred Pair (CBP) model for visual
data-hiding for security in camera surveillance. While most previous approaches
have focused on completely encrypting the video stream, we introduce a spatial
encryption scheme by blurring the image/video contents to create a CBP. Our
goal is to obscure detail in public video streams by blurring while allowing
behavior to be recognized and to quickly deblur the stream so that details are
available if behavior is recognized as suspicious. We create a CBP by blurring
the same latent image with two unknown kernels. The two kernels are coprime
when mapped to bivariate polynomials in the z domain. To deblur the CBP we
first use the coprime constraint to approximate the kernels and sample the
bivariate CBP polynomials in one dimension on the unit circle. At each sample
point, we factor the 1D polynomial pair and compose the results into a 2D
kernel matrix. Finally, we compute the inverse Fast Fourier Transform (FFT) of
the kernel matrices to recover the coprime kernels and then the latent video
stream. It is therefore only possible to deblur the video stream if a user has
access to both streams. To improve the practicability of our algorithm, we
implement our algorithm using a graphics processing unit (GPU) to decrypt the
blurred video streams in real-time, and extensive experimental results
demonstrate that our new scheme can effectively protect sensitive identity
information in surveillance videos and faithfully reconstruct the unblurred
video stream when two blurred sequences are available.


Scene-adaptive Coded Apertures Imaging

  Coded aperture imaging systems have recently shown great success in
recovering scene depth and extending the depth-of-field. The ideal pattern,
however, would have to serve two conflicting purposes: 1) be broadband to
ensure robust deconvolution and 2) has sufficient zero-crossings for a high
depth discrepancy. This paper presents a simple but effective scene-adaptive
coded aperture solution to bridge this gap. We observe that the geometric
structures in a natural scene often exhibit only a few edge directions, and the
successive frames are closely correlated. Therefore we adopt a spatial
partitioning and temporal propagation scheme. In each frame, we address one
principal direction by applying depth-discriminative codes along it and
broadband codes along its orthogonal direction. Since within a frame only the
regions with edge direction corresponding to its aperture code behaves well, we
utilize the close among-frame correlation to propagate the high quality single
frame results temporally to obtain high performance over the whole image
lattice. To physically implement this scheme, we use a Liquid Crystal on
Silicon (LCoS) microdisplay that permits fast changing pattern codes. Firstly,
we capture the scene with a pinhole and analyze the scene content to determine
primary edge orientations. Secondly, we sequentially apply the proposed coding
scheme with these orientations in the following frames. Experiments on both
synthetic and real scenes show that our technique is able to combine advantages
of the state-of-the-art patterns for recovering better quality depth map and
all-focus images.


Robust Guided Image Filtering

  The process of using one image to guide the filtering process of another one
is called Guided Image Filtering (GIF). The main challenge of GIF is the
structure inconsistency between the guidance image and the target image.
Besides, noise in the target image is also a challenging issue especially when
it is heavy. In this paper, we propose a general framework for Robust Guided
Image Filtering (RGIF), which contains a data term and a smoothness term, to
solve the two issues mentioned above. The data term makes our model
simultaneously denoise the target image and perform GIF which is robust against
the heavy noise. The smoothness term is able to make use of the property of
both the guidance image and the target image which is robust against the
structure inconsistency. While the resulting model is highly non-convex, it can
be solved through the proposed Iteratively Re-weighted Least Squares (IRLS) in
an efficient manner. For challenging applications such as guided depth map
upsampling, we further develop a data-driven parameter optimization scheme to
properly determine the parameter in our model. This optimization scheme can
help to preserve small structures and sharp depth edges even for a large
upsampling factor (8x for example). Moreover, the specially designed structure
of the data term and the smoothness term makes our model perform well in
edge-preserving smoothing for single-image tasks (i.e., the guidance image is
the target image itself). This paper is an extension of our previous work [1],
[2].


Growth and Thermo-driven Crystalline Phase Transition of Metastable
  Monolayer 1T'-WSe2 Thin Film

  Two-dimensional (2D) transition metal dichalcogenides MX2 (M = Mo, W, X = S,
Se, Te) attracts enormous research interests in recent years. Its 2H phase
possesses an indirect to direct bandgap transition in 2D limit, and thus shows
great application potentials in optoelectronic devices [1]. The 1T' crystalline
phase transition can drive the monolayer MX2 to be a 2D topological insulator.
Here we realized the molecular beam epitaxial (MBE) growth of both the 1T' and
2H phase monolayer WSe2 on bilayer graphene (BLG) substrate. The crystalline
structures of these two phases were characterized using scanning tunneling
microscopy. The monolayer 1T'-WSe2 was found to be metastable, and can
transform into 2H phase under post-annealing procedure. The phase transition
temperature of 1T'-WSe2 grown on BLG is lower than that of 1T' phase grown on
2H-WSe2 layers. This thermo-driven crystalline phase transition makes the
monolayer WSe2 to be an ideal platform for the controlling of topological phase
transitions in 2D materials family.


Hair Segmentation on Time-of-Flight RGBD Images

  Robust segmentation of hair from portrait images remains challenging: hair
does not conform to a uniform shape, style or even color; dark hair in
particular lacks features. We present a novel computational imaging solution
that tackles the problem from both input and processing fronts. We explore
using Time-of-Flight (ToF) RGBD sensors on recent mobile devices. We first
conduct a comprehensive analysis to show that scattering and inter-reflection
cause different noise patterns on hair vs. non-hair regions on ToF images, by
changing the light path and/or combining multiple paths. We then develop a deep
network based approach that employs both ToF depth map and the RGB gradient
maps to produce an initial hair segmentation with labeled hair components. We
then refine the result by imposing ToF noise prior under the conditional random
field. We collect the first ToF RGBD hair dataset with 20k+ head images
captured on 30 human subjects with a variety of hairstyles at different view
angles. Comprehensive experiments show that our approach outperforms the RGB
based techniques in accuracy and robustness and can handle traditionally
challenging cases such as dark hair, similar hair/background, similar
hair/foreground, etc.


Large-Scale 3D Shape Reconstruction and Segmentation from ShapeNet
  Core55

  We introduce a large-scale 3D shape understanding benchmark using data and
annotation from ShapeNet 3D object database. The benchmark consists of two
tasks: part-level segmentation of 3D shapes and 3D reconstruction from single
view images. Ten teams have participated in the challenge and the best
performing teams have outperformed state-of-the-art approaches on both tasks. A
few novel deep learning architectures have been proposed on various 3D
representations on both tasks. We report the techniques used by each team and
the corresponding performances. In addition, we summarize the major discoveries
from the reported results and possible trends for the future work in the field.


Measurement of the Integrated Luminosities of Cross-section Scan Data
  Samples Around the $ψ(3770)$ Mass Region

  To investigate the nature of the $\psi(3770)$ resonance and to measure the
cross section for $e^+e^- \to D\bar{D}$, a cross-section scan data sample,
distributed among 41 center-of-mass energy points from 3.73 to 3.89~GeV, was
taken with the BESIII detector operated at the BEPCII collider in the year
2010. By analyzing the large angle Bhabha scattering events, we measure the
integrated luminosity of the data sample at each center-of-mass energy point.
The total integrated luminosity of the data sample is
$76.16\pm0.04\pm0.61$~pb$^{-1}$, where the first uncertainty is statistical and
the second systematic.


