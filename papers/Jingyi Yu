Lagrangian Mean Curvature flow for entire Lipschitz graphs II

  We prove longtime existence and estimates for solutions to a fully nonlinearLagrangian parabolic equation with locally $C^{1,1}$ initial data $u_0$satisfying either (1) $-(1+\eta) I_n\leq D^2u_0 \leq (1+\eta)I_n$ for somepositive dimensional constant $\eta$, (2) $u_0$ is weakly convex everywhere or(3) $u_0$ satisfies a large supercritical Lagrangian phase condition.

Semantic See-Through Rendering on Light Fields

  We present a novel semantic light field (LF) refocusing technique that canachieve unprecedented see-through quality. Different from prior art, oursemantic see-through (SST) differentiates rays in their semantic meaning anddepth. Specifically, we combine deep learning and stereo matching to provideeach ray a semantic label. We then design tailored weighting schemes forblending the rays. Although simple, our solution can effectively removeforeground residues when focusing on the background. At the same time, SSTmaintains smooth transitions in varying focal depths. Comprehensive experimentson synthetic and new real indoor and outdoor datasets demonstrate theeffectiveness and usefulness of our technique.

Hamiltonian stationary cones with isotropic links

  We show that any closed oriented immersed Hamiltonian stationary isotropicsurface $\Sigma$ with genus $g_{\Sigma}$ in $S^{5}\subset\mathbb{C}^{3}$ is (1)Legendrian and minimal if $g_{\Sigma}=0$; (2) either Legendrian or with exactly$2g_{\Sigma}-2$ Legendrian points if $g_{\Sigma}\geq1.$ In general, everycompact oriented immersed isotropic submanifold $L^{n-1}\subsetS^{2n-1}\subset\mathbb{C}^{n}$ such that the cone $C\left( L^{n-1}\right) $ isHamiltonian stationary must be Legendrian and minimal if its first Betti numberis zero. Corresponding results for non-orientable links are also provided.

Rigidity of Entire self-shrinking solutions to curvature flows

  We show that (a) any entire graphic self-shrinking solution to the Lagrangianmean curvature flow in ${\mathbb C}^{m}$ with the Euclidean metric is flat; (b)any space-like entire graphic self-shrinking solution to the Lagrangian meancurvature flow in ${\mathbb C}^{m}$ with the pseudo-Euclidean metric is flat ifthe Hessian of the potential is bounded below quadratically; and (c) theHermitian counterpart of (b) for the K\"ahler Ricci flow.

Automatic Layer Separation using Light Field Imaging

  We propose a novel approach that jointly removes reflection or translucentlayer from a scene and estimates scene depth. The input data are captured vialight field imaging. The problem is couched as minimizing the rank of thetransmitted scene layer via Robust Principle Component Analysis (RPCA). We alsoimpose regularization based on piecewise smoothness, gradient sparsity, andlayer independence to simultaneously recover 3D geometry of the transmittedlayer. Experimental results on synthetic and real data show that our techniqueis robust and reliable, and can handle a broad range of layer separationproblems.

Zeta Function Regularization of Photon Polarization Tensor for a  Magnetized Vacuum

  In this paper, we have developed a systematic technique to regularize doublesummations of Landau levels and analytically evaluated the photon vacuumpolarization at an external magnetic field. The final results are described byLerch transcendent $\Phi(z,s,v)$ or its $z$-derivation. We have found that thetensor of vacuum polarization is split into not only longitudinal andtransverse parts but also another mixture component. We have obtained acomplete expression of the magnetized photon vacuum polarization at anykinematic regime and any strength of magnetic field for the first time. In theweak $B$-fields, after canceling out a logarithmic counter term, all threescalar functions are limited to the usual photon polarization tensor withoutturning on magnetic field. In the strong $B$-fields, the calculations underLowest Landau Level approximation are only valid at the region $M^2\ggq_{\shortparallel}^2$, but not correct while $q_{\shortparallel}^2\gg M^2$,where, an imaginary part has been missed. It reminds us, a recalculation of thegap equation under a full consideration of all Landau Levels is necessary inthe next future.

Robust 3D Human Motion Reconstruction Via Dynamic Template Construction

  In multi-view human body capture systems, the recovered 3D geometry or eventhe acquired imagery data can be heavily corrupted due to occlusions, noise,limited field of- view, etc. Direct estimation of 3D pose, body shape or motionon these low-quality data has been traditionally challenging.In this paper, wepresent a graph-based non-rigid shape registration framework that cansimultaneously recover 3D human body geometry and estimate pose/motion at highfidelity.Our approach first generates a global full-body template byregistering all poses in the acquired motion sequence.We then construct adeformable graph by utilizing the rigid components in the global template. Wedirectly warp the global template graph back to each motion frame in order tofill in missing geometry. Specifically, we combine local rigidity and temporalcoherence constraints to maintain geometry and motion consistencies.Comprehensive experiments on various scenes show that our method is accurateand robust even in the presence of drastic motions.

Towards 3D Human Shape Recovery Under Clothing

  We present a learning-based scheme for robustly and accurately estimatingclothing fitness as well as the human shape on clothed 3D human scans. Ourapproach maps the clothed human geometry to a geometry image that we callclothed-GI. To align clothed-GI under different clothing, we extend theparametric human model and employ skeleton detection and warping for reliablealignment. For each pixel on the clothed-GI, we extract a feature vectorincluding color/texture, position, normal, etc. and train a modifiedconditional GAN network for per-pixel fitness prediction using a comprehensive3D clothing. Our technique significantly improves the accuracy of human shapeprediction, especially under loose and fitted clothing. We further demonstrateusing our results for human/clothing segmentation and virtual clothes fittingat a high visual realism.

3D Face Reconstruction Using Color Photometric Stereo with Uncalibrated  Near Point Lights

  We present a new color photometric stereo (CPS) method that can recover highquality, detailed 3D face geometry in a single shot. Our system uses threeuncalibrated near point lights of different colors and a single camera. Wefirst utilize 3D morphable model (3DMM) and semantic segmentation of facialparts to achieve robust self-calibration of light sources. We then address thespectral ambiguity problem by incorporating albedo consensus, albedosimilarity, and proxy prior into a unified framework. We avoid the need forspatial constancy of albedo and use a new measure for albedo similarity that isbased on the albedo norm profile. Experiments show that our new approachproduces state-of-the-art results in single image with high-fidelity geometrythat includes details such as wrinkles.

Non-Lambertian Surface Shape and Reflectance Reconstruction Using  Concentric Multi-Spectral Light Field

  Recovering the shape and reflectance of non-Lambertian surfaces remains achallenging problem in computer vision since the view-dependent appearanceinvalidates traditional photo-consistency constraint. In this paper, weintroduce a novel concentric multi-spectral light field (CMSLF) design that isable to recover the shape and reflectance of surfaces with arbitrary materialin one shot. Our CMSLF system consists of an array of cameras arranged onconcentric circles where each ring captures a specific spectrum. Coupled with amulti-spectral ring light, we are able to sample viewpoint and lightingvariations in a single shot via spectral multiplexing. We further show thatsuch concentric camera/light setting results in a unique pattern of specularchanges across views that enables robust depth estimation. We formulate aphysical-based reflectance model on CMSLF to estimate depth and multi-spectralreflectance map without imposing any surface prior. Extensive synthetic andreal experiments show that our method outperforms state-of-the-art lightfield-based techniques, especially in non-Lambertian scenes.

A Precise Calculation of Delayed Coincidence Selection Efficiency and  Accidental Coincidence Rate

  A model is proposed to address issues on the precise background evaluationdue to the complex data structure defined by the delayed coincidence method,which is widely used in reactor electron-antineutrino oscillation experiments.In this model, the effects from the muon veto, uncorrelated random background,coincident signal and background are all studied with the analytical solutions,simplifying the estimation of the systematic uncertainties of signal efficiencyand accidental background rate determined by the unstable single rate. Theresult of calculation is validated numerically with a number of simulationstudies and is also applied and validated in the recent Daya Bayhydrogen-capture based oscillation measurement.

Resolving Scale Ambiguity Via XSlit Aspect Ratio Analysis

  In perspective cameras, images of a frontal-parallel 3D object preserve itsaspect ratio invariant to its depth. Such an invariance is useful inphotography but is unique to perspective projection. In this paper, we showthat alternative non-perspective cameras such as the crossed-slit or XSlitcameras exhibit a different depth-dependent aspect ratio (DDAR) property thatcan be used to 3D recovery. We first conduct a comprehensive analysis tocharacterize DDAR, infer object depth from its AR, and model recoverable depthrange, sensitivity, and error. We show that repeated shape patterns in realManhattan World scenes can be used for 3D reconstruction using a single XSlitimage. We also extend our analysis to model slopes of lines. Specifically,parallel 3D lines exhibit depth-dependent slopes (DDS) on their images whichcan also be used to infer their depths. We validate our analyses using realXSlit cameras, XSlit panoramas, and catadioptric mirrors. Experiments show thatDDAR and DDS provide important depth cues and enable effective single-imagescene reconstruction.

Robust High Quality Image Guided Depth Upsampling

  Time-of-Flight (ToF) depth sensing camera is able to obtain depth maps at ahigh frame rate. However, its low resolution and sensitivity to the noise arealways a concern. A popular solution is upsampling the obtained noisy lowresolution depth map with the guidance of the companion high resolution colorimage. However, due to the constrains in the existing upsampling models, thehigh resolution depth map obtained in such way may suffer from either texturecopy artifacts or blur of depth discontinuity. In this paper, a noveloptimization framework is proposed with the brand new data term and smoothnessterm. The comprehensive experiments using both synthetic data and real datashow that the proposed method well tackles the problem of texture copyartifacts and blur of depth discontinuity. It also demonstrates sufficientrobustness to the noise. Moreover, a data driven scheme is proposed toadaptively estimate the parameter in the upsampling optimization framework. Theencouraging performance is maintained even in the case of large upsampling e.g.$8\times$ and $16\times$.

Occlusion-Model Guided Anti-Occlusion Depth Estimation in Light Field

  Occlusion is one of the most challenging problems in depth estimation.Previous work has modeled the single-occluder occlusion in light field and getgood results, however it is still difficult to obtain accurate depth formulti-occluder occlusion. In this paper, we explore the multi-occluderocclusion model in light field, and derive the occluder-consistency between thespatial and angular space which is used as a guidance to select the un-occludedviews for each candidate occlusion point. Then an anti-occlusion energyfunction is built to regularize depth map. The experimental results on publiclight field datasets have demonstrated the advantages of the proposed algorithmcompared with other state-of-the-art light field depth estimation algorithms,especially in multi-occluder areas.

A Learning-based Framework for Hybrid Depth-from-Defocus and Stereo  Matching

  Depth from defocus (DfD) and stereo matching are two most studied passivedepth sensing schemes. The techniques are essentially complementary: DfD canrobustly handle repetitive textures that are problematic for stereo matchingwhereas stereo matching is insensitive to defocus blurs and can handle largedepth range. In this paper, we present a unified learning-based technique toconduct hybrid DfD and stereo matching. Our input is image triplets: a stereopair and a defocused image of one of the stereo views. We first applydepth-guided light field rendering to construct a comprehensive trainingdataset for such hybrid sensing setups. Next, we adopt the hourglass networkarchitecture to separately conduct depth inference from DfD and stereo.Finally, we exploit different connection methods between the two separatenetworks for integrating them into a unified solution to produce high fidelity3D disparity maps. Comprehensive experiments on real and synthetic data showthat our new learning-based hybrid 3D sensing technique can significantlyimprove accuracy and robustness in 3D reconstruction.

Hyperspectral Light Field Stereo Matching

  In this paper, we describe how scene depth can be extracted using ahyperspectral light field capture (H-LF) system. Our H-LF system consists of a5 x 6 array of cameras, with each camera sampling a different narrow band inthe visible spectrum. There are two parts to extracting scene depth. The firstpart is our novel cross-spectral pairwise matching technique, which involves anew spectral-invariant feature descriptor and its companion matching metric wecall bidirectional weighted normalized cross correlation (BWNCC). The secondpart, namely, H-LF stereo matching, uses a combination of spectral-dependentcorrespondence and defocus cues that rely on BWNCC. These two new cost termsare integrated into a Markov Random Field (MRF) for disparity estimation.Experiments on synthetic and real H-LF data show that our approach can producehigh-quality disparity maps. We also show that these results can be used toproduce the complete plenoptic cube in addition to synthesizing all-focus anddefocused color images under different sensor spectral responses.

Personalized Saliency and its Prediction

  Nearly all existing visual saliency models by far have focused on predictinga universal saliency map across all observers. Yet psychology studies suggestthat visual attention of different observers can vary significantly underspecific circumstances, especially a scene is composed of multiple salientobjects. To study such heterogenous visual attention pattern across observers,we first construct a personalized saliency dataset and explore correlationsbetween visual attention, personal preferences, and image contents.Specifically, we propose to decompose a personalized saliency map (referred toas PSM) into a universal saliency map (referred to as USM) predictable byexisting saliency detection models and a new discrepancy map across users thatcharacterizes personalized saliency. We then present two solutions towardspredicting such discrepancy maps, i.e., a multi-task convolutional neuralnetwork (CNN) framework and an extended CNN with Person-specific InformationEncoded Filters (CNN-PIEF). Extensive experimental results demonstrate theeffectiveness of our models for PSM prediction as well their generalizationcapability for unseen observers.

Deep Depth Inference using Binocular and Monocular Cues

  Human visual system relies on both binocular stereo cues and monocularfocusness cues to gain effective 3D perception. In computer vision, the twoproblems are traditionally solved in separate tracks. In this paper, we presenta unified learning-based technique that simultaneously uses both types of cuesfor depth inference. Specifically, we use a pair of focal stacks as input toemulate human perception. We first construct a comprehensive focal stacktraining dataset synthesized by depth-guided light field rendering. We thenconstruct three individual networks: a FocusNet to extract depth from a singlefocal stack, a EDoFNet to obtain the extended depth of field (EDoF) image fromthe focal stack, and a StereoNet to conduct stereo matching. We then integratethem into a unified solution to obtain high quality depth maps. Comprehensiveexperiments show that our approach outperforms the state-of-the-art in bothaccuracy and speed and effectively emulates human vision systems.

Sparse Photometric 3D Face Reconstruction Guided by Morphable Models

  We present a novel 3D face reconstruction technique that leverages sparsephotometric stereo (PS) and latest advances on face registration/modeling froma single image. We observe that 3D morphable faces approach provides areasonable geometry proxy for light position calibration. Specifically, wedevelop a robust optimization technique that can calibrate per-pixel lightingdirection and illumination at a very high precision without assuming uniformsurface albedos. Next, we apply semantic segmentation on input images and thegeometry proxy to refine hairy vs. bare skin regions using tailored filters.Experiments on synthetic and real data show that by using a very small set ofimages, our technique is able to reconstruct fine geometric details such aswrinkles, eyebrows, whelks, pores, etc, comparable to and sometimes surpassingmovie quality productions.

A Generic Multi-Projection-Center Model and Calibration Method for Light  Field Cameras

  Light field cameras can capture both spatial and angular information of lightrays, enabling 3D reconstruction by a single exposure. The geometry of 3Dreconstruction is affected by intrinsic parameters of a light field camerasignificantly. In the paper, we propose a multi-projection-center (MPC) modelwith 6 intrinsic parameters to characterize light field cameras based ontraditional two-parallel-plane (TPP) representation. The MPC model cangenerally parameterize light field in different imaging formations, includingconventional and focused light field cameras. By the constraints of 4D ray and3D geometry, a 3D projective transformation is deduced to describe therelationship between geometric structure and the MPC coordinates. Based on theMPC model and projective transformation, we propose a calibration algorithm toverify our light field camera model. Our calibration method includes aclose-form solution and a non-linear optimization by minimizing re-projectionerrors. Experimental results on both simulated and real scene data haveverified the performance of our algorithm.

4D Human Body Correspondences from Panoramic Depth Maps

  The availability of affordable 3D full body reconstruction systems has givenrise to free-viewpoint video (FVV) of human shapes. Most existing solutionsproduce temporally uncorrelated point clouds or meshes with unknownpoint/vertex correspondences. Individually compressing each frame isineffective and still yields to ultra-large data sizes. We present anend-to-end deep learning scheme to establish dense shape correspondences andsubsequently compress the data. Our approach uses sparse set of "panoramic"depth maps or PDMs, each emulating an inward-viewing concentric mosaics. Wethen develop a learning-based technique to learn pixel-wise feature descriptorson PDMs. The results are fed into an autoencoder-based network for compression.Comprehensive experiments demonstrate our solution is robust and effective onboth public and our newly captured datasets.

Deep Surface Light Fields

  A surface light field represents the radiance of rays originating from anypoints on the surface in any directions. Traditional approaches requireultra-dense sampling to ensure the rendering quality. In this paper, we presenta novel neural network based technique called deep surface light field or DSLFto use only moderate sampling for high fidelity rendering. DSLF automaticallyfills in the missing data by leveraging different sampling patterns across thevertices and at the same time eliminates redundancies due to the network'sprediction capability. For real data, we address the image registration problemas well as conduct texture-aware remeshing for aligning texture edges withvertices to avoid blurring. Comprehensive experiments show that DSLF canfurther achieve high data compression ratio while facilitating real-timerendering on the GPU.

Photo-Realistic Facial Details Synthesis from Single Immage

  We present a single-image 3D face synthesis technique that can handlechallenging facial expressions while recovering fine geometric details. Ourtechnique employs expression analysis for proxy face geometry generation andcombines supervised and unsupervised learning for facial detail synthesis. Onproxy generation, we conduct emotion prediction to determine a newexpression-informed proxy. On detail synthesis, we present a Deep Facial DetailNet (DFDN) based on Conditional Generative Adversarial Net (CGAN) that employsboth geometry and appearance loss functions. For geometry, we capture 366high-quality 3D scans from 122 different subjects under 3 facial expressions.For appearance, we use additional 20K in-the-wild face images and applyimage-based rendering to accommodate lighting variations. Comprehensiveexperiments demonstrate that our framework can produce high-quality 3D faceswith realistic details under challenging facial expressions.

Generic Multiview Visual Tracking

  Recent progresses in visual tracking have greatly improved the trackingperformance. However, challenges such as occlusion and view change remainobstacles in real world deployment. A natural solution to these challenges isto use multiple cameras with multiview inputs, though existing systems aremostly limited to specific targets (e.g. human), static cameras, and/or cameracalibration. To break through these limitations, we propose a generic multiviewtracking (GMT) framework that allows camera movement, while requiring neitherspecific object model nor camera calibration. A key innovation in our frameworkis a cross-camera trajectory prediction network (TPN), which implicitly anddynamically encodes camera geometric relations, and hence addresses missingtarget issues such as occlusion. Moreover, during tracking, we assembleinformation across different cameras to dynamically update a novelcollaborative correlation filter (CCF), which is shared among cameras toachieve robustness against view change. The two components are integrated intoa correlation filter tracking framework, where the features are trained offlineusing existing single view tracking datasets. For evaluation, we firstcontribute a new generic multiview tracking dataset (GMTD) with carefulannotations, and then run experiments on GMTD and the PETS2009 datasets. Onboth datasets, the proposed GMT algorithm shows clear advantages overstate-of-the-art ones.

A Co-Prime Blur Scheme for Data Security in Video Surveillance

  This paper presents a novel Coprime Blurred Pair (CBP) model for visualdata-hiding for security in camera surveillance. While most previous approacheshave focused on completely encrypting the video stream, we introduce a spatialencryption scheme by blurring the image/video contents to create a CBP. Ourgoal is to obscure detail in public video streams by blurring while allowingbehavior to be recognized and to quickly deblur the stream so that details areavailable if behavior is recognized as suspicious. We create a CBP by blurringthe same latent image with two unknown kernels. The two kernels are coprimewhen mapped to bivariate polynomials in the z domain. To deblur the CBP wefirst use the coprime constraint to approximate the kernels and sample thebivariate CBP polynomials in one dimension on the unit circle. At each samplepoint, we factor the 1D polynomial pair and compose the results into a 2Dkernel matrix. Finally, we compute the inverse Fast Fourier Transform (FFT) ofthe kernel matrices to recover the coprime kernels and then the latent videostream. It is therefore only possible to deblur the video stream if a user hasaccess to both streams. To improve the practicability of our algorithm, weimplement our algorithm using a graphics processing unit (GPU) to decrypt theblurred video streams in real-time, and extensive experimental resultsdemonstrate that our new scheme can effectively protect sensitive identityinformation in surveillance videos and faithfully reconstruct the unblurredvideo stream when two blurred sequences are available.

Scene-adaptive Coded Apertures Imaging

  Coded aperture imaging systems have recently shown great success inrecovering scene depth and extending the depth-of-field. The ideal pattern,however, would have to serve two conflicting purposes: 1) be broadband toensure robust deconvolution and 2) has sufficient zero-crossings for a highdepth discrepancy. This paper presents a simple but effective scene-adaptivecoded aperture solution to bridge this gap. We observe that the geometricstructures in a natural scene often exhibit only a few edge directions, and thesuccessive frames are closely correlated. Therefore we adopt a spatialpartitioning and temporal propagation scheme. In each frame, we address oneprincipal direction by applying depth-discriminative codes along it andbroadband codes along its orthogonal direction. Since within a frame only theregions with edge direction corresponding to its aperture code behaves well, weutilize the close among-frame correlation to propagate the high quality singleframe results temporally to obtain high performance over the whole imagelattice. To physically implement this scheme, we use a Liquid Crystal onSilicon (LCoS) microdisplay that permits fast changing pattern codes. Firstly,we capture the scene with a pinhole and analyze the scene content to determineprimary edge orientations. Secondly, we sequentially apply the proposed codingscheme with these orientations in the following frames. Experiments on bothsynthetic and real scenes show that our technique is able to combine advantagesof the state-of-the-art patterns for recovering better quality depth map andall-focus images.

Robust Guided Image Filtering

  The process of using one image to guide the filtering process of another oneis called Guided Image Filtering (GIF). The main challenge of GIF is thestructure inconsistency between the guidance image and the target image.Besides, noise in the target image is also a challenging issue especially whenit is heavy. In this paper, we propose a general framework for Robust GuidedImage Filtering (RGIF), which contains a data term and a smoothness term, tosolve the two issues mentioned above. The data term makes our modelsimultaneously denoise the target image and perform GIF which is robust againstthe heavy noise. The smoothness term is able to make use of the property ofboth the guidance image and the target image which is robust against thestructure inconsistency. While the resulting model is highly non-convex, it canbe solved through the proposed Iteratively Re-weighted Least Squares (IRLS) inan efficient manner. For challenging applications such as guided depth mapupsampling, we further develop a data-driven parameter optimization scheme toproperly determine the parameter in our model. This optimization scheme canhelp to preserve small structures and sharp depth edges even for a largeupsampling factor (8x for example). Moreover, the specially designed structureof the data term and the smoothness term makes our model perform well inedge-preserving smoothing for single-image tasks (i.e., the guidance image isthe target image itself). This paper is an extension of our previous work [1],[2].

Growth and Thermo-driven Crystalline Phase Transition of Metastable  Monolayer 1T'-WSe2 Thin Film

  Two-dimensional (2D) transition metal dichalcogenides MX2 (M = Mo, W, X = S,Se, Te) attracts enormous research interests in recent years. Its 2H phasepossesses an indirect to direct bandgap transition in 2D limit, and thus showsgreat application potentials in optoelectronic devices [1]. The 1T' crystallinephase transition can drive the monolayer MX2 to be a 2D topological insulator.Here we realized the molecular beam epitaxial (MBE) growth of both the 1T' and2H phase monolayer WSe2 on bilayer graphene (BLG) substrate. The crystallinestructures of these two phases were characterized using scanning tunnelingmicroscopy. The monolayer 1T'-WSe2 was found to be metastable, and cantransform into 2H phase under post-annealing procedure. The phase transitiontemperature of 1T'-WSe2 grown on BLG is lower than that of 1T' phase grown on2H-WSe2 layers. This thermo-driven crystalline phase transition makes themonolayer WSe2 to be an ideal platform for the controlling of topological phasetransitions in 2D materials family.

Hair Segmentation on Time-of-Flight RGBD Images

  Robust segmentation of hair from portrait images remains challenging: hairdoes not conform to a uniform shape, style or even color; dark hair inparticular lacks features. We present a novel computational imaging solutionthat tackles the problem from both input and processing fronts. We exploreusing Time-of-Flight (ToF) RGBD sensors on recent mobile devices. We firstconduct a comprehensive analysis to show that scattering and inter-reflectioncause different noise patterns on hair vs. non-hair regions on ToF images, bychanging the light path and/or combining multiple paths. We then develop a deepnetwork based approach that employs both ToF depth map and the RGB gradientmaps to produce an initial hair segmentation with labeled hair components. Wethen refine the result by imposing ToF noise prior under the conditional randomfield. We collect the first ToF RGBD hair dataset with 20k+ head imagescaptured on 30 human subjects with a variety of hairstyles at different viewangles. Comprehensive experiments show that our approach outperforms the RGBbased techniques in accuracy and robustness and can handle traditionallychallenging cases such as dark hair, similar hair/background, similarhair/foreground, etc.

Large-Scale 3D Shape Reconstruction and Segmentation from ShapeNet  Core55

  We introduce a large-scale 3D shape understanding benchmark using data andannotation from ShapeNet 3D object database. The benchmark consists of twotasks: part-level segmentation of 3D shapes and 3D reconstruction from singleview images. Ten teams have participated in the challenge and the bestperforming teams have outperformed state-of-the-art approaches on both tasks. Afew novel deep learning architectures have been proposed on various 3Drepresentations on both tasks. We report the techniques used by each team andthe corresponding performances. In addition, we summarize the major discoveriesfrom the reported results and possible trends for the future work in the field.

Measurement of the Integrated Luminosities of Cross-section Scan Data  Samples Around the $ψ(3770)$ Mass Region

  To investigate the nature of the $\psi(3770)$ resonance and to measure thecross section for $e^+e^- \to D\bar{D}$, a cross-section scan data sample,distributed among 41 center-of-mass energy points from 3.73 to 3.89~GeV, wastaken with the BESIII detector operated at the BEPCII collider in the year2010. By analyzing the large angle Bhabha scattering events, we measure theintegrated luminosity of the data sample at each center-of-mass energy point.The total integrated luminosity of the data sample is$76.16\pm0.04\pm0.61$~pb$^{-1}$, where the first uncertainty is statistical andthe second systematic.

