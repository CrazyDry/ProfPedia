Stacked Approximated Regression Machine: A Simple Deep Learning Approach

  With the agreement of my coauthors, I Zhangyang Wang would like to withdrawthe manuscript "Stacked Approximated Regression Machine: A Simple Deep LearningApproach". Some experimental procedures were not included in the manuscript,which makes a part of important claims not meaningful. In the relevantresearch, I was solely responsible for carrying out the experiments; the othercoauthors joined in the discussions leading to the main algorithm.  Please see the updated text for more details.

Decentralized Recommender Systems

  This paper proposes a decentralized recommender system by formulating thepopular collaborative filleting (CF) model into a decentralized matrixcompletion form over a set of users. In such a way, data storages andcomputations are fully distributed. Each user could exchange limitedinformation with its local neighborhood, and thus it avoids the centralizedfusion. Advantages of the proposed system include a protection on user privacy,as well as better scalability and robustness. We compare our proposed algorithmwith several state-of-the-art algorithms on the FlickerUserFavor dataset, anddemonstrate that the decentralized algorithm can gain a competitive performanceto others.

Designing A Composite Dictionary Adaptively From Joint Examples

  We study the complementary behaviors of external and internal examples inimage restoration, and are motivated to formulate a composite dictionary designframework. The composite dictionary consists of the global part learned fromexternal examples, and the sample-specific part learned from internal examples.The dictionary atoms in both parts are further adaptively weighted to emphasizetheir model statistics. Experiments demonstrate that the joint utilization ofexternal and internal examples leads to substantial improvements, withsuccessful applications in image denoising and super resolution.

Learning Super-Resolution Jointly from External and Internal Examples

  Single image super-resolution (SR) aims to estimate a high-resolution (HR)image from a lowresolution (LR) input. Image priors are commonly learned toregularize the otherwise seriously ill-posed SR problem, either using externalLR-HR pairs or internal similar patterns. We propose joint SR to adaptivelycombine the advantages of both external and internal SR methods. We define twoloss functions using sparse coding based external examples, and epitomicmatching based on internal examples, as well as a corresponding adaptive weightto automatically balance their contributions according to their reconstructionerrors. Extensive SR results demonstrate the effectiveness of the proposedmethod over the existing state-of-the-art methods, and is also verified by oursubjective evaluation study.

Self-Tuned Deep Super Resolution

  Deep learning has been successfully applied to image super resolution (SR).In this paper, we propose a deep joint super resolution (DJSR) model to exploitboth external and self similarities for SR. A Stacked Denoising ConvolutionalAuto Encoder (SDCAE) is first pre-trained on external examples with proper dataaugmentations. It is then fine-tuned with multi-scale self examples from eachinput, where the reliability of self examples is explicitly taken into account.We also enhance the model performance by sub-model training and selection. TheDJSR model is extensively evaluated and compared with state-of-the-arts, andshow noticeable performance improvements both quantitatively and perceptuallyon a wide range of images.

Learning A Task-Specific Deep Architecture For Clustering

  While sparse coding-based clustering methods have shown to be successful,their bottlenecks in both efficiency and scalability limit the practical usage.In recent years, deep learning has been proved to be a highly effective,efficient and scalable feature learning tool. In this paper, we propose toemulate the sparse coding-based clustering pipeline in the context of deeplearning, leading to a carefully crafted deep model benefiting from both. Afeed-forward network structure, named TAGnet, is constructed based on agraph-regularized sparse coding algorithm. It is then trained withtask-specific loss functions from end to end. We discover that connecting deeplearning to sparse coding benefits not only the model performance, but also itsinitialization and interpretation. Moreover, by introducing auxiliaryclustering tasks to the intermediate feature hierarchy, we formulate DTAGnetand obtain a further performance boost. Extensive experiments demonstrate thatthe proposed model gains remarkable margins over several state-of-the-artmethods.

Real-World Font Recognition Using Deep Network and Domain Adaptation

  We address a challenging fine-grain classification problem: recognizing afont style from an image of text. In this task, it is very easy to generatelots of rendered font examples but very hard to obtain real-world labeledimages. This real-to-synthetic domain gap caused poor generalization to newreal data in previous methods (Chen et al. (2014)). In this paper, we refer toConvolutional Neural Networks, and use an adaptation technique based on aStacked Convolutional Auto-Encoder that exploits unlabeled real-world imagescombined with synthetic data. The proposed method achieves an accuracy ofhigher than 80% (top-5) on a real-world dataset.

Learning Deep $\ell_0$ Encoders

  Despite its nonconvex nature, $\ell_0$ sparse approximation is desirable inmany theoretical and application cases. We study the $\ell_0$ sparseapproximation problem with the tool of deep learning, by proposing Deep$\ell_0$ Encoders. Two typical forms, the $\ell_0$ regularized problem and the$M$-sparse problem, are investigated. Based on solid iterative algorithms, wemodel them as feed-forward neural networks, through introducing novel neuronsand pooling functions. Enforcing such structural priors acts as an effectivenetwork regularization. The deep encoders also enjoy faster inference, largerlearning capacity, and better scalability compared to conventional sparsecoding solutions. Furthermore, under task-driven losses, the models can beconveniently optimized from end to end. Numerical results demonstrate theimpressive performances of the proposed encoders.

$\mathbf{D^3}$: Deep Dual-Domain Based Fast Restoration of  JPEG-Compressed Images

  In this paper, we design a Deep Dual-Domain ($\mathbf{D^3}$) based fastrestoration model to remove artifacts of JPEG compressed images. It leveragesthe large learning capacity of deep networks, as well as the problem-specificexpertise that was hardly incorporated in the past design of deeparchitectures. For the latter, we take into consideration both the priorknowledge of the JPEG compression scheme, and the successful practice of thesparsity-based dual-domain approach. We further design the One-Step SparseInference (1-SI) module, as an efficient and light-weighted feed-forwardapproximation of sparse coding. Extensive experiments verify the superiority ofthe proposed $D^3$ model over several state-of-the-art methods. Specifically,our best model is capable of outperforming the latest deep model for around 1dB in PSNR, and is 30 times faster.

Studying Very Low Resolution Recognition Using Deep Networks

  Visual recognition research often assumes a sufficient resolution of theregion of interest (ROI). That is usually violated in practice, inspiring us toexplore the Very Low Resolution Recognition (VLRR) problem. Typically, the ROIin a VLRR problem can be smaller than $16 \times 16$ pixels, and is challengingto be recognized even by human experts. We attempt to solve the VLRR problemusing deep learning methods. Taking advantage of techniques primarily in superresolution, domain adaptation and robust regression, we formulate a dedicateddeep learning method and demonstrate how these techniques are incorporated stepby step. Any extra complexity, when introduced, is fully justified by bothanalysis and simulation results. The resulting \textit{Robust Partially CoupledNetworks} achieves feature enhancement and recognition simultaneously. Itallows for both the flexibility to combat the LR-HR domain mismatch, and therobustness to outliers. Finally, the effectiveness of the proposed models isevaluated on three different VLRR tasks, including face identification, digitrecognition and font recognition, all of which obtain very impressiveperformances.

Brain-Inspired Deep Networks for Image Aesthetics Assessment

  Image aesthetics assessment has been challenging due to its subjectivenature. Inspired by the scientific advances in the human visual perception andneuroaesthetics, we design Brain-Inspired Deep Networks (BDN) for this task.BDN first learns attributes through the parallel supervised pathways, on avariety of selected feature dimensions. A high-level synthesis network istrained to associate and transform those attributes into the overall aestheticsrating. We then extend BDN to predicting the distribution of human ratings,since aesthetics ratings are often subjective. Another highlight is ourfirst-of-its-kind study of label-preserving transformations in the context ofaesthetics assessment, which leads to an effective data augmentation approach.Experimental results on the AVA dataset show that our biological inspired andtask-specific BDN model gains significantly performance improvement, comparedto other state-of-the-art models with the same or higher parameter capacity.

Learning A Deep $\ell_\infty$ Encoder for Hashing

  We investigate the $\ell_\infty$-constrained representation whichdemonstrates robustness to quantization errors, utilizing the tool of deeplearning. Based on the Alternating Direction Method of Multipliers (ADMM), weformulate the original convex minimization problem as a feed-forward neuralnetwork, named \textit{Deep $\ell_\infty$ Encoder}, by introducing the novelBounded Linear Unit (BLU) neuron and modeling the Lagrange multipliers asnetwork biases. Such a structural prior acts as an effective networkregularization, and facilitates the model initialization. We then investigatethe effective use of the proposed model in the application of hashing, bycoupling the proposed encoders under a supervised pairwise loss, to develop a\textit{Deep Siamese $\ell_\infty$ Network}, which can be optimized from end toend. Extensive experiments demonstrate the impressive performances of theproposed model. We also provide an in-depth analysis of its behaviors againstthe competitors.

Deep Double Sparsity Encoder: Learning to Sparsify Not Only Features But  Also Parameters

  This paper emphasizes the significance to jointly exploit the problemstructure and the parameter structure, in the context of deep modeling. As aspecific and interesting example, we describe the deep double sparsity encoder(DDSE), which is inspired by the double sparsity model for dictionary learning.DDSE simultaneously sparsities the output features and the learned modelparameters, under one unified framework. In addition to its intuitive modelinterpretation, DDSE also possesses compact model size and low complexity.Extensive simulations compare DDSE with several carefully-designed baselines,and verify the consistently superior performance of DDSE. We further apply DDSEto the novel application domain of brain encoding, with promising preliminaryresults achieved.

Predicting Depression Severity by Multi-Modal Feature Engineering and  Fusion

  We present our preliminary work to determine if patient's vocal acoustic,linguistic, and facial patterns could predict clinical ratings of depressionseverity, namely Patient Health Questionnaire depression scale (PHQ-8). Weproposed a multi modal fusion model that combines three different modalities:audio, video , and text features. By training over AVEC 2017 data set, ourproposed model outperforms each single modality prediction model, and surpassesthe data set baseline with ice margin.

Decomposition-Based Domain Adaptation for Real-World Font Recognition

  We present a domain adaption framework to address a domain mismatch betweensynthetic training and real-world testing data. We demonstrate our method on achallenging fine-grain classification problem: recognizing a font style from animage of text. In this task, it is very easy to generate lots of rendered fontexamples but very hard to obtain real-world labeled images. Thisreal-to-synthetic domain gap caused poor generalization to new real data inprevious font recognition methods (Chen et al. (2014)). In this paper, weintroduce a Convolutional Neural Network decomposition approach, leveraging alarge training corpus of synthetic data to obtain effective features forclassification. This is done using an adaptation technique based on a StackedConvolutional Auto-Encoder that exploits a large collection of unlabeledreal-world text images combined with synthetic data preprocessed in a specificway. The proposed DeepFont method achieves an accuracy of higher than 80%(top-5) on a new large labeled real-world dataset we collected.

DeepFont: Identify Your Font from An Image

  As font is one of the core design concepts, automatic font identification andsimilar font suggestion from an image or photo has been on the wish list ofmany designers. We study the Visual Font Recognition (VFR) problem, and advancethe state-of-the-art remarkably by developing the DeepFont system. First ofall, we build up the first available large-scale VFR dataset, named AdobeVFR,consisting of both labeled synthetic data and partially labeled real-worlddata. Next, to combat the domain mismatch between available training andtesting data, we introduce a Convolutional Neural Network (CNN) decompositionapproach, using a domain adaptation technique based on a Stacked ConvolutionalAuto-Encoder (SCAE) that exploits a large corpus of unlabeled real-world textimages combined with synthetic data preprocessed in a specific way. Moreover,we study a novel learning-based model compression approach, in order to reducethe DeepFont model size without sacrificing its performance. The DeepFontsystem achieves an accuracy of higher than 80% (top-5) on our collecteddataset, and also produces a good font similarity measure for font selectionand suggestion. We also achieve around 6 times compression of the model withoutany visible loss of recognition accuracy.

Deep $k$-Means: Re-Training and Parameter Sharing with Harder Cluster  Assignments for Compressing Deep Convolutions

  The current trend of pushing CNNs deeper with convolutions has created apressing demand to achieve higher compression gains on CNNs where convolutionsdominate the computation and parameter amount (e.g., GoogLeNet, ResNet and WideResNet). Further, the high energy consumption of convolutions limits itsdeployment on mobile devices. To this end, we proposed a simple yet effectivescheme for compressing convolutions though applying k-means clustering on theweights, compression is achieved through weight-sharing, by only recording $K$cluster centers and weight assignment indexes. We then introduced a novelspectrally relaxed $k$-means regularization, which tends to make hardassignments of convolutional layer weights to $K$ learned cluster centersduring re-training. We additionally propose an improved set of metrics toestimate energy consumption of CNN hardware implementations, whose estimationresults are verified to be consistent with previously proposed energyestimation tool extrapolated from actual hardware measurements. We finallyevaluated Deep $k$-Means across several CNN models in terms of both compressionratio and energy consumption reduction, observing promising results withoutincurring accuracy loss. The code is available athttps://github.com/Sandbox3aster/Deep-K-Means

Towards Privacy-Preserving Visual Recognition via Adversarial Training:  A Pilot Study

  This paper aims to improve privacy-preserving visual recognition, anincreasingly demanded feature in smart camera applications, by formulating aunique adversarial training framework. The proposed framework explicitly learnsa degradation transform for the original video inputs, in order to optimize thetrade-off between target task performance and the associated privacy budgets onthe degraded video. A notable challenge is that the privacy budget, oftendefined and measured in task-driven contexts, cannot be reliably indicatedusing any single model performance, because a strong protection of privacy hasto sustain against any possible model that tries to hack privacy information.Such an uncommon situation has motivated us to propose two strategies, i.e.,budget model restarting and ensemble, to enhance the generalization of thelearned degradation on protecting privacy against unseen hacker models. Noveltraining strategies, evaluation protocols, and result visualization methodshave been designed accordingly. Two experiments on privacy-preserving actionrecognition, with privacy budgets defined in various ways, manifest thecompelling effectiveness of the proposed framework in simultaneouslymaintaining high target task (action recognition) performance while suppressingthe privacy breach risk.

Adversarially Trained Model Compression: When Robustness Meets  Efficiency

  The robustness of deep models to adversarial attacks has gained significantattention in recent years, so has the model compactness and efficiency: yet thetwo have been mostly studied separately, with few relationships drawn betweeneach other. This paper is concerned with: how can we combine the best of bothworlds, obtaining a robust and compact network? The answer is not asstraightforward as it may seem, since the two goals of model robustness andcompactness may contradict from time to time. We formally study this newquestion, by proposing a novel Adversarially Trained Model Compression (ATMC)framework. A unified constrained optimization formulation is designed, with anefficient algorithm developed. An extensive group of experiments are thencarefully designed and presented, demonstrating that ATMC obtains remarkablymore favorable trade-off among model size, accuracy and robustness, overcurrently available alternatives in various settings.

UnitBox: An Advanced Object Detection Network

  In present object detection systems, the deep convolutional neural networks(CNNs) are utilized to predict bounding boxes of object candidates, and havegained performance advantages over the traditional region proposal methods.However, existing deep CNN methods assume the object bounds to be fourindependent variables, which could be regressed by the $\ell_2$ lossseparately. Such an oversimplified assumption is contrary to the well-receivedobservation, that those variables are correlated, resulting to less accuratelocalization. To address the issue, we firstly introduce a novel Intersectionover Union ($IoU$) loss function for bounding box prediction, which regressesthe four bounds of a predicted box as a whole unit. By taking the advantages of$IoU$ loss and deep fully convolutional networks, the UnitBox is introduced,which performs accurate and efficient localization, shows robust to objects ofvaried shapes and scales, and converges fast. We apply UnitBox on facedetection task and achieve the best performance among all published methods onthe FDDB benchmark.

Task-driven Visual Saliency and Attention-based Visual Question  Answering

  Visual question answering (VQA) has witnessed great progress since May, 2015as a classic problem unifying visual and textual data into a system. Manyenlightening VQA works explore deep into the image and question encodings andfusing methods, of which attention is the most effective and infusivemechanism. Current attention based methods focus on adequate fusion of visualand textual features, but lack the attention to where people focus to askquestions about the image. Traditional attention based methods attach a singlevalue to the feature at each spatial location, which losses many usefulinformation. To remedy these problems, we propose a general method to performsaliency-like pre-selection on overlapped region features by the interrelationof bidirectional LSTM (BiLSTM), and use a novel element-wise multiplicationbased attention method to capture more competent correlation informationbetween visual and textual features. We conduct experiments on the large-scaleCOCO-VQA dataset and analyze the effectiveness of our model demonstrated bystrong empirical results.

When Image Denoising Meets High-Level Vision Tasks: A Deep Learning  Approach

  Conventionally, image denoising and high-level vision tasks are handledseparately in computer vision. In this paper, we cope with the two jointly andexplore the mutual influence between them. First we propose a convolutionalneural network for image denoising which achieves the state-of-the-artperformance. Second we propose a deep neural network solution that cascades twomodules for image denoising and various high-level tasks, respectively, and usethe joint loss for updating only the denoising network via back-propagation. Wedemonstrate that on one hand, the proposed denoiser has the generality toovercome the performance degradation of different high-level vision tasks. Onthe other hand, with the guidance of high-level vision information, thedenoising network can generate more visually appealing results. To the best ofour knowledge, this is the first work investigating the benefit of exploitingimage semantics simultaneously for image denoising and high-level vision tasksvia deep learning. The code is available onlinehttps://github.com/Ding-Liu/DeepDenoising.

An All-in-One Network for Dehazing and Beyond

  This paper proposes an image dehazing model built with a convolutional neuralnetwork (CNN), called All-in-One Dehazing Network (AOD-Net). It is designedbased on a re-formulated atmospheric scattering model. Instead of estimatingthe transmission matrix and the atmospheric light separately as most previousmodels did, AOD-Net directly generates the clean image through a light-weightCNN. Such a novel end-to-end design makes it easy to embed AOD-Net into otherdeep models, e.g., Faster R-CNN, for improving high-level task performance onhazy images. Experimental results on both synthesized and natural hazy imagedatasets demonstrate our superior performance than the state-of-the-art interms of PSNR, SSIM and the subjective visual quality. Furthermore, whenconcatenating AOD-Net with Faster R-CNN and training the joint pipeline fromend to end, we witness a large improvement of the object detection performanceon hazy images.

Robust Emotion Recognition from Low Quality and Low Bit Rate Video: A  Deep Learning Approach

  Emotion recognition from facial expressions is tremendously useful,especially when coupled with smart devices and wireless multimediaapplications. However, the inadequate network bandwidth often limits thespatial resolution of the transmitted video, which will heavily degrade therecognition reliability. We develop a novel framework to achieve robust emotionrecognition from low bit rate video. While video frames are downsampled at theencoder side, the decoder is embedded with a deep network model for jointsuper-resolution (SR) and recognition. Notably, we propose a novel max-mixtraining strategy, leading to a single "One-for-All" model that is remarkablyrobust to a vast range of downsampling factors. That makes our framework welladapted for the varied bandwidths in real transmission scenarios, withouthampering scalability or efficiency. The proposed framework is evaluated on theAVEC 2016 benchmark, and demonstrates significantly improved stand-alonerecognition performance, as well as rate-distortion (R-D) performance, thaneither directly recognizing from LR frames, or separating SR and recognition.

End-to-End United Video Dehazing and Detection

  The recent development of CNN-based image dehazing has revealed theeffectiveness of end-to-end modeling. However, extending the idea to end-to-endvideo dehazing has not been explored yet. In this paper, we propose anEnd-to-End Video Dehazing Network (EVD-Net), to exploit the temporalconsistency between consecutive video frames. A thorough study has beenconducted over a number of structure options, to identify the best temporalfusion strategy. Furthermore, we build an End-to-End United Video Dehazing andDetection Network(EVDD-Net), which concatenates and jointly trains EVD-Net witha video object detection model. The resulting augmented end-to-end pipeline hasdemonstrated much more stable and accurate detection results in hazy video.

Benchmarking Single Image Dehazing and Beyond

  We present a comprehensive study and evaluation of existing single imagedehazing algorithms, using a new large-scale benchmark consisting of bothsynthetic and real-world hazy images, called REalistic Single Image DEhazing(RESIDE). RESIDE highlights diverse data sources and image contents, and isdivided into five subsets, each serving different training or evaluationpurposes. We further provide a rich variety of criteria for dehazing algorithmevaluation, ranging from full-reference metrics, to no-reference metrics, tosubjective evaluation and the novel task-driven evaluation. Experiments onRESIDE shed light on the comparisons and limitations of state-of-the-artdehazing algorithms, and suggest promising future directions.

Enhance Visual Recognition under Adverse Conditions via Deep Networks

  Visual recognition under adverse conditions is a very important andchallenging problem of high practical value, due to the ubiquitous existence ofquality distortions during image acquisition, transmission, or storage. Whiledeep neural networks have been extensively exploited in the techniques oflow-quality image restoration and high-quality image recognition tasksrespectively, few studies have been done on the important problem ofrecognition from very low-quality images. This paper proposes a deep learningbased framework for improving the performance of image and video recognitionmodels under adverse conditions, using robust adverse pre-training or itsaggressive variant. The robust adverse pre-training algorithms leverage thepower of pre-training and generalizes conventional unsupervised pre-trainingand data augmentation methods. We further develop a transfer learning approachto cope with real-world datasets of unknown adverse conditions. The proposedframework is comprehensively evaluated on a number of image and videorecognition benchmarks, and obtains significant performance improvements undervarious single or mixed adverse conditions. Our visualization and analysisfurther add to the explainability of results.

Learning Simple Thresholded Features with Sparse Support Recovery

  The thresholded feature has recently emerged as an extremely efficient, yetrough empirical approximation, of the time-consuming sparse coding inferenceprocess. Such an approximation has not yet been rigorously examined, andstandard dictionaries often lead to non-optimal performance when used forcomputing thresholded features. In this paper, we first present two theoreticalrecovery guarantees for the thresholded feature to exactly recover the nonzerosupport of the sparse code. Motivated by them, we then formulate the DictionaryLearning for Thresholded Features (DLTF) model, which learns an optimizeddictionary for applying the thresholded feature. In particular, for the $(k,2)$ norm involved, a novel proximal operator with log-linear time complexity$O(m\log m)$ is derived. We evaluate the performance of DLTF on a vast range ofsynthetic and real-data tasks, where DLTF demonstrates remarkable efficiency,effectiveness and robustness in all experiments. In addition, we brieflydiscuss the potential link between DLTF and deep learning building blocks.

Improved Techniques for Learning to Dehaze and Beyond: A Collective  Study

  Here we explore two related but important tasks based on the recentlyreleased REalistic Single Image DEhazing (RESIDE) benchmark dataset: (i) singleimage dehazing as a low-level image restoration problem; and (ii) high-levelvisual understanding (e.g., object detection) of hazy images. For the firsttask, we investigated a variety of loss functions and show thatperception-driven loss significantly improves dehazing performance. In thesecond task, we provide multiple solutions including using advanced modules inthe dehazing-detection cascade and domain-adaptive object detectors. In bothtasks, our proposed solutions significantly improve performance. GitHubrepository URL is: https://github.com/guanlongzhao/dehaze

U-Finger: Multi-Scale Dilated Convolutional Network for Fingerprint  Image Denoising and Inpainting

  This paper studies the challenging problem of fingerprint image denoising andinpainting. To tackle the challenge of suppressing complicated artifacts (blur,brightness, contrast, elastic transformation, occlusion, scratch, resolution,rotation, and so on) while preserving fine textures, we develop a multi-scaleconvolutional network, termed U- Finger. Based on the domain expertise, we showthat the usage of dilated convolutions as well as the removal of padding haveimportant positive impacts on the final restoration performance, in addition tomulti-scale cascaded feature modules. Our model achieves the overall ranking ofNo.2 in the ECCV 2018 Chalearn LAP Inpainting Competition Track 3 (FingerprintDenoising and Inpainting). Among all participating teams, we obtain the MSE of0.0231 (rank 2), PSNR 16.9688 dB (rank 2), and SSIM 0.8093 (rank 3) on thehold-out testing set.

Theoretical Linear Convergence of Unfolded ISTA and its Practical  Weights and Thresholds

  In recent years, unfolding iterative algorithms as neural networks has becomean empirical success in solving sparse recovery problems. However, itstheoretical understanding is still immature, which prevents us from fullyutilizing the power of neural networks. In this work, we study unfolded ISTA(Iterative Shrinkage Thresholding Algorithm) for sparse signal recovery. Weintroduce a weight structure that is necessary for asymptotic convergence tothe true sparse signal. With this structure, unfolded ISTA can attain a linearconvergence, which is better than the sublinear convergence of ISTA/FISTA ingeneral cases. Furthermore, we propose to incorporate thresholding in thenetwork to perform support selection, which is easy to implement and able toboost the convergence rate both theoretically and empirically. Extensivesimulations, including sparse vector recovery and a compressive sensingexperiment on real image data, corroborate our theoretical results anddemonstrate their practical usefulness. We have made our codes publiclyavailable: https://github.com/xchen-tamu/linear-lista-cpss.

Can We Gain More from Orthogonality Regularizations in Training Deep  CNNs?

  This paper seeks to answer the question: as the (near-) orthogonality ofweights is found to be a favorable property for training deep convolutionalneural networks, how can we enforce it in more effective and easy-to-use ways?We develop novel orthogonality regularizations on training deep CNNs, utilizingvarious advanced analytical tools such as mutual coherence and restrictedisometry property. These plug-and-play regularizations can be convenientlyincorporated into training almost any CNN without extra hassle. We thenbenchmark their effects on state-of-the-art models: ResNet, WideResNet, andResNeXt, on several most popular computer vision datasets: CIFAR-10, CIFAR-100,SVHN and ImageNet. We observe consistent performance gains after applying thoseproposed regularizations, in terms of both the final accuracies achieved, andfaster and more stable convergences. We have made our codes and pre-trainedmodels publicly available:https://github.com/nbansal90/Can-we-Gain-More-from-Orthogonality.

Single Image Deraining: A Comprehensive Benchmark Analysis

  We present a comprehensive study and evaluation of existing single imagederaining algorithms, using a new large-scale benchmark consisting of bothsynthetic and real-world rainy images.This dataset highlights diverse datasources and image contents, and is divided into three subsets (rain streak,rain drop, rain and mist), each serving different training or evaluationpurposes. We further provide a rich variety of criteria for dehazing algorithmevaluation, ranging from full-reference metrics, to no-reference metrics, tosubjective evaluation and the novel task-driven evaluation. Experiments on thedataset shed light on the comparisons and limitations of state-of-the-artderaining algorithms, and suggest promising future directions.

Subspace Network: Deep Multi-Task Censored Regression for Modeling  Neurodegenerative Diseases

  Over the past decade a wide spectrum of machine learning models have beendeveloped to model the neurodegenerative diseases, associating biomarkers,especially non-intrusive neuroimaging markers, with key clinical scoresmeasuring the cognitive status of patients. Multi-task learning (MTL) has beencommonly utilized by these studies to address high dimensionality and smallcohort size challenges. However, most existing MTL approaches are based onlinear models and suffer from two major limitations: 1) they cannot explicitlyconsider upper/lower bounds in these clinical scores; 2) they lack thecapability to capture complicated non-linear interactions among the variables.In this paper, we propose Subspace Network, an efficient deep modeling approachfor non-linear multi-task censored regression. Each layer of the subspacenetwork performs a multi-task censored regression to improve upon thepredictions from the last layer via sketching a low-dimensional subspace toperform knowledge transfer among learning tasks. Under mild assumptions, foreach layer the parametric subspace can be recovered using only one pass oftraining data. Empirical results demonstrate that the proposed subspace networkquickly picks up the correct parameter subspaces, and outperformsstate-of-the-arts in predicting neurodegenerative clinical scores usinginformation in brain imaging.

$L_p$-Norm Constrained Coding With Frank-Wolfe Network

  We investigate the problem of $L_p$-norm constrained coding, i.e. convertingsignal into code that lies inside an $L_p$-ball and most faithfullyreconstructs the signal. While previous works known as sparse coding haveaddressed the cases of $L_0$ and $L_1$ norms, more general cases with other $p$values, especially with unknown $p$, remain a difficulty. We propose theFrank-Wolfe Network (F-W Net), whose architecture is inspired by unrolling andtruncating the Frank-Wolfe algorithm for solving an $L_p$-norm constrainedproblem. We show that the Frank-Wolfe solver for the $L_p$-norm constraintleads to a novel closed-form nonlinear unit, which is parameterized by $p$ andtermed $pool_p$. The $pool_p$ unit links the conventional pooling, activation,and normalization operations, making F-W Net distinct from existing deepnetworks either heuristically designed or converted from projected gradientdescent algorithms. We further show that the hyper-parameter $p$ can be madelearnable instead of pre-chosen in F-W Net, which gracefully solves the$L_p$-norm constrained coding problem with unknown $p$. We evaluate theperformance of F-W Net on an extensive range of simulations as well as the taskof handwritten digit recognition, where F-W Net exhibits strong learningcapability. We then propose a convolutional version of F-W Net, and apply theconvolutional F-W Net into image denoising and super-resolution tasks, whereF-W Net all demonstrates impressive effectiveness, flexibility, and robustness.

DeepAffinity: Interpretable Deep Learning of Compound-Protein Affinity  through Unified Recurrent and Convolutional Neural Networks

  Motivation: Drug discovery demands rapid quantification of compound-proteininteraction (CPI). However, there is a lack of methods that can predictcompound-protein affinity from sequences alone with high applicability,accuracy, and interpretability.  Results: We present a seamless integration of domain knowledges andlearning-based approaches. Under novel representations ofstructurally-annotated protein sequences, a semi-supervised deep learning modelthat unifies recurrent and convolutional neural networks has been proposed toexploit both unlabeled and labeled data, for jointly encoding molecularrepresentations and predicting affinities. Our representations and modelsoutperform conventional options in achieving relative error in IC$_{50}$ within5-fold for test cases and 20-fold for protein classes not included fortraining. Performances for new protein classes with few labeled data arefurther improved by transfer learning. Furthermore, separate and jointattention mechanisms are developed and embedded to our model to add to itsinterpretability, as illustrated in case studies for predicting and explainingselective drug-target interactions. Lastly, alternative representations usingprotein sequences or compound graphs and a unified RNN/GCNN-CNN model usinggraph CNN (GCNN) are also explored to reveal algorithmic challenges ahead.  Availability: Data and source codes are available athttps://github.com/Shen-Lab/DeepAffinity  Supplementary Information: Supplementary data are available athttp://shen-lab.github.io/deep-affinity-bioinf18-supp-rev.pdf

DADA: Deep Adversarial Data Augmentation for Extremely Low Data Regime  Classification

  Deep learning has revolutionized the performance of classification, butmeanwhile demands sufficient labeled data for training. Given insufficientdata, while many techniques have been developed to help combat overfitting, thechallenge remains if one tries to train deep networks, especially in theill-posed extremely low data regimes: only a small set of labeled data areavailable, and nothing -- including unlabeled data -- else. Such regimes arisefrom practical situations where not only data labeling but also data collectionitself is expensive. We propose a deep adversarial data augmentation (DADA)technique to address the problem, in which we elaborately formulate dataaugmentation as a problem of training a class-conditional and supervisedgenerative adversarial network (GAN). Specifically, a new discriminator loss isproposed to fit the goal of data augmentation, through which both real andaugmented samples are enforced to contribute to and be consistent in findingthe decision boundaries. Tailored training techniques are developedaccordingly. To quantitatively validate its effectiveness, we first performextensive simulations to show that DADA substantially outperforms bothtraditional data augmentation and a few GAN-based options. We then extendexperiments to three real-world small labeled datasets where existing dataaugmentation and/or transfer learning strategies are either less effective orinfeasible. All results endorse the superior capability of DADA in enhancingthe generalization ability of deep networks trained in practical extremely lowdata regimes. Source code is available athttps://github.com/SchafferZhang/DADA.

Connecting Image Denoising and High-Level Vision Tasks via Deep Learning

  Image denoising and high-level vision tasks are usually handled independentlyin the conventional practice of computer vision, and their connection isfragile. In this paper, we cope with the two jointly and explore the mutualinfluence between them with the focus on two questions, namely (1) how imagedenoising can help improving high-level vision tasks, and (2) how the semanticinformation from high-level vision tasks can be used to guide image denoising.First for image denoising we propose a convolutional neural network in whichconvolutions are conducted in various spatial resolutions via downsampling andupsampling operations in order to fuse and exploit contextual information ondifferent scales. Second we propose a deep neural network solution thatcascades two modules for image denoising and various high-level tasks,respectively, and use the joint loss for updating only the denoising networkvia back-propagation. We experimentally show that on one hand, the proposeddenoiser has the generality to overcome the performance degradation ofdifferent high-level vision tasks. On the other hand, with the guidance ofhigh-level vision information, the denoising network produces more visuallyappealing results. Extensive experiments demonstrate the benefit of exploitingimage semantics simultaneously for image denoising and high-level vision tasksvia deep learning. The code is available online:https://github.com/Ding-Liu/DeepDenoising

Adaptive Activity Monitoring with Uncertainty Quantification in  Switching Gaussian Process Models

  Emerging wearable sensors have enabled the unprecedented ability tocontinuously monitor human activities for healthcare purposes. However, with somany ambient sensors collecting different measurements, it becomes importantnot only to maintain good monitoring accuracy, but also low power consumptionto ensure sustainable monitoring. This power-efficient sensing scheme can beachieved by deciding which group of sensors to use at a given time, requiringan accurate characterization of the trade-off between sensor energy usage andthe uncertainty in ignoring certain sensor signals while monitoring. To addressthis challenge in the context of activity monitoring, we have designed anadaptive activity monitoring framework. We first propose a switching Gaussianprocess to model the observed sensor signals emitting from the underlyingactivity states. To efficiently compute the Gaussian process model likelihoodand quantify the context prediction uncertainty, we propose a block circulantembedding technique and use Fast Fourier Transforms (FFT) for inference. Bycomputing the Bayesian loss function tailored to switching Gaussian processes,an adaptive monitoring procedure is developed to select features from availablesensors that optimize the trade-off between sensor power consumption and theprediction performance quantified by state prediction entropy. We demonstratethe effectiveness of our framework on the popular benchmark of UCI HumanActivity Recognition using Smartphones.

Convolutional Sparse Coding for Compressed Sensing CT Reconstruction

  Over the past few years, dictionary learning (DL)-based methods have beensuccessfully used in various image reconstruction problems. However,traditional DL-based computed tomography (CT) reconstruction methods arepatch-based and ignore the consistency of pixels in overlapped patches. Inaddition, the features learned by these methods always contain shifted versionsof the same features. In recent years, convolutional sparse coding (CSC) hasbeen developed to address these problems. In this paper, inspired by severalsuccessful applications of CSC in the field of signal processing, we explorethe potential of CSC in sparse-view CT reconstruction. By directly working onthe whole image, without the necessity of dividing the image into overlappedpatches in DL-based methods, the proposed methods can maintain more details andavoid artifacts caused by patch aggregation. With predetermined filters, analternating scheme is developed to optimize the objective function. Extensiveexperiments with simulated and real CT data were performed to validate theeffectiveness of the proposed methods. Qualitative and quantitative resultsdemonstrate that the proposed methods achieve better performance than severalexisting state-of-the-art methods.

UG$^{2+}$ Track 2: A Collective Benchmark Effort for Evaluating and  Advancing Image Understanding in Poor Visibility Environments

  The UG$^{2+}$ challenge in IEEE CVPR 2019 aims to evoke a comprehensivediscussion and exploration about how low-level vision techniques can benefitthe high-level automatic visual recognition in various scenarios. In its secondtrack, we focus on object or face detection in poor visibility enhancementscaused by bad weathers (haze, rain) and low light conditions. While existingenhancement methods are empirically expected to help the high-level end task,that is observed to not always be the case in practice. To provide a morethorough examination and fair comparison, we introduce three benchmark setscollected in real-world hazy, rainy, and low-light conditions, respectively,with annotate objects/faces annotated. To our best knowledge, this is the firstand currently largest effort of its kind. Baseline results by cascadingexisting enhancement and detection models are reported, indicating the highlychallenging nature of our new data as well as the large room for furthertechnical innovations. We expect a large participation from the broad researchcommunity to address these challenges together.

Bridging the Gap Between Computational Photography and Visual  Recognition

  What is the current state-of-the-art for image restoration and enhancementapplied to degraded images acquired under less than ideal circumstances? Canthe application of such algorithms as a pre-processing step to improve imageinterpretability for manual analysis or automatic visual recognition toclassify scene content? While there have been important advances in the area ofcomputational photography to restore or enhance the visual quality of an image,the capabilities of such techniques have not always translated in a useful wayto visual recognition tasks. Consequently, there is a pressing need for thedevelopment of algorithms that are designed for the joint problem of improvingvisual appearance and recognition, which will be an enabling factor for thedeployment of visual recognition tools in many real-world scenarios. To addressthis, we introduce the UG^2 dataset as a large-scale benchmark composed ofvideo imagery captured under challenging conditions, and two enhancement tasksdesigned to test algorithmic impact on visual quality and automatic objectrecognition. Furthermore, we propose a set of metrics to evaluate the jointimprovement of such tasks as well as individual algorithmic advances, includinga novel psychophysics-based evaluation regime for human assessment and arealistic set of quantitative measures for object recognition performance. Weintroduce six new algorithms for image restoration or enhancement, which werecreated as part of the IARPA sponsored UG^2 Challenge workshop held at CVPR2018. Under the proposed evaluation regime, we present an in-depth analysis ofthese algorithms and a host of deep learning-based and classic baselineapproaches. From the observed results, it is evident that we are in the earlydays of building a bridge between computational photography and visualrecognition, leaving many opportunities for innovation in this area.

