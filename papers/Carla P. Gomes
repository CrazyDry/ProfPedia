Algorithm Portfolio Design: Theory vs. Practice

  Stochastic algorithms are among the best for solving computationally hard
search and reasoning problems. The runtime of such procedures is characterized
by a random variable. Different algorithms give rise to different probability
distributions. One can take advantage of such differences by combining several
algorithms into a portfolio, and running them in parallel or interleaving them
on a single processor. We provide a detailed evaluation of the portfolio
approach on distributions of hard combinatorial search problems. We show under
what conditions the protfolio approach can have a dramatic computational
advantage over the best traditional methods.


Uniform Solution Sampling Using a Constraint Solver As an Oracle

  We consider the problem of sampling from solutions defined by a set of hard
constraints on a combinatorial space. We propose a new sampling technique that,
while enforcing a uniform exploration of the search space, leverages the
reasoning power of a systematic constraint solver in a black-box scheme. We
present a series of challenging domains, such as energy barriers and highly
asymmetric spaces, that reveal the difficulties introduced by hard constraints.
We demonstrate that standard approaches such as Simulated Annealing and Gibbs
Sampling are greatly affected, while our new technique can overcome many of
these difficulties. Finally, we show that our sampling scheme naturally defines
a new approximate model counting technique, which we empirically show to be
very accurate on a range of benchmark problems.


Taming the Curse of Dimensionality: Discrete Integration by Hashing and
  Optimization

  Integration is affected by the curse of dimensionality and quickly becomes
intractable as the dimensionality of the problem grows. We propose a randomized
algorithm that, with high probability, gives a constant-factor approximation of
a general discrete integral defined over an exponentially large set. This
algorithm relies on solving only a small number of instances of a discrete
combinatorial optimization problem subject to randomly generated parity
constraints used as a hash function. As an application, we demonstrate that
with a small number of MAP queries we can efficiently approximate the partition
function of discrete graphical models, which can in turn be used, for instance,
for marginal computation or model selection.


Playing games against nature: optimal policies for renewable resource
  allocation

  In this paper we introduce a class of Markov decision processes that arise as
a natural model for many renewable resource allocation problems. Upon extending
results from the inventory control literature, we prove that they admit a
closed form solution and we show how to exploit this structure to speed up its
computation. We consider the application of the proposed framework to several
problems arising in very different domains, and as part of the ongoing effort
in the emerging field of Computational Sustainability we discuss in detail its
application to the Northern Pacific Halibut marine fishery. Our approach is
applied to a model based on real world data, obtaining a policy with a
guaranteed lower bound on the utility function that is structurally very
different from the one currently employed.


Maximizing the Spread of Cascades Using Network Design

  We introduce a new optimization framework to maximize the expected spread of
cascades in networks. Our model allows a rich set of actions that directly
manipulate cascade dynamics by adding nodes or edges to the network. Our
motivating application is one in spatial conservation planning, where a cascade
models the dispersal of wild animals through a fragmented landscape. We propose
a mixed integer programming (MIP) formulation that combines elements from
network design and stochastic optimization. Our approach results in solutions
with stochastic optimality guarantees and points to conservation strategies
that are fundamentally different from naive approaches.


A Bayesian Approach to Tackling Hard Computational Problems

  We are developing a general framework for using learned Bayesian models for
decision-theoretic control of search and reasoningalgorithms. We illustrate the
approach on the specific task of controlling both general and domain-specific
solvers on a hard class of structured constraint satisfaction problems. A
successful strategyfor reducing the high (and even infinite) variance in
running time typically exhibited by backtracking search algorithms is to cut
off and restart the search if a solution is not found within a certainamount of
time. Previous work on restart strategies have employed fixed cut off values.
We show how to create a dynamic cut off strategy by learning a Bayesian model
that predicts the ultimate length of a trial based on observing the early
behavior of the search algorithm. Furthermore, we describe the general
conditions under which a dynamic restart strategy can outperform the
theoretically optimal fixed strategy.


Optimization With Parity Constraints: From Binary Codes to Discrete
  Integration

  Many probabilistic inference tasks involve summations over exponentially
large sets. Recently, it has been shown that these problems can be reduced to
solving a polynomial number of MAP inference queries for a model augmented with
randomly generated parity constraints. By exploiting a connection with
max-likelihood decoding of binary codes, we show that these optimizations are
computationally hard. Inspired by iterative message passing decoding
algorithms, we propose an Integer Linear Programming (ILP) formulation for the
problem, enhanced with new sparsification techniques to improve decoding
performance. By solving the ILP through a sequence of LP relaxations, we get
both lower and upper bounds on the partition function, which hold with high
probability and are much tighter than those obtained with variational methods.


Variable Elimination in the Fourier Domain

  The ability to represent complex high dimensional probability distributions
in a compact form is one of the key insights in the field of graphical models.
Factored representations are ubiquitous in machine learning and lead to major
computational advantages. We explore a different type of compact representation
based on discrete Fourier representations, complementing the classical approach
based on conditional independencies. We show that a large class of
probabilistic graphical models have a compact Fourier representation. This
theoretical result opens up an entirely new way of approximating a probability
distribution. We demonstrate the significance of this approach by applying it
to the variable elimination algorithm. Compared with the traditional bucket
representation and other approximate inference algorithms, we obtain
significant improvements.


Solving Marginal MAP Problems with NP Oracles and Parity Constraints

  Arising from many applications at the intersection of decision making and
machine learning, Marginal Maximum A Posteriori (Marginal MAP) Problems unify
the two main classes of inference, namely maximization (optimization) and
marginal inference (counting), and are believed to have higher complexity than
both of them. We propose XOR_MMAP, a novel approach to solve the Marginal MAP
Problem, which represents the intractable counting subproblem with queries to
NP oracles, subject to additional parity constraints. XOR_MMAP provides a
constant factor approximation to the Marginal MAP Problem, by encoding it as a
single optimization in polynomial size of the original problem. We evaluate our
approach in several machine learning and decision making applications, and show
that our approach outperforms several state-of-the-art Marginal MAP solvers.


XOR-Sampling for Network Design with Correlated Stochastic Events

  Many network optimization problems can be formulated as stochastic network
design problems in which edges are present or absent stochastically.
Furthermore, protective actions can guarantee that edges will remain present.
We consider the problem of finding the optimal protection strategy under a
budget limit in order to maximize some connectivity measurements of the
network. Previous approaches rely on the assumption that edges are independent.
In this paper, we consider a more realistic setting where multiple edges are
not independent due to natural disasters or regional events that make the
states of multiple edges stochastically correlated. We use Markov Random Fields
to model the correlation and define a new stochastic network design framework.
We provide a novel algorithm based on Sample Average Approximation (SAA)
coupled with a Gibbs or XOR sampler. The experimental results on real road
network data show that the policies produced by SAA with the XOR sampler have
higher quality and lower variance compared to SAA with Gibbs sampler.


Multi-Entity Dependence Learning with Rich Context via Conditional
  Variational Auto-encoder

  Multi-Entity Dependence Learning (MEDL) explores conditional correlations
among multiple entities. The availability of rich contextual information
requires a nimble learning scheme that tightly integrates with deep neural
networks and has the ability to capture correlation structures among
exponentially many outcomes. We propose MEDL_CVAE, which encodes a conditional
multivariate distribution as a generating process. As a result, the variational
lower bound of the joint likelihood can be optimized via a conditional
variational auto-encoder and trained end-to-end on GPUs. Our MEDL_CVAE was
motivated by two real-world applications in computational sustainability: one
studies the spatial correlation among multiple bird species using the eBird
data and the other models multi-dimensional landscape composition and human
footprint in the Amazon rainforest with satellite images. We show that
MEDL_CVAE captures rich dependency structures, scales better than previous
methods, and further improves on the joint likelihood taking advantage of very
large datasets that are beyond the capacity of previous methods.


End-to-End Learning for the Deep Multivariate Probit Model

  The multivariate probit model (MVP) is a popular classic model for studying
binary responses of multiple entities. Nevertheless, the computational
challenge of learning the MVP model, given that its likelihood involves
integrating over a multidimensional constrained space of latent variables,
significantly limits its application in practice. We propose a flexible deep
generalization of the classic MVP, the Deep Multivariate Probit Model (DMVP),
which is an end-to-end learning scheme that uses an efficient parallel sampling
process of the multivariate probit model to exploit GPU-boosted deep neural
networks. We present both theoretical and empirical analysis of the convergence
behavior of DMVP's sampling process with respect to the resolution of the
correlation structure. We provide convergence guarantees for DMVP and our
empirical analysis demonstrates the advantages of DMVP's sampling compared with
standard MCMC-based methods. We also show that when applied to multi-entity
modelling problems, which are natural DMVP applications, DMVP trains faster
than classical MVP, by at least an order of magnitude, captures rich
correlations among entities, and further improves the joint likelihood of
entities compared with several competitive models.


Bias Reduction via End-to-End Shift Learning: Application to Citizen
  Science

  Citizen science projects are successful at gathering rich datasets for
various applications. However, the data collected by citizen scientists are
often biased --- in particular, aligned more with the citizens' preferences
than with scientific objectives. We propose the Shift Compensation Network
(SCN), an end-to-end learning scheme which learns the shift from the scientific
objectives to the biased data while compensating for the shift by re-weighting
the training data. Applied to bird observational data from the citizen science
project eBird, we demonstrate how SCN quantifies the data distribution shift
and outperforms supervised learning models that do not address the data bias.
Compared with competing models in the context of covariate shift, we further
demonstrate the advantage of SCN in both its effectiveness and its capability
of handling massive high-dimensional data.


On the Erdos Discrepancy Problem

  According to the Erd\H{o}s discrepancy conjecture, for any infinite $\pm 1$
sequence, there exists a homogeneous arithmetic progression of unbounded
discrepancy. In other words, for any $\pm 1$ sequence $(x_1,x_2,...)$ and a
discrepancy $C$, there exist integers $m$ and $d$ such that $|\sum_{i=1}^m x_{i
\cdot d}| > C$. This is an $80$-year-old open problem and recent development
proved that this conjecture is true for discrepancies up to $2$. Paul Erd\H{o}s
also conjectured that this property of unbounded discrepancy even holds for the
restricted case of completely multiplicative sequences (CMSs), namely sequences
$(x_1,x_2,...)$ where $x_{a \cdot b} = x_{a} \cdot x_{b}$ for any $a,b \geq 1$.
The longest CMS with discrepancy $2$ has been proven to be of size $246$. In
this paper, we prove that any completely multiplicative sequence of size
$127,646$ or more has discrepancy at least $4$, proving the Erd\H{o}s
discrepancy conjecture for CMSs of discrepancies up to $3$. In addition, we
prove that this bound is tight and increases the size of the longest known
sequence of discrepancy $3$ from $17,000$ to $127,645$. Finally, we provide
inductive construction rules as well as streamlining methods to improve the
lower bounds for sequences of higher discrepancies.


Phase-Mapper: An AI Platform to Accelerate High Throughput Materials
  Discovery

  High-Throughput materials discovery involves the rapid synthesis,
measurement, and characterization of many different but structurally-related
materials. A key problem in materials discovery, the phase map identification
problem, involves the determination of the crystal phase diagram from the
materials' composition and structural characterization data. We present
Phase-Mapper, a novel AI platform to solve the phase map identification problem
that allows humans to interact with both the data and products of AI
algorithms, including the incorporation of human feedback to constrain or
initialize solutions. Phase-Mapper affords incorporation of any spectral
demixing algorithm, including our novel solver, AgileFD, which is based on a
convolutive non-negative matrix factorization algorithm. AgileFD can
incorporate constraints to capture the physics of the materials as well as
human feedback. We compare three solver variants with previously proposed
methods in a large-scale experiment involving 20 synthetic systems,
demonstrating the efficacy of imposing physical constrains using AgileFD.
Phase-Mapper has also been used by materials scientists to solve a wide variety
of phase diagrams, including the previously unsolved Nb-Mn-V oxide system,
which is provided here as an illustrative example.


Automated Phase Mapping with AgileFD and its Application to Light
  Absorber Discovery in the V-Mn-Nb Oxide System

  Rapid construction of phase diagrams is a central tenet of combinatorial
materials science with accelerated materials discovery efforts often hampered
by challenges in interpreting combinatorial x-ray diffraction datasets, which
we address by developing AgileFD, an artificial intelligence algorithm that
enables rapid phase mapping from a combinatorial library of x-ray diffraction
patterns. AgileFD models alloying-based peak shifting through a novel expansion
of convolutional nonnegative matrix factorization, which not only improves the
identification of constituent phases but also maps their concentration and
lattice parameter as a function of composition. By incorporating Gibbs phase
rule into the algorithm, physically meaningful phase maps are obtained with
unsupervised operation, and more refined solutions are attained by injecting
expert knowledge of the system. The algorithm is demonstrated through
investigation of the V-Mn-Nb oxide system where decomposition of eight oxide
phases, including two with substantial alloying, provides the first phase map
for this pseudo-ternary system. This phase map enables interpretation of
high-throughput band gap data, leading to the discovery of new solar light
absorbers and the alloying-based tuning of the direct-allowed band-gap energy
of MnV2O6. The open-source family of AgileFD algorithms can be implemented into
a broad range of high throughput workflows to accelerate materials discovery.


Automatic Detection and Compression for Passive Acoustic Monitoring of
  the African Forest Elephant

  In this work, we consider applying machine learning to the analysis and
compression of audio signals in the context of monitoring elephants in
sub-Saharan Africa. Earth's biodiversity is increasingly under threat by
sources of anthropogenic change (e.g. resource extraction, land use change, and
climate change) and surveying animal populations is critical for developing
conservation strategies. However, manually monitoring tropical forests or deep
oceans is intractable. For species that communicate acoustically, researchers
have argued for placing audio recorders in the habitats as a cost-effective and
non-invasive method, a strategy known as passive acoustic monitoring (PAM). In
collaboration with conservation efforts, we construct a large labeled dataset
of passive acoustic recordings of the African Forest Elephant via
crowdsourcing, compromising thousands of hours of recordings in the wild. Using
state-of-the-art techniques in artificial intelligence we improve upon
previously proposed methods for passive acoustic monitoring for classification
and segmentation. In real-time detection of elephant calls, network bandwidth
quickly becomes a bottleneck and efficient ways to compress the data are
needed. Most audio compression schemes are aimed at human listeners and are
unsuitable for low-frequency elephant calls. To remedy this, we provide a novel
end-to-end differentiable method for compression of audio signals that can be
adapted to acoustic monitoring of any species and dramatically improves over
naive coding strategies.


Search for CP violation in the decay $D^+ \to π^-π^+π^+$

  A search for CP violation in the phase space of the decay
$D^+\to\pi^-\pi^+\pi^+$ is reported using $pp$ collision data, corresponding to
an integrated luminosity of 1.0 fb$^{-1}$, collected by the LHCb experiment at
a centre-of-mass energy of 7 TeV. The Dalitz plot distributions for $3.1\times
10^6$ $D^+$ and $D^-$ candidates are compared with binned and unbinned
model-independent techniques. No evidence for CP violation is found.


