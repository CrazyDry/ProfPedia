Programs as Actual Causes: A Building Block for Accountability

  An updated version of this paper is available athttp://arxiv.org/abs/1505.01131

Report on the NSF Workshop on Formal Methods for Security

  Report on the NSF Workshop on Formal Methods for Security, held 19-20November 2015.

Influence in Classification via Cooperative Game Theory

  A dataset has been classified by some unknown classifier into two types ofpoints. What were the most important factors in determining the classificationoutcome? In this work, we employ an axiomatic approach in order to uniquelycharacterize an influence measure: a function that, given a set of classifiedpoints, outputs a value for each feature corresponding to its influence indetermining the classification outcome. We show that our influence measuretakes on an intuitive form when the unknown classifier is linear. Finally, weemploy our influence measure in order to analyze the effects of user profilingon Google's online display advertising.

Equivalence-based Security for Querying Encrypted Databases: Theory and  Application to Privacy Policy Audits

  Motivated by the problem of simultaneously preserving confidentiality andusability of data outsourced to third-party clouds, we present two differentdatabase encryption schemes that largely hide data but reveal enoughinformation to support a wide-range of relational queries. We provide asecurity definition for database encryption that captures confidentiality basedon a notion of equivalence of databases from the adversary's perspective. As aspecific application, we adapt an existing algorithm for finding violations ofprivacy policies to run on logs encrypted under our schemes and observe low tomoderate overheads.

Proxy Non-Discrimination in Data-Driven Systems

  Machine learnt systems inherit biases against protected classes, historicallydisparaged groups, from training data. Usually, these biases are not explicit,they rely on subtle correlations discovered by training algorithms, and aretherefore difficult to detect. We formalize proxy discrimination in data-drivensystems, a class of properties indicative of bias, as the presence of protectedclass correlates that have causal influence on the system's output. We evaluatean implementation on a corpus of social datasets, demonstrating how to validatesystems against these properties and to repair violations where they occur.

Case Study: Explaining Diabetic Retinopathy Detection Deep CNNs via  Integrated Gradients

  In this report, we applied integrated gradients to explaining a neuralnetwork for diabetic retinopathy detection. The integrated gradient is anattribution method which measures the contributions of input to the quantity ofinterest. We explored some new ways for applying this method such as explainingintermediate layers, filtering out unimportant units by their attribution valueand generating contrary samples. Moreover, the visualization results extend theuse of diabetic retinopathy detection model from merely predicting to assistingfinding potential lesions.

A Methodology for Information Flow Experiments

  Information flow analysis has largely ignored the setting where the analysthas neither control over nor a complete model of the analyzed system. Weformalize such limited information flow analyses and study an instance of it:detecting the usage of data by websites. We prove that these problems are onesof causal inference. Leveraging this connection, we push beyond traditionalinformation flow analysis to provide a systematic methodology based onexperimental science and statistical analysis. Our methodology allows us tosystematize prior works in the area viewing them as instances of a generalapproach. Our systematic study leads to practical advice for improving work ondetecting data usage, a previously unformalized area. We illustrate theseconcepts with a series of experiments collecting data on the use of informationby websites, which we statistically analyze.

Automated Experiments on Ad Privacy Settings: A Tale of Opacity, Choice,  and Discrimination

  To partly address people's concerns over web tracking, Google has created theAd Settings webpage to provide information about and some choice over theprofiles Google creates on users. We present AdFisher, an automated tool thatexplores how user behaviors, Google's ads, and Ad Settings interact. AdFishercan run browser-based experiments and analyze data using machine learning andsignificance tests. Our tool uses a rigorous experimental design andstatistical analysis to ensure the statistical soundness of our results. We useAdFisher to find that the Ad Settings was opaque about some features of auser's profile, that it does provide some choice on ads, and that these choicescan lead to seemingly discriminatory ads. In particular, we found that visitingwebpages associated with substance abuse changed the ads shown but not thesettings page. We also found that setting the gender to female resulted ingetting fewer instances of an ad related to high paying jobs than setting it tomale. We cannot determine who caused these findings due to our limitedvisibility into the ad ecosystem, which includes Google, advertisers, websites,and users. Nevertheless, these results can form the starting point for deeperinvestigations by either the companies themselves or by regulatory bodies.

On the Semantics of Purpose Requirements in Privacy Policies

  Privacy policies often place requirements on the purposes for which agoverned entity may use personal information. For example, regulations, such asHIPAA, require that hospital employees use medical information for only certainpurposes, such as treatment. Thus, using formal or automated methods forenforcing privacy policies requires a semantics of purpose requirements todetermine whether an action is for a purpose or not. We provide such asemantics using a formalism based on planning. We model planning using amodified version of Markov Decision Processes, which exclude redundant actionsfor a formal definition of redundant. We use the model to formalize when asequence of actions is only for or not for a purpose. This semantics enables usto provide an algorithm for automating auditing, and to describe formally andcompare rigorously previous enforcement methods.

Audit Games

  Effective enforcement of laws and policies requires expending resources toprevent and detect offenders, as well as appropriate punishment schemes todeter violators. In particular, enforcement of privacy laws and policies inmodern organizations that hold large volumes of personal information (e.g.,hospitals, banks, and Web services providers) relies heavily on internal auditmechanisms. We study economic considerations in the design of these mechanisms,focusing in particular on effective resource allocation and appropriatepunishment schemes. We present an audit game model that is a naturalgeneralization of a standard security game model for resource allocation withan additional punishment parameter. Computing the Stackelberg equilibrium forthis game is challenging because it involves solving an optimization problemwith non-convex quadratic constraints. We present an additive FPTAS thatefficiently computes a solution that is arbitrarily close to the optimalsolution.

Audit Games with Multiple Defender Resources

  Modern organizations (e.g., hospitals, social networks, government agencies)rely heavily on audit to detect and punish insiders who inappropriately accessand disclose confidential information. Recent work on audit games models thestrategic interaction between an auditor with a single audit resource andauditees as a Stackelberg game, augmenting associated well-studied securitygames with a configurable punishment parameter. We significantly generalizethis audit game model to account for multiple audit resources where eachresource is restricted to audit a subset of all potential violations, thusenabling application to practical auditing scenarios. We provide an FPTAS thatcomputes an approximately optimal solution to the resulting non-convexoptimization problem. The main technical novelty is in the design andcorrectness proof of an optimization transformation that enables theconstruction of this FPTAS. In addition, we experimentally demonstrate thatthis transformation significantly speeds up computation of solutions for aclass of audit games and security games.

System M: A Program Logic for Code Sandboxing and Identification

  Security-sensitive applications that execute untrusted code often check thecode's integrity by comparing its syntax to a known good value or sandbox thecode to contain its effects. System M is a new program logic for reasoningabout such security-sensitive applications. System M extends Hoare Type Theory(HTT) to trace safety properties and, additionally, contains two new reasoningprinciples. First, its type system internalizes logical equality, facilitatingreasoning about applications that check code integrity. Second, a confinementrule assigns an effect type to a computation based solely on knowledge of thecomputation's sandbox. We prove the soundness of system M relative to astep-indexed trace-based semantic model. We illustrate both new reasoningprinciples of system M by verifying the main integrity property of the designof Memoir, a previously proposed trusted computing system for ensuring statecontinuity of isolated security-sensitive applications.

Program Actions as Actual Causes: A Building Block for Accountability

  Protocols for tasks such as authentication, electronic voting, and securemultiparty computation ensure desirable security properties if agents followtheir prescribed programs. However, if some agents deviate from theirprescribed programs and a security property is violated, it is important tohold agents accountable by determining which deviations actually caused theviolation. Motivated by these applications, we initiate a formal study ofprogram actions as actual causes. Specifically, we define in an interactingprogram model what it means for a set of program actions to be an actual causeof a violation. We present a sound technique for establishing program actionsas actual causes. We demonstrate the value of this formalism in two ways.First, we prove that violations of a specific class of safety properties alwayshave an actual cause. Thus, our definition applies to relevant securityproperties. Second, we provide a cause analysis of a representative protocoldesigned to address weaknesses in the current public key certificationinfrastructure.

A Symbolic Logic with Concrete Bounds for Cryptographic Protocols

  We present a formal logic for quantitative reasoning about securityproperties of network protocols. The system allows us to derive concretesecurity bounds that can be used to choose key lengths and other securityparameters. We provide axioms for reasoning about digital signatures and randomnonces, with security properties based on the concrete security of signatureschemes and pseudorandom number generators (PRG). The formal logic supportsfirst-order reasoning and reasoning about protocol invariants, taking concretesecurity bounds into account. Proofs constructed in our logic also provideconventional asymptotic security guarantees because of the way that concretebounds accumulate in proofs. As an illustrative example, we use the formallogic to prove an authentication property with concrete bounds of asignature-based challenge-response protocol.

Information Flow for Security in Control Systems

  This paper considers the development of information flow analyses to supportresilient design and active detection of adversaries in cyber physical systems(CPS). The area of CPS security, though well studied, suffers fromfragmentation. In this paper, we consider control systems as an abstraction ofCPS. Here, we extend the notion of information flow analysis, a wellestablished set of methods developed in software security, to obtain a unifiedframework that captures and extends system theoretic results in control systemsecurity. In particular, we propose the Kullback Liebler (KL) divergence as acausal measure of information flow, which quantifies the effect of adversarialinputs on sensor outputs. We show that the proposed measure characterizes theresilience of control systems to specific attack strategies by relating the KLdivergence to optimal detection techniques. We then relate information flows tostealthy attack scenarios where an adversary can bypass detection. Finally,this article examines active detection mechanisms where a defenderintelligently manipulates control inputs or the system itself in order toelicit information flows from an attacker's malicious behavior. In all previouscases, we demonstrate an ability to investigate and extend existing results byutilizing the proposed information flow analyses.

Differential Privacy as a Causal Property

  We present associative and causal views of differential privacy. Under theassociative view, the possibility of dependencies between data points precludesa simple statement of differential privacy's guarantee as conditioning upon asingle changed data point. However, we show that a simple characterization ofdifferential privacy as limiting the effect of a single data point does existunder the causal view, without independence assumptions about data points. Webelieve this characterization resolves disagreement and confusion in prior workabout the consequences of differential privacy. The associative view needingassumptions boils down to the contrapositive of the maxim that correlationdoesn't imply causation: differential privacy ensuring a lack of (strong)causation does not imply a lack of (strong) association. Our characterizationalso opens up the possibility of applying results from statistics, experimentaldesign, and science about causation while studying differential privacy.

Latent Factor Interpretations for Collaborative Filtering

  Many machine learning systems utilize latent factors as internalrepresentations for making predictions. Since these latent factors are largelyuninterpreted, however, predictions made using them are opaque. Collaborativefiltering via matrix factorization is a prime example of such an algorithm thatuses uninterpreted latent features, and yet has seen widespread adoption formany recommendation tasks. We present Latent Factor Interpretation (LFI), amethod for interpreting models by leveraging interpretations of latent factorsin terms of human-understandable features. The interpretation of latent factorscan then replace the uninterpreted latent factors, resulting in a new modelthat expresses predictions in terms of interpretable features. This new modelcan then be interpreted using recently developed model explanation techniques.In this paper we develop LFI for collaborative filtering based recommendersystems. We illustrate the use of LFI interpretations on the MovieLens dataset,integrating auxiliary features from IMDB and DB tropes, and show that latentfactors can be predicted with sufficient accuracy for replicating thepredictions of the true model.

Influence-Directed Explanations for Deep Convolutional Networks

  We study the problem of explaining a rich class of behavioral properties ofdeep neural networks. Distinctively, our influence-directed explanationsapproach this problem by peering inside the network to identify neurons withhigh influence on a quantity and distribution of interest, using anaxiomatically-justified influence measure, and then providing an interpretationfor the concepts these neurons represent. We evaluate our approach bydemonstrating a number of its unique capabilities on convolutional neuralnetworks trained on ImageNet. Our evaluation demonstrates thatinfluence-directed explanations (1) identify influential concepts thatgeneralize across instances, (2) can be used to extract the "essence" of whatthe network learned about a class, and (3) isolate individual features thenetwork uses to make decisions and distinguish related classes.

Supervising Feature Influence

  Causal influence measures for machine learnt classifiers shed light on thereasons behind classification, and aid in identifying influential inputfeatures and revealing their biases. However, such analyses involve evaluatingthe classifier using datapoints that may be atypical of its trainingdistribution. Standard methods for training classifiers that minimize empiricalrisk do not constrain the behavior of the classifier on such datapoints. As aresult, training to minimize empirical risk does not distinguish amongclassifiers that agree on predictions in the training distribution but havewildly different causal influences. We term this problem covariate shift incausal testing and formally characterize conditions under which it arises. As asolution to this problem, we propose a novel active learning algorithm thatconstrains the influence measures of the trained model. We prove that any twopredictors whose errors are close on both the original training distributionand the distribution of atypical points are guaranteed to have causalinfluences that are also close. Further, we empirically demonstrate withsynthetic labelers that our algorithm trains models that (i) have similarcausal influences as the labeler's model, and (ii) generalize better toout-of-distribution points while (iii) retaining their accuracy onin-distribution points.

Gender Bias in Neural Natural Language Processing

  We examine whether neural natural language processing (NLP) systems reflecthistorical biases in training data. We define a general benchmark to quantifygender bias in a variety of neural NLP tasks. Our empirical evaluation withstate-of-the-art neural coreference resolution and textbook RNN-based languagemodels trained on benchmark datasets finds significant gender bias in howmodels view occupations. We then mitigate bias with CDA: a generic methodologyfor corpus augmentation via causal interventions that breaks associationsbetween gendered and gender-neutral words. We empirically show that CDAeffectively decreases gender bias while preserving accuracy. We also explorethe space of mitigation strategies with CDA, a prior approach to word embeddingdebiasing (WED), and their compositions. We show that CDA outperforms WED,drastically so when word embeddings are trained. For pre-trained embeddings,the two methods can be effectively composed. We also find that as trainingproceeds on the original data set with gradient descent the gender bias growsas the loss reduces, indicating that the optimization encourages bias; CDAmitigates this behavior.

Correspondences between Privacy and Nondiscrimination: Why They Should  Be Studied Together

  Privacy and nondiscrimination are related but different. We make thisobservation precise in two ways. First, we show that both privacy andnondiscrimination have two versions, a causal version and a staticalassociative version, with each version corresponding to a competing view of theproper goal of privacy or nondiscrimination. Second, for each version, we showthat a difference between the privacy edition of the version and thenondiscrimination edition of the version is related to the difference betweenBayesian probabilities and frequentist probabilities. In particular, privacyadmits both Bayesian and frequentist interpretations whereas nondiscriminationis limited to the frequentist interpretation. We show how the introducedcorrespondence allows results from one area of research to be used for theother.

Hunting for Discriminatory Proxies in Linear Regression Models

  A machine learning model may exhibit discrimination when used to makedecisions involving people. One potential cause for such outcomes is that themodel uses a statistical proxy for a protected demographic attribute. In thispaper we formulate a definition of proxy use for the setting of linearregression and present algorithms for detecting proxies. Our definition followsrecent work on proxies in classification models, and characterizes a model'sconstituent behavior that: 1) correlates closely with a protected randomvariable, and 2) is causally influential in the overall behavior of the model.We show that proxies in linear regression models can be efficiently identifiedby solving a second-order cone program, and further extend this result toaccount for situations where the use of a certain input variable is justifiedas a `business necessity'. Finally, we present empirical results on two lawenforcement datasets that exhibit varying degrees of racial disparity inprediction outcomes, demonstrating that proxies shed useful light on the causesof discriminatory behavior in models.

Feature-Wise Bias Amplification

  We study the phenomenon of bias amplification in classifiers, wherein amachine learning model learns to predict classes with a greater disparity thanthe underlying ground truth. We demonstrate that bias amplification can arisevia an inductive bias in gradient descent methods that results in theoverestimation of the importance of moderately-predictive "weak" features ifinsufficient training data is available. This overestimation gives rise tofeature-wise bias amplification -- a previously unreported form of bias thatcan be traced back to the features of a trained model. Through analysis andexperiments, we show that while some bias cannot be mitigated withoutsacrificing accuracy, feature-wise bias amplification can be mitigated throughtargeted feature selection. We present two new feature selection algorithms formitigating bias amplification in linear models, and show how they can beadapted to convolutional neural networks efficiently. Our experiments onsynthetic and real data demonstrate that these algorithms consistently lead toreduced bias without harming accuracy, in some cases eliminating predictivebias altogether while providing modest gains in accuracy.

Formal Verification of Differential Privacy for Interactive Systems

  Differential privacy is a promising approach to privacy preserving dataanalysis with a well-developed theory for functions. Despite recent work onimplementing systems that aim to provide differential privacy, the problem offormally verifying that these systems have differential privacy has not beenadequately addressed. This paper presents the first results towards automatedverification of source code for differentially private interactive systems. Wedevelop a formal probabilistic automaton model of differential privacy forsystems by adapting prior work on differential privacy for functions. The maintechnical result of the paper is a sound proof technique based on a form ofprobabilistic bisimulation relation for proving that a system modeled as aprobabilistic automaton satisfies differential privacy. The novelty lies in theway we track quantitative privacy leakage bounds using a relation familyinstead of a single relation. We illustrate the proof technique on arepresentative automaton motivated by PINQ, an implemented system that isintended to provide differential privacy. To make our proof technique easier toapply to realistic systems, we prove a form of refinement theorem and apply itto show that a refinement of the abstract PINQ automaton also satisfies ourdifferential privacy definition. Finally, we begin the process of automatingour proof technique by providing an algorithm for mechanically checking arestricted class of relations from the proof technique.

A Logical Method for Policy Enforcement over Evolving Audit Logs

  We present an iterative algorithm for enforcing policies represented in afirst-order logic, which can, in particular, express all transmission-relatedclauses in the HIPAA Privacy Rule. The logic has three features that raisechallenges for enforcement --- uninterpreted predicates (used to modelsubjective concepts in privacy policies), real-time temporal properties, andquantification over infinite domains (such as the set of messages containingpersonal information). The algorithm operates over audit logs that areinherently incomplete and evolve over time. In each iteration, the algorithmprovably checks as much of the policy as possible over the current log andoutputs a residual policy that can only be checked when the log is extendedwith additional information. We prove correctness and termination properties ofthe algorithm. While these results are developed in a general form, accountingfor many different sources of incompleteness in audit logs, we also prove thatfor the special case of logs that maintain a complete record of all relevantactions, the algorithm effectively enforces all safety and co-safetyproperties. The algorithm can significantly help automate enforcement ofpolicies derived from the HIPAA Privacy Rule.

Naturally Rehearsing Passwords

  We introduce quantitative usability and security models to guide the designof password management schemes --- systematic strategies to help users createand remember multiple passwords. In the same way that security proofs incryptography are based on complexity-theoretic assumptions (e.g., hardness offactoring and discrete logarithm), we quantify usability by introducingusability assumptions. In particular, password management relies on assumptionsabout human memory, e.g., that a user who follows a particular rehearsalschedule will successfully maintain the corresponding memory. These assumptionsare informed by research in cognitive science and validated through empiricalstudies. Given rehearsal requirements and a user's visitation schedule for eachaccount, we use the total number of extra rehearsals that the user would haveto do to remember all of his passwords as a measure of the usability of thepassword scheme. Our usability model leads us to a key observation: passwordreuse benefits users not only by reducing the number of passwords that the userhas to memorize, but more importantly by increasing the natural rehearsal ratefor each password. We also present a security model which accounts for thecomplexity of password management with multiple accounts and associatedthreats, including online, offline, and plaintext password leak attacks.Observing that current password management schemes are either insecure orunusable, we present Shared Cues--- a new scheme in which the underlying secretis strategically shared across accounts to ensure that most rehearsalrequirements are satisfied naturally while simultaneously providing strongsecurity. The construction uses the Chinese Remainder Theorem to achieve thesecompeting goals.

GOTCHA Password Hackers!

  We introduce GOTCHAs (Generating panOptic Turing Tests to Tell Computers andHumans Apart) as a way of preventing automated offline dictionary attacksagainst user selected passwords. A GOTCHA is a randomized puzzle generationprotocol, which involves interaction between a computer and a human.Informally, a GOTCHA should satisfy two key properties: (1) The puzzles areeasy for the human to solve. (2) The puzzles are hard for a computer to solveeven if it has the random bits used by the computer to generate the finalpuzzle --- unlike a CAPTCHA. Our main theorem demonstrates that GOTCHAs can beused to mitigate the threat of offline dictionary attacks against passwords byensuring that a password cracker must receive constant feedback from a humanbeing while mounting an attack. Finally, we provide a candidate construction ofGOTCHAs based on Inkblot images. Our construction relies on the usabilityassumption that users can recognize the phrases that they originally used todescribe each Inkblot image --- a much weaker usability assumption thanprevious password systems based on Inkblots which required users to recalltheir phrase exactly. We conduct a user study to evaluate the usability of ourGOTCHA construction. We also generate a GOTCHA challenge where we encourageartificial intelligence and security researchers to try to crack severalpasswords protected with our scheme.

Use Privacy in Data-Driven Systems: Theory and Experiments with Machine  Learnt Programs

  This paper presents an approach to formalizing and enforcing a class of useprivacy properties in data-driven systems. In contrast to prior work, we focuson use restrictions on proxies (i.e. strong predictors) of protectedinformation types. Our definition relates proxy use to intermediatecomputations that occur in a program, and identify two essential propertiesthat characterize this behavior: 1) its result is strongly associated with theprotected information type in question, and 2) it is likely to causally affectthe final output of the program. For a specific instantiation of thisdefinition, we present a program analysis technique that detects instances ofproxy use in a model, and provides a witness that identifies which parts of thecorresponding program exhibit the behavior. Recognizing that not all instancesof proxy use of a protected information type are inappropriate, we make use ofa normative judgment oracle that makes this inappropriateness determination fora given witness. Our repair algorithm uses the witness of an inappropriateproxy use to transform the model into one that provably does not exhibit proxyuse, while avoiding changes that unduly affect classification accuracy. Using acorpus of social datasets, our evaluation shows that these algorithms are ableto detect proxy use instances that would be difficult to find using existingtechniques, and subsequently remove them while maintaining acceptableclassification performance.

Adaptive Regret Minimization in Bounded-Memory Games

  Online learning algorithms that minimize regret provide strong guarantees insituations that involve repeatedly making decisions in an uncertainenvironment, e.g. a driver deciding what route to drive to work every day.While regret minimization has been extensively studied in repeated games, westudy regret minimization for a richer class of games called bounded memorygames. In each round of a two-player bounded memory-m game, both playerssimultaneously play an action, observe an outcome and receive a reward. Thereward may depend on the last m outcomes as well as the actions of the playersin the current round. The standard notion of regret for repeated games is nolonger suitable because actions and rewards can depend on the history of play.To account for this generality, we introduce the notion of k-adaptive regret,which compares the reward obtained by playing actions prescribed by thealgorithm against a hypothetical k-adaptive adversary with the reward obtainedby the best expert in hindsight against the same adversary. Roughly, ahypothetical k-adaptive adversary adapts her strategy to the defender's actionsexactly as the real adversary would within each window of k rounds. Ourdefinition is parametrized by a set of experts, which can include both fixedand adaptive defender strategies.  We investigate the inherent complexity of and design algorithms for adaptiveregret minimization in bounded memory games of perfect and imperfectinformation. We prove a hardness result showing that, with imperfectinformation, any k-adaptive regret minimizing algorithm (with fixed strategiesas experts) must be inefficient unless NP=RP even when playing against anoblivious adversary. In contrast, for bounded memory games of perfect andimperfect information we present approximate 0-adaptive regret minimizationalgorithms against an oblivious adversary running in time n^{O(1)}.

The Johnson-Lindenstrauss Transform Itself Preserves Differential  Privacy

  This paper proves that an "old dog", namely -- the classicalJohnson-Lindenstrauss transform, "performs new tricks" -- it gives a novel wayof preserving differential privacy. We show that if we take two databases, $D$and $D'$, such that (i) $D'-D$ is a rank-1 matrix of bounded norm and (ii) allsingular values of $D$ and $D'$ are sufficiently large, then multiplying either$D$ or $D'$ with a vector of iid normal Gaussians yields two statisticallyclose distributions in the sense of differential privacy. Furthermore, a small,deterministic and \emph{public} alteration of the input is enough to assertthat all singular values of $D$ are large.  We apply the Johnson-Lindenstrauss transform to the task of approximatingcut-queries: the number of edges crossing a $(S,\bar S)$-cut in a graph. Weshow that the JL transform allows us to \emph{publish a sanitized graph} thatpreserves edge differential privacy (where two graphs are neighbors if theydiffer on a single edge) while adding only $O(|S|/\epsilon)$ random noise toany given query (w.h.p). Comparing the additive noise of our algorithm toexisting algorithms for answering cut-queries in a differentially privatemanner, we outperform all others on small cuts ($|S| = o(n)$).  We also apply our technique to the task of estimating the variance of a givenmatrix in any given direction. The JL transform allows us to \emph{publish asanitized covariance matrix} that preserves differential privacy w.r.t boundedchanges (each row in the matrix can change by at most a norm-1 vector) whileadding random noise of magnitude independent of the size of the matrix (w.h.p).In contrast, existing algorithms introduce an error which depends on the matrixdimensions.

Differentially Private Data Analysis of Social Networks via Restricted  Sensitivity

  We introduce the notion of restricted sensitivity as an alternative to globaland smooth sensitivity to improve accuracy in differentially private dataanalysis. The definition of restricted sensitivity is similar to that of globalsensitivity except that instead of quantifying over all possible datasets, wetake advantage of any beliefs about the dataset that a querier may have, toquantify over a restricted class of datasets. Specifically, given a query f anda hypothesis H about the structure of a dataset D, we show generically how totransform f into a new query f_H whose global sensitivity (over all datasetsincluding those that do not satisfy H) matches the restricted sensitivity ofthe query f. Moreover, if the belief of the querier is correct (i.e., D is inH) then f_H(D) = f(D). If the belief is incorrect, then f_H(D) may beinaccurate.  We demonstrate the usefulness of this notion by considering the task ofanswering queries regarding social-networks, which we model as a combination ofa graph and a labeling of its vertices. In particular, while our genericprocedure is computationally inefficient, for the specific definition of H asgraphs of bounded degree, we exhibit efficient ways of constructing f_H usingdifferent projection-based techniques. We then analyze two important queryclasses: subgraph counting queries (e.g., number of triangles) and localprofile queries (e.g., number of people who know a spy and a computer-scientistwho know each other). We demonstrate that the restricted sensitivity of suchqueries can be significantly lower than their smooth sensitivity. Thus, usingrestricted sensitivity we can maintain privacy whether or not D is in H, whileproviding more accurate results in the event that H holds true.

Towards Human Computable Passwords

  An interesting challenge for the cryptography community is to designauthentication protocols that are so simple that a human can execute themwithout relying on a fully trusted computer. We propose several candidateauthentication protocols for a setting in which the human user can only receiveassistance from a semi-trusted computer --- a computer that stores informationand performs computations correctly but does not provide confidentiality. Ourschemes use a semi-trusted computer to store and display public challenges$C_i\in[n]^k$. The human user memorizes a random secret mapping$\sigma:[n]\rightarrow\mathbb{Z}_d$ and authenticates by computing responses$f(\sigma(C_i))$ to a sequence of public challenges where$f:\mathbb{Z}_d^k\rightarrow\mathbb{Z}_d$ is a function that is easy for thehuman to evaluate. We prove that any statistical adversary needs to sample$m=\tilde{\Omega}(n^{s(f)})$ challenge-response pairs to recover $\sigma$, fora security parameter $s(f)$ that depends on two key properties of $f$. Toobtain our results, we apply the general hypercontractivity theorem to lowerbound the statistical dimension of the distribution over challenge-responsepairs induced by $f$ and $\sigma$. Our lower bounds apply to arbitraryfunctions $f $ (not just to functions that are easy for a human to evaluate),and generalize recent results of Feldman et al. As an application, we propose afamily of human computable password functions $f_{k_1,k_2}$ in which the userneeds to perform $2k_1+2k_2+1$ primitive operations (e.g., adding two digits orremembering $\sigma(i)$), and we show that $s(f) = \min\{k_1+1, (k_2+1)/2\}$.For these schemes, we prove that forging passwords is equivalent to recoveringthe secret mapping. Thus, our human computable password schemes can maintainstrong security guarantees even after an adversary has observed the user loginto many different accounts.

Spaced Repetition and Mnemonics Enable Recall of Multiple Strong  Passwords

  We report on a user study that provides evidence that spaced repetition and aspecific mnemonic technique enable users to successfully recall multiple strongpasswords over time. Remote research participants were asked to memorize 4Person-Action-Object (PAO) stories where they chose a famous person from adrop-down list and were given machine-generated random action-object pairs.Users were also shown a photo of a scene and asked to imagine the PAO storytaking place in the scene (e.g., Bill Gates---swallowing---bike on a beach).Subsequently, they were asked to recall the action-object pairs when promptedwith the associated scene-person pairs following a spaced repetition scheduleover a period of 127+ days. While we evaluated several spaced repetitionschedules, the best results were obtained when users initially returned after12 hours and then in $1.5\times$ increasing intervals: 77% of the participantssuccessfully recalled all 4 stories in 10 tests over a period of 158 days. Muchof the forgetting happened in the first test period (12 hours): 89% ofparticipants who remembered their stories during the first test periodsuccessfully remembered them in every subsequent round. These findings, coupledwith recent results on naturally rehearsing password schemes, suggest that 4PAO stories could be used to create usable and strong passwords for 14sensitive accounts following this spaced repetition schedule, possibly with afew extra upfront rehearsals. In addition, we find that there is aninterference effect across multiple PAO stories: the recall rate of 100% (resp.90%) for participants who were asked to memorize 1 PAO story (resp. 2 PAOstories) is significantly better than the recall rate for participants who wereasked to memorize 4 PAO stories. These findings yield concrete advice forimproving constructions of password management schemes and future user studies.

CASH: A Cost Asymmetric Secure Hash Algorithm for Optimal Password  Protection

  An adversary who has obtained the cryptographic hash of a user's password canmount an offline attack to crack the password by comparing this hash value withthe cryptographic hashes of likely password guesses. This offline attacker islimited only by the resources he is willing to invest to crack the password.Key-stretching tools can help mitigate the threat of offline attacks by makingeach password guess more expensive for the adversary to verify. However,key-stretching increases authentication costs for a legitimate authenticationserver. We introduce a novel Stackelberg game model which captures theessential elements of this interaction between a defender and an offlineattacker. We then introduce Cost Asymmetric Secure Hash (CASH), a randomizedkey-stretching mechanism that minimizes the fraction of passwords that would becracked by a rational offline attacker without increasing amortizedauthentication costs for the legitimate authentication server. CASH ismotivated by the observation that the legitimate authentication server willtypically run the authentication procedure to verify a correct password, whilean offline adversary will typically use incorrect password guesses. By usingrandomization we can ensure that the amortized cost of running CASH to verify acorrect password guess is significantly smaller than the cost of rejecting anincorrect password. Using our Stackelberg game framework we can quantify thequality of the underlying CASH running time distribution in terms of thefraction of passwords that a rational offline adversary would crack. We providean efficient algorithm to compute high quality CASH distributions for thedefender. Finally, we analyze CASH using empirical data from two large scalepassword frequency datasets. Our analysis shows that CASH can significantlyreduce (up to $50\%$) the fraction of password cracked by a rational offlineadversary.

