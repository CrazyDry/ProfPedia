Programs as Actual Causes: A Building Block for Accountability

  An updated version of this paper is available at
http://arxiv.org/abs/1505.01131


Report on the NSF Workshop on Formal Methods for Security

  Report on the NSF Workshop on Formal Methods for Security, held 19-20
November 2015.


Influence in Classification via Cooperative Game Theory

  A dataset has been classified by some unknown classifier into two types of
points. What were the most important factors in determining the classification
outcome? In this work, we employ an axiomatic approach in order to uniquely
characterize an influence measure: a function that, given a set of classified
points, outputs a value for each feature corresponding to its influence in
determining the classification outcome. We show that our influence measure
takes on an intuitive form when the unknown classifier is linear. Finally, we
employ our influence measure in order to analyze the effects of user profiling
on Google's online display advertising.


Equivalence-based Security for Querying Encrypted Databases: Theory and
  Application to Privacy Policy Audits

  Motivated by the problem of simultaneously preserving confidentiality and
usability of data outsourced to third-party clouds, we present two different
database encryption schemes that largely hide data but reveal enough
information to support a wide-range of relational queries. We provide a
security definition for database encryption that captures confidentiality based
on a notion of equivalence of databases from the adversary's perspective. As a
specific application, we adapt an existing algorithm for finding violations of
privacy policies to run on logs encrypted under our schemes and observe low to
moderate overheads.


Proxy Non-Discrimination in Data-Driven Systems

  Machine learnt systems inherit biases against protected classes, historically
disparaged groups, from training data. Usually, these biases are not explicit,
they rely on subtle correlations discovered by training algorithms, and are
therefore difficult to detect. We formalize proxy discrimination in data-driven
systems, a class of properties indicative of bias, as the presence of protected
class correlates that have causal influence on the system's output. We evaluate
an implementation on a corpus of social datasets, demonstrating how to validate
systems against these properties and to repair violations where they occur.


Case Study: Explaining Diabetic Retinopathy Detection Deep CNNs via
  Integrated Gradients

  In this report, we applied integrated gradients to explaining a neural
network for diabetic retinopathy detection. The integrated gradient is an
attribution method which measures the contributions of input to the quantity of
interest. We explored some new ways for applying this method such as explaining
intermediate layers, filtering out unimportant units by their attribution value
and generating contrary samples. Moreover, the visualization results extend the
use of diabetic retinopathy detection model from merely predicting to assisting
finding potential lesions.


A Methodology for Information Flow Experiments

  Information flow analysis has largely ignored the setting where the analyst
has neither control over nor a complete model of the analyzed system. We
formalize such limited information flow analyses and study an instance of it:
detecting the usage of data by websites. We prove that these problems are ones
of causal inference. Leveraging this connection, we push beyond traditional
information flow analysis to provide a systematic methodology based on
experimental science and statistical analysis. Our methodology allows us to
systematize prior works in the area viewing them as instances of a general
approach. Our systematic study leads to practical advice for improving work on
detecting data usage, a previously unformalized area. We illustrate these
concepts with a series of experiments collecting data on the use of information
by websites, which we statistically analyze.


Automated Experiments on Ad Privacy Settings: A Tale of Opacity, Choice,
  and Discrimination

  To partly address people's concerns over web tracking, Google has created the
Ad Settings webpage to provide information about and some choice over the
profiles Google creates on users. We present AdFisher, an automated tool that
explores how user behaviors, Google's ads, and Ad Settings interact. AdFisher
can run browser-based experiments and analyze data using machine learning and
significance tests. Our tool uses a rigorous experimental design and
statistical analysis to ensure the statistical soundness of our results. We use
AdFisher to find that the Ad Settings was opaque about some features of a
user's profile, that it does provide some choice on ads, and that these choices
can lead to seemingly discriminatory ads. In particular, we found that visiting
webpages associated with substance abuse changed the ads shown but not the
settings page. We also found that setting the gender to female resulted in
getting fewer instances of an ad related to high paying jobs than setting it to
male. We cannot determine who caused these findings due to our limited
visibility into the ad ecosystem, which includes Google, advertisers, websites,
and users. Nevertheless, these results can form the starting point for deeper
investigations by either the companies themselves or by regulatory bodies.


On the Semantics of Purpose Requirements in Privacy Policies

  Privacy policies often place requirements on the purposes for which a
governed entity may use personal information. For example, regulations, such as
HIPAA, require that hospital employees use medical information for only certain
purposes, such as treatment. Thus, using formal or automated methods for
enforcing privacy policies requires a semantics of purpose requirements to
determine whether an action is for a purpose or not. We provide such a
semantics using a formalism based on planning. We model planning using a
modified version of Markov Decision Processes, which exclude redundant actions
for a formal definition of redundant. We use the model to formalize when a
sequence of actions is only for or not for a purpose. This semantics enables us
to provide an algorithm for automating auditing, and to describe formally and
compare rigorously previous enforcement methods.


Audit Games

  Effective enforcement of laws and policies requires expending resources to
prevent and detect offenders, as well as appropriate punishment schemes to
deter violators. In particular, enforcement of privacy laws and policies in
modern organizations that hold large volumes of personal information (e.g.,
hospitals, banks, and Web services providers) relies heavily on internal audit
mechanisms. We study economic considerations in the design of these mechanisms,
focusing in particular on effective resource allocation and appropriate
punishment schemes. We present an audit game model that is a natural
generalization of a standard security game model for resource allocation with
an additional punishment parameter. Computing the Stackelberg equilibrium for
this game is challenging because it involves solving an optimization problem
with non-convex quadratic constraints. We present an additive FPTAS that
efficiently computes a solution that is arbitrarily close to the optimal
solution.


Audit Games with Multiple Defender Resources

  Modern organizations (e.g., hospitals, social networks, government agencies)
rely heavily on audit to detect and punish insiders who inappropriately access
and disclose confidential information. Recent work on audit games models the
strategic interaction between an auditor with a single audit resource and
auditees as a Stackelberg game, augmenting associated well-studied security
games with a configurable punishment parameter. We significantly generalize
this audit game model to account for multiple audit resources where each
resource is restricted to audit a subset of all potential violations, thus
enabling application to practical auditing scenarios. We provide an FPTAS that
computes an approximately optimal solution to the resulting non-convex
optimization problem. The main technical novelty is in the design and
correctness proof of an optimization transformation that enables the
construction of this FPTAS. In addition, we experimentally demonstrate that
this transformation significantly speeds up computation of solutions for a
class of audit games and security games.


Program Actions as Actual Causes: A Building Block for Accountability

  Protocols for tasks such as authentication, electronic voting, and secure
multiparty computation ensure desirable security properties if agents follow
their prescribed programs. However, if some agents deviate from their
prescribed programs and a security property is violated, it is important to
hold agents accountable by determining which deviations actually caused the
violation. Motivated by these applications, we initiate a formal study of
program actions as actual causes. Specifically, we define in an interacting
program model what it means for a set of program actions to be an actual cause
of a violation. We present a sound technique for establishing program actions
as actual causes. We demonstrate the value of this formalism in two ways.
First, we prove that violations of a specific class of safety properties always
have an actual cause. Thus, our definition applies to relevant security
properties. Second, we provide a cause analysis of a representative protocol
designed to address weaknesses in the current public key certification
infrastructure.


A Symbolic Logic with Concrete Bounds for Cryptographic Protocols

  We present a formal logic for quantitative reasoning about security
properties of network protocols. The system allows us to derive concrete
security bounds that can be used to choose key lengths and other security
parameters. We provide axioms for reasoning about digital signatures and random
nonces, with security properties based on the concrete security of signature
schemes and pseudorandom number generators (PRG). The formal logic supports
first-order reasoning and reasoning about protocol invariants, taking concrete
security bounds into account. Proofs constructed in our logic also provide
conventional asymptotic security guarantees because of the way that concrete
bounds accumulate in proofs. As an illustrative example, we use the formal
logic to prove an authentication property with concrete bounds of a
signature-based challenge-response protocol.


Information Flow for Security in Control Systems

  This paper considers the development of information flow analyses to support
resilient design and active detection of adversaries in cyber physical systems
(CPS). The area of CPS security, though well studied, suffers from
fragmentation. In this paper, we consider control systems as an abstraction of
CPS. Here, we extend the notion of information flow analysis, a well
established set of methods developed in software security, to obtain a unified
framework that captures and extends system theoretic results in control system
security. In particular, we propose the Kullback Liebler (KL) divergence as a
causal measure of information flow, which quantifies the effect of adversarial
inputs on sensor outputs. We show that the proposed measure characterizes the
resilience of control systems to specific attack strategies by relating the KL
divergence to optimal detection techniques. We then relate information flows to
stealthy attack scenarios where an adversary can bypass detection. Finally,
this article examines active detection mechanisms where a defender
intelligently manipulates control inputs or the system itself in order to
elicit information flows from an attacker's malicious behavior. In all previous
cases, we demonstrate an ability to investigate and extend existing results by
utilizing the proposed information flow analyses.


System M: A Program Logic for Code Sandboxing and Identification

  Security-sensitive applications that execute untrusted code often check the
code's integrity by comparing its syntax to a known good value or sandbox the
code to contain its effects. System M is a new program logic for reasoning
about such security-sensitive applications. System M extends Hoare Type Theory
(HTT) to trace safety properties and, additionally, contains two new reasoning
principles. First, its type system internalizes logical equality, facilitating
reasoning about applications that check code integrity. Second, a confinement
rule assigns an effect type to a computation based solely on knowledge of the
computation's sandbox. We prove the soundness of system M relative to a
step-indexed trace-based semantic model. We illustrate both new reasoning
principles of system M by verifying the main integrity property of the design
of Memoir, a previously proposed trusted computing system for ensuring state
continuity of isolated security-sensitive applications.


Differential Privacy as a Causal Property

  We present associative and causal views of differential privacy. Under the
associative view, the possibility of dependencies between data points precludes
a simple statement of differential privacy's guarantee as conditioning upon a
single changed data point. However, we show that a simple characterization of
differential privacy as limiting the effect of a single data point does exist
under the causal view, without independence assumptions about data points. We
believe this characterization resolves disagreement and confusion in prior work
about the consequences of differential privacy. The associative view needing
assumptions boils down to the contrapositive of the maxim that correlation
doesn't imply causation: differential privacy ensuring a lack of (strong)
causation does not imply a lack of (strong) association. Our characterization
also opens up the possibility of applying results from statistics, experimental
design, and science about causation while studying differential privacy.


Latent Factor Interpretations for Collaborative Filtering

  Many machine learning systems utilize latent factors as internal
representations for making predictions. Since these latent factors are largely
uninterpreted, however, predictions made using them are opaque. Collaborative
filtering via matrix factorization is a prime example of such an algorithm that
uses uninterpreted latent features, and yet has seen widespread adoption for
many recommendation tasks. We present Latent Factor Interpretation (LFI), a
method for interpreting models by leveraging interpretations of latent factors
in terms of human-understandable features. The interpretation of latent factors
can then replace the uninterpreted latent factors, resulting in a new model
that expresses predictions in terms of interpretable features. This new model
can then be interpreted using recently developed model explanation techniques.
In this paper we develop LFI for collaborative filtering based recommender
systems. We illustrate the use of LFI interpretations on the MovieLens dataset,
integrating auxiliary features from IMDB and DB tropes, and show that latent
factors can be predicted with sufficient accuracy for replicating the
predictions of the true model.


Influence-Directed Explanations for Deep Convolutional Networks

  We study the problem of explaining a rich class of behavioral properties of
deep neural networks. Distinctively, our influence-directed explanations
approach this problem by peering inside the network to identify neurons with
high influence on a quantity and distribution of interest, using an
axiomatically-justified influence measure, and then providing an interpretation
for the concepts these neurons represent. We evaluate our approach by
demonstrating a number of its unique capabilities on convolutional neural
networks trained on ImageNet. Our evaluation demonstrates that
influence-directed explanations (1) identify influential concepts that
generalize across instances, (2) can be used to extract the "essence" of what
the network learned about a class, and (3) isolate individual features the
network uses to make decisions and distinguish related classes.


Supervising Feature Influence

  Causal influence measures for machine learnt classifiers shed light on the
reasons behind classification, and aid in identifying influential input
features and revealing their biases. However, such analyses involve evaluating
the classifier using datapoints that may be atypical of its training
distribution. Standard methods for training classifiers that minimize empirical
risk do not constrain the behavior of the classifier on such datapoints. As a
result, training to minimize empirical risk does not distinguish among
classifiers that agree on predictions in the training distribution but have
wildly different causal influences. We term this problem covariate shift in
causal testing and formally characterize conditions under which it arises. As a
solution to this problem, we propose a novel active learning algorithm that
constrains the influence measures of the trained model. We prove that any two
predictors whose errors are close on both the original training distribution
and the distribution of atypical points are guaranteed to have causal
influences that are also close. Further, we empirically demonstrate with
synthetic labelers that our algorithm trains models that (i) have similar
causal influences as the labeler's model, and (ii) generalize better to
out-of-distribution points while (iii) retaining their accuracy on
in-distribution points.


Gender Bias in Neural Natural Language Processing

  We examine whether neural natural language processing (NLP) systems reflect
historical biases in training data. We define a general benchmark to quantify
gender bias in a variety of neural NLP tasks. Our empirical evaluation with
state-of-the-art neural coreference resolution and textbook RNN-based language
models trained on benchmark datasets finds significant gender bias in how
models view occupations. We then mitigate bias with CDA: a generic methodology
for corpus augmentation via causal interventions that breaks associations
between gendered and gender-neutral words. We empirically show that CDA
effectively decreases gender bias while preserving accuracy. We also explore
the space of mitigation strategies with CDA, a prior approach to word embedding
debiasing (WED), and their compositions. We show that CDA outperforms WED,
drastically so when word embeddings are trained. For pre-trained embeddings,
the two methods can be effectively composed. We also find that as training
proceeds on the original data set with gradient descent the gender bias grows
as the loss reduces, indicating that the optimization encourages bias; CDA
mitigates this behavior.


Correspondences between Privacy and Nondiscrimination: Why They Should
  Be Studied Together

  Privacy and nondiscrimination are related but different. We make this
observation precise in two ways. First, we show that both privacy and
nondiscrimination have two versions, a causal version and a statical
associative version, with each version corresponding to a competing view of the
proper goal of privacy or nondiscrimination. Second, for each version, we show
that a difference between the privacy edition of the version and the
nondiscrimination edition of the version is related to the difference between
Bayesian probabilities and frequentist probabilities. In particular, privacy
admits both Bayesian and frequentist interpretations whereas nondiscrimination
is limited to the frequentist interpretation. We show how the introduced
correspondence allows results from one area of research to be used for the
other.


Hunting for Discriminatory Proxies in Linear Regression Models

  A machine learning model may exhibit discrimination when used to make
decisions involving people. One potential cause for such outcomes is that the
model uses a statistical proxy for a protected demographic attribute. In this
paper we formulate a definition of proxy use for the setting of linear
regression and present algorithms for detecting proxies. Our definition follows
recent work on proxies in classification models, and characterizes a model's
constituent behavior that: 1) correlates closely with a protected random
variable, and 2) is causally influential in the overall behavior of the model.
We show that proxies in linear regression models can be efficiently identified
by solving a second-order cone program, and further extend this result to
account for situations where the use of a certain input variable is justified
as a `business necessity'. Finally, we present empirical results on two law
enforcement datasets that exhibit varying degrees of racial disparity in
prediction outcomes, demonstrating that proxies shed useful light on the causes
of discriminatory behavior in models.


Feature-Wise Bias Amplification

  We study the phenomenon of bias amplification in classifiers, wherein a
machine learning model learns to predict classes with a greater disparity than
the underlying ground truth. We demonstrate that bias amplification can arise
via an inductive bias in gradient descent methods that results in the
overestimation of the importance of moderately-predictive "weak" features if
insufficient training data is available. This overestimation gives rise to
feature-wise bias amplification -- a previously unreported form of bias that
can be traced back to the features of a trained model. Through analysis and
experiments, we show that while some bias cannot be mitigated without
sacrificing accuracy, feature-wise bias amplification can be mitigated through
targeted feature selection. We present two new feature selection algorithms for
mitigating bias amplification in linear models, and show how they can be
adapted to convolutional neural networks efficiently. Our experiments on
synthetic and real data demonstrate that these algorithms consistently lead to
reduced bias without harming accuracy, in some cases eliminating predictive
bias altogether while providing modest gains in accuracy.


Formal Verification of Differential Privacy for Interactive Systems

  Differential privacy is a promising approach to privacy preserving data
analysis with a well-developed theory for functions. Despite recent work on
implementing systems that aim to provide differential privacy, the problem of
formally verifying that these systems have differential privacy has not been
adequately addressed. This paper presents the first results towards automated
verification of source code for differentially private interactive systems. We
develop a formal probabilistic automaton model of differential privacy for
systems by adapting prior work on differential privacy for functions. The main
technical result of the paper is a sound proof technique based on a form of
probabilistic bisimulation relation for proving that a system modeled as a
probabilistic automaton satisfies differential privacy. The novelty lies in the
way we track quantitative privacy leakage bounds using a relation family
instead of a single relation. We illustrate the proof technique on a
representative automaton motivated by PINQ, an implemented system that is
intended to provide differential privacy. To make our proof technique easier to
apply to realistic systems, we prove a form of refinement theorem and apply it
to show that a refinement of the abstract PINQ automaton also satisfies our
differential privacy definition. Finally, we begin the process of automating
our proof technique by providing an algorithm for mechanically checking a
restricted class of relations from the proof technique.


A Logical Method for Policy Enforcement over Evolving Audit Logs

  We present an iterative algorithm for enforcing policies represented in a
first-order logic, which can, in particular, express all transmission-related
clauses in the HIPAA Privacy Rule. The logic has three features that raise
challenges for enforcement --- uninterpreted predicates (used to model
subjective concepts in privacy policies), real-time temporal properties, and
quantification over infinite domains (such as the set of messages containing
personal information). The algorithm operates over audit logs that are
inherently incomplete and evolve over time. In each iteration, the algorithm
provably checks as much of the policy as possible over the current log and
outputs a residual policy that can only be checked when the log is extended
with additional information. We prove correctness and termination properties of
the algorithm. While these results are developed in a general form, accounting
for many different sources of incompleteness in audit logs, we also prove that
for the special case of logs that maintain a complete record of all relevant
actions, the algorithm effectively enforces all safety and co-safety
properties. The algorithm can significantly help automate enforcement of
policies derived from the HIPAA Privacy Rule.


Naturally Rehearsing Passwords

  We introduce quantitative usability and security models to guide the design
of password management schemes --- systematic strategies to help users create
and remember multiple passwords. In the same way that security proofs in
cryptography are based on complexity-theoretic assumptions (e.g., hardness of
factoring and discrete logarithm), we quantify usability by introducing
usability assumptions. In particular, password management relies on assumptions
about human memory, e.g., that a user who follows a particular rehearsal
schedule will successfully maintain the corresponding memory. These assumptions
are informed by research in cognitive science and validated through empirical
studies. Given rehearsal requirements and a user's visitation schedule for each
account, we use the total number of extra rehearsals that the user would have
to do to remember all of his passwords as a measure of the usability of the
password scheme. Our usability model leads us to a key observation: password
reuse benefits users not only by reducing the number of passwords that the user
has to memorize, but more importantly by increasing the natural rehearsal rate
for each password. We also present a security model which accounts for the
complexity of password management with multiple accounts and associated
threats, including online, offline, and plaintext password leak attacks.
Observing that current password management schemes are either insecure or
unusable, we present Shared Cues--- a new scheme in which the underlying secret
is strategically shared across accounts to ensure that most rehearsal
requirements are satisfied naturally while simultaneously providing strong
security. The construction uses the Chinese Remainder Theorem to achieve these
competing goals.


GOTCHA Password Hackers!

  We introduce GOTCHAs (Generating panOptic Turing Tests to Tell Computers and
Humans Apart) as a way of preventing automated offline dictionary attacks
against user selected passwords. A GOTCHA is a randomized puzzle generation
protocol, which involves interaction between a computer and a human.
Informally, a GOTCHA should satisfy two key properties: (1) The puzzles are
easy for the human to solve. (2) The puzzles are hard for a computer to solve
even if it has the random bits used by the computer to generate the final
puzzle --- unlike a CAPTCHA. Our main theorem demonstrates that GOTCHAs can be
used to mitigate the threat of offline dictionary attacks against passwords by
ensuring that a password cracker must receive constant feedback from a human
being while mounting an attack. Finally, we provide a candidate construction of
GOTCHAs based on Inkblot images. Our construction relies on the usability
assumption that users can recognize the phrases that they originally used to
describe each Inkblot image --- a much weaker usability assumption than
previous password systems based on Inkblots which required users to recall
their phrase exactly. We conduct a user study to evaluate the usability of our
GOTCHA construction. We also generate a GOTCHA challenge where we encourage
artificial intelligence and security researchers to try to crack several
passwords protected with our scheme.


Use Privacy in Data-Driven Systems: Theory and Experiments with Machine
  Learnt Programs

  This paper presents an approach to formalizing and enforcing a class of use
privacy properties in data-driven systems. In contrast to prior work, we focus
on use restrictions on proxies (i.e. strong predictors) of protected
information types. Our definition relates proxy use to intermediate
computations that occur in a program, and identify two essential properties
that characterize this behavior: 1) its result is strongly associated with the
protected information type in question, and 2) it is likely to causally affect
the final output of the program. For a specific instantiation of this
definition, we present a program analysis technique that detects instances of
proxy use in a model, and provides a witness that identifies which parts of the
corresponding program exhibit the behavior. Recognizing that not all instances
of proxy use of a protected information type are inappropriate, we make use of
a normative judgment oracle that makes this inappropriateness determination for
a given witness. Our repair algorithm uses the witness of an inappropriate
proxy use to transform the model into one that provably does not exhibit proxy
use, while avoiding changes that unduly affect classification accuracy. Using a
corpus of social datasets, our evaluation shows that these algorithms are able
to detect proxy use instances that would be difficult to find using existing
techniques, and subsequently remove them while maintaining acceptable
classification performance.


Differentially Private Data Analysis of Social Networks via Restricted
  Sensitivity

  We introduce the notion of restricted sensitivity as an alternative to global
and smooth sensitivity to improve accuracy in differentially private data
analysis. The definition of restricted sensitivity is similar to that of global
sensitivity except that instead of quantifying over all possible datasets, we
take advantage of any beliefs about the dataset that a querier may have, to
quantify over a restricted class of datasets. Specifically, given a query f and
a hypothesis H about the structure of a dataset D, we show generically how to
transform f into a new query f_H whose global sensitivity (over all datasets
including those that do not satisfy H) matches the restricted sensitivity of
the query f. Moreover, if the belief of the querier is correct (i.e., D is in
H) then f_H(D) = f(D). If the belief is incorrect, then f_H(D) may be
inaccurate.
  We demonstrate the usefulness of this notion by considering the task of
answering queries regarding social-networks, which we model as a combination of
a graph and a labeling of its vertices. In particular, while our generic
procedure is computationally inefficient, for the specific definition of H as
graphs of bounded degree, we exhibit efficient ways of constructing f_H using
different projection-based techniques. We then analyze two important query
classes: subgraph counting queries (e.g., number of triangles) and local
profile queries (e.g., number of people who know a spy and a computer-scientist
who know each other). We demonstrate that the restricted sensitivity of such
queries can be significantly lower than their smooth sensitivity. Thus, using
restricted sensitivity we can maintain privacy whether or not D is in H, while
providing more accurate results in the event that H holds true.


Towards Human Computable Passwords

  An interesting challenge for the cryptography community is to design
authentication protocols that are so simple that a human can execute them
without relying on a fully trusted computer. We propose several candidate
authentication protocols for a setting in which the human user can only receive
assistance from a semi-trusted computer --- a computer that stores information
and performs computations correctly but does not provide confidentiality. Our
schemes use a semi-trusted computer to store and display public challenges
$C_i\in[n]^k$. The human user memorizes a random secret mapping
$\sigma:[n]\rightarrow\mathbb{Z}_d$ and authenticates by computing responses
$f(\sigma(C_i))$ to a sequence of public challenges where
$f:\mathbb{Z}_d^k\rightarrow\mathbb{Z}_d$ is a function that is easy for the
human to evaluate. We prove that any statistical adversary needs to sample
$m=\tilde{\Omega}(n^{s(f)})$ challenge-response pairs to recover $\sigma$, for
a security parameter $s(f)$ that depends on two key properties of $f$. To
obtain our results, we apply the general hypercontractivity theorem to lower
bound the statistical dimension of the distribution over challenge-response
pairs induced by $f$ and $\sigma$. Our lower bounds apply to arbitrary
functions $f $ (not just to functions that are easy for a human to evaluate),
and generalize recent results of Feldman et al. As an application, we propose a
family of human computable password functions $f_{k_1,k_2}$ in which the user
needs to perform $2k_1+2k_2+1$ primitive operations (e.g., adding two digits or
remembering $\sigma(i)$), and we show that $s(f) = \min\{k_1+1, (k_2+1)/2\}$.
For these schemes, we prove that forging passwords is equivalent to recovering
the secret mapping. Thus, our human computable password schemes can maintain
strong security guarantees even after an adversary has observed the user login
to many different accounts.


Spaced Repetition and Mnemonics Enable Recall of Multiple Strong
  Passwords

  We report on a user study that provides evidence that spaced repetition and a
specific mnemonic technique enable users to successfully recall multiple strong
passwords over time. Remote research participants were asked to memorize 4
Person-Action-Object (PAO) stories where they chose a famous person from a
drop-down list and were given machine-generated random action-object pairs.
Users were also shown a photo of a scene and asked to imagine the PAO story
taking place in the scene (e.g., Bill Gates---swallowing---bike on a beach).
Subsequently, they were asked to recall the action-object pairs when prompted
with the associated scene-person pairs following a spaced repetition schedule
over a period of 127+ days. While we evaluated several spaced repetition
schedules, the best results were obtained when users initially returned after
12 hours and then in $1.5\times$ increasing intervals: 77% of the participants
successfully recalled all 4 stories in 10 tests over a period of 158 days. Much
of the forgetting happened in the first test period (12 hours): 89% of
participants who remembered their stories during the first test period
successfully remembered them in every subsequent round. These findings, coupled
with recent results on naturally rehearsing password schemes, suggest that 4
PAO stories could be used to create usable and strong passwords for 14
sensitive accounts following this spaced repetition schedule, possibly with a
few extra upfront rehearsals. In addition, we find that there is an
interference effect across multiple PAO stories: the recall rate of 100% (resp.
90%) for participants who were asked to memorize 1 PAO story (resp. 2 PAO
stories) is significantly better than the recall rate for participants who were
asked to memorize 4 PAO stories. These findings yield concrete advice for
improving constructions of password management schemes and future user studies.


Adaptive Regret Minimization in Bounded-Memory Games

  Online learning algorithms that minimize regret provide strong guarantees in
situations that involve repeatedly making decisions in an uncertain
environment, e.g. a driver deciding what route to drive to work every day.
While regret minimization has been extensively studied in repeated games, we
study regret minimization for a richer class of games called bounded memory
games. In each round of a two-player bounded memory-m game, both players
simultaneously play an action, observe an outcome and receive a reward. The
reward may depend on the last m outcomes as well as the actions of the players
in the current round. The standard notion of regret for repeated games is no
longer suitable because actions and rewards can depend on the history of play.
To account for this generality, we introduce the notion of k-adaptive regret,
which compares the reward obtained by playing actions prescribed by the
algorithm against a hypothetical k-adaptive adversary with the reward obtained
by the best expert in hindsight against the same adversary. Roughly, a
hypothetical k-adaptive adversary adapts her strategy to the defender's actions
exactly as the real adversary would within each window of k rounds. Our
definition is parametrized by a set of experts, which can include both fixed
and adaptive defender strategies.
  We investigate the inherent complexity of and design algorithms for adaptive
regret minimization in bounded memory games of perfect and imperfect
information. We prove a hardness result showing that, with imperfect
information, any k-adaptive regret minimizing algorithm (with fixed strategies
as experts) must be inefficient unless NP=RP even when playing against an
oblivious adversary. In contrast, for bounded memory games of perfect and
imperfect information we present approximate 0-adaptive regret minimization
algorithms against an oblivious adversary running in time n^{O(1)}.


The Johnson-Lindenstrauss Transform Itself Preserves Differential
  Privacy

  This paper proves that an "old dog", namely -- the classical
Johnson-Lindenstrauss transform, "performs new tricks" -- it gives a novel way
of preserving differential privacy. We show that if we take two databases, $D$
and $D'$, such that (i) $D'-D$ is a rank-1 matrix of bounded norm and (ii) all
singular values of $D$ and $D'$ are sufficiently large, then multiplying either
$D$ or $D'$ with a vector of iid normal Gaussians yields two statistically
close distributions in the sense of differential privacy. Furthermore, a small,
deterministic and \emph{public} alteration of the input is enough to assert
that all singular values of $D$ are large.
  We apply the Johnson-Lindenstrauss transform to the task of approximating
cut-queries: the number of edges crossing a $(S,\bar S)$-cut in a graph. We
show that the JL transform allows us to \emph{publish a sanitized graph} that
preserves edge differential privacy (where two graphs are neighbors if they
differ on a single edge) while adding only $O(|S|/\epsilon)$ random noise to
any given query (w.h.p). Comparing the additive noise of our algorithm to
existing algorithms for answering cut-queries in a differentially private
manner, we outperform all others on small cuts ($|S| = o(n)$).
  We also apply our technique to the task of estimating the variance of a given
matrix in any given direction. The JL transform allows us to \emph{publish a
sanitized covariance matrix} that preserves differential privacy w.r.t bounded
changes (each row in the matrix can change by at most a norm-1 vector) while
adding random noise of magnitude independent of the size of the matrix (w.h.p).
In contrast, existing algorithms introduce an error which depends on the matrix
dimensions.


CASH: A Cost Asymmetric Secure Hash Algorithm for Optimal Password
  Protection

  An adversary who has obtained the cryptographic hash of a user's password can
mount an offline attack to crack the password by comparing this hash value with
the cryptographic hashes of likely password guesses. This offline attacker is
limited only by the resources he is willing to invest to crack the password.
Key-stretching tools can help mitigate the threat of offline attacks by making
each password guess more expensive for the adversary to verify. However,
key-stretching increases authentication costs for a legitimate authentication
server. We introduce a novel Stackelberg game model which captures the
essential elements of this interaction between a defender and an offline
attacker. We then introduce Cost Asymmetric Secure Hash (CASH), a randomized
key-stretching mechanism that minimizes the fraction of passwords that would be
cracked by a rational offline attacker without increasing amortized
authentication costs for the legitimate authentication server. CASH is
motivated by the observation that the legitimate authentication server will
typically run the authentication procedure to verify a correct password, while
an offline adversary will typically use incorrect password guesses. By using
randomization we can ensure that the amortized cost of running CASH to verify a
correct password guess is significantly smaller than the cost of rejecting an
incorrect password. Using our Stackelberg game framework we can quantify the
quality of the underlying CASH running time distribution in terms of the
fraction of passwords that a rational offline adversary would crack. We provide
an efficient algorithm to compute high quality CASH distributions for the
defender. Finally, we analyze CASH using empirical data from two large scale
password frequency datasets. Our analysis shows that CASH can significantly
reduce (up to $50\%$) the fraction of password cracked by a rational offline
adversary.


