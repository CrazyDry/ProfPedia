A Game Theoretic Framework for Incentives in P2P Systems

  Peer-To-Peer (P2P) networks are self-organizing, distributed systems, with no
centralized authority or infrastructure. Because of the voluntary
participation, the availability of resources in a P2P system can be highly
variable and unpredictable. In this paper, we use ideas from Game Theory to
study the interaction of strategic and rational peers, and propose a
differential service-based incentive scheme to improve the system's
performance.


Capturing an Evader in Polygonal Environments: A Complete Information
  Game

  Suppose an unpredictable evader is free to move around in a polygonal
environment of arbitrary complexity that is under full camera surveillance. How
many pursuers, each with the same maximum speed as the evader, are necessary
and sufficient to guarantee a successful capture of the evader? The pursuers
always know the evader's current position through the camera network, but need
to physically reach the evader to capture it. We allow the evader the knowledge
of the current positions of all the pursuers as well---this accords with the
standard worst-case analysis model, but also models a practical situation where
the evader has "hacked" into the surveillance system.
  Our main result is to prove that three pursuers are always sufficient and
sometimes necessary to capture the evader. The bound is independent of the
number of vertices or holes in the polygonal environment. The result should be
contrasted with the incomplete information pursuit-evasion where at least
{\Omega}(\surd h + log n) pursuers are required just for detecting the evader
in an environment with n vertices and h holes.


Power Aware Routing for Sensor Databases

  Wireless sensor networks offer the potential to span and monitor large
geographical areas inexpensively. Sensor network databases like TinyDB are the
dominant architectures to extract and manage data in such networks. Since
sensors have significant power constraints (battery life), and high
communication costs, design of energy efficient communication algorithms is of
great importance. The data flow in a sensor database is very different from
data flow in an ordinary network and poses novel challenges in designing
efficient routing algorithms. In this work we explore the problem of energy
efficient routing for various different types of database queries and show that
in general, this problem is NP-complete. We give a constant factor
approximation algorithm for one class of query, and for other queries give
heuristic algorithms. We evaluate the efficiency of the proposed algorithms by
simulation and demonstrate their near optimal performance for various network
sizes.


Distributed Navigation Algorithms for Sensor Networks

  We propose efficient distributed algorithms to aid navigation of a user
through a geographic area covered by sensors. The sensors sense the level of
danger at their locations and we use this information to find a safe path for
the user through the sensor field. Traditional distributed navigation
algorithms rely upon flooding the whole network with packets to find an optimal
safe path. To reduce the communication expense, we introduce the concept of a
skeleton graph which is a sparse subset of the true sensor network
communication graph. Using skeleton graphs we show that it is possible to find
approximate safe paths with much lower communication cost. We give tight
theoretical guarantees on the quality of our approximation and by simulation,
show the effectiveness of our algorithms in realistic sensor network
situations.


Untangling the Braid: Finding Outliers in a Set of Streams

  Monitoring the performance of large shared computing systems such as the
cloud computing infrastructure raises many challenging algorithmic problems.
One common problem is to track users with the largest deviation from the norm
(outliers), for some measure of performance. Taking a stream-computing
perspective, we can think of each user's performance profile as a stream of
numbers (such as response times), and the aggregate performance profile of the
shared infrastructure as a "braid" of these intermixed streams. The monitoring
system's goal then is to untangle this braid sufficiently to track the top k
outliers. This paper investigates the space complexity of one-pass algorithms
for approximating outliers of this kind, proves lower bounds using multi-party
communication complexity, and proposes small-memory heuristic algorithms. On
one hand, stream outliers are easily tracked for simple measures, such as max
or min, but our theoretical results rule out even good approximations for most
of the natural measures such as average, median, or the quantiles. On the other
hand, we show through simulation that our proposed heuristics perform quite
well for a variety of synthetic data.


k-Capture in Multiagent Pursuit Evasion, or the Lion and the Hyenas

  We consider the following generalization of the classical pursuit-evasion
problem, which we call k-capture. A group of n pursuers (hyenas) wish to
capture an evader (lion) who is free to move in an m-dimensional Euclidean
space, the pursuers and the evader can move with the same maximum speed, and at
least k pursuers must simultaneously reach the evader's location to capture it.
If fewer than k pursuers reach the evader, then those pursuers get destroyed by
the evader. Under what conditions can the evader be k-captured? We study this
problem in the discrete time, continuous space model and prove that k-capture
is possible if and only there exists a time when the evader lies in the
interior of the pursuers' k-Hull. When the pursuit occurs inside a compact,
convex subset of the Euclidean space, we show through an easy constructive
strategy that k-capture is always possible.


Convex Hulls under Uncertainty

  We study the convex-hull problem in a probabilistic setting, motivated by the
need to handle data uncertainty inherent in many applications, including sensor
databases, location-based services and computer vision. In our framework, the
uncertainty of each input site is described by a probability distribution over
a finite number of possible locations including a \emph{null} location to
account for non-existence of the point. Our results include both exact and
approximation algorithms for computing the probability of a query point lying
inside the convex hull of the input, time-space tradeoffs for the membership
queries, a connection between Tukey depth and membership queries, as well as a
new notion of $\some$-hull that may be a useful representation of uncertain
hulls.


Trackability with Imprecise Localization

  Imagine a tracking agent $P$ who wants to follow a moving target $Q$ in
$d$-dimensional Euclidean space. The tracker has access to a noisy location
sensor that reports an estimate $\tilde{Q}(t)$ of the target's true location
$Q(t)$ at time $t$, where $||Q(T) - \tilde{Q}(T)||$ represents the sensor's
localization error. We study the limits of tracking performance under this kind
of sensing imprecision. In particular, we investigate (1) what is $P$'s best
strategy to follow $Q$ if both $P$ and $Q$ can move with equal speed, (2) at
what rate does the distance $||Q(t) - P(t)||$ grow under worst-case
localization noise, (3) if $P$ wants to keep $Q$ within a prescribed distance
$L$, how much faster does it need to move, and (4) what is the effect of
obstacles on the tracking performance, etc. Under a relative error model of
noise, we are able to give upper and lower bounds for the worst-case tracking
performance, both with or without obstacles.


Block Crossings in Storyline Visualizations

  Storyline visualizations help visualize encounters of the characters in a
story over time. Each character is represented by an x-monotone curve that goes
from left to right. A meeting is represented by having the characters that
participate in the meeting run close together for some time. In order to keep
the visual complexity low, rather than just minimizing pairwise crossings of
curves, we propose to count block crossings, that is, pairs of intersecting
bundles of lines.
  Our main results are as follows. We show that minimizing the number of block
crossings is NP-hard, and we develop, for meetings of bounded size, a
constant-factor approximation. We also present two fixed-parameter algorithms
and, for meetings of size 2, a greedy heuristic that we evaluate
experimentally.


Medians and Beyond: New Aggregation Techniques for Sensor Networks

  Wireless sensor networks offer the potential to span and monitor large
geographical areas inexpensively. Sensors, however, have significant power
constraint (battery life), making communication very expensive. Another
important issue in the context of sensor-based information systems is that
individual sensor readings are inherently unreliable. In order to address these
two aspects, sensor database systems like TinyDB and Cougar enable in-network
data aggregation to reduce the communication cost and improve reliability. The
existing data aggregation techniques, however, are limited to relatively simple
types of queries such as SUM, COUNT, AVG, and MIN/MAX. In this paper we propose
a data aggregation scheme that significantly extends the class of queries that
can be answered using sensor networks. These queries include (approximate)
quantiles, such as the median, the most frequent data values, such as the
consensus value, a histogram of the data distribution, as well as range
queries. In our scheme, each sensor aggregates the data it has received from
other sensors into a fixed (user specified) size message. We provide strict
theoretical guarantees on the approximation quality of the queries in terms of
the message size. We evaluate the performance of our aggregation scheme by
simulation and demonstrate its accuracy, scalability and low resource
utilization for highly variable input data sets.


Memory Efficient De Bruijn Graph Construction

  Massively parallel DNA sequencing technologies are revolutionizing genomics
research. Billions of short reads generated at low costs can be assembled for
reconstructing the whole genomes. Unfortunately, the large memory footprint of
the existing de novo assembly algorithms makes it challenging to get the
assembly done for higher eukaryotes like mammals. In this work, we investigate
the memory issue of constructing de Bruijn graph, a core task in leading
assembly algorithms, which often consumes several hundreds of gigabytes memory
for large genomes. We propose a disk-based partition method, called Minimum
Substring Partitioning (MSP), to complete the task using less than 10 gigabytes
memory, without runtime slowdown. MSP breaks the short reads into multiple
small disjoint partitions so that each partition can be loaded into memory,
processed individually and later merged with others to form a de Bruijn graph.
By leveraging the overlaps among the k-mers (substring of length k), MSP
achieves astonishing compression ratio: The total size of partitions is reduced
from $\Theta(kn)$ to $\Theta(n)$, where $n$ is the size of the short read
database, and $k$ is the length of a $k$-mer. Experimental results show that
our method can build de Bruijn graphs using a commodity computer for any
large-volume sequence dataset.


Observability of Lattice Graphs

  We consider a graph observability problem: how many edge colors are needed
for an unlabeled graph so that an agent, walking from node to node, can
uniquely determine its location from just the observed color sequence of the
walk?
  Specifically, let G(n,d) be an edge-colored subgraph of d-dimensional
(directed or undirected) lattice of size n^d = n * n * ... * n. We say that
G(n,d) is t-observable if an agent can uniquely determine its current position
in the graph from the color sequence of any t-dimensional walk, where the
dimension is the number of different directions spanned by the edges of the
walk. A walk in an undirected lattice G(n,d) has dimension between 1 and d, but
a directed walk can have dimension between 1 and 2d because of two different
orientations for each axis.
  We derive bounds on the number of colors needed for t-observability. Our main
result is that Theta(n^(d/t)) colors are both necessary and sufficient for
t-observability of G(n,d), where d is considered a constant.
  This shows an interesting dependence of graph observability on the ratio
between the dimension of the lattice and that of the walk. In particular, the
number of colors for full-dimensional walks is Theta(n^(1/2)) in the directed
case, and Theta(n) in the undirected case, independent of the lattice
dimension.
  All of our results extend easily to non-square lattices: given a lattice
graph of size N = n_1 * n_2 * ... * n_d, the number of colors for
t-observability is Theta (N^(1/t)).


Efficient Algorithms for k-Regret Minimizing Sets

  A regret minimizing set Q is a small size representation of a much larger
database P so that user queries executed on Q return answers whose scores are
not much worse than those on the full dataset. In particular, a k-regret
minimizing set has the property that the regret ratio between the score of the
top-1 item in Q and the score of the top-k item in P is minimized, where the
score of an item is the inner product of the item's attributes with a user's
weight (preference) vector. The problem is challenging because we want to find
a single representative set Q whose regret ratio is small with respect to all
possible user weight vectors.
  We show that k-regret minimization is NP-Complete for all dimensions d >= 3.
This settles an open problem from Chester et al. [VLDB 2014], and resolves the
complexity status of the problem for all d: the problem is known to have
polynomial-time solution for d <= 2. In addition, we propose two new
approximation schemes for regret minimization, both with provable guarantees,
one based on coresets and another based on hitting sets. We also carry out
extensive experimental evaluation, and show that our schemes compute
regret-minimizing sets comparable in size to the greedy algorithm proposed in
[VLDB 14] but our schemes are significantly faster and scalable to large data
sets.


Analytic tractography: A closed-form solution for estimating local white
  matter connectivity with diffusion MRI

  White matter structures composed of myelinated axons in the living human
brain are primarily studied by diffusion-weighted MRI (dMRI). These long-range
projections are typically characterized in a two-step process: dMRI is used to
estimate the orientation of axons within each voxel, then these local
orientations are linked together to estimate the spatial extent of putative
white matter bundles. Tractography, the process of tracing bundles across
voxels, either requires computationally expensive (probabilistic) simulations
to model uncertainty in fiber orientation or ignores it completely
(deterministic). Probabilistic simulation necessarily generates a finite number
of trajectories, introducing "simulation error" to trajectory estimates. Here
we introduce a method to analytically (via a closed-form solution) take an
orientation distribution function (ODF) from each voxel and calculate the
probabilities that a trajectory projects from a voxel into each directly
adjacent voxel. We validate our method by demonstrating that probabilistic
simulations converge to our analytically computed probabilities at the voxel
level as the number of simulated seeds increases. We show that our method
accurately calculates the ground-truth transition probabilities from a phantom
dataset. As a demonstration, we incoroporate our analytic method for voxel
transition probabilities into the Voxel Graph framework, creating a
quantitative framework for assessing white matter structure that we call
"analytic tractography". The long-range connectivity problem is reduced to
finding paths in a graph whose adjacency structure reflects voxel-to-voxel
analytic transition probabilities. We demonstrate this approach performs
comparably to many current probabilistic and deterministic approaches at a
fraction of the computational cost. Open source software software is provided.


Approximating Dominating Set on Intersection Graphs of Rectangles and
  L-frames

  We consider the Minimum Dominating Set (MDS) problem on the intersection
graphs of geometric objects. Even for simple and widely-used geometric objects
such as rectangles, no sub-logarithmic approximation is known for the problem
and (perhaps surprisingly) the problem is NP-hard even when all the rectangles
are "anchored" at a diagonal line with slope -1 (Pandit, CCCG 2017). In this
paper, we first show that for any $\epsilon>0$, there exists a
$(2+\epsilon)$-approximation algorithm for the MDS problem on
"diagonal-anchored" rectangles, providing the first $O(1)$-approximation for
the problem on a non-trivial subclass of rectangles. It is not hard to see that
the MDS problem on "diagonal-anchored" rectangles is the same as the MDS
problem on "diagonal-anchored" L-frames: the union of a vertical and a
horizontal line segment that share an endpoint. As such, we also obtain a
$(2+\epsilon)$-approximation for the problem with "diagonal-anchored" L-frames.
On the other hand, we show that the problem is APX-hard in case the input
L-frames intersect the diagonal, or the horizontal segments of the L-frames
intersect a vertical line. However, as we show, the problem is linear-time
solvable in case the L-frames intersect a vertical as well as a horizontal
line. Finally, we consider the MDS problem in the so-called "edge intersection
model" and obtain a number of results, answering two questions posed by Mehrabi
(WAOA 2017).


