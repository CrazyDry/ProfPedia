Scalable Planning and Learning for Multiagent POMDPs: Extended Version

  Online, sample-based planning algorithms for POMDPs have shown great promise
in scaling to problems with large state spaces, but they become intractable for
large action and observation spaces. This is particularly problematic in
multiagent POMDPs where the action and observation space grows exponentially
with the number of agents. To combat this intractability, we propose a novel
scalable approach based on sample-based planning and factored value functions
that exploits structure present in many multiagent settings. This approach
applies not only in the planning case, but also in the Bayesian reinforcement
learning setting. Experimental results show that we are able to provide high
quality solutions to large multiagent planning and learning problems.


Optimizing Memory-Bounded Controllers for Decentralized POMDPs

  We present a memory-bounded optimization approach for solving
infinite-horizon decentralized POMDPs. Policies for each agent are represented
by stochastic finite state controllers. We formulate the problem of optimizing
these policies as a nonlinear program, leveraging powerful existing nonlinear
optimization techniques for solving the problem. While existing solvers only
guarantee locally optimal solutions, we show that our formulation produces
higher quality controllers than the state-of-the-art approach. We also
incorporate a shared source of randomness in the form of a correlation device
to further increase solution quality with only a limited increase in space and
time. Our experimental results show that nonlinear optimization can be used to
provide high quality, concise solutions to decentralized decision problems
under uncertainty.


Planning for Decentralized Control of Multiple Robots Under Uncertainty

  We describe a probabilistic framework for synthesizing control policies for
general multi-robot systems, given environment and sensor models and a cost
function. Decentralized, partially observable Markov decision processes
(Dec-POMDPs) are a general model of decision processes where a team of agents
must cooperate to optimize some objective (specified by a shared reward or cost
function) in the presence of uncertainty, but where communication limitations
mean that the agents cannot share their state, so execution must proceed in a
decentralized fashion. While Dec-POMDPs are typically intractable to solve for
real-world problems, recent research on the use of macro-actions in Dec-POMDPs
has significantly increased the size of problem that can be practically solved
as a Dec-POMDP. We describe this general model, and show how, in contrast to
most existing methods that are specialized to a particular problem class, it
can synthesize control policies that use whatever opportunities for
coordination are present in the problem, while balancing off uncertainty in
outcomes, sensor information, and information about other agents. We use three
variations on a warehouse task to show that a single planner of this type can
generate cooperative behavior using task allocation, direct communication, and
signaling, as appropriate.


Internal Friction and Urbach Energy Correlation

  The understanding of how the structure rules the several properties that
distinguish amorphous solids from crystals is important for a technological
progress, regarding several applications. The random network structure that
characterises amorphous solids and causes the loose of a long-range order
forces the research to a continuous efforts in order to explain all the
properties that differ from crystals, through the creation of models without
using the Bloch theorem, which is based on the periodicity of the lattice. This
aspect of amorphous structure is responsible of interesting feature. In this
work the optical properties, related to the electronic density of states, and
mechanical properties, regarding the internal friction, are compared by their
relationship with the topological defects of structure in a short-medium-range
order. In particular, by studying the optical and mechanical properties of
oxides suitable for gravitational-wave detectors, a correlation between the
Urbach energy, related to the exponential behaviour of the absorption edge
caused by the transition from localized to extended states in the electronic
density of states, and the internal friction, related to the energy dissipation
in a two level system model, has been found.


High-Reflection Coatings for Gravitational-Wave Detectors: State of The
  Art and Future Developments

  We report on the optical, mechanical and structural characterization of the
sputtered coating materials of Advanced LIGO, Advanced Virgo and KAGRA
gravitational-waves detectors. We present the latest results of our research
program aiming at decreasing coating thermal noise through doping, optimization
of deposition parameters and post-deposition annealing. Finally, we propose
sputtered Si3N4 as a candidate material for the mirrors of future detectors.


Scaling Up Decentralized MDPs Through Heuristic Search

  Decentralized partially observable Markov decision processes (Dec-POMDPs) are
rich models for cooperative decision-making under uncertainty, but are often
intractable to solve optimally (NEXP-complete). The transition and observation
independent Dec-MDP is a general subclass that has been shown to have
complexity in NP, but optimal algorithms for this subclass are still
inefficient in practice. In this paper, we first provide an updated proof that
an optimal policy does not depend on the histories of the agents, but only the
local observations. We then present a new algorithm based on heuristic search
that is able to expand search nodes by using constraint optimization. We show
experimental results comparing our approach with the state-of-the-art DecMDP
and Dec-POMDP solvers. These results show a reduction in computation time and
an increase in scalability by multiple orders of magnitude in a number of
benchmarks.


Stick-Breaking Policy Learning in Dec-POMDPs

  Expectation maximization (EM) has recently been shown to be an efficient
algorithm for learning finite-state controllers (FSCs) in large decentralized
POMDPs (Dec-POMDPs). However, current methods use fixed-size FSCs and often
converge to maxima that are far from optimal. This paper considers a
variable-size FSC to represent the local policy of each agent. These
variable-size FSCs are constructed using a stick-breaking prior, leading to a
new framework called \emph{decentralized stick-breaking policy representation}
(Dec-SBPR). This approach learns the controller parameters with a variational
Bayesian algorithm without having to assume that the Dec-POMDP model is
available. The performance of Dec-SBPR is demonstrated on several benchmark
problems, showing that the algorithm scales to large problems while
outperforming other state-of-the-art methods.


Decentralized Control of Partially Observable Markov Decision Processes
  using Belief Space Macro-actions

  The focus of this paper is on solving multi-robot planning problems in
continuous spaces with partial observability. Decentralized partially
observable Markov decision processes (Dec-POMDPs) are general models for
multi-robot coordination problems, but representing and solving Dec-POMDPs is
often intractable for large problems. To allow for a high-level representation
that is natural for multi-robot problems and scalable to large discrete and
continuous problems, this paper extends the Dec-POMDP model to the
decentralized partially observable semi-Markov decision process (Dec-POSMDP).
The Dec-POSMDP formulation allows asynchronous decision-making by the robots,
which is crucial in multi-robot domains. We also present an algorithm for
solving this Dec-POSMDP which is much more scalable than previous methods since
it can incorporate closed-loop belief space macro-actions in planning. These
macro-actions are automatically constructed to produce robust solutions. The
proposed method's performance is evaluated on a complex multi-robot package
delivery problem under uncertainty, showing that our approach can naturally
represent multi-robot problems and provide high-quality solutions for
large-scale problems.


Scalable Accelerated Decentralized Multi-Robot Policy Search in
  Continuous Observation Spaces

  This paper presents the first ever approach for solving
\emph{continuous-observation} Decentralized Partially Observable Markov
Decision Processes (Dec-POMDPs) and their semi-Markovian counterparts,
Dec-POSMDPs. This contribution is especially important in robotics, where a
vast number of sensors provide continuous observation data. A
continuous-observation policy representation is introduced using Stochastic
Kernel-based Finite State Automata (SK-FSAs). An SK-FSA search algorithm titled
Entropy-based Policy Search using Continuous Kernel Observations (EPSCKO) is
introduced and applied to the first ever continuous-observation
Dec-POMDP/Dec-POSMDP domain, where it significantly outperforms
state-of-the-art discrete approaches. This methodology is equally applicable to
Dec-POMDPs and Dec-POSMDPs, though the empirical analysis presented focuses on
Dec-POSMDPs due to their higher scalability. To improve convergence, an entropy
injection policy search acceleration approach for both continuous and discrete
observation cases is also developed and shown to improve convergence rates
without degrading policy quality.


Deep Decentralized Multi-task Multi-Agent Reinforcement Learning under
  Partial Observability

  Many real-world tasks involve multiple agents with partial observability and
limited communication. Learning is challenging in these settings due to local
viewpoints of agents, which perceive the world as non-stationary due to
concurrently-exploring teammates. Approaches that learn specialized policies
for individual tasks face problems when applied to the real world: not only do
agents have to learn and store distinct policies for each task, but in practice
identities of tasks are often non-observable, making these approaches
inapplicable. This paper formalizes and addresses the problem of multi-task
multi-agent reinforcement learning under partial observability. We introduce a
decentralized single-task learning approach that is robust to concurrent
interactions of teammates, and present an approach for distilling single-task
policies into a unified policy that performs well across multiple related
tasks, without explicit provision of task identity.


Correlated evolution of structure and mechanical loss of a sputtered
  silica film

  Energy dissipation in amorphous coatings severely affects high-precision
optical and quantum transducers. In order to isolate the source of coating
loss, we performed an extensive study of Raman scattering and mechanical loss
of a thermally-treated sputtered silica coating. Our results show that loss is
correlated with the population of three-membered rings of Si-O$_4$ tetrahedral
units, and support the evidence that thermal treatment reduces the density of
metastable states separated by a characteristic energy of about 0.5 eV, in
favour of an increase of the states separated by smaller activation energies.
Finally, we conclude that three-fold rings are involved in the relaxation
mechanisms only if they belong to more complex chain-like structures of 10 to
100 tetrahedra.


Near-Optimal Adversarial Policy Switching for Decentralized Asynchronous
  Multi-Agent Systems

  A key challenge in multi-robot and multi-agent systems is generating
solutions that are robust to other self-interested or even adversarial parties
who actively try to prevent the agents from achieving their goals. The
practicality of existing works addressing this challenge is limited to only
small-scale synchronous decision-making scenarios or a single agent planning
its best response against a single adversary with fixed, procedurally
characterized strategies. In contrast this paper considers a more realistic
class of problems where a team of asynchronous agents with limited observation
and communication capabilities need to compete against multiple strategic
adversaries with changing strategies. This problem necessitates agents that can
coordinate to detect changes in adversary strategies and plan the best response
accordingly. Our approach first optimizes a set of stratagems that represent
these best responses. These optimized stratagems are then integrated into a
unified policy that can detect and respond when the adversaries change their
strategies. The near-optimality of the proposed framework is established
theoretically as well as demonstrated empirically in simulation and hardware.


Learning in POMDPs with Monte Carlo Tree Search

  The POMDP is a powerful framework for reasoning under outcome and information
uncertainty, but constructing an accurate POMDP model is difficult.
Bayes-Adaptive Partially Observable Markov Decision Processes (BA-POMDPs)
extend POMDPs to allow the model to be learned during execution. BA-POMDPs are
a Bayesian RL approach that, in principle, allows for an optimal trade-off
between exploitation and exploration. Unfortunately, BA-POMDPs are currently
impractical to solve for any non-trivial domain. In this paper, we extend the
Monte-Carlo Tree Search method POMCP to BA-POMDPs and show that the resulting
method, which we call BA-POMCP, is able to tackle problems that previous
solution methods have been unable to solve. Additionally, we introduce several
techniques that exploit the BA-POMDP structure to improve the efficiency of
BA-POMCP along with proof of their convergence.


Efficient Eligibility Traces for Deep Reinforcement Learning

  Eligibility traces are an effective technique to accelerate reinforcement
learning by smoothly assigning credit to recently visited states. However,
their online implementation is incompatible with modern deep reinforcement
learning algorithms, which rely heavily on i.i.d. training data and offline
learning. We utilize an efficient, recursive method for computing
{\lambda}-returns offline that can provide the benefits of eligibility traces
to any value-estimation or actor-critic method. We demonstrate how our method
can be combined with DQN, DRQN, and A3C to greatly enhance the learning speed
of these algorithms when playing Atari 2600 games, even under partial
observability. Our results indicate several-fold improvements to sample
efficiency on Seaquest and Q*bert. We expect similar results for other
algorithms and domains not considered here, including those with continuous
actions.


Bayesian Reinforcement Learning in Factored POMDPs

  Bayesian approaches provide a principled solution to the
exploration-exploitation trade-off in Reinforcement Learning. Typical
approaches, however, either assume a fully observable environment or scale
poorly. This work introduces the Factored Bayes-Adaptive POMDP model, a
framework that is able to exploit the underlying structure while learning the
dynamics in partially observable systems. We also present a belief tracking
method to approximate the joint posterior over state and model variables, and
an adaptation of the Monte-Carlo Tree Search solution method, which together
are capable of solving the underlying problem near-optimally. Our method is
able to learn efficiently given a known factorization or also learn the
factorization and the model parameters at the same time. We demonstrate that
this approach is able to outperform current methods and tackle problems that
were previously infeasible.


Decentralized Likelihood Quantile Networks for Improving Performance in
  Deep Multi-Agent Reinforcement Learning

  Recent successes of value-based multi-agent deep reinforcement learning
employ optimism by limiting underestimation updates of value function
estimator, through carefully controlled learning rate (Omidshafiei et al.,
2017) or reduced update probability (Palmer et al., 2018). To achieve full
cooperation when learning independently, an agent must estimate the state
values contingent on having optimal teammates; therefore, value overestimation
is frequency injected to counteract negative effects caused by unobservable
teammate sub-optimal policies and explorations. Aiming to solve this issue
through automatic scheduling, this paper introduces a decentralized quantile
estimator, which we found empirically to be more stable, sample efficient and
more likely to converge to the joint optimal policy.


Policy Iteration for Decentralized Control of Markov Decision Processes

  Coordination of distributed agents is required for problems arising in many
areas, including multi-robot systems, networking and e-commerce. As a formal
framework for such problems, we use the decentralized partially observable
Markov decision process (DEC-POMDP). Though much work has been done on optimal
dynamic programming algorithms for the single-agent version of the problem,
optimal algorithms for the multiagent case have been elusive. The main
contribution of this paper is an optimal policy iteration algorithm for solving
DEC-POMDPs. The algorithm uses stochastic finite-state controllers to represent
policies. The solution can include a correlation device, which allows agents to
correlate their actions without communicating. This approach alternates between
expanding the controller and performing value-preserving transformations, which
modify the controller without sacrificing value. We present two efficient
value-preserving transformations: one can reduce the size of the controller and
the other can improve its value while keeping the size fixed. Empirical results
demonstrate the usefulness of value-preserving transformations in increasing
value while keeping controller size to a minimum. To broaden the applicability
of the approach, we also present a heuristic version of the policy iteration
algorithm, which sacrifices convergence to optimality. This algorithm further
reduces the size of the controllers at each step by assuming that probability
distributions over the other agents actions are known. While this assumption
may not hold in general, it helps produce higher quality solutions in our test
problems.


Incremental Clustering and Expansion for Faster Optimal Planning in
  Dec-POMDPs

  This article presents the state-of-the-art in optimal solution methods for
decentralized partially observable Markov decision processes (Dec-POMDPs),
which are general models for collaborative multiagent planning under
uncertainty. Building off the generalized multiagent A* (GMAA*) algorithm,
which reduces the problem to a tree of one-shot collaborative Bayesian games
(CBGs), we describe several advances that greatly expand the range of
Dec-POMDPs that can be solved optimally. First, we introduce lossless
incremental clustering of the CBGs solved by GMAA*, which achieves exponential
speedups without sacrificing optimality. Second, we introduce incremental
expansion of nodes in the GMAA* search tree, which avoids the need to expand
all children, the number of which is in the worst case doubly exponential in
the nodes depth. This is particularly beneficial when little clustering is
possible. In addition, we introduce new hybrid heuristic representations that
are more compact and thereby enable the solution of larger Dec-POMDPs. We
provide theoretical guarantees that, when a suitable heuristic is used, both
incremental clustering and incremental expansion yield algorithms that are both
complete and search equivalent. Finally, we present extensive empirical results
demonstrating that GMAA*-ICE, an algorithm that synthesizes these advances, can
optimally solve Dec-POMDPs of unprecedented size.


Semantic-level Decentralized Multi-Robot Decision-Making using
  Probabilistic Macro-Observations

  Robust environment perception is essential for decision-making on robots
operating in complex domains. Intelligent task execution requires principled
treatment of uncertainty sources in a robot's observation model. This is
important not only for low-level observations (e.g., accelerometer data), but
also for high-level observations such as semantic object labels. This paper
formalizes the concept of macro-observations in Decentralized Partially
Observable Semi-Markov Decision Processes (Dec-POSMDPs), allowing scalable
semantic-level multi-robot decision making. A hierarchical Bayesian approach is
used to model noise statistics of low-level classifier outputs, while
simultaneously allowing sharing of domain noise characteristics between
classes. Classification accuracy of the proposed macro-observation scheme,
called Hierarchical Bayesian Noise Inference (HBNI), is shown to exceed
existing methods. The macro-observation scheme is then integrated into a
Dec-POSMDP planner, with hardware experiments running onboard a team of dynamic
quadrotors in a challenging domain where noise-agnostic filtering fails. To the
best of our knowledge, this is the first demonstration of a real-time,
convolutional neural net-based classification framework running fully onboard a
team of quadrotors in a multi-robot decision-making domain.


Learning for Multi-robot Cooperation in Partially Observable Stochastic
  Environments with Macro-actions

  This paper presents a data-driven approach for multi-robot coordination in
partially-observable domains based on Decentralized Partially Observable Markov
Decision Processes (Dec-POMDPs) and macro-actions (MAs). Dec-POMDPs provide a
general framework for cooperative sequential decision making under uncertainty
and MAs allow temporally extended and asynchronous action execution. To date,
most methods assume the underlying Dec-POMDP model is known a priori or a full
simulator is available during planning time. Previous methods which aim to
address these issues suffer from local optimality and sensitivity to initial
conditions. Additionally, few hardware demonstrations involving a large team of
heterogeneous robots and with long planning horizons exist. This work addresses
these gaps by proposing an iterative sampling based Expectation-Maximization
algorithm (iSEM) to learn polices using only trajectory data containing
observations, MAs, and rewards. Our experiments show the algorithm is able to
achieve better solution quality than the state-of-the-art learning-based
methods. We implement two variants of multi-robot Search and Rescue (SAR)
domains (with and without obstacles) on hardware to demonstrate the learned
policies can effectively control a team of distributed robots to cooperate in a
partially observable stochastic environment.


Learning to Teach in Cooperative Multiagent Reinforcement Learning

  Collective human knowledge has clearly benefited from the fact that
innovations by individuals are taught to others through communication. Similar
to human social groups, agents in distributed learning systems would likely
benefit from communication to share knowledge and teach skills. The problem of
teaching to improve agent learning has been investigated by prior works, but
these approaches make assumptions that prevent application of teaching to
general multiagent problems, or require domain expertise for problems they can
apply to. This learning to teach problem has inherent complexities related to
measuring long-term impacts of teaching that compound the standard multiagent
coordination challenges. In contrast to existing works, this paper presents the
first general framework and algorithm for intelligent agents to learn to teach
in a multiagent environment. Our algorithm, Learning to Coordinate and Teach
Reinforcement (LeCTR), addresses peer-to-peer teaching in cooperative
multiagent reinforcement learning. Each agent in our approach learns both when
and what to advise, then uses the received advice to improve local learning.
Importantly, these roles are not fixed; these agents learn to assume the role
of student and/or teacher at the appropriate moments, requesting and providing
advice in order to improve teamwide performance and learning. Empirical
comparisons against state-of-the-art teaching methods show that our teaching
agents not only learn significantly faster, but also learn to coordinate in
tasks where existing methods fail.


Measurement of $V^0$ production ratios in $pp$ collisions at $\sqrt{s} =
  0.9$ and 7\,TeV

  The $\bar{\Lambda} / \Lambda$ and $\bar{\Lambda} / K^0_\mathrm{S}$ production
ratios are measured by the LHCb detector from $0.3\,\mathrm{nb}^{-1}$ of $pp$
collisions delivered by the LHC at $\sqrt{s} = 0.9$\,TeV and
$1.8\,\mathrm{nb}^{-1}$ at $\sqrt{s} = 7$\,TeV. Both ratios are presented as a
function of transverse momentum, $p_\mathrm{T}$, and rapidity, $y$, in the
ranges {$0.15 < p_\mathrm{T} < 2.50\,\mathrm{GeV}/c$} and {$2.0<y<4.5$}.
Results at the two energies are in good agreement as a function of rapidity
loss, $\Delta y = y_\mathrm{beam} - y$, and are consistent with previous
measurements. The ratio $\bar{\Lambda} / \Lambda$, measuring the transport of
baryon number from the collision into the detector, is smaller in data than
predicted in simulation, particularly at high rapidity. The ratio
$\bar{\Lambda} / K^0_\mathrm{S}$, measuring the baryon-to-meson suppression in
strange quark hadronisation, is significantly larger than expected.


Differential branching fraction and angular analysis of the decay
  $B_s^0\toφμ^{+}μ^{-}$

  The determination of the differential branching fraction and the first
angular analysis of the decay $B_s^0\to\phi\mu^{+}\mu^{-}$ are presented using
data, corresponding to an integrated luminosity of $1.0\,{\rm fb}^{-1}$,
collected by the LHCb experiment at $\sqrt{s}=7\,{\rm TeV}$. The differential
branching fraction is determined in bins of $q^{2}$, the invariant dimuon mass
squared. Integration over the full $q^{2}$ range yields a total branching
fraction of ${\cal B}(B_s^0\to\phi\mu^{+}\mu^{-}) = (7.07\,^{+0.64}_{-0.59}\pm
0.17 \pm 0.71)\times 10^{-7}$, where the first uncertainty is statistical, the
second systematic, and the third originates from the branching fraction of the
normalisation channel. An angular analysis is performed to determine the
angular observables $F_{\rm L}$, $S_3$, $A_6$, and $A_9$. The observables are
consistent with Standard Model expectations.


LHCb Detector Performance

  The LHCb detector is a forward spectrometer at the Large Hadron Collider
(LHC) at CERN. The experiment is designed for precision measurements of CP
violation and rare decays of beauty and charm hadrons. In this paper the
performance of the various LHCb sub-detectors and the trigger system are
described, using data taken from 2010 to 2012. It is shown that the design
criteria of the experiment have been met. The excellent performance of the
detector has allowed the LHCb collaboration to publish a wide range of physics
results, demonstrating LHCb's unique role, both as a heavy flavour experiment
and as a general purpose detector in the forward region.


Angular analysis of the $B^{0}\rightarrow K^{*0}μ^{+}μ^{-}$ decay
  using $3\,\mbox{fb}^{-1}$ of integrated luminosity

  An angular analysis of the $B^{0}\rightarrow K^{*0}(\rightarrow
K^{+}\pi^{-})\mu^{+}\mu^{-}$ decay is presented. The dataset corresponds to an
integrated luminosity of $3.0\,{\mbox{fb}^{-1}}$ of $pp$ collision data
collected at the LHCb experiment. The complete angular information from the
decay is used to determine $C\!P$-averaged observables and $C\!P$ asymmetries,
taking account of possible contamination from decays with the $K^{+}\pi^{-}$
system in an S-wave configuration. The angular observables and their
correlations are reported in bins of $q^2$, the invariant mass squared of the
dimuon system. The observables are determined both from an unbinned maximum
likelihood fit and by using the principal moments of the angular distribution.
In addition, by fitting for $q^2$-dependent decay amplitudes in the region
$1.1<q^{2}<6.0\mathrm{\,Ge\kern -0.1em V}^{2}/c^{4}$, the zero-crossing points
of several angular observables are computed. A global fit is performed to the
complete set of $C\!P$-averaged observables obtained from the maximum
likelihood fit. This fit indicates differences with predictions based on the
Standard Model at the level of 3.4 standard deviations. These differences could
be explained by contributions from physics beyond the Standard Model, or by an
unexpectedly large hadronic effect that is not accounted for in the Standard
Model predictions.


Measurement of $CP$ violation in $B^0 \rightarrow J/ψK^0_S$ decays

  Measurements are presented of the $CP$ violation observables $S$ and $C$ in
the decays of $B^0$ and $\overline{B}{}^0$ mesons to the $J/\psi K^0_S$ final
state. The data sample corresponds to an integrated luminosity of
$3.0\,\text{fb}^{-1}$ collected with the LHCb experiment in proton-proton
collisions at center-of-mass energies of $7$ and $8\,\text{TeV}$. The analysis
of the time evolution of $41500$ $B^0$ and $\overline{B}{}^0$ decays yields $S
= 0.731 \pm 0.035 \, \text{(stat)} \pm 0.020 \,\text{(syst)}$ and $C = -0.038
\pm 0.032 \, \text{(stat)} \pm 0.005\,\text{(syst)}$. In the Standard Model,
$S$ equals $\sin(2\beta)$ to a good level of precision. The values are
consistent with the current world averages and with the Standard Model
expectations.


Observation of the rare $B^0_s\toμ^+μ^-$ decay from the combined
  analysis of CMS and LHCb data

  A joint measurement is presented of the branching fractions
$B^0_s\to\mu^+\mu^-$ and $B^0\to\mu^+\mu^-$ in proton-proton collisions at the
LHC by the CMS and LHCb experiments. The data samples were collected in 2011 at
a centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysis
produces the first observation of the $B^0_s\to\mu^+\mu^-$ decay, with a
statistical significance exceeding six standard deviations, and the best
measurement of its branching fraction so far. Furthermore, evidence for the
$B^0\to\mu^+\mu^-$ decay is obtained with a statistical significance of three
standard deviations. The branching fraction measurements are statistically
compatible with SM predictions and impose stringent constraints on several
theories beyond the SM.


