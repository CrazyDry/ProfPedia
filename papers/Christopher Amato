Scalable Planning and Learning for Multiagent POMDPs: Extended Version

  Online, sample-based planning algorithms for POMDPs have shown great promisein scaling to problems with large state spaces, but they become intractable forlarge action and observation spaces. This is particularly problematic inmultiagent POMDPs where the action and observation space grows exponentiallywith the number of agents. To combat this intractability, we propose a novelscalable approach based on sample-based planning and factored value functionsthat exploits structure present in many multiagent settings. This approachapplies not only in the planning case, but also in the Bayesian reinforcementlearning setting. Experimental results show that we are able to provide highquality solutions to large multiagent planning and learning problems.

Optimizing Memory-Bounded Controllers for Decentralized POMDPs

  We present a memory-bounded optimization approach for solvinginfinite-horizon decentralized POMDPs. Policies for each agent are representedby stochastic finite state controllers. We formulate the problem of optimizingthese policies as a nonlinear program, leveraging powerful existing nonlinearoptimization techniques for solving the problem. While existing solvers onlyguarantee locally optimal solutions, we show that our formulation produceshigher quality controllers than the state-of-the-art approach. We alsoincorporate a shared source of randomness in the form of a correlation deviceto further increase solution quality with only a limited increase in space andtime. Our experimental results show that nonlinear optimization can be used toprovide high quality, concise solutions to decentralized decision problemsunder uncertainty.

Planning for Decentralized Control of Multiple Robots Under Uncertainty

  We describe a probabilistic framework for synthesizing control policies forgeneral multi-robot systems, given environment and sensor models and a costfunction. Decentralized, partially observable Markov decision processes(Dec-POMDPs) are a general model of decision processes where a team of agentsmust cooperate to optimize some objective (specified by a shared reward or costfunction) in the presence of uncertainty, but where communication limitationsmean that the agents cannot share their state, so execution must proceed in adecentralized fashion. While Dec-POMDPs are typically intractable to solve forreal-world problems, recent research on the use of macro-actions in Dec-POMDPshas significantly increased the size of problem that can be practically solvedas a Dec-POMDP. We describe this general model, and show how, in contrast tomost existing methods that are specialized to a particular problem class, itcan synthesize control policies that use whatever opportunities forcoordination are present in the problem, while balancing off uncertainty inoutcomes, sensor information, and information about other agents. We use threevariations on a warehouse task to show that a single planner of this type cangenerate cooperative behavior using task allocation, direct communication, andsignaling, as appropriate.

Internal Friction and Urbach Energy Correlation

  The understanding of how the structure rules the several properties thatdistinguish amorphous solids from crystals is important for a technologicalprogress, regarding several applications. The random network structure thatcharacterises amorphous solids and causes the loose of a long-range orderforces the research to a continuous efforts in order to explain all theproperties that differ from crystals, through the creation of models withoutusing the Bloch theorem, which is based on the periodicity of the lattice. Thisaspect of amorphous structure is responsible of interesting feature. In thiswork the optical properties, related to the electronic density of states, andmechanical properties, regarding the internal friction, are compared by theirrelationship with the topological defects of structure in a short-medium-rangeorder. In particular, by studying the optical and mechanical properties ofoxides suitable for gravitational-wave detectors, a correlation between theUrbach energy, related to the exponential behaviour of the absorption edgecaused by the transition from localized to extended states in the electronicdensity of states, and the internal friction, related to the energy dissipationin a two level system model, has been found.

High-Reflection Coatings for Gravitational-Wave Detectors: State of The  Art and Future Developments

  We report on the optical, mechanical and structural characterization of thesputtered coating materials of Advanced LIGO, Advanced Virgo and KAGRAgravitational-waves detectors. We present the latest results of our researchprogram aiming at decreasing coating thermal noise through doping, optimizationof deposition parameters and post-deposition annealing. Finally, we proposesputtered Si3N4 as a candidate material for the mirrors of future detectors.

Scaling Up Decentralized MDPs Through Heuristic Search

  Decentralized partially observable Markov decision processes (Dec-POMDPs) arerich models for cooperative decision-making under uncertainty, but are oftenintractable to solve optimally (NEXP-complete). The transition and observationindependent Dec-MDP is a general subclass that has been shown to havecomplexity in NP, but optimal algorithms for this subclass are stillinefficient in practice. In this paper, we first provide an updated proof thatan optimal policy does not depend on the histories of the agents, but only thelocal observations. We then present a new algorithm based on heuristic searchthat is able to expand search nodes by using constraint optimization. We showexperimental results comparing our approach with the state-of-the-art DecMDPand Dec-POMDP solvers. These results show a reduction in computation time andan increase in scalability by multiple orders of magnitude in a number ofbenchmarks.

Decentralized Control of Partially Observable Markov Decision Processes  using Belief Space Macro-actions

  The focus of this paper is on solving multi-robot planning problems incontinuous spaces with partial observability. Decentralized partiallyobservable Markov decision processes (Dec-POMDPs) are general models formulti-robot coordination problems, but representing and solving Dec-POMDPs isoften intractable for large problems. To allow for a high-level representationthat is natural for multi-robot problems and scalable to large discrete andcontinuous problems, this paper extends the Dec-POMDP model to thedecentralized partially observable semi-Markov decision process (Dec-POSMDP).The Dec-POSMDP formulation allows asynchronous decision-making by the robots,which is crucial in multi-robot domains. We also present an algorithm forsolving this Dec-POSMDP which is much more scalable than previous methods sinceit can incorporate closed-loop belief space macro-actions in planning. Thesemacro-actions are automatically constructed to produce robust solutions. Theproposed method's performance is evaluated on a complex multi-robot packagedelivery problem under uncertainty, showing that our approach can naturallyrepresent multi-robot problems and provide high-quality solutions forlarge-scale problems.

Stick-Breaking Policy Learning in Dec-POMDPs

  Expectation maximization (EM) has recently been shown to be an efficientalgorithm for learning finite-state controllers (FSCs) in large decentralizedPOMDPs (Dec-POMDPs). However, current methods use fixed-size FSCs and oftenconverge to maxima that are far from optimal. This paper considers avariable-size FSC to represent the local policy of each agent. Thesevariable-size FSCs are constructed using a stick-breaking prior, leading to anew framework called \emph{decentralized stick-breaking policy representation}(Dec-SBPR). This approach learns the controller parameters with a variationalBayesian algorithm without having to assume that the Dec-POMDP model isavailable. The performance of Dec-SBPR is demonstrated on several benchmarkproblems, showing that the algorithm scales to large problems whileoutperforming other state-of-the-art methods.

Scalable Accelerated Decentralized Multi-Robot Policy Search in  Continuous Observation Spaces

  This paper presents the first ever approach for solving\emph{continuous-observation} Decentralized Partially Observable MarkovDecision Processes (Dec-POMDPs) and their semi-Markovian counterparts,Dec-POSMDPs. This contribution is especially important in robotics, where avast number of sensors provide continuous observation data. Acontinuous-observation policy representation is introduced using StochasticKernel-based Finite State Automata (SK-FSAs). An SK-FSA search algorithm titledEntropy-based Policy Search using Continuous Kernel Observations (EPSCKO) isintroduced and applied to the first ever continuous-observationDec-POMDP/Dec-POSMDP domain, where it significantly outperformsstate-of-the-art discrete approaches. This methodology is equally applicable toDec-POMDPs and Dec-POSMDPs, though the empirical analysis presented focuses onDec-POSMDPs due to their higher scalability. To improve convergence, an entropyinjection policy search acceleration approach for both continuous and discreteobservation cases is also developed and shown to improve convergence rateswithout degrading policy quality.

Deep Decentralized Multi-task Multi-Agent Reinforcement Learning under  Partial Observability

  Many real-world tasks involve multiple agents with partial observability andlimited communication. Learning is challenging in these settings due to localviewpoints of agents, which perceive the world as non-stationary due toconcurrently-exploring teammates. Approaches that learn specialized policiesfor individual tasks face problems when applied to the real world: not only doagents have to learn and store distinct policies for each task, but in practiceidentities of tasks are often non-observable, making these approachesinapplicable. This paper formalizes and addresses the problem of multi-taskmulti-agent reinforcement learning under partial observability. We introduce adecentralized single-task learning approach that is robust to concurrentinteractions of teammates, and present an approach for distilling single-taskpolicies into a unified policy that performs well across multiple relatedtasks, without explicit provision of task identity.

Correlated evolution of structure and mechanical loss of a sputtered  silica film

  Energy dissipation in amorphous coatings severely affects high-precisionoptical and quantum transducers. In order to isolate the source of coatingloss, we performed an extensive study of Raman scattering and mechanical lossof a thermally-treated sputtered silica coating. Our results show that loss iscorrelated with the population of three-membered rings of Si-O$_4$ tetrahedralunits, and support the evidence that thermal treatment reduces the density ofmetastable states separated by a characteristic energy of about 0.5 eV, infavour of an increase of the states separated by smaller activation energies.Finally, we conclude that three-fold rings are involved in the relaxationmechanisms only if they belong to more complex chain-like structures of 10 to100 tetrahedra.

Near-Optimal Adversarial Policy Switching for Decentralized Asynchronous  Multi-Agent Systems

  A key challenge in multi-robot and multi-agent systems is generatingsolutions that are robust to other self-interested or even adversarial partieswho actively try to prevent the agents from achieving their goals. Thepracticality of existing works addressing this challenge is limited to onlysmall-scale synchronous decision-making scenarios or a single agent planningits best response against a single adversary with fixed, procedurallycharacterized strategies. In contrast this paper considers a more realisticclass of problems where a team of asynchronous agents with limited observationand communication capabilities need to compete against multiple strategicadversaries with changing strategies. This problem necessitates agents that cancoordinate to detect changes in adversary strategies and plan the best responseaccordingly. Our approach first optimizes a set of stratagems that representthese best responses. These optimized stratagems are then integrated into aunified policy that can detect and respond when the adversaries change theirstrategies. The near-optimality of the proposed framework is establishedtheoretically as well as demonstrated empirically in simulation and hardware.

Learning in POMDPs with Monte Carlo Tree Search

  The POMDP is a powerful framework for reasoning under outcome and informationuncertainty, but constructing an accurate POMDP model is difficult.Bayes-Adaptive Partially Observable Markov Decision Processes (BA-POMDPs)extend POMDPs to allow the model to be learned during execution. BA-POMDPs area Bayesian RL approach that, in principle, allows for an optimal trade-offbetween exploitation and exploration. Unfortunately, BA-POMDPs are currentlyimpractical to solve for any non-trivial domain. In this paper, we extend theMonte-Carlo Tree Search method POMCP to BA-POMDPs and show that the resultingmethod, which we call BA-POMCP, is able to tackle problems that previoussolution methods have been unable to solve. Additionally, we introduce severaltechniques that exploit the BA-POMDP structure to improve the efficiency ofBA-POMCP along with proof of their convergence.

Efficient Eligibility Traces for Deep Reinforcement Learning

  Eligibility traces are an effective technique to accelerate reinforcementlearning by smoothly assigning credit to recently visited states. However,their online implementation is incompatible with modern deep reinforcementlearning algorithms, which rely heavily on i.i.d. training data and offlinelearning. We utilize an efficient, recursive method for computing{\lambda}-returns offline that can provide the benefits of eligibility tracesto any value-estimation or actor-critic method. We demonstrate how our methodcan be combined with DQN, DRQN, and A3C to greatly enhance the learning speedof these algorithms when playing Atari 2600 games, even under partialobservability. Our results indicate several-fold improvements to sampleefficiency on Seaquest and Q*bert. We expect similar results for otheralgorithms and domains not considered here, including those with continuousactions.

Bayesian Reinforcement Learning in Factored POMDPs

  Bayesian approaches provide a principled solution to theexploration-exploitation trade-off in Reinforcement Learning. Typicalapproaches, however, either assume a fully observable environment or scalepoorly. This work introduces the Factored Bayes-Adaptive POMDP model, aframework that is able to exploit the underlying structure while learning thedynamics in partially observable systems. We also present a belief trackingmethod to approximate the joint posterior over state and model variables, andan adaptation of the Monte-Carlo Tree Search solution method, which togetherare capable of solving the underlying problem near-optimally. Our method isable to learn efficiently given a known factorization or also learn thefactorization and the model parameters at the same time. We demonstrate thatthis approach is able to outperform current methods and tackle problems thatwere previously infeasible.

Decentralized Likelihood Quantile Networks for Improving Performance in  Deep Multi-Agent Reinforcement Learning

  Recent successes of value-based multi-agent deep reinforcement learningemploy optimism by limiting underestimation updates of value functionestimator, through carefully controlled learning rate (Omidshafiei et al.,2017) or reduced update probability (Palmer et al., 2018). To achieve fullcooperation when learning independently, an agent must estimate the statevalues contingent on having optimal teammates; therefore, value overestimationis frequency injected to counteract negative effects caused by unobservableteammate sub-optimal policies and explorations. Aiming to solve this issuethrough automatic scheduling, this paper introduces a decentralized quantileestimator, which we found empirically to be more stable, sample efficient andmore likely to converge to the joint optimal policy.

Policy Iteration for Decentralized Control of Markov Decision Processes

  Coordination of distributed agents is required for problems arising in manyareas, including multi-robot systems, networking and e-commerce. As a formalframework for such problems, we use the decentralized partially observableMarkov decision process (DEC-POMDP). Though much work has been done on optimaldynamic programming algorithms for the single-agent version of the problem,optimal algorithms for the multiagent case have been elusive. The maincontribution of this paper is an optimal policy iteration algorithm for solvingDEC-POMDPs. The algorithm uses stochastic finite-state controllers to representpolicies. The solution can include a correlation device, which allows agents tocorrelate their actions without communicating. This approach alternates betweenexpanding the controller and performing value-preserving transformations, whichmodify the controller without sacrificing value. We present two efficientvalue-preserving transformations: one can reduce the size of the controller andthe other can improve its value while keeping the size fixed. Empirical resultsdemonstrate the usefulness of value-preserving transformations in increasingvalue while keeping controller size to a minimum. To broaden the applicabilityof the approach, we also present a heuristic version of the policy iterationalgorithm, which sacrifices convergence to optimality. This algorithm furtherreduces the size of the controllers at each step by assuming that probabilitydistributions over the other agents actions are known. While this assumptionmay not hold in general, it helps produce higher quality solutions in our testproblems.

Incremental Clustering and Expansion for Faster Optimal Planning in  Dec-POMDPs

  This article presents the state-of-the-art in optimal solution methods fordecentralized partially observable Markov decision processes (Dec-POMDPs),which are general models for collaborative multiagent planning underuncertainty. Building off the generalized multiagent A* (GMAA*) algorithm,which reduces the problem to a tree of one-shot collaborative Bayesian games(CBGs), we describe several advances that greatly expand the range ofDec-POMDPs that can be solved optimally. First, we introduce losslessincremental clustering of the CBGs solved by GMAA*, which achieves exponentialspeedups without sacrificing optimality. Second, we introduce incrementalexpansion of nodes in the GMAA* search tree, which avoids the need to expandall children, the number of which is in the worst case doubly exponential inthe nodes depth. This is particularly beneficial when little clustering ispossible. In addition, we introduce new hybrid heuristic representations thatare more compact and thereby enable the solution of larger Dec-POMDPs. Weprovide theoretical guarantees that, when a suitable heuristic is used, bothincremental clustering and incremental expansion yield algorithms that are bothcomplete and search equivalent. Finally, we present extensive empirical resultsdemonstrating that GMAA*-ICE, an algorithm that synthesizes these advances, canoptimally solve Dec-POMDPs of unprecedented size.

Semantic-level Decentralized Multi-Robot Decision-Making using  Probabilistic Macro-Observations

  Robust environment perception is essential for decision-making on robotsoperating in complex domains. Intelligent task execution requires principledtreatment of uncertainty sources in a robot's observation model. This isimportant not only for low-level observations (e.g., accelerometer data), butalso for high-level observations such as semantic object labels. This paperformalizes the concept of macro-observations in Decentralized PartiallyObservable Semi-Markov Decision Processes (Dec-POSMDPs), allowing scalablesemantic-level multi-robot decision making. A hierarchical Bayesian approach isused to model noise statistics of low-level classifier outputs, whilesimultaneously allowing sharing of domain noise characteristics betweenclasses. Classification accuracy of the proposed macro-observation scheme,called Hierarchical Bayesian Noise Inference (HBNI), is shown to exceedexisting methods. The macro-observation scheme is then integrated into aDec-POSMDP planner, with hardware experiments running onboard a team of dynamicquadrotors in a challenging domain where noise-agnostic filtering fails. To thebest of our knowledge, this is the first demonstration of a real-time,convolutional neural net-based classification framework running fully onboard ateam of quadrotors in a multi-robot decision-making domain.

Learning for Multi-robot Cooperation in Partially Observable Stochastic  Environments with Macro-actions

  This paper presents a data-driven approach for multi-robot coordination inpartially-observable domains based on Decentralized Partially Observable MarkovDecision Processes (Dec-POMDPs) and macro-actions (MAs). Dec-POMDPs provide ageneral framework for cooperative sequential decision making under uncertaintyand MAs allow temporally extended and asynchronous action execution. To date,most methods assume the underlying Dec-POMDP model is known a priori or a fullsimulator is available during planning time. Previous methods which aim toaddress these issues suffer from local optimality and sensitivity to initialconditions. Additionally, few hardware demonstrations involving a large team ofheterogeneous robots and with long planning horizons exist. This work addressesthese gaps by proposing an iterative sampling based Expectation-Maximizationalgorithm (iSEM) to learn polices using only trajectory data containingobservations, MAs, and rewards. Our experiments show the algorithm is able toachieve better solution quality than the state-of-the-art learning-basedmethods. We implement two variants of multi-robot Search and Rescue (SAR)domains (with and without obstacles) on hardware to demonstrate the learnedpolicies can effectively control a team of distributed robots to cooperate in apartially observable stochastic environment.

Learning to Teach in Cooperative Multiagent Reinforcement Learning

  Collective human knowledge has clearly benefited from the fact thatinnovations by individuals are taught to others through communication. Similarto human social groups, agents in distributed learning systems would likelybenefit from communication to share knowledge and teach skills. The problem ofteaching to improve agent learning has been investigated by prior works, butthese approaches make assumptions that prevent application of teaching togeneral multiagent problems, or require domain expertise for problems they canapply to. This learning to teach problem has inherent complexities related tomeasuring long-term impacts of teaching that compound the standard multiagentcoordination challenges. In contrast to existing works, this paper presents thefirst general framework and algorithm for intelligent agents to learn to teachin a multiagent environment. Our algorithm, Learning to Coordinate and TeachReinforcement (LeCTR), addresses peer-to-peer teaching in cooperativemultiagent reinforcement learning. Each agent in our approach learns both whenand what to advise, then uses the received advice to improve local learning.Importantly, these roles are not fixed; these agents learn to assume the roleof student and/or teacher at the appropriate moments, requesting and providingadvice in order to improve teamwide performance and learning. Empiricalcomparisons against state-of-the-art teaching methods show that our teachingagents not only learn significantly faster, but also learn to coordinate intasks where existing methods fail.

Measurement of $V^0$ production ratios in $pp$ collisions at $\sqrt{s} =  0.9$ and 7\,TeV

  The $\bar{\Lambda} / \Lambda$ and $\bar{\Lambda} / K^0_\mathrm{S}$ productionratios are measured by the LHCb detector from $0.3\,\mathrm{nb}^{-1}$ of $pp$collisions delivered by the LHC at $\sqrt{s} = 0.9$\,TeV and$1.8\,\mathrm{nb}^{-1}$ at $\sqrt{s} = 7$\,TeV. Both ratios are presented as afunction of transverse momentum, $p_\mathrm{T}$, and rapidity, $y$, in theranges {$0.15 < p_\mathrm{T} < 2.50\,\mathrm{GeV}/c$} and {$2.0<y<4.5$}.Results at the two energies are in good agreement as a function of rapidityloss, $\Delta y = y_\mathrm{beam} - y$, and are consistent with previousmeasurements. The ratio $\bar{\Lambda} / \Lambda$, measuring the transport ofbaryon number from the collision into the detector, is smaller in data thanpredicted in simulation, particularly at high rapidity. The ratio$\bar{\Lambda} / K^0_\mathrm{S}$, measuring the baryon-to-meson suppression instrange quark hadronisation, is significantly larger than expected.

Differential branching fraction and angular analysis of the decay  $B_s^0\toφμ^{+}μ^{-}$

  The determination of the differential branching fraction and the firstangular analysis of the decay $B_s^0\to\phi\mu^{+}\mu^{-}$ are presented usingdata, corresponding to an integrated luminosity of $1.0\,{\rm fb}^{-1}$,collected by the LHCb experiment at $\sqrt{s}=7\,{\rm TeV}$. The differentialbranching fraction is determined in bins of $q^{2}$, the invariant dimuon masssquared. Integration over the full $q^{2}$ range yields a total branchingfraction of ${\cal B}(B_s^0\to\phi\mu^{+}\mu^{-}) = (7.07\,^{+0.64}_{-0.59}\pm0.17 \pm 0.71)\times 10^{-7}$, where the first uncertainty is statistical, thesecond systematic, and the third originates from the branching fraction of thenormalisation channel. An angular analysis is performed to determine theangular observables $F_{\rm L}$, $S_3$, $A_6$, and $A_9$. The observables areconsistent with Standard Model expectations.

LHCb Detector Performance

  The LHCb detector is a forward spectrometer at the Large Hadron Collider(LHC) at CERN. The experiment is designed for precision measurements of CPviolation and rare decays of beauty and charm hadrons. In this paper theperformance of the various LHCb sub-detectors and the trigger system aredescribed, using data taken from 2010 to 2012. It is shown that the designcriteria of the experiment have been met. The excellent performance of thedetector has allowed the LHCb collaboration to publish a wide range of physicsresults, demonstrating LHCb's unique role, both as a heavy flavour experimentand as a general purpose detector in the forward region.

Measurement of $CP$ violation in $B^0 \rightarrow J/ψK^0_S$ decays

  Measurements are presented of the $CP$ violation observables $S$ and $C$ inthe decays of $B^0$ and $\overline{B}{}^0$ mesons to the $J/\psi K^0_S$ finalstate. The data sample corresponds to an integrated luminosity of$3.0\,\text{fb}^{-1}$ collected with the LHCb experiment in proton-protoncollisions at center-of-mass energies of $7$ and $8\,\text{TeV}$. The analysisof the time evolution of $41500$ $B^0$ and $\overline{B}{}^0$ decays yields $S= 0.731 \pm 0.035 \, \text{(stat)} \pm 0.020 \,\text{(syst)}$ and $C = -0.038\pm 0.032 \, \text{(stat)} \pm 0.005\,\text{(syst)}$. In the Standard Model,$S$ equals $\sin(2\beta)$ to a good level of precision. The values areconsistent with the current world averages and with the Standard Modelexpectations.

Angular analysis of the $B^{0}\rightarrow K^{*0}μ^{+}μ^{-}$ decay  using $3\,\mbox{fb}^{-1}$ of integrated luminosity

  An angular analysis of the $B^{0}\rightarrow K^{*0}(\rightarrowK^{+}\pi^{-})\mu^{+}\mu^{-}$ decay is presented. The dataset corresponds to anintegrated luminosity of $3.0\,{\mbox{fb}^{-1}}$ of $pp$ collision datacollected at the LHCb experiment. The complete angular information from thedecay is used to determine $C\!P$-averaged observables and $C\!P$ asymmetries,taking account of possible contamination from decays with the $K^{+}\pi^{-}$system in an S-wave configuration. The angular observables and theircorrelations are reported in bins of $q^2$, the invariant mass squared of thedimuon system. The observables are determined both from an unbinned maximumlikelihood fit and by using the principal moments of the angular distribution.In addition, by fitting for $q^2$-dependent decay amplitudes in the region$1.1<q^{2}<6.0\mathrm{\,Ge\kern -0.1em V}^{2}/c^{4}$, the zero-crossing pointsof several angular observables are computed. A global fit is performed to thecomplete set of $C\!P$-averaged observables obtained from the maximumlikelihood fit. This fit indicates differences with predictions based on theStandard Model at the level of 3.4 standard deviations. These differences couldbe explained by contributions from physics beyond the Standard Model, or by anunexpectedly large hadronic effect that is not accounted for in the StandardModel predictions.

Observation of the rare $B^0_s\toμ^+μ^-$ decay from the combined  analysis of CMS and LHCb data

  A joint measurement is presented of the branching fractions$B^0_s\to\mu^+\mu^-$ and $B^0\to\mu^+\mu^-$ in proton-proton collisions at theLHC by the CMS and LHCb experiments. The data samples were collected in 2011 ata centre-of-mass energy of 7 TeV, and in 2012 at 8 TeV. The combined analysisproduces the first observation of the $B^0_s\to\mu^+\mu^-$ decay, with astatistical significance exceeding six standard deviations, and the bestmeasurement of its branching fraction so far. Furthermore, evidence for the$B^0\to\mu^+\mu^-$ decay is obtained with a statistical significance of threestandard deviations. The branching fraction measurements are statisticallycompatible with SM predictions and impose stringent constraints on severaltheories beyond the SM.

