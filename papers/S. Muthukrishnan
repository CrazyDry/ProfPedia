Doubleclick Ad Exchange Auction

  Display advertisements on the web are sold via ad exchanges that use realtime auction. We describe the challenges of designing a suitable auction, andpresent a simple auction called the Optional Second Price (OSP) auction that iscurrently used in Doubleclick Ad Exchange.

Nearly Optimal Private Convolution

  We study computing the convolution of a private input $x$ with a public input$h$, while satisfying the guarantees of $(\epsilon, \delta)$-differentialprivacy. Convolution is a fundamental operation, intimately related to FourierTransforms. In our setting, the private input may represent a time series ofsensitive events or a histogram of a database of confidential personalinformation. Convolution then captures important primitives including linearfiltering, which is an essential tool in time series analysis, and aggregationqueries on projections of the data.  We give a nearly optimal algorithm for computing convolutions whilesatisfying $(\epsilon, \delta)$-differential privacy. Surprisingly, we followthe simple strategy of adding independent Laplacian noise to each Fouriercoefficient and bounding the privacy loss using the composition theorem ofDwork, Rothblum, and Vadhan. We derive a closed form expression for the optimalnoise to add to each Fourier coefficient using convex programming duality. Ouralgorithm is very efficient -- it is essentially no more computationallyexpensive than a Fast Fourier Transform.  To prove near optimality, we use the recent discrepancy lowerbounds ofMuthukrishnan and Nikolov and derive a spectral lower bound using acharacterization of discrepancy in terms of determinants.

Internet Packet Filter Management and Rectangle Geometry

  We consider rule sets for internet packet routing and filtering, where eachrule consists of a range of source addresses, a range of destination addresses,a priority, and an action. A given packet should be handled by the action fromthe maximum priority rule that matches its source and destination. We describenew data structures for quickly finding the rule matching an incoming packet,in near-linear space, and a new algorithm for determining whether a rule setcontains any conflicts, in time O(n^{3/2}).

Group testing problems in experimental molecular biology

  In group testing, the task is to determine the distinguished members of a setof objects L by asking subset queries of the form ``does the subset Q of Lcontain a distinguished object?'' The primary biological application of grouptesting is for screening libraries of clones with hybridization probes. This isa crucial step in constructing physical maps and for finding genes. Grouptesting has also been considered for sequencing by hybridization. Anotherimportant application includes screening libraries of reagents for usefulchemically active zones. This preliminary report discusses some of theconstrained group testing problems which arise in biology.

Algorithmic Methods for Sponsored Search Advertising

  Modern commercial Internet search engines display advertisements along sidethe search results in response to user queries. Such sponsored search relies onmarket mechanisms to elicit prices for these advertisements, making use of anauction among advertisers who bid in order to have their ads shown for specifickeywords. We present an overview of the current systems for such auctions andalso describe the underlying game-theoretic aspects. The game involves threeparties--advertisers, the search engine, and search users--and we presentexample research directions that emphasize the role of each. The algorithms forbidding and pricing in these games use techniques from three mathematicalareas: mechanism design, optimization, and statistical estimation. Finally, wepresent some challenges in sponsored search advertising.

Range Medians

  We study a generalization of the classical median finding problem to batchedquery case: given an array of unsorted $n$ items and $k$ (not necessarilydisjoint) intervals in the array, the goal is to determine the median in {\emeach} of the intervals in the array. We give an algorithm that uses $O(n\log n+ k\log k \log n)$ comparisons and show a lower bound of $\Omega(n\log k)$comparisons for this problem. This is optimal for $k=O(n/\log n)$.

First Author Advantage: Citation Labeling in Research

  Citations among research papers, and the networks they form, are the primaryobject of study in scientometrics. The act of making a citation reflects theciter's knowledge of the related literature, and of the work being cited. Weaim to gain insight into this process by studying citation keys: user-chosenlabels to identify a cited work. Our main observation is that the first listedauthor is disproportionately represented in such labels, implying a strongmental bias towards the first author.

Optimal cache-aware suffix selection

  Given string $S[1..N]$ and integer $k$, the {\em suffix selection} problem isto determine the $k$th lexicographically smallest amongst the suffixes $S[i...N]$, $1 \leq i \leq N$. We study the suffix selection problem in thecache-aware model that captures two-level memory inherent in computing systems,for a \emph{cache} of limited size $M$ and block size $B$. The complexity ofinterest is the number of block transfers. We present an optimal suffixselection algorithm in the cache-aware model, requiring $\Thetah{N/B}$ blocktransfers, for any string $S$ over an unbounded alphabet (where characters canonly be compared), under the common tall-cache assumption (i.e.$M=\Omegah{B^{1+\epsilon}}$, where $\epsilon<1$). Our algorithm beats thebottleneck bound for permuting an input array to the desired output array,which holds for nearly any nontrivial problem in hierarchical memory models.

Pattern matching in Lempel-Ziv compressed strings: fast, simple, and  deterministic

  Countless variants of the Lempel-Ziv compression are widely used in manyreal-life applications. This paper is concerned with a natural modification ofthe classical pattern matching problem inspired by the popularity of suchcompression methods: given an uncompressed pattern s[1..m] and a Lempel-Zivrepresentation of a string t[1..N], does s occur in t? Farach and Thorup gave arandomized O(nlog^2(N/n)+m) time solution for this problem, where n is the sizeof the compressed representation of t. We improve their result by developing afaster and fully deterministic O(nlog(N/n)+m) time algorithm with the samespace complexity. Note that for highly compressible texts, log(N/n) might be oforder n, so for such inputs the improvement is very significant. A (tiny)fragment of our method can be used to give an asymptotically optimal solutionfor the substring hashing problem considered by Farach and Muthukrishnan.

Estimating Aggregate Properties on Probabilistic Streams

  The probabilistic-stream model was introduced by Jayram et al. \cite{JKV07}.It is a generalization of the data stream model that is suited to handling``probabilistic'' data where each item of the stream represents a probabilitydistribution over a set of possible events. Therefore, a probabilistic streamdetermines a distribution over potentially a very large number of classical"deterministic" streams where each item is deterministically one of the domainvalues. The probabilistic model is applicable for not only analyzing streamswhere the input has uncertainties (such as sensor data streams that measurephysical processes) but also where the streams are derived from the input databy post-processing, such as tagging or reconciling inconsistent and poorquality data.  We present streaming algorithms for computing commonly used aggregates on aprobabilistic stream. We present the first known, one pass streaming algorithmfor estimating the \AVG, improving results in \cite{JKV07}. We present thefirst known streaming algorithms for estimating the number of \DISTINCT itemson probabilistic streams. Further, we present extensions to other aggregatessuch as the repeat rate, quantiles, etc. In all cases, our algorithms work withprovable accuracy guarantees and within the space constraints of the datastream model.

Budget Optimization in Search-Based Advertising Auctions

  Internet search companies sell advertisement slots based on users' searchqueries via an auction. While there has been a lot of attention on the auctionprocess and its game-theoretic aspects, our focus is on the advertisers. Inparticular, the advertisers have to solve a complex optimization problem of howto place bids on the keywords of their interest so that they can maximize theirreturn (the number of user clicks on their ads) for a given budget. We modelthe entire process and study this budget optimization problem. While mostvariants are NP hard, we show, perhaps surprisingly, that simply randomizingbetween two uniform strategies that bid equally on all the keywords works well.More precisely, this strategy gets at least 1-1/e fraction of the maximumclicks possible. Such uniform strategies are likely to be practical. We alsopresent inapproximability results, and optimal algorithms for variants of thebudget optimization problem.

Stochastic Models for Budget Optimization in Search-Based Advertising

  Internet search companies sell advertisement slots based on users' searchqueries via an auction. Advertisers have to determine how to place bids on thekeywords of their interest in order to maximize their return for a givenbudget: this is the budget optimization problem. The solution depends on thedistribution of future queries.  In this paper, we formulate stochastic versions of the budget optimizationproblem based on natural probabilistic models of distribution over futurequeries, and address two questions that arise.  [Evaluation] Given a solution, can we evaluate the expected value of theobjective function?  [Optimization] Can we find a solution that maximizes the objective functionin expectation?  Our main results are approximation and complexity results for these twoproblems in our three stochastic models. In particular, our algorithmic resultsshow that simple prefix strategies that bid on all cheap keywords up to somelevel are either optimal or good approximations for many cases; we show othercases to be NP-hard.

Radix Sorting With No Extra Space

  It is well known that n integers in the range [1,n^c] can be sorted in O(n)time in the RAM model using radix sorting. More generally, integers in anyrange [1,U] can be sorted in O(n sqrt{loglog n}) time. However, thesealgorithms use O(n) words of extra memory. Is this necessary?  We present a simple, stable, integer sorting algorithm for words of sizeO(log n), which works in O(n) time and uses only O(1) words of extra memory ona RAM model. This is the integer sorting case most useful in practice. Weextend this result with same bounds to the case when the keys are read-only,which is of theoretical interest. Another interesting question is the case ofarbitrary c. Here we present a black-box transformation from any RAM sortingalgorithm to a sorting algorithm which uses only O(1) extra space and has thesame running time. This settles the complexity of in-place sorting in terms ofthe complexity of sorting.

Faster Least Squares Approximation

  Least squares approximation is a technique to find an approximate solution toa system of linear equations that has no exact solution. In a typical setting,one lets $n$ be the number of constraints and $d$ be the number of variables,with $n \gg d$. Then, existing exact methods find a solution vector in$O(nd^2)$ time. We present two randomized algorithms that provide very accuraterelative-error approximations to the optimal value and the solution vector of aleast squares approximation problem more rapidly than existing exactalgorithms. Both of our algorithms preprocess the data with the RandomizedHadamard Transform. One then uniformly randomly samples constraints and solvesthe smaller problem on those constraints, and the other performs a sparserandom projection and solves the smaller problem on those projectedcoordinates. In both cases, solving the smaller problem provides relative-errorapproximations, and, if $n$ is sufficiently larger than $d$, the approximatesolution can be computed in $O(nd \log d)$ time.

Quasi-Proportional Mechanisms: Prior-free Revenue Maximization

  Inspired by Internet ad auction applications, we study the problem ofallocating a single item via an auction when bidders place very differentvalues on the item. We formulate this as the problem of prior-free auction andfocus on designing a simple mechanism that always allocates the item. Ratherthan designing sophisticated pricing methods like prior literature, we designbetter allocation methods. In particular, we propose quasi-proportionalallocation methods in which the probability that an item is allocated to abidder depends (quasi-proportionally) on the bids.  We prove that corresponding games for both all-pay and winners-payquasi-proportional mechanisms admit pure Nash equilibria and this equilibriumis unique. We also give an algorithm to compute this equilibrium in polynomialtime. Further, we show that the revenue of the auctioneer is promisingly highcompared to the ultimate, i.e., the highest value of any of the bidders, andshow bounds on the revenue of equilibria both analytically, as well as usingexperiments for specific quasi-proportional functions. This is the first knownrevenue analysis for these natural mechanisms (including the special case ofproportional mechanism which is common in network resource allocationproblems).

Selective Call Out and Real Time Bidding

  Ads on the Internet are increasingly sold via ad exchanges such asRightMedia, AdECN and Doubleclick Ad Exchange. These exchanges allow real-timebidding, that is, each time the publisher contacts the exchange, the exchange``calls out'' to solicit bids from ad networks. This aspect of soliciting bidsintroduces a novel aspect, in contrast to existing literature. This suggestsdeveloping a joint optimization framework which optimizes over the allocationand well as solicitation. We model this selective call out as an onlinerecurrent Bayesian decision framework with bandwidth type constraints. Weobtain natural algorithms with bounded performance guarantees for severalnatural optimization criteria. We show that these results hold under differentcall out constraint models, and different arrival processes. Interestingly, thepaper shows that under MHR assumptions, the expected revenue of generalizedsecond price auction with reserve is constant factor of the expected welfare.Also the analysis herein allow us prove adaptivity gap type results for theadwords problem.

Node Classification in Social Networks

  When dealing with large graphs, such as those that arise in the context ofonline social networks, a subset of nodes may be labeled. These labels canindicate demographic values, interest, beliefs or other characteristics of thenodes (users). A core problem is to use this information to extend the labelingso that all nodes are assigned a label (or labels). In this chapter, we surveyclassification techniques that have been proposed for this problem. We considertwo broad categories: methods based on iterative application of traditionalclassifiers using graph information as features, and methods which propagatethe existing labels via random walks. We adopt a common perspective on thesemethods to highlight the similarities between different approaches within andacross the two categories. We also describe some extensions and relateddirections to the central problem of node classification.

Private Decayed Sum Estimation under Continual Observation

  In monitoring applications, recent data is more important than distant data.How does this affect privacy of data analysis? We study a general class of dataanalyses - computing predicate sums - with privacy. Formally, we study theproblem of estimating predicate sums {\em privately}, for sliding windows (andother well-known decay models of data, i.e. exponential and polynomial decay).We extend the recently proposed continual privacy model of Dwork et al.  We present algorithms for decayed sum which are $\eps$-differentiallyprivate, and are accurate. For window and exponential decay sums, ouralgorithms are accurate up to additive $1/\eps$ and polylog terms in the rangeof the computed function; for polynomial decay sums which are technically morechallenging because partial solutions do not compose easily, our algorithmsincur additional relative error. Further, we show lower bounds, tight withinpolylog factors and tight with respect to the dependence on the probability oferror.

Scienceography: the study of how science is written

  Scientific literature has itself been the subject of much scientific study,for a variety of reasons: understanding how results are communicated, how ideasspread, and assessing the influence of areas or individuals. However, mostprior work has focused on extracting and analyzing citation and stylisticpatterns. In this work, we introduce the notion of 'scienceography', whichfocuses on the writing of science. We provide a first large scale study usingdata derived from the arXiv e-print repository. Crucially, our data includesthe "source code" of scientific papers-the LaTEX source-which enables us tostudy features not present in the "final product", such as the tools used andprivate comments between authors. Our study identifies broad patterns andtrends in two example areas-computer science and mathematics-as well ashighlighting key differences in the way that science is written in thesefields. Finally, we outline future directions to extend the new topic ofscienceography.

Analyses of Cardinal Auctions

  We study cardinal auctions for selling multiple copies of a good, in whichbidders specify not only their bid or how much they are ready to pay for thegood, but also a cardinality constraint on the number of copies that will besold via the auction. We perform first known Price of Anarchy type analyseswith detailed comparison of the classical Vickrey-Clarke-Groves (VCG) auctionand one based on minimum pay property (MPP) which is similar to GeneralizedSecond Price auction commonly used in sponsored search. Without cardinalityconstraints, MPP has the same efficiency (total value to bidders) and at leastas much revenue (total income to the auctioneer) as VCG; this also holds forcertain other generalizations of MPP (e.g., prefix constrained auctions, as weshow here). In contrast, our main results are that, with cardinalityconstraints, (a) equilibrium efficiency of MPP is 1/2 of that of VCG and thisfactor is tight, and (b) in equilibrium MPP may collect as little as 1/2 therevenue of VCG. These aspects arise because in presence of cardinalityconstraints, more strategies are available to bidders in MPP, including biddingabove their value, and this makes analyses nontrivial.

A Time and Space Efficient Algorithm for Contextual Linear Bandits

  We consider a multi-armed bandit problem where payoffs are a linear functionof an observed stochastic contextual variable. In the scenario where thereexists a gap between optimal and suboptimal rewards, several algorithms havebeen proposed that achieve $O(\log T)$ regret after $T$ time steps. However,proposed methods either have a computation complexity per iteration that scaleslinearly with $T$ or achieve regrets that grow linearly with the number ofcontexts $|\myset{X}|$. We propose an $\epsilon$-greedy type of algorithm thatsolves both limitations. In particular, when contexts are variables in$\reals^d$, we prove that our algorithm has a constant computation complexityper iteration of $O(poly(d))$ and can achieve a regret of $O(poly(d) \log T)$even when $|\myset{X}| = \Omega (2^d) $. In addition, unlike previousalgorithms, its space complexity scales like $O(Kd^2)$ and does not grow with$T$.

Socializing the h-index

  A variety of bibliometric measures have been proposed to quantify the impactof researchers and their work. The h-index is a notable and widely-used examplewhich aims to improve over simple metrics such as raw counts of papers orcitations. However, a limitation of this measure is that it considers authorsin isolation and does not account for contributions through a collaborativeteam. To address this, we propose a natural variant that we dub the Socialh-index. The idea is to redistribute the h-index score to reflect anindividual's impact on the research community. In addition to describing thisnew measure, we provide examples, discuss its properties, and contrast withother measures.

A Consensus-Focused Group Recommender System

  In many cases, recommendations are consumed by groups of users rather thanindividuals. In this paper, we present a system which recommends social eventsto groups. The system helps groups to organize a joint activity andcollectively select which activity to perform among several possible options.We also facilitate the consensus making, following the principle of groupconsensus decision making. Our system allows users to asynchronously vote, addand comment on alternatives. We observe social influence within groups throughpost-recommendation feedback during the group decision making process. Wepropose a decision cascading model and estimate such social influence, whichcan be used to improve the performance of group recommendation. We conductexperiments to measure the prediction performance of our model. The resultshows that the model achieves better results than that of independent decisionmaking model.

People Like Us: Mining Scholarly Data for Comparable Researchers

  We present the problem of finding comparable researchers for any givenresearcher. This problem has many motivations. Firstly, know thyself. Theanswers of where we stand among research community and who we are most alikemay not be easily found by existing evaluations of ones' research mainly basedon citation counts. Secondly, there are many situations where one needs to findcomparable researchers e.g., for reviewing peers, constructing programmingcommittees or compiling teams for grants. It is often done through an ad hocand informal basis. Utilizing the large scale scholarly data accessible on theweb, we address the problem of automatically finding comparable researchers. Wepropose a standard to quantify the quality of research output, via the qualityof publishing venues. We represent a researcher as a sequence of herpublication records, and develop a framework of comparison of researchers bysequence matching. Several variations of comparisons are considered includingmatching by quality of publication venue and research topics, and performingprefix matching. We evaluate our methods on a large corpus and demonstrate theeffectiveness of our methods through examples. In the end, we identify severalpromising directions for further work.

Frugal Streaming for Estimating Quantiles:One (or two) memory suffices

  Modern applications require processing streams of data for estimatingstatistical quantities such as quantiles with small amount of memory. In manysuch applications, in fact, one needs to compute such statistical quantitiesfor each of a large number of groups, which additionally restricts the amountof memory available for the stream for any particular group. We address thischallenge and introduce frugal streaming, that is algorithms that work withtiny -- typically, sub-streaming -- amount of memory per group.  We design a frugal algorithm that uses only one unit of memory per group tocompute a quantile for each group. For stochastic streams where data items aredrawn from a distribution independently, we analyze and show that the algorithmfinds an approximation to the quantile rapidly and remains stably close to it.We also propose an extension of this algorithm that uses two units of memoryper group. We show with extensive experiments with real world data from HTTPtrace and Twitter that our frugal algorithms are comparable to existingstreaming algorithms for estimating any quantile, but these existing algorithmsuse far more space per group and are unrealistic in frugal applications;further, the two memory frugal algorithm converges significantly faster thanthe one memory algorithm.

Modeling Collaboration in Academia: A Game Theoretic Approach

  In this work, we aim to understand the mechanisms driving academiccollaboration. We begin by building a model for how researchers split theireffort between multiple papers, and how collaboration affects the number ofcitations a paper receives, supported by observations from a large real-worldpublication and citation dataset, which we call the h-Reinvestment model. Usingtools from the field of Game Theory, we study researchers' collaborativebehavior over time under this model, with the premise that each researcherwants to maximize his or her academic success. We find analytically that thereis a strong incentive to collaborate rather than work in isolation, and thatstudying collaborative behavior through a game-theoretic lens is a promisingapproach to help us better understand the nature and dynamics of academiccollaboration.

Heavy-Hitter Detection Entirely in the Data Plane

  Identifying the "heavy hitter" flows or flows with large traffic volumes inthe data plane is important for several applications e.g., flow-size awarerouting, DoS detection, and traffic engineering. However, measurement in thedata plane is constrained by the need for line-rate processing (at 10-100Gb/s)and limited memory in switching hardware. We propose HashPipe, a heavy hitterdetection algorithm using emerging programmable data planes. HashPipeimplements a pipeline of hash tables which retain counters for heavy flowswhile evicting lighter flows over time. We prototype HashPipe in P4 andevaluate it with packet traces from an ISP backbone link and a data center. Onthe ISP trace (which contains over 400,000 flows), we find that HashPipeidentifies 95% of the 300 heaviest flows with less than 80KB of memory.

Testable Bounded Degree Graph Properties Are Random Order Streamable

  We study which property testing and sublinear time algorithms can betransformed into graph streaming algorithms for random order streams. Our mainresult is that for bounded degree graphs, any property that is constant-querytestable in the adjacency list model can be tested with constant space in asingle-pass in random order streams. Our result is obtained by estimating thedistribution of local neighborhoods of the vertices on a random order graphstream using constant space.  We then show that our approach can also be applied to constant timeapproximation algorithms for bounded degree graphs in the adjacency list model:As an example, we obtain a constant-space single-pass random order streamingalgorithms for approximating the size of a maximum matching with additive error$\epsilon n$ ($n$ is the number of nodes).  Our result establishes for the first time that a large class of sublinearalgorithms can be simulated in random order streams, while $\Omega(n)$ space isneeded for many graph streaming problems for adversarial orders.

Stochastic Low-Rank Bandits

  Many problems in computer vision and recommender systems involve low-rankmatrices. In this work, we study the problem of finding the maximum entry of astochastic low-rank matrix from sequential observations. At each step, alearning agent chooses pairs of row and column arms, and receives the noisyproduct of their latent values as a reward. The main challenge is that thelatent values are unobserved. We identify a class of non-negative matriceswhose maximum entry can be found statistically efficiently and propose analgorithm for finding them, which we call LowRankElim. We derive a$\DeclareMathOperator{\poly}{poly} O((K + L) \poly(d) \Delta^{-1} \log n)$upper bound on its $n$-step regret, where $K$ is the number of rows, $L$ is thenumber of columns, $d$ is the rank of the matrix, and $\Delta$ is the minimumgap. The bound depends on other problem-specific constants that clearly do notdepend $K L$. To the best of our knowledge, this is the first such result inthe literature.

Offline Evaluation of Ranking Policies with Click Models

  Many web systems rank and present a list of items to users, from recommendersystems to search and advertising. An important problem in practice is toevaluate new ranking policies offline and optimize them before they aredeployed. We address this problem by proposing evaluation algorithms forestimating the expected number of clicks on ranked lists from historical loggeddata. The existing algorithms are not guaranteed to be statistically efficientin our problem because the number of recommended lists can grow exponentiallywith their length. To overcome this challenge, we use models of userinteraction with the list of items, the so-called click models, to constructestimators that learn statistically efficiently. We analyze our estimators andprove that they are more efficient than the estimators that do not use thestructure of the click model, under the assumption that the click model holds.We evaluate our estimators in a series of experiments on a real-world datasetand show that they consistently outperform prior estimators.

The Shapley Value in Knapsack Budgeted Games

  We propose the study of computing the Shapley value for a new class ofcooperative games that we call budgeted games, and investigate in particularknapsack budgeted games, a version modeled after the classical knapsackproblem. In these games, the "value" of a set $S$ of agents is determined onlyby a critical subset $T\subseteq S$ of the agents and not the entirety of $S$due to a budget constraint that limits how large $T$ can be. We show that theShapley value can be computed in time faster than by the na\"ive exponentialtime algorithm when there are sufficiently many agents, and also provide analgorithm that approximates the Shapley value within an additive error. For arelated budgeted game associated with a greedy heuristic, we show that theShapley value can be computed in pseudo-polynomial time. Furthermore, wegeneralize our proof techniques and propose what we term algorithmicrepresentation framework that captures a broad class of cooperative games withthe property of efficient computation of the Shapley value. The main idea isthat the problem of determining the efficient computation can be reduced tothat of finding an alternative representation of the games and an associatedalgorithm for computing the underlying value function with small time and spacecomplexities in the representation size.

Bidding to the Top: VCG and Equilibria of Position-Based Auctions

  Many popular search engines run an auction to determine the placement ofadvertisements next to search results. Current auctions at Google and Yahoo!let advertisers specify a single amount as their bid in the auction. This bidis interpreted as the maximum amount the advertiser is willing to pay per clickon its ad. When search queries arrive, the bids are used to rank the adslinearly on the search result page. The advertisers pay for each user whoclicks on their ad, and the amount charged depends on the bids of all theadvertisers participating in the auction. In order to be effective, advertisersseek to be as high on the list as their budget permits, subject to the market.  We study the problem of ranking ads and associated pricing mechanisms whenthe advertisers not only specify a bid, but additionally express theirpreference for positions in the list of ads. In particular, we study "prefixposition auctions" where advertiser $i$ can specify that she is interested onlyin the top $b_i$ positions.  We present a simple allocation and pricing mechanism that generalizes thedesirable properties of current auctions that do not have position constraints.In addition, we show that our auction has an "envy-free" or "symmetric" Nashequilibrium with the same outcome in allocation and pricing as the well-knowntruthful Vickrey-Clarke-Groves (VCG) auction. Furthermore, we show that thisequilibrium is the best such equilibrium for the advertisers in terms of theprofit made by each advertiser. We also discuss other position-based auctions.

On the Complexity of Processing Massive, Unordered, Distributed Data

  An existing approach for dealing with massive data sets is to stream over theinput in few passes and perform computations with sublinear resources. Thismethod does not work for truly massive data where even making a single passover the data with a processor is prohibitive. Successful log processingsystems in practice such as Google's MapReduce and Apache's Hadoop use multiplemachines. They efficiently perform a certain class of highly distributablecomputations defined by local computations that can be applied in any order tothe input.  Motivated by the success of these systems, we introduce a simple algorithmicmodel for massive, unordered, distributed (mud) computation. We initiate thestudy of understanding its computational complexity. Our main result is apositive one: any unordered function that can be computed by a streamingalgorithm can also be computed with a mud algorithm, with comparable space andcommunication complexity. We extend this result to some useful classes ofapproximate and randomized streaming algorithms. We also give negative results,using communication complexity arguments to prove that extensions to privaterandomness, promise problems and indeterminate functions are impossible.  We believe that the line of research we introduce in this paper has thepotential for tremendous impact. The distributed systems that motivate our worksuccessfully process data at an unprecedented scale, distributed over hundredsor even thousands of machines, and perform hundreds of such analyses each day.The mud model (and its generalizations) inspire a set of complexity-theoreticquestions that lie at their heart.

A Truthful Mechanism for Offline Ad Slot Scheduling

  We consider the "Offline Ad Slot Scheduling" problem, where advertisers mustbe scheduled to "sponsored search" slots during a given period of time.Advertisers specify a budget constraint, as well as a maximum cost per click,and may not be assigned to more than one slot for a particular search.  We give a truthful mechanism under the utility model where bidders try tomaximize their clicks, subject to their personal constraints. In addition, weshow that the revenue-maximizing mechanism is not truthful, but has a Nashequilibrium whose outcome is identical to our mechanism. As far as we can tell,this is the first treatment of sponsored search that directly incorporates bothmultiple slots and budget constraints into an analysis of incentives.  Our mechanism employs a descending-price auction that maintains a solution toa certain machine scheduling problem whose job lengths depend on the price, andhence is variable over the auction. The price stops when the set of biddersthat can afford that price pack exactly into a block of ad slots, at whichpoint the mechanism allocates that block and continues on the remaining slots.To prove our result on the equilibrium of the revenue-maximizing mechanism, wefirst show that a greedy algorithm suffices to solve the revenue-maximizinglinear program; we then use this insight to prove that bidders allocated in thesame block of our mechanism have no incentive to deviate from bidding the fixedprice of that block.

Online Ad Slotting With Cancellations

  Many advertisers buy advertisements (ads) on the Internet or on traditionalmedia and seek simple, online mechanisms to reserve ad slots in advance. Mediapublishers represent a vast and varying inventory, and they too seek automatic,online mechanisms for pricing and allocating such reservations. In this paper,we present and study a simple model for auctioning such ad slots in advance.Bidders arrive sequentially and report which slots they are interested in. Theseller must decide immediately whether or not to grant a reservation. Our modelallows a seller to accept reservations, but possibly cancel the allocationslater and pay the bidder a cancellation compensation (bump payment). Our mainresult is an online mechanism to derive prices and bump payments that isefficient to implement. This mechanism has many desirable properties. It isindividually rational; winners have an incentive to be honest and bidding one'strue value dominates any lower bid. Our mechanism's efficiency is within aconstant fraction of the a posteriori optimally efficient solution. Its revenueis within a constant fraction of the a posteriori revenue of theVickrey-Clarke-Groves mechanism. Our results make no assumptions about theorder of arrival of bids or the value distribution of bidders and still hold ifthe items for sale are elements of a matroid, a more general setting than slotallocation.

General Auction Mechanism for Search Advertising

  In sponsored search, a number of advertising slots is available on a searchresults page, and have to be allocated among a set of advertisers competing todisplay an ad on the page. This gives rise to a bipartite matching market thatis typically cleared by the way of an automated auction. Several auctionmechanisms have been proposed, with variants of the Generalized Second Price(GSP) being widely used in practice.  A rich body of work on bipartite matching markets builds upon the stablemarriage model of Gale and Shapley and the assignment model of Shapley andShubik. We apply insights from this line of research into the structure ofstable outcomes and their incentive properties to advertising auctions.  We model advertising auctions in terms of an assignment model with linearutilities, extended with bidder and item specific maximum and minimum prices.Auction mechanisms like the commonly used GSP or the well-knownVickrey-Clarke-Groves (VCG) are interpreted as simply computing a\emph{bidder-optimal stable matching} in this model, for a suitably defined setof bidder preferences. In our model, the existence of a stable matching isguaranteed, and under a non-degeneracy assumption a bidder-optimal stablematching exists as well. We give an algorithm to find such matching inpolynomial time, and use it to design truthful mechanism that generalizes GSP,is truthful for profit-maximizing bidders, implements features likebidder-specific minimum prices and position-specific bids, and works for richmixtures of bidders and preferences.

Bid Optimization in Broad-Match Ad auctions

  Ad auctions in sponsored search support ``broad match'' that allows anadvertiser to target a large number of queries while bidding only on a limitednumber. While giving more expressiveness to advertisers, this feature makes itchallenging to optimize bids to maximize their returns: choosing to bid on aquery as a broad match because it provides high profit results in one biddingfor related queries which may yield low or even negative profits.  We abstract and study the complexity of the {\em bid optimization problem}which is to determine an advertiser's bids on a subset of keywords (possiblyusing broad match) so that her profit is maximized. In the query language modelwhen the advertiser is allowed to bid on all queries as broad match, we presentan linear programming (LP)-based polynomial-time algorithm that gets theoptimal profit. In the model in which an advertiser can only bid on keywords,ie., a subset of keywords as an exact or broad match, we show that this problemis not approximable within any reasonable approximation factor unless P=NP. Todeal with this hardness result, we present a constant-factor approximation whenthe optimal profit significantly exceeds the cost. This algorithm is based onrounding a natural LP formulation of the problem. Finally, we study a budgetedvariant of the problem, and show that in the query language model, one can findtwo budget constrained ad campaigns in polynomial time that implement theoptimal bidding strategy. Our results are the first to address bid optimizationunder the broad match feature which is common in ad auctions.

Online Stochastic Matching: Beating 1-1/e

  We study the online stochastic bipartite matching problem, in a formmotivated by display ad allocation on the Internet. In the online, butadversarial case, the celebrated result of Karp, Vazirani and Vazirani gives anapproximation ratio of $1-1/e$. In the online, stochastic case when nodes aredrawn repeatedly from a known distribution, the greedy algorithm matches thisapproximation ratio, but still, no algorithm is known that beats the $1 - 1/e$bound.  Our main result is a 0.67-approximation online algorithm for stochasticbipartite matching, breaking this $1 - {1/e}$ barrier. Furthermore, we showthat no online algorithm can produce a $1-\epsilon$ approximation for anarbitrarily small $\epsilon$ for this problem.  We employ a novel application of the idea of the power of two choices fromload balancing: we compute two disjoint solutions to the expected instance, anduse both of them in the online algorithm in a prescribed preference order.  To identify these two disjoint solutions, we solve a max flow problem in aboosted flow graph, and then carefully decompose this maximum flow to twoedge-disjoint (near-)matchings. These two offline solutions are used tocharacterize an upper bound for the optimum in any scenario. This is done byidentifying a cut whose value we can bound under the arrival distribution.

Approximation Schemes for Sequential Posted Pricing in Multi-Unit  Auctions

  We design algorithms for computing approximately revenue-maximizing {\emsequential posted-pricing mechanisms (SPM)} in $K$-unit auctions, in a standardBayesian model. A seller has $K$ copies of an item to sell, and there are $n$buyers, each interested in only one copy, who have some value for the item. Theseller must post a price for each buyer, the buyers arrive in a sequenceenforced by the seller, and a buyer buys the item if its value exceeds theprice posted to it. The seller does not know the values of the buyers, but haveBayesian information about them. An SPM specifies the ordering of buyers andthe posted prices, and may be {\em adaptive} or {\em non-adaptive} in itsbehavior.  The goal is to design SPM in polynomial time to maximize expected revenue. Wecompare against the expected revenue of optimal SPM, and provide a polynomialtime approximation scheme (PTAS) for both non-adaptive and adaptive SPMs. Thisis achieved by two algorithms: an efficient algorithm that gives a$(1-\frac{1}{\sqrt{2\pi K}})$-approximation (and hence a PTAS for sufficientlylarge $K$), and another that is a PTAS for constant $K$. The first algorithmyields a non-adaptive SPM that yields its approximation guarantees against anoptimal adaptive SPM -- this implies that the {\em adaptivity gap} in SPMsvanishes as $K$ becomes larger.

Yield Optimization of Display Advertising with Ad Exchange

  In light of the growing market of Ad Exchanges for the real-time sale ofadvertising slots, publishers face new challenges in choosing between theallocation of contract-based reservation ads and spot market ads. In thissetting, the publisher should take into account the tradeoff between short-termrevenue from an Ad Exchange and quality of allocating reservation ads. In thispaper, we formalize this combined optimization problem as a stochastic controlproblem and derive an efficient policy for online ad allocation in settingswith general joint distribution over placement quality and exchange bids. Weprove asymptotic optimality of this policy in terms of any trade-off betweenquality of delivered reservation ads and revenue from the exchange, and providea rigorous bound for its convergence rate to the optimal policy. We also giveexperimental results on data derived from real publisher inventory, showingthat our policy can achieve any pareto-optimal point on the quality vs. revenuecurve. Finally, we study a parametric training-based algorithm in which insteadof learning the dual variables from a sample data (as is done in non-parametrictraining-based algorithms), we learn the parameters of the distribution andconstruct those dual variables from the learned parameter values. We compareparametric and non-parametric ways to estimate from data both analytically andexperimentally in the special case without the ad exchange, and show thatthough both methods converge to the optimal policy as the sample size grows,our parametric method converges faster, and thus performs better on smallersamples.

Optimal Private Halfspace Counting via Discrepancy

  A range counting problem is specified by a set $P$ of size $|P| = n$ ofpoints in $\mathbb{R}^d$, an integer weight $x_p$ associated to each point $p\in P$, and a range space ${\cal R} \subseteq 2^{P}$. Given a query range $R\in {\cal R}$, the target output is $R(\vec{x}) = \sum_{p \in R}{x_p}$. Rangecounting for different range spaces is a central problem in ComputationalGeometry.  We study $(\epsilon, \delta)$-differentially private algorithms for rangecounting. Our main results are for the range space given by hyperplanes, thatis, the halfspace counting problem. We present an $(\epsilon,\delta)$-differentially private algorithm for halfspace counting in $d$dimensions which achieves $O(n^{1-1/d})$ average squared error. This contrastswith the $\Omega(n)$ lower bound established by the classical result of Dinurand Nissim [PODS 2003] for arbitrary subset counting queries. We also show amatching lower bound on average squared error for any $(\epsilon,\delta)$-differentially private algorithm for halfspace counting. Both boundsare obtained using discrepancy theory. For the lower bound, we use a modifieddiscrepancy measure and bound approximation of $(\epsilon,\delta)$-differentially private algorithms for range counting queries in termsof this discrepancy. We also relate the modified discrepancy measure toclassical combinatorial discrepancy, which allows us to exploit knowndiscrepancy lower bounds. This approach also yields a lower bound of$\Omega((\log n)^{d-1})$ for $(\epsilon, \delta)$-differentially privateorthogonal range counting in $d$ dimensions, the first known superconstantlower bound for this problem. For the upper bound, we use an approach inspiredby partial coloring methods for proving discrepancy upper bounds, and obtain$(\epsilon, \delta)$-differentially private algorithms for range counting withpolynomially bounded shatter function range spaces.

Adscape: Harvesting and Analyzing Online Display Ads

  Over the past decade, advertising has emerged as the primary source ofrevenue for many web sites and apps. In this paper we report afirst-of-its-kind study that seeks to broadly understand the features,mechanisms and dynamics of display advertising on the web - i.e., the Adscape.Our study takes the perspective of users who are the targets of display adsshown on web sites. We develop a scalable crawling capability that enables usto gather the details of display ads including creatives and landing pages. Ourcrawling strategy is focused on maximizing the number of unique ads harvested.Of critical importance to our study is the recognition that a user's profile(i.e. browser profile and cookies) can have a significant impact on which adsare shown. We deploy our crawler over a variety of websites and profiles andthis yields over 175K distinct display ads.  We find that while targeting is widely used, there remain many instances inwhich delivered ads do not depend on user profile; further, ads vary more overuser profiles than over websites. We also assess the population of advertisersseen and identify over 3.7K distinct entities from a variety of businesssegments. Finally, we find that when targeting is used, the specific types ofads delivered generally correspond with the details of user profiles, and alsoon users' patterns of visit.

Graphical Model Sketch

  Structured high-cardinality data arises in many domains, and poses a majorchallenge for both modeling and inference. Graphical models are a popularapproach to modeling structured data but they are unsuitable forhigh-cardinality variables. The count-min (CM) sketch is a popular approach toestimating probabilities in high-cardinality data but it does not scale wellbeyond a few variables. In this work, we bring together the ideas of graphicalmodels and count sketches; and propose and analyze several approaches toestimating probabilities in structured high-cardinality streams of data. Thekey idea of our approximations is to use the structure of a graphical model andapproximately estimate its factors by "sketches", which hash high-cardinalityvariables using random projections. Our approximations are computationallyefficient and their space complexity is independent of the cardinality ofvariables. Our error bounds are multiplicative and significantly improve uponthose of the CM sketch, a state-of-the-art approach to estimating probabilitiesin streams. We evaluate our approximations on synthetic and real-worldproblems, and report an order of magnitude improvements over the CM sketch.

The Sparse Awakens: Streaming Algorithms for Matching Size Estimation in  Sparse Graphs

  Estimating the size of the maximum matching is a canonical problem in graphalgorithms, and one that has attracted extensive study over a range ofdifferent computational models. We present improved streaming algorithms forapproximating the size of maximum matching with sparse (bounded arboricity)graphs.  * Insert-Only Streams: We present a one-pass algorithm that takes O(c log^2n) space and approximates the size of the maximum matching in graphs witharboricity c within a factor of O(c). This improves significantly on thestate-of-the-art O~(cn^{2/3})-space streaming algorithms.  * Dynamic Streams: Given a dynamic graph stream (i.e., inserts and deletes)of edges of an underlying c-bounded arboricity graph, we present a one-passalgorithm that uses space O~(c^{10/3}n^{2/3}) and returns an O(c)-estimator forthe size of the maximum matching. This algorithm improves the state-of-the-artO~(cn^{4/5})-space algorithms, where the O~(.) notation hides logarithmic in$n$ dependencies.  In contrast to the previous works, our results take more advantage of thestreaming access to the input and characterize the matching size based on theordering of the edges in the stream in addition to the degree distributions andstructural properties of the sparse graphs.

Budget Feasible Mechanisms for Experimental Design

  In the classical experimental design setting, an experimenter E has access toa population of $n$ potential experiment subjects $i\in \{1,...,n\}$, eachassociated with a vector of features $x_i\in R^d$. Conducting an experimentwith subject $i$ reveals an unknown value $y_i\in R$ to E. E typically assumessome hypothetical relationship between $x_i$'s and $y_i$'s, e.g., $y_i \approx\beta x_i$, and estimates $\beta$ from experiments, e.g., through linearregression. As a proxy for various practical constraints, E may select only asubset of subjects on which to conduct the experiment.  We initiate the study of budgeted mechanisms for experimental design. In thissetting, E has a budget $B$. Each subject $i$ declares an associated cost $c_i>0$ to be part of the experiment, and must be paid at least her cost. Inparticular, the Experimental Design Problem (EDP) is to find a set $S$ ofsubjects for the experiment that maximizes $V(S) = \log\det(I_d+\sum_{i\inS}x_i\T{x_i})$ under the constraint $\sum_{i\in S}c_i\leq B$; our objectivefunction corresponds to the information gain in parameter $\beta$ that islearned through linear regression methods, and is related to the so-called$D$-optimality criterion. Further, the subjects are strategic and may lie abouttheir costs.  We present a deterministic, polynomial time, budget feasible mechanismscheme, that is approximately truthful and yields a constant factorapproximation to EDP. In particular, for any small $\delta > 0$ and $\epsilon >0$, we can construct a (12.98, $\epsilon$)-approximate mechanism that is$\delta$-truthful and runs in polynomial time in both $n$ and$\log\log\frac{B}{\epsilon\delta}$. We also establish that no truthful,budget-feasible algorithms is possible within a factor 2 approximation, andshow how to generalize our approach to a wide class of learning problems,beyond linear regression.

Pan-private Algorithms: When Memory Does Not Help

  Consider updates arriving online in which the $t$th input is $(i_t,d_t)$,where $i_t$'s are thought of as IDs of users. Informally, a randomized function$f$ is {\em differentially private} with respect to the IDs if the probabilitydistribution induced by $f$ is not much different from that induced by it on aninput in which occurrences of an ID $j$ are replaced with some other ID $k$Recently, this notion was extended to {\em pan-privacy} where the computationof $f$ retains differential privacy, even if the internal memory of thealgorithm is exposed to the adversary (say by a malicious break-in or by fiatby the government). This is a strong notion of privacy, and surprisingly, forbasic counting tasks such as distinct counts, heavy hitters and others, Dworket al~\cite{dwork-pan} present pan-private algorithms with reasonable accuracy.The pan-private algorithms are nontrivial, and rely on sampling. We reexaminethese basic counting tasks and show improved bounds. In particular, we estimatethe distinct count $\Dt$ to within $(1\pm \eps)\Dt \pm O(\polylog m)$, where$m$ is the number of elements in the universe. This uses suitably noisystatistics on sketches known in the streaming literature. We also present thefirst known lower bounds for pan-privacy with respect to a single intrusion.Our lower bounds show that, even if allowed to work with unbounded memory,pan-private algorithms for distinct counts can not be significantly moreaccurate than our algorithms. Our lower bound uses noisy decoding. For heavyhitter counts, we present a pan private streaming algorithm that is accurate towithin $O(k)$ in worst case; previously known bound for this problem isarbitrarily worse. An interesting aspect of our pan-private algorithms is that,they deliberately use very small (polylogarithmic) space and tend to bestreaming algorithms, even though using more space is not forbidden.

Partial Data Compression and Text Indexing via Optimal Suffix  Multi-Selection

  Consider an input text string T[1,N] drawn from an unbounded alphabet. Westudy partial computation in suffix-based problems for Data Compression andText Indexing such as  (I) retrieve any segment of K<=N consecutive symbols from the Burrows-Wheelertransform of T, and  (II) retrieve any chunk of K<=N consecutive entries of the Suffix Array orthe Suffix Tree.  Prior literature would take O(N log N) comparisons (and time) to solve theseproblems by solving the total problem of building the entire Burrows-Wheelertransform or Text Index for T, and performing a post-processing to single outthe wanted portion.  We introduce a novel adaptive approach to partial computational problemsabove, and solve both the partial problems in O(K log K + N) comparisons andtime, improving the best known running times of O(N log N) for K=o(N).  These partial-computation problems are intimately related since they share acommon bottleneck: the suffix multi-selection problem, which is to output thesuffixes of rank r_1,r_2,...,r_K under the lexicographic order, wherer_1<r_2<...<r_K, r_i in [1,N]. Special cases of this problem are well known:K=N is the suffix sorting problem that is the workhorse in Stringology withhundreds of applications, and K=1 is the recently studied suffix selection.  We show that suffix multi-selection can be solved in Theta(N log N -sum_{j=0}^K Delta_j log Delta_j+N) time and comparisons, where r_0=0,r_{K+1}=N+1, and Delta_j=r_{j+1}-r_j for 0<=j<=K. This is asymptoticallyoptimal, and also matches the bound in [Dobkin, Munro, JACM 28(3)] formulti-selection on atomic elements (not suffixes). Matching the bound known foratomic elements for strings is a long running theme and challenge from 70's,which we achieve for the suffix multi-selection problem. The partial suffixproblems as well as the suffix multi-selection problem have many applications.

Wavelet Trees Meet Suffix Trees

  We present an improved wavelet tree construction algorithm and discuss itsapplications to a number of rank/select problems for integer keys and strings.  Given a string of length n over an alphabet of size $\sigma\leq n$, ourmethod builds the wavelet tree in $O(n \log \sigma/ \sqrt{\log{n}})$ time,improving upon the state-of-the-art algorithm by a factor of $\sqrt{\log n}$.As a consequence, given an array of n integers we can construct in $O(n\sqrt{\log n})$ time a data structure consisting of $O(n)$ machine words andcapable of answering rank/select queries for the subranges of the array in$O(\log n / \log \log n)$ time. This is a $\log \log n$-factor improvement inquery time compared to Chan and P\u{a}tra\c{s}cu and a $\sqrt{\log n}$-factorimprovement in construction time compared to Brodal et al.  Next, we switch to stringological context and propose a novel notion ofwavelet suffix trees. For a string w of length n, this data structure occupies$O(n)$ words, takes $O(n \sqrt{\log n})$ time to construct, and simultaneouslycaptures the combinatorial structure of substrings of w while enablingefficient top-down traversal and binary search. In particular, with a waveletsuffix tree we are able to answer in $O(\log |x|)$ time the following twonatural analogues of rank/select queries for suffixes of substrings: forsubstrings x and y of w count the number of suffixes of x that arelexicographically smaller than y, and for a substring x of w and an integer k,find the k-th lexicographically smallest suffix of x.  We further show that wavelet suffix trees allow to compute arun-length-encoded Burrows-Wheeler transform of a substring x of w in $O(s \log|x|)$ time, where s denotes the length of the resulting run-length encoding.This answers a question by Cormode and Muthukrishnan, who considered ananalogous problem for Lempel-Ziv compression.

Relative-Error CUR Matrix Decompositions

  Many data analysis applications deal with large matrices and involveapproximating the matrix using a small number of ``components.'' Typically,these components are linear combinations of the rows and columns of the matrix,and are thus difficult to interpret in terms of the original features of theinput data. In this paper, we propose and study matrix approximations that areexplicitly expressed in terms of a small number of columns and/or rows of thedata matrix, and thereby more amenable to interpretation in terms of theoriginal data. Our main algorithmic results are two randomized algorithms whichtake as input an $m \times n$ matrix $A$ and a rank parameter $k$. In our firstalgorithm, $C$ is chosen, and we let $A'=CC^+A$, where $C^+$ is theMoore-Penrose generalized inverse of $C$. In our second algorithm $C$, $U$, $R$are chosen, and we let $A'=CUR$. ($C$ and $R$ are matrices that consist ofactual columns and rows, respectively, of $A$, and $U$ is a generalized inverseof their intersection.) For each algorithm, we show that with probability atleast $1-\delta$: $$ ||A-A'||_F \leq (1+\epsilon) ||A-A_k||_F, $$ where $A_k$is the ``best'' rank-$k$ approximation provided by truncating the singularvalue decomposition (SVD) of $A$. The number of columns of $C$ and rows of $R$is a low-degree polynomial in $k$, $1/\epsilon$, and $\log(1/\delta)$. Our twoalgorithms are the first polynomial time algorithms for such low-rank matrixapproximations that come with relative-error guarantees; previously, in somecases, it was not even known whether such matrix decompositions exist. Both ofour algorithms are simple, they take time of the order needed to approximatelycompute the top $k$ singular vectors of $A$, and they use a novel, intuitivesampling method called ``subspace sampling.''

Sponsored Search Auctions with Markovian Users

  Sponsored search involves running an auction among advertisers who bid inorder to have their ad shown next to search results for specific keywords.Currently, the most popular auction for sponsored search is the "GeneralizedSecond Price" (GSP) auction in which advertisers are assigned to slots in thedecreasing order of their "score," which is defined as the product of their bidand click-through rate. In the past few years, there has been significantresearch on the game-theoretic issues that arise in an advertiser's interactionwith the mechanism as well as possible redesigns of the mechanism, but thisranking order has remained standard.  From a search engine's perspective, the fundamental question is: what is thebest assignment of advertisers to slots? Here "best" could mean "maximizinguser satisfaction," "most efficient," "revenue-maximizing," "simplest tointeract with," or a combination of these. To answer this question we need tounderstand the behavior of a search engine user when she sees the displayedads, since that defines the commodity the advertisers are bidding on, and itsvalue. Most prior work has assumed that the probability of a user clicking onan ad is independent of the other ads shown on the page.  We propose a simple Markovian user model that does not make this assumption.We then present an algorithm to determine the most efficient assignment underthis model, which turns out to be different than that of GSP. A truthfulauction then follows from an application of the Vickrey-Clarke-Groves (VCG)mechanism. Further, we show that our assignment has many of the desirableproperties of GSP that makes bidding intuitive. At the technical core of ourresult are a number of insights about the structure of the optimal assignment.

