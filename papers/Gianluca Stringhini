Towards Detecting Compromised Accounts on Social Networks

  Compromising social network accounts has become a profitable course of actionfor cybercriminals. By hijacking control of a popular media or businessaccount, attackers can distribute their malicious messages or disseminate fakeinformation to a large user base. The impacts of these incidents range from atarnished reputation to multi-billion dollar monetary losses on financialmarkets. In our previous work, we demonstrated how we can detect large-scalecompromises (i.e., so-called campaigns) of regular online social network users.In this work, we show how we can use similar techniques to identify compromisesof individual high-profile accounts. High-profile accounts frequently have onecharacteristic that makes this detection reliable -- they show consistentbehavior over time. We show that our system, were it deployed, would have beenable to detect and prevent three real-world attacks against popular companiesand news agencies. Furthermore, our system, in contrast to popular media, wouldnot have fallen for a staged compromise instigated by a US restaurant chain forpublicity reasons.

All Your Cards Are Belong To Us: Understanding Online Carding Forums

  Underground online forums are platforms that enable trades of illicitservices and stolen goods. Carding forums, in particular, are known for beingfocused on trading financial information. However, little evidence exists aboutthe sellers that are present on carding forums, the precise types of productsthey advertise, and the prices buyers pay. Existing literature mainly focuseson the organisation and structure of the forums. Furthermore, studies oncarding forums are usually based on literature review, expert interviews, ordata from forums that have already been shut down. This paper providesfirst-of-its-kind empirical evidence on active forums where stolen financialdata is traded. We monitored 5 out of 25 discovered forums, collected postsfrom the forums over a three-month period, and analysed them quantitatively andqualitatively. We focused our analyses on products, prices, seller prolificacy,seller specialisation, and seller reputation.

Honey Sheets: What Happens to Leaked Google Spreadsheets?

  Cloud-based documents are inherently valuable, due to the volume and natureof sensitive personal and business content stored in them. Despite theimportance of such documents to Internet users, there are still large gaps inthe understanding of what cybercriminals do when they illicitly get access tothem by for example compromising the account credentials they are associatedwith. In this paper, we present a system able to monitor user activity onGoogle spreadsheets. We populated 5 Google spreadsheets with fake bank accountdetails and fake funds transfer links. Each spreadsheet was configured toreport details of accesses and clicks on links back to us. To study how peopleinteract with these spreadsheets in case they are leaked, we posted uniquelinks pointing to the spreadsheets on a popular paste site. We then monitoredactivity in the accounts for 72 days, and observed 165 accesses in total. Wewere able to observe interesting modifications to these spreadsheets performedby illicit accesses. For instance, we observed deletion of some fake bankaccount information, in addition to insults and warnings that some visitorsentered in some of the spreadsheets. Our preliminary results show that oursystem can be used to shed light on cybercriminal behavior with regards toleaked online documents.

That Ain't You: Detecting Spearphishing Emails Before They Are Sent

  One of the ways in which attackers try to steal sensitive information fromcorporations is by sending spearphishing emails. This type of emails typicallyappear to be sent by one of the victim's coworkers, but have instead beencrafted by an attacker. A particularly insidious type of spearphishing emailsare the ones that do not only claim to come from a trusted party, but wereactually sent from that party's legitimate email account that was compromisedin the first place. In this paper, we propose a radical change of focus in thetechniques used for detecting such malicious emails: instead of looking forparticular features that are indicative of attack emails, we look for possibleindicators of impersonation of the legitimate owners. We presentIdentityMailer, a system that validates the authorship of emails by learningthe typical email-sending behavior of users over time, and comparing anysubsequent email sent from their accounts against this model. Our experimentson real world e-mail datasets demonstrate that our system can effectively blockadvanced email attacks sent from genuine email accounts, which traditionalprotection systems are unable to detect. Moreover, we show that it is resilientto an attacker willing to evade the system. To the best of our knowledge,IdentityMailer is the first system able to identify spearphishing emails thatare sent from within an organization, by a skilled attacker having access to acompromised email account.

Master of Puppets: Analyzing And Attacking A Botnet For Fun And Profit

  A botnet is a network of compromised machines (bots), under the control of anattacker. Many of these machines are infected without their owners' knowledge,and botnets are the driving force behind several misuses and criminalactivities on the Internet (for example spam emails). Depending on itstopology, a botnet can have zero or more command and control (C&C) servers,which are centralized machines controlled by the cybercriminal that issuecommands and receive reports back from the co-opted bots.  In this paper, we present a comprehensive analysis of the command and controlinfrastructure of one of the world's largest proprietary spamming botnetsbetween 2007 and 2012: Cutwail/Pushdo. We identify the key functionalitiesneeded by a spamming botnet to operate effectively. We then develop a number ofattacks against the command and control logic of Cutwail that target thosefunctionalities, and make the spamming operations of the botnet less effective.This analysis was made possible by having access to the source code of the C&Csoftware, as well as setting up our own Cutwail C&C server, and by implementinga clone of the Cutwail bot. With the help of this tool, we were able toenumerate the number of bots currently registered with the C&C server,impersonate an existing bot to report false information to the C&C server, andmanipulate spamming statistics of an arbitrary bot stored in the C&C database.Furthermore, we were able to make the control server inaccessible by conductinga distributed denial of service (DDoS) attack. Our results may be used by lawenforcement and practitioners to develop better techniques to mitigate andcripple other botnets, since many of findings are generic and are due to theworkflow of C&C communication in general.

What's in a Name? Understanding Profile Name Reuse on Twitter

  Users on Twitter are commonly identified by their profile names. These namesare used when directly addressing users on Twitter, are part of their profilepage URLs, and can become a trademark for popular accounts, with peoplereferring to celebrities by their real name and their profile name,interchangeably. Twitter, however, has chosen to not permanently link profilenames to their corresponding user accounts. In fact, Twitter allows users tochange their profile name, and afterwards makes the old profile names availablefor other users to take. In this paper, we provide a large-scale study of thephenomenon of profile name reuse on Twitter. We show that this phenomenon isnot uncommon, investigate the dynamics of profile name reuse, and characterizethe accounts that are involved in it. We find that many of these accounts adoptabandoned profile names for questionable purposes, such as spreading maliciouscontent, and using the profile name's popularity for search engineoptimization. Finally, we show that this problem is not unique to Twitter (asother popular online social networks also release profile names) and argue thatthe risks involved with profile-name reuse outnumber the advantages provided bythis feature.

Email Babel: Does Language Affect Criminal Activity in Compromised  Webmail Accounts?

  We set out to understand the effects of differing language on the ability ofcybercriminals to navigate webmail accounts and locate sensitive information inthem. To this end, we configured thirty Gmail honeypot accounts with English,Romanian, and Greek language settings. We populated the accounts with emailmessages in those languages by subscribing them to selected online newsletters.We hid email messages about fake bank accounts in fifteen of the accounts tomimic real-world webmail users that sometimes store sensitive information intheir accounts. We then leaked credentials to the honey accounts via pastesites on the Surface Web and the Dark Web, and collected data for fifteen days.Our statistical analyses on the data show that cybercriminals are more likelyto discover sensitive information (bank account information) in the Greekaccounts than the remaining accounts, contrary to the expectation that Greekought to constitute a barrier to the understanding of non-Greek visitors to theGreek accounts. We also extracted the important words among the emails thatcybercriminals accessed (as an approximation of the keywords that they searchedfor within the honey accounts), and found that financial terms featured amongthe top words. In summary, we show that language plays a significant role inthe ability of cybercriminals to access sensitive information hidden incompromised webmail accounts.

Movie Pirates of the Caribbean: Exploring Illegal Streaming Cyberlockers

  Online video piracy (OVP) is a contentious topic, with strong proponents onboth sides of the argument. Recently, a number of illegal websites, calledstreaming cyberlockers, have begun to dominate OVP. These websites specialisein distributing pirated content, underpinned by third party indexing servicesoffering easy-to-access directories of content. This paper performs the firstexploration of this new ecosystem. It characterises the content, as well thestreaming cyberlockers' individual attributes. We find a remarkably centralisedsystem with just a few networks, countries and cyberlockers underpinning mostprovisioning. We also investigate the actions of copyright enforcers. We findthey tend to target small subsets of the ecosystem, although they appear quitesuccessful. 84% of copyright notices see content removed.

Who Watches the Watchmen: Exploring Complaints on the Web

  Under increasing scrutiny, many web companies now offer bespoke mechanismsallowing any third party to file complaints (e.g., requesting the de-listing ofa URL from a search engine). While this self-regulation might be a valuable webgovernance tool, it places huge responsibility within the hands of theseorganisations that demands close examination. We present the first large-scalestudy of web complaints (over 1 billion URLs). We find a range of complainants,largely focused on copyright enforcement. Whereas the majority of organisationsare occasional users of the complaint system, we find a number of bulk sendersspecialised in targeting specific types of domain. We identify a series oftrends and patterns amongst both the domains and complainants. By inspectingthe availability of the domains, we also observe that a sizeable portion gooffline shortly after complaints are generated. This paper sheds critical lighton how complaints are issued, who they pertain to and which domains go offlineafter complaints are issued.

Pythia: a Framework for the Automated Analysis of Web Hosting  Environments

  A common approach when setting up a website is to utilize third party Webhosting and content delivery networks. Without taking this trend into account,any measurement study inspecting the deployment and operation of websites canbe heavily skewed. Unfortunately, the research community lacks generalizabletools that can be used to identify how and where a given website is hosted.Instead, a number of ad hoc techniques have emerged, e.g., using AutonomousSystem databases, domain prefixes for CNAME records. In this work we proposePythia, a novel lightweight approach for identifying Web content hosted onthird-party infrastructures, including both traditional Web hosts and contentdelivery networks. Our framework identifies the organization to which a givenWeb page belongs, and it detects which Web servers are self-hosted and whichones leverage third-party services to provide contents. To test our frameworkwe run it on 40,000 URLs and evaluate its accuracy, both by comparing theresults with similar services and with a manually validated groundtruth. Ourtool achieves an accuracy of 90% and detects that under 11% of popular domainsare self-hosted. We publicly release our tool to allow other researchers toreproduce our findings, and to apply it to their own studies.

Kissing Cuisines: Exploring Worldwide Culinary Habits on the Web

  Food and nutrition occupy an increasingly prevalent space on the web, anddishes and recipes shared online provide an invaluable mirror into culinarycultures and attitudes around the world. More specifically, ingredients,flavors, and nutrition information become strong signals of the tastepreferences of individuals and civilizations. However, there is littleunderstanding of these palate varieties. In this paper, we present alarge-scale study of recipes published on the web and their content, aiming tounderstand cuisines and culinary habits around the world. Using a database ofmore than 157K recipes from over 200 different cuisines, we analyzeingredients, flavors, and nutritional values which distinguish dishes fromdifferent regions, and use this knowledge to assess the predictability ofrecipes from different cuisines. We then use country health statistics tounderstand the relation between these factors and health indicators ofdifferent nations, such as obesity, diabetes, migration, and healthexpenditure. Our results confirm the strong effects of geographical andcultural similarities on recipes, health indicators, and culinary preferencesacross the globe.

MaMaDroid: Detecting Android Malware by Building Markov Chains of  Behavioral Models

  The rise in popularity of the Android platform has resulted in an explosionof malware threats targeting it. As both Android malware and the operatingsystem itself constantly evolve, it is very challenging to design robustmalware mitigation techniques that can operate for long periods of time withoutthe need for modifications or costly re-training. In this paper, we presentMaMaDroid, an Android malware detection system that relies on app behavior.MaMaDroid builds a behavioral model, in the form of a Markov chain, from thesequence of abstracted API calls performed by an app, and uses it to extractfeatures and perform classification. By abstracting calls to their packages orfamilies, MaMaDroid maintains resilience to API changes and keeps the featureset size manageable. We evaluate its accuracy on a dataset of 8.5K benign and35.5K malicious apps collected over a period of six years, showing that it notonly effectively detects malware (with up to 99% F-measure), but also that themodel built by the system keeps its detection capabilities for long periods oftime (on average, 86% and 75% F-measure, respectively, one and two years aftertraining). Finally, we compare against DroidAPIMiner, a state-of-the-art systemthat relies on the frequency of API calls performed by apps, showing thatMaMaDroid significantly outperforms it.

Mean Birds: Detecting Aggression and Bullying on Twitter

  In recent years, bullying and aggression against users on social media havegrown significantly, causing serious consequences to victims of alldemographics. In particular, cyberbullying affects more than half of youngsocial media users worldwide, and has also led to teenage suicides, prompted byprolonged and/or coordinated digital harassment. Nonetheless, tools andtechnologies for understanding and mitigating it are scarce and mostlyineffective. In this paper, we present a principled and scalable approach todetect bullying and aggressive behavior on Twitter. We propose a robustmethodology for extracting text, user, and network-based attributes, studyingthe properties of cyberbullies and aggressors, and what features distinguishthem from regular users. We find that bully users post less, participate infewer online communities, and are less popular than normal users, whileaggressors are quite popular and tend to include more negativity in theirposts. We evaluate our methodology using a corpus of 1.6M tweets posted over 3months, and show that machine learning classification algorithms can accuratelydetect users exhibiting bullying and aggressive behavior, achieving over 90%AUC.

Measuring #GamerGate: A Tale of Hate, Sexism, and Bullying

  Over the past few years, online aggression and abusive behaviors haveoccurred in many different forms and on a variety of platforms. In extremecases, these incidents have evolved into hate, discrimination, and bullying,and even materialized into real-world threats and attacks against individualsor groups. In this paper, we study the Gamergate controversy. Started in August2014 in the online gaming world, it quickly spread across various socialnetworking platforms, ultimately leading to many incidents of cyberbullying andcyberaggression. We focus on Twitter, presenting a measurement study of adataset of 340k unique users and 1.6M tweets to study the properties of theseusers, the content they post, and how they differ from random Twitter users. Wefind that users involved in this "Twitter war" tend to have more friends andfollowers, are generally more engaged and post tweets with negative sentiment,less joy, and more hate than random users. We also perform preliminarymeasurements on how the Twitter suspension mechanism deals with such abusivebehaviors. While we focus on Gamergate, our methodology to collect and analyzetweets related to aggressive and bullying activities is of independentinterest.

The Web Centipede: Understanding How Web Communities Influence Each  Other Through the Lens of Mainstream and Alternative News Sources

  As the number and the diversity of news outlets on the Web grow, so does theopportunity for "alternative" sources of information to emerge. Using largesocial networks like Twitter and Facebook, misleading, false, or agenda-driveninformation can quickly and seamlessly spread online, deceiving people orinfluencing their opinions. Also, the increased engagement of tightly knitcommunities, such as Reddit and 4chan, further compounds the problem, as theirusers initiate and propagate alternative information, not only within their owncommunities, but also to different ones as well as various social media. Infact, these platforms have become an important piece of the modern informationecosystem, which, thus far, has not been studied as a whole.  In this paper, we begin to fill this gap by studying mainstream andalternative news shared on Twitter, Reddit, and 4chan. By analyzing millions ofposts around several axes, we measure how mainstream and alternative news flowsbetween these platforms. Our results indicate that alt-right communities within4chan and Reddit can have a surprising level of influence on Twitter, providingevidence that "fringe" communities often succeed in spreading alternative newsto mainstream social networks and the greater Web.

POISED: Spotting Twitter Spam Off the Beaten Paths

  Cybercriminals have found in online social networks a propitious medium tospread spam and malicious content. Existing techniques for detecting spaminclude predicting the trustworthiness of accounts and analyzing the content ofthese messages. However, advanced attackers can still successfully evade thesedefenses.  Online social networks bring people who have personal connections or sharecommon interests to form communities. In this paper, we first show that userswithin a networked community share some topics of interest. Moreover, contentshared on these social network tend to propagate according to the interests ofpeople. Dissemination paths may emerge where some communities post similarmessages, based on the interests of those communities. Spam and other maliciouscontent, on the other hand, follow different spreading patterns.  In this paper, we follow this insight and present POISED, a system thatleverages the differences in propagation between benign and malicious messageson social networks to identify spam and other unwanted content. We test oursystem on a dataset of 1.3M tweets collected from 64K users, and we show thatour approach is effective in detecting malicious messages, reaching 91%precision and 93% recall. We also show that POISED's detection is morecomprehensive than previous systems, by comparing it to three state-of-the-artspam detection systems that have been proposed by the research community in thepast. POISED significantly outperforms each of these systems. Moreover, throughsimulations, we show how POISED is effective in the early detection of spammessages and how it is resilient against two well-known adversarial machinelearning attacks.

Eight Years of Rider Measurement in the Android Malware Ecosystem:  Evolution and Lessons Learned

  Despite the growing threat posed by Android malware, the research communityis still lacking a comprehensive view of common behaviors and trends exposed bymalware families active on the platform. Without such view, the researchersincur the risk of developing systems that only detect outdated threats, missingthe most recent ones. In this paper, we conduct the largest measurement ofAndroid malware behavior to date, analyzing over 1.2 million malware samplesthat belong to 1.2K families over a period of eight years (from 2010 to 2017).We aim at understanding how the behavior of Android malware has evolved overtime, focusing on repackaging malware. In this type of threats differentinnocuous apps are piggybacked with a malicious payload (rider), allowinginexpensive malware manufacturing.  One of the main challenges posed when studying repackaged malware is slicingthe app to split benign components apart from the malicious ones. To addressthis problem, we use differential analysis to isolate software components thatare irrelevant to the campaign and study the behavior of malicious ridersalone. Our analysis framework relies on collective repositories and recentadvances on the systematization of intelligence extracted from multipleanti-virus vendors. We find that since its infancy in 2010, the Android malwareecosystem has changed significantly, both in the type of malicious activityperformed by the malicious samples and in the level of obfuscation used bymalware to avoid detection. We then show that our framework can aid analystswho attempt to study unknown malware families. Finally, we discuss what ourfindings mean for Android malware detection research, highlighting areas thatneed further attention by the research community.

Understanding Web Archiving Services and Their (Mis)Use on Social Media

  Web archiving services play an increasingly important role in today'sinformation ecosystem, by ensuring the continuing availability of information,or by deliberately caching content that might get deleted or removed. Amongthese, the Wayback Machine has been proactively archiving, since 2001, versionsof a large number of Web pages, while newer services like archive.is allowusers to create on-demand snapshots of specific Web pages, which serve as timecapsules that can be shared across the Web. In this paper, we present alarge-scale analysis of Web archiving services and their use on social media,shedding light on the actors involved in this ecosystem, the content that getsarchived, and how it is shared. We crawl and study: 1) 21M URLs fromarchive.is, spanning almost two years, and 2) 356K archive.is plus 391K WaybackMachine URLs that were shared on four social networks: Reddit, Twitter, Gab,and 4chan's Politically Incorrect board (/pol/) over 14 months. We observe thatnews and social media posts are the most common types of content archived,likely due to their perceived ephemeral and/or controversial nature. Moreover,URLs of archiving services are extensively shared on "fringe" communitieswithin Reddit and 4chan to preserve possibly contentious content. Lastly, wefind evidence of moderators nudging or even forcing users to use archives,instead of direct links, for news sources with opposing ideologies, potentiallydepriving them of ad revenue.

Large Scale Crowdsourcing and Characterization of Twitter Abusive  Behavior

  In recent years, offensive, abusive and hateful language, sexism, racism andother types of aggressive and cyberbullying behavior have been manifesting withincreased frequency, and in many online social media platforms. In fact, pastscientific work focused on studying these forms in popular media, such asFacebook and Twitter. Building on such work, we present an 8-month study of thevarious forms of abusive behavior on Twitter, in a holistic fashion. Departingfrom past work, we examine a wide variety of labeling schemes, which coverdifferent forms of abusive behavior, at the same time. We propose anincremental and iterative methodology, that utilizes the power of crowdsourcingto annotate a large scale collection of tweets with a set of abuse-relatedlabels. In fact, by applying our methodology including statistical analysis forlabel merging or elimination, we identify a reduced but robust set of labels.Finally, we offer a first overview and findings of our collected and annotateddataset of 100 thousand tweets, which we make publicly available for furtherscientific exploration.

What is Gab? A Bastion of Free Speech or an Alt-Right Echo Chamber?

  Over the past few years, a number of new "fringe" communities, like 4chan orcertain subreddits, have gained traction on the Web at a rapid pace. However,more often than not, little is known about how they evolve or what kind ofactivities they attract, despite recent research has shown that they influencehow false information reaches mainstream communities. This motivates the needto monitor these communities and analyze their impact on the Web's informationecosystem. In August 2016, a new social network called Gab was created as analternative to Twitter. It positions itself as putting "people and free speechfirst'", welcoming users banned or suspended from other social networks. Inthis paper, we provide, to the best of our knowledge, the firstcharacterization of Gab. We collect and analyze 22M posts produced by 336Kusers between August 2016 and January 2018, finding that Gab is predominantlyused for the dissemination and discussion of news and world events, and that itattracts alt-right users, conspiracy theorists, and other trolls. We alsomeasure the prevalence of hate speech on the platform, finding it to be muchhigher than Twitter, but lower than 4chan's Politically Incorrect board.

You are your Metadata: Identification and Obfuscation of Social Media  Users using Metadata Information

  Metadata are associated to most of the information we produce in our dailyinteractions and communication in the digital world. Yet, surprisingly,metadata are often still catergorized as non-sensitive. Indeed, in the past,researchers and practitioners have mainly focused on the problem of theidentification of a user from the content of a message.  In this paper, we use Twitter as a case study to quantify the uniqueness ofthe association between metadata and user identity and to understand theeffectiveness of potential obfuscation strategies. More specifically, weanalyze atomic fields in the metadata and systematically combine them in aneffort to classify new tweets as belonging to an account using differentmachine learning algorithms of increasing complexity. We demonstrate thatthrough the application of a supervised learning algorithm, we are able toidentify any user in a group of 10,000 with approximately 96.7% accuracy.Moreover, if we broaden the scope of our search and consider the 10 most likelycandidates we increase the accuracy of the model to 99.22%. We also found thatdata obfuscation is hard and ineffective for this type of data: even afterperturbing 60% of the training data, it is still possible to classify userswith an accuracy higher than 95%. These results have strong implications interms of the design of metadata obfuscation strategies, for example for dataset release, not only for Twitter, but, more generally, for most social mediaplatforms.

On the Origins of Memes by Means of Fringe Web Communities

  Internet memes are increasingly used to sway and manipulate public opinion.This prompts the need to study their propagation, evolution, and influenceacross the Web. In this paper, we detect and measure the propagation of memesacross multiple Web communities, using a processing pipeline based onperceptual hashing and clustering techniques, and a dataset of 160M images from2.6B posts gathered from Twitter, Reddit, 4chan's Politically Incorrect board(/pol/), and Gab, over the course of 13 months. We group the images posted onfringe Web communities (/pol/, Gab, and The_Donald subreddit) into clusters,annotate them using meme metadata obtained from Know Your Meme, and also mapimages from mainstream communities (Twitter and Reddit) to the clusters.  Our analysis provides an assessment of the popularity and diversity of memesin the context of each community, showing, e.g., that racist memes areextremely common in fringe Web communities. We also find a substantial numberof politics-related memes on both mainstream and fringe Web communities,supporting media reports that memes might be used to enhance or harmpoliticians. Finally, we use Hawkes processes to model the interplay betweenWeb communities and quantify their reciprocal influence, finding that /pol/substantially influences the meme ecosystem with the number of memes itproduces, while \td has a higher success rate in pushing them to othercommunities.

LOBO -- Evaluation of Generalization Deficiencies in Twitter Bot  Classifiers

  Botnets in online social networks are increasingly often affecting theregular flow of discussion, attacking regular users and their posts, spammingthem with irrelevant or offensive content, and even manipulating the popularityof messages and accounts. Researchers and cybercriminals are involved in anarms race, and new and updated botnets designed to defeat current detectionsystems are constantly developed, rendering such detection systems obsolete.  In this paper, we motivate the need for a generalized evaluation in Twitterbot detection and propose a methodology to evaluate bot classifiers by testingthem on unseen bot classes. We show that this methodology is empiricallyrobust, using bot classes of varying sizes and characteristics and reachingsimilar results, and argue that methods trained and tested on single botclasses or datasets might not able to generalize to new bot classes. We trainone such classifier on over 200,000 data points and show that it achieves over97% accuracy. The data used to train and test this classifier includes some ofthe largest and most varied collections of bots used in literature. We thentest this theoretically sound classifier using our methodology, highlightingthat it does not generalize well to unseen bot classes. Finally, we discuss theimplications of our results, and reasons why some bot classes are easier andfaster to detect than others.

Characterizing the Use of Images by State-Sponsored Troll Accounts on  Twitter

  Anecdotal evidence has emerged suggesting that state-sponsored organizations,like the Russian Internet Research Agency, have exploited mainstream social.Their primary goal is apparently to conduct information warfare operations tomanipulate public opinion using accounts disguised as "normal" people. Toincrease engagement and credibility of their posts, these accounts regularlyshare images. However, the use of images by state-sponsored accounts has yet tobe examined by the research community.  In this work, we address this gap by analyzing a ground truth dataset of 1.8Mimages posted to Twitter by called Russian trolls. More specifically, weanalyze the content of the images, as well as the posting activity of theaccounts. Among other things, we find that image posting activity of Russiantrolls is tightly coupled with real-world events, and that their targets, aswell as the content shared, changed over time. When looking at the interplaybetween domains that shared the same images as state-sponsored trolls, we findclear cut differences in the origin and/or spread of images across the Web.Overall, our findings provide new insight into how state-sponsored trollsoperate, and specifically how they use imagery to achieve their goals.

Disturbed YouTube for Kids: Characterizing and Detecting Disturbing  Content on YouTube

  A considerable number of the most-subscribed YouTube channels feature contentpopular among children of very young age. Hundreds of toddler-oriented channelson YouTube offer inoffensive, well produced, and educational videos.Unfortunately, inappropriate (disturbing) content that targets this demographicis also common. YouTube's algorithmic recommendation system regrettablysuggests inappropriate content because some of it mimics or is derived fromotherwise appropriate content. Considering the risk for early childhooddevelopment, and an increasing trend in toddler's consumption of YouTube media,this is a worrying problem. While there are many anecdotal reports of the scaleof the problem, there is no systematic quantitative measurement.  Hence, in this work, we develop a classifier able to detect toddler-orientedinappropriate content on YouTube with 82.8% accuracy, and we leverage it toperform a first-of-its-kind, large-scale, quantitative characterization thatreveals some of the risks of YouTube media consumption by young children. Ouranalysis indicates that YouTube's currently deployed counter-measures areineffective in terms of detecting disturbing videos in a timely manner.Finally, using our classifier, we assess how prominent the problem is onYouTube, finding that young children are likely to encounter disturbing videoswhen they randomly browse the platform starting from benign videos.

Kek, Cucks, and God Emperor Trump: A Measurement Study of 4chan's  Politically Incorrect Forum and Its Effects on the Web

  The discussion-board site 4chan has been part of the Internet's darkunderbelly since its inception, and recent political events have put itincreasingly in the spotlight. In particular, /pol/, the "PoliticallyIncorrect" board, has been a central figure in the outlandish 2016 US electionseason, as it has often been linked to the alt-right movement and its rhetoricof hate and racism. However, 4chan remains relatively unstudied by thescientific community: little is known about its user base, the content itgenerates, and how it affects other parts of the Web. In this paper, we startaddressing this gap by analyzing /pol/ along several axes, using a dataset ofover 8M posts we collected over two and a half months. First, we perform ageneral characterization, showing that /pol/ users are well distributed aroundthe world and that 4chan's unique features encourage fresh discussions. We alsoanalyze content, finding, for instance, that YouTube links and hate speech arepredominant on /pol/. Overall, our analysis not only provides the firstmeasurement study of /pol/, but also insight into online harassment and hatespeech trends in social media.

Hate is not Binary: Studying Abusive Behavior of #GamerGate on Twitter

  Over the past few years, online bullying and aggression have becomeincreasingly prominent, and manifested in many different forms on social media.However, there is little work analyzing the characteristics of abusive usersand what distinguishes them from typical social media users. In this paper, westart addressing this gap by analyzing tweets containing a great large amountof abusiveness. We focus on a Twitter dataset revolving around the Gamergatecontroversy, which led to many incidents of cyberbullying and cyberaggressionon various gaming and social media platforms. We study the properties of theusers tweeting about Gamergate, the content they post, and the differences intheir behavior compared to typical Twitter users.  We find that while their tweets are often seemingly about aggressive andhateful subjects, "Gamergaters" do not exhibit common expressions of onlineanger, and in fact primarily differ from typical users in that their tweets areless joyful. They are also more engaged than typical Twitter users, which is anindication as to how and why this controversy is still ongoing. Surprisingly,we find that Gamergaters are less likely to be suspended by Twitter, thus weanalyze their properties to identify differences from typical users and whatmay have led to their suspension. We perform an unsupervised machine learninganalysis to detect clusters of users who, though currently active, could beconsidered for suspension since they exhibit similar behaviors with suspendedusers. Finally, we confirm the usefulness of our analyzed features by emulatingthe Twitter suspension mechanism with a supervised learning method, achievingvery good precision and recall.

MaMaDroid: Detecting Android Malware by Building Markov Chains of  Behavioral Models (Extended Version)

  As Android has become increasingly popular, so has malware targeting it, thuspushing the research community to propose different detection techniques.However, the constant evolution of the Android ecosystem, and of malwareitself, makes it hard to design robust tools that can operate for long periodsof time without the need for modifications or costly re-training. Aiming toaddress this issue, we set to detect malware from a behavioral point of view,modeled as the sequence of abstracted API calls. We introduce MaMaDroid, astatic-analysis based system that abstracts the API calls performed by an appto their class, package, or family, and builds a model from their sequencesobtained from the call graph of an app as Markov chains. This ensures that themodel is more resilient to API changes and the features set is of manageablesize. We evaluate MaMaDroid using a dataset of 8.5K benign and 35.5K maliciousapps collected over a period of six years, showing that it effectively detectsmalware (with up to 0.99 F-measure) and keeps its detection capabilities forlong periods of time (up to 0.87 F-measure two years after training). We alsoshow that MaMaDroid remarkably outperforms DroidAPIMiner, a state-of-the-artdetection system that relies on the frequency of (raw) API calls. Aiming toassess whether MaMaDroid's effectiveness mainly stems from the API abstractionor from the sequencing modeling, we also evaluate a variant of it that usesfrequency (instead of sequences), of abstracted API calls. We find that it isnot as accurate, failing to capture maliciousness when trained on malwaresamples that include API calls that are equally or more frequently used bybenign apps.

Disinformation Warfare: Understanding State-Sponsored Trolls on Twitter  and Their Influence on the Web

  Over the past couple of years, anecdotal evidence has emerged linkingcoordinated campaigns by state-sponsored actors with efforts to manipulatepublic opinion on the Web, often around major political events, throughdedicated accounts, or "trolls." Although they are often involved in spreadingdisinformation on social media, there is little understanding of how thesetrolls operate, what type of content they disseminate, and most importantlytheir influence on the information ecosystem.  In this paper, we shed light on these questions by analyzing 27K tweetsposted by 1K Twitter users identified as having ties with Russia's InternetResearch Agency and thus likely state-sponsored trolls. We compare theirbehavior to a random set of Twitter users, finding interesting differences interms of the content they disseminate, the evolution of their account, as wellas their general behavior and use of Twitter. Then, using Hawkes Processes, wequantify the influence that trolls had on the dissemination of news on socialplatforms like Twitter, Reddit, and 4chan. Overall, our findings indicate thatRussian trolls managed to stay active for long periods of time and to reach asubstantial number of Twitter users with their tweets. When looking at theirability of spreading news content and making it viral, however, we find thattheir effect on social platforms was minor, with the significant exception ofnews published by the Russian state-sponsored news outlet RT (Russia Today).

A Family of Droids -- Android Malware Detection via Behavioral Modeling:  Static vs Dynamic Analysis

  Following the increasing popularity of mobile ecosystems, cybercriminals haveincreasingly targeted them, designing and distributing malicious apps thatsteal information or cause harm to the device's owner. Aiming to counter them,detection techniques based on either static or dynamic analysis that modelAndroid malware, have been proposed. While the pros and cons of these analysistechniques are known, they are usually compared in the context of theirlimitations e.g., static analysis is not able to capture runtime behaviors,full code coverage is usually not achieved during dynamic analysis, etc.Whereas, in this paper, we analyze the performance of static and dynamicanalysis methods in the detection of Android malware and attempt to comparethem in terms of their detection performance, using the same modeling approach.  To this end, we build on MaMaDroid, a state-of-the-art detection system thatrelies on static analysis to create a behavioral model from the sequences ofabstracted API calls. Then, aiming to apply the same technique in a dynamicanalysis setting, we modify CHIMP, a platform recently proposed to crowdsourcehuman inputs for app testing, in order to extract API calls' sequences from thetraces produced while executing the app on a CHIMP virtual device. We call thissystem AuntieDroid and instantiate it by using both automated (Monkey) anduser-generated inputs. We find that combining both static and dynamic analysisyields the best performance, with F-measure reaching 0.92. We also show thatstatic analysis is at least as effective as dynamic analysis, depending on howapps are stimulated during execution, and, finally, investigate the reasons forinconsistent misclassifications across methods.

"You Know What to Do": Proactive Detection of YouTube Videos Targeted by  Coordinated Hate Attacks

  Over the years, the Web has shrunk the world, allowing individuals to shareviewpoints with many more people than they are able to in real life. At thesame time, however, it has also enabled anti-social and toxic behavior to occurat an unprecedented scale. Video sharing platforms like YouTube receive uploadsfrom millions of users, covering a wide variety of topics and allowing othersto comment and interact in response. Unfortunately, these communities areperiodically plagued with aggression and hate attacks. In particular, recentwork has showed how these attacks often take place as a result of "raids,"i.e., organized efforts coordinated by ad-hoc mobs from third-partycommunities.  Despite the increasing relevance of this phenomenon, online services oftenlack effective countermeasures to mitigate it. Unlike well-studied problemslike spam and phishing, coordinated aggressive behavior both targets and isperpetrated by humans, making defense mechanisms that look for automatedactivity unsuitable. Therefore, the de-facto solution is to reactively rely onuser reports and human reviews. In this paper, we propose an automated solutionto identify videos that are likely to be targeted by coordinated harassers.First, we characterize and model YouTube videos along several axes (metadata,audio transcripts, thumbnails) based on a ground truth dataset of raid victims.Then, we use an ensemble of classifiers to determine the likelihood that avideo will be raided with high accuracy (AUC up to 94%). Overall, our workpaves the way for providing video platforms like YouTube with proactive systemsto detect and mitigate coordinated hate attacks.

Who Let The Trolls Out? Towards Understanding State-Sponsored Trolls

  Recent evidence has emerged linking coordinated campaigns by state-sponsoredactors to manipulate public opinion on the Web. Campaigns revolving aroundmajor political events are enacted via mission-focused "trolls." While trollsare involved in spreading disinformation on social media, there is littleunderstanding of how they operate, what type of content they disseminate, howtheir strategies evolve over time, and how they influence the Web's informationecosystem. In this paper, we begin to address this gap by analyzing 10M postsby 5.5K Twitter and Reddit users identified as Russian and Iranianstate-sponsored trolls. We compare the behavior of each group ofstate-sponsored trolls with a focus on how their strategies change over time,the different campaigns they embark on, and differences between the trollsoperated by Russia and Iran. Among other things, we find: 1) that Russiantrolls were pro-Trump while Iranian trolls were anti-Trump; 2) evidence thatcampaigns undertaken by such actors are influenced by real-world events; and 3)that the behavior of such actors is not consistent over time, hence automateddetection is not a straightforward task. Using the Hawkes Processes statisticalmodel, we quantify the influence these accounts have on pushing URLs on foursocial platforms: Twitter, Reddit, 4chan's Politically Incorrect board (/pol/),and Gab. In general, Russian trolls were more influential and efficient inpushing URLs to all the other platforms with the exception of /pol/ whereIranians were more influential. Finally, we release our data and source code toensure the reproducibility of our results and to encourage other researchers towork on understanding other emerging kinds of state-sponsored troll accounts onTwitter.

