Minimum Weight Cycles and Triangles: Equivalences and Algorithms

  We consider the fundamental algorithmic problem of finding a cycle of minimumweight in a weighted graph. In particular, we show that the minimum weightcycle problem in an undirected n-node graph with edge weights in {1,...,M} orin a directed n-node graph with edge weights in {-M,..., M} and no negativecycles can be efficiently reduced to finding a minimum weight triangle in anTheta(n)-node undirected graph with weights in {1,...,O(M)}. Roughly speaking,our reductions imply the following surprising phenomenon: a minimum cycle withan arbitrary number of weighted edges can be "encoded" using only three edgeswithin roughly the same weight interval! This resolves a longstanding openproblem posed by Itai and Rodeh [SIAM J. Computing 1978 and STOC'77].  A direct consequence of our efficient reductions are O (Mn^{omega})-timealgorithms using fast matrix multiplication (FMM) for finding a minimum weightcycle in both undirected graphs with integral weights from the interval [1,M]and directed graphs with integral weights from the interval [-M,M]. The latterseems to reveal a strong separation between the all pairs shortest paths (APSP)problem and the minimum weight cycle problem in directed graphs as the fastestknown APSP algorithm has a running time of O(M^{0.681}n^{2.575}) by Zwick [J.ACM 2002].  In contrast, when only combinatorial algorithms are allowed (that is, withoutFMM) the only known solution to minimum weight cycle is by computing APSP.Interestingly, any separation between the two problems in this case would be anamazing breakthrough as by a recent paper by Vassilevska W. and Williams[FOCS'10], any O(n^{3-eps})-time algorithm (eps>0) for minimum weight cycleimmediately implies a O(n^{3-delta})-time algorithm (delta>0) for APSP.

Faster Replacement Paths

  The replacement paths problem for directed graphs is to find for given nodess and t and every edge e on the shortest path between them, the shortest pathbetween s and t which avoids e. For unweighted directed graphs on n vertices,the best known algorithm runtime was \tilde{O}(n^{2.5}) by Roditty and Zwick.For graphs with integer weights in {-M,...,M}, Weimann and Yuster recentlyshowed that one can use fast matrix multiplication and solve the problem inO(Mn^{2.584}) time, a runtime which would be O(Mn^{2.33}) if the exponent\omega of matrix multiplication is 2.  We improve both of these algorithms. Our new algorithm also relies on fastmatrix multiplication and runs in O(M n^{\omega} polylog(n)) time if \omega>2and O(n^{2+\eps}) for any \eps>0 if \omega=2. Our result shows that, at leastfor small integer weights, the replacement paths problem in directed graphs maybe easier than the related all pairs shortest paths problem in directed graphs,as the current best runtime for the latter is \Omega(n^{2.5}) time even if\omega=2.

Finding heaviest H-subgraphs in real weighted graphs, with applications

  For a graph G with real weights assigned to the vertices (edges), the MAXH-SUBGRAPH problem is to find an H-subgraph of G with maximum total weight, ifone exists. The all-pairs MAX H-SUBGRAPH problem is to find for every pair ofvertices u,v, a maximum H-subgraph containing both u and v, if one exists. Ourmain results are new strongly polynomial algorithms for the all-pairs MAXH-SUBGRAPH problem for vertex weighted graphs. We also give improved algorithmsfor the MAX-H SUBGRAPH problem for edge weighted graphs, and various relatedproblems, including computing the first k most significant bits of the distanceproduct of two matrices. Some of our algorithms are based, in part, on fastmatrix multiplication.

A 7/3-Approximation for Feedback Vertex Sets in Tournaments

  We consider the minimum-weight feedback vertex set problem in tournaments:given a tournament with non-negative vertex weights, remove a minimum-weightset of vertices that intersects all cycles. This problem is $\mathsf{NP}$-hardto solve exactly, and Unique Games-hard to approximate by a factor better than2. We present the first $7/3$ approximation algorithm for this problem,improving on the previously best known ratio $5/2$ given by Cai et al. [FOCS1998, SICOMP 2001].

Approximating Cycles in Directed Graphs: Fast Algorithms for Girth and  Roundtrip Spanners

  The girth of a graph, i.e. the length of its shortest cycle, is a fundamentalgraph parameter. Unfortunately all known algorithms for computing, evenapproximately, the girth and girth-related structures in directed weighted$m$-edge and $n$-node graphs require $\Omega(\min\{n^{\omega}, mn\})$ time (for$2\leq\omega<2.373$). In this paper, we drastically improve these runtimes asfollows:  * Multiplicative Approximations in Nearly Linear Time: We give an algorithmthat in $\widetilde{O}(m)$ time computes an $\widetilde{O}(1)$-multiplicativeapproximation of the girth as well as an $\widetilde{O}(1)$-multiplicativeroundtrip spanner with $\widetilde{O}(n)$ edges with high probability (w.h.p).  * Nearly Tight Additive Approximations: For unweighted graphs and any $\alpha\in (0,1)$ we give an algorithm that in $\widetilde{O}(mn^{1 - \alpha})$ timecomputes an $O(n^\alpha)$-additive approximation of the girth w.h.p, andpartially derandomize it. We show that the runtime of our algorithm cannot besignificantly improved without a breakthrough in combinatorial Boolean matrixmultiplication.  Our main technical contribution to achieve these results is the first nearlylinear time algorithm for computing roundtrip covers, a directed graphdecomposition concept key to previous roundtrip spanner constructions.Previously it was not known how to compute these significantly faster than$\Omega(\min\{n^\omega, mn\})$ time. Given the traditional difficulty inefficiently processing directed graphs, we hope our techniques may find furtherapplications.

Graph pattern detection: Hardness for all induced patterns and faster  non-induced cycles

  We consider the pattern detection problem in graphs: given a constant sizepattern graph $H$ and a host graph $G$, determine whether $G$ contains asubgraph isomorphic to $H$. Our main results are:  * We prove that if a pattern $H$ contains a $k$-clique subgraph, thendetecting whether an $n$ node host graph contains a not necessarily inducedcopy of $H$ requires at least the time for detecting whether an $n$ node graphcontains a $k$-clique. The previous result of this nature required that $H$contains a $k$-clique which is disjoint from all other $k$-cliques of $H$.  * We show that if the famous Hadwiger conjecture from graph theory is true,then detecting whether an $n$ node host graph contains a not necessarilyinduced copy of a pattern with chromatic number $t$ requires at least the timefor detecting whether an $n$ node graph contains a $t$-clique. This impliesthat: (1) under Hadwiger's conjecture for every $k$-node pattern $H$, findingan induced copy of $H$ requires at least the time of $\sqrt k$-cliquedetection, and at least size $\omega(n^{\sqrt{k}/4})$ for any constant depthcircuit, and (2) unconditionally, detecting an induced copy of a random$G(k,p)$ pattern w.h.p. requires at least the time of $\Theta(k/\log k)$-cliquedetection, and hence also at least size $n^{\Omega(k/\log k)}$ for circuits ofconstant depth.  * Finally, we consider the case when the pattern is a directed cycle on $k$nodes, and we would like to detect whether a directed $m$-edge graph $G$contains a $k$-Cycle as a not necessarily induced subgraph. We resolve a 14year old conjecture of [Yuster-Zwick SODA'04] on the complexity of $k$-Cycledetection by giving a tight analysis of their $k$-Cycle algorithm. Our analysisimproves the best bounds for $k$-Cycle detection in directed graphs, for all$k>5$.

Popular conjectures imply strong lower bounds for dynamic problems

  We consider several well-studied problems in dynamic algorithms and provethat sufficient progress on any of them would imply a breakthrough on one offive major open problems in the theory of algorithms:  1. Is the 3SUM problem on $n$ numbers in $O(n^{2-\epsilon})$ time for some$\epsilon>0$?  2. Can one determine the satisfiability of a CNF formula on $n$ variables in$O((2-\epsilon)^n poly n)$ time for some $\epsilon>0$?  3. Is the All Pairs Shortest Paths problem for graphs on $n$ vertices in$O(n^{3-\epsilon})$ time for some $\epsilon>0$?  4. Is there a linear time algorithm that detects whether a given graphcontains a triangle?  5. Is there an $O(n^{3-\epsilon})$ time combinatorial algorithm for $n\timesn$ Boolean matrix multiplication?  The problems we consider include dynamic versions of bipartite perfectmatching, bipartite maximum weight matching, single source reachability, singlesource shortest paths, strong connectivity, subgraph connectivity, diameterapproximation and some nongraph problems such as Pagh's problem defined in arecent paper by Patrascu [STOC 2010].

Quantum algorithms for shortest paths problems in structured instances

  We consider the quantum time complexity of the all pairs shortest paths(APSP) problem and some of its variants. The trivial classical algorithm forAPSP and most all pairs path problems runs in $O(n^3)$ time, while the trivialalgorithm in the quantum setting runs in $\tilde{O}(n^{2.5})$ time, usingGrover search. A major open problem in classical algorithms is to obtain atruly subcubic time algorithm for APSP, i.e. an algorithm running in$O(n^{3-\varepsilon})$ time for constant $\varepsilon>0$. To approach thisproblem, many truly subcubic time classical algorithms have been devised forAPSP and its variants for structured inputs. Some examples of such problems areAPSP in geometrically weighted graphs, graphs with small integer edge weightsor a small number of weights incident to each vertex, and the all pairsearliest arrivals problem. In this paper we revisit these problems in thequantum setting and obtain the first nontrivial (i.e. $O(n^{2.5-\varepsilon})$time) quantum algorithms for the problems.

Quadratic-Time Hardness of LCS and other Sequence Similarity Measures

  Two important similarity measures between sequences are the longest commonsubsequence (LCS) and the dynamic time warping distance (DTWD). Thecomputations of these measures for two given sequences are central tasks in avariety of applications. Simple dynamic programming algorithms solve thesetasks in $O(n^2)$ time, and despite an extensive amount of research, noalgorithms with significantly better worst case upper bounds are known.  In this paper, we show that an $O(n^{2-\epsilon})$ time algorithm, for some$\epsilon>0$, for computing the LCS or the DTWD of two sequences of length $n$over a constant size alphabet, refutes the popular Strong Exponential TimeHypothesis (SETH). Moreover, we show that computing the LCS of $k$ strings overan alphabet of size $O(k)$ cannot be done in $O(n^{k-\epsilon})$ time, for any$\epsilon>0$, under SETH. Finally, we also address the time complexity ofapproximating the DTWD of two strings in truly subquadratic time.

Very Sparse Additive Spanners and Emulators

  We obtain new upper bounds on the additive distortion for graph emulators andspanners on relatively few edges. We introduce a new subroutine called "stripcreation," and we combine this subroutine with several other ideas to obtainthe following results:  \item Every graph has a spanner on $O(n^{1+\epsilon})$ edges with$\tilde{O}(n^{1/2 - \epsilon/2})$ additive distortion, for arbitrary$\epsilon\in [0,1]$. \item Every graph has an emulator on $\tilde{O}(n^{1 +\epsilon})$ edges with $\tilde{O}(n^{1/3 - 2\epsilon/3})$ additive distortionwhenever $\epsilon \in [0, \frac{1}{5}]$. \item Every graph has a spanner on$\tilde{O}(n^{1 + \epsilon})$ edges with $\tilde{O}(n^{2/3 - 5\epsilon/3})$additive distortion whenever $\epsilon \in [0, \frac{1}{4}]$.  Our first spanner has the new best known asymptotic edge-error tradeoff foradditive spanners whenever $\epsilon \in [0, \frac{1}{7}]$. Our second spannerhas the new best tradeoff whenever $\epsilon \in [\frac{1}{7}, \frac{3}{17}]$.Our emulator has the new best asymptotic edge-error tradeoff whenever $\epsilon\in [0, \frac{1}{5}]$.

Towards Tight Approximation Bounds for Graph Diameter and Eccentricities

  Among the most important graph parameters is the Diameter, the largestdistance between any two vertices. There are no known very efficient algorithmsfor computing the Diameter exactly. Thus, much research has been devoted to howfast this parameter can be approximated. Chechik et al. showed that thediameter can be approximated within a multiplicative factor of $3/2$ in$\tilde{O}(m^{3/2})$ time. Furthermore, Roditty and Vassilevska W. showed thatunless the Strong Exponential Time Hypothesis (SETH) fails, no$O(n^{2-\epsilon})$ time algorithm can achieve an approximation factor betterthan $3/2$ in sparse graphs. Thus the above algorithm is essentially optimalfor sparse graphs for approximation factors less than $3/2$. It was, however,completely plausible that a $3/2$-approximation is possible in linear time. Inthis work we conditionally rule out such a possibility by showing that unlessSETH fails no $O(m^{3/2-\epsilon})$ time algorithm can achieve an approximationfactor better than $5/3$.  Another fundamental set of graph parameters are the Eccentricities. TheEccentricity of a vertex $v$ is the distance between $v$ and the farthestvertex from $v$. Chechik et al. showed that the Eccentricities of all verticescan be approximated within a factor of $5/3$ in $\tilde{O}(m^{3/2})$ time andAbboud et al. showed that no $O(n^{2-\epsilon})$ algorithm can achieve betterthan $5/3$ approximation in sparse graphs. We show that the runtime of the$5/3$ approximation algorithm is also optimal under SETH. We also show that nonear-linear time algorithm can achieve a better than $2$ approximation for theEccentricities and that this is essentially tight: we give an algorithm thatapproximates Eccentricities within a $2+\delta$ factor in $\tilde{O}(m/\delta)$time for any $0<\delta<1$. This beats all Eccentricity algorithms in Cairo etal.

Deterministic Time-Space Tradeoffs for k-SUM

  Given a set of numbers, the $k$-SUM problem asks for a subset of $k$ numbersthat sums to zero. When the numbers are integers, the time and space complexityof $k$-SUM is generally studied in the word-RAM model; when the numbers arereals, the complexity is studied in the real-RAM model, and space is measuredby the number of reals held in memory at any point.  We present a time and space efficient deterministic self-reduction for the$k$-SUM problem which holds for both models, and has many interestingconsequences. To illustrate:  * $3$-SUM is in deterministic time $O(n^2 \lg\lg(n)/\lg(n))$ and space$O\left(\sqrt{\frac{n \lg(n)}{\lg\lg(n)}}\right)$. In general, anypolylogarithmic-time improvement over quadratic time for $3$-SUM can beconverted into an algorithm with an identical time improvement but low spacecomplexity as well. * $3$-SUM is in deterministic time $O(n^2)$ and space$O(\sqrt n)$, derandomizing an algorithm of Wang.  * A popular conjecture states that 3-SUM requires $n^{2-o(1)}$ time on theword-RAM. We show that the 3-SUM Conjecture is in fact equivalent to the(seemingly weaker) conjecture that every $O(n^{.51})$-space algorithm for$3$-SUM requires at least $n^{2-o(1)}$ time on the word-RAM.  * For $k \ge 4$, $k$-SUM is in deterministic $O(n^{k - 2 + 2/k})$ time and$O(\sqrt{n})$ space.

Tight Hardness for Shortest Cycles and Paths in Sparse Graphs

  Fine-grained reductions have established equivalences between many coreproblems with $\tilde{O}(n^3)$-time algorithms on $n$-node weighted graphs,such as Shortest Cycle, All-Pairs Shortest Paths (APSP), Radius, ReplacementPaths, Second Shortest Paths, and so on. These problems also have$\tilde{O}(mn)$-time algorithms on $m$-edge $n$-node weighted graphs, and suchalgorithms have wider applicability. Are these $mn$ bounds optimal when $m \lln^2$?  Starting from the hypothesis that the minimum weight $(2\ell+1)$-Cliqueproblem in edge weighted graphs requires $n^{2\ell+1-o(1)}$ time, we prove thatfor all sparsities of the form $m = \Theta(n^{1+1/\ell})$, there is no $O(n^2 +mn^{1-\epsilon})$ time algorithm for $\epsilon>0$ for \emph{any} of the belowproblems:  Minimum Weight $(2\ell+1)$-Cycle in a directed weighted graph,  Shortest Cycle in a directed weighted graph,  APSP in a directed or undirected weighted graph,  Radius (or Eccentricities) in a directed or undirected weighted graph,  Wiener index of a directed or undirected weighted graph,  Replacement Paths in a directed weighted graph,  Second Shortest Path in a directed weighted graph,  Betweenness Centrality of a given node in a directed weighted graph.  That is, we prove hardness for a variety of sparse graph problems from thehardness of a dense graph problem. Our results also lead to new conditionallower bounds from several related hypothesis for unweighted sparse graphproblems including $k$-cycle, shortest cycle, Radius, Wiener index and APSP.

Subtree Isomorphism Revisited

  The Subtree Isomorphism problem asks whether a given tree is contained inanother given tree. The problem is of fundamental importance and has beenstudied since the 1960s. For some variants, e.g., ordered trees, near-lineartime algorithms are known, but for the general case truly subquadraticalgorithms remain elusive.  Our first result is a reduction from the Orthogonal Vectors problem toSubtree Isomorphism, showing that a truly subquadratic algorithm for the latterrefutes the Strong Exponential Time Hypothesis (SETH).  In light of this conditional lower bound, we focus on natural special casesfor which no truly subquadratic algorithms are known. We classify these casesagainst the quadratic barrier, showing in particular that:  -- Even for binary, rooted trees, a truly subquadratic algorithm refutesSETH.  -- Even for rooted trees of depth $O(\log\log{n})$, where $n$ is the totalnumber of vertices, a truly subquadratic algorithm refutes SETH.  -- For every constant $d$, there is a constant $\epsilon_d>0$ and arandomized, truly subquadratic algorithm for degree-$d$ rooted trees of depthat most $(1+ \epsilon_d) \log_{d}{n}$. In particular, there is an $O(\min\{2.85^h ,n^2 \})$ algorithm for binary trees of depth $h$.  Our reductions utilize new "tree gadgets" that are likely useful for futureSETH-based lower bounds for problems on trees. Our upper bounds apply afolklore result from randomized decision tree complexity.

Who Can Win a Single-Elimination Tournament?

  A single-elimination (SE) tournament is a popular way to select a winner inboth sports competitions and in elections. A natural and well-studied questionis the tournament fixing problem (TFP): given the set of all pairwise matchoutcomes, can a tournament organizer rig an SE tournament by adjusting theinitial seeding so that their favorite player wins? We prove new sufficientconditions on the pairwise match outcome information and the favorite player,under which there is guaranteed to be a seeding where the player wins thetournament. Our results greatly generalize previous results. We alsoinvestigate the relationship between the set of players that can win an SEtournament under some seeding (so called SE winners) and other traditionaltournament solutions. In addition, we generalize and strengthen prior work onprobabilistic models for generating tournaments. For instance, we show that\emph{every} player in an $n$ player tournament generated by the CondorcetRandom Model will be an SE winner even when the noise is as small as possible,$p=\Theta(\ln n/n)$; prior work only had such results for $p\geq\Omega(\sqrt{\ln n/n})$. We also establish new results for significantly moregeneral generative models.

Conditional Hardness for Sensitivity Problems

  In recent years it has become popular to study dynamic problems in asensitivity setting: Instead of allowing for an arbitrary sequence of updates,the sensitivity model only allows to apply batch updates of small size to theoriginal input data. The sensitivity model is particularly appealing sincerecent strong conditional lower bounds ruled out fast algorithms for manydynamic problems, such as shortest paths, reachability, or subgraphconnectivity.  In this paper we prove conditional lower bounds for sensitivity problems. Forexample, we show that under the Boolean Matrix Multiplication (BMM) conjecturecombinatorial algorithms cannot compute the (4/3 - {\epsilon})-approximatediameter of an undirected unweighted dense graph with truly subcubicpreprocessing time and truly subquadratic update/query time. This result issurprising since in the static setting it is not clear whether a reduction fromBMM to diameter is possible. We further show under the BMM conjecture that manyproblems, such as reachability or approximate shortest paths, cannot be solvedfaster than by recomputation from scratch even after only one or two edgeinsertions. We give more lower bounds under the Strong Exponential TimeHypothesis and the All Pairs Shortest Paths Conjecture. Many of our lowerbounds also hold for static oracle data structures where no sensitivity isrequired. Finally, we give the first algorithm for the (1 +{\epsilon})-approximate radius, diameter, and eccentricity problems in directedor undirected unweighted graphs in case of single edges failures. The algorithmhas a truly subcubic running time for graphs with a truly subquadratic numberof edges; it is tight w.r.t. the conditional lower bounds we obtain.

Dynamic Parameterized Problems and Algorithms

  Fixed-parameter algorithms and kernelization are two powerful methods tosolve $\mathsf{NP}$-hard problems. Yet, so far those algorithms have beenlargely restricted to static inputs.  In this paper we provide fixed-parameter algorithms and kernelizations forfundamental $\mathsf{NP}$-hard problems with dynamic inputs. We consider avariety of parameterized graph and hitting set problems which are known to have$f(k)n^{1+o(1)}$ time algorithms on inputs of size $n$, and we consider thequestion of whether there is a data structure that supports small updates (suchas edge/vertex/set/element insertions and deletions) with an update time of$g(k)n^{o(1)}$; such an update time would be essentially optimal. Update andquery times independent of $n$ are particularly desirable. Among many otherresults, we show that Feedback Vertex Set and $k$-Path admit dynamic algorithmswith $f(k)\log^{O(1)}n$ update and query times for some function $f$ dependingon the solution size $k$ only.  We complement our positive results by several conditional and unconditionallower bounds. For example, we show that unlike their undirected counterparts,Directed Feedback Vertex Set and Directed $k$-Path do not admit dynamicalgorithms with $n^{o(1)}$ update and query times even for constant solutionsizes $k\leq 3$, assuming popular hardness hypotheses. We also show thatunconditionally, in the cell probe model, Directed Feedback Vertex Set cannotbe solved with update time that is purely a function of $k$.

Truly Sub-cubic Algorithms for Language Edit Distance and RNA Folding  via Fast Bounded-Difference Min-Plus Product

  It is a major open problem whether the $(\min,+)$-product of two $n\times n$matrices has a truly sub-cubic (i.e. $O(n^{3-\epsilon})$ for $\epsilon>0$) timealgorithm, in particular since it is equivalent to the famousAll-Pairs-Shortest-Paths problem (APSP) in $n$-vertex graphs. Some restrictionsof the $(\min,+)$-product to special types of matrices are known to admit trulysub-cubic algorithms, each giving rise to a special case of APSP that can besolved faster. In this paper we consider a new, different and powerfulrestriction in which all matrix entries are integers and one matrix can bearbitrary, as long as the other matrix has "bounded differences" in either itscolumns or rows, i.e. any two consecutive entries differ by only a smallamount. We obtain the first truly sub-cubic algorithm for thisbounded-difference $(\min,+)$-product (answering an open problem of Chan andLewenstein).  Our new algorithm, combined with a strengthening of an approach of L.~Valiantfor solving context-free grammar parsing with matrix multiplication, yields thefirst truly sub-cubic algorithms for the following problems: Language EditDistance (a major problem in the parsing community), RNA-folding (a majorproblem in bioinformatics) and Optimum Stack Generation (answering an openproblem of Tarjan).

Further limitations of the known approaches for matrix multiplication

  We consider the techniques behind the current best algorithms for matrixmultiplication. Our results are threefold.  (1) We provide a unifying framework, showing that all known matrixmultiplication running times since 1986 can be achieved from a single verynatural tensor - the structural tensor $T_q$ of addition modulo an integer $q$.  (2) We show that if one applies a generalization of the known techniques(arbitrary zeroing out of tensor powers to obtain independent matrix productsin order to use the asymptotic sum inequality of Sch\"{o}nhage) to an arbitrarymonomial degeneration of $T_q$, then there is an explicit lower bound,depending on $q$, on the bound on the matrix multiplication exponent $\omega$that one can achieve. We also show upper bounds on the value $\alpha$ that onecan achieve, where $\alpha$ is such that $n\times n^\alpha \times n$ matrixmultiplication can be computed in $n^{2+o(1)}$ time.  (3) We show that our lower bound on $\omega$ approaches $2$ as $q$ goes toinfinity. This suggests a promising approach to improving the bound on$\omega$: for variable $q$, find a monomial degeneration of $T_q$ which, usingthe known techniques, produces an upper bound on $\omega$ as a function of $q$.Then, take $q$ to infinity. It is not ruled out, and hence possible, that onecan obtain $\omega=2$ in this way.

Limits on All Known (and Some Unknown) Approaches to Matrix  Multiplication

  We study the known techniques for designing Matrix Multiplication algorithms.The two main approaches are the Laser method of Strassen, and the Grouptheoretic approach of Cohn and Umans. We define a generalization based onzeroing outs which subsumes these two approaches, which we call the Solarmethod, and an even more general method based on monomial degenerations, whichwe call the Galactic method.  We then design a suite of techniques for proving lower bounds on the value of$\omega$, the exponent of matrix multiplication, which can be achieved byalgorithms using many tensors $T$ and the Galactic method. Some of ourtechniques exploit `local' properties of $T$, like finding a sub-tensor of $T$which is so `weak' that $T$ itself couldn't be used to achieve a good bound on$\omega$, while others exploit `global' properties, like $T$ being a monomialdegeneration of the structural tensor of a group algebra.  Our main result is that there is a universal constant $\ell>2$ such that alarge class of tensors generalizing the Coppersmith-Winograd tensor $CW_q$cannot be used within the Galactic method to show a bound on $\omega$ betterthan $\ell$, for any $q$. We give evidence that previous lower-boundingtechniques were not strong enough to show this. We also prove a number ofcomplementary results along the way, including that for any group $G$, thestructural tensor of $\mathbb{C}[G]$ can be used to recover the best bound on$\omega$ which the Coppersmith-Winograd approach gets using $CW_{|G|-2}$ aslong as the asymptotic rank of the structural tensor is not too large.

Algorithms and Hardness for Diameter in Dynamic Graphs

  The diameter, radius and eccentricities are natural graph parameters. Whilethese problems have been studied extensively, there are no known dynamicalgorithms for them beyond the ones that follow from trivial recomputationafter each update or from solving dynamic All-Pairs Shortest Paths (APSP),which is very computationally intensive. This is the situation for dynamicapproximation algorithms as well, and even if only edge insertions or edgedeletions need to be supported.  This paper provides a comprehensive study of the dynamic approximation ofDiameter, Radius and Eccentricities, providing both conditional lower bounds,and new algorithms whose bounds are optimal under popular hypotheses infine-grained complexity. Some of the highlights include:  - Under popular hardness hypotheses, there can be no significantly betterfully dynamic approximation algorithms than recomputing the answer after eachupdate, or maintaining full APSP.  - Nearly optimal partially dynamic (incremental/decremental) algorithms canbe achieved via efficient reductions to (incremental/decremental) maintenanceof Single-Source Shortest Paths. For instance, a nearly$(3/2+\epsilon)$-approximation to Diameter in directed or undirected graphs canbe maintained decrementally in total time $m^{1+o(1)}\sqrt{n}/\epsilon^2$. Thisnearly matches the static $3/2$-approximation algorithm for the problem that isknown to be conditionally optimal.

Simulating Branching Programs with Edit Distance and Friends or: A  Polylog Shaved is a Lower Bound Made

  A recent and active line of work achieves tight lower bounds for fundamentalproblems under the Strong Exponential Time Hypothesis (SETH). A celebratedresult of Backurs and Indyk (STOC'15) proves that the Edit Distance of twosequences of length n cannot be computed in strongly subquadratic time underSETH. The result was extended by follow-up works to simpler looking problemslike finding the Longest Common Subsequence (LCS).  SETH is a very strong assumption, asserting that even linear size CNFformulas cannot be analyzed for satisfiability with an exponential speedup overexhaustive search. We consider much safer assumptions, e.g. that such a speedupis impossible for SAT on much more expressive representations, like NCcircuits. Intuitively, this seems much more plausible: NC circuits canimplement complex cryptographic primitives, while CNFs cannot evenapproximately compute an XOR of bits.  Our main result is a surprising reduction from SAT on Branching Programs tofundamental problems in P like Edit Distance, LCS, and many others. Trulysubquadratic algorithms for these problems therefore have consequences that weconsider to be far more remarkable than merely faster CNF SAT algorithms. Forexample, SAT on arbitrary o(n)-depth bounded fan-in circuits (and thereforealso NC-Circuit-SAT) can be solved in (2-eps)^n time.  A very interesting feature of our work is that we can prove majorconsequences even from mildly subquadratic algorithms for Edit Distance or LCS.For example, we show that if we can shave an arbitrarily large polylog factorfrom n^2 for Edit Distance then NEXP does not have non-uniform NC^1 circuits. Amore fine-grained examination shows that even shaving a $\log^c{n}$ factor, fora specific constant $c \approx 10^3$, already implies new circuit lower bounds.

Approximating the diameter of a graph

  In this paper we consider the fundamental problem of approximating thediameter $D$ of directed or undirected graphs. In a seminal paper, Aingworth,Chekuri, Indyk and Motwani [SIAM J. Comput. 1999] presented an algorithm thatcomputes in $\Ot(m\sqrt n + n^2)$ time an estimate $\hat{D}$ for the diameterof an $n$-node, $m$-edge graph, such that $\lfloor 2/3 D \rfloor \leq \hat{D}\leq D$. In this paper we present an algorithm that produces the same estimatein $\Ot(m\sqrt n)$ expected running time. We then provide strong evidence thata better approximation may be hard to obtain if we insist on an $O(m^{2-\eps})$running time. In particular, we show that if there is some constant $\eps>0$ sothat there is an algorithm for undirected unweighted graphs that runs in$O(m^{2-\eps})$ time and produces an approximation $\hat{D}$ such that $(2/3+\eps) D \leq \hat{D} \leq D$, then SAT for CNF formulas on $n$ variablescan be solved in $O^{*}((2-\delta)^{n})$ time for some constant $\delta>0$, andthe strong exponential time hypothesis of [Impagliazzo, Paturi, Zane JCSS'01]is false.  Motivated by this somewhat negative result, we study whether it is possibleto obtain a better approximation for specific cases. For unweighted directed orundirected graphs, we show that if $D=3h+z$, where $h\geq 0$ and $z\in{0,1,2}$, then it is possible to report in $\tilde{O}(\min{m^{2/3}n^{4/3},m^{2-1/(2h+3)}})$ time an estimate $\hat{D}$ such that $2h+z \leq\hat{D}\leq D$, thus giving a better than 3/2 approximation whenever $z\neq 0$.This is significant for constant values of $D$ which is exactly when thediameter approximation problem is hardest to solve. For the case of unweightedundirected graphs we present an $\tilde{O}(m^{2/3} n^{4/3})$ time algorithmthat reports an estimate $\hat{D}$ such that $\lfloor 4D/5\rfloor \leq\hat{D}\leq D$.

If the Current Clique Algorithms are Optimal, so is Valiant's Parser

  The CFG recognition problem is: given a context-free grammar $\mathcal{G}$and a string $w$ of length $n$, decide if $w$ can be obtained from$\mathcal{G}$. This is the most basic parsing question and is a core computerscience problem. Valiant's parser from 1975 solves the problem in$O(n^{\omega})$ time, where $\omega<2.373$ is the matrix multiplicationexponent. Dozens of parsing algorithms have been proposed over the years, yetValiant's upper bound remains unbeaten. The best combinatorial algorithms havemildly subcubic $O(n^3/\log^3{n})$ complexity.  Lee (JACM'01) provided evidence that fast matrix multiplication is needed forCFG parsing, and that very efficient and practical algorithms might be hard oreven impossible to obtain. Lee showed that any algorithm for a more generalparsing problem with running time $O(|\mathcal{G}|\cdot n^{3-\varepsilon})$ canbe converted into a surprising subcubic algorithm for Boolean MatrixMultiplication. Unfortunately, Lee's hardness result required that the grammarsize be $|\mathcal{G}|=\Omega(n^6)$. Nothing was known for the more relevantcase of constant size grammars.  In this work, we prove that any improvement on Valiant's algorithm, even forconstant size grammars, either in terms of runtime or by avoiding theinefficiencies of fast matrix multiplication, would imply a breakthroughalgorithm for the $k$-Clique problem: given a graph on $n$ nodes, decide ifthere are $k$ that form a clique.  Besides classifying the complexity of a fundamental problem, our reductionhas led us to similar lower bounds for more modern and well-studied cubic timeproblems for which faster algorithms are highly desirable in practice: RNAFolding, a central problem in computational biology, and Dyck Language EditDistance, answering an open question of Saha (FOCS'14).

Better Distance Preservers and Additive Spanners

  We make improvements to the upper bounds on several popular types of distancepreserving graph sketches. These sketches are all various restrictions of the{\em additive pairwise spanner} problem, in which one is given an undirectedunweighted graph $G$, a set of node pairs $P$, and an error allowance $+\beta$,and one must construct a sparse subgraph $H$ satisfying $\delta_H(u, v) \le\delta_G(u, v) + \beta$ for all $(u, v) \in P$.  The first part of our paper concerns {\em pairwise distance preservers},which make the restriction $\beta=0$ (i.e. distances must be preserved {\emexactly}). Our main result here is an upper bound of $|H| = O(n^{2/3}|P|^{2/3}+ n|P|^{1/3})$ when $G$ is undirected and unweighted. This improves on existingbounds whenever $|P| = \omega(n^{3/4})$, and it is the first such improvementin the last ten years.  We then devise a new application of distance preservers to graph clusteringalgorithms, and we apply this algorithm to {\em subset spanners}, which require$P = S \times S$ for some node subset $S$, and {\em (standard) spanners}, whichrequire $P = V \times V$. For both of these objects, our constructiongeneralizes the best known bounds when the error allowance is constant, and weobtain the strongest polynomial error/sparsity tradeoff that has yet beenreported (in fact, for subset spanners, ours is the {\em first} nontrivialconstruction that enjoys improved sparsity from a polynomial error allowance).  We leave open a conjecture that $O(n^{2/3}|P|^{2/3} + n)$ pairwise distancepreservers are possible for undirected unweighted graphs. Resolving thisconjecture in the affirmative would improve and simplify our upper bounds forall the graph sketches mentioned above.

Approximation and Fixed Parameter Subquadratic Algorithms for Radius and  Diameter

  The radius and diameter are fundamental graph parameters. They are defined asthe minimum and maximum of the eccentricities in a graph, respectively, wherethe eccentricity of a vertex is the largest distance from the vertex to anothernode. In directed graphs, there are several versions of these problems. Forinstance, one may choose to define the eccentricity of a node in terms of thelargest distance into the node, out of the node, the sum of the two directions(i.e. roundtrip) and so on. All versions of diameter and radius can be solvedvia solving all-pairs shortest paths (APSP), followed by a fast postprocessingstep. Solving APSP, however, on $n$-node graphs requires $\Omega(n^2)$ timeeven in sparse graphs, as one needs to output $n^2$ distances.  Motivated by known and new negative results on the impossibility of computingthese measures exactly in general graphs in truly subquadratic time, underplausible assumptions, we search for \emph{approximation} and \emph{fixedparameter subquadratic} algorithms, and for reasons why they do not exist.  Our results include: - Truly subquadratic approximation algorithms for mostof the versions of Diameter and Radius with \emph{optimal} approximationguarantees (given truly subquadratic time), under plausible assumptions. Inparticular, there is a $2$-approximation algorithm for directed Radius withone-way distances that runs in $\tilde{O}(m\sqrt{n})$ time, while a$(2-\delta)$-approximation algorithm in $O(n^{2-\epsilon})$ time is unlikely. -On graphs with treewidth $k$, we can solve the problems in$2^{O(k\log{k})}n^{1+o(1)}$ time. We show that these algorithms are nearoptimal since even a $(3/2-\delta)$-approximation algorithm that runs in time$2^{o(k)}n^{2-\epsilon}$ would refute the plausible assumptions.

Preserving Distances in Very Faulty Graphs

  Preservers and additive spanners are sparse (hence cheap to store) subgraphsthat preserve the distances between given pairs of nodes exactly or with somesmall additive error, respectively. Since real-world networks are prone tofailures, it makes sense to study fault-tolerant versions of the abovestructures. This turns out to be a surprisingly difficult task. For every smallbut arbitrary set of edge or vertex failures, the preservers and spanners needto contain {\em replacement paths} around the faulted set. In this paper wemake substantial progress on fault tolerant preservers and additive spanners:  (1) We present the first truly sub-quadratic size single-pair preservers inunweighted (possibly directed) graphs for \emph{any} fixed number $f$ offaults. Our result indeed generalizes to the single-source case, and can beused to build new fault-tolerant additive spanners (for all pairs).  (2) The size of the above single-pair preservers is $O(n^{2-g(f)})$ for somepositive function $g$, and grows to $O(n^2)$ for increasing $f$. We show thatthis is necessary even in undirected unweighted graphs, and even if you allowfor a small additive error: If you aim at size $O(n^{2-\epsilon})$ for$\epsilon>0$, then the additive error has to be $\Omega(\eps f)$. Thissurprisingly matches known upper bounds in the literature.  (3) For weighted graphs, we provide matching upper and lower bounds for thesingle pair case. Namely, the size of the preserver is $\Theta(n^2)$ for $f\geq2$ in both directed and undirected graphs, while for $f=1$ the size is$\Theta(n)$ in undirected graphs. For directed graphs, we have a superlinearupper bound and a matching lower bound.  Most of our lower bounds extend to the distance oracle setting, where ratherthan a subgraph we ask for any compact data structure.

Optimal Vertex Fault Tolerant Spanners (for fixed stretch)

  A $k$-spanner of a graph $G$ is a sparse subgraph $H$ whose shortest pathdistances match those of $G$ up to a multiplicative error $k$. In this paper westudy spanners that are resistant to faults. A subgraph $H \subseteq G$ is an$f$ vertex fault tolerant (VFT) $k$-spanner if $H \setminus F$ is a $k$-spannerof $G \setminus F$ for any small set $F$ of $f$ vertices that might "fail." Oneof the main questions in the area is: what is the minimum size of an $f$ faulttolerant $k$-spanner that holds for all $n$ node graphs (as a function of $f$,$k$ and $n$)? This question was first studied in the context of geometricgraphs [Levcopoulos et al. STOC '98, Czumaj and Zhao SoCG '03] and has morerecently been considered in general undirected graphs [Chechik et al. STOC '09,Dinitz and Krauthgamer PODC '11].  In this paper, we settle the question of the optimal size of a VFT spanner,in the setting where the stretch factor $k$ is fixed. Specifically, we provethat every (undirected, possibly weighted) $n$-node graph $G$ has a$(2k-1)$-spanner resilient to $f$ vertex faults with $O_k(f^{1 - 1/k} n^{1 +1/k})$ edges, and this is fully optimal (unless the famous Erdos GirthConjecture is false). Our lower bound even generalizes to imply that no datastructure capable of approximating $dist_{G \setminus F}(s, t)$ similarly canbeat the space usage of our spanner in the worst case. We also consider theedge fault tolerant (EFT) model, defined analogously with edge failures ratherthan vertex failures. We show that the same spanner upper bound applies in thissetting. Our data structure lower bound extends to the case $k=2$ (and hence weclose the EFT problem for $3$-approximations), but it falls to $\Omega(f^{1/2 -1/(2k)} \cdot n^{1 + 1/k})$ for $k \ge 3$. We leave it as an open problem toclose this gap.

Fine-Grained I/O Complexity via Reductions: New lower bounds, faster  algorithms, and a time hierarchy

  This paper initiates the study of I/O algorithms (minimizing cache misses)from the perspective of fine-grained complexity (conditional polynomial lowerbounds). Specifically, we aim to answer why sparse graph problems are so hard,and why the Longest Common Subsequence problem gets a savings of a factor ofthe size of cache times the length of a cache line, but no more. We take thereductions and techniques from complexity and fine-grained complexity and applythem to the I/O model to generate new (conditional) lower bounds as well asfaster algorithms. We also prove the existence of a time hierarchy for the I/Omodel, which motivates the fine-grained reductions.  Using fine-grained reductions, we give an algorithm for distinguishing 2 vs.3 diameter and radius that runs in $O(|E|^2/(MB))$ cache misses, which forsparse graphs improves over the previous $O(|V|^2/B)$ running time. We give newreductions from radius and diameter to Wiener index and median. We showmeaningful reductions between problems that have linear-time solutions in theRAM model. The reductions use low I/O complexity (typically $O(n/B)$), and thushelp to finely capture the relationship between "I/O linear time" $\Theta(n/B)$and RAM linear time $\Theta(n)$. We generate new I/O assumptions based on thedifficulty of improving sparse graph problem running times in the I/O model. Wecreate conjectures that the current best known algorithms for Single SourceShortest Paths (SSSP), diameter, and radius are optimal. From these I/O-modelassumptions, we show that many of the known reductions in the word-RAM modelcan naturally extend to hold in the I/O model as well (e.g., a lower bound onthe I/O complexity of Longest Common Subsequence that matches the best knownrunning time). Finally, we prove an analog of the Time Hierarchy Theorem in theI/O model.

Nearly Optimal Separation Between Partially And Fully Retroactive Data  Structures

  Since the introduction of retroactive data structures at SODA 2004, a majorunsolved problem has been to bound the gap between the best partiallyretroactive data structure (where changes can be made to the past, but only thepresent can be queried) and the best fully retroactive data structure (wherethe past can also be queried) for any problem. It was proved in 2004 that anypartially retroactive data structure with operation time $T(n,m)$ can betransformed into a fully retroactive data structure with operation time$O(\sqrt{m} \cdot T(n,m))$, where $n$ is the size of the data structure and $m$is the number of operations in the timeline [Demaine 2004], but it has beenopen for 14 years whether such a gap is necessary.  In this paper, we prove nearly matching upper and lower bounds on this gapfor all $n$ and $m$. We improve the upper bound for $n \ll \sqrt m$ by showinga new transformation with multiplicative overhead $n \log m$. We then prove alower bound of $\min\{n \log m, \sqrt m\}^{1-o(1)}$ assuming any of thefollowing conjectures:  - Conjecture I: Circuit SAT requires $2^{n - o(n)}$ time on $n$-inputcircuits of size $2^{o(n)}$. (Far weaker than the well-believed SETHconjecture, which asserts that CNF SAT with $n$ variables and $O(n)$ clausesalready requires $2^{n-o(n)}$ time.)  - Conjecture II: Online $(\min,+)$ product between an integer $n\times n$matrix and $n$ vectors requires $n^{3 - o(1)}$ time.  - Conjecture III (3-SUM Conjecture): Given three sets $A,B,C$ of integers,each of size $n$, deciding whether there exist $a \in A, b \in B, c \in C$ suchthat $a + b + c = 0$ requires $n^{2 - o(1)}$ time.  Our lower bound construction illustrates an interesting power of fullyretroactive queries: they can be used to quickly solve batched pair evaluation.We believe this technique can prove useful for other data structure lowerbounds, especially dynamic ones.

