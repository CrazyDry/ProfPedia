Minimum Weight Cycles and Triangles: Equivalences and Algorithms

  We consider the fundamental algorithmic problem of finding a cycle of minimum
weight in a weighted graph. In particular, we show that the minimum weight
cycle problem in an undirected n-node graph with edge weights in {1,...,M} or
in a directed n-node graph with edge weights in {-M,..., M} and no negative
cycles can be efficiently reduced to finding a minimum weight triangle in an
Theta(n)-node undirected graph with weights in {1,...,O(M)}. Roughly speaking,
our reductions imply the following surprising phenomenon: a minimum cycle with
an arbitrary number of weighted edges can be "encoded" using only three edges
within roughly the same weight interval! This resolves a longstanding open
problem posed by Itai and Rodeh [SIAM J. Computing 1978 and STOC'77].
  A direct consequence of our efficient reductions are O (Mn^{omega})-time
algorithms using fast matrix multiplication (FMM) for finding a minimum weight
cycle in both undirected graphs with integral weights from the interval [1,M]
and directed graphs with integral weights from the interval [-M,M]. The latter
seems to reveal a strong separation between the all pairs shortest paths (APSP)
problem and the minimum weight cycle problem in directed graphs as the fastest
known APSP algorithm has a running time of O(M^{0.681}n^{2.575}) by Zwick [J.
ACM 2002].
  In contrast, when only combinatorial algorithms are allowed (that is, without
FMM) the only known solution to minimum weight cycle is by computing APSP.
Interestingly, any separation between the two problems in this case would be an
amazing breakthrough as by a recent paper by Vassilevska W. and Williams
[FOCS'10], any O(n^{3-eps})-time algorithm (eps>0) for minimum weight cycle
immediately implies a O(n^{3-delta})-time algorithm (delta>0) for APSP.


Faster Replacement Paths

  The replacement paths problem for directed graphs is to find for given nodes
s and t and every edge e on the shortest path between them, the shortest path
between s and t which avoids e. For unweighted directed graphs on n vertices,
the best known algorithm runtime was \tilde{O}(n^{2.5}) by Roditty and Zwick.
For graphs with integer weights in {-M,...,M}, Weimann and Yuster recently
showed that one can use fast matrix multiplication and solve the problem in
O(Mn^{2.584}) time, a runtime which would be O(Mn^{2.33}) if the exponent
\omega of matrix multiplication is 2.
  We improve both of these algorithms. Our new algorithm also relies on fast
matrix multiplication and runs in O(M n^{\omega} polylog(n)) time if \omega>2
and O(n^{2+\eps}) for any \eps>0 if \omega=2. Our result shows that, at least
for small integer weights, the replacement paths problem in directed graphs may
be easier than the related all pairs shortest paths problem in directed graphs,
as the current best runtime for the latter is \Omega(n^{2.5}) time even if
\omega=2.


Finding heaviest H-subgraphs in real weighted graphs, with applications

  For a graph G with real weights assigned to the vertices (edges), the MAX
H-SUBGRAPH problem is to find an H-subgraph of G with maximum total weight, if
one exists. The all-pairs MAX H-SUBGRAPH problem is to find for every pair of
vertices u,v, a maximum H-subgraph containing both u and v, if one exists. Our
main results are new strongly polynomial algorithms for the all-pairs MAX
H-SUBGRAPH problem for vertex weighted graphs. We also give improved algorithms
for the MAX-H SUBGRAPH problem for edge weighted graphs, and various related
problems, including computing the first k most significant bits of the distance
product of two matrices. Some of our algorithms are based, in part, on fast
matrix multiplication.


A 7/3-Approximation for Feedback Vertex Sets in Tournaments

  We consider the minimum-weight feedback vertex set problem in tournaments:
given a tournament with non-negative vertex weights, remove a minimum-weight
set of vertices that intersects all cycles. This problem is $\mathsf{NP}$-hard
to solve exactly, and Unique Games-hard to approximate by a factor better than
2. We present the first $7/3$ approximation algorithm for this problem,
improving on the previously best known ratio $5/2$ given by Cai et al. [FOCS
1998, SICOMP 2001].


Approximating Cycles in Directed Graphs: Fast Algorithms for Girth and
  Roundtrip Spanners

  The girth of a graph, i.e. the length of its shortest cycle, is a fundamental
graph parameter. Unfortunately all known algorithms for computing, even
approximately, the girth and girth-related structures in directed weighted
$m$-edge and $n$-node graphs require $\Omega(\min\{n^{\omega}, mn\})$ time (for
$2\leq\omega<2.373$). In this paper, we drastically improve these runtimes as
follows:
  * Multiplicative Approximations in Nearly Linear Time: We give an algorithm
that in $\widetilde{O}(m)$ time computes an $\widetilde{O}(1)$-multiplicative
approximation of the girth as well as an $\widetilde{O}(1)$-multiplicative
roundtrip spanner with $\widetilde{O}(n)$ edges with high probability (w.h.p).
  * Nearly Tight Additive Approximations: For unweighted graphs and any $\alpha
\in (0,1)$ we give an algorithm that in $\widetilde{O}(mn^{1 - \alpha})$ time
computes an $O(n^\alpha)$-additive approximation of the girth w.h.p, and
partially derandomize it. We show that the runtime of our algorithm cannot be
significantly improved without a breakthrough in combinatorial Boolean matrix
multiplication.
  Our main technical contribution to achieve these results is the first nearly
linear time algorithm for computing roundtrip covers, a directed graph
decomposition concept key to previous roundtrip spanner constructions.
Previously it was not known how to compute these significantly faster than
$\Omega(\min\{n^\omega, mn\})$ time. Given the traditional difficulty in
efficiently processing directed graphs, we hope our techniques may find further
applications.


Graph pattern detection: Hardness for all induced patterns and faster
  non-induced cycles

  We consider the pattern detection problem in graphs: given a constant size
pattern graph $H$ and a host graph $G$, determine whether $G$ contains a
subgraph isomorphic to $H$. Our main results are:
  * We prove that if a pattern $H$ contains a $k$-clique subgraph, then
detecting whether an $n$ node host graph contains a not necessarily induced
copy of $H$ requires at least the time for detecting whether an $n$ node graph
contains a $k$-clique. The previous result of this nature required that $H$
contains a $k$-clique which is disjoint from all other $k$-cliques of $H$.
  * We show that if the famous Hadwiger conjecture from graph theory is true,
then detecting whether an $n$ node host graph contains a not necessarily
induced copy of a pattern with chromatic number $t$ requires at least the time
for detecting whether an $n$ node graph contains a $t$-clique. This implies
that: (1) under Hadwiger's conjecture for every $k$-node pattern $H$, finding
an induced copy of $H$ requires at least the time of $\sqrt k$-clique
detection, and at least size $\omega(n^{\sqrt{k}/4})$ for any constant depth
circuit, and (2) unconditionally, detecting an induced copy of a random
$G(k,p)$ pattern w.h.p. requires at least the time of $\Theta(k/\log k)$-clique
detection, and hence also at least size $n^{\Omega(k/\log k)}$ for circuits of
constant depth.
  * Finally, we consider the case when the pattern is a directed cycle on $k$
nodes, and we would like to detect whether a directed $m$-edge graph $G$
contains a $k$-Cycle as a not necessarily induced subgraph. We resolve a 14
year old conjecture of [Yuster-Zwick SODA'04] on the complexity of $k$-Cycle
detection by giving a tight analysis of their $k$-Cycle algorithm. Our analysis
improves the best bounds for $k$-Cycle detection in directed graphs, for all
$k>5$.


Quantum algorithms for shortest paths problems in structured instances

  We consider the quantum time complexity of the all pairs shortest paths
(APSP) problem and some of its variants. The trivial classical algorithm for
APSP and most all pairs path problems runs in $O(n^3)$ time, while the trivial
algorithm in the quantum setting runs in $\tilde{O}(n^{2.5})$ time, using
Grover search. A major open problem in classical algorithms is to obtain a
truly subcubic time algorithm for APSP, i.e. an algorithm running in
$O(n^{3-\varepsilon})$ time for constant $\varepsilon>0$. To approach this
problem, many truly subcubic time classical algorithms have been devised for
APSP and its variants for structured inputs. Some examples of such problems are
APSP in geometrically weighted graphs, graphs with small integer edge weights
or a small number of weights incident to each vertex, and the all pairs
earliest arrivals problem. In this paper we revisit these problems in the
quantum setting and obtain the first nontrivial (i.e. $O(n^{2.5-\varepsilon})$
time) quantum algorithms for the problems.


Very Sparse Additive Spanners and Emulators

  We obtain new upper bounds on the additive distortion for graph emulators and
spanners on relatively few edges. We introduce a new subroutine called "strip
creation," and we combine this subroutine with several other ideas to obtain
the following results:
  \item Every graph has a spanner on $O(n^{1+\epsilon})$ edges with
$\tilde{O}(n^{1/2 - \epsilon/2})$ additive distortion, for arbitrary
$\epsilon\in [0,1]$. \item Every graph has an emulator on $\tilde{O}(n^{1 +
\epsilon})$ edges with $\tilde{O}(n^{1/3 - 2\epsilon/3})$ additive distortion
whenever $\epsilon \in [0, \frac{1}{5}]$. \item Every graph has a spanner on
$\tilde{O}(n^{1 + \epsilon})$ edges with $\tilde{O}(n^{2/3 - 5\epsilon/3})$
additive distortion whenever $\epsilon \in [0, \frac{1}{4}]$.
  Our first spanner has the new best known asymptotic edge-error tradeoff for
additive spanners whenever $\epsilon \in [0, \frac{1}{7}]$. Our second spanner
has the new best tradeoff whenever $\epsilon \in [\frac{1}{7}, \frac{3}{17}]$.
Our emulator has the new best asymptotic edge-error tradeoff whenever $\epsilon
\in [0, \frac{1}{5}]$.


Popular conjectures imply strong lower bounds for dynamic problems

  We consider several well-studied problems in dynamic algorithms and prove
that sufficient progress on any of them would imply a breakthrough on one of
five major open problems in the theory of algorithms:
  1. Is the 3SUM problem on $n$ numbers in $O(n^{2-\epsilon})$ time for some
$\epsilon>0$?
  2. Can one determine the satisfiability of a CNF formula on $n$ variables in
$O((2-\epsilon)^n poly n)$ time for some $\epsilon>0$?
  3. Is the All Pairs Shortest Paths problem for graphs on $n$ vertices in
$O(n^{3-\epsilon})$ time for some $\epsilon>0$?
  4. Is there a linear time algorithm that detects whether a given graph
contains a triangle?
  5. Is there an $O(n^{3-\epsilon})$ time combinatorial algorithm for $n\times
n$ Boolean matrix multiplication?
  The problems we consider include dynamic versions of bipartite perfect
matching, bipartite maximum weight matching, single source reachability, single
source shortest paths, strong connectivity, subgraph connectivity, diameter
approximation and some nongraph problems such as Pagh's problem defined in a
recent paper by Patrascu [STOC 2010].


Quadratic-Time Hardness of LCS and other Sequence Similarity Measures

  Two important similarity measures between sequences are the longest common
subsequence (LCS) and the dynamic time warping distance (DTWD). The
computations of these measures for two given sequences are central tasks in a
variety of applications. Simple dynamic programming algorithms solve these
tasks in $O(n^2)$ time, and despite an extensive amount of research, no
algorithms with significantly better worst case upper bounds are known.
  In this paper, we show that an $O(n^{2-\epsilon})$ time algorithm, for some
$\epsilon>0$, for computing the LCS or the DTWD of two sequences of length $n$
over a constant size alphabet, refutes the popular Strong Exponential Time
Hypothesis (SETH). Moreover, we show that computing the LCS of $k$ strings over
an alphabet of size $O(k)$ cannot be done in $O(n^{k-\epsilon})$ time, for any
$\epsilon>0$, under SETH. Finally, we also address the time complexity of
approximating the DTWD of two strings in truly subquadratic time.


Towards Tight Approximation Bounds for Graph Diameter and Eccentricities

  Among the most important graph parameters is the Diameter, the largest
distance between any two vertices. There are no known very efficient algorithms
for computing the Diameter exactly. Thus, much research has been devoted to how
fast this parameter can be approximated. Chechik et al. showed that the
diameter can be approximated within a multiplicative factor of $3/2$ in
$\tilde{O}(m^{3/2})$ time. Furthermore, Roditty and Vassilevska W. showed that
unless the Strong Exponential Time Hypothesis (SETH) fails, no
$O(n^{2-\epsilon})$ time algorithm can achieve an approximation factor better
than $3/2$ in sparse graphs. Thus the above algorithm is essentially optimal
for sparse graphs for approximation factors less than $3/2$. It was, however,
completely plausible that a $3/2$-approximation is possible in linear time. In
this work we conditionally rule out such a possibility by showing that unless
SETH fails no $O(m^{3/2-\epsilon})$ time algorithm can achieve an approximation
factor better than $5/3$.
  Another fundamental set of graph parameters are the Eccentricities. The
Eccentricity of a vertex $v$ is the distance between $v$ and the farthest
vertex from $v$. Chechik et al. showed that the Eccentricities of all vertices
can be approximated within a factor of $5/3$ in $\tilde{O}(m^{3/2})$ time and
Abboud et al. showed that no $O(n^{2-\epsilon})$ algorithm can achieve better
than $5/3$ approximation in sparse graphs. We show that the runtime of the
$5/3$ approximation algorithm is also optimal under SETH. We also show that no
near-linear time algorithm can achieve a better than $2$ approximation for the
Eccentricities and that this is essentially tight: we give an algorithm that
approximates Eccentricities within a $2+\delta$ factor in $\tilde{O}(m/\delta)$
time for any $0<\delta<1$. This beats all Eccentricity algorithms in Cairo et
al.


Deterministic Time-Space Tradeoffs for k-SUM

  Given a set of numbers, the $k$-SUM problem asks for a subset of $k$ numbers
that sums to zero. When the numbers are integers, the time and space complexity
of $k$-SUM is generally studied in the word-RAM model; when the numbers are
reals, the complexity is studied in the real-RAM model, and space is measured
by the number of reals held in memory at any point.
  We present a time and space efficient deterministic self-reduction for the
$k$-SUM problem which holds for both models, and has many interesting
consequences. To illustrate:
  * $3$-SUM is in deterministic time $O(n^2 \lg\lg(n)/\lg(n))$ and space
$O\left(\sqrt{\frac{n \lg(n)}{\lg\lg(n)}}\right)$. In general, any
polylogarithmic-time improvement over quadratic time for $3$-SUM can be
converted into an algorithm with an identical time improvement but low space
complexity as well. * $3$-SUM is in deterministic time $O(n^2)$ and space
$O(\sqrt n)$, derandomizing an algorithm of Wang.
  * A popular conjecture states that 3-SUM requires $n^{2-o(1)}$ time on the
word-RAM. We show that the 3-SUM Conjecture is in fact equivalent to the
(seemingly weaker) conjecture that every $O(n^{.51})$-space algorithm for
$3$-SUM requires at least $n^{2-o(1)}$ time on the word-RAM.
  * For $k \ge 4$, $k$-SUM is in deterministic $O(n^{k - 2 + 2/k})$ time and
$O(\sqrt{n})$ space.


Tight Hardness for Shortest Cycles and Paths in Sparse Graphs

  Fine-grained reductions have established equivalences between many core
problems with $\tilde{O}(n^3)$-time algorithms on $n$-node weighted graphs,
such as Shortest Cycle, All-Pairs Shortest Paths (APSP), Radius, Replacement
Paths, Second Shortest Paths, and so on. These problems also have
$\tilde{O}(mn)$-time algorithms on $m$-edge $n$-node weighted graphs, and such
algorithms have wider applicability. Are these $mn$ bounds optimal when $m \ll
n^2$?
  Starting from the hypothesis that the minimum weight $(2\ell+1)$-Clique
problem in edge weighted graphs requires $n^{2\ell+1-o(1)}$ time, we prove that
for all sparsities of the form $m = \Theta(n^{1+1/\ell})$, there is no $O(n^2 +
mn^{1-\epsilon})$ time algorithm for $\epsilon>0$ for \emph{any} of the below
problems:
  Minimum Weight $(2\ell+1)$-Cycle in a directed weighted graph,
  Shortest Cycle in a directed weighted graph,
  APSP in a directed or undirected weighted graph,
  Radius (or Eccentricities) in a directed or undirected weighted graph,
  Wiener index of a directed or undirected weighted graph,
  Replacement Paths in a directed weighted graph,
  Second Shortest Path in a directed weighted graph,
  Betweenness Centrality of a given node in a directed weighted graph.
  That is, we prove hardness for a variety of sparse graph problems from the
hardness of a dense graph problem. Our results also lead to new conditional
lower bounds from several related hypothesis for unweighted sparse graph
problems including $k$-cycle, shortest cycle, Radius, Wiener index and APSP.


Who Can Win a Single-Elimination Tournament?

  A single-elimination (SE) tournament is a popular way to select a winner in
both sports competitions and in elections. A natural and well-studied question
is the tournament fixing problem (TFP): given the set of all pairwise match
outcomes, can a tournament organizer rig an SE tournament by adjusting the
initial seeding so that their favorite player wins? We prove new sufficient
conditions on the pairwise match outcome information and the favorite player,
under which there is guaranteed to be a seeding where the player wins the
tournament. Our results greatly generalize previous results. We also
investigate the relationship between the set of players that can win an SE
tournament under some seeding (so called SE winners) and other traditional
tournament solutions. In addition, we generalize and strengthen prior work on
probabilistic models for generating tournaments. For instance, we show that
\emph{every} player in an $n$ player tournament generated by the Condorcet
Random Model will be an SE winner even when the noise is as small as possible,
$p=\Theta(\ln n/n)$; prior work only had such results for $p\geq
\Omega(\sqrt{\ln n/n})$. We also establish new results for significantly more
general generative models.


Subtree Isomorphism Revisited

  The Subtree Isomorphism problem asks whether a given tree is contained in
another given tree. The problem is of fundamental importance and has been
studied since the 1960s. For some variants, e.g., ordered trees, near-linear
time algorithms are known, but for the general case truly subquadratic
algorithms remain elusive.
  Our first result is a reduction from the Orthogonal Vectors problem to
Subtree Isomorphism, showing that a truly subquadratic algorithm for the latter
refutes the Strong Exponential Time Hypothesis (SETH).
  In light of this conditional lower bound, we focus on natural special cases
for which no truly subquadratic algorithms are known. We classify these cases
against the quadratic barrier, showing in particular that:
  -- Even for binary, rooted trees, a truly subquadratic algorithm refutes
SETH.
  -- Even for rooted trees of depth $O(\log\log{n})$, where $n$ is the total
number of vertices, a truly subquadratic algorithm refutes SETH.
  -- For every constant $d$, there is a constant $\epsilon_d>0$ and a
randomized, truly subquadratic algorithm for degree-$d$ rooted trees of depth
at most $(1+ \epsilon_d) \log_{d}{n}$. In particular, there is an $O(\min\{
2.85^h ,n^2 \})$ algorithm for binary trees of depth $h$.
  Our reductions utilize new "tree gadgets" that are likely useful for future
SETH-based lower bounds for problems on trees. Our upper bounds apply a
folklore result from randomized decision tree complexity.


Conditional Hardness for Sensitivity Problems

  In recent years it has become popular to study dynamic problems in a
sensitivity setting: Instead of allowing for an arbitrary sequence of updates,
the sensitivity model only allows to apply batch updates of small size to the
original input data. The sensitivity model is particularly appealing since
recent strong conditional lower bounds ruled out fast algorithms for many
dynamic problems, such as shortest paths, reachability, or subgraph
connectivity.
  In this paper we prove conditional lower bounds for sensitivity problems. For
example, we show that under the Boolean Matrix Multiplication (BMM) conjecture
combinatorial algorithms cannot compute the (4/3 - {\epsilon})-approximate
diameter of an undirected unweighted dense graph with truly subcubic
preprocessing time and truly subquadratic update/query time. This result is
surprising since in the static setting it is not clear whether a reduction from
BMM to diameter is possible. We further show under the BMM conjecture that many
problems, such as reachability or approximate shortest paths, cannot be solved
faster than by recomputation from scratch even after only one or two edge
insertions. We give more lower bounds under the Strong Exponential Time
Hypothesis and the All Pairs Shortest Paths Conjecture. Many of our lower
bounds also hold for static oracle data structures where no sensitivity is
required. Finally, we give the first algorithm for the (1 +
{\epsilon})-approximate radius, diameter, and eccentricity problems in directed
or undirected unweighted graphs in case of single edges failures. The algorithm
has a truly subcubic running time for graphs with a truly subquadratic number
of edges; it is tight w.r.t. the conditional lower bounds we obtain.


Dynamic Parameterized Problems and Algorithms

  Fixed-parameter algorithms and kernelization are two powerful methods to
solve $\mathsf{NP}$-hard problems. Yet, so far those algorithms have been
largely restricted to static inputs.
  In this paper we provide fixed-parameter algorithms and kernelizations for
fundamental $\mathsf{NP}$-hard problems with dynamic inputs. We consider a
variety of parameterized graph and hitting set problems which are known to have
$f(k)n^{1+o(1)}$ time algorithms on inputs of size $n$, and we consider the
question of whether there is a data structure that supports small updates (such
as edge/vertex/set/element insertions and deletions) with an update time of
$g(k)n^{o(1)}$; such an update time would be essentially optimal. Update and
query times independent of $n$ are particularly desirable. Among many other
results, we show that Feedback Vertex Set and $k$-Path admit dynamic algorithms
with $f(k)\log^{O(1)}n$ update and query times for some function $f$ depending
on the solution size $k$ only.
  We complement our positive results by several conditional and unconditional
lower bounds. For example, we show that unlike their undirected counterparts,
Directed Feedback Vertex Set and Directed $k$-Path do not admit dynamic
algorithms with $n^{o(1)}$ update and query times even for constant solution
sizes $k\leq 3$, assuming popular hardness hypotheses. We also show that
unconditionally, in the cell probe model, Directed Feedback Vertex Set cannot
be solved with update time that is purely a function of $k$.


Truly Sub-cubic Algorithms for Language Edit Distance and RNA Folding
  via Fast Bounded-Difference Min-Plus Product

  It is a major open problem whether the $(\min,+)$-product of two $n\times n$
matrices has a truly sub-cubic (i.e. $O(n^{3-\epsilon})$ for $\epsilon>0$) time
algorithm, in particular since it is equivalent to the famous
All-Pairs-Shortest-Paths problem (APSP) in $n$-vertex graphs. Some restrictions
of the $(\min,+)$-product to special types of matrices are known to admit truly
sub-cubic algorithms, each giving rise to a special case of APSP that can be
solved faster. In this paper we consider a new, different and powerful
restriction in which all matrix entries are integers and one matrix can be
arbitrary, as long as the other matrix has "bounded differences" in either its
columns or rows, i.e. any two consecutive entries differ by only a small
amount. We obtain the first truly sub-cubic algorithm for this
bounded-difference $(\min,+)$-product (answering an open problem of Chan and
Lewenstein).
  Our new algorithm, combined with a strengthening of an approach of L.~Valiant
for solving context-free grammar parsing with matrix multiplication, yields the
first truly sub-cubic algorithms for the following problems: Language Edit
Distance (a major problem in the parsing community), RNA-folding (a major
problem in bioinformatics) and Optimum Stack Generation (answering an open
problem of Tarjan).


Further limitations of the known approaches for matrix multiplication

  We consider the techniques behind the current best algorithms for matrix
multiplication. Our results are threefold.
  (1) We provide a unifying framework, showing that all known matrix
multiplication running times since 1986 can be achieved from a single very
natural tensor - the structural tensor $T_q$ of addition modulo an integer $q$.
  (2) We show that if one applies a generalization of the known techniques
(arbitrary zeroing out of tensor powers to obtain independent matrix products
in order to use the asymptotic sum inequality of Sch\"{o}nhage) to an arbitrary
monomial degeneration of $T_q$, then there is an explicit lower bound,
depending on $q$, on the bound on the matrix multiplication exponent $\omega$
that one can achieve. We also show upper bounds on the value $\alpha$ that one
can achieve, where $\alpha$ is such that $n\times n^\alpha \times n$ matrix
multiplication can be computed in $n^{2+o(1)}$ time.
  (3) We show that our lower bound on $\omega$ approaches $2$ as $q$ goes to
infinity. This suggests a promising approach to improving the bound on
$\omega$: for variable $q$, find a monomial degeneration of $T_q$ which, using
the known techniques, produces an upper bound on $\omega$ as a function of $q$.
Then, take $q$ to infinity. It is not ruled out, and hence possible, that one
can obtain $\omega=2$ in this way.


Limits on All Known (and Some Unknown) Approaches to Matrix
  Multiplication

  We study the known techniques for designing Matrix Multiplication algorithms.
The two main approaches are the Laser method of Strassen, and the Group
theoretic approach of Cohn and Umans. We define a generalization based on
zeroing outs which subsumes these two approaches, which we call the Solar
method, and an even more general method based on monomial degenerations, which
we call the Galactic method.
  We then design a suite of techniques for proving lower bounds on the value of
$\omega$, the exponent of matrix multiplication, which can be achieved by
algorithms using many tensors $T$ and the Galactic method. Some of our
techniques exploit `local' properties of $T$, like finding a sub-tensor of $T$
which is so `weak' that $T$ itself couldn't be used to achieve a good bound on
$\omega$, while others exploit `global' properties, like $T$ being a monomial
degeneration of the structural tensor of a group algebra.
  Our main result is that there is a universal constant $\ell>2$ such that a
large class of tensors generalizing the Coppersmith-Winograd tensor $CW_q$
cannot be used within the Galactic method to show a bound on $\omega$ better
than $\ell$, for any $q$. We give evidence that previous lower-bounding
techniques were not strong enough to show this. We also prove a number of
complementary results along the way, including that for any group $G$, the
structural tensor of $\mathbb{C}[G]$ can be used to recover the best bound on
$\omega$ which the Coppersmith-Winograd approach gets using $CW_{|G|-2}$ as
long as the asymptotic rank of the structural tensor is not too large.


Algorithms and Hardness for Diameter in Dynamic Graphs

  The diameter, radius and eccentricities are natural graph parameters. While
these problems have been studied extensively, there are no known dynamic
algorithms for them beyond the ones that follow from trivial recomputation
after each update or from solving dynamic All-Pairs Shortest Paths (APSP),
which is very computationally intensive. This is the situation for dynamic
approximation algorithms as well, and even if only edge insertions or edge
deletions need to be supported.
  This paper provides a comprehensive study of the dynamic approximation of
Diameter, Radius and Eccentricities, providing both conditional lower bounds,
and new algorithms whose bounds are optimal under popular hypotheses in
fine-grained complexity. Some of the highlights include:
  - Under popular hardness hypotheses, there can be no significantly better
fully dynamic approximation algorithms than recomputing the answer after each
update, or maintaining full APSP.
  - Nearly optimal partially dynamic (incremental/decremental) algorithms can
be achieved via efficient reductions to (incremental/decremental) maintenance
of Single-Source Shortest Paths. For instance, a nearly
$(3/2+\epsilon)$-approximation to Diameter in directed or undirected graphs can
be maintained decrementally in total time $m^{1+o(1)}\sqrt{n}/\epsilon^2$. This
nearly matches the static $3/2$-approximation algorithm for the problem that is
known to be conditionally optimal.


Simulating Branching Programs with Edit Distance and Friends or: A
  Polylog Shaved is a Lower Bound Made

  A recent and active line of work achieves tight lower bounds for fundamental
problems under the Strong Exponential Time Hypothesis (SETH). A celebrated
result of Backurs and Indyk (STOC'15) proves that the Edit Distance of two
sequences of length n cannot be computed in strongly subquadratic time under
SETH. The result was extended by follow-up works to simpler looking problems
like finding the Longest Common Subsequence (LCS).
  SETH is a very strong assumption, asserting that even linear size CNF
formulas cannot be analyzed for satisfiability with an exponential speedup over
exhaustive search. We consider much safer assumptions, e.g. that such a speedup
is impossible for SAT on much more expressive representations, like NC
circuits. Intuitively, this seems much more plausible: NC circuits can
implement complex cryptographic primitives, while CNFs cannot even
approximately compute an XOR of bits.
  Our main result is a surprising reduction from SAT on Branching Programs to
fundamental problems in P like Edit Distance, LCS, and many others. Truly
subquadratic algorithms for these problems therefore have consequences that we
consider to be far more remarkable than merely faster CNF SAT algorithms. For
example, SAT on arbitrary o(n)-depth bounded fan-in circuits (and therefore
also NC-Circuit-SAT) can be solved in (2-eps)^n time.
  A very interesting feature of our work is that we can prove major
consequences even from mildly subquadratic algorithms for Edit Distance or LCS.
For example, we show that if we can shave an arbitrarily large polylog factor
from n^2 for Edit Distance then NEXP does not have non-uniform NC^1 circuits. A
more fine-grained examination shows that even shaving a $\log^c{n}$ factor, for
a specific constant $c \approx 10^3$, already implies new circuit lower bounds.


Approximating the diameter of a graph

  In this paper we consider the fundamental problem of approximating the
diameter $D$ of directed or undirected graphs. In a seminal paper, Aingworth,
Chekuri, Indyk and Motwani [SIAM J. Comput. 1999] presented an algorithm that
computes in $\Ot(m\sqrt n + n^2)$ time an estimate $\hat{D}$ for the diameter
of an $n$-node, $m$-edge graph, such that $\lfloor 2/3 D \rfloor \leq \hat{D}
\leq D$. In this paper we present an algorithm that produces the same estimate
in $\Ot(m\sqrt n)$ expected running time. We then provide strong evidence that
a better approximation may be hard to obtain if we insist on an $O(m^{2-\eps})$
running time. In particular, we show that if there is some constant $\eps>0$ so
that there is an algorithm for undirected unweighted graphs that runs in
$O(m^{2-\eps})$ time and produces an approximation $\hat{D}$ such that $
(2/3+\eps) D \leq \hat{D} \leq D$, then SAT for CNF formulas on $n$ variables
can be solved in $O^{*}((2-\delta)^{n})$ time for some constant $\delta>0$, and
the strong exponential time hypothesis of [Impagliazzo, Paturi, Zane JCSS'01]
is false.
  Motivated by this somewhat negative result, we study whether it is possible
to obtain a better approximation for specific cases. For unweighted directed or
undirected graphs, we show that if $D=3h+z$, where $h\geq 0$ and $z\in
{0,1,2}$, then it is possible to report in $\tilde{O}(\min{m^{2/3}
n^{4/3},m^{2-1/(2h+3)}})$ time an estimate $\hat{D}$ such that $2h+z \leq
\hat{D}\leq D$, thus giving a better than 3/2 approximation whenever $z\neq 0$.
This is significant for constant values of $D$ which is exactly when the
diameter approximation problem is hardest to solve. For the case of unweighted
undirected graphs we present an $\tilde{O}(m^{2/3} n^{4/3})$ time algorithm
that reports an estimate $\hat{D}$ such that $\lfloor 4D/5\rfloor \leq
\hat{D}\leq D$.


If the Current Clique Algorithms are Optimal, so is Valiant's Parser

  The CFG recognition problem is: given a context-free grammar $\mathcal{G}$
and a string $w$ of length $n$, decide if $w$ can be obtained from
$\mathcal{G}$. This is the most basic parsing question and is a core computer
science problem. Valiant's parser from 1975 solves the problem in
$O(n^{\omega})$ time, where $\omega<2.373$ is the matrix multiplication
exponent. Dozens of parsing algorithms have been proposed over the years, yet
Valiant's upper bound remains unbeaten. The best combinatorial algorithms have
mildly subcubic $O(n^3/\log^3{n})$ complexity.
  Lee (JACM'01) provided evidence that fast matrix multiplication is needed for
CFG parsing, and that very efficient and practical algorithms might be hard or
even impossible to obtain. Lee showed that any algorithm for a more general
parsing problem with running time $O(|\mathcal{G}|\cdot n^{3-\varepsilon})$ can
be converted into a surprising subcubic algorithm for Boolean Matrix
Multiplication. Unfortunately, Lee's hardness result required that the grammar
size be $|\mathcal{G}|=\Omega(n^6)$. Nothing was known for the more relevant
case of constant size grammars.
  In this work, we prove that any improvement on Valiant's algorithm, even for
constant size grammars, either in terms of runtime or by avoiding the
inefficiencies of fast matrix multiplication, would imply a breakthrough
algorithm for the $k$-Clique problem: given a graph on $n$ nodes, decide if
there are $k$ that form a clique.
  Besides classifying the complexity of a fundamental problem, our reduction
has led us to similar lower bounds for more modern and well-studied cubic time
problems for which faster algorithms are highly desirable in practice: RNA
Folding, a central problem in computational biology, and Dyck Language Edit
Distance, answering an open question of Saha (FOCS'14).


Better Distance Preservers and Additive Spanners

  We make improvements to the upper bounds on several popular types of distance
preserving graph sketches. These sketches are all various restrictions of the
{\em additive pairwise spanner} problem, in which one is given an undirected
unweighted graph $G$, a set of node pairs $P$, and an error allowance $+\beta$,
and one must construct a sparse subgraph $H$ satisfying $\delta_H(u, v) \le
\delta_G(u, v) + \beta$ for all $(u, v) \in P$.
  The first part of our paper concerns {\em pairwise distance preservers},
which make the restriction $\beta=0$ (i.e. distances must be preserved {\em
exactly}). Our main result here is an upper bound of $|H| = O(n^{2/3}|P|^{2/3}
+ n|P|^{1/3})$ when $G$ is undirected and unweighted. This improves on existing
bounds whenever $|P| = \omega(n^{3/4})$, and it is the first such improvement
in the last ten years.
  We then devise a new application of distance preservers to graph clustering
algorithms, and we apply this algorithm to {\em subset spanners}, which require
$P = S \times S$ for some node subset $S$, and {\em (standard) spanners}, which
require $P = V \times V$. For both of these objects, our construction
generalizes the best known bounds when the error allowance is constant, and we
obtain the strongest polynomial error/sparsity tradeoff that has yet been
reported (in fact, for subset spanners, ours is the {\em first} nontrivial
construction that enjoys improved sparsity from a polynomial error allowance).
  We leave open a conjecture that $O(n^{2/3}|P|^{2/3} + n)$ pairwise distance
preservers are possible for undirected unweighted graphs. Resolving this
conjecture in the affirmative would improve and simplify our upper bounds for
all the graph sketches mentioned above.


Approximation and Fixed Parameter Subquadratic Algorithms for Radius and
  Diameter

  The radius and diameter are fundamental graph parameters. They are defined as
the minimum and maximum of the eccentricities in a graph, respectively, where
the eccentricity of a vertex is the largest distance from the vertex to another
node. In directed graphs, there are several versions of these problems. For
instance, one may choose to define the eccentricity of a node in terms of the
largest distance into the node, out of the node, the sum of the two directions
(i.e. roundtrip) and so on. All versions of diameter and radius can be solved
via solving all-pairs shortest paths (APSP), followed by a fast postprocessing
step. Solving APSP, however, on $n$-node graphs requires $\Omega(n^2)$ time
even in sparse graphs, as one needs to output $n^2$ distances.
  Motivated by known and new negative results on the impossibility of computing
these measures exactly in general graphs in truly subquadratic time, under
plausible assumptions, we search for \emph{approximation} and \emph{fixed
parameter subquadratic} algorithms, and for reasons why they do not exist.
  Our results include: - Truly subquadratic approximation algorithms for most
of the versions of Diameter and Radius with \emph{optimal} approximation
guarantees (given truly subquadratic time), under plausible assumptions. In
particular, there is a $2$-approximation algorithm for directed Radius with
one-way distances that runs in $\tilde{O}(m\sqrt{n})$ time, while a
$(2-\delta)$-approximation algorithm in $O(n^{2-\epsilon})$ time is unlikely. -
On graphs with treewidth $k$, we can solve the problems in
$2^{O(k\log{k})}n^{1+o(1)}$ time. We show that these algorithms are near
optimal since even a $(3/2-\delta)$-approximation algorithm that runs in time
$2^{o(k)}n^{2-\epsilon}$ would refute the plausible assumptions.


Preserving Distances in Very Faulty Graphs

  Preservers and additive spanners are sparse (hence cheap to store) subgraphs
that preserve the distances between given pairs of nodes exactly or with some
small additive error, respectively. Since real-world networks are prone to
failures, it makes sense to study fault-tolerant versions of the above
structures. This turns out to be a surprisingly difficult task. For every small
but arbitrary set of edge or vertex failures, the preservers and spanners need
to contain {\em replacement paths} around the faulted set. In this paper we
make substantial progress on fault tolerant preservers and additive spanners:
  (1) We present the first truly sub-quadratic size single-pair preservers in
unweighted (possibly directed) graphs for \emph{any} fixed number $f$ of
faults. Our result indeed generalizes to the single-source case, and can be
used to build new fault-tolerant additive spanners (for all pairs).
  (2) The size of the above single-pair preservers is $O(n^{2-g(f)})$ for some
positive function $g$, and grows to $O(n^2)$ for increasing $f$. We show that
this is necessary even in undirected unweighted graphs, and even if you allow
for a small additive error: If you aim at size $O(n^{2-\epsilon})$ for
$\epsilon>0$, then the additive error has to be $\Omega(\eps f)$. This
surprisingly matches known upper bounds in the literature.
  (3) For weighted graphs, we provide matching upper and lower bounds for the
single pair case. Namely, the size of the preserver is $\Theta(n^2)$ for $f\geq
2$ in both directed and undirected graphs, while for $f=1$ the size is
$\Theta(n)$ in undirected graphs. For directed graphs, we have a superlinear
upper bound and a matching lower bound.
  Most of our lower bounds extend to the distance oracle setting, where rather
than a subgraph we ask for any compact data structure.


Optimal Vertex Fault Tolerant Spanners (for fixed stretch)

  A $k$-spanner of a graph $G$ is a sparse subgraph $H$ whose shortest path
distances match those of $G$ up to a multiplicative error $k$. In this paper we
study spanners that are resistant to faults. A subgraph $H \subseteq G$ is an
$f$ vertex fault tolerant (VFT) $k$-spanner if $H \setminus F$ is a $k$-spanner
of $G \setminus F$ for any small set $F$ of $f$ vertices that might "fail." One
of the main questions in the area is: what is the minimum size of an $f$ fault
tolerant $k$-spanner that holds for all $n$ node graphs (as a function of $f$,
$k$ and $n$)? This question was first studied in the context of geometric
graphs [Levcopoulos et al. STOC '98, Czumaj and Zhao SoCG '03] and has more
recently been considered in general undirected graphs [Chechik et al. STOC '09,
Dinitz and Krauthgamer PODC '11].
  In this paper, we settle the question of the optimal size of a VFT spanner,
in the setting where the stretch factor $k$ is fixed. Specifically, we prove
that every (undirected, possibly weighted) $n$-node graph $G$ has a
$(2k-1)$-spanner resilient to $f$ vertex faults with $O_k(f^{1 - 1/k} n^{1 +
1/k})$ edges, and this is fully optimal (unless the famous Erdos Girth
Conjecture is false). Our lower bound even generalizes to imply that no data
structure capable of approximating $dist_{G \setminus F}(s, t)$ similarly can
beat the space usage of our spanner in the worst case. We also consider the
edge fault tolerant (EFT) model, defined analogously with edge failures rather
than vertex failures. We show that the same spanner upper bound applies in this
setting. Our data structure lower bound extends to the case $k=2$ (and hence we
close the EFT problem for $3$-approximations), but it falls to $\Omega(f^{1/2 -
1/(2k)} \cdot n^{1 + 1/k})$ for $k \ge 3$. We leave it as an open problem to
close this gap.


Fine-Grained I/O Complexity via Reductions: New lower bounds, faster
  algorithms, and a time hierarchy

  This paper initiates the study of I/O algorithms (minimizing cache misses)
from the perspective of fine-grained complexity (conditional polynomial lower
bounds). Specifically, we aim to answer why sparse graph problems are so hard,
and why the Longest Common Subsequence problem gets a savings of a factor of
the size of cache times the length of a cache line, but no more. We take the
reductions and techniques from complexity and fine-grained complexity and apply
them to the I/O model to generate new (conditional) lower bounds as well as
faster algorithms. We also prove the existence of a time hierarchy for the I/O
model, which motivates the fine-grained reductions.
  Using fine-grained reductions, we give an algorithm for distinguishing 2 vs.
3 diameter and radius that runs in $O(|E|^2/(MB))$ cache misses, which for
sparse graphs improves over the previous $O(|V|^2/B)$ running time. We give new
reductions from radius and diameter to Wiener index and median. We show
meaningful reductions between problems that have linear-time solutions in the
RAM model. The reductions use low I/O complexity (typically $O(n/B)$), and thus
help to finely capture the relationship between "I/O linear time" $\Theta(n/B)$
and RAM linear time $\Theta(n)$. We generate new I/O assumptions based on the
difficulty of improving sparse graph problem running times in the I/O model. We
create conjectures that the current best known algorithms for Single Source
Shortest Paths (SSSP), diameter, and radius are optimal. From these I/O-model
assumptions, we show that many of the known reductions in the word-RAM model
can naturally extend to hold in the I/O model as well (e.g., a lower bound on
the I/O complexity of Longest Common Subsequence that matches the best known
running time). Finally, we prove an analog of the Time Hierarchy Theorem in the
I/O model.


Nearly Optimal Separation Between Partially And Fully Retroactive Data
  Structures

  Since the introduction of retroactive data structures at SODA 2004, a major
unsolved problem has been to bound the gap between the best partially
retroactive data structure (where changes can be made to the past, but only the
present can be queried) and the best fully retroactive data structure (where
the past can also be queried) for any problem. It was proved in 2004 that any
partially retroactive data structure with operation time $T(n,m)$ can be
transformed into a fully retroactive data structure with operation time
$O(\sqrt{m} \cdot T(n,m))$, where $n$ is the size of the data structure and $m$
is the number of operations in the timeline [Demaine 2004], but it has been
open for 14 years whether such a gap is necessary.
  In this paper, we prove nearly matching upper and lower bounds on this gap
for all $n$ and $m$. We improve the upper bound for $n \ll \sqrt m$ by showing
a new transformation with multiplicative overhead $n \log m$. We then prove a
lower bound of $\min\{n \log m, \sqrt m\}^{1-o(1)}$ assuming any of the
following conjectures:
  - Conjecture I: Circuit SAT requires $2^{n - o(n)}$ time on $n$-input
circuits of size $2^{o(n)}$. (Far weaker than the well-believed SETH
conjecture, which asserts that CNF SAT with $n$ variables and $O(n)$ clauses
already requires $2^{n-o(n)}$ time.)
  - Conjecture II: Online $(\min,+)$ product between an integer $n\times n$
matrix and $n$ vectors requires $n^{3 - o(1)}$ time.
  - Conjecture III (3-SUM Conjecture): Given three sets $A,B,C$ of integers,
each of size $n$, deciding whether there exist $a \in A, b \in B, c \in C$ such
that $a + b + c = 0$ requires $n^{2 - o(1)}$ time.
  Our lower bound construction illustrates an interesting power of fully
retroactive queries: they can be used to quickly solve batched pair evaluation.
We believe this technique can prove useful for other data structure lower
bounds, especially dynamic ones.


