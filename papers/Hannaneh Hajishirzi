Reasoning about RoboCup Soccer Narratives

  This paper presents an approach for learning to translate simple narratives,i.e., texts (sequences of sentences) describing dynamic systems, into coherentsequences of events without the need for labeled training data. Our approachincorporates domain knowledge in the form of preconditions and effects ofevents, and we show that it outperforms state-of-the-art supervised learningsystems on the task of reconstructing RoboCup soccer games from theircommentaries.

Sampling First Order Logical Particles

  Approximate inference in dynamic systems is the problem of estimating thestate of the system given a sequence of actions and partial observations. Highprecision estimation is fundamental in many applications like diagnosis,natural language processing, tracking, planning, and robotics. In this paper wepresent an algorithm that samples possible deterministic executions of aprobabilistic sequence. The algorithm takes advantage of a compactrepresentation (using first order logic) for actions and world states toimprove the precision of its estimation. Theoretical and empirical results showthat the algorithm's expected error is smaller than propositional sampling andSequential Monte Carlo (SMC) sampling techniques.

Data-Driven Methods for Solving Algebra Word Problems

  We explore contemporary, data-driven techniques for solving math wordproblems over recent large-scale datasets. We show that well-tuned neuralequation classifiers can outperform more sophisticated models such as sequenceto sequence and self-attention across these datasets. Our error analysisindicates that, while fully data driven models show some promise, semantic andworld knowledge is necessary for further advances.

Semantic Understanding of Professional Soccer Commentaries

  This paper presents a novel approach to the problem of semantic parsing vialearning the correspondences between complex sentences and rich sets of events.Our main intuition is that correct correspondences tend to occur morefrequently. Our model benefits from a discriminative notion of similarity tolearn the correspondence between sentence and an event and a ranking machinerythat scores the popularity of each correspondence. Our method can discover agroup of events (called macro-events) that best describes a sentence. Weevaluate our method on our novel dataset of professional soccer commentaries.The empirical results show that our method significantly outperforms thestate-of-theart.

Talking to the crowd: What do people react to in online discussions?

  This paper addresses the question of how language use affects communityreaction to comments in online discussion forums, and the relative importanceof the message vs. the messenger. A new comment ranking task is proposed basedon community annotated karma in Reddit discussions, which controls for topicand timing of comments. Experimental work with discussion threads from sixsubreddits shows that the importance of different types of language featuresvaries with the community of interest.

Scientific Information Extraction with Semi-supervised Neural Tagging

  This paper addresses the problem of extracting keyphrases from scientificarticles and categorizing them as corresponding to a task, process, ormaterial. We cast the problem as sequence tagging and introduce semi-supervisedmethods to a neural tagging model, which builds on recent advances in namedentity recognition. Since annotated training data is scarce in this domain, weintroduce a graph-based semi-supervised algorithm together with a dataselection scheme to leverage unannotated articles. Both inductive andtransductive semi-supervised learning strategies outperform state-of-the-artinformation extraction performance on the 2017 SemEval Task 10 ScienceIE task.

Semi-Supervised Event Extraction with Paraphrase Clusters

  Supervised event extraction systems are limited in their accuracy due to thelack of available training data. We present a method for self-training eventextraction systems by bootstrapping additional training data. This is done bytaking advantage of the occurrence of multiple mentions of the same eventinstances across newswire articles from multiple sources. If our system canmake a highconfidence extraction of some mentions in such a cluster, it canthen acquire diverse training examples by adding the other mentions as well.Our experiments show significant performance improvements on multiple eventextractors over ACE 2005 and TAC-KBP 2015 datasets.

Scientific Relation Extraction with Selectively Incorporated Concept  Embeddings

  This paper describes our submission for the SemEval 2018 Task 7 shared taskon semantic relation extraction and classification in scientific papers. Weextend the end-to-end relation extraction model of (Miwa and Bansal) withenhancements such as a character-level encoding attention mechanism onselecting pretrained concept candidate embeddings. Our official submissionranked the second in relation classification task (Subtask 1.1 and Subtask 2Senerio 2), and the first in the relation extraction task (Subtask 2 Scenario1).

Multi-Task Identification of Entities, Relations, and Coreference for  Scientific Knowledge Graph Construction

  We introduce a multi-task setup of identifying and classifying entities,relations, and coreference clusters in scientific articles. We create SciERC, adataset that includes annotations for all three tasks and develop a unifiedframework called Scientific Information Extractor (SciIE) for with shared spanrepresentations. The multi-task setup reduces cascading errors between tasksand leverages cross-sentence relations through coreference links. Experimentsshow that our multi-task model outperforms previous models in scientificinformation extraction without using any domain-specific features. We furthershow that the framework supports construction of a scientific knowledge graph,which we use to analyze information in scientific literature.

Are Elephants Bigger than Butterflies? Reasoning about Sizes of Objects

  Human vision greatly benefits from the information about sizes of objects.The role of size in several visual reasoning tasks has been thoroughly exploredin human perception and cognition. However, the impact of the information aboutsizes of objects is yet to be determined in AI. We postulate that this ismainly attributed to the lack of a comprehensive repository of sizeinformation. In this paper, we introduce a method to automatically infer objectsizes, leveraging visual and textual information from web. By maximizing thejoint likelihood of textual and visual observations, our method learns reliablerelative size estimates, with no explicit human supervision. We introduce therelative size dataset and show that our method outperforms competitive textualand visual baselines in reasoning about size comparisons.

A Diagram Is Worth A Dozen Images

  Diagrams are common tools for representing complex concepts, relationshipsand events, often when it would be difficult to portray the same informationwith natural images. Understanding natural images has been extensively studiedin computer vision, while diagram understanding has received little attention.In this paper, we study the problem of diagram interpretation and reasoning,the challenging task of identifying the structure of a diagram and thesemantics of its constituents and their relationships. We introduce DiagramParse Graphs (DPG) as our representation to model the structure of diagrams. Wedefine syntactic parsing of diagrams as learning to infer DPGs for diagrams andstudy semantic interpretation and reasoning of diagrams in the context ofdiagram question answering. We devise an LSTM-based method for syntacticparsing of diagrams and introduce a DPG-based attention model for diagramquestion answering. We compile a new dataset of diagrams with exhaustiveannotations of constituents and relationships for over 5,000 diagrams and15,000 questions and answers. Our results show the significance of our modelsfor syntactic parsing and question answering in diagrams using DPGs.

Disfluency Detection using a Bidirectional LSTM

  We introduce a new approach for disfluency detection using a BidirectionalLong-Short Term Memory neural network (BLSTM). In addition to the wordsequence, the model takes as input pattern match features that were developedto reduce sensitivity to vocabulary size in training, which lead to improvedperformance over the word sequence alone. The BLSTM takes advantage of explicitrepair states in addition to the standard reparandum states. The final outputleverages integer linear programming to incorporate constraints of disfluencystructure. In experiments on the Switchboard corpus, the model achievesstate-of-the-art performance for both the standard disfluency detection taskand the correction detection task. Analysis shows that the model has betterdetection of non-repetition disfluencies, which tend to be much harder todetect.

Query-Reduction Networks for Question Answering

  In this paper, we study the problem of question answering when reasoning overmultiple facts is required. We propose Query-Reduction Network (QRN), a variantof Recurrent Neural Network (RNN) that effectively handles both short-term(local) and long-term (global) sequential dependencies to reason over multiplefacts. QRN considers the context sentences as a sequence of state-changingtriggers, and reduces the original query to a more informed query as itobserves each trigger (context sentence) through time. Our experiments showthat QRN produces the state-of-the-art results in bAbI QA and dialog tasks, andin a real goal-oriented dialog dataset. In addition, QRN formulation allowsparallelization on RNN's time axis, saving an order of magnitude in timecomplexity for training and inference.

View-Driven Deduplication with Active Learning

  Visual analytics systems such as Tableau are increasingly popular forinteractive data exploration. These tools, however, do not currently assistusers with detecting or resolving potential data quality problems including thewell-known deduplication problem. Recent approaches for deduplication focus oncleaning entire datasets and commonly require hundreds to thousands of userlabels. In this paper, we address the problem of deduplication in the contextof visual data analytics. We present a new approach for record deduplicationthat strives to produce the cleanest view possible with a limited budget fordata labeling. The key idea behind our approach is to consider the impact thatindividual tuples have on a visualization and to monitor how the view changesduring cleaning. With experiments on nine different visualizations for tworeal-world datasets, we show that our approach produces significantly cleanerviews for small labeling budgets than state-of-the-art alternatives and that italso stops the cleaning process after requesting fewer labels.

A Theme-Rewriting Approach for Generating Algebra Word Problems

  Texts present coherent stories that have a particular theme or overallsetting, for example science fiction or western. In this paper, we present atext generation method called {\it rewriting} that edits existinghuman-authored narratives to change their theme without changing the underlyingstory. We apply the approach to math word problems, where it might helpstudents stay more engaged by quickly transforming all of their homeworkassignments to the theme of their favorite movie without changing the mathconcepts that are being taught. Our rewriting method uses a two-stage decodingprocess, which proposes new words from the target theme and scores theresulting stories according to a number of factors defining aspects ofsyntactic, semantic, and thematic coherence. Experiments demonstrate that thefinal stories typically represent the new theme well while still testing theoriginal math concepts, outperforming a number of baselines. We also release anew dataset of human-authored rewrites of math word problems in several themes.

Bidirectional Attention Flow for Machine Comprehension

  Machine comprehension (MC), answering a query about a given contextparagraph, requires modeling complex interactions between the context and thequery. Recently, attention mechanisms have been successfully extended to MC.Typically these methods use attention to focus on a small portion of thecontext and summarize it with a fixed-size vector, couple attentionstemporally, and/or often form a uni-directional attention. In this paper weintroduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stagehierarchical process that represents the context at different levels ofgranularity and uses bi-directional attention flow mechanism to obtain aquery-aware context representation without early summarization. Ourexperimental evaluations show that our model achieves the state-of-the-artresults in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail clozetest.

Question Answering through Transfer Learning from Large Fine-grained  Supervision Data

  We show that the task of question answering (QA) can significantly benefitfrom the transfer learning of models trained on a different large, fine-grainedQA dataset. We achieve the state of the art in two well-studied QA datasets,WikiQA and SemEval-2016 (Task 3A), through a basic transfer learning techniquefrom SQuAD. For WikiQA, our model outperforms the previous best model by morethan 8%. We demonstrate that finer supervision provides better guidance forlearning lexical and syntactic information than coarser supervision, throughquantitative results and visual analysis. We also show that a similar transferlearning procedure achieves the state of the art on an entailment task.

Neural Speed Reading via Skim-RNN

  Inspired by the principles of speed reading, we introduce Skim-RNN, arecurrent neural network (RNN) that dynamically decides to update only a smallfraction of the hidden state for relatively unimportant input tokens. Skim-RNNgives computational advantage over an RNN that always updates the entire hiddenstate. Skim-RNN uses the same input and output interfaces as a standard RNN andcan be easily used instead of RNNs in existing models. In our experiments, weshow that Skim-RNN can achieve significantly reduced computational cost withoutlosing accuracy compared to standard RNNs across five different naturallanguage tasks. In addition, we demonstrate that the trade-off between accuracyand speed of Skim-RNN can be dynamically controlled during inference time in astable manner. Our analysis also shows that Skim-RNN running on a single CPUoffers lower latency compared to standard RNNs on GPUs.

Identifying Most Walkable Direction for Navigation in an Outdoor  Environment

  We present an approach for identifying the most walkable direction fornavigation using a hand-held camera. Our approach extracts semantically richcontextual information from the scene using a custom encoder-decoderarchitecture for semantic segmentation and models the spatial and temporalbehavior of objects in the scene using a spatio-temporal graph. The systemlearns to minimize a cost function over the spatial and temporal objectattributes to identify the most walkable direction. We construct a newannotated navigation dataset collected using a hand-held mobile camera in anunconstrained outdoor environment, which includes challenging settings such ashighly dynamic scenes, occlusion between objects, and distortions. Our systemachieves an accuracy of 84% on predicting a safe direction. We also show thatour custom segmentation network is both fast and accurate, achieving mIOU (meanintersection over union) scores of 81 and 44.7 on the PASCAL VOC and the PASCALContext datasets, respectively, while running at about 21 frames per second.

ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic  Segmentation

  We introduce a fast and efficient convolutional neural network, ESPNet, forsemantic segmentation of high resolution images under resource constraints.ESPNet is based on a new convolutional module, efficient spatial pyramid (ESP),which is efficient in terms of computation, memory, and power. ESPNet is 22times faster (on a standard GPU) and 180 times smaller than thestate-of-the-art semantic segmentation network PSPNet, while its category-wiseaccuracy is only 8% less. We evaluated ESPNet on a variety of semanticsegmentation datasets including Cityscapes, PASCAL VOC, and a breast biopsywhole slide image dataset. Under the same constraints on memory andcomputation, ESPNet outperforms all the current efficient CNN networks such asMobileNet, ShuffleNet, and ENet on both standard metrics and our newlyintroduced performance metrics that measure efficiency on edge devices. Ournetwork can process high resolution images at a rate of 112 and 9 frames persecond on a standard GPU and edge device, respectively.

Phrase-Indexed Question Answering: A New Challenge for Scalable Document  Comprehension

  We formalize a new modular variant of current question answering tasks byenforcing complete independence of the document encoder from the questionencoder. This formulation addresses a key challenge in machine comprehension byrequiring a standalone representation of the document discourse. Itadditionally leads to a significant scalability advantage since the encoding ofthe answer candidate phrases in the document can be pre-computed and indexedoffline for efficient retrieval. We experiment with baseline models for the newtask, which achieve a reasonable accuracy but significantly underperformunconstrained QA models. We invite the QA research community to engage inPhrase-Indexed Question Answering (PIQA, pika) for closing the gap. Theleaderboard is at: nlp.cs.washington.edu/piqa

Pyramidal Recurrent Unit for Language Modeling

  LSTMs are powerful tools for modeling contextual information, as evidenced bytheir success at the task of language modeling. However, modeling contexts invery high dimensional space can lead to poor generalizability. We introduce thePyramidal Recurrent Unit (PRU), which enables learning representations in highdimensional space with more generalization power and fewer parameters. PRUsreplace the linear transformation in LSTMs with more sophisticated interactionsincluding pyramidal and grouped linear transformations. This architecture givesstrong results on word-level language modeling while reducing the number ofparameters significantly. In particular, PRU improves the perplexity of arecent state-of-the-art language model Merity et al. (2018) by up to 1.3 pointswhile learning 15-20% fewer parameters. For similar number of model parameters,PRU outperforms all previous RNN models that exploit different gatingmechanisms and transformations. We provide a detailed examination of the PRUand its behavior on the language modeling tasks. Our code is open-source andavailable at https://sacmehta.github.io/PRU/

ESPNetv2: A Light-weight, Power Efficient, and General Purpose  Convolutional Neural Network

  We introduce a light-weight, power efficient, and general purposeconvolutional neural network, ESPNetv2, for modeling visual and sequentialdata. Our network uses group point-wise and depth-wise dilated separableconvolutions to learn representations from a large effective receptive fieldwith fewer FLOPs and parameters. The performance of our network is evaluated onfour different tasks: (1) object classification, (2) semantic segmentation, (3)object detection, and (4) language modeling. Experiments on these tasks,including image classification on the ImageNet and language modeling on thePenTree bank dataset, demonstrate the superior performance of our method overthe state-of-the-art methods. Our network outperforms ESPNet by 4-5% and has2-4x fewer FLOPs on the PASCAL VOC and the Cityscapes dataset. Compared toYOLOv2 on the MS-COCO object detection, ESPNetv2 delivers 4.4% higher accuracywith 6x fewer FLOPs. Our experiments show that ESPNetv2 is much more powerefficient than existing state-of-the-art efficient methods includingShuffleNets and MobileNets. Our code is open-source and available athttps://github.com/sacmehta/ESPNetv2

Text Generation from Knowledge Graphs with Graph Transformers

  Generating texts which express complex ideas spanning multiple sentencesrequires a structured representation of their content (document plan), butthese representations are prohibitively expensive to manually produce. In thiswork, we address the problem of generating coherent multi-sentence texts fromthe output of an information extraction system, and in particular a knowledgegraph. Graphical knowledge representations are ubiquitous in computing, butpose a significant challenge for text generation techniques due to theirnon-hierarchical nature, collapsing of long-distance dependencies, andstructural variety. We introduce a novel graph transforming encoder which canleverage the relational structure of such knowledge graphs without imposinglinearization or hierarchical constraints. Incorporated into an encoder-decodersetup, we provide an end-to-end trainable system for graph-to-text generationthat we apply to the domain of scientific text. Automatic and human evaluationsshow that our technique produces more informative texts which exhibit betterdocument structure than competitive encoder-decoder methods.

A General Framework for Information Extraction using Dynamic Span Graphs

  We introduce a general framework for several information extraction tasksthat share span representations using dynamically constructed span graphs. Thegraphs are constructed by selecting the most confident entity spans and linkingthese nodes with confidence-weighted relation types and coreferences. Thedynamic span graph allows coreference and relation type confidences topropagate through the graph to iteratively refine the span representations.This is unlike previous multi-task frameworks for information extraction inwhich the only interaction between tasks is in the shared first-layer LSTM. Ourframework significantly outperforms the state-of-the-art on multipleinformation extraction tasks across multiple datasets reflecting differentdomains. We further observe that the span enumeration approach is good atdetecting nested span entities, with significant F1 score improvement on theACE dataset.

