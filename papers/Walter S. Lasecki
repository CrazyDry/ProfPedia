Effect of Group Means on the Probability of Consensus

  In this study, groups who could not reach a consensus were investigated using
the group polarization paradigm. The purpose was to explore the conditions
leading to intragroup disagreement and attitude change following disagreement
among 269 participants. Analysis indicated that the probability of consensus
was low when the group means differed from the grand mean of the entire sample.
When small differences among group members were found, depolarization (reverse
direction of polarization) followed disagreement. These results suggested the
groups which deviated most from the population tendency were the most likely to
cause within-group disagreement, while within-group variances determined the
direction of attitude change following disagreement within the group.


Social Technologies for Developing Collective Intelligence in Networked
  Society

  The scientific problem in our project is defined as a question: how social
technologies could contribute to the development of smart and inclusive
society? The subject of our research are networked projects (virtual CI
systems) which include collective decision making tools and innovation
mechanisms allowing and encouraging individual and team creativity,
entrepreneurship, online collaboration, new forms of self-regulation and
self-governance, self-configuration of communities by considering these
projects as being catalyst for emergence of CI. The answers to these
theoretical questions could have huge practical implications by influencing
more reasonable and sophisticated application of social technologies in
practice.


Collective intelligence in Massive Online Dialogues

  The emergence and ongoing development of Web 2.0 technologies have enabled
new and advanced forms of collective intelligence at unprecedented scales,
allowing large numbers of individuals to act collectively and create high
quality intellectual artifacts. However, little is known about how and when
they indeed promote collective intelligence. In this manuscript, we provide a
survey of the automated tools developed to analyze discourse-centric collective
intelligence. By conducting a thematic analysis of the current research
direction, a set of gaps and limitations are identified.


Crowd Memory: Learning in the Collective

  Crowd algorithms often assume workers are inexperienced and thus fail to
adapt as workers in the crowd learn a task. These assumptions fundamentally
limit the types of tasks that systems based on such algorithms can handle. This
paper explores how the crowd learns and remembers over time in the context of
human computation, and how more realistic assumptions of worker experience may
be used when designing new systems. We first demonstrate that the crowd can
recall information over time and discuss possible implications of crowd memory
in the design of crowd algorithms. We then explore crowd learning during a
continuous control task. Recent systems are able to disguise dynamic groups of
workers as crowd agents to support continuous tasks, but have not yet
considered how such agents are able to learn over time. We show, using a
real-time gaming setting, that crowd agents can learn over time, and `remember'
by passing strategies from one generation of workers to the next, despite high
turnover rates in the workers comprising them. We conclude with a discussion of
future research directions for crowd memory and learning.


Tuning the Diversity of Open-Ended Responses from the Crowd

  Crowdsourcing can solve problems that current fully automated systems cannot.
Its effectiveness depends on the reliability, accuracy, and speed of the crowd
workers that drive it. These objectives are frequently at odds with one
another. For instance, how much time should workers be given to discover and
propose new solutions versus deliberate over those currently proposed? How do
we determine if discovering a new answer is appropriate at all? And how do we
manage workers who lack the expertise or attention needed to provide useful
input to a given task? We present a mechanism that uses distinct payoffs for
three possible worker actions---propose,vote, or abstain---to provide workers
with the necessary incentives to guarantee an effective (or even optimal)
balance between searching for new answers, assessing those currently available,
and, when they have insufficient expertise or insight for the task at hand,
abstaining. We provide a novel game theoretic analysis for this mechanism and
test it experimentally on an image---labeling problem and show that it allows a
system to reliably control the balance betweendiscovering new answers and
converging to existing ones.


Architectures of Virtual Decision-Making: The Emergence of Gender
  Discrimination on a Crowdfunding Website

  The increasing relevance of Internet-based markets requires a sustained
investigation into the relationship between design and user behavior. This
research begins within the sociology of quantification and markets to
investigate the impacts of basic design decisions on user behavior and
individual success on a widely used crowdfunding website. This study looks at
one common design feature, publishing recipients' sex, on the probability of
receiving funding. Following research in the sociology of gender, these effects
are defined along individual, behavioral, and structural dimensions. The
results reveal that before teachers' sex was published, gender discrimination
was weak and inconsistent. However, afterward gender discrimination increases
by an order of magnitude and becomes systematized. Contrary to expectation,
donors did not discriminate by sex category, but by teachers' structural
position and the kinds of language they used. Implications for research on
gender discrimination, priming, and online behavior are discussed.


Less-is-more in a 5-star rating system: an experimental study of human
  combined decisions in a multi-armed bandit problem

  Given the rapid proliferation of advanced information technologies, including
the Internet, modern humans can easily access vast amount of socially
transmitted information. Intuitively, this situation is isomorphic to some
eusocial insects that are known to solve the exploration-exploitation dilemma
collectively through information transfer (e.g., honeybees [Seeley et al.,
1991]; and ants [Shaffer, Sasaki & Pratt, 2013]). Yet, in contrast from the
eusocial insects, whose colonies are composed of kin, human collective
performance may be affected by an inherent free-rider problem [Bolton & Harris,
1999; Kameda, Tsukasaki, Hastie & Berg, 2011]. Specifically, in groups
involving non-kin members, it is expected that free-riders, who allow others to
search for better alternatives and then exploit their findings through social
learning ("information scroungers"), will frequently appear, and consequently
undermine the advantage of collective intelligence [Rogers, 1998; Kameda &
Nakanishi, 2003].


When none of us perform better than all of us together: the role of
  analogical decision rules in groups

  During social interactions, groups develop collective competencies that
(ideally) should assist groups to outperform average standalone individual
members (weak cognitive synergy) or the best performing member in the group
(strong cognitive synergy). In two experimental studies we manipulate the type
of decision rule used in group decision-making (identify the best vs.
collaborative), and the way in which the decision rules are induced (direct vs.
analogical) and we test the effect of these two manipulations on the emergence
of strong and weak cognitive synergy. Our most important results indicate that
an analogically induced decision rule (imitate-the-successful heuristic) in
which groups have to identify the best member and build on his/her performance
(take-the-best heuristic) is the most conducive for strong cognitive synergy.
Our studies bring evidence for the role of analogy-making in groups as well as
the role of fast-and-frugal heuristics for group decision-making.


Analytical reasoning task reveals limits of social learning in networks

  Social learning -by observing and copying others- is a highly successful
cultural mechanism for adaptation, outperforming individual information
acquisition and experience. Here, we investigate social learning in the context
of the uniquely human capacity for reflective, analytical reasoning. A hallmark
of the human mind is our ability to engage analytical reasoning, and suppress
false associative intuitions. Through a set of lab-based network experiments,
we find that social learning fails to propagate this cognitive strategy. When
people make false intuitive conclusions, and are exposed to the analytic output
of their peers, they recognize and adopt this correct output. But they fail to
engage analytical reasoning in similar subsequent tasks. Thus, humans exhibit
an 'unreflective copying bias,' which limits their social learning to the
output, rather than the process, of their peers' reasoning -even when doing so
requires minimal effort and no technical skill. In contrast to much recent work
on observation-based social learning, which emphasizes the propagation of
successful behavior through copying, our findings identify a limit on the power
of social networks in situations that require analytical reasoning.


Understanding Task Design Trade-offs in Crowdsourced Paraphrase
  Collection

  Linguistically diverse datasets are critical for training and evaluating
robust machine learning systems, but data collection is a costly process that
often requires experts. Crowdsourcing the process of paraphrase generation is
an effective means of expanding natural language datasets, but there has been
limited analysis of the trade-offs that arise when designing tasks. In this
paper, we present the first systematic study of the key factors in
crowdsourcing paraphrase collection. We consider variations in instructions,
incentives, data domains, and workflows. We manually analyzed paraphrases for
correctness, grammaticality, and linguistic diversity. Our observations provide
new insight into the trade-offs between accuracy and diversity in crowd
responses that arise as a result of task design, providing guidance for future
paraphrase generation procedures.


Transient Leadership and Collective Cell Movement in Early Diverged
  Multicellular Animals

  Collective motion of cells is critical to some of the most vital tasks
including wound healing, development, and immune response [Friedl and Gilmour
2009; Tokarski et al. 2012; Lee et al. 2012; Beltman et al. 2009], and is
common to many pathological processes including cancer cell invasion and
teratogenesis [Khalil and Friedl 2010]. The extensive understanding of movement
by single cells [R{\o}rth 2011; Insall and Machesky 2011; Houk et al. 2012] is
insufficient to predict the behavior of cellular groups [Theveneau et al. 2013;
Trepat, X. and Fredberg 2011], and identifying underlying rules of coordination
in collective cell migration is still evasive. Few of the supposed benefits of
collective motion have ever been tested at the cellular scale. As an example,
though collective sensing allows for larger groups to exhibit greater accuracy
in navigation [Simons 2004; Berdahl et al. 2013] and group taxis is possible
through the leadership of only a few individuals [Couzin et al. 2005], such
effects have never been investigated in collective cell migration. We will
investigate collective motion and decision-making in a primitive multicellular
animal, Trichoplax adhaerens to understand how intercellular coordination
affects animal behavior and how migration accuracy scales with cellular group
size.


Analyzing Assumptions in Conversation Disentanglement Research Through
  the Lens of a New Dataset and Model

  Disentangling conversations mixed together in a single stream of messages is
a difficult task with no large annotated datasets. We created a new dataset
that is 25 times the size of any previous publicly available resource, has
samples of conversation from 152 points in time across a decade, and is
annotated with both threads and a within-thread reply-structure graph. We also
developed a new neural network model, which extracts conversation threads
substantially more accurately than prior work. Using our annotated data and our
model we tested assumptions in prior work, revealing major issues in
heuristically constructed resources, and identifying how small datasets have
biased our understanding of multi-party multi-conversation chat.


Dialog System Technology Challenge 7

  This paper introduces the Seventh Dialog System Technology Challenges (DSTC),
which use shared datasets to explore the problem of building dialog systems.
Recently, end-to-end dialog modeling approaches have been applied to various
dialog tasks. The seventh DSTC (DSTC7) focuses on developing technologies
related to end-to-end dialog systems for (1) sentence selection, (2) sentence
generation and (3) audio visual scene aware dialog. This paper summarizes the
overall setup and results of DSTC7, including detailed descriptions of the
different tracks and provided datasets. We also describe overall trends in the
submitted systems and the key results. Each track introduced new datasets and
participants achieved impressive results using state-of-the-art end-to-end
technologies.


Information Spread in a Connected World

  In the following work, we compare the spread of information by word-of-mouth
(WOM) to the spread of information through search engines. We assume that the
initial acknowledgement of new information derives from social interactions but
that solid opinions are only formed after further evaluation through search
engines. Search engines can be viewed as central hubs that connect information
presented in relevant websites to searchers. Since they construct new
connections between searchers and information in every query performed, the
network structure is less relevant. Although models of viral spread of ideas
have been inspected in many previous works [1], [2], [3], [4], [5], [6], [7],
[8], [9], [10], [11], [12], [13], only few assume the acceptance of a novel
concept to be solely based on the evaluation of the opinions of others [8],
[5]. Following this approach, combined with that of models of information
spread with threshold [1] that claim the propagation in a network to occur only
if a threshold of neighbors hold an opinion, the proposed work adds a new
theoretical perspective that is relevant to the daily use of search engines as
a major information search tool. We continue by presenting some justifications
based on experimentations. Last we discuss possible outcomes of over use of
search engines vs. WOM, and suggest an hypothesis that such overuse might
actually narrow the collective information set.


Open Collaboration for Innovation: Principles and Performance

  The principles of open collaboration for innovation (and production), once
distinctive to open source software, are now found in many other ventures. Some
of these ventures are internet-based: Wikipedia, online forums and communities.
Others are off-line: in medicine, science, and everyday life. Such ventures
have been affecting traditional firms, and may represent a new organizational
form. Despite the impact of such ventures, questions remain about their
operating principles and performance. Here we define open collaboration (OC),
the underlying set of principles, and propose that it is a robust engine for
innovation and production. First, we review multiple OC ventures and identify
four defining principles. In all instances, participants create goods and
services of economic value, they exchange and reuse each other's work, they
labor purposefully with just loose coordination, and they permit anyone to
contribute and consume. These principles distinguish OC from other
organizational forms, such as firms or cooperatives. Next, we turn to
performance. To understand the performance of OC, we develop a computational
model, combining innovation theory with recent evidence on human cooperation.
We identify and investigate three elements that affect performance: the
cooperativeness of participants, the diversity of their needs, and the degree
to which the goods are rival (subtractable). Through computational experiments,
we find that OC performs well even in seemingly harsh environments: when
cooperators are a minority, free riders are present, diversity is lacking, or
goods are rival. We conclude that OC is viable and likely to expand into new
domains. The findings also inform the discussion on new organizational forms,
collaborative and communal.


Crowdsourcing for Participatory Democracies: Efficient Elicitation of
  Social Choice Functions

  We present theoretical and empirical results demonstrating the usefulness of
voting rules for participatory democracies. We first give algorithms which
efficiently elicit \epsilon-approximations to two prominent voting rules: the
Borda rule and the Condorcet winner. This result circumvents previous
prohibitive lower bounds and is surprisingly strong: even if the number of
ideas is as large as the number of participants, each participant will only
have to make a logarithmic number of comparisons, an exponential improvement
over the linear number of comparisons previously needed. We demonstrate the
approach in an experiment in Finland's recent off-road traffic law reform,
observing that the total number of comparisons needed to achieve a fixed
\epsilon approximation is linear in the number of ideas and that the constant
is not large.
  Finally, we note a few other experimental observations which support the use
of voting rules for aggregation. First, we observe that rating, one of the
common alternatives to ranking, manifested effects of bias in our data. Second,
we show that very few of the topics lacked a Condorcet winner, one of the
prominent negative results in voting. Finally, we show data hinting at a
potential future direction: the use of partial rankings as opposed to pairwise
comparisons to further decrease the elicitation time.


Influence Process Structural Learning and the Emergence of Collective
  Intelligence

  Recent work [Hazy 2012] has demonstrated computationally that collectives
that are organized into networks which govern the flow of resources can learn
to recognize newly emerging opportunities distributed in the environment. This
paper argues that the system does this through a process analogous to neural
network learning with relative status playing the role of synaptic weights.
Hazy showed computationally that learning of this type can occur even when
resource allocation decision makers have no direct visibility into the
environment, have no direct understanding of the opportunity, and are not
involved in their exploitation except to the extent that they evaluate the
success or failure of funded projects. Effectively, the system of interactions
learns which individuals have the best access to information and other
resources within the ecosystem. Hazy [2012] calls this previously unidentified
emergence phenomenon: Influence Process Structural Learning (IPSL). In the
prior model of IPSL, a three-tiered organizational structure was predetermined
in the model design [Hazy 2012]. These initial conditions delimit the extent to
which the emergence of collective intelligence can be posited because the model
itself assumes a defined structure. This work contributes to the field by
extending the IPSL argument for collective intelligence to a holistic emergence
argument. It begins by briefly reviewing previously published work. It
continues the conversation by adding two additional steps: Firstly, it shows
how a three-tier organizing structure might emerge through known complexity
mechanisms. In this case the mechanism identified is preferential attachment
[Barabasi 2002]. Secondly, the paper shows how collective intelligence can
emerge within a system of agents when the influence structure among these
agents is treated as a the genetic algorithm.


Collective Intelligence in Citizen Science -- A Study of Performers and
  Talkers

  The recent emergence of online citizen science is illustrative of an
efficient and effective means to harness the crowd in order to achieve a range
of scientific discoveries. Fundamentally, citizen science projects draw upon
crowds of non-expert volunteers to complete short Tasks, which can vary in
domain and complexity. However, unlike most human-computational systems,
participants in these systems, the `citizen scientists' are volunteers, whereby
no incentives, financial or otherwise, are offered. Furthermore, encouraged by
citizen science platforms such as Zooniverse, online communities have emerged,
providing them with an environment to discuss, share ideas, and solve problems.
In fact, it is the result of these forums that has enabled a number of
scientific discoveries to be made. In this paper we explore the phenomenon of
collective intelligence via the relationship between the activities of online
citizen science communities and the discovery of scientific knowledge. We
perform a cross-project analysis of ten Zooniverse citizen science projects and
analyse the behaviour of users with regards to their Task completion activity
and participation in discussion and discover collective behaviour amongst
highly active users. Whilst our findings have implications for future citizen
science design, we also consider the wider implications for understanding
collective intelligence research in general.


Human Communication Systems Evolve by Cultural Selection

  Human communication systems, such as language, evolve culturally; their
components undergo reproduction and variation. However, a role for selection in
cultural evolutionary dynamics is less clear. Often neutral evolution (also
known as 'drift') models, are used to explain the evolution of human
communication systems, and cultural evolution more generally. Under this
account, cultural change is unbiased: for instance, vocabulary, baby names and
pottery designs have been found to spread through random copying.
  While drift is the null hypothesis for models of cultural evolution it does
not always adequately explain empirical results. Alternative models include
cultural selection, which assumes variant adoption is biased. Theoretical
models of human communication argue that during conversation interlocutors are
biased to adopt the same labels and other aspects of linguistic representation
(including prosody and syntax). This basic alignment mechanism has been
extended by computer simulation to account for the emergence of linguistic
conventions. When agents are biased to match the linguistic behavior of their
interlocutor, a single variant can propagate across an entire population of
interacting computer agents. This behavior-matching account operates at the
level of the individual. We call it the Conformity-biased model. Under a
different selection account, called content-biased selection, functional
selection or replicator selection, variant adoption depends upon the intrinsic
value of the particular variant (e.g., ease of learning or use). This second
alternative account operates at the level of the cultural variant. Following
Boyd and Richerson we call it the Content-biased model. The present paper tests
the drift model and the two biased selection models' ability to explain the
spread of communicative signal variants in an experimental micro-society.


When is a crowd wise?

  Numerous studies and anecdotes demonstrate the "wisdom of the crowd," the
surprising accuracy of a group's aggregated judgments. Less is known, however,
about the generality of crowd wisdom. For example, are crowds wise even if
their members have systematic judgmental biases, or can influence each other
before members render their judgments? If so, are there situations in which we
can expect a crowd to be less accurate than skilled individuals? We provide a
precise but general definition of crowd wisdom: A crowd is wise if a linear
aggregate, for example a mean, of its members' judgments is closer to the
target value than a randomly, but not necessarily uniformly, sampled member of
the crowd. Building on this definition, we develop a theoretical framework for
examining, a priori, when and to what degree a crowd will be wise. We
systematically investigate the boundary conditions for crowd wisdom within this
framework and determine conditions under which the accuracy advantage for
crowds is maximized. Our results demonstrate that crowd wisdom is highly
robust: Even if judgments are biased and correlated, one would need to nearly
deterministically select only a highly skilled judge before an individual's
judgment could be expected to be more accurate than a simple averaging of the
crowd. Our results also provide an accuracy rationale behind the need for
diversity of judgments among group members. Contrary to folk explanations of
crowd wisdom which hold that judgments should ideally be independent so that
errors cancel out, we find that crowd wisdom is maximized when judgments
systematically differ as much as possible. We re-analyze data from two
published studies that confirm our theoretical results.


Wisdom of the Confident: Using Social Interactions to Eliminate the Bias
  in Wisdom of the Crowds

  Human groups can perform extraordinary accurate estimations compared to
individuals by simply using the mean, median or geometric mean of the
individual estimations [Galton 1907, Surowiecki 2005, Page 2008]. However, this
is true only for some tasks and in general these collective estimations show
strong biases. The method fails also when allowing for social interactions,
which makes the collective estimation worse as individuals tend to converge to
the biased result [Lorenz et al. 2011]. Here we show that there is a bright
side of this apparently negative impact of social interactions into collective
intelligence. We found that some individuals resist the social influence and,
when using the median of this subgroup, we can eliminate the bias of the wisdom
of the full crowd. To find this subgroup of individuals more confident in their
private estimations than in the social influence, we model individuals as
estimators that combine private and social information with different relative
weights [Perez-Escudero & de Polavieja 2011, Arganda et al. 2012]. We then
computed the geometric mean for increasingly smaller groups by eliminating
those using in their estimations higher values of the social influence weight.
The trend obtained in this procedure gives unbiased results, in contrast to the
simpler method of computing the median of the complete group. Our results show
that, while a simple operation like the mean, median or geometric mean of a
group may not allow groups to make good estimations, a more complex operation
taking into account individuality in the social dynamics can lead to a better
collective intelligence.


On Manipulation in Prediction Markets When Participants Influence
  Outcomes Directly

  Prediction markets are often used as mechanisms to aggregate information
about a future event, for example, whether a candidate will win an election.
The event is typically assumed to be exogenous. In reality, participants may
influence the outcome, and therefore (1) running the prediction market could
change the incentives of participants in the process that creates the outcome
(for example, agents may want to change their vote in an election), and (2)
simple results such as the myopic incentive compatibility of proper scoring
rules no longer hold in the prediction market itself. We introduce a model of
games of this kind, where agents first trade in a prediction market and then
take an action that influences the market outcome. Our two-stage two-player
model, despite its simplicity, captures two aspects of real-world prediction
markets: (1) agents may directly influence the outcome, (2) some of the agents
instrumental in deciding the outcome may not take part in the prediction
market. We show that this game has two different types of perfect Bayesian
equilibria, which we term LPP and HPP, depending on the values of the belief
parameters: in the LPP domain, equilibrium prices reveal expected market
outcomes conditional on the participants' private information, whereas HPP
equilibria are collusive -- participants effectively coordinate in an
uninformative and untruthful way.


"Is there anything else I can help you with?": Challenges in Deploying
  an On-Demand Crowd-Powered Conversational Agent

  Intelligent conversational assistants, such as Apple's Siri, Microsoft's
Cortana, and Amazon's Echo, have quickly become a part of our digital life.
However, these assistants have major limitations, which prevents users from
conversing with them as they would with human dialog partners. This limits our
ability to observe how users really want to interact with the underlying
system. To address this problem, we developed a crowd-powered conversational
assistant, Chorus, and deployed it to see how users and workers would interact
together when mediated by the system. Chorus sophisticatedly converses with end
users over time by recruiting workers on demand, which in turn decide what
might be the best response for each user sentence. Up to the first month of our
deployment, 59 users have held conversations with Chorus during 320
conversational sessions. In this paper, we present an account of Chorus'
deployment, with a focus on four challenges: (i) identifying when conversations
are over, (ii) malicious users and workers, (iii) on-demand recruiting, and
(iv) settings in which consensus is not enough. Our observations could assist
the deployment of crowd-powered conversation systems and crowd-powered systems
in general.


