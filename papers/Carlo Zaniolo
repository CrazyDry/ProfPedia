The Deductive Database System LDL++

  This paper describes the LDL++ system and the research advances that haveenabled its design and development. We begin by discussing the new nonmonotonicand nondeterministic constructs that extend the functionality of the LDL++language, while preserving its model-theoretic and fixpoint semantics. Then, wedescribe the execution model and the open architecture designed to supportthese new constructs and to facilitate the integration with existing DBMSs andapplications. Finally, we describe the lessons learned by using LDL++ onvarious tested applications, such as middleware and datamining.

Greedy Algorithms in Datalog

  In the design of algorithms, the greedy paradigm provides a powerful tool forsolving efficiently classical computational problems, within the framework ofprocedural languages. However, expressing these algorithms within thedeclarative framework of logic-based languages has proven a difficult researchchallenge. In this paper, we extend the framework of Datalog-like languages toobtain simple declarative formulations for such problems, and propose effectiveimplementation techniques to ensure computational complexities comparable tothose of procedural formulations. These advances are achieved through the useof the "choice" construct, extended with preference annotations to effect theselection of alternative stable-models and nondeterministic fixpoints. We showthat, with suitable storage structures, the differential fixpoint computationof our programs matches the complexity of procedural algorithms in classicalsearch and optimization problems.

Early Accurate Results for Advanced Analytics on MapReduce

  Approximate results based on samples often provide the only way in whichadvanced analytical applications on very massive data sets can satisfy theirtime and resource constraints. Unfortunately, methods and tools for thecomputation of accurate early results are currently not supported inMapReduce-oriented systems although these are intended for `big data'.Therefore, we proposed and implemented a non-parametric extension of Hadoopwhich allows the incremental computation of early results for arbitrarywork-flows, along with reliable on-line estimates of the degree of accuracyachieved so far in the computation. These estimates are based on a techniquecalled bootstrapping that has been widely employed in statistics and can beapplied to arbitrary functions and data distributions. In this paper, wedescribe our Early Accurate Result Library (EARL) for Hadoop that was designedto minimize the changes required to the MapReduce framework. Various tests ofEARL of Hadoop are presented to characterize the frequent situations where EARLcan provide major speed-ups over the current version of Hadoop.

Co-training Embeddings of Knowledge Graphs and Entity Descriptions for  Cross-lingual Entity Alignment

  Multilingual knowledge graph (KG) embeddings provide latent semanticrepresentations of entities and structured knowledge with cross-lingualinferences, which benefit various knowledge-driven cross-lingual NLP tasks.However, precisely learning such cross-lingual inferences is usually hinderedby the low coverage of entity alignment in many KGs. Since many multilingualKGs also provide literal descriptions of entities, in this paper, we introducean embedding-based approach which leverages a weakly aligned multilingual KGfor semi-supervised cross-lingual learning using entity descriptions. Ourapproach performs co-training of two embedding models, i.e. a multilingual KGembedding model and a multilingual literal description embedding model. Themodels are trained on a large Wikipedia-based trilingual dataset where mostentity alignment is unknown to training. Experimental results show that theperformance of the proposed approach on the entity alignment task improves ateach iteration of co-training, and eventually reaches a stage at which itsignificantly surpasses previous approaches. We also show that our approach haspromising abilities for zero-shot entity alignment, and cross-lingual KGcompletion.

Learning to Represent Bilingual Dictionaries

  Bilingual word embeddings have been widely used to capture the similarity oflexical semantics in different human languages. However, many applications,such as cross-lingual semantic search and question answering, can be largelybenefited from the cross-lingual correspondence between sentences and lexicons.To bridge this gap, we propose a neural embedding model that leveragesbilingual dictionaries. The proposed model is trained to map the literal worddefinitions to the cross-lingual target words, for which we explore withdifferent sentence encoding techniques. To enhance the learning process onlimited resources, our model adopts several critical learning strategies,including multi-task learning on different bridges of languages, and jointlearning of the dictionary model with a bilingual word embedding model.Experimental evaluation focuses on two applications. The results of thecross-lingual reverse dictionary retrieval task show our model's promisingability of comprehending bilingual concepts based on descriptions, andhighlight the effectiveness of proposed learning strategies in improvingperformance. Meanwhile, our model effectively addresses the bilingualparaphrase identification problem and significantly outperforms previousapproaches.

On2Vec: Embedding-based Relation Prediction for Ontology Population

  Populating ontology graphs represents a long-standing problem for theSemantic Web community. Recent advances in translation-based graph embeddingmethods for populating instance-level knowledge graphs lead to promising newapproaching for the ontology population problem. However, unlike instance-levelgraphs, the majority of relation facts in ontology graphs come withcomprehensive semantic relations, which often include the properties oftransitivity and symmetry, as well as hierarchical relations. Thesecomprehensive relations are often too complex for existing graph embeddingmethods, and direct application of such methods is not feasible. Hence, wepropose On2Vec, a novel translation-based graph embedding method for ontologypopulation. On2Vec integrates two model components that effectivelycharacterize comprehensive relation facts in ontology graphs. The first is theComponent-specific Model that encodes concepts and relations intolow-dimensional embedding spaces without a loss of relational properties; thesecond is the Hierarchy Model that performs focused learning of hierarchicalrelation facts. Experiments on several well-known ontology graphs demonstratethe promising capabilities of On2Vec in predicting and verifying new relationfacts. These promising results also make possible significant improvements inrelated methods.

Quantification and Analysis of Scientific Language Variation Across  Research Fields

  Quantifying differences in terminologies from various academic domains hasbeen a longstanding problem yet to be solved. We propose a computationalapproach for analyzing linguistic variation among scientific research fields bycapturing the semantic change of terms based on a neural language model. Themodel is trained on a large collection of literature in five computer scienceresearch fields, for which we obtain field-specific vector representations forkey terms, and global vector representations for other words. Severalquantitative approaches are introduced to identify the terms whose semanticshave drastically changed, or remain unchanged across different research fields.We also propose a metric to quantify the overall linguistic variation ofresearch fields. After quantitative evaluation on human annotated data andqualitative comparison with other methods, we show that our model can improvecross-disciplinary data collaboration by identifying terms that potentiallyinduce confusion during interdisciplinary studies.

Succinct Sampling on Streams

  A streaming model is one where data items arrive over long period of time,either one item at a time or in bursts. Typical tasks include computing variousstatistics over a sliding window of some fixed time-horizon. What makes thestreaming model interesting is that as the time progresses, old items expireand new ones arrive. One of the simplest and central tasks in this model issampling. That is, the task of maintaining up to $k$ uniformly distributeditems from a current time-window as old items expire and new ones arrive. Wecall sampling algorithms {\bf succinct} if they use provably optimal (up toconstant factors) {\bf worst-case} memory to maintain $k$ items (either with orwithout replacement). We stress that in many applications structures that have{\em expected} succinct representation as the time progresses are notsufficient, as small probability events eventually happen with probability 1.Thus, in this paper we ask the following question: are Succinct Sampling onStreams (or $S^3$-algorithms)possible, and if so for what models? Perhapssomewhat surprisingly, we show that $S^3$-algorithms are possible for {\em all}variants of the problem mentioned above, i.e. both with and without replacementand both for one-at-a-time and bursty arrival models. Finally, we use $S^3$algorithms to solve various problems in sliding windows model, includingfrequency moments, counting triangles, entropy and density estimations. Forthese problems we present \emph{first} solutions with provable worst-casememory guarantees.

Multilingual Knowledge Graph Embeddings for Cross-lingual Knowledge  Alignment

  Many recent works have demonstrated the benefits of knowledge graphembeddings in completing monolingual knowledge graphs. Inasmuch as relatedknowledge bases are built in several different languages, achievingcross-lingual knowledge alignment will help people in constructing a coherentknowledge base, and assist machines in dealing with different expressions ofentity relationships across diverse human languages. Unfortunately, achievingthis highly desirable crosslingual alignment by human labor is very costly anderrorprone. Thus, we propose MTransE, a translation-based model formultilingual knowledge graph embeddings, to provide a simple and automatedsolution. By encoding entities and relations of each language in a separatedembedding space, MTransE provides transitions for each embedding vector to itscross-lingual counterparts in other spaces, while preserving thefunctionalities of monolingual embeddings. We deploy three different techniquesto represent cross-lingual transitions, namely axis calibration, translationvectors, and linear transformations, and derive five variants for MTransE usingdifferent loss functions. Our models can be trained on partially alignedgraphs, where just a small portion of triples are aligned with theircross-lingual counterparts. The experiments on cross-lingual entity matchingand triple-wise alignment verification show promising results, with somevariants consistently outperforming others on different tasks. We also explorehow MTransE preserves the key properties of its monolingual counterpart TransE.

Fixpoint Semantics and Optimization of Recursive Datalog Programs with  Aggregates

  A very desirable Datalog extension investigated by many researchers in thelast thirty years consists in allowing the use of the basic SQL aggregates min,max, count and sum in recursive rules. In this paper, we propose a simplecomprehensive solution that extends the declarative least-fixpoint semantics ofHorn Clauses, along with the optimization techniques used in the bottom-upimplementation approach adopted by many Datalog systems. We start byidentifying a large class of programs of great practical interest in which theuse of min or max in recursive rules does not compromise the declarativefixpoint semantics of the programs using those rules. Then, we revisit themonotonic versions of count and sum aggregates proposed in (Mazuran et al.2013b) and named, respectively, mcount and msum. Since mcount, and also msum onpositive numbers, are monotonic in the lattice of set-containment, theypreserve the fixpoint semantics of Horn Clauses. However, in many applicationsof practical interest, their use can lead to inefficiencies, that can beeliminated by combining them with max, whereby mcount and msum become thestandard count and sum. Therefore, the semantics and optimization techniques ofDatalog are extended to recursive programs with min, max, count and sum, makingpossible the advanced applications of superior performance and scalabilitydemonstrated by BigDatalog (Shkapsky et al. 2016) and Datalog-MC (Yang et al.2017). This paper is under consideration for acceptance in TPLP.

Scaling-Up Reasoning and Advanced Analytics on BigData

  BigDatalog is an extension of Datalog that achieves performance andscalability on both Apache Spark and multicore systems to the point that itsgraph analytics outperform those written in GraphX. Looking back, we see howthis realizes the ambitious goal pursued by deductive database researchersbeginning forty years ago: this is the goal of combining the rigor and power oflogic in expressing queries and reasoning with the performance and scalabilityby which relational databases managed Big Data. This goal led to Datalog whichis based on Horn Clauses like Prolog but employs implementation techniques,such as Semi-naive Fixpoint and Magic Sets, that extend the bottom-upcomputation model of relational systems, and thus obtain the performance andscalability that relational systems had achieved, as far back as the 80s, usingdata-parallelization on shared-nothing architectures. But this goal proveddifficult to achieve because of major issues at (i) the language level and (ii)at the system level. The paper describes how (i) was addressed by simple rulesunder which the fixpoint semantics extends to programs using count, sum andextrema in recursion, and (ii) was tamed by parallel compilation techniquesthat achieve scalability on multicore systems and Apache Spark. This paper isunder consideration for acceptance in Theory and Practice of Logic Programming(TPLP).

Neural Article Pair Modeling for Wikipedia Sub-article Matching

  Nowadays, editors tend to separate different subtopics of a long Wiki-pediaarticle into multiple sub-articles. This separation seeks to improve humanreadability. However, it also has a deleterious effect on many Wikipedia-basedtasks that rely on the article-as-concept assumption, which requires eachentity (or concept) to be described solely by one article. This underlyingassumption significantly simplifies knowledge representation and extraction,and it is vital to many existing technologies such as automated knowledge baseconstruction, cross-lingual knowledge alignment, semantic search and datalineage of Wikipedia entities. In this paper we provide an approach to matchthe scattered sub-articles back to their corresponding main-articles, with theintent of facilitating automated Wikipedia curation and processing. Theproposed model adopts a hierarchical learning structure that combines multiplevariants of neural document pair encoders with a comprehensive set of explicitfeatures. A large crowdsourced dataset is created to support the evaluation andfeature extraction for the task. Based on the large dataset, the proposed modelachieves promising results of cross-validation and significantly outperformsprevious approaches. Large-scale serving on the entire English Wikipedia alsoproves the practicability and scalability of the proposed model by effectivelyextracting a vast collection of newly paired main and sub-articles.

Embedding Uncertain Knowledge Graphs

  Embedding models for deterministic Knowledge Graphs (KG) have beenextensively studied, with the purpose of capturing latent semantic relationsbetween entities and incorporating the structured knowledge into machinelearning. However, there are many KGs that model uncertain knowledge, whichtypically model the inherent uncertainty of relations facts with a confidencescore, and embedding such uncertain knowledge represents an unresolvedchallenge. The capturing of uncertain knowledge will benefit manyknowledge-driven applications such as question answering and semantic search byproviding more natural characterization of the knowledge. In this paper, wepropose a novel uncertain KG embedding model UKGE, which aims to preserve bothstructural and uncertainty information of relation facts in the embeddingspace. Unlike previous models that characterize relation facts with binaryclassification techniques, UKGE learns embeddings according to the confidencescores of uncertain relation facts. To further enhance the precision of UKGE,we also introduce probabilistic soft logic to infer confidence scores forunseen relation facts during training. We propose and evaluate two variants ofUKGE based on different learning objectives. Experiments are conducted on threereal-world uncertain KGs via three tasks, i.e. confidence prediction, relationfact ranking, and relation fact classification. UKGE shows effectiveness incapturing uncertain knowledge by achieving promising results on these tasks,and consistently outperforms baselines on these tasks.

How Much Are You Willing to Share? A "Poker-Styled" Selective Privacy  Preserving Framework for Recommender Systems

  Most industrial recommender systems rely on the popular collaborativefiltering (CF) technique for providing personalized recommendations to itsusers. However, the very nature of CF is adversarial to the idea of userprivacy, because users need to share their preferences with others in order tobe grouped with like-minded people and receive accurate recommendations. Whileprevious privacy preserving approaches have been successful inasmuch as theyconcealed user preference information to some extent from a centralizedrecommender system, they have also, nevertheless, incurred significanttrade-offs in terms of privacy, scalability, and accuracy. They are alsovulnerable to privacy breaches by malicious actors. In light of theseobservations, we propose a novel selective privacy preserving (SP2) paradigmthat allows users to custom define the scope and extent of their individualprivacies, by marking their personal ratings as either public (which can beshared) or private (which are never shared and stored only on the user device).Our SP2 framework works in two steps: (i) First, it builds an initialrecommendation model based on the sum of all public ratings that have beenshared by users and (ii) then, this public model is fine-tuned on each user'sdevice based on the user private ratings, thus eventually learning a moreaccurate model. Furthermore, in this work, we introduce three differentalgorithms for implementing an end-to-end SP2 framework that can scaleeffectively from thousands to hundreds of millions of items. Our user surveyshows that an overwhelming fraction of users are likely to rate much more itemsto improve the overall recommendations when they can control what ratings willbe publicly shared with others.

